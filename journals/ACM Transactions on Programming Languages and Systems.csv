Title,Year,Source title,Link,Abstract,Author Keywords,Index Keywords
Selective X-sensitive analysis guided by impact pre-analysis,2015,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84953277475&doi=10.1145%2f2821504&partnerID=40&md5=ee1ff702359ab9ddfbcd85b4bb6b8f6f,"We present a method for selectively applying context-sensitivity during interprocedural program analysis. Our method applies context-sensitivity only when and where doing so is likely to improve the precision that matters for resolving given queries. The idea is to use a pre-analysis to estimate the impact of contextsensitivity on the main analysis's precision, and to use this information to find out when and where the main analysis should turn on or off its context-sensitivity. We formalize this approach and prove that the analysis always benefits from the pre-analysis-guided context-sensitivity. We implemented this selective method for an existing industrial-strength interval analyzer for full C. The method reduced the number of (false) alarms by 24.4% while increasing the analysis cost by 27.8% on average. The use of the selective method is not limited to context-sensitivity. We demonstrate this generality by following the same principle and developing a selective relational analysis and a selective flow-sensitive analysis. Our experiments show that the method cost-effectively improves the precision in the these analyses as well.",,Linguistics; Software engineering; Analysis costs; Context sensitivity; Flow-sensitive analysis; Industrial strength; Interprocedural program analysis; Relational analysis; Sensitive analysis; Cost benefit analysis
Compiler-driven software speculation for thread-level parallelism,2015,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84953217091&doi=10.1145%2f2821505&partnerID=40&md5=4d1659c9cb9135971daeb2683f8e615a,"Current parallelizing compilers can tackle applications exercising regular access patterns on arrays or affine indices, where data dependencies can be expressed in a linear form. Unfortunately, there are cases that independence between statements of code cannot be guaranteed and thus the compiler conservatively produces sequential code. Programs that involve extensive pointer use, irregular access patterns, and loops with unknown number of iterations are examples of such cases. This limits the extraction of parallelism in cases where dependencies are rarely or never triggered at runtime. Speculative parallelism refers to methods employed during program execution that aim to produce a valid parallel execution schedule for programs immune to static parallelization. The motivation for this article is to review recent developments in the area of compiler-driven software speculation for thread-level parallelism and how they came about. The article is divided into two parts. In the first part the fundamentals of speculative parallelization for thread-level parallelism are explained along with a design choice categorization for implementing such systems. Design choices include the ways speculative data is handled, how data dependence violations are detected and resolved, how the correct data are made visible to other threads, or how speculative threads are scheduled. The second part is structured around those design choices providing the advances and trends in the literature with reference to key developments in the area. Although the focus of the article is in software speculative parallelization, a section is dedicated for providing the interested reader with pointers and references for exploring similar topics such as hardware thread-level speculation, transactional memory, and automatic parallelization.",Automatic parallelization; Multicore processors; Runtime parallelization; Speculative parallelization; Thread-level speculation,Multicore programming; Automatic Parallelization; Multi-core processor; Parallelizations; Speculative parallelization; Thread level speculation; Program compilers
A logical approach to deciding semantic subtyping,2015,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84946576026&doi=10.1145%2f2812805&partnerID=40&md5=8b72e672457970ef1a2f348b9718743c,"We consider a type algebra equipped with recursive, product, function, intersection, union, and complement types, together with type variables. We consider the subtyping relation defined by Castagna and Xu [2011] over such type expressions and show how this relation can be decided in EXPTIME, answering an open question. The novelty, originality and strength of our solution reside in introducing a logical modeling for the semantic subtyping framework. We model semantic subtyping in a tree logic and use a satisfiability-testing algorithm in order to decide subtyping. We report on practical experiments made with a full implementation of the system. This provides a powerful polymorphic type system aiming at maintaining full static type-safety of functional programs that manipulate trees, even with higher-order functions, which is particularly useful in the context of XML. © 2015 ACM.",Algorithms; Design; Languages; Theory; Verification,Algorithms; Design; Forestry; Query languages; Semantics; Trees (mathematics); Verification; Functional programs; Higher order functions; Logical approaches; Polymorphic type systems; Satisfiability testing; Semantic subtyping; Subtyping relation; Theory; Formal languages
A dynamic continuation-passing style for dynamic delimited continuations,2015,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84946576630&doi=10.1145%2f2794078&partnerID=40&md5=94d0645ebdd7b984cc7130efba76e409,"We put a preexisting definitional abstract machine for dynamic delimited continuations in defunctionalized form, and we present the consequences of this adjustment. We first prove the correctness of the adjusted abstract machine. Because it is in defunctionalized form, we can refunctionalize it into a higher-order evaluation function. This evaluation function, which is compositional, is in continuation+state-passing style and threads a trail of delimited continuations and a meta-continuation. Since this style accounts for dynamic delimited continuations, we refer to it as ""dynamic continuation-passing style"" and we present the corresponding dynamic CPS transformation. We show that the notion of computation induced by dynamic CPS takes the form of a continuation monad with a recursive answer type. This continuation monad suggests a new simulation of dynamic delimited continuations in terms of static ones. Finally, we present new applications of dynamic delimited continuations, including a meta-circular evaluator. The significance of the present work is that the computational artifacts surrounding dynamic CPS are not independent designs: they are mechanical consequences of having put the definitional abstract machine in defunctionalized form. © 2015 ACM.",Algorithms; Languages; Theory,Algorithms; Computation theory; Query languages; Abstract machines; Continuation-passing style; Evaluation function; Higher-order; Independent design; New applications; Theory; Function evaluation
Fast: A transducer-based language for tree manipulation,2015,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84946570136&doi=10.1145%2f2791292&partnerID=40&md5=c7d7debc4ea0ea48d71d589dee194c1c,"Tree automata and transducers are used in a wide range of applications in software engineering. While these formalisms are of immense practical use, they can only model finite alphabets. To overcome this problem we augment tree automata and transducers with symbolic alphabets represented as parametric theories. Admitting infinite alphabets makes these models more general and succinct than their classic counterparts. Despite this, we show how the main operations, such as composition and language equivalence, remain computable given a decision procedure for the alphabet theory. We introduce a high-level language called FAST that acts as a front-end for the preceding formalisms. © 2015 ACM.",Algorithms; Languages; Verification,Algorithms; Application programs; Automata theory; Computability and decidability; Decision theory; Equivalence classes; Forestry; Query languages; Robots; Transducers; Verification; Decision procedure; Finite alphabet; Front end; Practical use; Tree automata; High level languages
"Behavioral subtyping, specification inheritance, and modular reasoning",2015,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84939839513&doi=10.1145%2f2766446&partnerID=40&md5=cb238007d3ceb46e996b970ac14d05ea,"Verification of a dynamically dispatched method call, E.m(), seems to depend on E's dynamic type. To avoid case analysis and allow incremental development, object-oriented program verification uses supertype abstraction. In other words, one reasons about E.m() using m's specification for E's static type. Supertype abstraction is valid when each subtype in the program is a behavioral subtype. This article semantically formalizes supertype abstraction and behavioral subtyping for a Java-like sequential language with mutation and proves that behavioral subtyping is both necessary and sufficient for the validity of supertype abstraction. Specification inheritance, as in JML, is also formalized and proved to entail behavioral subtyping. © 2015 ACM.",Behavioral subtyping; Dynamic dispatch; Eiffel language; JML language; Modularity; Predicate transformer; Refinement; Specification; Specification inheritance; State transformer; Supertype abstraction; Verification,Abstracting; Computational linguistics; Formal languages; Java programming language; Specifications; Verification; Behavioral subtyping; Dynamic dispatch; Eiffel language; JML language; Modularity; Predicate transformers; Refinement; State-transformers; Supertype; Object oriented programming
Polyhedral AST generation is more than scanning polyhedra,2015,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84939790774&doi=10.1145%2f2743016&partnerID=40&md5=da339d630b27f3416ea1f0103dcfe470,"Mathematical representations such as integer polyhedra have been shown to be useful to precisely analyze computational kernels and to express complex loop transformations. Such transformations rely on abstract syntax tree (AST) generators to convert the mathematical representation back to an imperative program. Such generic AST generators avoid the need to resort to transformation-specific code generators, which may be very costly or technically difficult to develop as transformations become more complex. Existing AST generators have proven their effectiveness, but they hit limitations in more complex scenarios. Specifically, (1) they do not support or may fail to generate control flow for complex transformations using piecewise schedules or mappings involving modulo arithmetic; (2) they offer limited support for the specialization of the generated code exposing compact, straightline, vectorizable kernels with high arithmetic intensity necessary to exploit the peak performance of modern hardware; (3) they offer no support for memory layout transformations; and (4) they provide insufficient control over the AST generation strategy, preventing their application to complex domain-specific optimizations. We present a new AST generation approach that extends classical polyhedral scanning to the full generality of Presburger arithmetic, including existentially quantified variables and piecewise schedules, and introduce new optimizations for the detection of components and shifted strides. Not limiting ourselves to control flow generation, we expose functionality to generate AST expressions from arbitrary piecewise quasi-affine expressions, which enables the use of our AST generator for data-layout transformations. We complement this with support for specialization by polyhedral unrolling, user-directed versioning, and specialization of AST expressions according to the location at which they are generated, and we complete this work with finegrained user control over the AST generation strategies used. Using this generalized idea of AST generation, we present how to implement complex domain-specific transformations without the need to write specialized code generators, but instead relying on a generic AST generator parametrized to a specific problem domain. © 2015.",Code generation; Index set splitting; Polyhedral compilation; Presburger relations; Unrolling,Codes (symbols); Digital arithmetic; Geometry; Program compilers; Trees (mathematics); Code Generation; Index-set splitting; Polyhedral compilation; Presburger; Unrolling; Mathematical transformations
Affine refinement types for secure distributed programming,2015,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84939865212&doi=10.1145%2f2743018&partnerID=40&md5=7add289283dcd6ad90f7c1b9c54bcd26,"Recent research has shown that it is possible to leverage general-purpose theorem-proving techniques to develop powerful type systems for the verification of a wide range of security properties on application code. Although successful in many respects, these type systems fall short of capturing resource-conscious properties that are crucial in large classes of modern distributed applications. In this article, we propose the first type system that statically enforces the safety of cryptographic protocol implementations with respect to authorization policies expressed in affine logic. Our type system draws on a novel notion of ""exponential serialization"" of affine formulas, a general technique to protect affine formulas from the effect of duplication. This technique allows formulate of an expressive logical encoding of the authentication mechanisms underpinning distributed resource-aware authorization policies. We discuss the effectiveness of our approach on two case studies: the EPMO e-commerce protocol and the Kerberos authentication protocol. We finally devise a sound and complete type-checking algorithm, which is the key to achieving an efficient implementation of our analysis technique. © 2015 ACM.",Analysis of security protocols; Substructural logics; Type systems,Java programming language; Network security; Authentication mechanisms; Distributed applications; Distributed programming; Efficient implementation; Kerberos authentication; Security protocols; Substructural logic; Type systems; Authentication
An extension of ATL with strategy interaction,2015,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84933050655&doi=10.1145%2f2734117&partnerID=40&md5=5763275e37f37047eb9c1f7b14b61c45,"We propose an extension to ATL (alternating-time temporal logic), called BSIL (basic strategy-interaction logic), for specifying collaboration among agents in a multiagent system. We show that BSIL is strictly more expressive than ATL+ but incomparable with ATL∗, GL (game logic), and AMC (alternating μ-calculus) in expressiveness. We show that a memoryful strategy is necessary for fulfilling a specification in BSIL. We establish that the BSIL model-checking problem is PSPACE-complete. However, BSIL model checking can be performed in time quadratic in the model for fixed formulas. The BSIL (and hence ATL+) satisfiability is 2EXPTIME-complete. Finally, we report our experiment with a model checker for BSIL. © 2015 ACM.",Concurrent; Expressiveness; Games; Logic; Model checking; Multiagent; Realizability; Satisfiability; Strategy; Turn based,Calculations; Formal logic; Multi agent systems; Temporal logic; Concurrent; Expressiveness; Games; Logic; Multiagent; Realizability; Satisfiability; Strategy; Model checking
Automated classification of data races under both strong and weak memory models,2015,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930367149&doi=10.1145%2f2734118&partnerID=40&md5=ffae61a310a97745dfcc0d8ac1ba0728,"Data races are one of the main causes of concurrency problems in multithreaded programs.Whether all data races are bad, or some are harmful and others are harmless, is still the subject of vigorous scientific debate [Narayanasamy et al. 2007; Boehm 2012]. What is clear, however, is that today's code has many data races [Kasikci et al. 2012; Jin et al. 2012; Erickson et al. 2010], and fixing data races without introducing bugs is time consuming [Godefroid and Nagappan 2008]. Therefore, it is important to efficiently identify data races in code and understand their consequences to prioritize their resolution. We present Portend+, a tool that not only detects races but also automatically classifies them based on their potential consequences: Could they lead to crashes or hangs? Could their effects be visible outside the program? Do they appear to be harmless? How do their effects change under weak memory models? Our proposed technique achieves high accuracy by efficiently analyzing multiple paths and multiple thread schedules in combination, and by performing symbolic comparison between program outputs. We ran Portend+ on seven real-world applications: it detected 93 true data races and correctly classified 92 of them, with no human effort. Six of them were harmful races. Portend+'s classification accuracy is up to 89% higher than that of existing tools, and it produces easy-to-understand evidence of the consequences of ""harmful"" races, thus both proving their harmfulness and making debugging easier. We envision Portend+ being used for testing and debugging, as well as for automatically triaging bug reports. © 2015 ACM.",Concurrency; Data races; Symbolic execution; Triage,Codes (symbols); Program debugging; Well testing; Automated classification; Classification accuracy; Concurrency; Data races; Symbolic execution; Testing and debugging; Triage; Weak memory models; Racing automobiles
The design and implementation of a verification technique for GPU kernels,2015,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930362476&doi=10.1145%2f2743017&partnerID=40&md5=68bc54eea221c5bfc228871116cebaf9,"We present a technique for the formal verification of GPU kernels, addressing two classes of correctness properties: data races and barrier divergence. Our approach is founded on a novel formal operational semantics for GPU kernels termed synchronous, delayed visibility (SDV) semantics, which captures the execution of a GPU kernel by multiple groups of threads. The SDV semantics provides operational definitions for barrier divergence and for both inter-and intra-group data races. We build on the semantics to develop a method for reducing the task of verifying a massively parallel GPU kernel to that of verifying a sequential program. This completely avoids the need to reason about thread interleavings, and allows existing techniques for sequential program verification to be leveraged. We describe an efficient encoding of data race detection and propose a method for automatically inferring the loop invariants that are required for verification. We have implemented these techniques as a practical verification tool, GPUVerify, that can be applied directly to OpenCL and CUDA source code. We evaluate GPUVerify with respect to a set of 162 kernels drawn from public and commercial sources. Our evaluation demonstrates that GPUVerify is capable of efficient, automatic verification of a large number of real-world kernels. © 2015 ACM.",Barrier synchronization; Concurrency; Data races; GPUs; Verification,Graphics processing unit; Program processors; Semantics; Verification; Automatic verification; Barrier synchronization; Concurrency; Correctness properties; Data races; Design and implementations; GPUs; Verification techniques; Formal verification
MCALIB: Measuring sensitivity to rounding error with Monte Carlo programming,2015,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84928540012&doi=10.1145%2f2665073&partnerID=40&md5=460c1bf43b9e6d901b39f24645699b02,"Runtime analysis provides an effective method for measuring the sensitivity of programs to rounding errors. To date, implementations have required significant changes to source code, detracting from their widespread application. In this work, we present an open source system that automates the quantitative analysis of floating point rounding errors through the use of C-based source-to-source compilation and a Monte Carlo arithmetic library. We demonstrate its application to the comparison of algorithms, detection of catastrophic cancellation, and determination of whether single precision floating point provides sufficient accuracy for a given application. Methods for obtaining quantifiable measurements of sensitivity to rounding error are also detailed. ©2015 ACM.",Dynamic error analysis; Floating point arithmetic; Monte carlo arithmetic,Digital arithmetic; Errors; Monte Carlo methods; Open source software; Open systems; Dynamic error; Floating points; ITS applications; Monte Carlo arithmetic; Open source system; Run-time analysis; Single precision; Source-to-source compilation; C (programming language)
Verification of a cryptographic primitive: SHA-256 7,2015,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84928543472&doi=10.1145%2f2701415&partnerID=40&md5=b811acd668a695ceb93fd2e44b90f600,"This article presents a full formal machine-checked verification of a C program: theOpenSSL implementation of SHA-256. This is an interactive proof of functional correctness in the Coq proof assistant, using the Verifiable C program logic. Verifiable C is a separation logic for the C language, proved sound with respect to the operational semantics for C, connected to the CompCert verified optimizing C compiler. © 2015 ACM 0164-0925/2015/04-ART7 $15.00 Additional.",Cryptography,Computer circuits; Cryptography; Semantics; Theorem proving; C compilers; C language; Coq proof assistant; Cryptographic primitives; Functional correctness; Interactive proofs; Operational semantics; Separation logic; C (programming language)
Secure compilation to protected module architectures,2015,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84928542742&doi=10.1145%2f2699503&partnerID=40&md5=b8afa1cb99d3fad553b792594a4b4ae4,"A fully abstract compiler prevents security features of the source language from being bypassed by an attacker operating at the target language level. Unfortunately, developing fully abstract compilers is very complex, and it is even more so when the target language is an untyped assembly language. To provide a fully abstract compiler that targets untyped assembly, it has been suggested to extend the target language with a protected module architecture-an assembly-level isolation mechanism which can be found in nextgeneration processors. This article provides a fully abstract compilation scheme whose source language is an object-oriented, high-level language and whose target language is such an extended assembly language. The source language enjoys features such as dynamic memory allocation and exceptions. Secure compilation of first-order method references, cross-package inheritance, and inner classes is also presented. Moreover, this article contains the formal proof of full abstraction of the compilation scheme. Measurements of the overhead introduced by the compilation scheme indicate that it is negligible. ©2015 ACM.",Fully abstract compilation; Protected module architecture,Abstracting; Memory architecture; Numerical analysis; Program compilers; Assembly language; Dynamic memory allocation; First order method; Full abstraction; Fully abstract compilation; Module architecture; Security features; Target language; High level languages
Interval analysis and machine arithmetic: Why signedness ignorance is bliss,2015,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84921910303&doi=10.1145%2f2651360&partnerID=40&md5=b00994899e7d817603ecf01a3d908019,"The most commonly used integer types have fixed bit-width, making it possible for computations to wrap around, and many programs depend on this behaviour. Yet much work to date on program analysis and verification of integer computations treats integers as having infinite precision, and most analyses that do respect fixed width lose precision when overflow is possible. We present a novel integer interval abstract domain that correctly handles wrap-around. The analysis is signedness agnostic. By treating integers as strings of bits, only considering signedness for operations that treat them differently, we produce precise, correct results at a modest cost in execution time.",,Linguistics; Software engineering; Abstract domains; Bit-Width; Fixed width; Interval analysis; Program analysis; Integer programming
Foundations of typestate-oriented programming,2014,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84907007885&doi=10.1145%2f2629609&partnerID=40&md5=44b29ff7c8914bf22bf4f66bef3ecce1,"Typestate reflects how the legal operations on imperative objects can change at runtime as their internal state changes. A typestate checker can statically ensure, for instance, that an object method is only called when the object is in a state for which the operation is well defined. Prior work has shown how modular typestate checking can be achieved thanks to access permissions and state guarantees. However, typestate was not treated as a primitive language concept: typestate checkers are an additional verification layer on top of an existing language. In contrast, a typestate-oriented programming (TSOP) language directly supports expressing typestates. For example, in the Plaid programming language, the typestate of an object directly corresponds to its class, and that class can change dynamically. Plaid objects have not only typestate-dependent interfaces but also typestate-dependent behaviors and runtime representations. This article lays foundations for TSOP by formalizing a nominal object-oriented language with mutable state that integrates typestate change and typestate checking as primitive concepts. We first describe a statically typed language - Featherweight Typestate (FT) - where the types of object references are augmented with access permissions and state guarantees. We describe a novel flow-sensitive permission-based type system for FT. Because static typestate checking is still too rigid for some applications, we then extend this language into a gradually typed language - Gradual Featherweight Typestate (GFT). This language extends the notion of gradual typing to account for typestate: gradual typestate checking seamlessly combines static and dynamic checking by automatically inserting runtime checks into programs. The gradual type system of GFT allows programmers to write dynamically safe code even when the static type checker can only partly verify it. © 2014 ACM.",Access permissions; Gradual typing; Types; Typestates,Linguistics; Software engineering; Access permissions; Dynamic checking; Gradual typing; Internal state; Object reference; Run-time checks; Types; Typestates; Object oriented programming
A widening approach to multithreaded program verification,2014,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84910071853&doi=10.1145%2f2629608&partnerID=40&md5=3101375261f0fdf2bc7d6310a642483e,"Pthread-style multithreaded programs feature rich thread communication mechanisms, such as shared variables, signals, and broadcasts. In this article, we consider the automated verification of such programs where an unknown number of threads execute a given finite-data procedure in parallel. Such procedures are typically obtained as predicate abstractions of recursion-free source code written in C or Java. Many safety problems over finite-data replicated multithreaded programs are decidable via a reduction to the coverability problem in certain types of well-ordered infinite-state transition systems. On the other hand, in full generality, this problem is Ackermann-hard, which seems to rule out efficient algorithmic treatment. We present a novel, sound, and complete yet empirically efficient solution. Our approach is to judiciously widen the original set of coverability targets by configurations that involve fewer threads and are thus easier to decide, and whose exploration may well be sufficient: if they turn out uncoverable, so are the original targets. To soften the impact of ""bad guesses"" - configurations that turn out coverable - the exploration is accompanied by a parallel engine that generates coverable configurations; none of these is ever selected for widening. Its job being merely to prevent bad widening choices, such an engine need not be complete for coverability analysis, which enables a range of existing partial (e.g., nonterminating) techniques.We present extensive experiments on multithreaded C programs, including device driver code from FreeBSD, Solaris, and Linux distributions. Our approach outperforms existing coverability methods by orders of magnitude. 2014 Copyright is held by the owner/author(s). Publication rights licensed to ACM.",Coverability; Multithreaded program verification; Well-structured transition systems,Codes (symbols); Computer operating systems; Engines; Automated verification; Communication mechanisms; Coverability; Coverability problem; Linux distributions; Multi-threaded programs; Predicate abstractions; Well-structured transition systems; C (programming language)
Editorial,2014,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84911473682&doi=10.1145%2f2683389&partnerID=40&md5=c06c41a65273edcfb4a9bec82e61d1d2,[No abstract available],,
Practical fine-grained information flow control using laminar,2014,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84911493607&doi=10.1145%2f2638548&partnerID=40&md5=48993ff7ce29fd30de16b785f1541357,"Decentralized Information Flow Control (DIFC) is a promising model for writing programs with powerful, end-to-end security guarantees. Current DIFC systems that run on commodity hardware can be broadly categorized into two types: language-level and operating system-level DIFC. Language solutions provide no guarantees against security violations on system resources such as files and sockets. Operating system solutions mediate accesses to system resources but are either inefficient or imprecise at monitoring the flow of information through fine-grained program data structures. This article describes Laminar, the first system to implement DIFC using a unified set of abstractions for OS resources and heap-allocated objects. Programmers express security policies by labeling data with secrecy and integrity labels and access the labeled data in security methods. Laminar enforces the security policies specified by the labels at runtime. Laminar is implemented using amodified Java virtualmachine and a new Linux security module. This article shows that security methods ease incremental deployment and limit dynamic security checks by retrofitting DIFC policies on four application case studies. Replacing the applications' ad hoc security policies changes less than 10% of the code and incurs performance overheads from 5% to 56%. Compared to prior DIFC systems, Laminar supports a more general class of multithreaded DIFC programs efficiently and integrates language and OS abstractions. © 2014 ACM 0164-0925/2014/11-ART4 $15.00.",Information flow control; Java virtual machine; Operating systems; Security method,Abstracting; Computer operating systems; Flow control; Java programming language; Security systems; Decentralized information flow control; Heap-allocated objects; Incremental deployment; Information flow control; Java virtual machines; Linux security modules; Security methods; Security violations; Network security
"Kitsune: Efficient, general-purpose dynamic software updating for C",2014,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84910049840&doi=10.1145%2f2629460&partnerID=40&md5=51065514ed764eda29b3b4a69018bee8,"Dynamic software updating (DSU) systems facilitate software updates to running programs, thereby permitting developers to add features and fix bugs without downtime. This article introduces Kitsune, a DSU system for C. Kitsune's design has three notable features. First, Kitsune updates the whole program, rather than individual functions, using a mechanism that places no restrictions on data representations or allowed compiler optimizations. Second, Kitsune makes the important aspects of updating explicit in the program text, making the program's semantics easy to understand while minimizing programmer effort. Finally, the programmer can write simple specifications to direct Kitsune to generate code that traverses and transforms old-version state for use by new code; such state transformation is often necessary and is significantly more difficult in prior DSU systems. We have used Kitsune to update six popular, open-source, single- and multithreaded programs and find that few program changes are required to use Kitsune, that it incurs essentially no performance overhead, and that update times are fast. © 2014 ACM.",Dynamic software updating,Codes (symbols); Open source software; Program compilers; Semantics; Compiler optimizations; Data representations; Dynamic software updating; Multi-threaded programs; Open sources; Software updates; State transformation; Program debugging
A scheduling framework for spatial architectures across multiple constraint-solving theories,2014,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84911482504&doi=10.1145%2f2658993&partnerID=40&md5=447e893da6af93341f16efff527a7930,"Spatial architectures provide energy-efficient computation but require effective scheduling algorithms. Existing heuristic-based approaches offer low compiler/architect productivity, little optimality insight, and low architectural portability. We seek to develop a spatial-scheduling framework by utilizing constraint-solving theories and find that architecture primitives and scheduler responsibilities can be related through five abstractions: computation placement, data routing, event timing, resource utilization, and the optimization objective. We encode these responsibilities as 20 mathematical constraints, using SMT and ILP, and create schedulers for the TRIPS, DySER, and PLUG architectures. Our results show that a general declarative approach using constraint solving is implementable, is practical, and can outperform specialized schedulers. © 2014 ACM 0164-0925/2014/11-ART2 $15.00.",Integer linear programming; Satisfiability Modulo Theories; Spatial architecture scheduling; Spatial architectures,Codes (symbols); Computation theory; Computational efficiency; Energy efficiency; Integer programming; Logic programming; Optimization; Scheduling algorithms; Constraint Solving; Energy efficient; Integer Linear Programming; Multiple constraint; Resource utilizations; Satisfiability modulo Theories; Scheduling frameworks; Spatial scheduling; Scheduling
Abstract domains of affine relations,2014,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84910025942&doi=10.1145%2f2651361&partnerID=40&md5=3ebdbc88a1eddd581caac14e7e959ae4,"This article considers some known abstract domains for affine-relation analysis (ARA), along with several variants, and studies how they relate to each other. The various domains represent sets of points that satisfy affine relations over variables that hold machine integers and are based on an extension of linear algebra to modules over a ring (in particular, arithmetic performed modulo 2w, for some machine-integer width w). We show that the abstract domains of Müller-Olm/Seidl (MOS) and King/Søndergaard (KS) are, in general, incomparable. However, we give sound interconversion methods. In other words, we give an algorithm to convert a KS element vKS to an overapproximating MOS element vMOS - that is, γ (vKS) ⊆ γ(vMOS) - as well as an algorithm to convert an MOS element wMOS to an overapproximating KS element wKS - that is, γ(wMOS) ⊆ γ(wKS). The article provides insight on the range of options that one has for performing ARA in a program analyzer: - We describe how to perform a greedy, operator-by-operator abstraction method to obtain KS abstract transformers. - We also describe a more global approach to obtaining KS abstract transformers that considers the semantics of an entire instruction, basic block, or other loop-free program fragment. The latter method can yield best abstract transformers, and hence can be more precise than the former method. However, the latter method is more expensive. We also explain how to use the KS domain for interprocedural program analysis using a bit-precise concrete semantics, but without bit blasting. © 2014 ACM.",Abstract domain; Abstract interpretation; Affine relation; Howell form; Modular arithmetic; Static analysis; Symbolic abstraction,Linear algebra; Semantics; Static analysis; Abstract domains; Abstract interpretations; Affine relation; Modular arithmetic; Symbolic abstraction; Abstracting
Pattern-based verification for multithreaded programs,2014,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84907210681&doi=10.1145%2f2629644&partnerID=40&md5=689d1c18aba129b5b4cec8534048a7b1,"Pattern-based verification checks the correctness of program executions that follow a given pattern, a regular expression over the alphabet of program transitions of the form w∗1. . . w∗n. For multithreaded programs, the alphabet of the pattern is given by the reads and writes to the shared storage. We study the complexity of pattern-based verification for multithreaded programs with shared counters and finite variables. While unrestricted verification is undecidable for abstracted multithreaded programs with recursive procedures and PSPACE-complete for abstracted multithreaded while-programs (even without counters), we show that pattern-based verification is NP-complete for both classes, even in the presence of counters.We then conduct a multiparameter analysis to study the complexity of the problem on its three natural parameters (number of threads+counters+variables, maximal size of a thread, size of the pattern) and on two parameters related to thread structure (maximal number of procedures per thread and longest simple path of procedure calls). We present an algorithm that for a fixed number of threads, counters, variables, and pattern size solves the verification problem in stO(lsp+[log(pr+1)]) time, where st is the maximal size of a thread, pr is the maximal number of procedures per thread, and lsp is the longest simple path of procedure calls. © 2014 ACM.",Algorithms; Languages; Reliability; Verification,Algorithms; Linguistics; Query languages; Reliability; Software engineering; Verification; Multi-threaded programs; Multiparameter analysis; Number of threads; Program execution; Recursive procedure; Regular expressions; Shared counters; Verification problems; Abstracting
Lazy scheduling: A runtime adaptive scheduler for declarative parallelism,2014,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84907197447&doi=10.1145%2f2629643&partnerID=40&md5=b26702b150b4d0946f92fd03e89bd0f0,"Lazy scheduling is a runtime scheduler for task-parallel codes that effectively coarsens parallelism on load conditions in order to significantly reduce its overheads compared to existing approaches, thus enabling the efficient execution of more fine-grained tasks. Unlike other adaptive dynamic schedulers, lazy scheduling does not maintain any additional state to infer system load and does not make irrevocable serialization decisions. These two features allow it to scale well and to provide excellent load balancing in practice but at a much lower overhead cost compared to work stealing, the golden standard of dynamic schedulers. We evaluate three variants of lazy scheduling on a set of benchmarks on three different platforms and find it to substantially outperform popular work stealing implementations on fine-grained codes. Furthermore, we show that the vast performance gap between manually coarsened and fully parallel code is greatly reduced by lazy scheduling, and that, with minimal static coarsening, lazy scheduling delivers performance very close to that of fully tuned code. The tedious manual coarsening required by the best existing work stealing schedulers and its damaging effect on performance portability have kept novice and general-purpose programmers from parallelizing their codes. Lazy scheduling offers the foundation for a declarative parallel programming methodology that should attract those programmers by minimizing the need for manual coarsening and by greatly enhancing the performance portability of parallel code. © 2014 ACM.",Algorithms; Languages; Performance,Algorithms; Coarsening; Ostwald ripening; Parallel programming; Query languages; Adaptive dynamics; Damaging effects; Dynamic schedulers; General-purpose programmers; Performance; Performance gaps; Performance portability; Programming methodology; Scheduling
Reactive imperative programming with dataflow constraints,2014,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84911493278&doi=10.1145%2f2623200&partnerID=40&md5=94b56c5d96df57c2390a34de9fe7b624,"Dataflow languages provide natural support for specifying constraints between objects in dynamic applications, where programs need to react efficiently to changes in their environment. In this article, we show that one-way dataflow constraints, largely explored in the context of interactive applications, can be seamlessly integrated in any imperative language and can be used as a general paradigm for writing performance-critical reactive applications that require efficient incremental computations. In our framework, programmers can define ordinary statements of the imperative host language that enforce constraints between objects stored in special memory locations designated as ""reactive."" Reactive objects can be of any legal type in the host language, including primitive data types, pointers, arrays, and structures. Statements defining constraints are automatically re-executed every time their input memory locations change, letting a program behave like a spreadsheet where the values of some variables depend on the values of other variables. The constraintsolving mechanism is handled transparently by altering the semantics of elementary operations of the host language for reading and modifying objects. We provide a formal semantics and describe a concrete embodiment of our technique into C/C++, showing how to implement it efficiently in conventional platforms using off-the-shelf compilers.We discuss common coding idioms and relevant applications to reactive scenarios, including incremental computation, observer design pattern, data structure repair, and software visualization. The performance of our implementation is compared to problem-specific change propagation algorithms, as well as to language-centric approaches such as self-adjusting computation and subject/observer communication mechanisms, showing that the proposed approach is efficient in practice. © 2014 ACM 0164-0925/2014/11-ART2 $15.00.",Constraint solving; Data structure repair; Dataflow programming; Imperative programming; Incremental computation; Observer design pattern; One-way dataflow constraints; Reactive programming; Software visualization,Application programs; Computational efficiency; Data flow analysis; Data structures; Data visualization; Formal methods; Repair; Semantics; Visualization; Constraint Solving; Data structure repairs; Dataflow; Dataflow programming; Imperative programming; Incremental computation; Observer design patterns; Reactive programming; Software visualization; C (programming language)
Global sparse analysis framework,2014,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84908137944&doi=10.1145%2f2590811&partnerID=40&md5=65197bc90f7e3404923eb6807f40b12c,"In this article, we present a general method for achieving global static analyzers that are precise and sound, yet also scalable. Our method, on top of the abstract interpretation framework, is a general sparse analysis technique that supports relational as well as nonrelational semantics properties for various programming languages. Analysis designers first use the abstract interpretation framework to have a global and correct static analyzer whose scalability is unattended. Upon this underlying sound static analyzer, analysis designers add our generalized sparse analysis techniques to improve its scalability while preserving the precision of the underlying analysis. Our method prescribes what to prove to guarantee that the resulting sparse version should preserve the precision of the underlying analyzer. We formally present our framework and show that existing sparse analyses are all restricted instances of our framework. In addition, we show more semantically elaborate design examples of sparse nonrelational and relational static analyses. We then present their implementation results that scale to globally analyze up to one million lines of C programs. We also show a set of implementation techniques that turn out to be critical to economically support the sparse analysis process. © 2014 ACM.",,Abstracting; Model checking; Scalability; Semantics; Static analysis; Abstract interpretations; C programs; General method; Implementation techniques; Sparse analysis; Static analyzers; C (programming language)
"Herding cats: Modelling, simulation, testing, and data mining for weak memory",2014,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84906846720&doi=10.1145%2f2627752&partnerID=40&md5=cbdff2412a3f80c5a3a7125368c1c767,"We propose an axiomatic generic framework for modelling weak memory. We show how to instantiate this framework for Sequential Consistency (SC), Total Store Order (TSO), C++ restricted to release-acquire atomics, and Power. For Power, we compare our model to a preceding operational model in which we found a flaw. To do so, we define an operational model that we show equivalent to our axiomatic model. We also propose a model for ARM. Our testing on this architecture revealed a behaviour later acknowledged as a bug by ARM, and more recently, 31 additional anomalies. We offer a new simulation tool, called herd, which allows the user to specify the model of his choice in a concise way. Given a specification of a model, the tool becomes a simulator for that model. The tool relies on an axiomatic description; this choice allows us to outperform all previous simulation tools. Additionally, we confirm that verification time is vastly improved, in the case of bounded model checking. Finally, we put our models in perspective, in the light of empirical data obtained by analysing the C and C++ code of a Debian Linux distribution. We present our new analysis tool, called mole, which explores a piece of code to find the weak memory idioms that it uses. © 2014 ACM.",Concurrency; Software verification; Weak memory models,ARM processors; Computer operating systems; Model checking; Bounded model checking; Concurrency; Generic frameworks; Linux distributions; Sequential consistency; Software verification; Total store order; Weak memory models; C++ (programming language)
Atomicity refinement for verified compilation,2014,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84906859506&doi=10.1145%2f2601339&partnerID=40&md5=b5f58d154c642f5a7db256489f564556,"We consider the verified compilation of high-level managed languages like Java or C# whose intermediate representations provide support for shared-memory synchronization and automatic memory management. Our development is framed in the context of the Total Store Order relaxedmemorymodel. Ensuring complier correctness is challenging because high-level actions are translated into sequences of nonatomic actions with compiler-injected snippets of racy code; the behavior of this code depends not only on the actions of other threads but also on out-of-order executions performed by the processor. A näýve proof of correctness would require reasoning over all possible thread interleavings. In this article, we propose a refinement-based proof methodology that precisely relates concurrent code expressed at different abstraction levels, cognizant throughout of the relaxed memory semantics of the underlying processor. Our technique allows the compiler writer to reason compositionally about the atomicity of low-level concurrent code used to implementmanaged services. We illustrate our approach with examples taken from the verification of a concurrent garbage collector. © 2014 ACM.",Atomicity; Compiler transformations and optimizations; Concurrency; Garbage collection; Managed languages; Mechanized proof assistant (coq); Refinement; Verified compilation,Codes (symbols); High level languages; Java programming language; Semantics; Atomicity; Compiler transformations; Concurrency; Garbage collection; Mechanized proofs; Refinement; Verified compilation; Program compilers
Specialization slicing,2014,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84906891388&doi=10.1145%2f2566620&partnerID=40&md5=8179539310faa92831fa0d679ed0f9d5,"This paper defines a new variant of program slicing, called specialization slicing, and presents an algorithm for the specialization-slicing problem that creates an optimal output slice. An algorithm for specialization slicing is polyvariant: for a given procedure p, the algorithm may create multiple specialized copies of p. In creating specialized procedures, the algorithm must decide for which patterns of formal parameters a given procedure should be specialized and which program elements should be included in each specialized procedure. We formalize the specialization-slicing problem as a partitioning problem on the elements of the possibly infinite unrolled program. To manipulate possibly infinite sets of program elements, the algorithm makes use of automata-theoretic techniques originally developed in the model-checking community. The algorithm returns a finite answer that is optimal (with respect to a criterion defined in the article). In particular, (i) each element replicated by the specialization-slicing algorithm provides information about specialized patterns of program behavior that are intrinsic to the program, and (ii) the answer is of minimal size (i.e., among all possible answers with property (i), there is no smaller one). The specialization-slicing algorithm provides a newway to create executable slices.Moreover, by combining specialization slicing with forward slicing, we obtain a method for removing unwanted features from a program. While it was previously known how to solve the feature-removal problem for single-procedure programs, it was not known how to solve it for programs with procedure calls. © 2014 ACM.",Executable slice; Feature removal; Program dependence graph; Program slicing; Program specialization; Pushdown system; Reverse-deterministic automaton,Automata theory; Model checking; Executable slice; Program dependence graph; Program slicing; Program specialization; Pushdown systems; Reverse-deterministic automaton; Algorithms
Formal verification of an ssa-based middle-end for compcert,2014,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896965526&doi=10.1145%2f2579080&partnerID=40&md5=b0b8fb824a1f5b97345df57268604141,"CompCert is a formally verified compiler that generates compact and efficient code for a large subset of the C language. However, CompCert foregoes using SSA, an intermediate representation employed by many compilers that enables writing simpler, faster optimizers. In fact, it has remained an open problem to verify formally an SSA-based compiler. We report on a formally verified, SSA-based middle-end for CompCert. In addition to providing a formally verified SSA-based middle-end, we address two problems raised by Leroy in [2009]: giving an intuitive formal semantics to SSA, and leveraging its global properties to reason locally about program optimizations. © 2014 ACM.",Compiler verification; Mechanized proof; Single static assignment,C (programming language); Compiler verifications; Formal Semantics; Formal verifications; Global properties; Intermediate representations; Mechanized proofs; Program optimization; Static assignment; Program compilers
Æminium: A permission-based concurrent-by-default programming language approach,2014,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896965101&doi=10.1145%2f2543920&partnerID=40&md5=0dfe5ef3363aaf28d6c223d3f6cd0ac5,"Writing concurrent applications is extremely challenging, not only in terms of producing bug-free and maintainable software, but also for enabling developer productivity. In this article we present the ÆMINIUM concurrent-by-default programming language. Using ÆMINIUM programmers express data dependencies rather than control flow between instructions. Dependencies are expressed using permissions, which are used by the type system to automatically parallelize the application. The ÆMINIUM approach provides a modular and composable mechanism for writing concurrent applications, preventing data races in a provable way. This allows programmers to shift their attention from low-level, error-prone reasoning about thread interleaving and synchronization to focus on the core functionality of their applications. We study the semantics of ÆMINIUM through μÆMINIUM, a sound core calculus that leverages permission flow to enable concurrent-by-default execution. After discussing our prototype implementation we present several case studies of our system. Our case studies show up to 6.5X speedup on an eight-core machine when leveraging data group permissions to manage access to shared state, and more than 70% higher throughput in a Web server application. © 2014 ACM.",Access permissions; Concurrency; Data groups; Permissions,Application programs; Computer programming languages; Semantics; Access permissions; Concurrency; Core functionality; Data dependencies; Data groups; Permissions; Prototype implementations; Web server applications; Concurrency control
Rely-guarantee-based simulation for compositional verification of concurrent program transformations,2014,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84897010341&doi=10.1145%2f2576235&partnerID=40&md5=d723ba29730491370c6f61bd74a8fe95,"Verifying program transformations usually requires proving that the resulting program (the target) refines or is equivalent to the original one (the source). However, the refinement relation between individual sequential threads cannot be preserved in general with the presence of parallel compositions, due to instruction reordering and the different granularities of atomic operations at the source and the target. On the other hand, the refinement relation defined based on fully abstract semantics of concurrent programs assumes arbitrary parallel environments, which is too strong and cannot be satisfied by many well-known transformations. In this article, we propose a Rely-Guarantee-based Simulation (RGSim) to verify concurrent program transformations. The relation is parametrized with constraints of the environments that the source and the target programs may compose with. It considers the interference between threads and their environments, thus is less permissive than relations over sequential programs. It is compositional with respect to parallel compositions as long as the constraints are satisfied. Also, RGSim does not require semantics preservation under all environments, and can incorporate the assumptions about environments made by specific program transformations in the form of rely/guarantee conditions. We use RGSim to reason about optimizations and prove atomicity of concurrent objects. We also propose a general garbage collector verification framework based on RGSim, and verify the Boehm et al. concurrent mark-sweep GC. © 2014 ACM.",Concurrency; Program transformation; Rely-guarantee reasoning; Simulation,Semantics; Compositional verification; Concurrency; Different granularities; Parallel environment; Program transformations; Rely guarantees; Simulation; Verification framework; Abstracting
Extending type inference to variational programs,2014,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896951625&doi=10.1145%2f2518190&partnerID=40&md5=265ae07fb3545239e74d7fb89f485455,"Through the use of conditional compilation and related tools, many software projects can be used to generate a huge number of related programs. The problem of typing such variational software is difficult. The brute-force strategy of generating all variants and typing each one individually is: (1) usually infeasible for efficiency reasons and (2) produces results that do not map well to the underlying variational program. Recent research has focused mainly on efficiency and addressed only the problem of type checking. In this work we tackle the more general problem of variational type inference and introduce variational types to represent the result of typing a variational program. We introduce the variational lambda calculus (VLC) as a formal foundation for research on typing variational programs. We define a type system for VLC in which VLC expressions are mapped to correspondingly variational types. We show that the type system is correct by proving that the typing of expressions is preserved over the process of variation elimination, which eventually results in a plain lambda calculus expression and its corresponding type. We identify a set of equivalence rules for variational types and prove that the type unification problem modulo these equivalence rules is unitary and decidable; we also present a sound and complete unification algorithm. Based on the unification algorithm, the variational type inference algorithm is an extension of algorithm W . We show that it is sound and complete and computes principal types. We also consider the extension of VLC with sum types, a necessary feature for supporting variational data types, and demonstrate that the previous theoretical results also hold under this extension. Finally, we characterize the complexity of variational type inference and demonstrate the efficiency gains over the brute-force strategy. © 2014 ACM.",Variational lambda calculus; Variational type inference; Variational types,Algorithms; Computational mechanics; Differentiation (calculus); Efficiency; Conditional compilations; Sound and complete; Unification algorithms; Unification problem; Variational lambda calculus; Variational softwares; Variational type inferences; Variational types; Inference engines
Making the java memory model safe,2013,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891757156&doi=10.1145%2f2518191&partnerID=40&md5=e3ec79878b8d5f353b38356b85f545d9,"This work presents a machine-checked formalisation of the Java memory model and connects it to an operational semantics for Java and Java bytecode. For the whole model, I prove the data race freedom guarantee and type safety. The model extends previous formalisations by dynamicmemory allocation, thread spawns and joins, infinite executions, the wait-notify mechanism, and thread interruption, all of which interact in subtle ways with the memory model. The formalisation resulted in numerous clarifications of and fixes to the existing JMM specification. © 2013 ACM.",Data race freedom; Java memory model; Operational semantics; Type safety,Linguistics; Software engineering; Data races; Dynamic memory; Formalisation; Java byte codes; Java Memory model; Memory modeling; Operational semantics; Type safety; Java programming language
Divergence analysis,2013,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891820254&doi=10.1145%2f2523815&partnerID=40&md5=59fb7e6aea3c908a877643cf31dff3db,"Growing interest in graphics processing units has brought renewed attention to the Single Instruction Multiple Data (SIMD) execution model. SIMD machines give application developers tremendous computational power; however, programming them is still challenging. In particular, developers must deal with memory and control-flow divergences. These phenomena stem from a condition that we call data divergence, which occurs whenever two processing elements (PEs) see the same variable name holding different values. This article introduces divergence analysis, a static analysis that discovers data divergences. This analysis, currently deployed in an industrial quality compiler, is useful in several ways: it improves the translation of SIMD code to non-SIMD CPUs, it helps developers to manually improve their SIMD applications, and it also guides the automatic optimization of SIMD programs. We demonstrate this last point by introducing the notion of a divergence-aware register spiller. This spiller uses information from our analysis to either rematerialize or share common data between PEs. As a testimony of its effectiveness, we have tested it on a suite of 395 CUDA kernels from well-known benchmarks. The divergence-aware spiller produces GPU code that is 26.21% faster than the code produced by the register allocator used in the baseline compiler. © 2013 ACM.",Divergence analysis; Graphics processing units; High performance; SIMD; Static program analysis,Application programs; Computer graphics; Program compilers; Program translators; Static analysis; Divergence analysis; Graphics Processing Unit; High performance; SIMD; Static program analysis; Quality control
Contracts for first-class classes,2013,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887852852&doi=10.1145%2f2518189&partnerID=40&md5=12c11abd1d159f3ec03d39c7fb0192c4,"First-class classes enable programmers to abstract over patterns in the class hierarchy and to experiment with new forms of object-oriented programming such as mixins and traits. This increase in expressive power calls for tools to control the complexity of the software architecture. A contract system is one possible tool that has seen much use in object-oriented programming languages, but existing contract systems cannot cope with first-class classes. On the one hand, the typical contract language deals only with plain values such as numbers, while classes are higher-order values. On the other hand, contract specifications are usually contained within class definitions, while classes as values call for a separate contract language. This article presents the design and implementation of a contract system for first-class classes as well as a two-pronged evaluation. The first one states and proves a blame correctness theorem for a model of our language. The theorem shows that when the contract system assigns blame to a component for a contract violation, the component is indeed responsible for providing the nonconforming value. The second part, consisting of benchmarks and case studies, demonstrates the need for the rich contract language and validates that our implementation approach is performant with respect to time. © 2013 ACM.",Contracts; First-class class systems,Abstracting; Contracts; Object oriented programming; Class hierarchies; Contract languages; Contract specifications; Contract violation; Correctness theorem; Design and implementations; Implementation approach; Object-oriented programming languages; Tools
Analysis of recursively parallel programs,2013,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887834097&doi=10.1145%2f2518188&partnerID=40&md5=78f88b968d6abfa3e4fac1f9b88c0c20,"We propose a general formal model of isolated hierarchical parallel computations, and identify several fragments to match the concurrency constructs present in real-world programming languages such as Cilk and X10. By associating fundamental formal models (vector addition systems with recursive transitions) to each fragment, we provide a common platform for exposing the relative difficulties of algorithmic reasoning. For each case we measure the complexity of deciding state reachability for finite-data recursive programs, and propose algorithms for the decidable cases. The complexities which include PTIME, NP, EXPSPACE, and 2EXPTIME contrast with undecidable state reachability for recursive multithreaded programs. © 2013 ACM.",Concurrency; Parallelism; Verification,Algorithms; Formal methods; Multitasking; Parallel architectures; Vectors; Verification; Algorithmic reasoning; Concurrency; Multi-threaded programs; Parallel Computation; Parallelism; Recursive programs; State reachability; Vector addition systems; Petri nets
Probabilistic relational reasoning for differential privacy,2013,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887832703&doi=10.1145%2f2492061&partnerID=40&md5=7ce2595038f7ce61f80cf40623f71db6,"Differential privacy is a notion of confidentiality that allows useful computations on sensible data while protecting the privacy of individuals. Proving differential privacy is a difficult and error-prone task that calls for principled approaches and tool support. Approaches based on linear types and static analysis have recently emerged; however, an increasing number of programs achieve privacy using techniques that fall out of their scope. Examples include programs that aim for weaker, approximate differential privacy guarantees and programs that achieve differential privacy without using any standard mechanisms. Providing support for reasoning about the privacy of such programs has been an open problem. We report on CertiPriv, a machine-checked framework for reasoning about differential privacy built on top of the Coq proof assistant. The central component of CertiPriv is a quantitative extension of probabilistic relational Hoare logic that enables one to derive differential privacy guarantees for programs from first principles. We demonstrate the applicability of CertiPriv on a number of examples whose formal analysis is out of the reach of previous techniques. In particular, we provide the first machine-checked proofs of correctness of the Laplacian, Gaussian, and exponential mechanisms and of the privacy of randomized and streaming algorithms from the literature. © 2013 ACM.",Coq proof assistant; Differential privacy; Relational Hoare logic,Theorem proving; Central component; Coq proof assistant; Differential privacies; First principles; Machine-checked proofs; Relational hoare logic; Relational reasoning; Streaming algorithm; Static analysis
A transformation framework for optimizing task-parallel programs,2013,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84877904344&doi=10.1145%2f2450136.2450138&partnerID=40&md5=4dda3e5447721f5eb5b046f9ec211bef,"Task parallelism has increasingly become a trend with programming models such as OpenMP 3.0, Cilk, Java Concurrency, X10, Chapel and Habanero-Java (HJ) to address the requirements of multicore programmers. While task parallelism increases productivity by allowing the programmer to express multiple levels of parallelism, it can also lead to performance degradation due to increased overheads. In this article, we introduce a transformation framework for optimizing task-parallel programs with a focus on task creation and task termination operations. These operations can appear explicitly in constructs such as async, finish in X10 and HJ, task, taskwait in OpenMP 3.0, and spawn, sync in Cilk, or implicitly in composite code statements such as foreach and ateach loops in X10, forall and foreach loops in HJ, and parallel loop in OpenMP. Our framework includes a definition of data dependence in task-parallel programs, a happens-before analysis algorithm, and a range of program transformations for optimizing task parallelism. Broadly, our transformations cover three different but interrelated optimizations: (1) finish-elimination, (2) forall-coarsening, and (3) loop-chunking. Finish-elimination removes redundant task termination operations, forall-coarsening replaces expensive task creation and termination operations with more efficient synchronization operations, and loop-chunking extracts useful parallelism from ideal parallelism. All three optimizations are specified in an iterative transformation framework that applies a sequence of relevant transformations until a fixed point is reached. Further, we discuss the impact of exception semantics on the specified transformations, and extend them to handle task-parallel programs with precise exception semantics. Experimental results were obtained for a collection of task-parallel benchmarks on three multicore platforms: a dual-socket 128-thread (16-core) Niagara T2 system, a quad-socket 16-core Intel Xeon SMP, and a quad-socket 32-core Power7 SMP. We have observed that the proposed optimizations interact with each other in a synergistic way, and result in an overall geometric average performance improvement between 6.28× and 10.30×, measured across all three platforms for the benchmarks studied. © 2013 ACM.",Algorithms; Experimentation; Performance,Algorithms; Application programming interfaces (API); Benchmarking; Iterative methods; Java programming language; Multicore programming; Parallel architectures; Semantics; Experimentation; Iterative transformation; Multiple levels of parallelisms; Performance; Performance degradation; Performance improvements; Program transformations; Synchronization operation; Optimization
Editorial,2013,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84877916606&doi=10.1145%2f2450136.2450141&partnerID=40&md5=322f89eefb78fc5db4667d9c792629cb,[No abstract available],,
Mixin' up the ML module system,2013,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84877885177&doi=10.1145%2f2450136.2450137&partnerID=40&md5=1b5f6291cecb2a9b56b39e2391ef3d6b,"ML modules provide hierarchical namespace management, as well as fine-grained control over the propagation of type information, but they do not allow modules to be broken up into mutually recursive, separately compilable components. Mixin modules facilitate recursive linking of separately compiled components, but they are not hierarchically composable and typically do not support type abstraction. We synthesize the complementary advantages of these two mechanisms in a novel module system design we call MixML. A MixML module is like an ML structure in which some of the components are specified but not defined. In other words, it unifies the ML structure and signature languages into one. MixML seamlessly integrates hierarchical composition, translucent ML-style data abstraction, and mixin-style recursive linking. Moreover, the design of MixML is clean and minimalist; it emphasizes how all the salient, semantically interesting features of the ML module system (and several proposed extensions to it) can be understood simply as stylized uses of a small set of orthogonal underlying constructs, with mixin composition playing a central role. We provide a declarative type system for MixML, including two important extensions: higher-order modules, and modules as first-class values. We also present a sound and complete, three-pass type-checking algorithm for this system. The operational semantics of MixML is defined by an elaboration translation into an internal core language called LTG - namely, a polymorphic lambda calculus with single-assignment references and recursive type generativity - which employs a linear type and kind system to track definedness of term and type imports. © 2013 ACM.",Abstract data types; Hierarchical composability; Mixin modules; ML modules; Recursive modules; Type systems,Abstract data types; Differentiation (calculus); Java programming language; Composability; Mixin modules; ML modules; Recursive modules; Type systems; Hierarchical systems
TSL: A system for generating abstract interpreters and its application to machine-code analysis,2013,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84877897897&doi=10.1145%2f2450136.2450139&partnerID=40&md5=07b07bf6b90ba27086b1da780d413f0a,"This article describes the design and implementation of a system, called TSL (for Transformer Specification Language), that provides a systematic solution to the problem of creating retargetable tools for analyzing machine code. TSL is a tool generator - that is, a metatool - that automatically creates different abstract interpreters for machine-code instruction sets. The most challenging technical issue that we faced in designing TSL was how to automate the generation of the set of abstract transformers for a given abstract interpretation of a given instruction set. From a description of the concrete operational semantics of an instruction set, together with the datatypes and operations that define an abstract domain, TSL automatically creates the set of abstract transformers for the instructions of the instruction set. TSL advances the state-of-the-art in program analysis because it provides two dimensions of parameterizability: (i) a given analysis component can be retargeted to different instruction sets; (ii) multiple analysis components can be created automatically from a single specification of the concrete operational semantics of the language to be analyzed. TSL is an abstract-transformer-generator generator. The article describes the principles behind TSL, and discusses how one uses TSL to develop different abstract interpreters. © 2013 ACM.",Algorithms; Languages; Security; Theory; Verification,Algorithms; Analog computers; Computer programming languages; Concretes; Query languages; Specification languages; Verification; Abstract interpretations; Design and implementations; ITS applications; Operational semantics; Parameterizability; Program analysis; Security; Theory; Abstracting
"Practical integrated analysis of pointers, dataflow and control flow",2013,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84877907032&doi=10.1145%2f2450136.2450140&partnerID=40&md5=2dfb446a3772b69ed68dd1a6810bde1b,"This article presents a family of static analyses to determine pointer targets, control flow, and dataflow in combination. The integrated solution to these mutually dependent problems approaches the result from the optimistic side. It is a general strategy for static program analysis and does not need any upfront approximation for one of the problems to overcome the mutual dependencies. A degenerated case yields Andersen's famous pointer analysis; otherwise, the analyses are flow-sensitive and can support direct and indirect strong updates, within the same cubic asymptotic complexity as known for Andersen, albeit with larger constants. Surprisingly, the ideas behind the integrated analysis are intuitive. The strategy we describe naturally evolves from considering the mutual dependencies between the three problems, or from generalizing Andersen's analysis to flow sensitivity. Such a flow-sensitive Andersen analysis not only computes pointer targets with higher precision than the original analysis, but it also creates an interprocedural SSA form at the same time. Our extensive experimental evaluation shows that the integrated solution is practical as it can be applied to reasonably large real-world programs within a few seconds or minutes. This uses some optimizations which together achieve a speedup of more than 100 for several programs. We compare several members of the family of analyses, from flow- and field-insensitive to flow- and field-sensitive with strong updates, both with and without optimizations. This gives some insights into the effects of these dimensions of precision on the results. It also sheds new light on the benefits of flow sensitivity versus the costs associated with it. © 2013 ACM.",Andersen analysis; Flow sensitivity; Pointer analysis; SSA,Optimization; Static analysis; Andersen analysis; Asymptotic complexity; Experimental evaluation; Flow sensitivity; Integrated solutions; Pointer analysis; SSA; Static program analysis; Data flow analysis
Efficient identification of linchpin vertices in dependence clusters,2013,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896938948&doi=10.1145%2f2491522.2491524&partnerID=40&md5=f1c11c4ad3111e860d2f0ae3bbfab20f,"Several authors have found evidence of large dependence clusters in the source code of a diverse range of systems, domains, and programming languages. This raises the question of how we might efficiently locate the fragments of code that give rise to large dependence clusters.We introduce an algorithm for the identification of linchpin vertices, which hold together large dependence clusters, and prove correctness properties for the algorithm's primary innovations.We also report the results of an empirical study concerning the reduction in analysis time that our algorithm yields over its predecessor using a collection of 38 programs containing almost half a million lines of code. Our empirical findings indicate improvements of almost two orders of magnitude, making it possible to process larger programs for which it would have previously been impractical. ©2013 ACM 0164-0925/2013/07-ART7 $15.00.",Empirical study; Internal representation; Performance enhancement; Slicing,Linguistics; Software engineering; Correctness properties; Empirical findings; Empirical studies; Internal representation; Lines of code; Orders of magnitude; Performance enhancements; Slicing; Clustering algorithms
Dependent type theory for verification of information flow and access control policies,2013,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84894063204&doi=10.1145%2f2491522.2491523&partnerID=40&md5=4e4790861b33c5e1c52a684cabbd47d8,"We present Relational Hoare Type Theory (RHTT), a novel language and verification system capable of expressing and verifying rich information flow and access control policies via dependent types. We show that a number of security policies which have been formalized separately in the literature can all be expressed in RHTT using only standard type-theoretic constructions such as monads, higher-order functions, abstract types, abstract predicates, and modules. Example security policies include conditional declassification, information erasure, and state-dependent information flow and access control. RHTT can reason about such policies in the presence of dynamic memory allocation, deallocation, pointer aliasing and arithmetic.©2013 ACM 0164-0925/2013/07-ART7 $15.00.",Access control; Information flow; Type theory,Formal languages; Security systems; Access control policies; Dependent type theory; Dynamic memory allocation; Higher-order functions; Information erasure; Information flows; Type theory; Verification systems; Access control
Proof-directed parallelization synthesis by separation logic,2013,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893120972&doi=10.1145%2f2491522.2491525&partnerID=40&md5=459d74f05049246b7ee608157556426f,"We present an analysis which takes as its input a sequential program, augmented with annotations indicating potential parallelization opportunities, and a sequential proof, written in separation logic, and produces a correctly synchronized parallelized program and proof of that program. Unlike previous work, ours is not a simple independence analysis that admits parallelization only when threads do not interfere; rather, we insert synchronization to preserve dependencies in the sequential program that might be violated by a naivetranslation. Separation logic allows us to parallelize fine-grained patterns of resource usage, moving beyond straightforward points-to analysis. The sequential proof need only represent shape properties, meaning we can handle complex algorithms without verifying every aspect of their behavior.©2013 ACM 0164-0925/2013/07-ART8 $15.00.",Abduction; Deterministic Parallelism; Frame inference; Separation logic,Formal logic; Separation; Abduction; Complex algorithms; Deterministic parallelisms; Frame inference; Parallelizations; Points-to analysis; Separation logic; Sequential programs; Computer circuits
Reachability analysis of program variables,2013,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891750312&doi=10.1145%2f2529990&partnerID=40&md5=619c9ea73f5d26cf756df9a0eacc1b72,"Reachability from a program variable v to a program variable w states that from v, it is possible to follow a path of memory locations that leads to the object bound to w. We present a new abstract domain for the static analysis of possible reachability between program variables or, equivalently, definite unreachability between them. This information is important for improving the precision of other static analyses, such as side-effects, field initialization, cyclicity and path-length analysis, as well as more complex analyses built upon them, such as nullness and termination analysis.We define and prove correct our reachability analysis for Java bytecode, defined as a constraint-based analysis, where the constraint is a graph whose nodes are the program points and whose arcs propagate reachability information in accordance to the abstract semantics of each bytecode instruction. For each program point p, our reachability analysis produces an overapproximation of the ordered pairs of variables {v,w} such that v might reach w at p. Seen the other way around, if a pair {v,w} is not present in the overapproximation at p, then v definitely does not reach w at p. We have implemented the analysis inside the Julia static analyzer. Our experiments of analysis of nontrivial Java and Android programs show the improvement of precision due to the presence of reachability information. Moreover, reachability analysis actually reduces the overall cost of nullness and termination analysis. © 2013 ACM.",Abstract interpretation; Constraint-based analysis; Java bytecode; Pointer analysis; Reachability analysis; Static analysis,Computer software; Java programming language; Semantics; Abstract interpretations; Constraint-based analysis; Java byte codes; Pointer analysis; Reachability analysis; Static analysis
Bisimulation for quantum processes,2012,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872413764&doi=10.1145%2f2400676.2400680&partnerID=40&md5=93324f49f61ce0a7cbe6eae8d36c1c0f,"Quantum cryptographic systems have been commercially available, with a striking advantage over classical systems that their security and ability to detect the presence of eavesdropping are provable based on the principles of quantum mechanics. On the other hand, quantum protocol designers may commit more faults than classical protocol designers since human intuition is poorly adapted to the quantum world. To offer formal techniques formodeling and verification of quantum protocols, several quantum extensions of process algebra have been proposed. An important issue in quantum process algebra is to discover a quantum generalization of bisimulation preserved by various process constructs, in particular, parallel composition, where one of the major differences between classical and quantum systems, namely quantum entanglement, is present. Quite a few versions of bisimulation have been defined for quantum processes in the literature, but in the best case they are only proved to be preserved by parallel composition of purely quantum processes where no classical communication is involved. Many quantum cryptographic protocols, however, employ the LOCC (Local Operations and Classical Communication) scheme, where classical communication must be explicitly specified. So, a notion of bisimulation preserved by parallel composition in the circumstance of both classical and quantum communication is crucial for process algebra approach to verification of quantum cryptographic protocols. In this article we introduce novel notions of strong bisimulation and weak bisimulation for quantum processes, and prove that they are congruent with respect to various process algebra combinators including parallel composition even when both classical and quantum communication are present. We also establish some basic algebraic laws for these bisimulations. In particular, we show the uniqueness of the solutions to recursive equations of quantum processes, which proves useful in verifying complex quantum protocols. To capture the idea that a quantum process approximately implements its specification, and provide techniques and tools for approximate reasoning, a quantified version of strong bisimulation, which defines for each pair of quantum processes a bisimulation-based distance characterizing the extent to which they are strongly bisimilar, is also introduced. © 2012 ACM.",Bisimulation; Congruence; Quantum communication; Quantum computing; Quantum process algebra,Algebra; Communication; Cryptography; Quantum communication; Quantum computers; Quantum electronics; Quantum optics; Algebraic laws; Approximate reasoning; Bisimulations; Classical communication; Classical systems; Combinators; Congruence; Cryptographic systems; Formal techniques; Local operations and classical communications; Parallel composition; Process algebras; Protocol designers; Quantum Computing; Quantum process; Quantum process algebra; Quantum protocols; Quantum system; Quantum world; Quantum-cryptographic protocols; Recursive equations; Strong bisimulation; Weak bisimulation; Fault tolerance
Natural and flexible error recovery for generated modular language environments,2012,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872375948&doi=10.1145%2f2400676.2400678&partnerID=40&md5=ce23eb5e85540c070472d83b2d8eba15,"Integrated Development Environments (IDEs) increase programmer productivity, providing rapid, interactive feedback based on the syntax and semantics of a language. Unlike conventional parsing algorithms, scannerless generalized-LR parsing supports the full set of context-free grammars, which is closed under composition, and hence can parse languages composed from separate grammar modules. To apply this algorithm in an interactive environment, this article introduces a novel error recoverymechanism. Our approach is language independent, and relies on automatic derivation of recovery rules from grammars. By taking layout information into consideration it can efficiently suggest natural recovery suggestions. © 2012 ACM.",Error recovery; Generalized parsing,Algorithms; Computer system recovery; Semantics; Automatic derivation; Generalized parsing; Integrated development environment; Interactive Environments; Interactive feedback; Layout information; Modular language; Natural recovery; Parsing algorithm; Programmer productivity; Formal languages
On the termination of integer loops,2012,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872372082&doi=10.1145%2f2400676.2400679&partnerID=40&md5=9e1dd7fd8a55ba2004c8aae5db6c7eea,"In this article we study the decidability of termination of several variants of simple integer loops, without branching in the loop body and with affine constraints as the loop guard (and possibly a precondition). We show that termination of such loops is undecidable in some cases, in particular, when the body of the loop is expressed by a set of linear inequalities where the coefficients are from Z?{r} with r an arbitrary irrational; when the loop is a sequence of instructions, that compute either linear expressions or the step function; and when the loop body is a piecewise linear deterministic update with two pieces. The undecidability result is proven by a reduction from counter programs, whose termination is known to be undecidable. For the common case of integer linear-constraint loops with rational coefficients we have not succeeded in proving either decidability or undecidability of termination, but we show that a Petri net can be simulated with such a loop; this implies some interesting lower bounds. For example, termination for a partially specified input is at least EXPSPACE-hard. © 2012 ACM.",Integer loops; Linear constraints; Termination,Petri nets; Piecewise linear techniques; Affine Constraints; Integer loops; Linear constraints; Linear expression; Linear inequalities; Lower bounds; Piecewise linear; Rational coefficients; Step functions; Termination; Undecidability; Computability and decidability
Space overhead bounds for dynamic memory management with partial compaction,2012,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84869433746&doi=10.1145%2f2362389.2362392&partnerID=40&md5=1f1333c1e246b381bc3e85bfea4c80c8,"Dynamic memory allocation is ubiquitous in today's runtime environments. Allocation and deallocation of objects during program execution may cause fragmentation and foil the program's ability to allocate objects. Robson [1971] has shown that a worst-case scenario can create a space overhead within a factor of log n of the space that is actually required by the program, where n is the size of the largest possible object. Compaction can eliminate fragmentation, but is too costly to be run frequently. Many runtime systems employ partial compaction, in which only a small fraction of the allocated objects are moved. Partial compaction reduces some of the existing fragmentation at an acceptable cost. In this article we study the effectiveness of partial compaction and provide the first rigorous lower and upper bounds on its effectiveness in reducing fragmentation at a low cost. © 2012 ACM.",Compaction; Dynamic storage allocation; Memory management; Partial compaction; Runtime systems; Storage allocation,Storage allocation (computer); De-allocation; Dynamic memory allocation; Dynamic memory management; Dynamic storage allocation; Low costs; Lower and upper bounds; Memory management; Program execution; Runtime environments; Runtime systems; Space overhead; Worst case scenario; Compaction
Equivalence checking of static affine programs using widening to handle recurrences,2012,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84869481338&doi=10.1145%2f2362389.2362390&partnerID=40&md5=d97c3f42c708cbf01c4a6aee7999aa54,"Designers often apply manual or semi-automatic loop and data transformations on array- and loop-intensive programs to improve performance. It is crucial that such transformations preserve the functionality of the program. This article presents an automatic method for constructing equivalence proofs for the class of static affine programs. The equivalence checking is performed on a dependence graph abstraction and uses a new approach based on widening to find the proper induction hypotheses for reasoning about recurrences. Unlike transitive-closure-based approaches, this widening approach can also handle nonuniform recurrences. The implementation is publicly available and is the first of its kind to fully support commutative operations. © 2012 ACM.",Commutativity; Equivalence checking; Polytope model; Recurrences; Widening,Model checking; Widening (transportation arteries); Affine program; Automatic method; Commutativity; Data transformation; Dependence graphs; Equivalence checking; Induction hypothesis; Polytopes; Recurrences; Semi-automatics; Equivalence classes
Essential AOP: The a calculus,2012,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84869383748&doi=10.1145%2f2362389.2362391&partnerID=40&md5=4c722f66c44f5b68d11379a3013d89a3,"Aspect-oriented programming (AOP) has produced interesting language designs, but also ad hoc semantics that needs clarification. We contribute to this clarification with a calculus that models essential AOP, both simpler and more general than existing formalizations. In AOP, advice may intercept method invocations, and proceed executes the suspended call. Proceed is an ad hoc mechanism, only usable inside advice bodies. Many pointcut mechanisms, for example, wildcards, also lack regularity. We model proceed using first-class closures, and shift complexity from pointcuts to ordinary object-oriented code. Two well-known pointcut categories, call and execution, are commonly considered similar. We formally expose their differences, and resolve the associated soundness problem. Our calculus includes type ranges, an intuitive and concise alternative to explicit type variables that allows advice to be polymorphic over intercepted methods. We use calculus parameters to cover type safety for a wide design space of other features. Type soundness is verified in Coq. © 2012 ACM.",Aspect-oriented programming; Semantics; Typing,Aspect oriented programming; Clarifiers; Semantics; Aspect-oriented; Cover types; Design spaces; Intercept methods; Language design; Object-oriented code; Pointcut; Type variables; Typing; Calculations
Multivariate amortized resource analysis,2012,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84869453029&doi=10.1145%2f2362389.2362393&partnerID=40&md5=d17e7348640bd74acf55c61e85c39c58,"We study the problem of automatically analyzing the worst-case resource usage of procedures with several arguments. Existing automatic analyses based on amortization or sized types bound the resource usage or result size of such a procedure by a sum of unary functions of the sizes of the arguments. In this article we generalize this to arbitrary multivariate polynomial functions thus allowing bounds of the form mn which had to be grossly overestimated by m 2 + n2 before. Our framework even encompasses bounds like Σi j≥n mimj where the mi are the sizes of the entries of a list of length n. This allows us for the first time to derive useful resource bounds for operations on matrices that are represented as lists of lists and to considerably improve bounds on other superlinear operations on lists such as longest common subsequence and removal of duplicates from lists of lists. Furthermore, resource bounds are now closed under composition which improves accuracy of the analysis of composed programs when some or all of the components exhibit superlinear resource or size behavior. The analysis is based on a novel multivariate amortized resource analysis. We present it in form of a type system for a simple first-order functional language with lists and trees, prove soundness, and describe automatic type inference based on linear programming. We have experimentally validated the automatic analysis on a wide range of examples from functional programming with lists and trees. The obtained bounds were compared with actual resource consumption. All bounds were asymptotically tight, and the constants were close or even identical to the optimal ones. © 2012 ACM.",Amortized analysis; Functional programming; Quantitative analysis; Resource consumption; Static analysis,Chemical Analysis; Computer Programing; Forestry; Quantitative Analysis; Chemical analysis; Depreciation; Forestry; Static analysis; Amortized analysis; Automatic analysis; First-order functional languages; Longest common subsequences; Multivariate polynomial; Resource bounds; Resource consumption; Resource usage; Superlinear; Type inferences; Type systems; Unary functions; Functional programming
Scalaextrap: Trace-based communication extrapolation for SPMD programs,2012,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860547780&doi=10.1145%2f2160910.2160914&partnerID=40&md5=6cbd6d5a0bbe278a2ac9e16ce07b31ba,"Performancemodeling for scientific applications is important for assessing potential application performance and systems procurement in high-performance computing (HPC). Recent progress on communication tracing opens up novel opportunities for communication modeling due to its lossless yet scalable trace collection. Estimating the impact of scaling on communication efficiency still remains nontrivial due to execution-time variations and exposure to hardware and software artifacts. This work contributes a fundamentally novel modeling scheme.We synthetically generate the application trace for large numbers of nodes via extrapolation from a set of smaller traces. We devise an innovative approach for topology extrapolation of single program, multiple data (SPMD) codes with stencil or mesh communication. Experimental results show that the extrapolated traces precisely reflect the communication behavior and the performance characteristics at the target scale for both strong and weak scaling applications. The extrapolated trace can subsequently be (a) replayed to assess communication requirements before porting an application, (b) transformed to autogenerate communication benchmarks for various target platforms, and (c) analyzed to detect communication inefficiencies and scalability limitations. To the best of our knowledge, rapidly obtaining the communication behavior of parallel applications at arbitrary scale with the availability of timed replay, yet without actual execution of the application, at this scale, is without precedence and has the potential to enable otherwise infeasible system simulation at the exascale level. © 2012 ACM.",Communication; Compression; Trace extrapolation; Tracing,"Benchmarking; Compaction; Computer software selection and evaluation; Extrapolation; Information theory; Communication behavior; Communication efficiency; Hardware and software; High-performance computing; Innovative approaches; Lossless; Parallel application; Performance characteristics; Potential applications; Recent progress; Scientific applications; Single program , multiple datum; System simulations; Trace collection; Tracing; Communication"
Editorial,2012,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860549897&doi=10.1145%2f2160910.2180860&partnerID=40&md5=1964d810933670cfa81b8e8504653a18,[No abstract available],,
A data-centric approach to synchronization,2012,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860572783&doi=10.1145%2f2160910.2160913&partnerID=40&md5=4d740c5d2a8c607a4210a9f748c6ae2f,"Concurrency-related errors, such as data races, are frustratingly difficult to track down and eliminate in large object-oriented programs. Traditional approaches to preventing data races rely on protecting instruction sequences with synchronization operations. Such control-centric approaches are inherently brittle, as the burden is on the programmer to ensure that all concurrently accessed memory locations are consistently protected. Data-centric synchronization is an alternative approach that offloads some of the work on the language implementation. Data-centric synchronization groups fields of objects into atomic sets to indicate that these fields must always be updated atomically. Each atomic set has associated units of work, that is, code fragments that preserve the consistency of that atomic set. Synchronization operations are added automatically by the compiler. We present an extension to the Java programming language that integrates annotations for data-centric concurrency control. The resulting language, called AJ, relies on a type system that enables separate compilation and supports atomic sets that span multiple objects and that also supports full encapsulation for more efficient code generation. We evaluate our proposal by refactoring classes from standard libraries, as well as a number ofmultithreaded benchmarks, to use atomic sets. Our results suggest that data-centric synchronization is easy to use and enjoys low annotation overhead, while successfully preventing data races. Moreover, experiments on the SPECjbb benchmark suggest that acceptable performance can be achieved with a modest amount of tuning. © 2012 ACM.",Concurrent object-oriented programming; Data races; Programming model; Serializability,Atoms; Benchmarking; Concurrency control; Java programming language; Object oriented programming; Program compilers; Alternative approach; Code fragments; Code Generation; Data centric; Data races; Data-centric approaches; Language implementations; Memory locations; Multiple objects; Object-oriented program; Programming models; Refactorings; Separate compilation; Serializability; Standard libraries; Synchronization operation; Type systems; Synchronization
Implicit dynamic frames,2012,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860593744&doi=10.1145%2f2160910.2160911&partnerID=40&md5=54d81845a4a322edd3842389994cc277,"An important, challenging problem in the verification of imperative programs with shared, mutable state is the frame problem in the presence of data abstraction. That is, one must be able to specify and verify upper bounds on the set of memory locations a method can read and write without exposing that method's implementation. Separation logic is now widely considered the most promising solution to this problem. However, unlike conventional verification approaches, separation logic assertions cannot mention heap-dependent expressions from the host programming language, such as method calls familiar to many developers. Moreover, separation logic-based verifiers are often based on symbolic execution. These symbolic execution-based verifiers typically do not support non-separating conjunction, and some of them rely on the developer to explicitly fold and unfold predicate definitions. Furthermore, several researchers have wondered whether it is possible to use verification condition generation and standard first-order provers instead of symbolic execution to automatically verify conformance with a separation logic specification. In this article, we propose a variant of separation logic called implicit dynamic frames that supports heap-dependent expressions inside assertions. Conformance with an implicit dynamic frames specification can be checked by proving the validity of a number of first-order verification conditions. To show that these verification conditions can be discharged automatically by standard first-order provers,we have implemented our approach in a verifier prototype and have used this prototype to verify several challenging examples from related work. Our prototype automatically folds and unfolds predicate definitions, as required, during the proof and can reason about non-separating conjunction which is used in the specifications of some of these examples. Finally, we prove the soundness of the approach. © 2012 ACM.",Frame problem; Program verification; Separation logic,COBOL (programming language); Problem oriented languages; Specifications; Data abstraction; First-order; Frame problem; Imperative programs; Implicit dynamics; Memory locations; Program Verification; Separation logic; Symbolic execution; Upper Bound; Verification condition; Formal logic
Algorithmic verification of asynchronous programs,2012,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860569696&doi=10.1145%2f2160910.2160915&partnerID=40&md5=ca6228a0ff30e67f3a8e02fcefbdb53c,"Asynchronous programming is a ubiquitous systems programming idiom for managing concurrent interactions with the environment. In this style, instead of waiting for time-consuming operations to complete, the programmer makes a non-blocking call to the operation and posts a callback task to a task buffer that is executed later when the time-consuming operation completes. A cooperative scheduler mediates the interaction by picking and executing callback tasks from the task buffer to completion (and these callbacks can post further callbacks to be executed later). Writing correct asynchronous programs is hard because the use of callbacks, while efficient, obscures program control flow. We provide a formal model underlying asynchronous programs and study verification problems for this model. We show that the safety verification problem for finite-data asynchronous programs is EXPSPACE- complete. We show that liveness verification for finite-data asynchronous programs is decidable and polynomial-time equivalent to Petri net reachability. Decidability is not obvious, since even if the data is finite-state, asynchronous programs constitute infinite-state transition systems: both the program stack for an executing task and the task buffer of pending calls to tasks can be potentially unbounded. Our main technical constructions are polynomial-time, semantics-preserving reductions from asynchronous programs to Petri nets and back. The first reduction allows the use of algorithmic techniques on Petri nets for the verification of asynchronous programs, and the second allows lower bounds on Petri nets to apply also to asynchronous programs. We also study several extensions to the basic models of asynchronous programs that are inspired by additional capabilities provided by implementations of asynchronous libraries and classify the decidability and undecidability of verification questions on these extensions. © 2012 ACM.",Asynchronous (event-driven) programming; Fair termination; Liveness; Petri nets,Algorithms; Computer programming; Petri nets; Semantics; Algorithmic techniques; Algorithmic verification; Asynchronous programming; Basic models; Concurrent interactions; Fair termination; Finite-state; Formal model; Infinite-state; Liveness; Lower bounds; Non-blocking; Polynomial-time; Program control; Reachability; Safety verification; Transition system; Undecidability; Verification problems; Computability and decidability
Parameterized loop tiling,2012,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860559400&doi=10.1145%2f2160910.2160912&partnerID=40&md5=4b9a79d5b1a209da7d85b6dd101ec99d,"Loop tiling is a widely used program optimization that improves data locality and enables coarse-grained parallelism. Parameterized tiled loops, where the tile sizes remain symbolic parameters until runtime, are quite useful for iterative compilers and autotuners that produce highly optimized libraries and codes. Although it is easy to generate such loops for (hyper-) rectangular iteration spaces tiled with (hyper-) rectangular tiles, many important computations do not fall into this restricted domain. In the past, parameterized tiled code generation for the general case of convex iteration spaces being tiled by (hyper-) rectangular tiles has been solved with bounding box approaches or with sophisticated and expensive machinery. We present a novel formulation of the parameterized tiled loop generation problem using a polyhedral set called the outset. By reducing the problem of parameterized tiled code generation to that of generating standard loops and simple postprocessing of these loops, the outset method achieves a code generation efficiency that is comparable to existing code generation techniques, including those for fixed tile sizes. We compare the performance of our technique with several other tiled loop generation methods on kernels from BLAS3 and scientific computations. The simplicity of our solution makes it well suited for use in production compilers-in particular, the IBM XL compiler uses the inset-based technique introduced in this article for register tiling.We also provide a complete coverage of parameterized tiling of perfect loop nests by describing three related techniques: (i) a scheme for separating full and partial tiles; (ii) a scheme for generating tiled loops directly from the abstract syntax tree representation of loops; (iii) a formal characterization of parameterized loop tiling using bilinear forms and a Symbolic Fourier-Motzkin Elimination (SFME)-based parameterized tiled loop generation method. © 2012 ACM.",Bounding box; Code generation; Fourier-Motzkin elimination; Parameterized tiling,Cache memory; Machinery; Network components; Parameterization; Abstract Syntax Trees; Bilinear form; Bounding box; Coarse-grained; Code Generation; Data locality; Fourier-motzkin elimination; Generation method; Iteration spaces; Loop nests; Loop tiling; Parameterized; Polyhedral set; Program optimization; Restricted-domain; Runtimes; Scientific computation; Tile size; Program compilers
Structured communication-centered programming for web services,2012,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866017941&doi=10.1145%2f2220365.2220367&partnerID=40&md5=4550f44f229601b1803c21e8ac07108e,"This article relates two different paradigms of descriptions of communication behavior, one focusing on global message flows and another on end-point behaviors, using formal calculi based on session types. The global calculus, which originates from a Web service description language (W3C WS-CDL), describes an interaction scenario from a vantage viewpoint; the end-point calculus, an applied typed π-calculus, precisely identifies a local behavior of each participant. We explore a theory of end-point projection, by which we can map a global description to its end-point counterparts preserving types and dynamics. Three principles of well-structured description and the type structures play a fundamental role in the theory. © 2012 ACM.",Choreography; Communication; End-point projection; Process calculi; Session types; Type system; Web services,Biomineralization; Calculations; Communication; Pathology; Point contacts; Websites; Choreography; End points; Process calculi; Session types; Type systems; Web services
Partially evaluating finite-state runtime monitors ahead of time,2012,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866010121&doi=10.1145%2f2220365.2220366&partnerID=40&md5=27e1ef9a7b800eb7cffe2d16dd5c6c05,"Finite-state properties account for an important class of program properties, typically related to the order of operations invoked on objects. Many library implementations therefore include manually written finitestate monitors to detect violations of finite-state properties at runtime. Researchers have recently proposed the explicit specification of finite-state properties and automatic generation of monitors from the specification. However, runtime monitoring only shows the presence of violations, and typically cannot prove their absence. Moreover, inserting a runtime monitor into a program under test can slow down the program by several orders of magnitude. In this work, we therefore present a set of four static whole-program analyses that partially evaluate runtime monitors at compile time, with increasing cost and precision. As we show, ahead-of-time evaluation can often evaluate the monitor completely statically. This may prove that the program cannot violate the property on any execution or may prove that violations do exist. In the remaining cases, the partial evaluation converts the runtime monitor into a residual monitor. This monitor only receives events from program locations that the analyses failed to prove irrelevant. This makes the residual monitor much more efficient than a full monitor, while still capturing all property violations at runtime. We implemented the analyses in CLARA, a novel framework for the partial evaluation of AspectJ-based runtime monitors, and validated our approach by applying CLARA to finite-state properties over several large-scale Java programs. CLARA proved that most of the programs never violate our example properties. Some programs required monitoring, but in those cases CLARA could often reduce the monitoring overhead to below 10%. We observed that several programs did violate the stated properties. © 2012 ACM.",Runtime monitoring; Static analysis; Typestate analysis,Software testing; Specifications; Automatic Generation; Increasing costs; Orders of magnitude; Partial evaluation; Program properties; Runtime Monitoring; Typestate analysis; Whole-program analysis; Static analysis
Reasoning about Web applications: An operational semantics for HOP,2012,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866015788&doi=10.1145%2f2220365.2220369&partnerID=40&md5=60d06eb3e34e0add0fbe439e2a54752a,"We propose a small-step operational semantics to support reasoning about Web applications written in the multitier languageHOP. The semantics covers both server side and client side computations, as well as their interactions, and includes creation of Web services, distributed client-server communications, concurrent evaluation of service requests at server side, elaboration of HTML documents, DOM operations, evaluation of script nodes in HTML documents and actions from HTML pages at client side. We also model the browser same origin policy (SOP) in the semantics. We propose a safety property by which programs do not get stuck due to a violation of the SOP and a type system to enforce it. © 2012 ACM.",Functional languages; Multitier languages; Web programming,Computer programming languages; Functional programming; HTML; Petroleum reservoir evaluation; Semantics; Web services; Client server communications; Functional languages; Multi-tier languages; Operational semantics; Safety property; Same-origin policy; Service requests; Web programming; Semantic Web
On a technique for transparently empowering classical compiler optimizations on multithreaded code,2012,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866010095&doi=10.1145%2f2220365.2220368&partnerID=40&md5=9b20c8ebb44d978200a3165c744c5d7b,"A large body of data-flow analyses exists for analyzing and optimizing sequential code. Unfortunately, much of it cannot be directly applied on parallel code, for reasons of correctness. This article presents a technique to automatically, aggressively, yet safely apply sequentially-sound data-flow transformations, without change, on shared-memory programs. The technique is founded on the notion of program references being ""siloed"" on certain control-flow paths. Intuitively, siloed references are free of interference from other threads within the confines of such paths. Data-flow transformations can, in general, be unblocked on siloed references. The solution has been implemented in a widely used compiler. Results on benchmarks from SPLASH-2 show that performance improvements of up to 41% are possible, with an average improvement of 6% across all the tested programs over all thread counts. © 2012 ACM.",Data-flow analysis; Parallel-program optimization,Benchmarking; Data flow analysis; Data transfer; Program compilers; Compiler optimizations; Control flows; Data-flow transformations; Multithreaded; Parallel code; Parallel program; Shared-memory programs; Tested programs; Codes (symbols)
Fast interprocedural linear two-variable equalities,2011,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84856024001&doi=10.1145%2f2049706.2049710&partnerID=40&md5=0ee691101b3972591a02e9b16fb59bc7,"In this article we provide an interprocedural analysis of linear two-variable equalities. The novel algorithm has a worst-case complexity of O(n. k4), where k is the number of variables and n is the program size. Thus, it saves a factor of k4 in comparison to a related algorithm based on full linear algebra. We also indicate how the practical runtime can be further reduced significantly. The analysis can be applied, for example, for register coalescing, for identifying local variables and thus for interprocedurally observing stack pointer modifications as well as for an analysis of array index expressions, when analyzing low-level code. © 2011 ACM.",Abstract interpretation; Interprocedural analysis; Linear equalities; Linear two-variable equalities; Low-level code analysis; Static analysis; Summary functions; Variable differences,Abstracting; Algorithms; Codes (symbols); Flocculation; Linear algebra; Abstract interpretations; Code analysis; Inter-procedural analysis; Linear equality; Linear two-variable equalities; Variable differences; Static analysis
Separating ownership topology and encapsulation with generic universe types,2011,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84856023820&doi=10.1145%2f2049706.2049709&partnerID=40&md5=b51aa36864caae487120a07c02c07ac5,"Ownership is a powerful concept to structure the object store and to control aliasing and modifications of objects. This article presents an ownership type system for a Java-like programming language with generic types. Like our earlier Universe type system, Generic Universe Types structure the heap hierarchically. In contrast to earlier work, we separate the enforcement of an ownership topology from an encapsulation system. The topological system uses an existential modifier to express that no ownership information is available statically. On top of the topological system, we build an encapsulation system that enforces the owner-as-modifier discipline. This discipline does not restrict aliasing, but requires modifications of an object to be initiated by its owner. This allows owner objects to control state changes of owned objects-for instance, to maintain invariants. Separating the topological system from the encapsulation system allows for a cleaner formalization, separation of concerns, and simpler reuse of the individual systems in different contexts. © 2011 ACM.",Encapsulation; Generic; Owner-as-modifier; Ownership types; Topology; Universe types,Encapsulation; Java programming language; Separation; Aliasing; Control state; Encapsulation systems; Generic; Generic types; Individual systems; Object store; Owner-as-modifier; Ownership type; Ownership types; Programming language; Separation of concerns; System use; Type systems; Universe types; Topology
Editorial note,2011,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84856008572&doi=10.1145%2f2049706.2049707&partnerID=40&md5=2fe0faaa11e5a5e3204ba2972f7a9a80,[No abstract available],,
Floyd-hoare logic for quantum programs,2011,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84856001673&doi=10.1145%2f2049706.2049708&partnerID=40&md5=b398e1b22c11a6c4fb8daa5be1f45fe8,"Floyd-Hoare logic is a foundation of axiomatic semantics of classical programs, and it provides effective proof techniques for reasoning about correctness of classical programs. To offer similar techniques for quantum program verification and to build a logical foundation of programming methodology for quantum computers, we develop a full-fledged Floyd-Hoare logic for both partial and total correctness of quantum programs. It is proved that this logic is (relatively) complete by exploiting the power of weakest preconditions and weakest liberal preconditions for quantum programs. © 2011 ACM.",Axiomatic semantics; Completeness; Floyd-hoare logic; Programming language; Quantum computation,Computational linguistics; Computer programming; Quantum computers; Semantics; Axiomatic semantics; Completeness; Floyd-hoare logic; Logical foundations; Partial and total correctness; Program Verification; Programming language; Programming methodology; Weakest precondition; Formal logic
Software model checking using languages of nested trees,2011,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-82155177291&doi=10.1145%2f2039346.2039347&partnerID=40&md5=3d950ed2198e6c2af493c6df1ba17b5e,"While model checking of pushdown systems is by now an established technique in software verification, temporal logics and automata traditionally used in this area are unattractive on two counts. First, logics and automata traditionally used in model checking cannot express requirements such as pre/post-conditions that are basic to analysis of software. Second, unlike in the finite-state world, where the μ-calculus has a symbolic model-checking algorithm and serves as an ""assembly language"" to which temporal logics can be compiled, there is no common formalism\-either fixpoint-based or automata-theoretic\-to model-check requirements on pushdown models. In this article,we introduce a new theory of temporal logics and automata that addresses the above issues, and provides a unified foundation for the verification of pushdown systems. The key idea here is to view a program as a generator of structures known as nested trees as opposed to trees. A fixpoint logic (called NT-μ) and a class of automata (called nested tree automata) interpreted on languages of these structures are now defined, and branching-time model-checking is phrased as language inclusion and membership problems for these languages. We show that NT-μ and nested tree automata allow the specification of a new frontier of requirements usable in software verification. At the same time, their model checking problem has the same worst-case complexity as their traditional analogs, and can be solved symbolically using a fixpoint computation that generalizes, and includes as a special case, ""summary""- based computations traditionally used in interprocedural program analysis.We also show that our logics and automata define a robust class of languages\-in particular, just as the μ-calculus is equivalent to alternating parity automata on trees, NT-μ is equivalent to alternating parity automata on nested trees. © 2011 ACM.",μ-calculus; Games; Infinite-state; Interprocedural analysis; Logic; Modelchecking; Pushdown systems; Specification; Verification,Algorithms; Computer Programs; Forestry; Fuzzy Logic; Internet; Mathematical Models; Problem Solving; Automata theory; Calculations; Forestry; Specifications; Temporal logic; Verification; Games; Infinite-state; Inter-procedural analysis; Logic; Pushdown systems; Model checking
Bottom-up shape analysis using LISF,2011,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-82155164508&doi=10.1145%2f2039346.2039349&partnerID=40&md5=fc922ad57a49b9bd3f2ab43843d5283f,"In this article, we present a new shape analysis algorithm. The key distinguishing aspect of our algorithm is that it is completely compositional, bottom-up and noniterative. We present our algorithm as an inference system for computing Hoare triples summarizing heap manipulating programs. Our inference rules are compositional: Hoare triples for a compound statement are computed from the Hoare triples of its component statements. These inference rules are used as the basis for bottom-up shape analysis of programs. Specifically, we present a Logic of Iterated Separation Formulae (LISF), which uses the iterated separating conjunct of Reynolds [2002] to represent program states. A key ingredient of our inference rules is a strong bi-abduction operation between two logical formulas. We describe sound strong bi-abduction and satisfiability procedures for LISF. We have built a tool called SPINE that implements these inference rules and have evaluated it on standard shape analysis benchmark programs. Our experiments show that SPINE can generate expressive summaries, which are complete functional specifications in many cases. © 2011 ACM.",Compositional analysis; Hoare logic; Separation logic,Algorithms; Formal logic; Separation; Benchmark programs; Compositional analysis; Functional specification; Hoare Logic; Inference rules; Inference systems; Logical formulas; Non-iterative; Program state; Reynolds; Satisfiability procedure; Separation logic; Shape analysis; Inference engines
On contract satisfaction in a higher-order world,2011,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-82155164516&doi=10.1145%2f2039346.2039348&partnerID=40&md5=71a7a84c92a97ef12066594068abab7d,"Behavioral software contracts have become a popular mechanism for specifying and ensuring logical claims about a program's flow of values. While contracts for first-order functions come with a natural interpretation and are well understood, the various incarnations of higher-order contracts adopt, implicitly or explicitly, different views concerning the meaning of contract satisfaction. In this article, we define various notions of contract satisfaction in terms of observational equivalence and compare them with each other and notions in the literature. Specifically, we introduce a small model language with higher-order contracts and use it to formalize different notions of contract satisfaction. Each of them demands that the contract parties satisfy certain observational equivalences. © 2011 ACM.",Contract satisfaction; Higher-order contracts,Linguistics; First-order; Higher order; Model languages; Observational equivalences; Software contracts; Software engineering
An abstract model of certificate translation,2011,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79960807029&doi=10.1145%2f1985342.1985344&partnerID=40&md5=a073c14770791acb9a3db0d5a2b6392f,"A certificate is a mathematical object that can be used to establish that a piece of mobile code satisfies some security policy. In general, certificates cannot be generated automatically. There is thus an interest in developing methods to reuse certificates generated for source code to provide strong guarantees of the compiled code correctness. Certificate translation is a method to transform certificates of program correctness along semantically justified program transformations. These methods have been developed in previous work, but they were strongly dependent on particular programming and verification settings. This article provides a more general development in the setting of abstract interpretation, showing the scalability of certificate translation. © 2011 ACM.",Program optimizations; Program verification; Proofcarrying code; Static analysis,Abstracting; Program translators; Abstract interpretations; Abstract models; Mathematical objects; Mobile codes; Program correctness; Program optimization; Program transformations; Program Verification; Proof-carrying code; Security policy; Source codes; Static analysis
A theory of synchronous relational interfaces,2011,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79960813848&doi=10.1145%2f1985342.1985345&partnerID=40&md5=80d6d1a2dde08520a4a1f80360fab7bd,"Compositional theories are crucial when designing large and complex systems from smaller components. In this work we propose such a theory for synchronous concurrent systems. Our approach follows so-called interface theories, which use game-theoretic interpretations of composition and refinement. These are appropriate for systems with distinct inputs and outputs, and explicit conditions on inputs that must be enforced during composition. Our interfaces model systems that execute in an infinite sequence of synchronous rounds. At each round, a contract must be satisfied. The contract is simply a relation specifying the set of valid input/output pairs. Interfaces can be composed by parallel, serial or feedback composition. A refinement relation between interfaces is defined, and shown to have two main properties: (1) it is preserved by composition, and (2) it is equivalent to substitutability, namely, the ability to replace an interface by another one in any context. Shared refinement and abstraction operators, corresponding to greatest lower and least upper bounds with respect to refinement, are also defined. Input-complete interfaces, that impose no restrictions on inputs, and deterministic interfaces, that produce a unique output for any legal input, are discussed as special cases, and an interesting duality between the two classes is exposed. A number of illustrative examples are provided, as well as algorithms to compute compositions, check refinement, and so on, for finite-state interfaces. © 2011 ACM.",Compositionality; Interfaces; Refinement; Substitutability,Game theory; Compositional theory; Compositionality; Concurrent systems; Finite-state; Illustrative examples; Input/output; Model system; Refinement; Substitutability; Upper Bound; Mathematical operators
JavaGI: The interaction of type classes with interfaces and inheritance,2011,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79960810109&doi=10.1145%2f1985342.1985343&partnerID=40&md5=de5be40ece312d9305f8d63ae11f09c8,"The language JavaGI extends Java 1.5 conservatively by a generalized interface mechanism. The generalization subsumes retroactive and type-conditional interface implementations, binary methods, symmetric multiple dispatch, interfaces over families of types, and static interface methods. These features make certain coding patterns redundant, increase the expressiveness of the type system, and permit solutions to extension and integration problems with components in binary form, for which previously several unrelated extensions had been suggested. This article explains JavaGI and motivates its design. Moreover, it formalizes a core calculus for JavaGI and proves type soundness, decidability of typechecking, and determinacy of evaluation. The article also presents the implementation of a JavaGI compiler and an accompanying run-time system. The compiler, based on the Eclipse Compiler for Java, offers mostly modular static typechecking and fully modular code generation. It defers certain well-formedness checks until load time to increase flexibility and to enable full support for dynamic loading. Benchmarks show that the code generated by the compiler offers good performance. Several case studies demonstrate the practical utility of the language and its implementation. © 2011 ACM.",Benchmarks; Binary methods; Case studies; Constraint entailment; Explicit implementing types; External methods; Formalization; Generalized interfaces; Implementation; Implementation constraints; Java; JavaGI; Multiheaded interfaces; Multimethods; Multiple dispatch; Open classes; Retroactive interface implementations; Static interfacemethods; Subtyping; Type conditionals,Computability and decidability; Dynamic loads; Java programming language; Benchmarks; Binary method; Constraint entailment; Explicit implementing types; External methods; Formalization; Generalized interfaces; Implementation; Implementation constraints; Java; JavaGI; Multi methods; Multiheaded interfaces; Multiple dispatch; Open classes; Retroactive interface implementations; Static interfacemethods; Subtypings; Type conditionals; Program compilers
Mathematical foundation of trace scheduling,2011,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79955695227&doi=10.1145%2f1961204.1961206&partnerID=40&md5=a86ffc2bef5164131393450c4775b126,"Since its introduction by Joseph A. Fisher in 1979, trace scheduling has influenced much of the work on compile-time ILP (Instruction Level Parallelism) transformations. Initially developed for use in microcode compaction, it quickly became the main technique for machine-level compile-time parallelism exploitation. Although it has been used since the 1980s in many state-of-the-art compilers (e.g., Intel, Fujitsu, HP), a rigorous theory of trace scheduling is still lacking in the existing literature. This is reflected in the ad hoc way compensation code is inserted after a trace compaction, in the total absence of any attempts to measure the size of that compensation code, and so on. The aim of this article is to create a mathematical theory of the foundation of trace scheduling. We give a clear algorithm showing how to insert compensation code after a trace is replaced with its schedule, and then prove that the resulting program is indeed equivalent to the original program. We derive an upper bound on the size of that compensation code, and show that this bound can be actually attained. We also give a very simple proof that the trace scheduling algorithm always terminates. © 2011 ACM.",Compensation code; Trace replacement; Trace scheduling,Compaction; Compensation code; Compile time; Fujitsu; Instruction level parallelism; Mathematical foundations; Mathematical theory; Theory of traces; Trace replacement; Trace scheduling; Upper Bound; Scheduling algorithms
Solving systems of rational equations through strategy iteration,2011,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79955676587&doi=10.1145%2f1961204.1961207&partnerID=40&md5=f187deb3a7a662b3deaef6e63b324802,"We present practical algorithms for computing exact least solutions of equation systems over the reals with addition, multiplication by positive constants, minimum and maximum. The algorithms are based on strategy iteration. Our algorithms can, for instance, be used for the analysis of recursive stochastic games. In the present article we apply our techniques for computing abstract least fixpoint semantics of affine programs over the relational template polyhedra domain. In particular, we thus obtain practical algorithms for computing abstract least fixpoint semantics over the abstract domains of intervals, zones, and octagons. © 2011 ACM.",Abstract interpretation; Fixpoint equation systems; Recursive stochastic games; Static program analysis; Strategy improvement algorithms,Algorithms; Geometry; Iterative methods; Semantics; Stochastic systems; Abstract interpretations; Fixpoints; Static program analysis; Stochastic game; Strategy improvement algorithms; Abstracting
Refactoring using type constraints,2011,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79955704455&doi=10.1145%2f1961204.1961205&partnerID=40&md5=e8e2e8f02c16dc9a0ddd1aaf7b6d890a,"Type constraints express subtype relationships between the types of program expressions, for example, those relationships that are required for type correctness. Type constraints were originally proposed as a convenient framework for solving type checking and type inference problems. This paper shows how type constraints can be used as the basis for practical refactoring tools. In our approach, a set of type constraints is derived from a type-correct program P. The main insight behind our work is the fact that P constitutes just one solution to this constraint system, and that alternative solutions may exist that correspond to refactored versions of P. We show how a number of refactorings for manipulating types and class hierarchies can be expressed naturally using type constraints. Several refactorings in the standard distribution of Eclipse are based on our work. © 2011 ACM.",Program transformation; Refactoring; Type constraints,Class hierarchies; Constraint systems; Program transformations; Refactoring tools; Refactorings; Standard distributions; Type constraints; Type correctness; Type inferences; Typechecking
Refinement types for secure implementations,2011,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79951546693&doi=10.1145%2f1890028.1890031&partnerID=40&md5=3c3d09cd07c7591d4657fcddfa5a19f7,"We present the design and implementation of a typechecker for verifying security properties of the source code of cryptographic protocols and access control mechanisms. The underlying type theory is a λ-calculus equipped with refinement types for expressing pre- and post-conditions within first-order logic.We derive formal cryptographic primitives and represent active adversaries within the type theory. Well-typed programs enjoy assertion-based security properties, with respect to a realistic threat model including key compromise. The implementation amounts to an enhanced typechecker for the general-purpose functional language F#; typechecking generates verification conditions that are passed to an SMT solver. We describe a series of checked examples. This is the first tool to verify authentication properties of cryptographic protocols by typechecking their source code. © 2011.",,Access control; Calculations; Formal logic; Access control mechanism; Active adversary; Cryptographic primitives; Cryptographic protocols; First-order; Functional languages; Key compromise; Secure implementation; Security properties; Source codes; Type theory; Typechecking; Verification condition; Cryptography
Morphing: Structurally shaping a class by reflecting on others,2011,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79951548478&doi=10.1145%2f1890028.1890029&partnerID=40&md5=b84f7a09e17fa60e32ce59ae4954c298,"We presentMorphJ: a language for specifying general classes whose members are produced by iterating over members of other classes. We call this technique ""class morphing"" or just ""morphing."" Morphing extends the notion of genericity so that not only types of methods and fields, but also the structure of a class can vary according to type variables. This adds a disciplined form of metaprogramming to mainstream languages and allows expressing common programming patterns in a highly generic way that is otherwise not supported by conventional techniques. For instance, morphing lets us write generic proxies (i.e., classes that can be parameterized with another class and export the same public methods as that class); default implementations (e.g., a generic do-nothing type, configurable for any interface); semantic extensions (e.g., specialized behavior for methods that declare a certain annotation); and more. MorphJ's hallmark feature is that, despite its emphasis on generality, it allows modular type-checking: a MorphJ class can be checked independently of its uses. Thus, the possibility of supplying a type parameter that will lead to invalid code is detected early, an invaluable feature for highly general components that will be statically instantiated by other programmers. We demonstrate the benefits of morphing with several examples, including a MorphJ reimplementation of DSTM2, a software transactional memory library which reduces 1,484 lines of Java reflection and bytecode engineering library calls to just 586 lines of MorphJ code. © 2011.",Language extensions; Metaprogramming; Morphing,Computer software; Semantics; Bytecode engineering; Configurable; Conventional techniques; General class; Genericity; Language extensions; Metaprogramming; Morphing; Parameterized; Programming patterns; Semantic extensions; Software transactional memory; Type variables; Typechecking; Java programming language
Automated termination proofs for haskell by term rewriting,2011,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79951536877&doi=10.1145%2f1890028.1890030&partnerID=40&md5=f7c86a1a6ddd2bf353a694e1610e12af,"There are many powerful techniques for automated termination analysis of term rewriting. However, up to now they have hardly been used for real programming languages.We present a new approach which permits the application of existing techniques from term rewriting to prove termination of most functions defined in Haskell programs. In particular, we show how termination techniques for ordinary rewriting can be used to handle those features of Haskell which are missing in term rewriting (e.g., lazy evaluation, polymorphic types, and higher-order functions).We implemented our results in the termination prover AProVE and successfully evaluated them on existing Haskell libraries. © 2011.",Dependency pairs; Functional programming; Haskell; Term rewriting; Termination analysis,Automated termination; Automated termination proofs; Dependency pairs; Haskell; Haskell programs; Higher-order functions; Lazy evaluation; New approaches; Polymorphic types; Term rewriting; Termination analysis; Functional programming
Mechanically verified proof obligations for linearizability,2011,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79251565836&doi=10.1145%2f1889997.1890001&partnerID=40&md5=5d96727502df257cd63312697f80468a,"Concurrent objects are inherently complex to verify. In the late 80s and early 90s,Herlihy andWing proposed linearizability as a correctness condition for concurrent objects, which, once proven, allows us to reason about concurrent objects using pre- and postconditions only. A concurrent object is linearizable if all of its operations appear to take effect instantaneously some time between their invocation and return. In this article we define simulation-based proof conditions for linearizability and apply them to two concurrent implementations, a lock-free stack and a set with lock-coupling. Similar to other approaches, we employ a theorem prover (here, KIV) to mechanize our proofs. Contrary to other approaches, we also use the prover to mechanically check that our proof obligations actually guarantee linearizability. This check employs the original ideas of Herlihy and Wing of verifying linearizability via possibilities. © 2011 ACM.",Concurrent access; KIV; Linearizability; Nonatomic refinement; Refinement; Theorem proving; Z,Problem solving; Concurrent access; KIV; Linearizability; Non-atomic refinement; Refinement; Z; Theorem proving
Semantics of transactional memory and automatic mutual exclusion,2011,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79251536856&doi=10.1145%2f1889997.1889999&partnerID=40&md5=b8b2a4dca659a5e9396df5cc9aa1381d,"Software Transactional Memory (STM) is an attractive basis for the development of language features for concurrent programming. However, the semantics of these features can be delicate and problematic. In this article we explore the trade-offs semantic simplicity, the viability of efficient implementation strategies, and the flexibility of language constructs. Specifically, we develop semantics and type systems for the constructs of the Automatic Mutual Exclusion (AME) programming model; our results apply also to other constructs, such as atomic blocks. With this semantics as a point of reference, we study several implementation strategies. We model STM systems that use in-place update, optimistic concurrency, lazy conflict detection, and rollback. These strategies are correct only under nontrivial assumptions that we identify and analyze. One important source of errors is that some efficient implementations create dangerous ""zombie"" computations where a transaction keeps running after experiencing a conflict; the assumptions confine the effects of these computations. © 2011 ACM.",Atomicity; Correctness,Computer programming; Java programming language; Storage allocation (computer); Atomic blocks; Atomicity; Concurrent programming; Conflict detection; Correctness; Efficient implementation; Implementation strategies; In-place update; Language constructs; Language features; Mutual exclusions; Optimistic concurrency; Programming models; Software transactional memory; Transactional memory; Type systems; Semantics
ACM Transactions on Programming Languages and Systems: Editorial,2011,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79251550440&doi=10.1145%2f1889997.1889998&partnerID=40&md5=3fac829fd2c178d778b88c433cf4621b,[No abstract available],,
Environmental bisimulations for higher-order languages,2011,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79251560630&doi=10.1145%2f1889997.1890002&partnerID=40&md5=d24a70d3a03a922868c172f63a1c8adb,"Developing a theory of bisimulation in higher-order languages can be hard. Particularly challenging can be: (1) the proof of congruence, as well as enhancements of the bisimulation proof method with ""up-to context"" techniques, and (2) obtaining definitions and results that scale to languages with different features. To meet these challenges, we present environment bisimulations, a form of bisimulation for higher-order languages, and its basic theory. We consider four representative calculi: pure λ-calculi (call-by-name and call-by-value), call-by-value λ-calculus with higher-order store, and then Higher-Order p-calculus. In each case: we present the basic properties of environment bisimilarity, including congruence; we show that it coincides with contextual equivalence; we develop some upto techniques, including up-to context, as examples of possible enhancements of the associated bisimulation method. Unlike previous approaches (such as applicative bisimulations, logical relations, Sumii-Pierce-Koutavas-Wand), our method does not require induction/indices on evaluation derivation/steps (which may complicate the proofs of congruence, transitivity, and the combination with up-to techniques), or sophisticated methods such as Howe's for proving congruence. It also scales from the pure λ-calculi to the richer calculi with simple congruence proofs. © 2011 ACM.",λ-calculus; Bisimulation; Congruence; Higher-order languages; Higher-order p-calculus,Biomineralization; Fault tolerance; Pathology; Quality assurance; Basic properties; Basic theory; Bisimilarity; Bisimulations; Call-by-name; Congruence; Contextual equivalence; Higher order; Higher-order languages; Logical relations; Proof methods; Calculations
LOCKSMITH: Practical static race detection for C,2011,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79251557175&doi=10.1145%2f1889997.1890000&partnerID=40&md5=00d87e815f09624381c915947bc561e4,"LOCKSMITH is a static analysis tool for automatically detecting data races in C programs. In this article, we describe each of LOCKSMITH's component analyses precisely, and present systematic measurements that isolate interesting trade-offs between precision and efficiency in each analysis. Using a benchmark suite comprising stand-alone applications and Linux device drivers totaling more than 200,000 lines of code, we found that a simple no-worklist strategy yielded the most efficient interprocedural dataflow analysis; that our sharing analysis was able to determine that most locations are thread-local, and therefore need not be protected by locks; that modeling C structs and void pointers precisely is key to both precision and efficiency; and that context sensitivity yields a much more precise analysis, though with decreased scalability. Put together, our results illuminate some of the key engineering challenges in building LOCKSMITH and data race detection analyses in particular, and constraint-based program analyses in general. © 2011 ACM.",Context sensitivity; Contextual effects; Correlation inference; Data race; Locksmith; Race detection; Sharing analysis; Static analysis,Benchmarking; Computer operating systems; Data flow analysis; Static analysis; Context sensitivity; Contextual effects; Correlation inference; Data races; Locksmith; Race detection; Sharing analysis; Sensitivity analysis
Finite differencing of logical formulas for static analysis,2010,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77955647629&doi=10.1145%2f1749608.1749613&partnerID=40&md5=e797b5d7b359adda8369d40fd58f4676,"This article concerns mechanisms for maintaining the value of an instrumentation relation (also known as a derived relation or view), defined via a logical formula over core relations, in response to changes in the values of the core relations. It presents an algorithm for transforming the instrumentation relation's defining formula into a relation-maintenance formula that captures what the instrumentation relation's new value should be. The algorithm runs in time linear in the size of the defining formula. The technique applies to program analysis problems in which the semantics of statements is expressed using logical formulas that describe changes to core relation values. It provides a way to obtain values of the instrumentation relations that reflect the changes in core relation values produced by executing a given statement. We present experimental evidence that our technique is an effective one: for a variety of benchmarks, the relation-maintenance formulas produced automatically using our approach yield the same precision as the best available hand-crafted ones. © 2010 ACM.",3-valued logic; Abstract interpretation; Finite differencing; Materialized view; Shape analysis; Static analysis,Instruments; Maintenance; Program interpreters; Static analysis; 3-valued logic; Abstract interpretations; Finite differencing; Materialized view; Shape analysis; Abstracting
WYSINWYX: What you see is not what you execute,2010,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77955623537&doi=10.1145%2f1749608.1749612&partnerID=40&md5=85fbc99ede93bc3b1e9e9c583ea414ef,"Over the last seven years, we have developed static-analysis methods to recover a good approximation to the variables and dynamically allocated memory objects of a stripped executable, and to track the flow of values through them. The article presents the algorithms that we developed, explains how they are used to recover Intermediate Representations (IRs) from executables that are similar to the IRs that would be available if one started from source code, and describes their application in the context of program understanding and automated bug hunting. Unlike algorithms for analyzing executables that existed prior to our work, the ones presented in this article provide useful information about memory accesses, even in the absence of debugging information. The ideas described in the article are incorporated in a tool for analyzing Intel x86 executables, called CodeSurfer/x86. CodeSurfer/x86 builds a system dependence graph for the program, and provides a GUI for exploring the graph by (i) navigating its edges, and (ii) invoking operations, such as forward slicing, backward slicing, and chopping, to discover how parts of the program can impact other parts. To assess the usefulness of the IRs recovered by CodeSurfer/x86 in the context of automated bug hunting, we built a tool on top of CodeSurfer/x86, called Device-Driver Analyzer for x86 (DDA/x86), which analyzes device-driver executables for bugs. Without the benefit of either source code or symbol-table/debugging information, DDA/x86 was able to find known bugs (that had been discovered previously by source-code analysis tools), along with useful error traces, while having a low false-positive rate. DDA/x86 is the first known application of program analysis/verification techniques to industrial executables. © 2010 ACM.",Abstract interpretation; Context-sensitive analysis; Data structure recovery; Interprocedural dataflow analysis; Pointer analysis; Reverse engineering; Static analysis,Abstracting; Data handling; Data structures; Program interpreters; Recovery; Reverse engineering; Static analysis; Abstract interpretations; Context-sensitive analysis; Data structure recovery; Interprocedural dataflow analysis; Pointer analysis; Data flow analysis
La prossima vita at TOPLAS,2010,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77955633850&doi=10.1145%2f1749608.1749609&partnerID=40&md5=0602a1b0a0f569a35edc7edb8e025170,[No abstract available],,
Typing linear constraints,2010,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77955621924&doi=10.1145%2f1749608.1749610&partnerID=40&md5=ba6a7ca1e08dde7491ec1b30f72efc99,"We present a type system for linear constraints over the reals intended for reasoning about the input-output directionality of variables. Types model the properties of definiteness, range width or approximation, lower and upper bounds of variables in a linear constraint. Several proof procedures are presented for inferring the type of a variable and for checking validity of type assertions. We rely on theory and tools for linear programming problems, linear algebra, parameterized polyhedra and negative constraints. An application of the type system is proposed in the context of the static analysis of constraint logic programs. Type assertions are at the basis of the extension of wellmoding from pure logic programming. The proof procedures (both for type assertion validity and for well-moding) are implemented and their computational omplexity is discussed. We report experimental results demonstrating the efficiency in practice of the proposed approach. © 2010 ACM.",Constraint logic programming; Definiteness; Linear constraints; Polyhedra; Well-moding,Automata theory; Geometry; Logic programming; Real variables; Constraint Logic Programming; Definiteness; Linear constraints; Polyhedra; Well-moding; Computer programming languages
Semantics of fractional permissions with nesting,2010,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77955647304&doi=10.1145%2f1749608.1749611&partnerID=40&md5=498d68a3d19be5e87010201767fc9e66,"Permissions specify mutable state that can be accessed by a program. Fractions distinguish write access (1) from read access (any smaller fraction). Nesting can model object invariants and ownership. Fractional permissions provides a foundation the meaning of many of access-based annotations: uniqueness, read-only, immutability, method effects, guarded state, etc. The semantics of fractional permissions with nesting is given in terms of ""fractional heaps."" We show that the fraction law π = 1/2π + 1/2π permits sound reasoning and that nesting can be carried out safely using only local reasoning. © 2010 ACM.",Languages; Theory; Verification,Linguistics; Languages; Local reasoning; Object invariants; Theory; Semantics
A calculus for uniform feature composition,2010,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77952941288&doi=10.1145%2f1745312.1745316&partnerID=40&md5=46cda4319e3ed4316a34cea20beda7bc,"The goal of feature-oriented programming (FOP) is to modularize software systems in terms of features. A feature refines the content of a base program. Both base programs and features may contain various kinds of software artifacts, for example, source code in different languages, models, build scripts, and documentation. We and others have noticed that when composing features, different kinds of software artifacts can be refined in a uniform way, regardless of what they represent. We present gDeep, a core calculus for feature composition, which captures the language independence of FOP; it can be used to compose features containing many different kinds of artifact in a type-safe way. The calculus allows us to gain insight into the principles of FOP and to define general algorithms for feature composition and validation. We provide the formal syntax, operational semantics, and type system of gDeep and outline how languages like Java, Haskell, Bali, and XML can be plugged in. © 2010 ACM.",Feature composition; Feature-oriented programming; Principle of uniformity; Type systems,Calculations; Computer software; Linguistics; Query languages; Refining; CoRE calculus; Feature-oriented programming; Gain insight; Haskell; Language independence; Operational semantics; Software artifacts; Software systems; Source codes; Type systems; Java programming language
A hybrid type system for lock-freedom of mobile processes,2010,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77952901027&doi=10.1145%2f1745312.1745313&partnerID=40&md5=5367ca96eb6915a774304c9b5556286f,"We propose a type system for lock-freedom in the π-calculus, which guarantees that certain communications will eventually succeed. Distinguishing features of our type system are: it can verify lock-freedom of concurrent programs that have sophisticated recursive communication structures; it can be fully automated; it is hybrid, in that it combines a type system for lock-freedom with local reasoning about deadlock-freedom, termination, and confluence analyses. Moreover, the type system is parameterized by deadlock-freedom/termination/confluence analyses, so that any methods (e.g. type systems and model checking) can be used for those analyses. A lock-freedom analysis tool has been implemented based on the proposed type system, and tested for nontrivial programs. © 2010 ACM.",Concurrency; Mobile processes; Type systems,Automata theory; Calculations; Analysis tools; Communication structures; Concurrent program; Confluence analysis; Deadlock freedom; Hybrid type; Local reasoning; Mobile process; Non-trivial programs; Parameterized; Type systems; Model checking
Verifying safety properties of concurrent heap-manipulating programs,2010,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77952929313&doi=10.1145%2f1745312.1745315&partnerID=40&md5=7a990f3627cb2447996336a4c2eb6a96,"We provide a parametric framework for verifying safety properties of concurrent heap-manipulating programs. The framework combines thread-scheduling information with information about the shape of the heap. This leads to verification algorithms that are more precise than existing techniques. The framework also provides a precise shape-analysis algorithm for concurrent programs. In contrast to most existing verification techniques, we do not put a bound on the number of allocated objects. The framework produces interesting results even when analyzing programs with an unbounded number of threads. The framework is applied to successfully verify the following properties of a concurrent program: Concurrent manipulation of linked-list based ADT preserves the ADT datatype invariant. The program does not perform inconsistent updates due to interference. The program does not reach a deadlock. The program does not produce runtime errors due to illegal thread interactions. We also found bugs in erroneous programs violating such properties. A prototype of our framework has been implemented and applied to small, but interesting, example programs. © 2010 ACM.",Abstract interpretation; Concurrency; Java; Safety properties; Shapeanalysis; Verification,Abstracting; Program interpreters; Abstract interpretations; Concurrent program; Data type; Java; Number of threads; Run-time errors; Safety property; Scheduling information; Shape-analysis; Verification algorithms; Verification techniques; Program debugging
Execution suppression: An automated iterative technique for locating memory errors,2010,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77952898210&doi=10.1145%2f1745312.1745314&partnerID=40&md5=1234f79c7d55ff2db0bb466777acaa91,"By studying the behavior of several programs that crash due to memory errors, we observed that locating the errors can be challenging because significant propagation of corrupt memory values can occur prior to the point of the crash. In this article, we present an automated approach for locating memory errors in the presence of memory corruption propagation. Our approach leverages the information revealed by a program crash: when a crash occurs, this reveals a subset of the memory corruption that exists in the execution. By suppressing (nullifying) the effect of this known corruption during execution, the crash is avoided and any remaining (hidden) corruption may then be exposed by subsequent crashes. The newly exposed corruption can then be suppressed in turn. By iterating this process until no further crashes occur, the first point of memory corruption-and the likely root cause of the program failure-can be identified. However, this iterative approach may terminate prematurely, since programs may not crash even when memory corruption is present during execution. To address this, we show how crashes can be exposed in an execution by manipulating the relative ordering of particular variables within memory. By revealing crashes through this variable re-ordering, the effectiveness and applicability of the execution suppression approach can be improved. We describe a set of experiments illustrating the effectiveness of our approach in consistently and precisely identifying the first points of memory corruption in executions that fail due to memory errors. We also discuss a baseline software implementation of execution suppression that incurs an average overhead of 7.2x, and describe how to reduce this overhead to 1.8x through hardware support. © 2010 ACM.",Execution suppression; Fault localization; Hardware support; Memory corruption propagation; Memory errors; Variable re-ordering,Errors; Automated approach; Fault localization; Hardware supports; Iterative approach; Iterative technique; Memory corruption; Memory error; Root cause; Software implementation; Crime
Santa Claus: Formal analysis of a process-oriented solution,2010,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77951589594&doi=10.1145%2f1734206.1734211&partnerID=40&md5=e49381646a8bee8a8c7c4efbba5dcb3f,"With the commercial development of multicore processors, the challenges of writing multithreaded programs to take advantage of these new hardware architectures are becoming more and more pertinent. Concurrent programming is necessary to achieve the performance that the hardware offers. Traditional approaches present concurrency as an advanced topic: they have proven difficult to use, reason about with confidence, and scale up to high levels of concurrency. This article reviews process-oriented design, based on Hoare's algebra of Communicating Sequential Processes (CSP), and proposes that this approach to concurrency leads to solutions that are manageable by novice programmers; that is, they are easy to design and maintain, that they are scalable for complexity, obviously correct, and relatively easy to verify using formal reasoning and/or model checkers. These solutions can be developed in conventional programming languages (through CSP libraries) or specialized ones (such as occam-π) in a manner that directly reflects their formal expression. Systems can be developed without needing specialist knowledge of the CSP formalism, since the supporting mathematics is burnt into the tools and languages supporting it. We illustrate these concepts with the Santa Claus problem, which has been used as a challenge for concurrency mechanisms since 1994. We consider this problem as an example control system, producing external signals reporting changes of internal state (that model the external world). We claim our occam-π solution is correct-by-design, but follow this up with formal verification (using the FDR model checker for CSP) that the system is free from deadlock and livelock, that the produced control signals obey crucial ordering constraints, and that the system has key liveness properties. © 2010 ACM.",Concurrency; CSP; Deadlock; Event ordering; Liveness; Novice programmer; Occam-pi; Process orientation; Verification,Computer architecture; Computer operating procedures; Computer programming; Design; Linguistics; Multitasking; Problem oriented languages; Commercial development; Communicating sequential process; Concurrent programming; Control signal; Deadlock; External signals; Formal analysis; Formal expressions; Formal reasoning; Formal verifications; Hardware architecture; Internal state; Liveness; Liveness properties; Model checker; Multi-core processor; Multi-threaded programs; Novice programmer; Ordering constraints; Process orientation; Process-oriented; Programming language; Scale-up; Model checking
La dolce vita at TOPLAS,2010,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77951435694&doi=10.1145%2f1734206.1734207&partnerID=40&md5=97e6da347370dfeb89a5ae8940b4d4e2,[No abstract available],,
An optimization framework for embedded processors with auto-addressing mode,2010,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77951481231&doi=10.1145%2f1734206.1734208&partnerID=40&md5=0b08c587348aeb34b907453bf1749b00,"Modern embedded processors with dedicated address generation unit support memory accesses through auto-increment/decrement addressing mode. The auto-increment/decrement mode, if properly utilized, can save address arithmetic instructions, reduce static and dynamic memory footprint of the program, and speed up the execution as well. Liao [1995, 1996] categorized this problem as Simple Offset Assignment (SOA) and General Offset Assignment (GOA), which involves storage layout of variables and assignment of address registers, respectively, proposing several heuristic solutions. This article proposes a new direction for investigating the solution space of the problem. The general idea [Zhuang 2003] is to perform simplification of the underlying access graph through coalescence of the memory locations of program variables. A comprehensive framework is proposed including coalescence-based offset assignment and post/pre-optimization. Variables not interfering with others (not simultaneously live at any program point) can be coalesced into the same memory location. Coalescing allows simplifications of the access graph yielding better SOA solutions; it also reduces the address register pressure to such low values that some GOA solutions become optimal. Moreover, it can reduce the memory footprint both statically and at runtime for stack variables. Our second optimization (post/pre-optimization) considers both post- and pre-modification mode for optimizing code across basic blocks, which makes it useful. Making use of both addressing modes further reduces SOA/GOA cost and our post/pre-optimization phase is optimal in selecting post or pre mode after variable offsets have been determined. We have shown the advantages of our framework over previous approaches to capture more opportunities to reduce both stack size and SOA/GOA cost, leading to more speedup. © 2010 ACM.",Auto-modification addressing mode; Digital signal processing; GOA; Layout assignment; Offset assignment; SOA; Variable coalescence,Coalescence; Cost reduction; Digital signal processing; Flocculation; Location; Signal processing; Address generation units; Arithmetic instructions; Auto-modification; Auto-modification addressing mode; Basic blocks; Embedded processors; Heuristic solutions; Memory access; Memory footprint; Memory locations; New directions; Optimization framework; Program points; Program variables; Register pressure; Runtimes; Second optimization; Simple offset assignments; Solution space; Speed-ups; Static and dynamic; Storage layouts; Optimization
Scratchpad allocation for concurrent embedded software,2010,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77951490831&doi=10.1145%2f1734206.1734210&partnerID=40&md5=fd7798830f2043923178b1b9276f1f5e,"Software-controlled scratchpad memory is increasingly employed in embedded systems as it offers better timing predictability compared to caches. Previous scratchpad allocation algorithms typically consider single-process applications. But embedded applications are mostly multitasking with real-time constraints, where the scratchpad memory space has to be shared among interacting processes that may preempt each other. In this work, we develop a novel dynamic scratchpad allocation technique that takes these process interferences into account to improve the performance and predictability of the memory system. We model the application as a Message Sequence Chart (MSC) to best capture the interprocess interactions. Our goal is to optimize the Worst-Case Response Time (WCRT) of the application through runtime reloading of the scratchpad memory content at appropriate execution points. We propose an iterative allocation algorithm that consists of two critical steps: (1) analyze the MSC along with the existing allocation to determine potential interference patterns, and (2) exploit this interference information to tune the scratchpad reloading points and content so as to best improve the WCRT. We present various alternative scratchpad allocation heuristics and evaluate their effectiveness in reducing the WCRT. The scheme is also extended to work on Message Sequence Graph models. We evaluate our memory allocation scheme on two real-world embedded applications controlling an Unmanned Aerial Vehicle (UAV) and an in-orbit monitoring instrument, respectively. © 2010 ACM.",Compiler controlled memories; Message sequence chart; Multicore architectures; Scratchpad memory; UML sequence diagram; Worst-case response time,Embedded systems; Flowcharting; Program compilers; Remotely operated vehicles; Response time (computer systems); Software architecture; Unmanned aerial vehicles (UAV); Message Sequence Charts; Multicore architectures; Scratch pad memory; UML sequence diagrams; Worst case response time; Embedded software
"Nomadic pict: Programming languages, communication infrastructure overlays, and semantics for mobile computation",2010,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77951492292&doi=10.1145%2f1734206.1734209&partnerID=40&md5=dc2468784ff1589c653ceb8536b5b59f,"Mobile computation, in which executing computations can move from one physical computing device to another, is a recurring theme: from OS process migration, to language-level mobility, to virtual machine migration. This article reports on the design, implementation, and verification of overlay networks to support reliable communication between migrating computations, in the Nomadic Pict project. We define two levels of abstraction as calculi with precise semantics: a low-level Nomadic π calculus with migration and location-dependent communication, and a high-level calculus that adds location-independent communication. Implementations of location-independent communication, as overlay networks that track migrations and forward messages, can be expressed as translations of the high-level calculus into the low. We discuss the design space of such overlay network algorithms and define three precisely, as such translations. Based on the calculi, we design and implement the Nomadic Pict distributed programming language, to let such algorithms (and simple applications above them) to be quickly prototyped. We go on to develop the semantic theory of the Nomadic π calculi, proving correctness of one example overlay network. This requires novel equivalences and congruence results that take migration into account, and reasoning principles for agents that are temporarily immobile (e.g., waiting on a lock elsewhere in the system). The whole stands as a demonstration of the use of principled semantics to address challenging system design problems. © 2010 ACM.",,Biomineralization; Calculations; Communication; Computer software; Design; Linguistics; Overlay networks; Pathology; Query languages; Semantics; Translation (languages); Communication infrastructure; Design spaces; Distributed programming languages; Levels of abstraction; Mobile computations; Physical computing; Process migration; Programming language; Reasoning principles; Reliable communication; Semantic theories; System design; Virtual machines; Wireless networks
Detecting bugs in register allocation,2010,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77951435695&doi=10.1145%2f1734206.1734212&partnerID=40&md5=1accf7c0494b49cd905592578835b9ea,"Although register allocation is critical for performance, the implementation of register allocation algorithms is difficult, due to the complexity of the algorithms and target machine architectures. It is particularly difficult to detect register allocation errors if the output code runs to completion, as bugs in the register allocator can cause the compiler to produce incorrect output code. The output code may even execute properly on some test data, but errors can remain. In this article, we propose novel data flow analyses to statically check that the value flow of the output code from the register allocator is the same as the value flow of its input code. The approach is accurate, fast, and can identify and report error locations and types. It is independent of the register allocator and uses only the input and output code of the register allocator. It can be used with different register allocators, including those that perform coalescing and rematerialization. The article describes our approach, called SARAC, and a tool that statically checks a register allocation and reports the errors and their types that it finds. The tool has an average compile-time overhead of only 8% and a modest average memory overhead of 85KB. Our techniques can be used by compiler developers during regression testing and as a command-line-enabled debugging pass for mysterious compiler behavior. © 2010 ACM.",Register allocation,Data flow analysis; Errors; Flocculation; Software testing; Allocators; Compile time; Detecting bugs; Error location; Input and outputs; Memory overheads; Register allocation; Regression testing; Rematerialization; Target machines; Test data; Value flow; Program compilers
Semantic foundations for typed assembly languages,2010,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77949560721&doi=10.1145%2f1709093.1709094&partnerID=40&md5=aebfa9c3e893511f55cc215f700aad32,"Typed Assembly Languages (TALs) are used to validate the safety of machine-language programs. The Foundational Proof-Carrying Code project seeks to verify the soundness of TALs using the smallest possible set of axioms: the axioms of a suitably expressive logic plus a specification of machine semantics. This article proposes general semantic foundations that permit modular proofs of the soundness of TALs. These semantic foundations include Typed Machine Language (TML), a type theory for specifying properties of low-level data with powerful and orthogonal type constructors, and Lc, a compositional logic for specifying properties of machine instructions with simplified reasoning about unstructured control flow. Both of these components, whose semantics we specify using higher-order logic, are useful for proving the soundness of TALs. We demonstrate this by using TML and Lc to verify the soundness of a low-level, typed assembly language, LTAL, which is the target of our core-ML-to-sparc compiler. To prove the soundness of the TML type system we have successfully applied a new approach, that of step-indexed logical relations. This approach provides the first semantic model for a type system with updatable references to values of impredicative quantified types. Both impredicative polymorphism and mutable references are essential when representing function closures in compilers with typed closure conversion, or when compiling objects to simpler typed primitives. © 2010 ACM.",Control flow; Logical relations; Proof-carrying code; Semantic models; Typed assembly languages,Linguistics; Program compilers; Query languages; Closure conversion; Compositional logic; Control flows; Foundational proof-carrying codes; Higher order logic; Logical relations; Machine instructions; Machine languages; New approaches; Proof-carrying code; Semantic foundation; Semantic Model; Type systems; Type theory; Typed assembly language; Semantics
A termination analyzer for Java bytecode based on path-length,2010,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77949565529&doi=10.1145%2f1709093.1709095&partnerID=40&md5=c76b0cb4a1efd512980d441a67d031e2,"It is important to prove that supposedly terminating programs actually terminate, particularly if those programs must be run on critical systems or downloaded into a client such as a mobile phone. Although termination of computer programs is generally undecidable, it is possible and useful to prove termination of a large, nontrivial subset of the terminating programs. In this article, we present our termination analyzer for sequential Java bytecode, based on a program property called path-length. We describe the analyses which are needed before the path-length can be computed such as sharing, cyclicity, and aliasing. Then we formally define the path-length analysis and prove it correct with respect to a reference denotational semantics of the bytecode. We show that a constraint logic program PCLP can be built from the result of the path-length analysis of a Java bytecode program P and formally prove that if PCLP terminates, then P also terminates. Hence a termination prover for constraint logic programs can be applied to prove the termination of P. We conclude with some discussion of the possibilities and limitations of our approach. Ours is the first existing termination analyzer for Java bytecode dealing with any kind of data structures dynamically allocated on the heap and which does not require any help or annotation on the part of the user. © 2010 ACM.",Abstract interpretation; Java; Java bytecode; Termination analysis,Abstracting; Data structures; Logic programming; Program interpreters; Software agents; Telecommunication equipment; Telephone systems; Abstract interpretations; Aliasing; Bytecodes; Computer program; Constraint logic programs; Critical systems; Cyclicity; Denotational semantics; Java byte codes; Path length; Program properties; Termination analysis; Java programming language
Satin: A high-level and efficient grid programming model,2010,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77949562027&doi=10.1145%2f1709093.1709096&partnerID=40&md5=7e6a241d4add3f36ff6ebe4937bcb660,"Computational grids have an enormous potential to provide compute power. However, this power remains largely unexploited today for most applications, except trivially parallel programs. Developing parallel grid applications simply is too difficult. Grids introduce several problems not encountered before, mainly due to the highly heterogeneous and dynamic computing and networking environment. Furthermore, failures occur frequently, and resources may be claimed by higher-priority jobs at any time. In this article, we solve these problems for an important class of applications: divide-and-conquer. We introduce a system called Satin that simplifies the development of parallel grid applications by providing a rich high-level programming model that completely hides communication. All grid issues are transparently handled in the runtime system, not by the programmer. Satin's programming model is based on Java, features spawn-sync primitives and shared objects, and uses asynchronous exceptions and an abort mechanism to support speculative parallelism. To allow an efficient implementation, Satin consistently exploits the idea that grids are hierarchically structured. Dynamic load-balancing is done with a novel cluster-aware scheduling algorithm that hides the long wide-area latencies by overlapping them with useful local work. Satin's shared object model lets the application define the consistency model it needs. If an application needs only loose consistency, it does not have to pay high performance penalties for wide-area communication and synchronization. We demonstrate how grid problems such as resource changes and failures can be handled transparently and efficiently. Finally, we show that adaptivity is important in grids. Satin can increase performance considerably by adding and removing compute resources automatically, based on the application's requirements and the utilization of the machines and networks in the grid. Using an extensive evaluation on real grids with up to 960 cores, we demonstrate that it is possible to provide a simple high-level programming model for divide-and-conquer applications, while achieving excellent performance on grids. At the same time, we show that the divide-and-conquer model scales better on large systems than the master-worker approach, since it has no single central bottleneck. © 2010 ACM.",Divide-and-conquer; Grid computing; Programming model,Computational efficiency; Data flow analysis; Grid computing; Java programming language; Multitasking; Network management; Parallel architectures; Parallel programming; Problem solving; Scheduling algorithms; Silk; Adaptivity; Computational grids; Compute resources; Consistency model; Divide-and-conquer; Dynamic load balancing; Efficient implementation; Excellent performance; Grid problems; Grid programming models; High-level programming models; Large system; Model scale; Networking environment; Parallel grids; Parallel program; Performance penalties; Programming models; Runtime systems; Shared objects; Wide area; Wide-area communication; Computer systems programming
Hybrid type checking,2010,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-76749109920&doi=10.1145%2f1667048.1667051&partnerID=40&md5=df908086430ba19c921aecc721cfa304,"Traditional static type systems are effective for verifying basic interface specifications. Dynamically checked contracts support more precise specifications, but these are not checked until runtime, resulting in incomplete detection of defects. Hybrid type checking is a synthesis of these two approaches that enforces precise interface specifications, via static analysis where possible, but also via dynamic checks where necessary. This article explores the key ideas and implications of hybrid type checking, in the context of the -calculus extended with contract types, that is, with dependent function types and with arbitrary refinements of base types. © 2010 ACM.",Contracts; Dynamic checking; Static checking; Type systems,Specifications; Dependent functions; Detection of defects; Dynamic checking; Hybrid type; Interface specification; Runtimes; Static checking; Static type systems; Type systems; Object oriented programming
A relational approach to interprocedural shape analysis,2010,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-76749153897&doi=10.1145%2f1667048.1667050&partnerID=40&md5=3e35799060db27d44befe28c5a8f7f29,"This article addresses the verification of properties of imperative programs with recursive procedure calls, heap-allocated storage, and destructive updating of pointer-valued fields, that is, interprocedural shape analysis. The article makes three contributions. It introduces a new method for abstracting relations over memory configurations for use in abstract interpretation. It shows how this method furnishes the elements needed for a compositional approach to shape analysis. In particular, abstracted relations are used to represent the shape transformation performed by a sequence of operations, and an overapproximation to relational composition can be performed using the meet operation of the domain of abstracted relations. It applies these ideas in a new algorithm for context-sensitive interprocedural shape analysis. The algorithm creates procedure summaries using abstracted relations over memory configurations, and the meet-based composition operation provides a way to apply the summary transformer for a procedure P at each call site from which P is called. The algorithm has been applied successfully to establish properties of both (i) recursive programs that manipulate lists and (ii) recursive programs that manipulate binary trees. © 2010 ACM.",3-valued logic; Abstract interpretation; Context-sensitive analysis; Destructive updating; Interprocedural dataflow analysis; Pointer analysis; Shape analysis; Static analysis,Abstracting; Binary trees; Program interpreters; Static analysis; 3-valued logic; Abstract interpretations; Context-sensitive analysis; Interprocedural dataflow analysis; Pointer analysis; Shape analysis; Data flow analysis
JavaCOP: Declarative pluggable types for java,2010,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-76749167110&doi=10.1145%2f1667048.1667049&partnerID=40&md5=bc048ff3e6399403839a160316c3e1d3,"Pluggable types enable users to enforce multiple type systems in one programming language. We have developed a suite of tools, called the JavaCOP framework, that allows developers to create pluggable type systems for Java. JavaCOP provides a simple declarative language in which program constraints are defined over a program's abstract syntax tree. The JavaCOP compiler automatically enforces these constraints on programs during compilation. The JavaCOP framework also includes a dataflow analysis API in order to support type systems which depend on flow-sensitive information. Finally, JavaCOP includes a novel test framework which helps users gain confidence in the correctness of their pluggable type systems. We demonstrate the framework by discussing a number of pluggable type systems which have been implemented in JavaCOP in order to detect errors and enforce strong invariants in programs. These type systems range from general-purpose checkers, such as a type system for nonnull references, to domain-specific ones, such as a checker for conformance to a library's usage rules. © 2010 ACM.",JavaCOP; Pluggable type systems,Application programming interfaces (API); Computer aided software engineering; Data flow analysis; Linguistics; XML; Abstract Syntax Trees; Declarative Languages; Domain specific; On flow; Program constraints; Programming language; Sensitive informations; Support types; Test framework; Type systems; Security of data
Dependence clusters in source code,2009,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-72349083398&doi=10.1145%2f1596527.1596528&partnerID=40&md5=f24cbcea5611196cf6e3905b73628ba0,"A dependence cluster is a set of program statements, all of which are mutually inter-dependent. This article reports a large scale empirical study of dependence clusters in C program source code. The study reveals that large dependence clusters are surprisingly commonplace. Most of the 45 programs studied have clusters of dependence that consume more than 10% of the whole program. Some even have clusters consuming 80% or more. The widespread existence of clusters has implications for source code analyses such as program comprehension, software maintenance, software testing, reverse engineering, reuse, and parallelization. © 2009 ACM.",Dependence; Program comprehension; Program slicing,Computer programming; Computer software maintenance; Computer software selection and evaluation; Reengineering; Reverse engineering; Software testing; C programs; Empirical studies; Parallelizations; Program comprehension; Program slicing; Program statements; Software maintenance; Source code analysis; Source codes; Computer software reusability
Parametric polymorphism for XML,2009,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-75649112624&doi=10.1145%2f1596527.1596529&partnerID=40&md5=c202b80a30279533027e92bb8bd5c4e3,"Despite the extensiveness of recent investigations on static typing for XML, parametric polymorphism has rarely been treated. This well-established typing discipline can also be useful in XML processing in particular for programs involving parametric schemas, that is, schemas parameterized over other schemas (e.g., SOAP). The difficulty in treating polymorphism for XML lies in how to extend the semantic approach used in the mainstream (monomorphic) XML type systems. A naive extension would be semantic quantification over all substitutions for type variables. However, this approach reduces to an NEXPTIME-complete problem for which no practical algorithm is known and induces a subtyping relation that may not always match the programmer's intuition. In this article, we propose a different method that smoothly extends the semantic approach yet is algorithmically easier. The key idea here is to devise a novel and simple marking technique, where we interpret a polymorphic type as a set of values with annotations of which subparts are parameterized. We exploit this interpretation in every ingredient of our polymorphic type system such as subtyping, inference of type arguments, etc. As a result, we achieve a sensible system that directly represents a usual expected behavior of polymorphic type systemsvalues of abstract types are never reconstructedin a reminiscence of Reynold's parametricity theory. Also, we obtain a set of practical algorithms for typechecking by local modifications to existing ones for a monomorphic system. © 2009 ACM.",Polymorphism; Subtyping; Tree automata; XML,Polymorphism; Program interpreters; Semantics; Translation (languages); XML; Abstract types; Complete problems; Parameterized; Parametric polymorphism; Parametricity; Polymorphic type systems; Polymorphic types; Practical algorithms; Schemas; Semantic approach; Static typing; Subtyping relation; Subtypings; Tree automata; Type arguments; Type variables; Typechecking; XML processing; XML types; Markup languages
An experimental analysis of self-adjusting computation,2009,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-75649151689&doi=10.1145%2f1596527.1596530&partnerID=40&md5=b4edcd94ae4cbc2c2ae4ee3b9ed8bdf2,"Recent work on adaptive functional programming (AFP) developed techniques for writing programs that can respond to modifications to their data by performing change propagation. To achieve this, executions of programs are represented with dynamic dependence graphs (DDGs) that record data dependences and control dependences in a way that a change-propagation algorithm can update the computation as if the program were from scratch, by re-executing only the parts of the computation affected by the changes. Since change-propagation only re-executes parts of the computation, it can respond to certain incremental modifications asymptotically faster than recomputing from scratch, potentially offering significant speedups. Such asymptotic speedups, however, are rare: for many computations and modifications, change propagation is no faster than recomputing from scratch. In this article, we realize a duality between dynamic dependence graphs and memoization, and combine them to give a change-propagation algorithm that can dramatically increase computation reuse. The key idea is to use DDGs to identify and re-execute the parts of the computation that are affected by modifications, while using memoization to identify the parts of the computation that remain unaffected by the changes. We refer to this approach as self-adjusting computation. Since DDGs are imperative, but (traditional) memoization requires purely functional computation, reusing computation correctly via memoization becomes a challenge. We overcome this challenge with a technique for remembering and reusing not just the results of function calls (as in conventional memoization), but their executions represented with DDGs. We show that the proposed approach is realistic by describing a library for self-adjusting computation, presenting efficient algorithms for realizing the library, and describing and evaluating an implementation. Our experimental evaluation with a variety of applications, ranging from simple list primitives to more sophisticated computational geometry algorithms, shows that the approach is effective in practice: compared to recomputing from-scratch; self-adjusting programs respond to small modifications to their data orders of magnitude faster. © 2009 ACM.",Computational geometry; Dynamic algorithms; Dynamic dependence graphs; Memoization; Performance; Self-adjusting computation,Algorithms; Asymptotic analysis; Computation theory; Computational geometry; Functional programming; Dependence graphs; Dynamic algorithm; Dynamic algorithms; Memoization; Self-adjusting; Self-adjusting computation; Computational efficiency
From datalog rules to efficient programs with time and space guarantees,2009,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-70349729888&doi=10.1145%2f1552309.1552311&partnerID=40&md5=0916630868260253de2888c27da69e8e,"This article describes a method for transforming any given set of Datalog rules into an efficient specialized implementation with guaranteed worst-case time and space complexities, and for computing the complexities from the rules. The running time is optimal in the sense that only useful combinations of facts that lead to all hypotheses of a rule being simultaneously true are considered, and each such combination is considered exactly once in constant time. The associated space usage may sometimes be reduced using scheduling optimizations to eliminate some summands in the space usage formula. The transformation is based on a general method for algorithm design that exploits fixed-point computation, incremental maintenance of invariants, and combinations of indexed and linked data structures. We apply the method to a number of analysis problems, some with improved algorithm complexities and all with greatly improved algorithm understanding and greatly simplified complexity analysis. © 2009 ACM.",Complexity analysis; Data structure design; Datalog; Incremental computation; Indexed representations; Indexing; Linked representations; Optimization; Program transformation; Recursion; Tabling,Data structures; Indexing (of information); Optimization; Systems analysis; Complexity analysis; Data structure design; Datalog; Incremental computation; Indexed representations; Indexing; Linked representations; Program transformation; Recursion; Tabling; Computational efficiency
Self-stabilization preserving compiler,2009,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-70349745360&doi=10.1145%2f1552309.1552312&partnerID=40&md5=436d67282687e9841f869405a0efb9ce,"Self-stabilization is an elegant approach for designing fault tolerant systems. A system is considered self-stabilizing if, starting in any state, it converges to the desired behavior. Self-stabilizing algorithms were designed for solving fundamental distributed tasks, such as leader election, token circulation and communication network protocols. The algorithms were expressed using guarded commands or pseudo-code. The realization of these algorithms requires the existence of a (self-stabilizing) infrastructure such as a self-stabilizing microprocessor and a self-stabilizing operating system for their execution. Moreover, the high-level description of the algorithms needs to be converted into machine language of the microprocessor. In this article, we present our design for a self-stabilization preserving compiler. The compiler we designed and implemented transforms programs written in a language similar to the abstract state machine (ASM). The compiler preserves the stabilization property of the high level program. © 2009 ACM.",Abstract state machines; Compilers; Self-stabilization,Abstracting; Algorithms; Computer operating systems; Contour followers; Fault tolerant computer systems; High level languages; Linguistics; Microprocessor chips; Network protocols; Stabilization; Abstract state machines; Communication networks; Compilers; Distributed tasks; Fault tolerant systems; Guarded commands; High level description; High-level program; Leader election; Machine languages; Operating systems; Pseudo-code; Self-stabilization; Self-stabilizing algorithm; Stabilization property; Token circulation; Program compilers
Program locality analysis using reuse distance,2009,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-70349743894&doi=10.1145%2f1552309.1552310&partnerID=40&md5=dd405370b275e8004dad314b23fa55ef,"On modern computer systems, the memory performance of an application depends on its locality. For a single execution, locality-correlated measures like average miss rate or working-set size have long been analyzed using reuse distancethe number of distinct locations accessed between consecutive accesses to a given location. This article addresses the analysis problem at the program level, where the size of data and the locality of execution may change significantly depending on the input. The article presents two techniques that predict how the locality of a program changes with its input. The first is approximate reuse-distance measurement, which is asymptotically faster than exact methods while providing a guaranteed precision. The second is statistical prediction of locality in all executions of a program based on the analysis of a few executions. The prediction process has three steps: dividing data accesses into groups, finding the access patterns in each group, and building parameterized models. The resulting prediction may be used on-line with the help of distance-based sampling. When evaluated on fifteen benchmark applications, the new techniques predicted program locality with good accuracy, even for test executions that are orders of magnitude larger than the training executions. The two techniques are among the first to enable quantitative analysis of whole-program locality in general sequential code. These findings form the basis for a unified understanding of program locality and its many facets. Concluding sections of the article present a taxonomy of related literature along five dimensions of locality and discuss the role of reuse distance in performance modeling, program optimization, cache and virtual memory management, and network traffic analysis. © 2009 ACM.",Program locality; Reuse distance; Stack distance; Training-based analysis,Benchmarking; Cache memory; Internet; Location; Taxonomies; Access patterns; Analysis problems; Benchmark applications; Data access; Distance-based; Exact methods; Memory performance; Miss-rate; Modern computer systems; Network traffic analysis; Orders of magnitude; Parameterized model; Performance Modeling; Prediction process; Program locality; Program optimization; Quantitative analysis; Reuse distance; Stack distance; Statistical prediction; Test execution; Training-based analysis; Virtual memory management; Computer software reusability
Local policies for resource usage analysis,2009,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-70349755573&doi=10.1145%2f1552309.1552313&partnerID=40&md5=859d8a17a627db49e0c115bad8ca0509,"An extension of the -calculus is proposed, to study resource usage analysis and verification. It features usage policies with a possibly nested, local scope, and dynamic creation of resources. We define a type and effect system that, given a program, extracts a history expression, that is, a sound overapproximation to the set of histories obtainable at runtime. After a suitable transformation, history expressions are model-checked for validity. A program is resource-safe if its history expression is verified valid: If such, no runtime monitor is needed to safely drive its executions. © 2009 ACM.",Model-checking; Type and effect systems; Usage policies,Local policies; Resource usage analysis; Runtime monitors; Runtimes; Type and effect systems; Usage policies; Model checking
Certificate translation for optimizing compilers,2009,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-68549120861&doi=10.1145%2f1538917.1538919&partnerID=40&md5=770dbbb4322ec1a308f5896f2e7cbfe8,"Proof Carrying Code provides trust in mobile code by requiring certificates that ensure the code adherence to specific conditions. The prominent approach to generate certificates for compiled code is Certifying Compilation, that automatically generates certificates for simple safety properties. In this work, we present Certificate Translation, a novel extension for standard compilers that automatically transforms formal proofs for more expressive and complex properties of the source program to certificates for the compiled code. The article outlines the principles of certificate translation, instantiated for a nonoptimizing compiler and for standard compiler optimizations in the context of an intermediate RTL Language. © 2009 ACM.",Program optimizations; Program verification; Proof-carrying code; Static analysis,Formal logic; Program translators; Static analysis; Certifying compilation; Compiler optimizations; Complex properties; Formal proofs; Mobile codes; Optimizing compilers; Program optimizations; Program verification; Proof-carrying code; Safety property; Program compilers
Mostly static program partitioning of binary executables,2009,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-68549130616&doi=10.1145%2f1538917.1538918&partnerID=40&md5=68990e77edcabb282cf2b7ab4a3b5c29,"We have built a runtime compilation system that takes unmodified sequential binaries and improves their performance on off-the-shelf multiprocessors using dynamic vectorization and loop-level parallelization techniques. Our system, Azure, is purely software based and requires no specific hardware support for speculative thread execution, yet it is able to break even in most cases; that is, the achieved speedup exceeds the cost of runtime monitoring and compilation, often by significant amounts. Key to this remarkable performance is an offline preprocessing step that extracts a mostly correct control flow graph (CFG) from the binary program ahead of time. This statically obtained CFG is incomplete in that it may be missing some edges corresponding to computed branches. We describe how such additional control flow edges are discovered and handled at runtime, so that an incomplete static analysis never leads to an incorrect optimization result. The availability of a mostly correct CFG enables us to statically partition a binary executable into single-entry multiple-exit regions and to identify potential parallelization candidates ahead of execution. Program regions that are not candidates for parallelization can thereby be excluded completely from runtime monitoring and dynamic recompilation. Azure's extremely low overhead is a direct consequence of this design. © 2009 ACM.",Binary translation; Continuous compilation and optimization; Dynamic parallelization,Binary codes; Network components; Additional control; Binary programs; Binary translation; Continuous compilation and optimization; Control flow graphs; Dynamic vectorization; Executables; Low overhead; Offline; Parallelizations; Pre-processing step; Recompilation; Runtime Monitoring; Runtimes; Software-based; Specific hardware; Static program; Optimization
A theory of contracts for web services,2009,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-68549111018&doi=10.1145%2f1538917.1538920&partnerID=40&md5=c02fb8d5f69a8cdd36dee0e2a514a31d,"Contracts are behavioral descriptions of Web services. We devise a theory of contracts that formalizes the compatibility of a client with a service, and the safe replacement of a service with another service. The use of contracts statically ensures the successful completion of every possible interaction between compatible clients and services. The technical device that underlies the theory is the filter, which is an explicit coercion preventing some possible behaviors of services and, in doing so, make services compatible with different usage scenarios. We show that filters can be seen as proofs of a sound and complete subcontracting deduction system which simultaneously refines and extends Hennessy's classical axiomatization of the must testing preorder. The relation is decidable, and the decision algorithm is obtained via a cut-elimination process that proves the coherence of subcontracting as a logical system. Despite the richness of the technical development, the resulting approach is based on simple ideas and basic intuitions. Remarkably, its application is mostly independent of the language used to program the services or the clients. We outline the practical aspects of our theory by studying two different concrete syntaxes for contracts and applying each of them to Web services languages. We also explore implementation issues of filters and discuss the perspectives of future research this work opens. © 2009 ACM.",Ccs; Concurrency theory; Contracts; Explicit coercions; Must testing; Subtyping; Type theory; Web services,Linguistics; Query languages; Ccs; Concurrency theory; Explicit coercions; Must testing; Subtyping; Type theory; Web services
Remembrances of things past,2009,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-69349101902&doi=10.1145%2f1538824.1538825&partnerID=40&md5=70f51509a901592a45d682fc4fa44ee0,[No abstract available],,
Operational semantics for multi-language programs,2009,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-70349898704&doi=10.1145%2f1498926.1498930&partnerID=40&md5=776372c7feb124442ef272465c97869e,"Interoperability is big business, a fact to which .NET, the JVM, and COM can attest. Language designers are well aware of this, and they are designing programming languages that reflect itfor instance, SML.NET, F, Mondrian, and Scala all treat interoperability as a central design feature. Still, current multi-language research tends not to focus on the semantics of these features, but only on how to implement them efficiently. In this article, we attempt to rectify that by giving a technique for specifying the operational semantics of a multi-language system as a composition of the models of its constituent languages. Our technique abstracts away the low-level details of interoperability like garbage collection and representation coherence, and lets us focus on semantic properties like type-safety, equivalence, and termination behavior. In doing so it allows us to adapt standard theoretical techniques such as subject-reduction, logical relations, and operational equivalence for use on multi-language systems. Generally speaking, our proofs of properties in a multi-language context are mutually referential versions of their single language counterparts. We demonstrate our technique with a series of strategies for embedding a Scheme-like language into an ML-like language. We start by connecting very simple languages with a very simple strategy, and work our way up to languages that interact in sophisticated ways and have sophisticated features such as polymorphism and effects. Along the way, we prove relevant results such as type-soundness and termination for each system we present using adaptations of standard techniques. Beyond giving simple expressive models, our studies have uncovered several interesting facts about interoperability. For example, higher-order function contracts naturally emerge as the glue to ensure that interoperating languages respect each other's type systems. Our models also predict that the embedding strategy where foreign values are opaque is as expressive as the embedding strategy where foreign values are translated to corresponding values in the other language, and we were able to experimentally verify this behavior using PLT Scheme's foreign function interface. © 2009 ACM.",Interoperability; Operational semantics,Computational linguistics; Interoperability; Linguistics; Refuse collection; Semantics; Waste disposal; Design features; Foreign function interface; Garbage collection; Higher-order functions; Logical relations; ML-like languages; Mondrian; Operational semantics; Programming language; Semantic properties; Subject-reduction; Theoretical technique; Type systems; Query languages
Program transformations using temporal logic side conditions,2009,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-69149107998&doi=10.1145%2f1516507.1516509&partnerID=40&md5=32a39ed07ea8ef44b74e44ad0204b160,"This article describes an approach to program optimization based on transformations, where temporal logic is used to specify side conditions, and strategies are created which expand the repertoire of transformations and provide a suitable level of abstraction. We demonstrate the power of this approach by developing a set of optimizations using our transformation language and showing how the transformations can be converted into a form which makes it easier to apply them, while maintaining trust in the resulting optimizing steps. The approach is illustrated through a transformational case study where we apply several optimizations to a small program.",Optimizing compilers; Program transformation; Rewriting; Temporal logic,Optimization; Program compilers; Systems analysis; Level of abstraction; Optimizing compilers; Program optimization; Program transformation; Program transformations; Rewriting; Temporal logic
Ranking functions for size-change termination,2009,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-70349736085&doi=10.1145%2f1498926.1498928&partnerID=40&md5=be8db09f33609e185cff1919ea62d8e8,"This article explains how to construct a ranking function for any program that is proved terminating by size-change analysis. The principle of size-change termination for a first-order functional language with well-ordered data is intuitive: A program terminates on all inputs, if every infinite call sequence (following program control flow) would imply an infinite descent in some data values. Size-change analysis is based on information associated with the subject program's call-sites. This information indicates, for each call-site, strict or weak data decreases observed as a computation traverses the call-site. The set DESC of call-site sequences for which the size-changes imply infinite descent is -regular, as is the set FLOW of infinite call-site sequences following the program flowchart. If FLOW DESC (a decidable problem), every infinite call sequence would imply infinite descent in a well-orderingan impossibilityso the program must terminate. This analysis accounts for termination arguments applicable to different call-site sequences, without indicating a ranking function for the program's termination. In this article, it is explained how one can be constructed whenever size-change analysis succeeds. The constructed function has an unexpectedly simple form; it is expressed using only min, max, and lexicographic tuples of parameters and constants. In principle, such functions can be tested to determine whether size-change analysis will be successful. As a corollary, if a program verified as terminating performs only multiply recursive operations, the function that it computes is multiply recursive. The ranking function construction is connected with the determinization of the Büchi automaton for DESC. While the result is not practical, it is of value in addressing the scope of size-change reasoning. This reasoning has been applied broadly, in the analysis of functional and logic programs, as well as term rewrite systems. © 2009 ACM.",ω-Automaton; Determinization; Multiple recursion; Ranking function; Size-change termination; Termination analysis,Automata theory; Computability and decidability; Logic programming; Robots; Set theory; Translation (languages); Determinization; Multiple recursion; Ranking function; Size-change termination; Termination analysis; Recursive functions
Separation and information hiding,2009,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-70349756867&doi=10.1145%2f1498926.1498929&partnerID=40&md5=cee6e08c21254d16b23fed5b888f37d6,"We investigate proof rules for information hiding, using the formalism of separation logic. In essence, we use the separating conjunction to partition the internal resources of a module from those accessed by the module's clients. The use of a logical connective gives rise to a form of dynamic partitioning, where we track the transfer of ownership of portions of heap storage between program components. It also enables us to enforce separation in the presence of mutable data structures with embedded addresses that may be aliased. © 2009 ACM.",Modularity; Resource protection; Separation logic,Data structures; Aliased; Dynamic partitioning; Information hiding; Internal resources; Logical connectives; Modularity; Program components; Resource protection; Separation logic; Separation
Sequent calculi and abstract machines,2009,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-69149110610&doi=10.1145%2f1516507.1516508&partnerID=40&md5=7c46e2e9e095efe3a5bb4188983791df,"We propose a sequent calculus derived from the μμ̃-calculus of Curien and Herbelin that is expressive enough to directly represent the fine details of program evaluation using typical abstract machines. Not only does the calculus easily encode the usual components of abstract machines such as environments and stacks, but it can also simulate the transition steps of the abstract machine with just a constant overhead. Technically this is achieved by ensuring that reduction in the calculus always happens at a bounded depth from the root of the term. We illustrate these properties by providing shallow encodings of the Krivine (call-by-name) and the CEK (call-by-value) abstract machines in the calculus.",Curry-Howard isomorphism; Duality; Explicit substitutions; Krivine machine; Natural deduction,Abstracting; Biomineralization; Differentiation (calculus); Formal logic; Machine components; Set theory; Curry-Howard isomorphism; Duality; Explicit substitutions; Krivine machine; Natural deduction; Calculations
Term transformers: A new approach to state,2009,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-69149084237&doi=10.1145%2f1516507.1516511&partnerID=40&md5=66fd79372279f69269f3a330779dd3b0,"We present a new approach to adding state and state-changing commands to a term language. As a formal semantics it can be seen as a generalization of predicate transformer semantics, but beyond that it brings additional opportunities for specifying and verifying programs. It is based on a construct called a phrase, which is a term of the form C ▷ t, where C stands for a command and t stands for a term of any type. If R is boolean, C ▷ R is closely related to the weakest precondition wp(C,R). The new theory draws together functional and imperative programming in a simple way. In particular, imperative procedures and functions are seen to be governed by the same laws as classical functions. We get new techniques for reasoning about programs, including the ability to dispense with logical variables and their attendant complexities. The theory covers both programming and specification languages, and supports unbounded demonic and angelic nondeterminacy in both commands and terms.",Nondeterminacy; Nondeterminism; Predicate transformers; Procedures; Refinement calculus; State,Formal methods; Functional programming; Linguistics; Query languages; Specification languages; Nondeterminacy; Nondeterminism; Predicate transformers; Procedures; Refinement calculus; State; Semantics
Deferring design pattern decisions and automating structural pattern changes using a design-pattern-based programming system,2009,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-70349731642&doi=10.1145%2f1498926.1498927&partnerID=40&md5=5cd1f041bbe34e99d3bc4590cbc79f76,"In the design phase of software development, the designer must make many fundamental design decisions concerning the architecture of the system. Incorrect decisions are relatively easy and inexpensive to fix if caught during the design process, but the difficulty and cost rise significantly if problems are not found until after coding begins. Unfortunately, it is not always possible to find incorrect design decisions during the design phase. To reduce the cost of expensive corrections, it would be useful to have the ability to defer some design decisions as long as possible, even into the coding stage. Failing that, tool support for automating design changes would give more freedom to revisit and change these decisions when needed. This article shows how a design-pattern-based programming system based on generative design patterns can support the deferral of design decisions where possible, and automate changes where necessary. A generative design pattern is a parameterized pattern form that is capable of generating code for different versions of the underlying design pattern. We demonstrate these ideas in the context of a parallel application written with the CO2P3S pattern-based parallel programming system. We show that CO2P3S can defer the choice of execution architecture (shared-memory or distributed-memory), and can automate several changes to the application structure that would normally be daunting to tackle late in the development cycle. Although we have done this work with a pattern-based parallel programming system, it can be generalized to other domains. © 2009 ACM.",Design decisions; Design patterns; Object-oriented frameworks; Parallel programming; Software maintenance,Computer software; Computer software maintenance; Object oriented programming; Parallel programming; Software architecture; Application structure; Design change; Design decisions; Design patterns; Design phase; Design process; Development cycle; Distributed memory; Fundamental design; Generative design patterns; Object-oriented frameworks; Parallel application; Parameterized; Programming system; Shared memories; Software development; Software maintenance; Structural pattern; Tool support; Design
On the origins of bisimulation and coinduction,2009,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-69149096212&doi=10.1145%2f1516507.1516510&partnerID=40&md5=3a52ee4ba849b9d7d4e42b4ee5e40b08,"The origins of bisimulation and bisimilarity are examined, in the three fields where they have been independently discovered: Computer Science, Philosophical Logic (precisely, Modal Logic), Set Theory. Bisimulation and bisimilarity are coinductive notions, and as such are intimately related to fixed points, in particular greatest fixed points. Therefore also the appearance of coinduction and fixed points is discussed, though in this case only within Computer Science. The paper ends with some historical remarks on the main fixed-point theorems (such as Knaster-Tarski) that underpin the fixed-point theory presented.",Bisimulation; Coinduction; Fixed points; Greatest fixed points; History,Philosophical aspects; Set theory; Bisimilarity; Bisimulation; Bisimulations; Coinduction; Fixed point theorems; Fixed point theory; Fixed points; Greatest fixed points; Modal logic; Computer science
Expressive and modular predicate dispatch for Java,2009,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-62149120912&doi=10.1145%2f1462166.1462168&partnerID=40&md5=e5211c473c1afcaef688e78dcaf3aecf,"Predicate dispatch is an object-oriented (OO) language mechanism for determining the method implementation to be invoked upon a message send. With predicate dispatch, each method implementation includes a predicate guard specifying the conditions under which the method should be invoked, and logical implication of predicates determines the method overriding relation. Predicate dispatch naturally unifies and generalizes several common forms of dynamic dispatch, including traditional OO dispatch, multimethod dispatch, and functional-style pattern matching. Unfortunately, prior languages supporting predicate dispatch have had several deficiencies that limit the practical utility of this language feature. We describe JPred, a backward-compatible extension to Java supporting predicate dispatch. While prior languages with predicate dispatch have been extensions to toy or nonmainstream languages, we show how predicate dispatch can be naturally added to a traditional OO language. While prior languages with predicate dispatch have required the whole program to be available for typechecking and compilation, JPred retains Java's modular typechecking and compilation strategies. While prior languages with predicate dispatch have included special-purpose algorithms for reasoning about predicates, JPred employs general-purpose, off-the-shelf decision procedures. As a result, JPred's type system is more flexible, allowing several useful programming idioms that are spuriously rejected by those other languages. After describing the JPred language informally, we present an extension to Featherweight Java that formalizes the language and its modular type system, which we have proven sound. Finally, we discuss two case studies that illustrate the practical utility of JPred, including its use in the detection of several errors. © 2009 ACM.",Dynamic dispatch; Modular typechecking; Predicate dispatch,Computer software; Java programming language; Linguistics; Pattern matching; Query languages; Decision procedures; Dynamic dispatch; Featherweight javas; Language features; Logical implications; Modular typechecking; Multimethod dispatches; Object-oriented; Predicate dispatch; Programming idioms; Type systems; Type-checking; Object oriented programming
Revisiting coroutines,2009,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-62149100997&doi=10.1145%2f1462166.1462167&partnerID=40&md5=128368d8b7093760b78f133ff8bd9b7b,"This article advocates the revival of coroutines as a convenient general control abstraction. After proposing a new classification of coroutines, we introduce the concept of full asymmetric coroutines and provide a precise definition for it through an operational semantics. We then demonstrate that full coroutines have an expressive power equivalent to one-shot continuations and one-shot delimited continuations. We also show that full asymmetric coroutines and one-shot delimited continuations have many similarities, and therefore present comparable benefits. Nevertheless, coroutines are easier implemented and understood, especially in the realm of procedural languages. © 2009 ACM.",Continuations; Generators; Multitasking,Information theory; Continuations; Control abstractions; Expressive power; Generators; Operational semantics; Procedural languages; Multitasking
Erratum: Efficient constraint propagation engines (ACM Transactions on Programming Languages and Systems 31:1 DOI 10.1145/1452044/1452046),2009,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-62149090981&doi=10.1145%2f1462166.1462170&partnerID=40&md5=b82169c0b28ac3893498287044ca4880,[No abstract available],,
The computational power and complexity of constraint handling rules,2009,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-62149105474&doi=10.1145%2f1462166.1462169&partnerID=40&md5=cee5e6025165b1076b796694c1394be5,"Constraint Handling Rules (CHR) is a high-level rule-based programming language which is increasingly used for general-purpose programming. We introduce the CHR machine, a model of computation based on the operational semantics of CHR. Its computational power and time complexity properties are compared to those of the well-understood Turing machine and Random Access Memory machine. This allows us to prove the interesting result that every algorithm can be implemented in CHR with the best known time and space complexity. We also investigate the practical relevance of this result and the constant factors involved. Finally we expand the scope of the discussion to other (declarative) programming languages. © 2009 ACM.",Complexity; Constant factors; Constraint Handling Rules,Computational linguistics; Computer programming; Computer software; Computers; High level languages; Information theory; Linguistics; Query languages; Random access storage; Turing machines; Complexity; Computational power; Constant factors; Constraint Handling Rules; High-level rules; Model of computations; Operational semantics; Programming languages; Random access memories; Time and spaces; Time complexity; Computational complexity
Decomposing bytecode verification by abstract interpretation,2008,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-57949101914&doi=10.1145%2f1452044.1452047&partnerID=40&md5=c47a3406eb48b5cfc1f1923deca7e902,Bytecode verification is a key point in the security chain of the Java platform. This feature is only optional in many embedded devices since the memory requirements of the verification process are too high. In this article we propose an approach that significantly reduces the use of memory by a serial/parallel decomposition of the verification into multiple specialized passes. The algorithm reduces the type encoding space by operating on different abstractions of the domain of types. The results of our evaluation show that this bytecode verification can be performed directly on small memory systems. The method is formalized in the framework of abstract interpretation. © 2008 ACM.,Abstract interpretation; Bytecode verification,Computer programming; Decomposition; Abstract interpretation; Bytecode verification; Do-mains; Embedded devices; Memory requirements; Memory systems; Security chains; Verification processes; Abstracting
Efficient constraint propagation engines,2008,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-57949102453&doi=10.1145%2f1452044.1452046&partnerID=40&md5=2987c1f7733d7fd1b2b758397675d78f,"This article presents a model and implementation techniques for speeding up constraint propagation. Three fundamental approaches to improving constraint propagation based on propagators as implementations of constraints are explored: keeping track of which propagators are at fixpoint, choosing which propagator to apply next, and how to combine several propagators for the same constraint. We show how idempotence reasoning and events help track fixpoints more accurately. We improve these methods by using them dynamically (taking into account current variable domains to improve accuracy). We define priority-based approaches to choosing a next propagator and show that dynamic priorities can improve propagation. We illustrate that the use of multiple propagators for the same constraint can be advantageous with priorities, and introduce staged propagators that combine the effects of multiple propagators with priorities for greater efficiency. © 2008 ACM.",Constraint (logic) programming; Constraint propagation; Events; Finite domain constraints; Fixpoint reasoning; Priorities,Constraint (logic) programming; Constraint propagation; Events; Finite domain constraints; Fixpoint reasoning; Priorities; Portals
A programming model for concurrent object-oriented programs,2008,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-57849116513&doi=10.1145%2f1452044.1452045&partnerID=40&md5=f73bcc24c217afa19521889b74fab6a6,"Reasoning about multithreaded object-oriented programs is difficult, due to the nonlocal nature of object aliasing and data races. We propose a programming regime (or programming model) that rules out data races, and enables local reasoning in the presence of object aliasing and concurrency. Our programming model builds on the multithreading and synchronization primitives as they are present in current mainstream programming languages. Java or C programs developed according to our model can be annotated by means of stylized comments to make the use of the model explicit. We show that such annotated programs can be formally verified to comply with the programming model. If the annotated program verifies, the underlying Java or C program is guaranteed to be free from data races, and it is sound to reason locally about program behavior. Verification is modular: a program is valid if all methods are valid, and validity of a method does not depend on program elements that are not visible to the method. We have implemented a verifier for programs developed according to our model in a custom build of the Spec programming system, and we have validated our approach on a case study. © 2008 ACM.",Aliasing; Data races; Local reasoning; Modular reasoning; Ownership; Verification condition generation,C (programming language); Computer programming languages; Computer software; Computer software selection and evaluation; Concurrency control; Formal logic; Object oriented programming; Aliasing; Data races; Local reasoning; Modular reasoning; Ownership; Verification condition generation; Java programming language
A probabilistic language based on sampling functions,2008,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-57849134671&doi=10.1145%2f1452044.1452048&partnerID=40&md5=93e63487c3caf100e8997ca7c7ec4874,"As probabilistic computations play an increasing role in solving various problems, researchers have designed probabilistic languages which treat probability distributions as primitive datatypes. Most probabilistic languages, however, focus only on discrete distributions and have limited expressive power. This article presents a probabilistic language, called λ̊, whose expressive power is beyond discrete distributions. Rich expressiveness of λ̊ is due to its use of sampling functions, that is, mappings from the unit interval (0.0, 1.0] to probability domains, in specifying probability distributions. As such, λ̊ enables programmers to formally express and reason about sampling methods developed in simulation theory. The use of λ̊ is demonstrated with three applications in robotics: robot localization, people tracking, and robotic mapping. All experiments have been carried out with real robots. © 2008 ACM.",Probabilistic language; Probability distribution; Robotics; Sampling function,Arsenic; Functions; Image segmentation; Linguistics; Probability; Probability density function; Query languages; Random processes; Robot applications; Robotics; Robots; Datatypes; Discrete distributions; Do-mains; Expressive powers; People tracking; Probabilistic computations; Probabilistic language; Real robots; Robot localizations; Robotic mappings; Sampling function; Sampling methods; Simulation theories; Unit intervals; Probability distributions
Verified interoperable implementations of security protocols,2008,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-57849126802&doi=10.1145%2f1452044.1452049&partnerID=40&md5=7573171e4579cacc14fea151e78d3ae4,"We present an architecture and tools for verifying implementations of security protocols. Our implementations can run with both concrete and symbolic implementations of cryptographic algorithms. The concrete implementation is for production and interoperability testing. The symbolic implementation is for debugging and formal verification. We develop our approach for protocols written in F, a dialect of ML, and verify them by compilation to ProVerif, a resolution-based theorem prover for cryptographic protocols. We establish the correctness of this compilation scheme, and we illustrate our approach with protocols for Web Services security. © 2008 ACM.",Functional programming; Pi calculus; Web services; XML security,Computational methods; Computer programming; Concrete testing; Cryptography; Functional programming; Functions; Interoperability; Markup languages; Network architecture; Security of data; Web services; World Wide Web; XML; Cryptographic algorithms; Cryptographic protocols; Formal verifications; Interoperability testing; Interoperable; Pi calculus; Security protocols; XML security; Internet protocols
Domain specific language implementation via compile-time meta-programming,2008,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-56349147876&doi=10.1145%2f1391956.1391958&partnerID=40&md5=739115d791a83ced010513ac301131b6,"Domain specific languages (DSLs) are mini-languages that are increasingly seen as being a valuable tool for software developers and non-developers alike. DSLs must currently be created in an ad-hoc fashion, often leading to high development costs and implementations of variable quality. In this article, I show how expressive DSLs can be hygienically embedded in the Converge programming language using its compile-time meta-programming facility, the concept of DSL blocks, and specialised error reporting techniques. By making use of pre-existing facilities, and following a simple methodology, DSL implementation costs can be significantly reduced whilst leading to higher quality DSL implementations. © 2008 ACM.",Compile-time meta-programming; Domain specific languages; Syntax extension,Object oriented programming; Problem oriented languages; Compile-time meta-programming; Development costs; Domain specific languages; Implementation cost; Software developer; Syntax Extension; Digital subscriber lines
Perfect hashing as an almost perfect subtype test,2008,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-56349144764&doi=10.1145%2f1391956.1391960&partnerID=40&md5=2f93d5d365bc417a07d7b3a3cf2be2b4,"Subtype tests are an important issue in the implementation of object-oriented programming languages. Many techniques have been proposed, but none of them perfectly fulfills the five requirements that we have identified: constant-time, linear-space, multiple inheritance, dynamic loading and inlining. In this article, we propose a subtyping test implementation that involves a combination of usual hashtables and Cohen's display, which is a well-known technique for single inheritance hierarchies. This novel approach is based on perfect hashing, that is, an optimized and truly constant-time variant of hashing that applies to immutable hashtables. We show that the resulting technique closely meets all five requirements. Furthermore, in the framework of JAVA-like languagescc-characterized by single inheritance of classes and multiple subtyping of interfaces-perfect hashing also applies to method invocation when the receiver is typed by an interface. The proposed technique is compared to some alternatives, including the proposal by Palacz and Vitek [2003]. Time-efficiency is assessed at the cycle level in the framework of Driesen's pseudo-code and the linear-space criterion is validated by statistical simulation on benchmarks consisting of large-scale class hierarchies. © 2008 ACM.",Casting; Coloring; Downcast; Dynamic loading; Interfaces; Method tables; Multiple inheritance; Multiple subtyping; Perfect hashing; Single inheritance; Subtype test; Virtual function tables,Dynamic loads; Testing; Downcast; Dynamic loading; Interfaces; Method tables; Multiple inheritance; Multiple subtyping; Perfect hashing; Single inheritance; Subtype test; Virtual function tables; Object oriented programming
Verifying policy-based web services security,2008,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-56349105161&doi=10.1145%2f1391956.1391957&partnerID=40&md5=0e324acf010a6f60f01afbbcfca3410a,"WS-SecurityPolicy is a declarative language for configuring web services security mechanisms. We describe a formal semantics for WS-SecurityPolicy and propose a more abstract language for specifying secure links between web services and their clients. We present the architecture and implementation of tools that (1) compile policy files from link specifications, and (2) verify by invoking a theorem prover whether a set of policy files run by any number of senders and receivers correctly implements the goals of a link specification, in spite of active attackers. Policy-driven web services implementations are prone to the usual subtle vulnerabilities associated with cryptographic protocols; our tools help prevent such vulnerabilities. We can verify policies when first compiled from link specifications, and also re-verify policies against their original goals after any modifications during deployment. Moreover, we present general security theorems for all configurations that rely on compiled policies. © 2008 ACM.",Pi calculus; Web services; XML security,Calculations; Formal methods; Semantics; Specifications; Telecommunication services; Websites; Abstract languages; Cryptographic protocols; Declarative Languages; Pi calculus; Security theorem; Web services security; WS-SecurityPolicy; XML security; Web services
XARK: An extensible framework for automatic recognition of computational kernels,2008,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-56349135455&doi=10.1145%2f1391956.1391959&partnerID=40&md5=07c19d0481e68be7dffd87e8a06c274e,"The recognition of program constructs that are frequently used by software developers is a powerful mechanism for optimizing and parallelizing compilers to improve the performance of the object code. The development of techniques for automatic recognition of computational kernels such as inductions, reductions and array recurrences has been an intensive research area in the scope of compiler technology during the 90's. This article presents a new compiler framework that, unlike previous techniques that focus on specific and isolated kernels, recognizes a comprehensive collection of computational kernels that appear frequently in full-scale real applications. The XARK compiler operates on top of the Gated Single Assignment (GSA) form of a high-level intermediate representation (IR) of the source code. Recognition is carried out through a demand-driven analysis of this high-level IR at two different levels. First, the dependences between the statements that compose the strongly connected components (SCCs) of the data-dependence graph of the GSA form are analyzed. As a result of this intra-SCC analysis, the computational kernels corresponding to the execution of the statements of the SCCs are recognized. Second, the dependences between statements of different SCCs are examined in order to recognize more complex kernels that result from combining simpler kernels in the same code. Overall, the XARK compiler builds a hierarchical representation of the source code as kernels and dependence relationships between those kernels. This article describes in detail the collection of computational kernels recognized by the XARK compiler. Besides, the internals of the recognition algorithms are presented. The design of the algorithms enables to extend the recognition capabilities of XARK to cope with new kernels, and provides an advanced symbolic analysis framework to run other compiler techniques on demand. Finally, extensive experiments showing the effectiveness of XARK for a collection of benchmarks from different application domains are presented. In particular, the SparsKit-II library for the manipulation of sparse matrices, the Perfect benchmarks, the SPEC CPU2000 collection and the PLTMG package for solving elliptic partial differential equations are analyzed in detail. © 2008 ACM.",Automatic kernel recognition; Demand-driven algorithms; Gated single assignment; Strongly connected component; Symbolic analysis; Use-def chains,Arsenic; Benchmarking; Codes (symbols); Partial differential equations; Automatic kernel recognition; Demand-driven algorithms; Gated single assignment; Strongly connected component; Symbolic analysis; Use-def chains; Program compilers
Dually nondeterministic functions,2008,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-47949090700&doi=10.1145%2f1391956.1391961&partnerID=40&md5=9da9bb695bce771b3f342d484a103400,"Nondeterminacy is a fundamental notion in computing. We show that it can be described by a general theory that accounts for it in the form in which it occurs in many programming contexts, among them specifications, competing agents, data refinement, abstract interpretation, imperative programming, process algebras, and recursion theory. Underpinning these applications is a theory of nondeterministic functions; we construct such a theory. The theory consists of an algebra with which practitioners can reason about nondeterministic functions, and a denotational model to establish the soundness of the theory. The model is based on the idea of free completely distributive lattices over partially ordered sets. We deduce the important properties of nondeterministic functions. © 2008 ACM.",Angelic nondeterminacy; Demonic nondeterminacy; Free completely distributive lattice; Modeling nondeterminacy; Nondeterminism; Nondeterministic functions,Algebra; Probability density function; Program interpreters; Set theory; Angelic nondeterminacy; Demonic nondeterminacy; Free completely distributive lattice; Modeling nondeterminacy; Nondeterminism; Nondeterministic functions; Programming theory
A capability calculus for concurrency and determinism,2008,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-51849136830&doi=10.1145%2f1387673.1387676&partnerID=40&md5=45783e4692f289ab2de254074f6d89ee,"This article presents a static system for checking determinism (technically, partial confluence) of communicating concurrent processes. Our approach automatically detects partial confluence in programs communicating via a mix of different kinds of communication methods: rendezvous channels, buffered channels, broadcast channels, and reference cells. Our system reduces the partial confluence checking problem in polynomial time (in the size of the program) to the problem of solving a system of rational linear inequalities, and is thus efficient. © 2008 ACM.",Capabilities; Determinism; Type systems,Polynomial approximation; Broadcast channels; Capabilities; Communication methods; Concurrent processing; Determinism; Linear inequalities; Polynomial-time; Type systems; Problem solving
Nominal logic programming,2008,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-51849109373&doi=10.1145%2f1387673.1387675&partnerID=40&md5=a67ce3d0d93f270c1d4edff4ae9c81e6,"Nominal logic is an extension of first-order logic which provides a simple foundation for formalizing and reasoning about abstract syntax modulo consistent renaming of bound names (that is, α-equivalence). This article investigates logic programming based on nominal logic. We describe some typical nominal logic programs, and develop the model-theoretic, proof-theoretic, and operational semantics of such programs. Besides being of interest for ensuring the correct behavior of implementations, these results provide a rigorous foundation for techniques for analysis and reasoning about nominal logic programs, as we illustrate via examples. © 2008 ACM.",Logic programming; Name-binding; Nominal logic; Semantics,Computer programming; Formal logic; Foundations; Information theory; Abstract syntax; First-order logic; Name-binding; Nominal logic; Operational semantics; Semantics; Logic programming
Two-dimensional bidirectional object layout,2008,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-51849114579&doi=10.1145%2f1387673.1387677&partnerID=40&md5=1072c10c6b2291bae09b4e8c47288ad2,"Object layout schemes used in C++ and other languages rely on (sometimes numerous) compiler generated fields. We describe a language-independent object layout scheme, which is space optimal, that is, objects are contiguous, and contain no compiler generated fields other than a single type identifier. As in C++ and other multiple inheritance languages such as CECIL and DYLAN, the new scheme sometimes requires extra levels of indirection to access some of the fields. Using a data set of 28 hierarchies, totaling almost 50,000 types, we show that this scheme improves field access efficiency over standard implementations, and competes favorably with (the non-space-optimal) highly optimized C++ specific implementations. The benchmark includes an analytical model for computing the frequency of indirections in a sequence of field access operations. Our layout scheme relies on whole-program analysis, which requires about 10 microseconds per type on a contemporary architecture (Pentium III, 900Mhz, 256MB machine), even in very large hierarchies. We also present a layout scheme for separate compilation using the user-annotation of virtual inheritance edge that is used in C++. © 2008 ACM.",Bidirectional; Coloring; Hierarchy; Inheritance; Layout,Coloring; Data flow analysis; Program compilers; Bidirectional; Contemporary architectures; Hierarchy; Inheritance; Language independents; Layout; Separate compilation; Whole-program analysis; C++ (programming language)
A type system equivalent to a model checker,2008,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-51849131926&doi=10.1145%2f1387673.1387678&partnerID=40&md5=a08f9e388d46ccdbe9808fb56a2c996f,"Type systems and model checking are two prevalent approaches to program verification. A prominent difference between them is that type systems are typically defined in a syntactic and modular style whereas model checking is usually performed in a semantic and whole-program style. This difference between the two approaches makes them complementary to each other: type systems are good at explaining why a program was accepted while model checkers are good at explaining why a program was rejected. We present a type system that is equivalent to a model checker for verifying temporal safety properties of imperative programs. The model checker is natural and may be instantiated with any finite-state abstraction scheme such as predicate abstraction. The type system, which is also parametric, type checks exactly those programs that are accepted by the model checker. It uses a variant of function types to capture flow sensitivity and intersection and union types to capture context sensitivity. Our result sheds light on the relationship between type systems and model checking, provides a methodology for studying their relative expressiveness, is a step towards sharing results between the two approaches, and motivates synergistic program analyses involving interplay between them. © 2008 ACM.",Model checking; Type systems,Abstracting; Codes (symbols); Information theory; Sensitivity analysis; Capture flow; Context-sensitivity; Finite-state abstractions; Imperative programs; Model checkers; Predicate abstractions; Program verification; Temporal safety; Type systems; Union types; Model checking
A semantics-based approach to malware detection,2008,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-51849164885&doi=10.1145%2f1387673.1387674&partnerID=40&md5=60cfa311b9c7fb66030f3c680287b76f,"Malware detection is a crucial aspect of software security. Current malware detectors work by checking for signatures, which attempt to capture the syntactic characteristics of the machine-level byte sequence of the malware. This reliance on a syntactic approach makes current detectors vulnerable to code obfuscations, increasingly used by malware writers, that alter the syntactic properties of the malware byte sequence without significantly affecting their execution behavior. This paper takes the position that the key to malware identification lies in their semantics. It proposes a semantics-based framework for reasoning about malware detectors and proving properties such as soundness and completeness of these detectors. Our approach uses a trace semantics to characterize the behavior of malware as well as that of the program being checked for infection, and uses abstract interpretation to ""hide"" irrelevant aspects of these behaviors. As a concrete application of our approach, we show that (1) standard signature matching detection schemes are generally sound but not complete, (2) the semantics-aware malware detector proposed by Christodorescu et al. is complete with respect to a number of common obfuscations used by malware writers and (3) the malware detection scheme proposed by Kinder et al. and based on standard model-checking techniques is sound in general and complete on some, but not all, obfuscations handled by the semantics-aware malware detector. © 2008 ACM.",Abstract interpretation; Malware detection; Obfuscation; Trace semantics,Codes (symbols); Detectors; Information theory; Model checking; Program interpreters; Semantics; Standards; Syntactics; Abstract interpretation; Abstract interpretations; Current detectors; Detection schemes; Malware; Malware detection; Obfuscation; Signature matching; Software security; Soundness and completeness; Standard model; Syntactic approach; Syntactic properties; Trace semantics; Computer crime
Local reasoning about a copying garbage collector,2008,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-49449084639&doi=10.1145%2f1377492.1377499&partnerID=40&md5=fd62779e389b0b35de40dad64270900a,"We present a programming language, model, and logic appropriate for implementing and reasoning about a memory management system. We state semantically what is meant by correctness of a copying garbage collector, and employ a variant of the novel separation logics to formally specify partial correctness of Cheney's copying garbage collector in our program logic. Finally, we prove that our implementation of Cheney's algorithm meets its specification using the logic we have given and auxiliary variables. © 2008 ACM.",Copying garbage collector; Local reasoning; Separation logic,Computer programming languages; Computer software; Copying; Separation; Auxiliary variables; Copying garbage collector; Garbage Collector; Local reasoning; Memory-management; Partial correctness; Program logics; Programming languages; Separation logic; BASIC (programming language)
Relations as an abstraction for BDD-based program analysis,2008,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-49449093928&doi=10.1145%2f1377492.1377494&partnerID=40&md5=6fb966f6d43298748071fba65f134785,"In this article we present Jedd, a language extension to Java that supports a convenient way of programming with Binary Decision Diagrams (BDDs). The Jedd language abstracts BDDs as database-style relations and operations on relations, and provides static type rules to ensure that relational operations are used correctly. The article provides a description of the Jedd language and reports on the design and implementation of the Jedd translator and associated runtime system. Of particular interest is the approach to assigning attributes from the high-level relations to physical domains in the underlying BDDs, which is done by expressing the constraints as a SAT problem and using a modern SAT solver to compute the solution. Further, a runtime system is defined that handles memory management issues and supports a browsable profiling tool for tuning the key BDD operations. The motivation for designing Jedd was to support the development of interrelated whole program analyses based on BDDs. We have successfully used Jedd to build Paddle, a framework of context-sensitive program analyses, including points-to analysis and call graph construction, as well as several client analyses. © 2008 ACM.",Binary decision diagrams; Boolean formula satisfiability; Java; Language design; Physical domain assignment; Points-to analysis; Program analysis; Relations,Abstracting; Binary decision diagrams; Computer programming languages; Computer software; Data structures; Decision theory; File organization; Linguistics; Boolean formula satisfiability; Call graph construction; Context-sensitive; Java; Language design; Memory-management; Physical domain assignment; Points-to analysis; Program analysis; Relations; Run-time systems; SAT problems; SAT solver; Java programming language
Remote specialization for efficient embedded operating systems,2008,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-49449101269&doi=10.1145%2f1377492.1377497&partnerID=40&md5=adf33140150fcbddb705dbcf29f4232c,"Prior to their deployment on an embedded system, operating systems are commonly tailored to reduce code size and improve runtime performance. Program specialization is a promising match for this process: it is predictable and modules, and it allows the reuse of previously implemented specializations. A specialization engine for embedded systems must overcome three main obstacles: (i) Reusing existing compilers for embedded systems, (ii) supporting specialization on a resource-limited system and (iii) coping with dynamic applications by supporting specialization on demand. In this article, we describe a runtime specialization infrastructure that addresses these problems. Our solution proposes: (i) Specialization in two phases of which the former generates specialized C templates and the latter uses a dedicated compiler to generate efficient native code. (ii) A virtualization mechanism that facilitates specialization of code at a remote location. (iii) An API and supporting OS extensions that allow applications to produce, manage and dispose of specialized code. We evaluate our work through two case studies: (i) The TCP/IP implementation of Linux and (ii) The TUX embedded web server. We report appreciable improvements in code size and performance. We also quantify the overhead of specialization and argue that a specialization server can scale to support a sizable workload. © 2008 ACM.",Compilers; Performance analysis; Remote specialization; Specialization server,Application programming interfaces (API); Codes (standards); Codes (symbols); Computer operating systems; Computer programming languages; Computer software; Computer software reusability; Computer systems programming; Integrated circuits; Program compilers; Case studies; Code size; Compilers; Dynamic applications; Embedded operating systems; Native code; On-demand; Operating systems; Performance analysis; Program specialization; Remote locations; Remote specialization; Runtime performance; Runtime specialization; Specialization server; TCP/IP implementation; Two phases; Virtualization; Web servers; Embedded systems
Checking type safety of foreign function calls,2008,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-49449088110&doi=10.1145%2f1377492.1377493&partnerID=40&md5=e50eef8f7cab72946fb6e616328fe3ab,"Foreign function interfaces (FFIs) allow components in different languages to communicate directly with each other. While FFIs are useful, they often require writing tricky low-level code and include little or no static safety checking, thus providing a rich source of hard-to-find programming errors. In this article, we study the problem of enforcing type safety across the OCaml-to-C FFI and the Java Native Interface (JNI). We present O-Saffire and J-Saffire, a pair of multilingual type inference systems that ensure C code that uses these FFIs accesses high-level data safely. Our inference systems use representational types to model C's low-level view of OCaml and Java values, and singleton types to track integers, strings, memory offsets, and type tags through C. J-Saffire, our Java system, uses a polymorphic flow-insensitive, unification-based analysis. Polymorphism is important because it allows us to precisely model user-defined wrapper functions and the more than 200 JNI functions. O-Saffire, our OCaml system, uses a monomorphic flow-sensitive analysis because, while polymorphism is much less important for the OCaml FFI flow-sensitivity is critical to track conditional branches, which are used when pattern matching OCaml data in C. O-Saffire also tracks garbage collection information to ensure that local C pointers to the OCaml heap are registered properly, which is not necessary for the JNI. We have applied O-Saffire and J-Saffire to a set of benchmarks and found many bugs and questionable coding practices. These results suggest that static checking of FFIs can be a valuable tool in writing correct multilingual software. © 2008 ACM.",Dataflow analysis; FFI; Flow-sensitive type system; Foreign function calls; Foreign function interface; Java; Java Native Interface; JNI; Multilingual type inference; Multilingual type system; OCaml; Representational type,Accident prevention; Codes (standards); Codes (symbols); Computer programming languages; Data storage equipment; Java programming language; Pattern matching; Polymorphism; Programming theory; Refuse collection; Refuse disposal; Security of data; Sensitivity analysis; Waste disposal; Conditional branches; FFI; Flow-sensitive analysis; Flow-sensitive type system; Foreign function calls; Foreign function interface; Foreign-function interfaces; Function calls; Garbage collection; Inference systems; Java; Java Native Interface; Java systems; JNI; Multilingual type inference; Multilingual type system; OCaml; Programming errors; Representational type; Static checking; Type inferences; Type safety; Wrapper functions; Data flow analysis
Register allocation for software pipelined multidimensional loops,2008,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-49449108557&doi=10.1145%2f1377492.1377498&partnerID=40&md5=fce73d5f6781f34174caecb98ba3dc1e,"This article investigates register allocation for software pipelined multidimensional loops where the execution of successive iterations from an n-dimensional loop is overlapped. For single loop software pipelining, the lifetimes of a loop variable in successive iterations of the loop form a repetitive pattern. An effective register allocation method is to represent the pattern as a vector of lifetimes (or a vector lifetime using Rau's terminology [Rau 1992]) and map it to rotating registers. Unfortunately, the software pipelined schedule of a multidimensional loop is considerably more complex and so are the vector lifetimes in it. In this article, we develop a way to normalize and represent the vector lifetimes, which captures their complexity, while exposing their regularity that enables a simple solution. The problem is formulated as bin-packing of the multidimensional vector lifetimes on the surface of a space-time cylinder. A metric, called distance, is calculated either conservatively or aggressively to guide the bin-packing process, so that there is no overlapping between any two vector lifetimes, and the register requirement is minimized. This approach subsumes the classical register allocation for software pipelined single loops as a special case. The method has been implemented in the ORC compiler and produced code for the IA-64 architecture. Experimental results show the effectiveness. Several strategies for register allocation are compared and analyzed. © 2008 ACM.",Register allocation; Software pipelining,Bins; Chlorine compounds; Computer architecture; Digital signal processing; Terminology; Bin-packing; Register allocation; Register requirement; Single loop; Software pipelining; Vectors
Types for atomicity: Static checking and inference for Java,2008,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-49449116974&doi=10.1145%2f1377492.1377495&partnerID=40&md5=b4ef548c92e6ceedce23accaf8105b2c,"Atomicity is a fundamental correctness property in multithreaded programs. A method is atomic if, for every execution, there is an equivalent serial execution in which the actions of the method are not interleaved with actions of other threads. Atomic methods are amenable to sequential reasoning, which significantly facilitates subsequent analysis and verification. This article presents a type system for specifying and verifying the atomicity of methods in multithreaded Java programs using a synthesis of Lipton's theory of reduction and type systems for race detection. The type system supports guarded, write-guarded, and unguarded fields, as well as thread-local data, parameterized classes and methods, and protected locks. We also present an algorithm for verifying atomicity via type inference. We have applied our type checker and type inference tools to a number of commonly used Java library classes and programs. These tools were able to verify the vast majority of methods in these benchmarks as atomic, indicating that atomicity is a widespread methodology for multithreaded programming. In addition, reported atomicity violations revealed some subtle errors in the synchronization disciplines of these programs. © 2008 ACM.",Atomicity; Concurrent programs; Type inference; Type systems,Atomic physics; Atoms; Computer programming languages; Computer software; Inference engines; Information retrieval systems; Multitasking; Atomicity; Concurrent programs; Java library; Local data; Multi-threaded Java programs; Multi-threaded programming; Multi-threaded programs; Parameterized; Race detection; Serial execution; Static checking; Subsequent analysis; Type checker; Type inference; Type inferences; Type systems; Java programming language
Java bytecode verification via static single assignment form,2008,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-49449097647&doi=10.1145%2f1377492.1377496&partnerID=40&md5=c83d617e108b547e1d38c0527f6d44b4,"Java Virtual Machines (JVMs) traditionally perform bytecode verification by way of an iterative dataflow analysis. Bytecode verification is necessary to ensure type safety because temporary variables in the JVM are not statically typed. We present an alternative verification mechanism that transforms JVM bytecode into Static Single Assignment Form (SSA) and thereby propagates definitions directly to uses. Type checking at control flow merge points can then be performed in a single pass. Our prototype implementation of the new algorithm is faster than the standard JVM bytecode verifier. It has the additional benefit of generating SSA as a side effect, which may be immediately useful for a subsequent dynamic compilation stage. © 2008 ACM.",Dataflow analysis; Java bytecode verification; Static single assignment form,Data flow analysis; Standards; Byte code verification; Byte codes; Control flows; Dynamic compilation; Java byte codes; Java bytecode verification; Java Virtual Machines; New algorithm; Prototype implementations; Side effects; Single pass; Static single assignment form; Temporary variables; Type checking; Type safety; Computer programming languages
On the complexity of partially-flow-sensitive alias analysis,2008,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-44249095594&doi=10.1145%2f1353445.1353447&partnerID=40&md5=090122334b0b6d3644b006e78675f7d8,"We introduce the notion of a partially-flow-sensitive analysis based on the number of read and write operations that are guaranteed to be analyzed in a sequential manner. We study the complexity of partially-flow-sensitive alias analysis and show that precise alias analysis with a very limited flow-sensitivity is as hard as precise flow-sensitive alias analysis, both when dynamic memory allocation is allowed, as well as in the absence of dynamic memory allocation. © 2008 ACM.",Alias analysis; Flow-insensitive; Flow-sensitive,Dynamic programming; Sensitivity analysis; Storage allocation (computer); Dynamic memory allocation; Partially-flow-sensitive alias analysis; Computational complexity
AspectML: A polymorphic aspect-oriented functional programming language,2008,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-44249122826&doi=10.1145%2f1353445.1353448&partnerID=40&md5=19bd305d47755e4b44d20f145b1e6684,"This article defines AspectML, a typed functional, aspect-oriented programming language. The main contribution of AspectML is the seamless integration of polymorphism, run-time type analysis and aspect-oriented programming language features. In particular, AspectML allows programmers to define type-safe polymorphic advice using pointcuts constructed from a collection of polymorphic join points. AspectML also comes equipped with a type inference algorithm that conservatively extends Hindley - Milner type inference. To support first-class polymorphic point-cut designators, a crucial feature for developing aspect-oriented profiling or logging libraries, the algorithm blends the conventional Hindley - Milner type inference algorithm with a simple form of local type inference. We give our language operational meaning via a type-directed translation into an expressive type-safe intermediate language. Many complexities of the source language are eliminated in this translation, leading to a modular specification of its semantics. One of the novelties of the intermediate language is the definition of polymorphic labels for marking control-flow points. When a set of labels is assembled as a pointcut, the type of each label is an instance of the type of the pointcut. © 2008 ACM.",Aspect-oriented programming; Functional languages; Parametric and ad-hoc polymorphism; Type inference; Type systems,Computational complexity; Feature extraction; Semantics; Unified Modeling Language; Aspect-oriented profiling; Logging libraries; Polymorphic join points; Computer programming languages
Size-change termination with difference constraints,2008,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-44249110961&doi=10.1145%2f1353445.1353450&partnerID=40&md5=8a766ff482e564abb880a8260db446ab,"This article considers an algorithmic problem related to the termination analysis of programs. More specifically, we are given bounds on differences in sizes of data values before and after every transition in the program's control-flow graph. Our goal is to infer program termination via the following reasoning (the size-change principle): if in any infinite (hypothetic) execution of the program, some size must descend unboundedly, the program must always terminate, since infinite descent of a natural number is impossible. The problem of inferring termination from such abstract information is not the halting problem for programs and may well be decidable. If this is the case, the decision algorithm forms a back end of a termination verifier, and it is interesting to find out the computational complexity of the problem. A restriction of the problem described above, which only uses monotonicity information (but not difference bounds), is already known to be decidable. We prove that the unrestricted problem is undecidable, which gives a theoretical argument for studying restricted cases. We consider a case where the termination proof is allowed to make use of at most one bound per target variable in each transition. For this special case, which we claim is practically significant, we give (for the first time) an algorithm and show that the problem is in PSPACE, in fact that it is PSPACE-complete. The algorithm is based on combinatorial arguments and results from the theory of integer programming not previously used for similar problems. The algorithm has interesting connections to other work in termination, in particular to methods for generating linear ranking functions or invariants. © 2008 ACM.",Abstraction; Program analysis; Size-change graph; Size-change termination; Termination analysis,Abstracting; Computational complexity; Constraint theory; Graph theory; Problem solving; Verification; Program analysis; Size-change graph; Size-change termination; Termination analysis; Computer software
PEAK-a fast and effective performance tuning system via compiler optimization orchestration,2008,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-44249112350&doi=10.1145%2f1353445.1353451&partnerID=40&md5=fa083f4f5969a3a48277e49d527d8559,"Compile-time optimizations generally improve program performance. Nevertheless, degradations caused by individual compiler optimization techniques are to be expected. Feedback-directed optimization orchestration systems generate optimized code versions under a series of optimization combinations, evaluate their performance, and search for the best version. One challenge to such systems is to tune program performance quickly in an exponential search space. Another challenge is to achieve high program performance, considering that optimizations interact. Aiming at these two goals, this article presents an automated performance tuning system, PEAK, which searches for the best compiler optimization combinations for the important code sections in a program. The major contributions made in this work are as follows: (1) An algorithm called Combined Elimination (CE) is developed to explore the optimization space quickly and effectively; (2) Three fast and accurate rating methods are designed to evaluate the performance of an optimized code section based on a partial execution of the program; (3) An algorithm is developed to identify important code sections as candidates for performance tuning by trading off tuning speed and tuned program performance; and (4) A set of compiler tools are implemented to automate optimization orchestration. Orchestrating optimization options in SUN Forte compilers at the whole-program level, our CE algorithm improves performance by 10.8% over the SPEC CPU2000 FP baseline setting, compared to 5.6% improved by manual tuning. Orchestrating GCC O3 optimizations, CE improves performance by 12% over O3, the highest optimization level. Applying the rating methods, PEAK reduces tuning time from 2.19 hours to 5.85 minutes on average, while achieving equal or better program performance. © 2008 ACM.",Dynamic compilation; Optimization orchestration; Performance tuning,Codes (symbols); Optimization; Supervisory and executive programs; Tuning; Dynamic compilation; Optimization orchestration; Performance tuning; Tuning systems; Program compilers
The pitfalls of verifying floating-point computations,2008,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-44249093716&doi=10.1145%2f1353445.1353446&partnerID=40&md5=a7b647ca2ecd0af3de44ae41646355ad,"Current critical systems often use a lot of floating-point computations, and thus the testing or static analysis of programs containing floating-point operators has become a priority. However, correctly defining the semantics of common implementations of floating-point is tricky, because semantics may change according to many factors beyond source-code level, such as choices made by compilers. We here give concrete examples of problems that can appear and solutions for implementing in analysis software. © 2008 ACM.",Abstract interpretation; AMD64; Embedded software; Floating point; FPU; IA32; IEEE-754; PowerPC; Program testing; Rounding; Safety-critical software; Static analysis; Verification; x87,Codes (symbols); Program compilers; Semantics; Verification; Floating-point computations; Source-code level; Computation theory
Witnessing side effects,2008,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-44249094188&doi=10.1145%2f1353445.1353449&partnerID=40&md5=e5eb7b08c1c371ebd8660f72f3c5ede1,"We present a new approach to the old problem of adding global mutable state to purely functional languages. Our idea is to extend the language with ""witnesses,"" which is based on an arguably more pragmatic motivation than past approaches. We give a semantic condition for correctness and prove it is sufficient. We also give a somewhat surprising static checking algorithm that makes use of a network flow property equivalent to the semantic condition via reduction to a satisfaction problem for a system of linear inequalities. © 2008 ACM.",Mutable state; Side effects,Algorithms; Linear programming; Problem solving; Semantics; Functional languages; Pragmatic motivation; Computer programming languages
Exceptional situations and program reliability,2008,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-41149163668&doi=10.1145%2f1330017.1330019&partnerID=40&md5=da51bb20f60062d3235607bac8c11c9b,"It is difficult to write programs that behave correctly in the presence of run-time errors. Proper behavior in the face of exceptional situations is important to the reliability of long-running programs. Existing programming language features often provide poor support for executing clean-up code and for restoring invariants. We present a data-flow analysis for finding a certain class of exception-handling defects: those related to a failure to release resources or to clean up properly along all paths. Many real-world programs violate such resource usage rules because of incorrect exception handling. Our flow-sensitive analysis keeps track of outstanding obligations along program paths and does a precise modeling of control flow in the presence of exceptions. Using it, we have found over 1,300 exception handling defects in over 5 million lines of Java code. Based on those defects we propose a programming language feature, the compensation stack, that keeps track of obligations at run time and ensures that they are discharged. We present a type system for compensation stacks that tracks collections of obligations. Finally, we present case studies to demonstrate that this feature is natural, efficient, and can improve reliability. © 2008 ACM.",Compensating transactions; Error handling; Linear sagas; Linear types; Resource management,Data flow analysis; Error analysis; Java programming language; Reliability analysis; Compensating transactions; Error handling; Linear sagas; Linear types; Resource management; Run-time errors; Computer program listings
"Normalize, transpose, and distribute: An automatic approach for handling nonscalars",2008,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-41149131178&doi=10.1145%2f1330017.1330020&partnerID=40&md5=e3de02f882b5a774604506052977bffd,"SequenceL is a concise, high-level language with a simple semantics that provides for the automatic derivation of many iterative and parallel control structures. The semantics repeatedly applies a Normalize-Transpose-Distribute operation to functions and operators until base cases are discovered. Base cases include the grounding of variables and the application of built-in operators to operands of appropriate types. This article introduces the results of a 24-month effort to reduce the language to a very small set of primitives. Included are comparisons with other languages, the formal syntax and semantics, and the traces of several example problems run with a prototype interpreter developed in 2006. © 2008 ACM.",Automatic loop generation; Automatic parallelisms,Semantics; Automatic loop generation; Automatic parallelisms; Computer programming languages
FeatherTrait: A modest extension of Featherweight Java,2008,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-41149113370&doi=10.1145%2f1330017.1330022&partnerID=40&md5=a6f94e24a715e33a5cfa86485808ed9e,"In the context of statically typed, class-based languages, we investigate classes that can be extended with trait composition. A trait is a collection of methods without state; it can be viewed as an incomplete stateless class. Traits can be composed in any order, but only make sense when imported by a class that provides state variables and additional methods to disambiguate conflicting names arising between the imported traits. We introduce FeatherTrait Java (FTJ), a conservative extension of the simple lightweight class-based calculus Featherweight Java (FJ) with statically typed traits. In FTJ, classes can be built using traits as basic behavioral bricks; method conflicts between imported traits must be resolved explicitly by the user either by (i) aliasing or excluding method names in traits, or by (ii) overriding explicitly the conflicting methods in the class or in the trait itself. We present an operational semantics with a lookup algorithm, and a sound type system that guarantees that evaluating a well-typed expression never yields a message-not-understood run-time error nor gets the interpreter stuck. We give examples of the increased expressive power of the trait-based inheritance model. The resulting calculus appears to be a good starting point for a rigorous mathematical analysis of typed class-based languages featuring trait-based inheritance. © 2008 ACM.",Inheritance; Java; Language design; Language semantics,Algorithms; Computer programming; Mathematical models; Semantics; Inheritance model; Language design; Language semantics; Java programming language
Dynamic slicing on Java bytecode traces,2008,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-41149168609&doi=10.1145%2f1330017.1330021&partnerID=40&md5=a2f86f7facf1b806f9836748e0b1989d,"Dynamic slicing is a well-known technique for program analysis, debugging and understanding. Given a program P and input I, it finds all program statements which directly/indirectly affect the values of some variables' occurrences when P is executed with I. In this article, we develop a dynamic slicing method for Java programs. Our technique proceeds by backwards traversal of the bytecode trace produced by an input I in a given program P. Since such traces can be huge, we use results from data compression to compactly represent bytecode traces. The major space savings in our method come from the optimized representation of (a) data addresses used as operands by memory reference bytecodes, and (b) instruction addresses used as operands by control transfer bytecodes. We show how dynamic slicing algorithms can directly traverse our compact bytecode traces without resorting to costly decompression. We also extend our dynamic slicing algorithm to perform relevant slicing. The resultant slices can be used to explain omission errors that is, why some events did not happen during program execution. Detailed experimental results on space/time overheads of tracing and slicing are reported in the article. The slices computed at the bytecode level are translated back by our tool to the source code level with the help of information available in Java class files. Our JSlice dynamic slicing tool has been integrated with the Eclipse platform and is available for usage in research and development. © 2008 ACM.",Debugging; Program slicing; Tracing,Algorithms; Data compression; Program debugging; Program diagnostics; Bytecode traces; Dynamic slicing; Dynamic slicing algorithm; Optimized representation; Java programming language
Reverse-mode AD in a functional framework: Lambda the ultimate backpropagator,2008,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-41149127139&doi=10.1145%2f1330017.1330018&partnerID=40&md5=fa05e5e40784b63379765540954aa59b,"We show that reverse-mode AD (Automatic Differentiation) - -a generalized gradient-calculation operator - -can be incorporated as a first-class function in an augmented lambda calculus, and therefore into a functional-programming language. Closure is achieved, in that the new operator can be applied to any expression in the augmented language, yielding an expression in that language. This requires the resolution of two major technical issues: (a) how to transform nested lambda expressions, including those with free-variable references, and (b) how to support self application of the AD machinery. AD transformations preserve certain complexity properties, among them that the reverse phase of the reverse-mode AD transformation of a function have the same temporal complexity as the original untransformed function. First-class unrestricted AD operators increase the expressive power available to the numeric programmer, and may have significant practical implications for the construction of numeric software that is robust, modular, concise, correct, and efficient. © 2008 ACM.",Closures; Derivatives; Forward-mode AD; Higher-order AD; Higher-order functional languages; Jacobian; Program transformation; Reflection,Backpropagation; Computer software; Functional programming; Jacobian matrices; Forward-mode AD; Higher-order AD; Higher-order functional languages; Program transformation; Computer programming languages
Empirical study of optimization techniques for massive slicing,2007,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-37149037750&doi=10.1145%2f1290520.1290523&partnerID=40&md5=1f1c2223a66ff0ff459bd84f9d421b3f,"This article presents results from a study of techniques that improve the performance of graph-based interprocedural slicing of the System Dependence Graph (SDG). This is useful in ""massive slicing"" where slices are required for many or all of the possible set of slicing criteria. Several different techniques are considered, including forming strongly connected components, topological sorting, and removing transitive edges. Data collected from a test bed of just over 1,000,000 lines of code are presented. This data illustrates the impact on computation time of the techniques. Together, the best combination produces a 71% reduction in run-time (and a 64% reduction in memory usage). The complete set of techniques also illustrates the point at which faster computation is not viable due to prohibitive preprocessing costs. © 2007 ACM.",Empirical study; Internal representation; Performance enhancement; Slicing,Cost effectiveness; Data mining; Optimization; Internal representation; Massive slicing; Performance enhancement; System Dependence Graph (SDG); Topological sorting; Graph theory
Heap reference analysis using access graphs,2007,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-37048998515&doi=10.1145%2f1290520.1290521&partnerID=40&md5=626cfe9feebdf23aa8bbf53c8ac06bbc,"Despite significant progress in the theory and practice of program analysis, analyzing properties of heap data has not reached the same level of maturity as the analysis of static and stack data. The spatial and temporal structure of stack and static data is well understood while that of heap data seems arbitrary and is unbounded. We devise bounded representations that summarize properties of the heap data. This summarization is based on the structure of the program that manipulates the heap. The resulting summary representations are certain kinds of graphs called access graphs. The boundedness of these representations and the monotonicity of the operations to manipulate them make it possible to compute them through data flow analysis. An important application that benefits from heap reference analysis is garbage collection, where currently liveness is conservatively approximated by reachability from program variables. As a consequence, current garbage collectors leave a lot of garbage uncollected, a fact that has been confirmed by several empirical studies. We propose the first ever end-to-end static analysis to distinguish live objects from reachable objects. We use this information to make dead objects unreachable by modifying the program. This application is interesting because it requires discovering data flow information representing complex semantics. In particular, we formulate the following new analyses for heap data: liveness, availability, and anticipability and propose solution methods for them. Together, they cover various combinations of directions of analysis (i.e., forward and backward) and confluence of information (i.e. union and intersection). Our analysis can also be used for plugging memory leaks in C/C++ languages. © 2007 ACM.",Aliasing; Data flow analysis; Heap references; Liveness,Computer programming; Data flow analysis; Data reduction; Graph theory; Random processes; Heap reference analysis; Liveness; Data structures
Efficient dynamic dispatching with type slicing,2007,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-37049017128&doi=10.1145%2f1290520.1290525&partnerID=40&md5=f4122cd05fbbf7d6b5557e6a81577111,"A fundamental problem in the implementation of object-oriented languages is that of a frugal implementation of dynamic dispatching, that is, a small footprint data structure that supports quick response to runtime dispatching queries of the following format: which method should be executed in response to a certain message sent to a given object. Previous theoretical algorithms for this problem tend to be impractical due to their conceptual complexity and large hidden constants. In contrast, successful practical heuristics lack theoretical support. The contribution of this article is in a novel type slicing technique, which results in two dispatching schemes: TS and CT d. We make the case for these schemes both practically and theoretically. The empirical findings on a corpus of 35 hierarchies totaling some 64 thousand types from eight different languages, demonstrate improvement over previous results in terms of the space required for the representation, and the time required for computing it. The theoretical analysis is with respect to ι, the best possible compression factor of the dispatching matrix. The results are expressed as a function of a parameter κ, which can be thought of as a metric of the complexity of the topology of a multiple inheritance hierarchy. In single inheritance hierarchies κ = 1, but although κ can be in the order of the size of the hierarchy, it is typically a small constant in actual use of inheritance; in our corpus, the median value of κ is 5, while its average is 6.4. The TS scheme generalizes the famous interval containment technique to multiple inheritance. TS achieves a compression factor of ι/κ, that is, our generalization comes with an increase to the space requirement by a small factor of κ. The pay is in the dispatching time, which is no longer constant as in a naive matrix implementation, but logarithmic in the number of different method implementations. In practice, dispatching uses one indirect branch and, on average, only 2.5 binary branches. The CT schemes are a sequence of algorithms CT 1, CT 2, CT 3, ..., where CT d uses d memory dereferencing operations during dispatch, and achieves a compression factor of 1/dι 1-1/d in a single inheritance setting. A generalization of these algorithms to a multiple inheritance setting, increases the space by a factor of (2κ) 1-1/d. This trade-off represents the first bounds on the compression ratio of constant-time dispatching algorithms. We also present an incremental variant of the CT d suited for languages such as Java. © 2007 ACM.",CT (compact dispatch tables); Dispatch; Dynamic-typing; Hierarchy; Incremental; Message; Subtyping; Type slicing,Computer programming languages; Data structures; Heuristic algorithms; Information retrieval; Query processing; Storage allocation (computer); Compact dispatch tables; Dynamic typing; Subtyping; Type slicing; Object oriented programming
Forma: A framework for safe automatic array reshaping,2007,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-37049020422&doi=10.1145%2f1290520.1290522&partnerID=40&md5=77a5b4eb0cc796d765047e23508fbd31,"This article presents Forma, a practical, safe, and automatic data reshaping framework that reorganizes arrays to improve data locality. Forma splits large aggregated data-types into smaller ones to improve data locality. Arrays of these large data types are then replaced by multiple arrays of the smaller types. These new arrays form natural data streams that have smaller memory footprints, better locality, and are more suitable for hardware stream prefetching. Forma consists of a field-sensitive alias analyzer, a data type checker, a portable structure reshaping planner, and an array reshaper. An extensive experimental study compares different data reshaping strategies in two dimensions: (1) how the data structure is split into smaller ones (maximal partition × frequency-based partition × affinity-based partition); and (2) how partitioned arrays are linked to preserve program semantics (address arithmetic-based reshaping × pointer-based reshaping). This study exposes important characteristics of array reshaping. First, a practical data reshaper needs not only an inter-procedural analysis but also a data-type checker to make sure that array reshaping is safe. Second, the performance improvement due to array reshaping can be dramatic: standard benchmarks can run up to 2.1 times faster after array reshaping. Array reshaping may also result in some performance degradation for certain benchmarks. An extensive micro-architecture-level performance study identifies the causes for this degradation. Third, the seemingly naive maximal partition achieves best or close-to-best performance in the benchmarks studied. This article presents an analysis that explains this surprising result. Finally, address-arithmetic-based reshaping always performs better than its pointer-based counterpart. © 2007 ACM.",Arrays; Data structure; Reference analysis,Array processing; Data mining; Data reduction; Information retrieval; Storage allocation (computer); Automatic array reshaping; Data locality; Data types; Pointers; Data structures
Efficient field-sensitive pointer analysis of C,2007,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-37049033028&doi=10.1145%2f1290520.1290524&partnerID=40&md5=701e0449d87527cae55107b2f5720f00,"The subject of this article is flow- and context-insensitive pointer analysis. We present a novel approach for precisely modelling struct variables and indirect function calls. Our method emphasises efficiency and simplicity and is based on a simple language of set constraints. We obtain an O(v4) bound on the time needed to solve a set of constraints from this language, where v is the number of constraint variables. This gives, for the first time, some insight into the hardness of performing field-sensitive pointer analysis of C. Furthermore, we experimentally evaluate the time versus precision trade-off for our method by comparing against the field-insensitive equivalent. Our benchmark suite consists of 11 common C programs ranging in size from 15,000 to 200,000 lines of code. Our results indicate the field-sensitive analysis is more expensive to compute, but yields significantly better precision. In addition, our technique has been integrated into the latest release (version 4.1) of the GNU Compiler GCC. Finally, we identify several previously unknown issues with an alternative and less precise approach to modelling struct variables, known as field-based analysis. © 2007 ACM.",Pointer analysis; Set-constraints,C (programming language); Constraint theory; Information retrieval; Program compilers; Storage allocation (computer); Function calls; Pointer analysis; Set constraints; Struct variables; Data structures
Run-time principals in information-flow type systems,2007,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-37049026868&doi=10.1145%2f1290520.1290526&partnerID=40&md5=c7643efc7d2c6a5af91e6ca3e391a2b1,"Information-flow type systems are a promising approach for enforcing strong end-to-end confidentiality and integrity policies. Such policies, however, are usually specified in terms of static information - -data is labeled high or low security at compile time. In practice, the confidentiality of data may depend on information available only while the system is running. This article studies language support for run-time principals, a mechanism for specifying security policies that depend on which principals interact with the system. We establish the basic property of noninterference for programs written in such language, and use run-time principals for specifying run-time authority in downgrading mechanisms such as declassification. In addition to allowing more expressive security policies, run-time principals enable the integration of language-based security mechanisms with other existing approaches such as Java stack inspection and public key infrastructures. We sketch an implementation of run-time principals via public keys such that principal delegation is verified by certificate chains. © 2007 ACM.",Decentralized label model; Dynamic principals; Information-flow; Noninterference; Run-time principals; Security-typed language; Soundness; Type systems,Computer programming languages; Data structures; Information systems; Program compilers; Security of data; Signal interference; Decentralized label models; Dynamic principals; Information flow; Noninterference; Run time principals; Security typed language; Soundness; Type systems; Data flow analysis
A step towards unifying schedule and storage optimization,2007,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-36048962096&doi=10.1145%2f1286821.1286825&partnerID=40&md5=508ef5804e1f873bf9790270a16a1837,"We present a unified mathematical framework for analyzing the tradeoffs between parallelism and storage allocation within a parallelizing compiler. Using this framework, we show how to find a good storage mapping for a given schedule, a good schedule for a given storage mapping, and a good storage mapping that is valid for all legal (one-dimensional affine) schedules. We consider storage mappings that collapse one dimension of a multidimensional array, and programs that are in a single assignment form and accept a one-dimensional affine schedule. Our method combines affine scheduling techniques with occupancy vector analysis and incorporates general affine dependences across statements and loop nests. We formulate the constraints imposed by the data dependences and storage mappings as a set of linear inequalities, and apply numerical programming techniques to solve for the shortest occupancy vector. We consider our method to be a first step towards automating a procedure that finds the optimal tradeoff between parallelism and storage space. © 2007 ACM.",Affine recurrence equations; Affine scheduling; Automatic parallelization; Occupancy vectors; Polyhedral model; Storage optimization,Computer programming; Data storage equipment; Mathematical models; Numerical methods; Optimization; Program compilers; Affine recurrence equations; Affine scheduling; Automatic parallelization; Occupancy vectors; Polyhedral models; Storage optimization; Scheduling
"The embedded machine: Predictable, portable real-time code",2007,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-36048995287&doi=10.1145%2f1286821.1286824&partnerID=40&md5=f4a412a7940733c03da44859d1c00b35,"The Embedded Machine is a virtual machine that mediates in real time the interaction between software processes and physical processes. It separates the compilation of embedded programs into two phases. The first phase, the platform-independent compiler phase, generates E code (code executed by the Embedded Machine), which supervises the timing, not the scheduling of, application tasks relative to external events such as clock ticks and sensor interrupts. E code is portable and, given an input behavior, exhibits predictable (i.e., deterministic) timing and output behavior. The second phase, the platform-dependent compiler phase, checks the time safety of the E code, that is, whether platform performance (determined by the hardware) and platform utilization (determined by the scheduler of the operating system) enable its timely execution. We have used the Embedded Machine to compile and execute high-performance control applications written in Giotto, such as the flight control system of an autonomous model helicopter. © 2007 ACM.",Real time; Virtual machine,Codes (symbols); Computer operating systems; Flight control systems; Helicopters; Real time systems; Virtual reality; Embedded machine; Physical processes; Sensor interrupts; Virtual machines; Embedded systems
An improved bound for call strings based interprocedural analysis of bit vector frameworks,2007,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-36048991757&doi=10.1145%2f1286821.1286829&partnerID=40&md5=01ac98d274787d8998f59ff3273d321a,"Interprocedural data flow analysis extends the scope of analysis across procedure boundaries in search of increased optimization opportunities. Call strings based approach is a general approach for performing flow and context sensitive interprocedural analysis. It maintains a history of calls along with the data flow information in the form of call strings, which are sequences of unfinished calls. Recursive programs may need infinite call strings for interprocedural data flow analysis. For bit vector frameworks this method is believed to require all call strings of lengths up to 3K, where K is the maximum number of distinct call sites in any call chain. We combine the nature of information flows in bit-vector data flow analysis with the structure of interprocedurally valid paths to bound the call strings. Instead of bounding the length of call strings, we bound the number of occurrences of any call site in a call string. We show that the call strings in which a call site appears at most three times, are sufficient for convergence on interprocedural maximum fixed point solution. Though this results in the same worst case length of call strings, it does not require constructing all call strings up to length 3K. Our empirical measurements on recursive programs show that our bound reduces the lengths and the number of call strings, and hence the analysis time, significantly. © 2007 ACM.",Bit Vector Data Flow Frameworks; Interprocedural Data Flow Analysis,Binary codes; Convergence of numerical methods; Information analysis; Optimization; Recursive functions; Sensitivity analysis; Bit Vector Data Flow Frameworks; Call strings; Interprocedural Data Flow Analysis; Recursive programs; Data reduction
Dynamic graph-based software fingerprinting,2007,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-36049034911&doi=10.1145%2f1286821.1286826&partnerID=40&md5=84e1e3f1b5290025e2e47c46be1c4b71,"Fingerprinting embeds a secret message into a cover message. In media fingerprinting, the secret is usually a copyright notice and the cover a digital image. Fingerprinting an object discourages intellectual property theft, or when such theft has occurred, allows us to prove ownership. The Software Fingerprinting problem can be described as follows. Embed a structure W into a program P such that: W can be reliably located and extracted from P even after P has been subjected to code transformations such as translation, optimization and obfuscation; W is stealthy; W has a high data rate; embedding W into P does not adversely affect the performance of P; and W has a mathematical property that allows us to argue that its presence in P is the result of deliberate actions. In this article, we describe a software fingerprinting technique in which a dynamic graph fingerprint is stored in the execution state of a program. Because of the hardness of pointer alias analysis such fingerprints are difficult to attack automatically. © 2007 ACM.",Software piracy; Software protection; Watermarking,Optimization; Problem Solving; Water Marks; Computer crime; Computer software; Message passing; Optimization; Problem solving; Watermarking; Graph fingerprint; Mathematical property; Software fingerprinting; Software protection; Electronic document identification systems
Editorial: A changing of the guard,2007,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-36048947624&doi=10.1145%2f1286821.1293892&partnerID=40&md5=d9568a1089342d260d85878be2bd0639,[No abstract available],,
Encapsulating objects with confined types,2007,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-36048984982&doi=10.1145%2f1286821.1286823&partnerID=40&md5=62168ba8d0ae76a172ea942541c562c9,"Object-oriented languages provide little support for encapsulating objects. Reference semantics allows objects to escape their defining scope, and the pervasive aliasing that ensues remains a major source of software defects. This paper presents Kacheck/J, a tool for inferring object encapsulation properties of large Java programs. Our goal is to develop practical tools to assist software engineers, thus we focus on simple and scalable techniques. Kacheck/J is able to infer confinement - -the property that all instances of a given type are encapsulated in their defining package. This simple property can be used to identify accidental leaks of sensitive objects, as well as for compiler optimizations. We report on the analysis of a large body of code and discuss language support and refactoring for confinement. © 2007 ACM.",,Computer software; Java programming language; Optimization; Program compilers; Semantics; Ubiquitous computing; Compiler optimizations; Encapsulating objects; Software defects; Software engineers; Object oriented programming
Goal-directed weakening of abstract interpretation results,2007,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-36048931056&doi=10.1145%2f1286821.1286830&partnerID=40&md5=c45a696588c43339657151b5c3882c8d,"One proposal for automatic construction of proofs about programs is to combine Hoare logic and abstract interpretation. Constructing proofs is in Hoare logic. Discovering programs' invariants is done by abstract interpreters. One problem of this approach is that abstract interpreters often compute invariants that are not needed for the proof goal. The reason is that the abstract interpreter does not know what the proof goal is, so it simply tries to find as strong invariants as possible. These unnecessary invariants increase the size of the constructed proofs. Unless the proof-construction phase is notified which invariants are not needed, it blindly proves all the computed invariants. In this article, we present a framework for designing algorithms, called abstract-value slicers, that slice out unnecessary invariants from the results of forward abstract interpretation. The framework provides a generic abstract-value slicer that can be instantiated into a slicer for a particular abstract interpretation. Such an instantiated abstract-value slicer works as a post-processor to an abstract interpretation in the whole proof-construction process, and notifies to the next proof-construction phase which invariants it does not have to prove. Using the framework, we designed an abstract-value slicer for an existing relational analysis and applied it on programs. In this experiment, the slicer identified 62% - 81% of the computed invariants as unnecessary, and resulted in 52% - 84% reduction in the size of constructed proofs. © 2007 ACM.",Abstract interpretation; Backward analysis; Hoare logic; Program verification; Static analysis,Algorithms; Problem solving; Program interpreters; Static analysis; Abstract interpretation; Backward analysis; Hoare logic; Program verification; Abstracting
A proof theory for machine code,2007,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-36048966264&doi=10.1145%2f1286821.1286827&partnerID=40&md5=06264451f92beb2db323cb25028dfb94,"This article develops a proof theory for low-level code languages. We first define a proof system, which we refer to as the sequential sequent calculus, and show that it enjoys the cut elimination property and that its expressive power is the same as that of the natural deduction proof system. We then establish the Curry-Howard isomorphism between this proof system and a low-level code language by showing the following properties: (1) the set of proofs and the set of typed codes is in one-to-one correspondence, (2) the operational semantics of the code language is directly derived from the cut elimination procedure of the proof system, and (3) compilation and decompilation algorithms between the code language and the typed lambda calculus are extracted from the proof transformations between the sequential sequent calculus and the natural deduction proof system. This logical framework serves as a basis for the development of type systems of various low-level code languages, type-preserving compilation, and static code analysis. © 2007 ACM.",Curry-Howard isomorphism,Algorithms; Computer programming languages; Security systems; Semantics; Code languages; Curry Howard isomorphism; Machine codes; Proof systems; Codes (symbols)
Optimizing indirect branch prediction accuracy in virtual machine interpreters,2007,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-36048973219&doi=10.1145%2f1286821.1286828&partnerID=40&md5=ea918f2312428fd80f21e2fff1bfdd28,"Interpreters designed for efficiency execute a huge number of indirect branches and can spend more than half of the execution time in indirect branch mispredictions. Branch target buffers (BTBs) are the most widely available form of indirect branch prediction; however, their prediction accuracy for existing interpreters is only 2% - 50%. In this article we investigate two methods for improving the prediction accuracy of BTBs for interpreters: replicating virtual machine (VM) instructions and combining sequences of VM instructions into superinstructions. We investigate static (interpreter build-time) and dynamic (interpreter runtime) variants of these techniques and compare them and several combinations of these techniques. To show their generality, we have implemented these optimizations in VMs for both Java and Forth. These techniques can eliminate nearly all of the dispatch branch mispredictions, and have other benefits, resulting in speedups by a factor of up to 4.55 over efficient threaded-code interpreters, and speedups by a factor of up to 1.34 over techniques relying on dynamic superinstructions alone. © 2007 ACM.",Branch prediction; Branch target buffer; Code replication; Interpreter; Superinstruction,Dynamic programming; Java programming language; Optimization; Virtual reality; Branch prediction; Branch target buffer; Code replication; Superinstruction; Program interpreters
A uniform type structure for secure information flow,2007,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-36048945344&doi=10.1145%2f1286821.1286822&partnerID=40&md5=b4cdc475764b6c95b6b5fa148641b293,"The π-calculus, a calculus of mobile processes, can compositionally represent dynamics of major programming constructs by decomposing them into name passing. The present work reports our experience in using a linear/affine typed π-calculus for the analysis and development of type-based analyses for programming languages, focussing on secure information flow analysis. After presenting a basic typed calculus for secrecy, we demonstrate its usage by a sound embedding of the dependency core calculus (DCC) and the development of the call-by-value version of DCC. The secrecy analysis is then extended to stateful computation, for which we develop a novel type discipline for imperative programming language that extends a secure multi-threaded imperative language by Smith and Volpano with general references and higher-order procedures. In each analysis, the embedding gives a simple proof of noninterference. © 2007 ACM.",Secure information flow; The π-calculus; Type-based program analysis; Typing system,Computer programming languages; Mobile telecommunication systems; Security systems; Systems analysis; Dependency core calculus (DCC); Secure information flow; Type based program analysis; Typing systems; Information systems
"BI-hyperdoctrines, higher-order separation logic, and abstraction",2007,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34548416785&doi=10.1145%2f1275497.1275499&partnerID=40&md5=52ad684b03fc97a1ceb2fbc7f0af36e2,"We present a precise correspondence between separation logic and a simple notion of predicate BI, extending the earlier correspondence given between part of separation logic and propositional BI. Moreover, we introduce the notion of a BI hyperdoctrine, show that it soundly models classical and intuitionistic first- and higher-order predicate BI, and use it to show that we may easily extend separation logic to higher-order. We also demonstrate that this extension is important for program proving, since it provides sound reasoning principles for data abstraction in the presence of aliasing. © 2007 ACM.",Abstraction; Hyperdoctrines; Separation logic,Abstracting; Case based reasoning; Computer program listings; Computer programming languages; Data reduction; Hyperdoctrines; Separation logics; Formal logic
Enforcing resource bounds via static verification of dynamic checks,2007,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34548408787&doi=10.1145%2f1275497.1275503&partnerID=40&md5=4ad55f8335b945dcedae003ed035c790,"We show how to limit a program's resource usage in an efficient way, using a novel combination of dynamic checks and static analysis. Usually, dynamic checking is inefficient due to the overhead of checks, while static analysis is difficult and rejects many safe programs. We propose a hybrid approach that solves these problems. We split each resource-consuming operation into two parts. The first is a dynamic check, called reserve. The second is the actual operation, called consume, which does not perform any dynamic checks. The programmer is then free to hoist and combine reserve operations. Combining reserve operations reduces their overhead, while hoisting reserve operations ensures that the program does not run out of resources at an inconvenient time. A static verifier ensures that the program reserves resources before it consumes them. This verification is both easier and more flexible than an a priori static verification of resource usage. We present a sound and efficient static verifier based on Hoare logic and linear inequalities. As an example, we present a version of tar written in Java. © 2007 ACM.",Dynamic; Resource bounds; Static,Computer program listings; Java programming language; Logic programming; Static analysis; Dynamic checks; Static verification; Verification
A new foundation for control dependence and slicing for modern program structures,2007,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34548433567&doi=10.1145%2f1275497.1275502&partnerID=40&md5=0367ccca6ac4173ab0f5aedc2a739252,"The notion of control dependence underlies many program analysis and transformation techniques. Despite being widely used, existing definitions and approaches to calculating control dependence are difficult to apply directly to modern program structures because these make substantial use of exception processing and increasingly support reactive systems designed to run indefinitely. This article revisits foundational issues surrounding control dependence, and develops definitions and algorithms for computing several variations of control dependence that can be directly applied to modern program structures. To provide a foundation for slicing reactive systems, the article proposes a notion of slicing correctness based on weak bisimulation, and proves that some of these new definitions of control dependence generate slices that conform to this notion of correctness. This new framework of control dependence definitions, with corresponding correctness results, is even able to support programs with irreducible control flow graphs. Finally, a variety of properties show that the new definitions conservatively extend classic definitions. These new definitions and algorithms form the basis of the Indus Java slicer, a publicly available program slicer that has been implemented for full Java. © 2007 ACM.",Bisimulation; Control dependence; Indus; Nontermination; Order dependence; Program slicing,Computer program listings; Computer simulation; Graphic methods; Java programming language; Systems analysis; Control dependence; Order dependence; Program slicing; Program structures; High level languages
Introduction to special ESOP'05 issue,2007,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34548449474&doi=10.1145%2f1275497.1275498&partnerID=40&md5=52569843b568fb82b1dcebc6b72ebbee,[No abstract available],,
The trace partitioning abstract domain,2007,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34548461979&doi=10.1145%2f1275497.1275501&partnerID=40&md5=3f3dfc90179a109f1975ae00ba97c896,"In order to achieve better precision of abstract interpretation-based static analysis, we introduce a new generic abstract domain, the trace partitioning abstract domain. We develop a theoretical framework allowing a wide range of instantiations of the domain, proving that all these instantiations give correct results. From this theoretical framework, we go into implementation details of a particular instance developed in the Astrée static analyzer. We show how the domain is automatically configured in Astrée and the gain and cost in terms of performance and precision. © 2007 ACM.",,Computational methods; Computer programming languages; Theorem proving; Abstract domain; Static analyzers; Trace partitioning; Static analysis
A type discipline for authorization policies,2007,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34548463070&doi=10.1145%2f1275497.1275500&partnerID=40&md5=8802db8ec2fb7ee98a50317f21acc2ac,"Distributed systems and applications are often expected to enforce high-level authorization policies. To this end, the code for these systems relies on lower-level security mechanisms such as digital signatures, local ACLs, and encrypted communications. In principle, authorization specifications can be separated from code and carefully audited. Logic programs in particular can express policies in a simple, abstract manner. We consider the problem of checking whether a distributed implementation based on communication channels and cryptography complies with a logical authorization policy. We formalize authorization policies and their connection to code by embedding logical predicates and claims within a process calculus. We formulate policy compliance operationally by composing a process model of the distributed system with an arbitrary opponent process. Moreover, we propose a dependent type system for verifying policy compliance of implementation code. Using Datalog as an authorization logic, we show how to type several examples using policies and present a general schema for compiling policies. © 2007 ACM.",Authorization; Process calculus; Spi calculus; Type systems,Cryptography; Electronic document identification systems; High level languages; Logic programming; Security of data; Authorization logics; Authorization policies; Process calculus; Distributed computer systems
Analysis of modular arithmetic,2007,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34548380503&doi=10.1145%2f1275497.1275504&partnerID=40&md5=31f57dda410791b18fa6a0f177a3c7dd,"We consider integer arithmetic modulo a power of 2 as provided by mainstream programming languages like Java or standard implementations of C. The difficulty here is that, for w > 1, the ring Zm of integers modulo m = 2w has zero divisors and thus cannot be embedded into a field. Not withstanding that, we present intra- and interprocedural algorithms for inferring for every program point u affine relations between program variables valid at u. If conditional branching is replaced with nondeterministic branching, our algorithms are not only sound but also complete in that they detect all valid affine relations in a natural class of programs. Moreover, they run in time linear in the program size and polynomial in the number of program variables and can be implemented by using the same modular integer arithmetic as the target language to be analyzed. We also indicate how our analysis can be extended to deal with equality guards, even in an interprocedural setting. © 2007 ACM.",Abstract interpretation; Affine relation; Interprocedural analysis; Modular arithmetic; Program analysis,C (programming language); Embedded systems; Integer programming; Polynomials; Abstract interpretation; Interprocedural analysis; Modular arithmetic; Program analysis; Java programming language
Static validation of XSL transformations,2007,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34547975870&doi=10.1145%2f1255450.1255454&partnerID=40&md5=1cc1f0e3546f3ad199faaa63fce9cc50,"XSL Transformations (XSLT) is a programming language for defining transformations among XML languages. The structure of these languages is formally described by schemas, for example using DTD or XML Schema, which allows individual documents to be validated. However, existing XSLT tools offer no static guarantees that, under the assumption that the input is valid relative to the input schema, the output of the transformation is valid relative to the output schema. We present a validation technique for XSLT based on the XML graph formalism introduced in the static analysis of JWIG Web services and X ACT XML transformations. Being able to provide static guarantees, we can detect a large class of errors in an XSLT stylesheet at the time it is written instead of later when it has been deployed, and thereby provide benefits similar to those of static type checkers for modern programming languages. Our analysis takes a pragmatic approach that focuses its precision on the essential language features but still handles the entire XSLT language. We evaluate the analysis precision on a range of real stylesheets and demonstrate how it may be useful in practice. © 2007 ACM.",DTD; Static analysis; XML Schema; XSLT,Data structures; Error analysis; Mathematical transformations; Static analysis; Web services; XML; Language features; Stylesheets; XSL Transformations (XSLT); XSLT language; Computer programming languages
"Design, implementation, and evaluation of a compilation server",2007,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34547959591&doi=10.1145%2f1255450.1255451&partnerID=40&md5=13d8c3b91eb744aa6f50954716873fd6,"Modern JVM implementations interleave execution with compilation of hot methods to achieve reasonable performance. Since compilation overhead impacts the execution time of the application and induces run-time pauses, we explore offloading compilation onto a compilation server. In this article, we present the design, implementation, and evaluation of a compilation server that compiles and optimizes Java bytecodes on behalf of its clients. We show that the compilation server provides the following benefits for our benchmark programs: (i) lower execution time by reducing the compilation overhead and by enabling more aggressive optimizations; (ii) lower memory allocation by eliminating allocations due to optimizing compilation and the footprint of the optimizing compiler; (iii) lower execution time of the application due to sharing of profile information across different runs of the same application and runs of different applications. We implemented the compilation server in Jikes RVM, and our results indicate that it can reduce running time by an average of 20.5%, interruptions due to compilation by an average of 81.0%, and dynamic memory allocation by 8.6% for our benchmark programs. Simulation results indicate that our current implementation of the compilation server can handle more than 50 concurrent clients while still allowing them to outperform the best performing adaptive configuration. © 2007 ACM.",Compilation server; Java virtual machine,Codes (symbols); Java programming language; Optimization; Program compilers; Storage allocation (computer); Virtual reality; Bytecodes; Compilation server; Dynamic memory allocation; Interleave execution; Servers
A practical interprocedural dominance algorithm,2007,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34547952848&doi=10.1145%2f1255450.1255452&partnerID=40&md5=9841f6e1307ff6371609b0d5e8e4c274,"Existing algorithms for computing dominators are formulated for control flow graphs of single procedures. With the rise of computing power, and the viability of whole-program analyses and optimizations, there is a growing need to extend the dominator computation algorithms to context-sensitive interprocedural dominators. Because the transitive reduction of the interprocedural dominator graph is not a tree, as in the intraprocedural case, it is not possible to extend existing algorithms directly. In this article, we propose a new algorithm for computing interprocedural dominators. Although the theoretical complexity of this new algorithm is as high as that of a straightforward iterative solution of the data flow equations, our experimental evaluation demonstrates that the algorithm is practically viable, even for programs consisting of several hundred thousands of basic blocks. © 2007 ACM.",Dominators; Interprocedural analysis; Interprocedural control flow graph,Algorithms; Computational complexity; Optimization; Trees (mathematics); Dominators; Interprocedural analysis; Interprocedural control flow graph; Computer programming languages
An efficient on-the-fly cycle collection,2007,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34547944236&doi=10.1145%2f1255450.1255453&partnerID=40&md5=fcdbbc508a3c388f348b34492d387cbf,"A reference-counting garbage collector cannot reclaim unreachable cyclic structures of objects. Therefore, reference-counting collectors either use a backup tracing collector infrequently, or employ a cycle collector to reclaim cyclic structures. We propose a new concurrent cycle collector, one that runs concurrently with the program threads, imposing negligible pauses (of around 1ms) on a multiprocessor. Our new collector combines a state-of-the-art cycle collector [Bacon and Rajan 2001] with sliding-views collectors [Levanoni and Petrank 2001, 2006; Azatchi et al. 2003]. The use of sliding views for cycle collection yields two advantages. First, it drastically reduces the number of cycle candidates, which in turn drastically reduces the work required to record and trace these candidates. Consequentially, a large improvement in cycle collection efficiency is achieved. Second, it eliminates the theoretical termination problem that appeared in the earlier concurrent cycle collector. There, a rare race may delay the reclamation of an unreachable cyclic structure forever. The sliding-views cycle collector guarantees reclamation of all unreachable cyclic structures. © 2007 ACM.",Concurrent cycle collection; Garbage collection; Memory management; Programming languages; Reference counting; Runtime systems,Problem solving; Sliding mode control; Storage allocation (computer); Concurrent cycle collection; Memory management; Reference counting; Runtime systems; Computer programming languages
Mutatis Mutandis: Safe and predictable dynamic software updating,2007,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34547938351&doi=10.1145%2f1255450.1255455&partnerID=40&md5=a8499bd1e9aa1155cd33013f5b8056d4,"This article presents Proteus, a core calculus that models dynamic software updating, a service for fixing bugs and adding features to a running program. Proteus permits a program's type structure to change dynamically but guarantees the updated program remains type-correct by ensuring a property we call con-freeness. We show how con-freeness can be enforced dynamically, and how it can be approximated via a novel static analysis. This analysis can be used to assess the implications of a program's structure on future updates in order to make update success more predictable. We have implemented Proteus for C, and briefly discuss our implementation which we have tested on several well-known programs. © 2007 ACM.",Capability; Dynamic software updating; Proteus; Type inference; Updateability analysis,Approximation theory; C (programming language); Mathematical models; Capability; Dynamic software updating; Type inference; Updateability analysis; Computer software
Transition predicate abstraction and fair termination,2007,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34249902297&doi=10.1145%2f1232420.1232422&partnerID=40&md5=e4b1a18263da530039d56f99a0f06982,"Predicate abstraction is the basis of many program verification tools. Until now, the only known way to overcome the inherent limitation of predicate abstraction to safety properties was to manually annotate the finite-state abstraction of a program. We extend predicate abstraction to transition predicate abstraction. Transition predicate abstraction goes beyond the idea of finite abstract-state programs (and checking the absence of loops). Instead, our abstraction algorithm transforms a program into a finite abstract-transition program. Then a second algorithm checks fair termination. The two algorithms together yield an automated method for the verification of liveness properties under full fairness assumptions (impartiality, justice, and compassion). In summary, we exhibit principles that extend the applicability of predicate abstraction-based program verification to the full set of temporal properties. © 2007 ACM.",Fair termination; Liveness; Software model checking; Transition predicate abstraction,Algorithms; Computer programming; Computer software; Security of data; Fair termination; Finite abstract-state programs; Liveness; Transition predicate abstraction; Abstracting
Saturn: A scalable framework for error detection using Boolean satisfiability,2007,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34249879570&doi=10.1145%2f1232420.1232423&partnerID=40&md5=bbef8168e829ecd5f39b7c858918e424,"This article presents Saturn, a general framework for building precise and scalable static error detection systems. Saturn exploits recent advances in Boolean satisfiability (SAT) solvers and is path sensitive, precise down to the bit level, and models pointers and heap data. Our approach is also highly scalable, which we achieve using two techniques. First, for each program function, several optimizations compress the size of the Boolean formulas that model the control flow and data flow and the heap locations accessed by a function. Second, summaries in the spirit of type signatures are computed for each function, allowing interprocedural analysis without a dramatic increase in the size of the Boolean constraints to be solved. We have experimentally validated our approach by conducting two case studies involving a Linux lock checker and a memory leak checker. Results from the experiments show that our system scales well, parallelizes well, and finds more errors with fewer false positives than previous static error detection systems. © 2007 ACM.",Boolean satisfiability; Error detection; Program analysis,Boolean functions; Constraint theory; Data acquisition; Optimization; Advances in Boolean satisfiability (SAT); Boolean constraints; Building precise; Data flow; Saturn; Error detection
Combinators for bidirectional tree transformations: A linguistic approach to the view-update problem,2007,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34249865033&doi=10.1145%2f1232420.1232424&partnerID=40&md5=ff8e308a04fefd133fac9a099744db4c,"We propose a novel approach to the view-update problem for tree-structured data: a domain-specific programming language in which all expressions denote bidirectional transformations on trees. In one direction, these transformations - -dubbed lenses - -map a concrete tree into a simplified abstract view; in the other, they map a modified abstract view, together with the original concrete tree, to a correspondingly modified concrete tree. Our design emphasizes both robustness and ease of use, guaranteeing strong well-behavedness and totality properties for well-typed lenses. We begin by identifying a natural space of well-behaved bidirectional transformations over arbitrary structures, studying definedness and continuity in this setting. We then instantiate this semantic framework in the form of a collection of lens combinators that can be assembled to describe bidirectional transformations on trees. These combinators include familiar constructs from functional programming (composition, mapping, projection, conditionals, recursion) together with some novel primitives for manipulating trees (splitting, pruning, merging, etc.). We illustrate the expressiveness of these combinators by developing a number of bidirectional list-processing transformations as derived forms. An extended example shows how our combinators can be used to define a lens that translates between a native HTML representation of browser bookmarks and a generic abstract bookmark format. © 2007 ACM.",Bidirectional programming; Harmony; Lenses; View update problem; XML,Computer programming languages; Data acquisition; Functional programming; Linguistics; Problem solving; Robustness (control systems); Trees (mathematics); XML; Arbitrary structures; Bidirectional programming; Tree transformations; Well-typed lenses; Mathematical transformations
ACM Transactions on Programming Languages and Systems: Editorial,2007,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34249895547&doi=10.1145%2f1232420.1232421&partnerID=40&md5=b766bb50189d12f7d695a30911ce0696,[No abstract available],,
METRIC: Memory tracing via dynamic binary rewriting to identify cache inefficiencies,2007,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34247268507&doi=10.1145%2f1216374.1216380&partnerID=40&md5=79c1236baad36ccd34f97fe54635293e,"With the diverging improvements in CPU speeds and memory access latencies, detecting and removing memory access bottlenecks becomes increasingly important. In this work we present METRIC, a software framework for isolating and understanding such bottlenecks using partial access traces. METRIC extracts access traces from executing programs without special compiler or linker support. We make four primary contributions. First, we present a framework for extracting partial access traces based on dynamic binary rewriting of the executing application. Second, we introduce a novel algorithm for compressing these traces. The algorithm generates constant space representations for regular accesses occurring in nested loop structures. Third, we use these traces for offline incremental memory hierarchy simulation. We extract symbolic information from the application executable and use this to generate detailed source-code correlated statistics including per-reference metrics, cache evictor information, and stream metrics. Finally, we demonstrate how this information can be used to isolate and understand memory access inefficiencies. This illustrates a potential advantage of METRIC over compile-time analysis for sample codes, particularly when interprocedural analysis is required. © 2007 ACM.",Cache analysis; Data trace compression; Data trace generation; Dynamic binary rewriting; Program instrumentation,Algorithms; Codes (symbols); Computer simulation; Data storage equipment; Hierarchical systems; Program compilers; Statistics; Cache analysis; Data trace compression; Data trace generation; Dynamic binary rewriting; Program instrumentation; Buffer storage
A provenly correct translation of Fickle into Java,2007,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34247204748&doi=10.1145%2f1216374.1216381&partnerID=40&md5=40dfd42a849c591a21e0479d85200111,"We present a translation from Fickle, a small object-oriented language allowing objects to change their class at runtime, into Java. The translation is provenly correct in the sense that it preserves the static and dynamic semantics. Moreover, it is compatible with separate compilation, since the translation of a Fickle class does not depend on the implementation of used classes. Based on the formal system, we have developed an implementation. The translation turned out to be a more subtle problem than we expected. In this article, we discuss four possible approaches we considered for the design of the translation and to justify our choice, we present formally the translation and proof of preservation of the static and dynamic semantics, and discuss the prototype implementation. Moreover, we outline an alternative translation based on generics that avoids most of the casts (but not all) needed in the previous translation. The language Fickle has undergone and is still undergoing several phases of development. In this article we are discussing the translation of FickleII. © 2007 ACM.",Semantics preserving translation; Type and effect systems,Computer aided language translation; Java programming language; Semantics; Software prototyping; Fickle; Prototype implementation; Semantics preserving translation; Object oriented programming
Slicing as a program transformation,2007,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34247264314&doi=10.1145%2f1216374.1216375&partnerID=40&md5=41b8b7cdfeb05df8831bd5efd0ddc194,"The aim of this article is to provide a unified mathematical framework for program slicing which places all slicing work for sequential programs on a sound theoretical foundation. The main advantage to a mathematical approach is that it is not tied to a particular representation. In fact the mathematics provides a sound basis for any particular representation. We use the WSL (wide-spectrum language) program transformation theory as our framework. Within this framework we define a new semantic relation, semirefinement, which lies between semantic equivalence and semantic refinement. Combining this semantic relation, a syntactic relation (called reduction), and WSL's remove statement, we can give mathematical definitions for backwards slicing, conditioned slicing, static and dynamic slicing, and semantic slicing as program transformations in the WSL transformation theory. A novel technique of encoding operational semantics within a denotational semantics allows the framework to handle operational slicing. The theory also enables the concept of slicing to be applied to nondeterministic programs. These transformations are implemented in the industry-strength FermaT transformation system. © 2007 ACM.",FermaT; Formal methods; Program transformation; Reengineering; Reverse engineering; Slicing,Encoding (symbols); Formal methods; Mathematical transformations; Reengineering; Reverse engineering; Semantics; FermaT; Operational slicing; Program transformation; Slicing; Computer programming languages
Fast online pointer analysis,2007,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34247276506&doi=10.1145%2f1216374.1216379&partnerID=40&md5=cf5fb761c571751aaadb74dae0e41cff,"Pointer analysis benefits many useful clients, such as compiler optimizations and bug finding tools. Unfortunately, common programming language features such as dynamic loading, reflection, and foreign language interfaces, make pointer analysis difficult. This article describes how to deal with these features by performing pointer analysis online during program execution. For example, dynamic loading may load code that is not available for analysis before the program starts. Only an online analysis can analyze such code, and thus support clients that optimize or find bugs in it. This article identifies all problems in performing Andersen's pointer analysis for the full Java language, presents solutions to these problems, and uses a full implementation of the solutions in a Java virtual machine for validation and performance evaluation. Our analysis is fast: On average over our benchmark suite, if the analysis recomputes points-to results upon each program change, most analysis pauses take under 0.1 seconds, and add up to 64.5 seconds. © 2007 ACM.",Class loading; Native interface; Pointer analysis; Reflection,Dynamic loads; Interfaces (computer); Java programming language; Optimization; Problem solving; Class loading; Native interface; Pointer analysis; Virtual machines; Online systems
Termination analysis of logic programs through combination of type-based norms,2007,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34247228022&doi=10.1145%2f1216374.1216378&partnerID=40&md5=f9974365e20e218db7ef83b9f05d3d2e,"This article makes two contributions to the work on semantics-based termination analysis for logic programs. The first involves a novel notion of type-based norm where for a given type, a corresponding norm is defined to count in a term the number of subterms of that type. This provides a collection of candidate norms, one for each type defined in the program. The second enables an analyzer to base termination proofs on the combination of several different norms. This is useful when different norms are better suited to justify the termination of different parts of the program. Application of the two contributions together consists in considering the combination of the type-based candidate norms for a given program. This results in a powerful and practical technique. Both contributions have been introduced into a working termination analyzer. Experimentation indicates that they yield state-of-the-art results in a fully automatic analysis tool, improving with respect to methods that do not use both types and combined norms. © 2007 ACM.",Abstract interpretation; Dataflow analysis; Global analysis; Groundness analysis; Program analysis; Termination analysis,Automation; Data flow analysis; Global optimization; Semantics; Abstract interpretation; Global analysis; Groundness analysis; Program analysis; Termination analysis; Logic programming
Allocating architected registers through differential encoding,2007,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34247264878&doi=10.1145%2f1216374.1216377&partnerID=40&md5=82412e1a2fcbd37fff34eae8d17ce935,"Micro-architecture designers are very cautious about expanding the number of architected and exposed registers in the instruction set because increasing the register field adds to the code size, raises the I-cache and memory pressure, and may complicate the processor pipeline. Especially for low-end processors, encoding space could be extremely limited due to area and power considerations. On the other hand, the number of architected registers exposed to the compiler could directly affect the effectiveness of compiler analysis and optimization. For high-performance computers, register pressure can be higher than the available registers in some regions. This could be due to optimizations like aggressive function inlining, software pipelining, etc. The compiler cannot effectively perform compilation and optimization if only a small number of registers are exposed through the ISA. Therefore, it is crucial that more architected registers are available at the compiler's disposal, without expanding the code size significantly. In this article, we devise a new register encoding scheme, called differential encoding, that allows more registers to be addressed in the operand field of instructions than the direct encoding currently being used. We show that this can be implemented with very low overhead. Based upon differential encoding, we apply it in several ways such that the extra architected registers can benefit the performance. Three schemes are devised to integrate differential encoding with register allocation. We demonstrate that differential register allocation is helpful in improving the performance of both high-end and low-end processors. Moreover, we can combine it with software pipelining to provide more registers and reduce spills. Our results show that differential encoding significantly reduces the number of spills and speeds-up program execution. For a low-end configuration, we achieve over 14% speedup while keeping code size almost unaffected. For a high-end VLIW in-order machine, it can significantly speed-up loops with high register pressure (about 80% speedup) and the overall speedup is about 15%. Moreover, our scheme can be applied in an adaptive manner, making its overhead much smaller. © 2007 ACM.",Architected register; Differential encoding; Register allocation,Buffer storage; Computer architecture; Optimization; Program processors; Resource allocation; Set theory; Architected register; Compiler analysis; Differential encoding; Register allocation; Software pipelining; Encoding (symbols)
A deterministic logical semantics for pure Esterel,2007,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34247227102&doi=10.1145%2f1216374.1216376&partnerID=40&md5=cd2276281cbe3ee11577ec2c6e6a84e7,"Esterel is a synchronous design language for the specification of reactive systems. There exist two main semantics for Esterel. On the one hand, the logical behavioral semantics provides a simple and compact formalization of the behavior of programs using SOS rules. But it does not ensure deterministic deadlock-free executions, as it may define zero, one, or many possible behaviors for a given program and input sequence. Since nondeterministic programs have to be rejected by compilers, this means that it defines behaviors for incorrect programs, which is awkward. On the other hand, the constructive semantics is deterministic (amongst other properties) but at the expense of a much more complex formalism. In this work, we build and thoroughly analyze a new deterministic semantics for Esterel that retains the simplicity of the logical behavioral semantics from which it derives. It defines, at most, one behavior per program and input sequence. We further extend this semantics with the ability to deal with errors so that incorrect programs are no longer (negatively) characterized by a lack of behavior, but (positively) by the existence of an incorrect behavior. In our view, this new semantics, with or without explicit errors, provides a better framework for formal and automated reasoning about Esterel programs. © 2007 ACM.",Structural operational semantics; Synchronous languages,Automation; Computer programming languages; Computer system recovery; Errors; Formal logic; Program compilers; Esterel; Structural operational semantics; Synchronous languages; Semantics
A static type system for JVM access control,2007,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33846636279&doi=10.1145%2f1180475.1180479&partnerID=40&md5=be54e7b0cc54b8f0aa399e467f4442d0,"This article presents a static type system for the Java virtual machine (JVM) code that enforces an access control mechanism similar to that found in a Java implementation. In addition to verifying type consistency of a given JVM code, the type system statically verifies whether the code accesses only those resources that are granted by the prescribed access policy. The type system is proved to be sound with respect to an operational semantics that enforces access control dynamically, similar to Java stack inspection. This result ensures that well-typed code cannot violate access policy. The authors then develop a type inference algorithm and show that it is sound with respect to the type system. These results allow us to develop a static system for JVM access control, without resorting to costly runtime stack inspection. © 2007 ACM.",Access control; JVM; Stack inspection; Type inference; Type system,Algorithms; Control systems; Public policy; Software engineering; Virtual reality; Access control; JVM; Stack inspection; Type inference; Java programming language
Program termination analysis in polynomial time,2007,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33846634514&doi=10.1145%2f1180475.1180480&partnerID=40&md5=ec8354ccec232359e40d36f00ec147ce,"A size-change termination algorithm takes as input abstract information about a program in the form of size-change graphs and uses it to determine whether any infinite computation would imply that some data decrease in size infinitely. Since such an infinite descent is presumed impossible, this proves program termination. The property of the graphs that implies program termination is called SCT. There are many examples of practical programs whose termination can be verified by creating size-change graphs and testing them for SCT.The size-change graph abstraction is useful because the graphs often carry sufficient information to deduce termination, and at the same time are simple enough to be analyzed automatically. However, there is a tradeoff between the completeness and efficiency of this analysis, and complete algorithms in the literature can easily be pushed to an exponential combinatorial search by certain patterns in the graph structures.We therefore propose a novel algorithm to detect common forms of parameter-descent behavior efficiently. Specifically, we target lexicographic descent, multiset descent, and min- and max-descent. Our algorithm makes it possible to verify practical instances of SCT while guarding against unwarranted combinatorial search. It has worst-case time complexity cubic in the input size, and its effectiveness is demonstrated empirically using a test suite of over 90 programs. © 2007 ACM.",Abstraction; Program analysis; Size-change graph; Size-change termination; Termination analysis,Algorithms; Computational complexity; Graph theory; Information retrieval; Program analysis; Size-change graph; Size-change termination; Termination analysis; Abstracting
PPMexe: Program compression,2007,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33846574622&doi=10.1145%2f1180475.1180478&partnerID=40&md5=b738059aa839198f1e829874172dd091,"With the emergence of software delivery platforms, code compression has become an important system component that strongly affects performance. This article presents PPMexe, a compression mechanism for program binaries that analyzes their syntax and semantics to achieve superior compression ratios. We use the generic paradigm of prediction by partial matching (PPM) as the foundation of our compression codec. PPMexe combines PPM with two preprocessing steps: (i) instruction rescheduling to improve prediction rates and (ii) heuristic partitioning of a program binary into streams with high autocorrelation. We improve the traditional PPM algorithm by (iii) using an additional alphabet of frequent variable-length supersymbols extracted from the input stream of fixed-length symbols. In addition, PPMexe features (iv) a low-overhead mechanism that enables decompression starting from an arbitrary instruction of the executable, a property pivotal for runtime software delivery. We implemented PPMexe for x86 binaries and tested it on several large applications. Binaries compressed using PPMexe were 18 - 24% smaller than files created using off-the-shelf PPMD, one of the best available compressors. © 2007 ACM.",Instruction scheduling; Prediction by partial matching; Random access compression; Software compression; Software distribution,Binary sequences; Computer programming languages; Computer software; Heuristic methods; Instruction scheduling; Prediction by partial matching; Random access compression; Software compression; Software distribution; Data compression
Flow-insensitive type qualifiers,2006,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33845512960&doi=10.1145%2f1186632.1186635&partnerID=40&md5=5701b107f5e76f0c9bca62f39c17c505,"We describe flow-insensitive type qualifiers, a lightweight, practical mechanism for specifying and checking properties not captured by traditional type systems. We present a framework for adding new, user-specified type qualifiers to programming languages with static type systems, such as C and Java. In our system, programmers add a few type qualifier annotations to their program, and automatic type qualifier inference determines the remaining qualifiers and checks the annotations for consistency. We describe a tool CQual for adding type qualifiers to the C programming language. Our tool CQual includes a visualization component for displaying browsable inference results to the programmer. Finally, we present several experiments using our tool, including inferring const qualifiers, finding security vulnerabilities in several popular C programs, and checking initialization data usage in the Linux kernel. Our results suggest that inference and visualization make type qualifiers lightweight, that type qualifier inference scales to large programs, and that type qualifiers are applicable to a wide variety of problems. © 2006 ACM.",Const; Constraints; Security; Static analysis; Taint; Type qualifiers; Types,Computer aided software engineering; Computer operating systems; Visualization; Web browsers; Flow-insensitive type qualifiers; Static analysis; Type qualifiers; Computer programming languages
Type-based publish/subscribe: Concepts and experiences,2007,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33846609442&doi=10.1145%2f1180475.1180481&partnerID=40&md5=8c524e7d0c0f2adb648c98fabf5631d1,"A continuously increasing number of interconnected computer devices makes the requirement for programming abstractions for remote one-to-many interaction yet more stringent. The publish/subscribe paradigm has been advocated as a candidate abstraction for such one-to-many interaction at large scale. Common practices in publish/subscribe, however, include low-level abstractions which hardly leverage type safety, and provide only poor support for object encapsulation. This tends to put additional burden on software developers; guarantees such as the aforementioned type safety and object encapsulation become of increasing importance with an accrued number of software components, which modern applications also involve, besides an increasing number of hardware components.Type-based publish/subscribe (TPS) is a high-level variant of the publish/subscribe paradigm which aims precisely at providing guarantees such as type safety and encapsulation. We present the rationale and principles underlying TPS, as well as two implementations in Java: the first based on a specific extension of the Java language, and a second novel implementation making use of recent general-purpose features of Java, such as generics and behavioral reflection. We compare the two approaches, thereby evaluating the aforementioned features - -as well as additional features which have been included in the most recent Java 1.5 release - -in the context of distributed and concurrent programming. We discuss the benefits of alternative programming languages and features for implementing TPS. By revisiting alternative abstractions for distributed programming, including classic and recent ones, we extend our investigations to programming language support for distributed programming in general, pointing out that overall, the support in current mainstream programming languages is still insufficient. © 2007 ACM.",Abstraction; Distribution; Generics; Java; Publish/subscribe; Reflection; Type,Abstracting; Computer programming; Distributed computer systems; Java programming language; Software engineering; Interconnected computer devices; Object encapsulation; Type-based publish/subscribe (TPS); Large scale systems
Interprocedural slicing of multithreaded programs with applications to Java,2006,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33845521068&doi=10.1145%2f1186632.1186636&partnerID=40&md5=bad1a3e3b9ed3166479e54e0f16fdadb,"Slicing is a well-known program reduction technique where for a given program P and a variable of interest v at some statement P in the program, a program slice contains those set of statements belonging to P that affect v. This article presents two algorithms for interprocedural slicing of concurrent programs - a context-insensitive algorithm and a context-sensitive algorithm. The context-insensitive algorithm is efficient and correct (it includes every statement that may affect the slicing criterion) but is imprecise since it may include certain extra statements that are unnecessary. Precise slicing has been shown to be undecidable for concurrent programs. However, the context-sensitive algorithm computes correct and reasonably precise slices, but has a worst-case exponential-time complexity. Our context-sensitive algorithm computes a closure of dependencies while ensuring that statements sliced in each thread belong to a realizable path in that thread.A realizable path in a thread with procedure calls is one that reflects the fact that when a procedure finishes, execution returns to the site of the most recently executed call in that thread. One of the novelties of this article is a practical solution to determine whether a given set of statements in a thread may belong to a realizable path. This solution is precise even in the presence of recursion and long call chains in the flow graph.The slicing algorithms are applicable to concurrent programs with shared memory, interleaving semantics, explicit wait/notify synchronization and monitors. We first give a solution for a simple model of concurrency and later show how to extend the solution to the Java concurrency model. We have implemented the algorithms for Java bytecode and give experimental results. © 2006 ACM.",Context-sensitivity; Data dependence; Interference dependence; Multithreading; Program slicing; Strongly connected regions,Algorithms; Computer programming; Mathematical models; Semantics; Synchronization; Interference dependence; Multithreading; Program slicing; Strongly connected regions; Java programming language
A constraint-based approach to guarded algebraic data types,2007,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33846595127&doi=10.1145%2f1180475.1180476&partnerID=40&md5=db7ed694a88db828cb4ad37fd15506bc,"We study HMG(X), an extension of the constraint-based type system HM(X) with deep pattern matching, polymorphic recursion, and guarded algebraic data types. Guarded algebraic data types subsume the concepts known in the literature as indexed types, guarded recursive datatype constructors, (first-class) phantom types, and equality qualified types, and are closely related to inductive types. Their characteristic property is to allow every branch of a case construct to be typechecked under different assumptions about the type variables in scope. We prove that HMG(X) is sound and that, provided recursive definitions carry a type annotation, type inference can be reduced to constraint solving. Constraint solving is decidable, at least for some instances of X, but prohibitively expensive. Effective type inference for guarded algebraic data types is left as an issue for future research. © 2007 ACM.",Constraint-based type inference; GADTs; Generalized algebraic data types,Algebra; Inference engines; Pattern matching; Recursive functions; Constraint solving; Constraint-based type inference; Generalized algebraic data types; Constraint theory
On minimizing materializations of array-valued temporaries,2006,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33845523535&doi=10.1145%2f1186632.1186637&partnerID=40&md5=ba2924c87b57900b7b4ea0f0a003ada7,"We consider the analysis and optimization of code utilizing operations and functions operating on entire arrays. Models are developed for studying the minimization of the number of materializations of array-valued temporaries in basic blocks, each consisting of a sequence of assignment statements involving array-valued variables. We derive lower bounds on the number of materializations required, and develop several algorithms minimizing the number of materializations, subject to a simple constraint on allowable statement rearrangement. In contrast, we also show that when statement rearrangement is unconstrained, minimizing the number of materializations becomes NP-complete, even for very simple basic blocks. © 2006 ACM.",Array operations; Array temporary materializations; Compiler optimization; Copy minimization; Program transformation,Algorithms; Codes (symbols); Optimization; Program compilers; Array operations; Array temporary materializations; Compiler optimization; Program transformation; Computer programming
Profile-based pretenuring,2007,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33846561462&doi=10.1145%2f1180475.1180477&partnerID=40&md5=6755271e114f1fdb0c72b7225c4552cc,"Pretenuring can reduce copying costs in garbage collectors by allocating long-lived objects into regions that the garbage collector will rarely, if ever, collect. We extend previous work on pretenuring as follows: (1) We produce pretenuring advice that is neutral with respect to the garbage collector algorithm and configuration. We thus can and do combine advice from different applications. We find for our benchmarks that predictions using object lifetimes at each allocation site in Java programs are accurate, which simplifies the pretenuring implementation. (2) We gather and apply advice to both applications and Jikes RVM, a compiler and runtime system for Java written in Java. Our results demonstrate that building combined advice into Jikes RVM from different application executions improves performance, regardless of the application Jikes RVM is compiling and executing. This build-time advice thus gives user applications some benefits of pretenuring, without any application profiling. No previous work uses profile feedback to pretenure in the runtime system. (3) We find that application-only advice also consistently improves performance, but that the combination of build-time and application-specific advice is almost always noticeably better. (4) Our same advice improves the performance of generational, Older First, and Beltway collectors, illustrating that it is collector neutral. (5) We include an immortal allocation space in addition to a nursery and older generation, and show that pretenuring to immortal space has substantial benefit. © 2007 ACM.",Garbage collection; Lifetime prediction; Pretenuring; Profiling,Algorithms; Benchmarking; Computer applications; Java programming language; Storage allocation (computer); Garbage collection; Lifetime prediction; Pretenuring; Profiling; Data storage equipment
Adaptive functional programming,2006,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33845541245&doi=10.1145%2f1186632.1186634&partnerID=40&md5=1a9e993088efc20e560b1468a092e029,"We present techniques for incremental computing by introducing adaptive functional programming. As an adaptive program executes, the underlying system represents the data and control dependences in the execution in the form of a dynamic dependence graph. When the input to the program changes, a change propagation algorithm updates the output and the dynamic dependence graph by propagating changes through the graph and re-executing code where necessary. Adaptive programs adapt their output to any change in the input, small or large.We show that adaptivity techniques are practical by giving an efficient implementation as a small ML library. The library consists of three operations for making a program adaptive, plus two operations for making changes to the input and adapting the output to these changes. We give a general bound on the time it takes to adapt the output, and based on this, show that an adaptive Quicksort adapts its output in logarithmic time when its input is extended by one key.To show the safety and correctness of the mechanism we give a formal definition of AFL, a call-by-value functional language extended with adaptivity primitives. The modal type system of AFL enforces correct usage of the adaptivity mechanism, which can only be checked at run time in the ML library. Based on the AFL dynamic semantics, we formalize thechange-propagation algorithm and prove its correctness. © 2006 ACM.",Adaptive computation; Dynamic algorithms; Incremental computation,Algorithms; Digital libraries; Graph theory; Mathematical models; Adaptive computation; Dynamic algorithms; Incremental computation; Computer programming languages
Calculational semantics: Deriving programming theories from equations by functional predicate calculus,2006,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33747145596&doi=10.1145%2f1146809.1146814&partnerID=40&md5=3217340f4311bf8cb2a58e45df54cbb0,"The objects of programming semantics, namely, programs and languages, are inherently formal, but the derivation of semantic theories is all too often informal, deprived of the benefits of formal calculation ""guided by the shape of the formulas."" Therefore, the main goal of this article is to provide for the study of semantics an approach with the same convenience and power of discovery that calculus has given for many years to applied mathematics, physics, and engineering. The approach uses functional predicate calculus and concrete generic functionals; in fact, a small part suffices. Application to a semantic theory proceeds by describing program behavior in the simplest possible way, namely by program equations, and discovering the axioms of the theory as theorems by calculation. This is shown in outline for a few theories, and in detail for axiomatic semantics, fulfilling a second goal of this article. Indeed, a chafing problem with classical axiomatic semantics is that some axioms are unintuitive at first, and that justifications via denotational semantics are too elaborate to be satisfactory. Derivation provides more transparency. Calculation of formulas for ante- and postconditions is shown in general, and for the major language constructs in particular. A basic problem reported in the literature, whereby relations are inadequate for handling nondeterminacy and termination, is solved here through appropriately defined program equations. Several variants and an example in mathematical analysis are also presented. One conclusion is that formal calculation with quantifiers is one of the most important elements for unifying continuous and discrete mathematics in general, and traditional engineering with computing science, in particular. © 2006 ACM.",Assignment; Axiomatic semantics; Calculational reasoning,Computation theory; Computer programming languages; Function evaluation; Natural sciences computing; Problem solving; Assignment; Axiomatic semantics; Calculational reasoning; Semantic theories; Semantics
Message analysis for concurrent programs using message passing,2006,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33747095859&doi=10.1145%2f1146809.1146813&partnerID=40&md5=6a0797c5d4096310bf2af4fd453f9611,"We describe an analysis-driven storage allocation scheme for concurrent systems that use message passing with copying semantics. The basic principle is that in such a system, data which is not part of any message does not need to be allocated in a shared data area. This allows for the deallocation of thread-specific data without requiring global synchronization and often without even triggering garbage collection. On the other hand, data that is part of a message should preferably be allocated on a shared area since this allows for fast (O(1)) interprocess communication that does not require actual copying. In the context of a dynamically typed, higher-order concurrent functional language, we present a static message analysis which guides the allocation. As shown by our performance evaluation, conducted using a production-quality language implementation, the analysis is effective enough to discover most data which is to be used as a message, and to allow the allocation scheme to combine the best performance characteristics of both a process-centric and a communal memory architecture. © 2006 ACM.",Concurrent languages; Erlang; Message passing; Runtime systems; Static analysis,Computer programming languages; Concurrent engineering; Data reduction; Semantics; Concurrent languages; Erlang; Message passing; Runtime systems; Static analysis; Storage allocation (computer)
"A machine-checked model for a Java-like language, virtual machine, and compiler",2006,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33747124759&doi=10.1145%2f1146809.1146811&partnerID=40&md5=38d714ad4937c88cc6dfdc752f3f8807,"We introduce Jinja, a Java-like programming language with a formal semantics designed to exhibit core features of the Java language architecture. Jinja is a compromise between the realism of the language and the tractability and clarity of its formal semantics. The following aspects are formalised: a big and a small step operational semantics for Jinja and a proof of their equivalence, a type system and a definite initialisation analysis, a type safety proof of the small step semantics, a virtual machine (JVM), its operational semantics and its type system, a type safety proof for the JVM; a bytecode verifier, that is, a data flow analyser for the JVM, a correctness proof of the bytecode verifier with respect to the type system, and a compiler and a proof that it preserves semantics and well-typedness. The emphasis of this work is not on particular language features but on providing a unified model of the source language, the virtual machine, and the compiler. The whole development has been carried out in the theorem prover Isabelle/HOL. © 2006 ACM.",Java; Operational semantics; Theorem proving,Mathematical models; Program compilers; Semantics; Theorem proving; Virtual reality; Bytecode verifiers; Java; Operational semantics; Java programming language
Variant parametric types: A flexible subtyping scheme for generics,2006,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33748563968&doi=10.1145%2f1152649.1152650&partnerID=40&md5=67ebc8ede7f87d2de1addae0466367ff,"We develop the mechanism of variant parametric types as a means to enhance synergy between parametric and inclusion polymorphism in object-oriented programming languages. Variant parametric types are used to control both the subtyping between different instantiations of one generic class and the accessibility of their fields and methods. On one hand, one parametric class can be used to derive covariant types, contravariant types, and bivariant types (generally called variant parametric types) by attaching a variance annotation to a type argument. On the other hand, the type system prohibits certain method/field accesses, according to variance annotations, when these accesses may otherwise make the program unsafe. By exploiting variant parametric types, a programmer can write generic code abstractions that work on a wide range of parametric types in a safe manner. For instance, a method that only reads the elements of a container of numbers can be easily modified so as to accept containers of integers, floating-point numbers, or any subtype of the number type. Technical subtleties in typing for the proposed mechanism are addressed in terms of an intuitive correspondence between variant parametric and bounded existential types. Then, for a rigorous argument of correctness of the proposed typing rules, we extend Featherweight GJ - an existing formal core calculus for Java with generics - with variant parametric types and prove type soundness. © 2006 ACM.",Generic classes; Java; Language design; Language semantics; Subtyping; Variance,Computer aided design; Computer programming languages; Control systems; Parameter estimation; Semantics; Generic classes; Language design; Language semantics; Subtyping; Variance; Object oriented programming
Right nulled GLR parsers,2006,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33747117006&doi=10.1145%2f1146809.1146810&partnerID=40&md5=3412261bb0a3f523985006ece0152088,"The right nulled generalized LR parsing algorithm is a new generalization of LR parsing which provides an elegant correction to, and extension of, Tomita's GLR methods whereby we extend the notion of a reduction in a shift-reduce parser to include right nulled items. The result is a parsing technique which runs in linear time on LR(1) grammars and whose performance degrades gracefully to a polynomial bound in the presence of nonLR(1) rules. Compared to other GLR-based techniques, our algorithm is simpler and faster. © 2006 ACM.",General context-free grammars; Generalized LR parsing,Algorithms; Polynomials; General context-free grammars; Generalized LR parsing; Shift-reduce parsers; Context free grammars
Controlling garbage collection and heap growth to reduce the execution time of java applications,2006,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33748573985&doi=10.1145%2f1152649.1152652&partnerID=40&md5=f974a09a91903244bec8cbb2bf4ef5b2,"In systems that support garbage collection, a tension exists between collecting garbage too frequently and not collecting it frequently enough. Garbage collection that occurs too frequently may introduce unnecessary overheads at the risk of not collecting much garbage during each cycle. On the other hand, collecting garbage too infrequently can result in applications that execute with a large amount of virtual memory (i.e., with a large footprint) and suffer from increased execution times due to paging. In this article, we use a large set of Java applications and the highly tuned and widely used Boehm-Demers-Weiser (BDW) conservative mark-and-sweep garbage collector to experimentally examine the extent to which the frequency of garbage collection impacts an application's execution time, footprint, and pause times. We use these results to devise some guidelines for controlling garbage collection and heap growth in a conservative garbage collector in order to minimize application execution times. Then we describe new strategies for controlling garbage collection and heap growth that impact not only the frequency with which garbage collection occurs but also the points at which it occurs. Experimental results demonstrate that when compared with the existing approach used in the standard BDW collector, our new strategy can significantly reduce application execution times. Our goal is to obtain a better understanding of how to control garbage collection and heap growth for an individual application executing in isolation. These results can be applied in a number of highperformance computing and server environments, in addition to some single-user environments. This work should also provide insights into how to make better decisions that impact garbage collection in multiprogrammed environments. © 2006 ACM.",Garbage collection; Heap growth; Implementation; Java; Memory management; Performance measurement; Programming languages,Control systems; Data storage equipment; Decision making; Information management; Virtual reality; Garbage collection; Heap growth; Implementation; Memory management; Performance measurement; Java programming language
Exploiting reference idempotency to reduce speculative storage overflow,2006,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33748522708&doi=10.1145%2f1152649.1152653&partnerID=40&md5=6f57824430950b8261be37b8bbeca188,"Recent proposals for multithreaded architectures employ speculative execution to allow threads with unknown dependences to execute speculatively in parallel. The architectures use hardware speculative storage to buffer speculative data, track data dependences and correct incorrect executions through roll-backs. Because all memory references access the speculative storage, current proposals implement speculative storage using small memory structures to achieve fast access. The limited capacity of the speculative storage causes considerable performance loss due to speculative storage overflow whenever a thread's speculative state exceeds the speculative storage capacity. Larger threads exacerbate the overflow problem but are preferable to smaller threads, as larger threads uncover more parallelism. In this article, we discover a new program property called memory reference idempotency, Idempotent references are guaranteed to be eventually corrected, though the references may be temporarily incorrect in the process of speculation. Therefore, idempotent references, even from nonparallelizable program sections, need not be tracked in the speculative storage, and instead can directly access nonspeculative storage (i.e., conventional memory hierarchy). Thus, we reduce the demand for speculative storage space in large threads. We define a formal framework for reference idempotency and present a novel compiler-assisted speculative execution model. We prove the necessary and sufficient conditions for reference idempotency using our model. We present a compiler algorithm to label idempotent memory references for the hardware. Experimental results show that for our benchmarks, over 60% of the references in nonparallelizable program sections are idempotent. © 2006 ACM.",Compiler-assisted speculative execution; Idempotent references; Speculation,Algorithms; Computer hardware; Data storage equipment; Data structures; Hierarchical systems; Mathematical models; Compiler-assisted speculative execution; Idempotent references; Speculation; Computer architecture
An algebraic array shape inference system for MATLAB®,2006,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33748521244&doi=10.1145%2f1152649.1152651&partnerID=40&md5=5c071db0719800327b5bef1e669e2863,"The problem of inferring array shapes ahead of time in languages that exhibit both implicit and dynamic typing is a critical one because the ramifications of its solution are the better organization of array storage through compaction and reuse, and the generation of high-performance code through specialization by shape. This article addresses the problem in a prototypical implicitly and dynamically typed array language called MATLAB. The approach involves modeling the language's shape semantics using an algebraic system, and applying term rewriting techniques to evaluate expressions under this algebra. Unlike prior efforts at array shape determination, this enables the deduction of valuable shape information even when array extents are compile-time unknowns. Furthermore, unlike some previous methods, our approach doesn't impose monotonicity requirements on an operator's shape semantics. The work also describes an inference methodology and reports measurements from a type inference engine called MAGICA. In a benchmark suite of 17 programs, the shape inference subsystem in MAGICA detected the equivalence of over 61% of the symbolic shapes in six programs, and over 57% and 37% of the symbolic shapes in two others. In the remaining nine programs, all array shapes were inferred to be compile-time constants. © 2006 ACM.",Shape algebras; Term rewriting; Typeless array languages,Algebra; Information analysis; Problem solving; Search engines; Semantics; Shape algebras; Term rewriting; Typeless array languages; Computer programming languages
Fast partial evaluation of pattern matching in strings,2006,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33747147128&doi=10.1145%2f1146809.1146812&partnerID=40&md5=8e4f451f84b15d894643082f5cee40a6,"We show how to obtain all of Knuth, Morris, and Pratt's linear-time string matcher by specializing a quadratic-time string matcher with respect to a pattern string. Although it has been known for fifteen years how to obtain this linear matcher by partial evaluation of a quadratic one, how to obtain it in linear time has remained an open problem. Obtaining a linear matcher by the partial evaluation of a quadratic one is achieved by performing its backtracking at specialization time and memoizing its results. We show (1) how to rewrite the source matcher such that its static intermediate computations can be shared at specialization time and (2) how to extend the memoization capabilities of a partial evaluator to static functions. Such an extended partial evaluator, if its memoization is implemented efficiently, specializes the rewritten source matcher in linear time. Finally, we show that the method also applies to a variant of Boyer and Moore's string matcher. © 2006 ACM.",Memoization; Partial evaluation; String matching,Computation theory; Computer programming languages; Functions; Problem solving; Backtracking; Memoization; Partial evaluation; String matching; Character recognition
"MultiJava: Design rationale, compiler implementation, and applications",2006,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745444346&doi=10.1145%2f1133651.1133655&partnerID=40&md5=3e3d4727234cba895c4dd49c7ee84cfd,"MultiJava is a conservative extension of the Java programming language that adds symmetric multiple dispatch and open classes. Among other benefits, multiple dispatch provides a solution to the binary method problem. Open classes provide a solution to the extensibility problem of object-oriented programming languages, allowing the modular addition of both new types and new operations to an existing type hierarchy. This article illustrates and motivates the design of MultiJava and describes its modular static typechecking and modular compilation strategies. Although MultiJava extends Java, the key ideas of the language design are applicable to other object-oriented languages, such as C# and C++, and even, with some modifications, to functional languages such as ML. This article also discusses the variety of application domains in which MultiJava has been successfully used by others, including pervasive computing, graphical user interfaces, and compilers. MultiJava allows users to express desired programming idioms in a way that is declarative and supports static typechecking, in contrast to the tedious and type-unsafe workarounds required in Java. MultiJava also provides opportunities for new kinds of extensibility that are not easily available in Java. © 2006 ACM.",Encapsulation; Extensible classes; Extensible external methods; External methods; Generic functions; Method families; Modularity; Multimethods; Multiple dispatch; Object-oriented programming languages; Open classes; Open objects; Single dispatch,Binary codes; Graphical user interfaces; Modula (programming language); Object oriented programming; Problem solving; Program compilers; Extensible classes; Extensible external methods; External methods; Generic functions; Method families; Multimethods; Multiple dispatch; Open classes; Open objects; Single dispatch; Java programming language
Effective sign extension elimination for Java,2006,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745351338&doi=10.1145%2f1111596.1111599&partnerID=40&md5=eadf8a5540d4c6d40e91c46a0b3d974e,"Computer designs are shifting from 32-bit architectures to 64-bit architectures, while most of the programs available today are still designed for 32-bit architectures. Java, for example, specifies the frequently used ""int"" as a 32-bit signed integer. If such Java programs are executed on a 64-bit architecture, many 32-bit signed integers must be sign-extended to 64-bit signed integers for correct operations. This causes serious performance overhead. In this article, we present a fast and effective algorithm for eliminating sign extensions. We implemented this algorithm in the IBM Java Just-in-Time (JIT) compiler for IA-64. Our experimental results show that our algorithm effectively eliminates the majority of sign extensions. They also show that it improves performance by 6.9% for jBYTEmark and 3.3% for SPECjvm98 over the previously known best algorithm, while it increases JIT compilation time by only 0.11%. © 2006 ACM.",64-bit architecture; IA-64; Java; JIT compilers; Sign extension,Algorithms; Computer aided design; Computer architecture; Integer programming; Program compilers; 64-bit architecture; IA-64; JIT compilers; Sign extension; Java programming language
A region-based compilation technique for dynamic compilers,2006,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745411475&doi=10.1145%2f1111596.1111600&partnerID=40&md5=d7fa3a5b282a39f84eea4cb7b1736608,"Method inlining and data flow analysis are two major optimization components for effective program transformations, but they often suffer from the existence of rarely or never executed code contained in the target method. One major problem lies in the assumption that the compilation unit is partitioned at method boundaries. This article describes the design and implementation of a region-based compilation technique in our dynamic optimization framework, in which the compiled regions are selected as code portions without rarely executed code. The key parts of this technique are the region selection, partial inlining, and region exit handling. For region selection, we employ both static heuristics and dynamic profiles to identify and eliminate rare sections of code. The region selection process and method inlining decisions are interwoven, so that method inlining exposes other targets for region selection, while the region selection in the inline target conserves the inlining budget, allowing more method inlining to be performed. The inlining process can be performed for parts of a method, not just for the entire body of the method. When the program attempts to exit from a region boundary, we trigger recompilation and then use on-stack replacement to continue the execution from the corresponding entry point in the recompiled code. We have implemented these techniques in our Java JIT compiler, and conducted a comprehensive evaluation. The experimental results show that our region-based compilation approach achieves approximately 4% performance improvement on average, while reducing the compilation overhead by 10% to 30%, in comparison to the traditional method-based compilation techniques. © 2006 ACM.",Dynamic compilation; JIT compiler; On-stack replacement; Partial inlining; Region-based compilation,Computer architecture; Data reduction; Heuristic methods; Java programming language; Optimization; Dynamic compilation; JIT compiler; On-stack replacement; Partial inlining; Region-based compilation; Program compilers
An on-the-fly reference-counting garbage collector for java,2006,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745393872&doi=10.1145%2f1111596.1111597&partnerID=40&md5=27fd1c241d1e9a841ebcb20566c2ebcd,"Reference-counting is traditionally considered unsuitable for multiprocessor systems. According to conventional wisdom, the update of reference slots and reference-counts requires atomic or synchronized operations. In this work we demonstrate this is not the case by presenting a novel reference-counting algorithm suitable for a multiprocessor system that does not require any synchronized operation in its write barrier (not even a compare-and-swap type of synchronization). A second novelty of this algorithm is that it allows eliminating a large fraction of the reference-count updates, thus, drastically reducing the reference-counting traditional overhead. This article includes a full proof of the algorithm showing that it is safe (does not reclaim live objects) and live (eventually reclaims all unreachable objects). We have implemented our algorithm on Sun Microsystems' Java Virtual Machine (JVM) 1.2.2 and ran it on a four-way IBM Netfinity 8500R server with 550-MHz Intel Pentium III Xeon and 2 GB of physical memory. Our results show that the algorithm has an extremely low latency and throughput that is comparable to the stop-the-world mark and sweep algorithm used in the original JVM. © 2006 ACM.",Garbage collection; Memory management; Programming languages; Reference-counting,Algorithms; Computer programming languages; Data storage equipment; Electronic communities; Multiprocessing systems; Synchronization; Garbage collection; Memory management; Reference-counting; Java programming language
Computability classes for enforcement mechanisms,2006,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745329295&doi=10.1145%2f1111596.1111601&partnerID=40&md5=7931ecbbef0cd34e5745a8447bbb5da7,"A precise characterization of those security policies enforceable by program rewriting is given. This also exposes and rectifies problems in prior work, yielding a better characterization of those security policies enforceable by execution monitors as well as a taxonomy of enforceable security policies. Some but not all classes can be identified with known classes from computational complexity theory. © 2006 ACM.",Edit automata; Execution monitoring; Inlined reference monitoring; Program rewriting; Reference monitors; Security automata,Computability and decidability; Computational complexity; Computer programming; Problem solving; Edit automata; Execution monitoring; Inlined reference monitoring; Program rewriting; Reference monitors; Security automata; Security of data
Type inference for unique pattern matching,2006,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745396295&doi=10.1145%2f1133651.1133652&partnerID=40&md5=dfe43e5a0f81fa5f589b29c81161c0ae,"Regular expression patterns provide a natural, declarative way to express constraints on semistructured data and to extract relevant information from it. Indeed, it is a core feature of the programming language Perl, surfaces in various UNIX tools such as sad and awk, and has recently been proposed in the context of the XML programming language XDuce. Since regular expressions can be ambiguous in general, different disambiguation policies have been proposed to get a unique matching strategy. We formally define the matching semantics under both (1) the POSIX, and (2) the first and longest match disambiguation strategies. We show that the generally accepted method of defining the longest match in terms of the first match and recursion does not conform to the natural notion of longest match. We continue by solving the type inference problem for both disambiguation strategies, which consists of calculating the set of all subparts of input values a subexpression can match under the given policy. © 2006 ACM.",Disambiguation policies; Pattern matching; Programming languages; XML,Computer operating systems; Computer programming languages; Constraint theory; Data structures; Inference engines; Problem solving; Disambiguation policies; Expression patterns; Relevant information; Unique matching; Pattern matching
EDO: Exception-Directed Optimization in Java,2006,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745341735&doi=10.1145%2f1111596.1111598&partnerID=40&md5=ba2e635d9c9f66d9ba844a3d0b48665f,"Optimizing exception handling is critical for programs that frequently throw exceptions. We observed that there are many such exception-intensive programs written in Java. There are two commonly used exception handling techniques, stack unwinding and stack cutting. Stack unwinding optimizes the normal path by leaving the exception handling path unoptimized, while stack cutting optimizes the exception handling path by adding extra work to the normal path. However, there has been no single exception handling technique to optimize the exception handling path without incurring any overhead to the normal path. We propose a new technique called Exception-Directed Optimization (EDO) that optimizes exception-intensive programs without slowing down exception-minimal programs. It is a feedback-directed dynamic optimization consisting of three steps: exception path profiling, exception path inlining, and throw elimination. Exception path profiling attempts to detect hot exception paths. Exception path inlining embeds every hot exception path into the corresponding catching method. Throw elimination replaces a throw with a branch to the corresponding handler. We implemented EDO in IBM's production Just-in-Time compiler and made several experiments. In summary, it improved the performance of exception-intensive programs by up to 18.3% without decreasing the performance of exception-minimal programs for SPECjvm98. We also found an opportunity for performance improvement using EDO in the startup of a Java application server. © 2006 ACM.",Dynamic compilers; Exception handling; Feedback-directed dynamic optimization; Inlining,Feedback; Java programming language; Motion planning; Performance; Program compilers; Servers; Dynamic compilers; Exception handling; Feedback-directed dynamic optimization; Inlining; Optimization
Generating object lifetime traces with Merlin,2006,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745341736&doi=10.1145%2f1133651.1133654&partnerID=40&md5=75d36d07782852aae1dd9f756ce261d4,"Programmers are writing a rapidly growing number of programs in object-oriented languages, such as Java and C#, that require garbage collection. Garbage collection traces and simulation speed up research by enabling deeper understandings of object lifetime behavior and quick exploration and design of new garbage collection algorithms. When generating perfect traces, the brute-force method of computing object lifetimes requires a whole-heap garbage collection at every potential collection point in the program. Because this process is prohibitively expensive, researchers often use granulated traces by collecting only periodically, for example, every 32 KB of allocation. We extend the state of the art for simulating garbage collection algorithms in two ways. First, we develop a systematic methodology for simulation studies of copying garbage collection and present results showing the effects of trace granularity on these simulations. We show that trace granularity often distorts simulated garbage collection results compared with perfect traces. Second, we present and measure the performance of a new algorithm called Merlin for computing object lifetimes. Merlin timeatamps objects and later uses the timestamps of dead objects to reconstruct when they died. The Merlin algorithm piggybacks on garbage collections performed by the base system. Experimental results show that Merlin can generate traces over two orders of magnitude faster than the brute-force method which collects after every object allocation. We also use Merlin to produce visualizations of heap behavior that expose new object lifetime behaviors. © 2006 ACM.",Garbage collection; Object lifetime analysis; Trace design; Trace generation,Algorithms; Computer simulation; Data mining; Object oriented programming; Object recognition; Resource allocation; Garbage collection; Object lifetime analysis; Trace design; Trace generation; Computer programming
Quantified types in an imperative language,2006,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745400931&doi=10.1145%2f1133651.1133653&partnerID=40&md5=a981f2a0c4321f3fd0c23f8236869a72,"We describe universal types, existential types, and type constructors in Cyclone, a strongly typed C-like language. We show how the language naturally supports first-class polymorphism and polymorphic recursion while requiring an acceptable amount of explicit type information. More importantly, we consider the soundness of type variables in the presence of C-style mutation and the address-of operator. For polymorphic references, we describe a solution more natural for the C level than the ML-style ""value restriction."" For existential types, we discover and subsequently avoid a subtle unsoundness issue resulting from the address-of operator. We develop a formal abstract machine and type-safety proof that capture the essence of type variables at the C level. © 2006 ACM.",Cyclone; Existential types; Polymorphism; Type variables,Abstracting; Information retrieval; Cyclone; Existential types; Polymorphism; Type variables; Computer programming languages
Denali: A practical algorithm for generating optimal code,2006,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33845521509&doi=10.1145%2f1186632.1186633&partnerID=40&md5=e6048fd4ab8c66041381999689222aff,"This article presents a design for the Denali-2 superoptimizer, which will generate minimum-instruction-length machine code for realistic machine architectures using automatic theorem-proving technology: specifically, using E-graph matching (a technique for pattern matching in the presence of equality information) and Boolean satisfiability solving.This article presents a precise definition of the underlying automatic programming problem solved by the Denali-2 superoptimizer. It sketches the E-graph matching phase and presents a detailed exposition and proof of soundness of the reduction of the automatic programming problem to the Boolean satisfiability problem. © 2006 ACM.",Code generation; Compilation; Practical optimal code generation,Automatic programming; Graph theory; Optimization; Problem solving; Program compilers; E-graph matching; Practical optimal code generation; Codes (symbols)
Nontermination inference of logic programs,2006,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745414500&doi=10.1145%2f1119479.1119481&partnerID=40&md5=97cbc55443d67807782a40b437d7208f,"We present a static analysis technique for nontermination inference of logic programs. Our framework relies on an extension of the subsumption test, where some specific argument positions can be instantiated while others are generalized. We give syntactic criteria to statically identify such argument positions from the text of a program. Atomic left looping queries are generated bottom-up from selected subsets of the binary unfoldings of the program of interest. We propose a set of correct algorithms for automating the approach. Then, nontermination inference is tailored to attempt proofs of optimality of left termination conditions computed by a termination inference tool. An experimental evaluation is reported and the analyzers can be tried online at http://www.univ-reunion.fr/~gcc. When termination and nontermination analysis produce complementary results for a logic procedure, then with respect to the leftmost selection rule and the language used to describe sets of atomic queries, each analysis is optimal and together, they induce a characterization of the operational behavior of the logic procedure. © 2006 ACM.",Logic programming; Nontermination analysis; Optimal termination condition; Static analysis,Algorithms; Automation; Inference engines; Query languages; Statistical methods; Nontermination analysis; Optimal termination condition; Static analysis; Logic programming
Types for safe locking: Static race detection for Java,2006,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745425614&doi=10.1145%2f1119479.1119480&partnerID=40&md5=e46b42cddc12970c77190a693f906db8,"This article presents a static race-detection analysis for multithreaded shared-memory programs, focusing on the Java programming language. The analysis is based on a type system that captures many common synchronization patterns. It supports classes with internal synchronization, classes that require client-side synchronization, and thread-local classes. In order to demonstrate the effectiveness of the type system, we have implemented it in a checker and applied it to over 40,000 lines of hand-annotated Java code. We found a number of race conditions in the standard Java libraries and other test programs. The checker required fewer than 20 additional type annotations per 1,000 lines of code. This article also describes two improvements that facilitate checking much larger programs: an algorithm for annotation inference and a user interface that clarifies warnings generated by the checker. These extensions have enabled us to use the checker for identifying race conditions in large-scale software systems with up to 500,000 lines of code. © 2006 ACM.",Concurrent programs; Race conditions; Type inference; Type system,Codes (standards); Computer software; Digital libraries; Java programming language; Synchronization; User interfaces; Concurrent programs; Race conditions; Type inference; Type system; Object recognition
Traits: A mechanism for fine-grained reuse,2006,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745361068&doi=10.1145%2f1119479.1119483&partnerID=40&md5=4da0e17379e9449bba9a2c6864a2833c,"Inheritance is well-known and accepted as a mechanism for reuse in object-oriented languages. Unfortunately, due to the coarse granularity of inheritance, it may be difficult to decompose an application into an optimal class hierarchy that maximizes software reuse. Existing schemes based on single inheritance, multiple inheritance, or mixins, all pose numerous problems for reuse. To overcome these problems we propose traits, pure units of reuse consisting only of methods. We develop a formal model of traits that establishes how traits can be composed, either to form other traits, or to form classes. We also outline an experimental validation in which we apply traits to refactor a nontrivial application into composable units. © 2006 ACM.",Inheritance; Languages; Mixins; Multiple inheritance; Reuse; Smalltalk; Traits,Computer applications; Computer software; Hierarchical systems; Problem solving; Reusability; Inheritance; Languages; Mixins; Multiple inheritance; Reuse; Traits; Object oriented programming
A bisimulation-based semantic theory of Safe Ambients,2006,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745389061&doi=10.1145%2f1119479.1119482&partnerID=40&md5=eb639167c75f8c92a1d5170ea9909cc4,"We develop a semantics theory for SAP, a variant of Levi and Sangiorgi's Safe Ambients, SA. The dynamics of SA relies upon capabilities (and co-capabilities) exercised by mobile agents, called ambients, to interact with each other. These capabilities contain references, the names of ambients with which they wish to interact. In SAP we generalize the notion of capability: in order to interact with an ambient n, an ambient m must exercise a capability indicating both n and a password h to access n; the interaction between n and m takes place only if n is willing to perform a corresponding co-capability with the same password h. The name h can also be looked upon as a port to access ambient n via port h. In SAP, by managing passwords/ports, for example generating new ones and distributing them selectively, an ambient may now program who may migrate into its computation space, and when. Moreover in SAP, an ambient may provide different services/resources depending on the port accessed by the incoming clients. Then we give an lts-based operational semantics for SAP and a labelled bisimulation equivalence, which is proved to coincide with reduction barbed congruence. We use our notion of bisimulation to prove a set of algebraic laws that are subsequently exploited to prove more significant examples. © 2006 ACM.",Bisimulation; Distributed systems; Mobile agents,Algebra; Computer simulation; Information theory; Mobile computing; Resource allocation; Bisimulation; Distributed systems; Mobile agents; Operational semantics; Semantics
Dynamic software updating,2005,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745422353&doi=10.1145%2f1108970.1108971&partnerID=40&md5=30e011f470833290db2a5fac84b1852c,"Many important applications must run continuously and without interruption, and yet also must be changed to fix bugs or upgrade functionality. No prior general-purpose methodology for dynamic updating achieves a practical balance between flexibility, robustness, low overhead, ease of use, and low cost. We present an approach for C-like languages that provides type-safe dynamic updating of native code in an extremely flexible manner - code, data, and types may be updated, at programmer-determined times-and permits the use of automated tools to aid the programmer in the updating process. Our system is based on dynamic patches that contain both the updated code and the code needed to transition from the old version to the new. A novel aspect of our patches is that they consist of verifiable native code (e.g. Proof-Carrying Code or Typed Assembly Language), which is native code accompanied by annotations that allow online verification of the code's safety. We discuss how patches are generated mostly automatically, how they are applied using dynamic-linking technology, and how code is compiled to make it updateable. To concretely illustrate our system, we have implemented a dynamically updateable web server, FlashEd. We discuss our experience building and maintaining FlashEd, and generalize to present observations about updateable software development. Performance experiments show that for FlashEd, the overhead due to updating is low: typically less than 1 percent. © 2005 ACM.",Dynamic software updating; Typed assembly language,Computer programming languages; Program compilers; Program debugging; Robustness (control systems); Servers; Dynamic software updating; FlashEd; Online verification; Typed assembly language; Software engineering
Functional declarative language design and predicate calculus: A practical approach,2005,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745383621&doi=10.1145%2f1086642.1086647&partnerID=40&md5=eed8e7e100dab1aee4c5ba8c3391ffe7,"In programming language and software engineering, the main mathematical tool is de facto some form of predicate logic. Yet, as elsewhere in applied mathematics, it is used mostly far below its potential, due to its traditional formulation as just a topic in logic instead of a calculus for everyday practical use. The proposed alternative combines a language of utmost simplicity (four constructs only) that is devoid of the defects of common mathematical conventions, with a set of convenient calculation rules that is sufficiently comprehensive to make it practical for everyday use in most (if not all) domains of interest. Its main elements are a functional predicate calculus and concrete generic functionals. The first supports formal calculation with quantifiers with the same fluency as with derivatives and integrals in classical applied mathematics and engineering. The second achieves the same for calculating with functionals, including smooth transition between pointwise and point-free expression. The extensive collection of examples pertains mainly to software specification, language semantics and its mathematical basis, program calculation etc., but occasionally shows wider applicability throughout applied mathematics and engineering. Often it illustrates how formal reasoning guided by the shape of the expressions is an instrument for discovery and expanding intuition, or highlights design opportunities in declarative and (functional) programming languages. © 2005 ACM.",Analysis; Binary algebra; Calculational reasoning; Databases; Declarative languages; Elastic operators; Function equality; Functional predicate calculus; Generic functionals; Leibniz's principle; Limits; Program semantics; Programming languages; Quantifiers,Computer aided design; Differentiating circuits; Function evaluation; Mathematical operators; Semantics; Software engineering; Binary algebra; Calculational reasoning; Declarative languages; Elastic operators; Function equality; Functional predicate calculus; Generic functionals; Leibniz's principle; Limits; Quantifiers; Computer programming languages
Cost and precision tradeoffs of dynamic data slicing algorithms,2005,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745387492&doi=10.1145%2f1075382.1075384&partnerID=40&md5=e42baf16985a33e424edf4ad8df8b218,"Dynamic slicing algorithms are used to narrow the attention of the user or an algorithm to a relevant subset of executed program statements. Although dynamic slicing was first introduced to aid in user level debugging, increasingly applications aimed at improving software quality, reliability, security, and performance are finding opportunities to make automated use of dynamic slicing. In this paper we present the design and evaluation of three precise dynamic data slicing algorithms called the full preprocessing (FP), no preprocessing (NP) and limited preprocessing (LP) algorithms. The algorithms differ in the relative timing of constructing the dynamic data dependence graph and its traversal for computing requested dynamic data slices. Our experiments show that the LP algorithm is a fast and practical precise data slicing algorithm. In fact we show that while precise data slices can be orders of magnitude smaller than imprecise dynamic data slices, for small number of data slicing requests, the LP algorithm is faster than an imprecise dynamic data slicing algorithm proposed by Agrawal and Horgan. © 2005 ACM.",Data dependences; Debugging; Pointer references; Program slicing,Data reduction; Program debugging; Program processors; Set theory; Data dependences; Dynamic slicing algorithms; Pointer references; Program slicing; Learning algorithms
A theory of overloading,2005,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745386200&doi=10.1145%2f1108970.1108974&partnerID=40&md5=c230bc71c90b5446d2d520e7852e1da6,"We present a novel approach to allow for overloading of identifiers in the spirit of type classes. Our approach relies on a combination of the HM(X) type system framework with Constraint Handling Rules (CHRs). CHRs are a declarative language for writing incremental constraint solvers, that provide our scheme with a form of programmable type language. CHRs allow us to precisely describe the relationships among overloaded identifiers. Under some sufficient conditions on the CHRs we achieve decidable type inference and the semantic meaning of programs is unambiguous. Our approach provides a common formal basis for many type class extensions such as multiparameter type classes and functional dependencies. © 2005 ACM.",Coherence; Constraints; Evidence translation; Overloading; Type classes; Type inference,Computer programming languages; Identification (control systems); Parameter estimation; Problem solving; Semantics; Evidence translation; Overloading; Type classes; Type inference; Constraint theory
Implicit-signal monitors,2005,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745377868&doi=10.1145%2f1108970.1108975&partnerID=40&md5=711157b54ec83c7d326abfadbafddeaf,"An implicit (automatic) signal monitor uses a waltuntll predicate statement to construct synchronization, as opposed to an explicit-signal monitor using condition variables and signal/walt statements for synchronization. Of the two synchronization approaches, the implicit-signal monitor is often easier to use and prove correct, but has an inherently high execution cost. Hence, its primary use is for prototyping concurrent systems using monitors, where speed and accuracy of software development override execution performance. After a concurrent system is working, any implicit-signal monitor that is a performance bottleneck can be converted to an explicit-signal monitor. Unfortunately, many monitor-based concurrency systems provide only explicit-signal monitors, precluding the design benefits of implicit-signal monitors. This article presents a historical look at the development of the implicit-signal monitor in relation to its counterpart the explicit-signal monitor. An analysis of the different kinds of implicit-signal monitors shows the effects certain design decisions have on the problems that can be solved and the performance of the solutions. Finally, an extensive discussion is presented on simulating an implicit-signal monitor via different explicit-signal monitors. These simulations are reasonably complex, depending on the kind of explicit-signal monitor available for the simulation and the desired semantics required for the implicit-signal monitor. Interestingly, the complexity of the simulations also illustrates certain deficiencies with explicit-signal monitors, which are discussed in detail. Performance comparisons are made among the different simulations with monitors from the concurrent systems PThreads, Java, and μC++. © 2005 ACM.",Automatic signal; Concurrency; Explicit signal; Implicit signal; Monitor; Parallel; Simulation,Computer simulation; Concurrent engineering; Java programming language; Software engineering; Software prototyping; Synchronization; Automatic signal; Explicit signal; Implicit signal; Parallel; Signal processing
Efficient subtyping tests with PQ-encoding,2005,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745403211&doi=10.1145%2f1086642.1086643&partnerID=40&md5=5138b158ac5e1433cbc4fe7c80cb2947,"Given a type hierarchy, a subtyping test determines whether one type is a direct or indirect descendant of another type. Such tests are a frequent operation during the execution of object-oriented programs. The implementation challenge is in a space-efficient encoding of the type hierarchy that simultaneously permits efficient subtyping tests. We present a new scheme for encoding multiple-and single-inheritance hierarchies, which, in the standard benchmark hierarchies, reduces the footprint of all previously published schemes. Our scheme is called PQ-encoding (PQE) after PQ-trees, a data structure previously used in graph theory for finding the orderings that satisfy a collection of constraints. In particular, we show that in the traditional object layout model, the extra memory requirements for single-inheritance hierarchies is zero. In the PQE subtyping, tests are constant time, and use only two comparisons. The encoding creation time of PQE also compares favorably with previous results. It is less than 1 s on all standard benchmarks on a contemporary architecture, while the average time for processing a type is less than 1 ms. However, PQE is not an incremental algorithm. Other than PQ-trees, PQE employs several novel optimization techniques. These techniques are applicable also in improving the performance of other, previously published, encoding schemes. © 2005 ACM.",Casting; Encoding; Hierarchy; Inheritance; Partially ordered sets; PQ; PQE; Subtyping; Type inclusion,Computer architecture; Constraint theory; Data structures; Hierarchical systems; Object oriented programming; Trees (mathematics); Inheritance; Partially ordered sets; PQ; PQE; Subtyping; Type inclusion; Encoding (symbols)
The KaffeOS Java runtime system,2005,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745413400&doi=10.1145%2f1075382.1075383&partnerID=40&md5=cc3bd519dffa34e43c1ecdd39599df18,"Single-language runtime systems, in the form of Java virtual machines, are widely deployed platforms for executing untrusted mobile code. These runtimes provide some of the features that operating systems provide: interapplication memory protection and basic system services. They do not, however, provide the ability to isolate applications from each other. Neither do they provide the ability to limit the resource consumption of applications. Consequently, the performance of current systems degrades severely in the presence of malicious or buggy code that exhibits ill-behaved resource usage. We show that Java runtime systems can be extended to support processes, and that processes can provide robust and efficient support for untrusted applications. We have designed and built KaffeOS, a Java runtime system that provides support for processes. KaffeOS isolates processes and manages the physical resources available to them: CPU and memory. Unlike existing Java virtual machines, KaffeOS can safely terminate processes without adversely affecting the integrity of the system, and it can fully reclaim a terminated process's resources. Finally, KaffeOS requires no changes to the Java language. The novel aspects of the KaffeOS architecture include the application of a user/kernel boundary as a structuring principle for runtime systems, the employment of garbage collection techniques for resource management and isolation, and a model for direct sharing of objects between untrusted applications. The difficulty in designing KaffeOS lay in balancing the goals of isolation and resource management against the goal of allowing direct sharing of objects. For the SpecJVM benchmarks, the overhead that our KaffeOS prototype incurs ranges from 0% to 25%, when compared to the open-source JVM on which it is based. We consider this overhead acceptable for the safety that KaffeOS provides. In addition, our KaffeOS prototype can scale to run more applications than running multiple JVMs. Finally, in the presence of malicious or buggy code that engages in a denial-of-service attack, KaffeOS can contain the attack, remove resources from the attacked applications, and continue to provide robust service to other clients. © 2005 ACM.",Garbage collection; Isolation; Language runtimes; Resource management; Robustness; Termination; Virtual machines,Codes (symbols); Computer architecture; Computer operating systems; Information services; Resource allocation; Virtual reality; Garbage collection; Isolation; Language runtimes; Resource management; Termination; Virtual machines; Java programming language
"A parallel, incremental, mostly concurrent garbage collector for servers",2005,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745353412&doi=10.1145%2f1108970.1108972&partnerID=40&md5=c47b9867c2c74e9ad3fdc15d3cc88bdd,"Multithreaded applications with multigigabyte heaps running on modern servers provide new challenges for garbage collection (GC). The challenges for ""server-oriented"" GC include: ensuring short pause times on a multigigabyte heap while minimizing throughput penalty, good scaling on multiprocessor hardware, and keeping the number of expensive multicycle fence instructions required by weak ordering to a minimum. We designed and implemented a collector facing these demands building on the mostly concurrent garbage collector proposed by Boehm et al. [1991]. Our collector incorporates new ideas into the original collector. We make it parallel and incremental; we employ concurrent low-priority background GC threads to take advantage of processor idle time; we propose novel algorithmic improvements to the basic mostly concurrent algorithm improving its efficiency and shortening its pause times; and finally, we use advanced techniques, such as a low-overhead work packet mechanism to enable full parallelism among the incremental and concurrent collecting threads and ensure load balancing. We compared the new collector to the mature, well-optimized, parallel, stop-the-world mark-sweep collector already in the IBM JVM. When allowed to run aggressively, using 72% of the CPU utilization during a short concurrent phase, our collector prototype reduces the maximum pause time from 161 ms to 46 ms while only losing 11.5% throughput when running the SPECjbb2000 benchmark on a 600-MB heap on an 8-way PowerPC 1.1-GHz processors. When the collector is limited to a nonintrusive operation using only 29% of the CPU utilization, the maximum pause time obtained is 79 ms and the loss in throughput is 15.4%. © 2005 ACM.",Concurrent garbage collection; Garbage collection; JVM,Benchmarking; Computer hardware; Multiprocessing systems; Servers; Software prototyping; Concurrent garbage collection; Garbage collection; JVM; Nonintrusive operation; Concurrent engineering
Design and evaluation of dynamic optimizations for a Java just-in-time compiler,2005,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745400520&doi=10.1145%2f1075382.1075386&partnerID=40&md5=4d54f7ce1778fb7c2dfba3176d490187,"The high performance implementation of Java Virtual Machines (JVM) and Just-In-Time (JIT) compilers is directed toward employing a dynamic compilation system on the basis of online run-time profile information. The trade-off between the compilation overhead and performance benefit is a crucial issue for such a system. This article describes the design and implementation of a dynamic optimization framework in a production-level Java JIT compiler, together with two techniques for profile-directed optimizations: method inlining and code specialization. Our approach is to employ a mixed mode interpreter and a three-level optimizing compiler, supporting level-1 to level-3 optimizations, each of which has a different set of trade-offs between compilation over-head and execution speed. A lightweight sampling profiler operates continuously during the entire period while applications are running to monitor the programs' hot spots. Detailed information on runtime behavior can be collected by dynamically generating instrumentation code that is installed to and uninstalled from the specified recompilation target code. Value profiling with this instrumentation mechanism allows fully automatic profile-directed method inlining and code specialization to be performed on the basis of call site information or specific parameter values at the higher optimization levels. The experimental results show that our approach offers high performance and low compilation overhead in both program startup and steady state measurements in comparison to the previous systems. The two profile-directed optimization techniques contribute significant portions of the improvements. © 2005 ACM.",Adaptive optimization; Code specialization; Dynamic compilation; JIT compiler; Profile-directed method inlining; Recompilation,Automatic programming; Codes (symbols); Computer programming languages; Java programming language; Just in time production; Optimization; Adaptive optimization; Code specialization; Dynamic compilation; JIT compiler; Profile-directed method inlining; Recompilation; Program compilers
When do bounds and domain propagation lead to the same search space?,2005,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745348779&doi=10.1145%2f1065887.1065889&partnerID=40&md5=c1753b1414d47165787c8e7dd39d0807,"This article explores the question of when two propagation-based constraint systems have the same behavior, in terms of search space. We categorize the behavior of domain and bounds propagators for primitive constraints, and provide theorems that allow us to determine propagation behaviors for conjunctions of constraints. We then show how we can use this to analyze CLP(FD) programs to determine when we can safely replace domain propagators by more efficient bounds propagators without increasing search space. Empirical evaluation shows that programs optimized by the analysis' results are considerably more efficient. © 2005 ACM.",Abstract interpretation; Bounds propagation; Constraint (logic) programming; Domain propagation; Finite domain constraints; Program analysis,Constraint theory; Information retrieval systems; Logic programming; Optimization; Theorem proving; Abstract interpretation; Bounds propagation; Bounds propagators; Constraint (logic) programming; Domain propagation; Finite domain constraints; Program analysis; Search engines
A practical and fast iterative algorithm for φ-function computation using DJ graphs,2005,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745409579&doi=10.1145%2f1065887.1065890&partnerID=40&md5=6f9d43e224801474755436cb7b8f8f28,"We present a new and practical method of computing φ-function for all variables in a function for Static Single Assignment (SSA) form. The new algorithm is based on computing the Merge set of each node in the control flow graph of a function (a node here represents a basic block and the terms will be used interchangeably). Merge set of a node n is the set of nodes N, where φ-functions may need to be placed if variables are defined in n. It is not necessary for n to have a definition of a variable in it. Thus, the merge set of n is dictated by the underlying structure of the CFG. The new method presented here precomputes the merge set of every node in the CFG using an iterative approach. Later, these merge sets are used to carry out the actual φ-function placement. The advantages are in examples where dense definitions of variables are present (i.e., original definitions of variables - user defined or otherwise, in a majority of basic blocks). Our experience with SSA in the High Level Optimizer (optimization levels +O3/+O4) shows that most examples from the Spec2000 benchmark suite require a high percentage of basic blocks to have their points computed. Previous methods of computing the same relied on the dominance frontier (DF) concept, first introduced by Cytron et al. The method presented in this paper gives a new effective iterative solution to the problem. Also, in cases, where the control flow graph does not change, our method does not require any additional computation for new definitions introduced as part of optimizations. We present implementation details with results from Spec2000 benchmarks. Our algorithm runs faster than the existing methods used. © 2005 ACM.",Dj graph; Dominance Frontier; Static Single Assignment Form,Computation theory; Function evaluation; Graph theory; Optimization; Set theory; State assignment; Dj graph; Dominance Frontier; Static Single Assignment (SSA) form; Iterative methods
Comparing conservative coalescing criteria,2005,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745371555&doi=10.1145%2f1065887.1065894&partnerID=40&md5=fa140608ae932796c2fe0552383c6907,"Graph-coloring register allocators can eliminate copy instructions from a program by coalescing the interference graph nodes corresponding to the source and destination. Briggs showed that by limiting coalescing to those situations that he dubbed ""conservative,"" it could be prevented from causing spilling, that is, a situation where the allocator fails to assign a register to each live range. George and Appel adopted Briggs's conservativeness criterion in general, but provided an alternative criterion (the George test) to use in those cases where one of the nodes has been ""precolored,"" that is, preassigned a specific register. They motivated this alternative criterion by efficiency considerations, and provided no indication of the relative power of the two criteria. Thus it remained an open question whether the efficiency had been bought at the expense of reduced coalescing. Their implementation also used a limited version of the Briggs test, in place of the original, full version, without any comment on the impact of this substitution. In this article, we also present an analogously limited version of the George test. Thus we are now confronted with four different criteria for conservative coalescing: the full and limited Briggs tests and the full and limited George tests. We present a number of theorems characterizing the relative power of these different criteria, and a number of theorems characterizing the form of safety that each achieves. For example, we show that for coalescing with precolored nodes, the full George criterion is strictly more powerful than the full Briggs criterion, while offering an equally strong safety guarantee. Thus no coalesces are lost through George and Appel's introduction of the George test, and some can be gained without sacrificing safety. We also show that George and Appel's limited version of the Briggs test is probably undesirable. Although a slightly stronger safety result applies to it than to the full Briggs test, this comes at the expense of eliminating all coalesces that can reduce spilling. © 2005 ACM.",Copy propagation; Graph coloring; Register allocation; Register coalescing,Coalescence; Inference engines; Probability; Theorem proving; Copy propagation; Graph coloring; Register allocation; Register coalescing; Graph theory
Mixin modules in a call-by-value setting,2005,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745376296&doi=10.1145%2f1086642.1086644&partnerID=40&md5=e0eff0b755ffad59cd95b06da0466801,"The ML module system provides powerful parameterization facilities, but lacks the ability to split mutually recursive definitions across modules and provides insufficient support for incremental programming. A promising approach to solve these issues is Ancona and Zucca's mixin module calculus CMS. However, the straightforward way to adapt it to ML fails, because it allows arbitrary recursive definitions to appear at any time, which ML does not otherwise support. In this article, we enrich CMS with a refined type system that controls recursive definitions through the use of dependency graphs. We then develop and prove sound a separate compilation scheme, directed by dependency graphs, that translates mixin modules down to a call-by-value λ-calculus extended with a nonstandard let rec construct. © 2005 ACM.",Mixins; Modules; Recursion; Type systems,Differentiation (calculus); Graph theory; Information retrieval systems; Parameter estimation; Mixins; Modules; Recursion; Type systems; Computer programming languages
Birrell's distributed reference listing revisited,2005,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745420082&doi=10.1145%2f1108970.1108976&partnerID=40&md5=dc7f99ebd31c9dafe2d410129ae923ec,"The Java RMI collector is arguably the most widely used distributed garbage collector. Its distributed reference listing algorithm was introduced by Birrell et al. in the context of Network Objects, where the description was informal and heavily biased toward implementation. In this article, we formalize this algorithm in an implementation-independent manner, which allows us to clarify weaknesses of the initial presentation. In particular, we discover cases critical to the correctness of the algorithm that were not accounted for by Birrell. We use our formalization to derive an invariant-based proof of correctness of the algorithm that avoids notoriously difficult temporal reasoning. Furthermore, we offer a novel graphical representation of the state transition diagram, which we use to provide intuitive explanations of the algorithm and to investigate its tolerance to faults in a systematic manner. Finally, we examine how the algorithm may be optimized, either by placing constraints on message channels or by tightening the coupling between the application program and distributed garbage collector. © 2005 ACM.",Distributed garbage collection; Distributed reference counting/listing; Proof of correctness,Algorithms; Constraint theory; Distributed computer systems; Formal logic; Graphical user interfaces; Java programming language; Application program; Distributed garbage collection; Distributed reference counting/listing; Proof of correctness; Computer program listings
An accurate cost model for guiding data locality transformations,2005,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745423635&doi=10.1145%2f1086642.1086646&partnerID=40&md5=cd0a5dba22e672f7d3b4ba23a75919ac,"Caches have become increasingly important with the widening gap between main memory and processor speeds. Small and fast cache memories are designed to bridge this discrepancy. However, they are only effective when programs exhibit sufficient data locality. The performance of the memory hierarchy can be improved by means of data and loop transformations. Tiling is a loop transformation that aims at reducing capacity misses by shortening the reuse distance. Padding is a data layout transformation targeted to reduce conflict misses. This article presents an accurate cost model that describes misses across different hierarchy levels and considers the effects of other hardware components such as branch predictors. The cost model drives the application of tiling and padding transformations. We combine the cost model with a genetic algorithm to compute the tile and pad factors that enhance the program performance. To validate our strategy, we ran experiments for a set of benchmarks on a large set of modern architectures. Our results show that this scheme is useful to optimize programs' performance. When compared to previous approaches, we observe that with a reasonable compile-time overhead, our approach gives significant performance improvements for all studied kernels on all architectures. © 2005 ACM.",Cache memories; Genetic algorithms; Padding; Tiling,Cache memory; Costs; Data reduction; Genetic algorithms; Hierarchical systems; Mathematical models; Program processors; Cost model; Data locality; Padding; Tiling; Data transfer
Termination analysis and specialization-point insertion in offline partial evaluation,2005,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745338876&doi=10.1145%2f1108970.1108973&partnerID=40&md5=b7063d8623209b3b697444fa36da8760,"Recent research suggests that the goal of fully automatic and reliable program generation for a broad range of applications is coming nearer to feasibility. However, several interesting and challenging problems remain to be solved before it becomes a reality. Solving them is also necessary, if we hope ever to elevate software engineering from its current state (a highly developed handiwork) into a successful branch of engineering, capable of solving a wide range of new problems by systematic, well-automated and well-founded methods. A key problem in all program generation is termination of the generation process. This article focuses on off-line partial evaluation and describes recent progress towards automatically solving the termination problem, first for individual programs, and then for specializers and ""generating extensions,"" the program generators that most offline partial evaluators produce. The technique is based on size-change graphs that approximate the changes in parameter sizes at function calls. We formulate a criterion, bounded anchoring, for detecting parameters known to be bounded during specialization: a bounded parameter can act as an anchor for other parameters. Specialization points necessary for termination are computed by adding a parameter that tracks call depth, and then selecting a specialization point in every call loop where it is unanchored. By generalizing all unbounded parameters, we compute a binding-time division which together with the set of specialization points guarantees termination. Contributions of this article include a proof, based on the operational semantics of partial evaluation with memoization, that the analysis guarantees termination; and an in-depth description of safety of the increasing size approximation operator required for termination analysis in partial evaluation. Initial experiments with a prototype shows that the analysis overall yields binding-time divisions that can achieve a high degree of specialization, while still guaranteeing termination. The article ends with a list of challenging problems whose solution would bring the community closer to the goal of broad-spectrum, fully automatic and reliable program generation. © 2005 ACM.",Binding-time analysis; Quasitermination; Size-change graphs; Termination,Computation theory; Computer applications; Computer simulation; Problem solving; Semantics; Software engineering; Binding-time analysis; Quasitermination; Size-change graphs; Termination; Computer programming
Resource aware programming,2005,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33646422806&doi=10.1145%2f1065887.1065891&partnerID=40&md5=3755e3979ef1091f171442efe2ae7a40,"We introduce the Resource Aware Programming framework, which allows users to monitor the resources used by their programs and to programmatically express policies for the management of such resources. The framework is based on a notion of hierarchical groups, which act as resource containers for the computations they sponsor. Asynchronous notifications for resource exhaustion and for computation termination can be handled by arbitrary user code, which is also executed under the control of this hierarchical group structure. Resources are manipulated by the programmer using resource descriptors, whose operations are specified by a resource algebra. In this article, we overview the Resource Aware Programming framework and describe its semantics in the form of a language-independent abstract machine able to model both shared and distributed memory environments. Finally, we discuss a prototype implementation of the Resource Aware Programming framework in Java. © 2005 ACM.",Abstract machine; Resource algebra; Resource management; Semantics,Algebra; Data structures; Hierarchical systems; Information management; Mathematical models; Resource allocation; Semantics; Abstract machine; Resource algebra; Resource aware programming; Resource management; Computer systems programming
Efficient and effective array bound checking,2005,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33244463626&doi=10.1145%2f1065887.1065893&partnerID=40&md5=5640bfbb5e22684170b11ff65ddcee36,"Array bound checking refers to determining whether all array references in a program are within their declared ranges. This checking is critical for software verification and validation because subscripting arrays beyond their declared sizes may produce unexpected results, security holes, or failures. It is available in most commercial compilers but current implementations are not as efficient and effective as one may have hoped: (1) the execution times of array bound checked programs are increased by a factor of up to 5, (2) the compilation times are increased, which is detrimental to development and debugging, (3) the related error messages do not usually carry information to locate the faulty references, and (4) the consistency between actual array sizes and formal array declarations is not often checked. This article presents two optimization techniques that deal with Points 1, 2, and 3, and a new algorithm to tackle Point 4, which is not addressed by the current literature. The first optimization technique is based on the elimination of redundant tests, to provide very accurate information about faulty references during development and testing phases. The second one is based on the insertion of unavoidable tests to provide the smallest possible slowdown during the production phase. The new algorithm ensures the absence of bound violations in every array access in the called procedure with respect to the array declarations in the calling procedure. Our experiments suggest that the optimization of array bound checking depends on several factors, not only the percentage of removed checks, usually considered as the best improvement measuring metrics. The debugging capability and compile-time and run-time performances of our techniques are better than current implementations. The execution times of SPEC95 CFP benchmarks with range checking added by PIPS, our Fortran research compiler, are slightly longer, less than 20%, than that of unchecked programs. More problems due to functional and data recursion would have to be solved to extend these results from Fortran to other languages such as C, C++, or Java, but the issues addressed in this article are nevertheless relevant. © 2005 ACM.",Array bound checking; Interprocedural analysis,Computer programming languages; Computer system recovery; Optimization; Program compilers; Software prototyping; Array bound checking; Data recursion; Faulty references; Interprocedural analysis; Arrays
CCured: Type-safe retrofitting of legacy software,2005,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33646032658&doi=10.1145%2f1065887.1065892&partnerID=40&md5=b2b1155d4f64c9e19f39793a47f3df58,"This article describes CCured, a program transformation system that adds type safety guarantees to existing C programs. CCured attempts to verify statically that memory errors cannot occur, and it inserts run-time checks where static verification is insufficient. CCured extends C's type system by separating pointer types according to their usage, and it uses a surprisingly simple type inference algorithm that is able to infer the appropriate pointer kinds for existing C programs. CCured uses physical subtyping to recognize and verify a large number of type casts at compile time. Additional type casts are verified using run-time type information. CCured uses two instrumentation schemes, one that is optimized for performance and one in which metadata is stored in a separate data structure whose shape mirrors that of the original user data. This latter scheme allows instrumented programs to invoke external functions directly on the program's data without the use of a wrapper function. We have used CCured on real-world security-critical network daemons to produce instrumented versions without memory-safety vulnerabilities, and we have found several bugs in these programs. The instrumented code is efficient enough to be used in day-to-day operations. © 2005 ACM.",Libraries; Memory safety; Pointer qualifier; Subtyping,Algorithms; Computer crime; Computer programming languages; Data structures; Mathematical transformations; Retrofitting; Memory safety; Pointer qualifier; Subtyping; Legacy systems
Link-time binary rewriting techniques for program compaction,2005,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-31844441420&doi=10.1145%2f1086642.1086645&partnerID=40&md5=dfcda71cddc913eece4553daed601646,"Small program size is an important requirement for embedded systems with limited amounts of memory. We describe how link-time compaction through binary rewriting can achieve code size reductions of up to 62% for statically bound languages such as C, C++, and Fortran, without compromising on performance. We demonstrate how the limited amount of information about a program at link time can be exploited to overcome overhead resulting from separate compilation. This is done with scalable, cost-effective, whole-program analyses, optimizations, and duplicate code and data elimination techniques. The discussed techniques are evaluated and their cost-effectiveness is quantified with SQUEEZE++, a prototype link-time compactor. © 2005 ACM.",Binary rewriting; Code abstraction; Compaction; Interprocedural analysis; Linker; Program representation; Whole-program optimization,Codes (symbols); Computer programming languages; Embedded systems; Optimization; Requirements engineering; Technical writing; Binary rewriting; Code abstraction; Interprocedural analysis; Linker; Program representation; Whole-program optimization; Computer systems programming
Interprocedural parallelization analysis in SUIF,2005,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33645444470&doi=10.1145%2f1075382.1075385&partnerID=40&md5=ff14792c898d6770ec5d342a4705f238,"As shared-memory multiprocessor systems become widely available, there is an increasing need for tools to simplify the task of developing parallel programs. This paper describes one such tool, the automatic parallelization system in the Stanford SUIF compiler. This article represents a culmination of a several-year research effort aimed at making parallelizing compilers significantly more effective. We have developed a system that performs full interprocedural parallelization analyses, including array privatization analysis, array reduction recognition, and a suite of scalar data-flow analyses including symbolic analysis. These analyses collaborate in an integrated fashion to exploit coarse-grain parallel loops, computationally intensive loops that can execute on multiple processors independently with no cross-processor synchronization or communication. The system has successfully parallelized large interprocedural loops over a thousand lines of code completely automatically from sequential applications. This article provides a comprehensive description of the analyses in the SUIF system. We also present extensive empirical results on four benchmark suites, showing the contribution of individual analysis techniques both in executing more of the computation in parallel, and in increasing the granularity of the parallel computations. These results demonstrate the importance of interprocedural array data-flow analysis, array privatization and array reduction recognition; a third of the programs spend more than 50% of their execution time in computations that are parallelized with these techniques. Overall, these results indicate that automatic parallelization can be effective on sequential scientific computations, but only if the compiler incorporates all of these analyses. © 2005 ACM.",Data dependence analysis; Interprocedural data-flow analysis; Parallelization; Symbolic analysis,Computation theory; Data flow analysis; Data storage equipment; Multiprocessing programs; Parallel processing systems; Privatization; Synchronization; Data dependence analysis; Interprocedural data-flow analysis; Parallelization; Symbolic analysis; Program compilers
A systematic approach to static access control,2005,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-27844549441&doi=10.1145%2f1057387.1057392&partnerID=40&md5=5b447a65c864584f6b4fd5c89050332f,"The Java Security Architecture includes a dynamic mechanism for enforcing access control checks, the so-called stack inspection process. While the architecture has several appealing features, access control checks are all implemented via dynamic method calls. This is a highly nondeclarative form of specification that is hard to read, and that leads to additional run-time overhead. This article develops type systems that can statically guarantee the success of these checks. Our systems allow security properties of programs to be clearly expressed within the types themselves, which thus serve as static declarations of the security policy. We develop these systems using a systematic methodology: we show that the security-passing style translation, proposed by Wallach et al. [2000] as a dynamic implementation technique, also gives rise to static security-aware type systems, by composition with conventional type systems. To define the latter, we use the general HM(X) framework, and easily construct several constraint- and unification-based type systems. © 2005 ACM.",Access control; Stack inspection; Type systems,Inspection; Public policy; Access control; Stack inspection; Type systems; Control systems
Dealing with incomplete knowledge on CLP(FD) variable domains,2005,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-27844562275&doi=10.1145%2f1057387.1057389&partnerID=40&md5=0185d6aecec54a7dd0b62f611c69a888,"Constraint Logic Programming languages on Finite Domains, CLP(FD), provide a declarative framework for Artificial Intelligence problems. However, in many real life cases, domains are not known and must be acquired or computed. In systems that interact with the outer world, domain elements synthesize information on the environment, they are not all known at the beginning of the computation, and must be retrieved through an expensive acquisition process. In this article, we extend the CLP(FD) language by combining it with a new sort (called Incrementally specified Sets, I-Set). In the resulting language, CLP(FD + I-Set), FD variables can be defined on partially or fully unknown domains (I-Set). Domains can be linked each other through relations, and constraints can be imposed on them. We describe a propagation algorithm (called Known Arc Consistency (KAC)) based on known domain elements, and theoretically compare it with arc-consistency. The language can be implemented on top of different CLP systems, thus letting the user exploit different possible semantics for domains (e.g., lists, sets or streams). We state the specifications that the employed system should provide, and we show that two different CLP systems (Conjunto and {log}) can be effectively used. We provide motivating examples and describe promising applications. © 2005 ACM.",Constraints; Domain acquisition; Interaction; Lazy evaluation; Sets; Streams,Artificial intelligence; Logic programming; Problem solving; Interaction; Sets; Streams; Computer programming languages
Analysis of recursive state machines,2005,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-26444540951&doi=10.1145%2f1075382.1075387&partnerID=40&md5=7ea5d4cf46356d6c1bbb8ffaa3dbbb82,"Recursive state machines (RSMs) enhance the power of ordinary state machines by allowing vertices to correspond either to ordinary states or to potentially recursive invocations of other state machines. RSMs can model the control flow in sequential imperative programs containing recursive procedure calls. They can be viewed as a visual notation extending Statecharts-like hierarchical state machines, where concurrency is disallowed but recursion is allowed. They are also related to various models of pushdown systems studied in the verification and program analysis communities. After introducing RSMs and comparing their expressiveness with other models, we focus on whether verification can be efficiently performed for RSMs. Our first goal is to examine the verification of linear time properties of RSMs. We begin this study by dealing with two key components for algorithmic analysis and model checking, namely, reachability (Is a target state reachable from initial states?) and cycle detection (Is there a reachable cycle containing an accepting state?). We show that both these problems can be solved in time O(nθ 2) and space O(nθ), where n is the size of the recursive machine and θ is the maximum, over all component state machines, of the minimum of the number of entries and the number of exits of each component. From this, we easily derive algorithms for linear time temporal logic model checking with the same complexity in the model. We then turn to properties in the branching time logic CTL*, and again demonstrate a bound linear in the size of the state machine, but only for the case of RSMs with a single exit node. © 2005 ACM.",Context-free languages; Model checking; Program analysis; Pushdown automata; Recursive state machines; Software verification; Temporal logic,Algorithms; Concurrency control; Flow control; Hierarchical systems; Mathematical models; Recursive functions; Model checking; Program analysis; Pushdown automata; Recursive state machines; Software verification; Temporal logic; Automata theory
"Symbolic bounds analysis of pointers, array indices, and accessed memory regions",2005,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-27844444121&doi=10.1145%2f1057387.1057388&partnerID=40&md5=9728140e9db3747137ef7102044730ca,"This article presents a novel framework for the symbolic bounds analysis of pointers, array indices, and accessed memory regions. Our framework formulates each analysis problem as a system of inequality constraints between symbolic bound polynomials. It then reduces the constraint system to a linear program. The solution to the linear program provides symbolic lower and upper bounds for the values of pointer and array index variables and for the regions of memory that each statement and procedure accesses. This approach eliminates fundamental problems associated with applying standard fixed-point approaches to symbolic analysis problems. Experimental results from our implemented compiler show that the analysis can solve several important problems, including static race detection, automatic parallelization, static detection of array bounds violations, elimination of array bounds checks, and reduction of the number of bits used to store computed values. © 2005 ACM.",Parallelization; Static race detection; Symbolic analysis,Number theory; Polynomials; Static race detection; Symbolic analysis; Computer programming languages
Resource usage analysis,2005,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-27844447213&doi=10.1145%2f1057387.1057390&partnerID=40&md5=4097ab6c3962434af10439d90686a8ce,"It is an important criterion of program correctness that a program accesses resources in a valid manner. For example, a memory region that has been allocated should eventually be deallocated, and after the deallocation, the region should no longer be accessed. A file that has been opened should be eventually closed. So far, most of the methods to analyze this kind of property have been proposed in rather specific contexts (like studies of memory management and verification of usage of lock primitives), and it was not clear what the essence of those methods was or how methods proposed for individual problems are related. To remedy this situation, we formalize a general problem of analyzing resource usage as a resource usage analysis problem, and propose a type-based method as a solution to the problem. © 2005 ACM.",Resource usage; Type inference,Problem solving; Resource allocation; Memory region; Resource usage analysis; Type inference; Computer programming languages
Polymorphic predicate abstraction,2005,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-27844563963&doi=10.1145%2f1057387.1057391&partnerID=40&md5=1c9f711c625eeac07f24d066ddf62014,"Predicate abstraction is a technique for creating abstract models of software that are amenable to model checking algorithms. We show how polymorphism, a well-known concept in programming languages and program analysis, can be incorporated in a predicate abstraction algorithm for C programs. The use of polymorphism in predicates, via the introduction of symbolic names for values, allows us to model the effect of a procedure independent of its calling contexts. Therefore, we can safely and precisely abstract a procedure once and then reuse this abstraction across multiple calls and multiple applications containing the procedure. Polymorphism also enables us to handle programs that need to be analyzed in an open environment, for all possible callers. We have proved that our algorithm is sound and have implemented it in the C2BP tool as part of the SLAM software model checking toolkit. © 2005 ACM.",Polymorphism; Predicate abstraction; Software model checking,Abstracting; Algorithms; Computer software; Model checking algorithms; Predicate abstraction; Computer programming languages
"Erratum: A new, simpler linear-time dominators algorithm (ACM Transactions on Programming Languages and Systems (1998) 20:6 (1265-1296))",2005,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745370564&doi=10.1145%2f1065887.1065888&partnerID=40&md5=3c3a23c6ac278ba3ac70cd8e9130bbbb,[No abstract available],Compilers; Dominators; Flowgraphs; Microtrees; Path compression,
A type system for certified binaries,2005,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-13644274214&doi=10.1145%2f1053468.1053469&partnerID=40&md5=69fe84dfc8f4172eb9b7b28d8f8a85d0,"A certified binary is a value together with a proof that the value satisfies a given specification. Existing compilers that generate certified code have focused on simple memory and control-flow safety rather than more advanced properties. In this article, we present a general framework for explicitly representing complex propositions and proofs in typed intermediate and assembly languages. The new framework allows us to reason about certified programs that involve effects while still maintaining decidable typechecking. We show how to integrate an entire proof system (the calculus of inductive constructions) into a compiler intermediate language and how the intermediate language can undergo complex transformations (CPS and closure conversion) while preserving proofs represented in the type system. Our work provides a foundation for the process of automatically generating certified binaries in a type-theoretic framework.",Certified code; Proof-preserving compilation; Typed intermediate languages,Computational linguistics; Data structures; Program assemblers; Program compilers; Query languages; Theorem proving; Certified code; Proof-preserving compilation; Type systems; Typed intermediate languages; Binary codes
Optimizing aggregate array computations in loops,2005,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-13644284563&doi=10.1145%2f1053468.1053471&partnerID=40&md5=ec65dee4eeceea4713cd4bbc9941e955,"An aggregate array computation is a loop that computes accumulated quantities over array elements. Such computations are common in programs that use arrays, and the array elements involved in such computations often overlap, especially across iterations of loops, resulting in significant redundancy in the overall computations. This article presents a method and algorithms that eliminate such overlapping aggregate array redundancies and shows analytical and experimental performance improvements. The method is based on incrementalization, that is, updating the values of aggregate array computations from iteration to iteration rather than computing them from scratch in each iteration. This involves maintaining additional values not maintained in the original program. We reduce various analysis problems to solving inequality constraints on loop variables and array subscripts, and we apply results from work on array data dependence analysis. For aggregate array computations that have significant redundancy, incrementalization produces drastic speedup compared to previous optimizations; when there is little redundancy, the benefit might be offset by cache effects and other factors. Previous methods for loop optimizations of arrays do not perform incrementalization, and previous techniques for loop incrementalization do not handle arrays.",Array dependence analysis; Caching intermediate results; Incremental computation; Loop optimization; Program transformation,Automatic programming; Computational methods; Graphic methods; Iterative methods; Matrix algebra; Multimedia systems; Optimization; Problem solving; Semantics; Array computations; Code generation; Loop incrementalization; Program analysis; Computer programming languages
Pretty printing with lazy dequeues,2005,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-13644281056&doi=10.1145%2f1053468.1053473&partnerID=40&md5=dcd42fe6d0f0bc5a46ac17cda749b9dc,"There are several purely functional libraries for converting tree structured data into indented text, but they all make use of some backtracking. Over twenty years ago, Oppen published a more efficient imperative implementation of a pretty printer. This article shows that the same efficiency is also obtainable without destructive updates by developing a similar but purely functional Haskell implementation with the same complexity bounds. At its heart lie two lazy double ended queues.",Haskell; Lazy functional programming,Computational complexity; Computer programming languages; Data structures; Functions; Indentation; Printers (computer); Functionality; Haskell; Lazy functional programming; Pretty printers; Printing
Automatic discovery of covariant read-only fields,2005,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-13644274759&doi=10.1145%2f1053468.1053472&partnerID=40&md5=52f8b7e282cd3c7cef2bd5bd1f78426d,"Read-only fields are useful in object calculi, pi calculi, and statically typed intermediate languages because they admit covariant subtyping, unlike updateable fields. For example, Glew's translation of classes and objects to an intermediate calculus relies crucially on covariant subtyping of read-only fields to ensure that subclasses are translated to subtypes. In this article, we present a type inference algorithm for an Abadi-Cardelli object calculus in which fields are marked either as updateable or as read-only. The type inference problem is P-complete, and our algorithm runs in O(n 3) time. The same complexity results hold for the calculus in which the fields are not explicitly annotated as updateable or read-only; perhaps surprisingly, the annotations do not make type inference easier. We show that type inference is equivalent to the problem of solving type constraints, and this forms the core of our algorithm and implementation.",Constraints; Types,Algorithms; Computer programming languages; Problem solving; ROM; Semantics; Set theory; Flexible matching; Intermediate languages; Subtyping; Type systems; Automatic programming
Regular expression types for XML,2005,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-13644276604&doi=10.1145%2f1053468.1053470&partnerID=40&md5=debeb1f7d1d8cac0506f09f05390cd00,"We propose regular expression types as a foundation for statically typed XML processing languages. Regular expression types, like most schema languages for XML, introduce regular expression notations such as repetition (*), alternation (|), etc., to describe XML documents. The novelty of our type system is a semantic presentation of subtyping, as inclusion between the sets of documents denoted by two types. We give several examples illustrating the usefulness of this form of subtyping in XML processing. The decision problem for the subtype relation reduces to the inclusion problem between tree automata, which is known to be EXPTIME-complete. To avoid this high complexity in typical cases, we develop a practical algorithm that, unlike classical algorithms based on determinization of tree automata, checks the inclusion relation by a top-down traversal of the original type expressions. The main advantage of this algorithm is that it can exploit the property that type expressions being compared often share portions of their representations. Our algorithm is a variant of Aiken and Murphy's set-inclusion constraint solver, to which are added several new implementation techniques, correctness proofs, and preliminary performance measurements on some small programs in the domain of typed XML processing.",Subtyping; Type systems; XML,Algorithms; Automata theory; Computational complexity; Semantics; Set theory; Theorem proving; Correctness proofs; Implementation techniques; Subtyping; Type systems; XML
A tail-recursive machine with stack inspection,2004,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-24644501711&doi=10.1145%2f1034774.1034778&partnerID=40&md5=021c17ece7b5587b1b3a484316d0ccf0,"Security folklore holds that a security mechanism based on stack inspection is incompatible with a global tail call optimization policy; that an implementation of such a language must allocate memory for a source-code tail call, and a program that uses only tail calls (and no other memory-allocating construct) may nevertheless exhaust the available memory. In this article, we prove this widely held belief wrong. We exhibit an abstract machine for a language with security stack inspection whose space consumption function is equivalent to that of the canonical tail call optimizing abstract machine. Our machine is surprisingly simple and suggests that tail calls are as easy to implement in a security setting as they are in a conventional one. © 2004 ACM.",Stack inspection; Tail call optimization; Tail recursion,Computer programming languages; Optimization; Storage allocation (computer); Optimization policy; Stack compensation; Tail call optimization; Tail recursive; Security of data
The pattern calculus,2004,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-24644462277&doi=10.1145%2f1034774.1034775&partnerID=40&md5=ad5d3b54ef08e2a1c82c1f03a12afcf7,There is a significant class of operations such as mapping that are common to all data structures. The goal of generic programming is to support these operations on arbitrary data types without having to recode for each new type. The pattern calculus and combinatory type system reach this goal by representing each data structure as a combination of names and a finite set of constructors. These can be used to define generic functions by pattern-matching programs in which each pattern has a different type. Evaluation is type-free. Polymorphism is captured by quantifying over type variables that represent unknown structures. A practical type inference algorithm is provided. © 2004 ACM.,Constructor calculus; Functional programming; Generic programming; Pattern calculus; Pattern-matching,Algorithms; Functions; Mathematical techniques; Pattern matching; Constructor calculus; Functional programming; Generic programming; Pattern calculus; Data structures
Automatic tiling of iterative stencil loops,2004,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-24644456455&doi=10.1145%2f1034774.1034777&partnerID=40&md5=86ac9258c70961a3643b464e59eb4ea6,"Iterative stencil loops are used in scientific programs to implement relaxation methods for numerical simulation and signal processing. Such loops iteratively modify the same array elements over different time steps, which presents opportunities for the compiler to improve the temporal data locality through loop tiling. This article presents a compiler framework for automatic tiling of iterative stencil loops, with the objective of improving the cache performance. The article first presents a technique which allows loop tiling to satisfy data dependences in spite of the difficulty created by imperfectly nested inner loops. It does so by skewing the inner loops over the time steps and by applying a uniform skew factor to all loops at the same nesting level. Based on a memory cost analysis, the article shows that the skew factor must be minimized at every loop level in order to minimize cache misses. A graph-theoretical algorithm, which takes polynomial time, is presented to determine the minimum skew factor. Furthermore, the memory-cost analysis derives the tile size which minimizes capacity misses. Given the tile size, an efficient and general array-padding scheme is applied to remove conflict misses. Experiments were conducted on 16 test programs and preliminary results showed an average speedup of 1.58 and a maximum speedup of 5.06 across those test programs. © 2004 ACM.",Caches; Loop transformations; Optimizing compilers,Buffer storage; Computer simulation; Iterative methods; Polynomials; Caches; Iterative stencil loops; Loop transformations; Optimizing compilers; Computer programming
"A fast, memory-efficient register allocation framework for embedded systems",2004,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-24644491283&doi=10.1145%2f1034774.1034776&partnerID=40&md5=6e0c5fa3b309f10d2abe8b59f89f0d9a,"In this work, we describe a ""just-in-time,"" usage density-based register allocator geared toward embedded systems with a limited general-purpose register set wherein speed, code size, and memory requirements are of equal concern. The main attraction of the allocator is that it does not make use of the traditional live range and interval analysis nor does it perform advanced optimizations based on range splitting but results in very good code quality. We circumvent the need for traditional analysis by using a measure of usage density of a variable. The usage density of a variable at a program point represents both the frequency and the density of the uses. We contend that by using this measure we can capture both range and frequency information which is essentially used by the good allocators based on splitting. We describe a framework based on this measure which has a linear complexity in terms of the program size. We perform comparisons with the static allocators based on graph coloring and the ones targeted toward just-in-time compilation systems like linear scan of live ranges. Through comparisons with graph coloring (Brigg's style) and live range-based (linear scan) allocators, we show that the memory footprint and the size of our allocator are smaller by 20% to 30%. The speed of allocation is comparable and the speed of the generated code is better and its size smaller. These attributes make the allocator an attractive candidate for performing a fast, memory-efficient register allocation for embedded devices with a small number of registers. © 2004 ACM.",Code generation; Compiler optimizations; Compilers; Dynamic compilation; Embedded systems; Register allocation,Codes (symbols); Graph theory; Optimization; Storage allocation (computer); Code generation; Compiler optimizations; Compilers; Dynamic compilation; Register allocation; Embedded systems
Modular typechecking for hierarchically extensible datatypes and functions,2004,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-11344283305&doi=10.1145%2f1018203.1018207&partnerID=40&md5=9853eb3b6b73ce42a776cd82d91f2353,"One promising approach for adding object-oriented (OO) facilities to functional languages like ML is to generalize the existing datatype and function constructs to be hierarchical and extensible, so that datatype variants simulate classes and function cases simulate methods. This approach allows existing datatypes to be easily extended with both new operations and new variants, resolving a longstanding conflict between the functional and OO styles. However, previous designs based on this approach have been forced to give up modular typechecking, requiring whole-program checks to ensure type safety. We describe Extensible ML (EML), an ML-like language that supports hierarchical, extensible datatypes and functions while preserving purely modular typechecking. To achieve this result, EML'S type system imposes a few requirements on datatype and function extensibility, but EML is still able to express both traditional functional and OO idioms. We have formalized a core version of EML and proven the associated type system sound, and we have developed a prototype interpreter for the language.",Extensible datatypes; Extensible functions; Modular typechecking,Computer programming languages; Functions; Hierarchical systems; Modula (programming language); Theorem proving; XML; Extensible datatypes; Extensible functions; Idioms; Modular typechecking; Object oriented programming
Symmetry and reduced symmetry in model checking,2004,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-4344617683&doi=10.1145%2f1011508.1011511&partnerID=40&md5=3e998ecfa530d70a047287d37a0fa0fc,"Symmetry reduction methods exploit symmetry in a system in order to efficiently verify its temporal properties. Two problems may prevent the use of symmetry reduction in practice: (1) the property to be checked may distinguish symmetric states and hence not be preserved by the symmetry, and (2) the system may exhibit little or no symmetry, In this article, we present a general framework that addresses both of these problems. We introduce ""Guarded Annotated Quotient Structures"" for compactly representing the state space of systems even when those are asymmetric. We then present algorithms for checking any temporal property on such representations, including non-symmetric properties.",Formula decomposition; Model checking algorithms and tools; State space explosion; Symmetry reductions; Temporal logics,Algorithms; Computer aided analysis; Computer simulation; Computer software; Formal logic; Formula decomposition; Model checking; State space explosion; Symmetry reductions; Temporal logic; Computer programming languages
Modem concurrency abstractions for C,2004,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-11344295053&doi=10.1145%2f1018203.1018205&partnerID=40&md5=0bcd7b3edb33af2a55b8fbd0488fe6e9,"Polyphonie C# is an extension of the C# language with new asynchronous concurrency constructs, based on the join calculus. We describe the design and implementation of the language and give examples of its use in addressing a range of concurrent programming problems.",Asynchrony; Chords; Events; Join calculus; Messages; Polyphonic c; Synchronization; Threads,Abstracting; Calculations; Codes (symbols); Computer programming languages; Concurrency control; Mathematical models; Problem solving; Synchronization; Asynchrony; Chords; Events; Join calculus; Messages; Polyphonic C#; Threads; Object oriented programming
A semantics for advice and dynamic join points in aspect-oriented programming,2004,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-11344289845&doi=10.1145%2f1018203.1018208&partnerID=40&md5=02ebb87f89b0881182a38eee8cf54ff8,"A characteristic of aspect-oriented programming, as embodied in AspectJ, is the use of advice and pointcuts to define behavior that crosscuts the structure of the rest of the code. The events during execution at which advice may execute are called join points. A pointcut is a set of join points. An advice is an action to be taken at the join points in a particular pointcut. In this model of aspect-oriented programming, join points are dynamic in that they refer to events during the flow of execution of the program. We give a denotational semantics for a minilanguage that embodies the key features of dynamic join points, pointcuts, and advice. This is the first semantics for aspect-oriented programming that handles dynamic join points and recursive procedures. It is intended as a baseline semantics against which future correctness results may be measured.",Advice; Aspect Sand Box; Aspect-oriented programming; AspectJ; Join point; Pointcut,Codes (symbols); Computer programming languages; Computer systems programming; Object oriented programming; Recursive functions; Advice; Aspect sand box; Aspect-oriented prorgramming; AspectJ; Join point; Pointcut; Semantics
Extensible objects without labels,2004,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-11344259515&doi=10.1145%2f1018203.1018206&partnerID=40&md5=28a299ba19b404fa40d9e9ca82ec72a5,"Typed object calculi that permit adding new methods to existing objects must address the problem of name clashes: what happens if a new method is added to an object already having one with the same name but a different type? Most systems statically forbid such clashes by restricting the allowable subtypings. In contrast, by reconsidering the runtime meaning of object extension, the object calculus studied in the author's previous work with Jon Riecke allowed any object to be soundly extended with any method of any name, with unrestricted width subtyping. That language permitted a simple encoding of classes as object-generators. Because of width subtyping, subclasses could be typechecked and compiled with little knowledge of the class hierarchy and without any information about superclasses' private components; this made derived classes more robust to changes in the implementations of base classes. However, the system was not well suited for encoding mixins or by-name subtyping of objects. This article addresses those deficiencies by presenting the Calculus of Objects and Indices (COI), a lower-level typed object calculus in which extensible objects are more analogous to tuples than to records. An object is simply a finite sequence of unnamed components referenced by their index in the sequence. Names are then reintroduced by allowing these indices to be first-class values (analogous to pointers to members in C++) that can be bound to variables. Since variablesunlike record labels-freely alpha-vary, difficulties caused by statically undetectable name clashes disappear. By combining COI objects with standard type-theoretic mechanisms, one can encode mixins and classes having the by-name subtyping of languages like C++ or Java but with the robustness of the object-generator encodings. Using records, more standard extensible objects with named components can also be encoded.",Extensible objects; Object calculi,Boolean functions; Calculations; Encoding (symbols); Java programming language; Knowledge based systems; Problem solving; Robustness (control systems); Set theory; Theorem proving; Extensible objects; Object calculi; Pointers; Subtypings; Object oriented programming
First-class monadic schedules,2004,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-4344686764&doi=10.1145%2f1011508.1011509&partnerID=40&md5=fcd4f910401d3acbbed6cec374a9c6b8,"Parallel functional languages often use meta-linguistic annotations to provide control over parallel evaluation. In this paper we explore a flexible mechanism to control when an expression is evaluated: first-class monadic schedules. We discuss the advantages of using such first-class values over traditional annotation-based systems. In particular, it is often desirable to make decisions about the operational behavior of parallel programs depending on the dynamic state of the system. For example, we may want to measure the system load before deciding to evaluate expressions in parallel. For this purpose, we show how monads can be used to access dynamic system parameters in a referentially transparent manner (up to termination). As a mechanism to reason about schedules, we present a set of algebraic properties that any implementation of schedules must satisfy. We also describe an implementation that translates schedules into a dialect of Scheme extended with futures. We prove that this implementation satisfies the given set of algebraic properties, and give performance results for a parallel solution to the n-body problem using the Barnes-Hut method. Although our ideas were developed specifically for nonstrict functional languages such as Haskell, we briefly discuss how they can be used with strict functional languages and imperative languages as well.",Languages,Calculations; Computer simulation; Computer systems; Multiprocessing programs; Parallel processing systems; Program compilers; Functional languages; Multiprocessors; Parallel programs; Computer programming languages
Optimistic register coalescing,2004,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-4344611593&doi=10.1145%2f1011508.1011512&partnerID=40&md5=f51cb44c24b340a35ea0fe4807259bac,"Graph-coloring register allocators eliminate copies by coalescing the source and target nodes of a copy if they do not interfere in the interference graph. Coalescing, however, can be harmful to the colorability of the graph because it tends to yield a graph with nodes of higher degrees. Unlike aggressive coalescing, which coalesces any pair of noninterfering copy-related nodes, conservative coalescing or iterated coalescing perform safe coalescing that preserves the colorability. Unfortunately, these heuristics give up coalescing too early, losing many opportunities for coalescing that would turn out to be safe. Moreover, they ignore the fact that coalescing may even improve the colorability of the graph by reducing the degree of neighbor nodes that are interfering with both the source and target nodes being coalesced. This article proposes a new heuristic called optimistic coalescing which optimistically performs aggressive coalescing, thus exploiting the positive impact of coalescing aggressively, but when a coalesced node is to be spilled, it is split back into separate nodes. Since there is a better chance of coloring one of those splits, we can reduce the overall spill amount.",Copy coalescing; Graph coloring; Noncopy coalescing; Register allocation,Computational geometry; Computer simulation; Heuristic methods; Logic design; Logic programming; Copy coalescing; Graph coloring; Noncopy coalescing; Register allocation; Computer programming languages
Polymorphic specialization for ML,2004,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-4344650953&doi=10.1145%2f1011508.1011510&partnerID=40&md5=459b04f4567c560dff5b22cd5d8df44b,"We present a framework for offline partial evaluation for call-by-value functional programming languages with an ML-style typing discipline. This includes a binding-time analysis which is (1) polymorphic with respect to binding times; (2) allows the use of polymorphic recursion with respect to binding times; (3) is applicable to a polymorphically typed term; and (4) is proven correct with respect to a novel small-step specialization semantics. The main innovation is to build the analysis on top of the region calculus of Tofte and Talpin [1994], thus leveraging the tools and techniques developed for it. Our approach factorizes the binding-time analysis into region inference and a subsequent constraint analysis. The key insight underlying our framework is to consider binding times as properties of regions. Specialization is specified as a small-step semantics, building on previous work on syntactic-type soundness results for the region calculus. Using similar syntactic proof techniques, we prove soundness of the binding-time analysis with respect to the specializer. In addition, we prove that specialization preserves the call-by-value semantics of the region calculus by showing that the reductions of the specializer are contextual equivalences in the region calculus.",Binding-time analysis; Program specialization; Regions,Artificial intelligence; Calculations; Computer simulation; Logic programming; Program compilers; Semantics; Binding-time analysis; Program specialization; Program synthesis; Regions; Computer programming languages
An unfold/fold transformation framework for definite logic programs,2004,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-4444258176&doi=10.1145%2f982158.982160&partnerID=40&md5=5087aa1921b4b853498aa81a9b7596f1,"An unfold/fold transformation framework for definite logic programs is presented. Unfold/fold transformations have been widely used for improving program efficiency and for reasoning about programs. Existing unfold/fold transformations for logic programs restrict the application of folding by placing conditions that are sufficient to guarantee the correctness of folding. The proposed framework places no syntactic restriction on the application of folding and it can be used to derive transformation systems. A new transformation system, SCOUT is derived as an instance of the framework and its power relative to the existing transformation systems.",Folding; Inductive theorem proving; Logic programming; Program transformation; Unfolding,Approximation theory; Automation; Computer programming languages; Encoding (symbols); Optimization; Parameter estimation; Semantics; Theorem proving; Circularity; Folding; Program transformation; Unfold/fold transformation systems; Logic programming
Natural semantics as a static program analysis framework,2004,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-4444295305&doi=10.1145%2f982158.982161&partnerID=40&md5=fe28ff0b6bfc3ad9c93c51bf05d9975c,"Natural semantics specifications have become mainstream in the formal specification of programming language semantics during the last 10 years. In this article, we set up sorted natural semantics as a specification framework which is able to express static semantic information of programming languages declaratively in a uniform way and allows one at the same time to generate corresponding analyses. Such static semantic information comprises context-sensitive properties which are checked in the semantic analysis phase of compilers as well as further static program analyses such as, for example, classical data and control flow analyses or type and effect systems. The latter require fixed-point analyses to determine their solutions. We show that, given a sorted natural semantics specification, we can generate the corresponding analysis. Therefore, we classify the solution of such an analysis by the notion of a proof tree. We show that a proof tree can be computed by solving an equivalent residuation problem. In case of the semantic analysis, this solution can be found by a basic algorithm. We show that its efficiency can be enhanced using solution strategies. We also demonstrate our prototype implementation of the basic algorithm which proves its applicability in practical situations. With the results of this article, we have established natural semantics as a framework which closes the gap between declarative and operational specification methods for static semantic properties as well as between specification frameworks for the semantic analysis. In particular, we show that natural semantics is expressive enough to define fixed-point program analyses.",Compiler generators; Constraint solving; Fixed-point program analyses; Natural semantics; Semantic analysis; Static program analysis,Algorithms; Computational methods; Data reduction; Problem solving; Semantics; Specifications; Compiler generators; Constraint solving; Fixed-point program analysis; Static program analysis; Computer programming languages
A framework for the integration of partial evaluation and abstract interpretation of logic programs,2004,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-4444259355&doi=10.1145%2f982158.982159&partnerID=40&md5=5f00ab2250c28b7ab96c5b3a931f15d0,"Recently the relationship between abstract interpretation and program specialization has received a lot of scrutiny, and the need has been identified to extend program specialization techniques so as to make use of more refined abstract domains and operators. This article clarifies this relationship in the context of logic programming, by expressing program specialization in terms of abstract interpretation. Based on this, a novel specialization framework, along with generic correctness results for computed answers and finite failure under SLD-resolution, is developed. This framework can be used to extend existing logic program specialization methods, such as partial deduction and conjunctive partial deduction, to make use of more refined abstract domains. It is also shown how this opens up the way for new optimizations. Finally, as shown in the paper, the framework also enables one to prove correctness of new or existing specialization techniques in a simpler manner. The framework has already been applied in the literature to develop and prove correct specialization algorithms using regular types, which in turn have been applied to the verification of infinite state process algebras.",Abstract interpretation; Flow analysis; Logic programming; Partial deduction; Partial evaluation; Program transformation,Abstracting; Algebra; Information analysis; Mathematical transformations; Optimization; Program compilers; Abstract interpretation; Partial deduction; Partial evaluation; Program transformation; Logic programming
JR: Flexible distributed programming in an extended Java,2004,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-4444343319&doi=10.1145%2f982158.982162&partnerID=40&md5=f0526313f743de8a06b12417389a6654,"Java provides a clean object-oriented programming model and allows for inherently system-independent programs. Unfortunately, Java has a limited concurrency model, providing only threads and remote method invocation (RMI). The JR programming language extends Java to provide a rich concurrency model, based on that of SR. JR provides dynamic remote virtual machine creation, dynamic remote object creation, remote method invocation, asynchronous communication, rendezvous, and dynamic process creation, JR's concurrency model stems from the addition of operations (a generalization of procedures) and JR supports the redefinition of operations through inheritance. JR programs are written in an extended Java and then translated into standard Java programs. The JR run-time support system is also written in standard Java. This paper describes the JR programming language and its implementation. Some initial measurements of the performance of the implementation are also included.",Concurrency; Concurrent object-oriented programming; Java; SR,Abstracting; Benchmarking; Codes (symbols); Mathematical models; Optimization; Semantics; Synchronization; Syntactics; Concurrent object-oriented programming; Dynamic processes; Remote method invocation (RMI); Virtual machine; Java programming language
Hancock: A language for analyzing transactional data streams,2004,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-1842735907&doi=10.1145%2f973097.973100&partnerID=40&md5=6f298ff4d2e07d9913d4a17b787ab388,"Massive transaction streams present a number of opportunities for data mining techniques. The transactions in such streams might represent calls on a telephone network, commercial credit card purchases, stock market trades, or HTTP requests to a web server. While historically such data have been collected for billing or security purposes, they are now being used to discover how the transactors, for example, credit-card number or IP addresses, uses the associated services. Over the past 5 years, we have computed evolving profiles (called signatures) of transactors in several very large data streams. The signature for each transactor captures the salient features of his or her behavior through time. Programs for processing signatures must be highly optimized because of the size of the data stream (several gigabytes per day) and the number of signatures to maintain (hundreds of millions). Originally, we wrote such programs directly in C, but because these programs often sacrificed readability for performance, they were difficult to verify and maintain. Hancock is a domain-specific language we created to express computationally efficient signature programs cleanly. In this paper, we describe the obstacles to computing signatures from massive streams and explain how Hancock addresses these problems. For expository purpose, we present Hancock using a running example from the telecommunications industry; however, the language itself is general and applies equally well to other data sources.",Data mining; Domain-specific languages; Statistical models,Computer architecture; Computer simulation; Data mining; Database systems; Pattern recognition; Statistical methods; Domain-specific languages; Statistical models; Computer programming languages
Traversals of object structures: Specification and efficient implementation,2004,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-1842685662&doi=10.1145%2f973097.973102&partnerID=40&md5=9614ab37bd0328695fdb0666fafc3f9b,"Separation of concerns and loose coupling of concerns are important issues in software engineering. In this paper we show how to separate traversal-related concerns from other concerns, how to loosely couple traversal-related concerns to the structural concern, and how to efficiently implement traversal-related concerns. The stress is on the detailed description of our algorithms and the traversal specifications they operate on. Traversal of object structures is a ubiquitous routine in most types of information processing. Ad hoc implementations of traversals lead to scattered and tangled code and in this paper we present a new approach, called traversal strategies, to succinctly modularize traversals. In our approach traversals are defined using a high-level directed graph description, which is compiled into a dynamic road map to assist run-time traversals. The complexity of the compilation algorithm is polynomial in the size of the traversal strategy graph and the class of graph of the given application. Prototypes of the system have been developed and are being successfully used to implement traversals for Java and Aspect J [Kiczales et al. 2001] and for generating adapters for software components. Our previous approach, called traversal specifications [Lieberherr 1992; Palsberg et al. 1995], was less general and less succinct, and its compilation algorithm was of exponential complexity in some cases. In an additional result we show that this bad behavior is inherent to the static traversal code generated by previous implementations, where traversals are carried out by invoking methods without parameters.",Adaptive programming; Aspect-oriented programming; Class graphs; Low of demeter; Object graphs; Strategy graphs; Structure-shy software,Computer hardware description languages; Computer software reusability; Data structures; Database systems; Graph theory; Java programming language; Software engineering; High-level directed graph description; Traversal specifications; Traversal strategies; Algorithms
Logic of global synchrony,2004,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-1842786452&doi=10.1145%2f973097.973098&partnerID=40&md5=7a849237bf607e00ec278416d8f3869e,"An intermediate-level specification formalism (i.e., specification language supported by laws and a semantic model), Logs, is presented for PRAM and BSP styles of parallel programming. It extends pre-post sequential semantics to reveal states at points of global synchronization. The result is an integration of the pre-post and reactive process styles of specification. The language consists of only six commands from which other useful commands can be derived. Parallel composition is simply logical conjunction and hence compositional. A simple predicative semantics and a complete set of algebraic laws are presented. Novel ingredients include the separation, in our reactive context, of the processes for nontermination and for abortion which coincide in standard programming models; the use of partitions, combining the terminating behavior of one program with the nonterminating behavior of another; and a fixpoint operator, the partitioned fixpoint. Our semantics benefits from the recent ""healthiness function"" approach for predicative semantics. Use of Logs, along with the laws for reasoning about it, is demonstrated on two problems: matrix multiplication (a terminating numerical computation) and the dining philosophers (a reactive computation). The style of reasoning is so close to programming practice that direct transformation from Logs specifications to real PRAM and BSP programs becomes possible.",Bulk-synchronous parallelism; PRAM; Reactive programming,Algorithms; Boolean algebra; Computer programming; Formal logic; Parallel processing systems; Semantics; Synchronization; Bulk-synchronous parallelism; Parallel random-access machine; Reactive programming; Computer hardware description languages
A fast and accurate framework to analyze and optimize cache memory behavior,2004,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-1842635044&doi=10.1145%2f973097.973099&partnerID=40&md5=431aee6952e3e84dd81609094e688681,"The gap between processor and main memory performance increases every year. In order to overcome this problem, cache memories are widely used. However, they are only effective when programs exhibit sufficient data locality. Compile-time program transformations can significantly improve the performance of the cache. To apply most of these transformations, the compiler requires a precise knowledge of the locality of the different sections of the code, both before and after being transformed. Cache miss equations (CMEs) allow us to obtain an analytical and precise description of the cache memory behavior for loop-oriented codes. Unfornately, a direct solution of the CMEs is computationally intractable due to its NP-complete nature. This article proposes a fast and accurate approach to estimate the solution of the CMEs. We use sampling techniques to approximate the absolute miss ratio of each reference by analyzing a small subset of the iteration space. The size of the subset, and therefore the analysis time, is determined by the accuracy selected by the user. In order to reduce the complexity of the algorithm to solve CMEs, effective mathematical techniques have been developed to analyze the subset of the iteration space that is being considered. These techniques exploit some properties of the particular polyhedra represented by CMEs.",Cache memories; Optimization; Sampling,Algorithms; Computational complexity; Mathematical techniques; Probability distributions; Program compilers; Sampling; Cache miss equations; Cache memory
Modular refinement of hierarchic reactive machines,2004,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-1842786418&doi=10.1145%2f973097.973101&partnerID=40&md5=764d1330fa3fd9efb65e45121e2915a4,"Scalable formal analysis of reactive programs demands integration of modular reasoning techniques with existing analysis tools. Modular reasoning principles such as abstraction, compositional refinement, and ssume-guarantee reasoning are well understood for architectural hierarchy that describes the communication structure between component processes, and have been shown to be useful. In this paper, we develop the theory of modular reasoning for behavior hierarchy that describes control structure using hierarchic modes. From statecharts to UML, behavior hierarchy has been an integral component of many software design languages, but only syntactically. We present the hierarchic reactive modules language that retains powerful features such as nested modes, mode reuse, exceptions, group transitions, history, and conjunctive modes, and yet has a semantic notion of mode hierarchy. We present an observational trace semantics for modes that provides the basis for mode refinement. We show the refinement to be compositional with respect to the mode constructors, and develop an assume-guarantee reasoning principle.",Assume-guarantee reasoning; Compositional semantics; Hierarchical state machines; Refinement,Computational methods; Computer hardware description languages; Computer software selection and evaluation; Semantics; Software engineering; Syntactics; Assume-guarantee reasoning; Compositional semantics; Hierarchic reactive machines; Refinement; Computer programming languages
An Interval Constraint System for Lattice Domains,2004,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-10744226620&doi=10.1145%2f963778.963779&partnerID=40&md5=7cc1fcf6c3983f640d76c75e177535e0,"We present a generic framework for defining and solving interval constraints on any set of domains (finite or infinite) that are lattices. The approach is based on the use of a single form of constraint similar to that of an indexical used by CLP for finite domains and on a particular generic definition of an interval domain built from an arbitrary lattice. We provide the theoretical foundations for this framework and a schematic procedure for the operational semantics. Examples are provided that illustrate how new (compound) constraint solvers can be constructed from existing solvers using lattice combinators and how different solvers (possibly on distinct domains) can communicate and hence, cooperate in solving a problem. We describe the language clp(ℒ), which is a prototype implementation of this framework and discuss ways in which this implementation may be improved.",Constraint; Cooperation; Indexicals; Lattice; Propagation,Algorithms; Computer programming languages; Constraint theory; Formal logic; Problem solving; Semantics; Indexicals; Lattice domains; Computer programming
Access Control for Mobile Agents: The Calculus of Boxed Ambients,2004,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-1342347429&doi=10.1145%2f963778.963781&partnerID=40&md5=c9dbaa61a0857493808706f2376b0dbe,"Boxed Ambients are a variant of Mobile Ambients that result from dropping the open capability and introducing new primitives for ambient communication. The new model of communication is faithful to the principles of distribution and location-awareness of Mobile Ambients, and complements the constructs in and out for mobility with finer-grained mechanisms for ambient interaction. We introduce the new calculus, study the impact of the new mechanisms for communication of typing and mobility, and show that they yield an effective framework for resource protection and access control in distributed systems.",Access control systems; Ambient calculi; Mobile computation; Type safety; Type systems,Computational methods; Computer programming; Computer programming languages; Formal logic; Resource allocation; Security of data; Calculus; Mobile agents; Software agents
Offline Partial Evaluation Can Be as Accurate as Online Partial Evaluation,2004,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-10744225927&doi=10.1145%2f963778.963784&partnerID=40&md5=e6585f89b65e01812ded466495690c64,"We show that the accuracy of online partial evaluation, or polyvariant specialization based on constant propagation, can be simulated by offline partial evaluation using a maximally polyvariant binding-time analysis. We point out that, while their accuracy is the same, online partial evaluation offers better opportunities for powerful generalization strategies. Our results are presented using a flowchart language with recursive procedures.",Binding-time analysis; Constant propagation; Generalization; Metacomputation; Offline partial evaluation; Online partial evaluation; Program specialization,Computer programming languages; Computer simulation; Flowcharting; Formal logic; Online systems; Recursive functions; Semantics; Offline partial evaluation; Online partial evaluation; Program specialization; Computer programming
Send-Receive Considered Harmful: Myths and Realities of Message Passing,2004,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-1242332596&doi=10.1145%2f963778.963780&partnerID=40&md5=d344d10e660504a10729bcf42443b023,"During the software crisis of the 1960s, Dijkstra's famous thesis ""goto considered harmful"" paved the way for structured programming. This short communication suggests that many current difficulties of parallel programming based on message passing are caused by poorly structured communication, which is a consequence of using low-level send-receive primitives. We argue that, like goto in sequential programs, send-receive should be avoided as far as possible and replaced by collective operations in the setting of message passing. We dispute some widely held opinions about the apparent superiority of pairwise communication over collective communication and present substantial theoretical and empirical evidence to the contrary in the context of MPI (Message Passing Interface).",Message Passing Interface (MPI); Programming methodology,Algorithms; Computer programming languages; Computer software; Concurrent engineering; Interfaces (computer); Parallel processing systems; Structured programming; Message passing interfaces (MPI); Sequential programming; Computer networks
On Exponential-Time Completeness of the Circularity Problem for Attribute Grammars,2004,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-1242332609&doi=10.1145%2f963778.963783&partnerID=40&md5=5d3af82734ce51fd70ff1b5243a630b4,"Attribute grammars (AGs) are a formal technique for defining semantics of programming languages. Existing complexity proofs on the circularity problem of AGs are based on automata theory, such as writing pushdown acceptor and alternating Turing machines. They reduced the acceptance problems of above automata, which are exponential-time (EXPTIME) complete, to the AG circularity problem. These proofs thus show that the circularity problem is EXPTIME-hard, at least as hard as the most difficult problems in EXPTIME. However, none has shown that the problem is EXPTIME-complete. This paper presents an alternating Turing machine for the circularity problem. The alternating Turing machine requires polynomial space. Thus, the circularity problem is in EXPTIME and is then EXPTIME-complete.",Alternating Turing machines; Attribute grammars; Circularity problem; EXPTIME-complete,Algorithms; Automata theory; Computer programming languages; Decision making; Formal logic; Polynomials; Program compilers; Semantics; Turing machines; Attribute grammars (AG); Specification errors; Computational grammars
Synthesis of Fault-Tolerant Concurrent Programs,2004,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-1242287765&doi=10.1145%2f963778.963782&partnerID=40&md5=b759377227f4e43df541721656fec7c2,"Methods for mechanically synthesizing concurrent programs from temporal logic specifications obviate the need to manually construct a program and compose a proof of its correctness. A serious drawback of extant synthesis methods, however, is that they produce concurrent programs for models of computation that are often unrealistic. In particular, these methods assume completely fault-free operation, that is, the programs they produce are fault-intolerant. In this paper, we show how to mechanically synthesize fault-tolerant concurrent programs for various fault classes. We illustrate our method by synthesizing fault-tolerant solutions to the mutual exclusion and barrier synchronization problems.",Concurrent programs; Fault-tolerance; Program synthesis; Specification; Temporal logic,Artificial intelligence; Concurrent engineering; Fault tolerant computer systems; Formal logic; Problem solving; Software engineering; Program synthesis; Temporal logic; Computer programming
Fractal symbolic analysis,2003,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-4544346973&doi=10.1145%2f945885.945888&partnerID=40&md5=1624cd55a6afb08657b1024be229e4ba,"Modern compilers restructure programs to improve their efficiency. Dependence analysis is the most widely used technique for proving the correctness of such transformations, but it suffers from the limitation that it considers only the memory locations read and written by a statement without considering what is being computed by that statement. Exploiting the semantics of program statements permits more transformations to be proved correct, and is critical for automatic restructuring of codes such as LU with partial pivoting. One approach to exploiting the semantics of program statements is symbolic analysis and comparison of programs. In principle, this technique is very powerful, but in practice, it is intractable for all but the simplest programs. In this paper, we propose a new form of symbolic analysis and comparison of programs which is appropriate for use in restructuring compilers. Fractal symbolic analysis is an approximate symbolic analysis that compares a program and its transformed version by repeatedly simplifying these programs until symbolic analysis becomes tractable while ensuring that equality of the simplified programs is sufficient to guarantee equality of the original programs. Fractal symbolic analysis combines some of the power of symbolic analysis with the tractability of dependence analysis. We discuss a prototype implementation of fractal symbolic analysis, and show how it can be used to solve the long-open problem of verifying the correctness of transformations required to improve the cache performance of LU factorization with partial pivoting. © 2003 ACM.",Algorithms; Languages; Theory,Algorithms; Buffer storage; Codes (symbols); Computer programming languages; Mathematical transformations; Problem solving; Program compilers; Rapid prototyping; Semantics; Storage allocation (computer); Fractal symbolic analysis; Languages; Memory locations; Program statements; Fractals
Generating LR syntax error messages from examples,2003,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-24644485665&doi=10.1145%2f937563.937566&partnerID=40&md5=ead8b109ab6d1f8498d1012014bc5393,"LR parser generators are powerful and well-understood, but the parsers they generate are not suited to provide good error messages. Many compilers incur extensive modifications to the source grammar to produce useful syntax error messages. Interpreting the parse state (and input token) at the time of error is a nonintrusive alternative that does not entangle the error recovery mechanism in error message production. Unfortunately, every change to the grammar may significantly alter the mapping from parse states to diagnostic messages, creating a maintenance problem. Merr is a tool that allows a compiler writer to associate diagnostic messages with syntax errors by example, avoiding the need to add error productions to the grammar or interpret integer parse states. From a specification of errors and messages, Merr runs the compiler on each example error to obtain the relevant parse state and input token, and generates a yyerror () function that maps parse states and input tokens to diagnostic messages. Merr enables useful syntax error messages in LR-based compilers in a manner that is robust in the presence of grammar changes. © 2003 ACM.",LR parsers; Syntax error messages,Computer hardware description languages; Integer programming; Problem solving; Program compilers; Syntactics; Compilers; Diagnostic messages; LR parsers; Syntax error messages; Error correction
Extending Java for high-level Web service construction,2003,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-1542595072&doi=10.1145%2f945885.945890&partnerID=40&md5=1b8355a26abd2730ed7ee41b201486ec,"We incorporate innovations from the <bigwig> project into the Java language to provide high-level features for Web service programming. The resulting language, JWIG, contains an advanced session model and a flexible mechanism for dynamic construction of XML documents, in particular XHTML. To support program development we provide a suite of program analyses that at compile time verify for a given program that no runtime errors can occur while building documents or receiving form input, and that all documents being shown are valid according to the document type definition for XHTML 1.0. We compare JWIG with Servlets and JSP which are widely used Web service development platforms. Our implementation and evaluation of JWIG indicate that the language extensions can simplify the program structure and that the analyses are sufficiently fast and precise to be practically useful. © 2003 ACM.",Data-flow analysis; Interactive Web services; XML,HTML; Information retrieval systems; Program compilers; World Wide Web; XML; Data-flow analysis; High-level features; Interactive Web services; Program structures; Java programming language
The receptive distributed π-calculus,2003,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-24644440924&doi=10.1145%2f937563.937564&partnerID=40&md5=35e5a7480d44a1059664816eac4acc87,"We study an asynchronous distributed π-calculus, with constructs for localities and migration. We show that a static analysis ensures the receptiveness of channel names, which, together with a simple type system, guarantees the message deliverability property. This property states that any migrating message will find an appropriate receiver at its destination locality. We argue that this distributed, receptive calculus is still expressive enough while allowing for an effective type inference à la ML. © 2003 ACM.",π-Calculus; Distribution; Receptivity; Resource access control,Channel capacity; Computational methods; Resource allocation; π-calculus; Distribution; Receptivity; Resource access control; Distributed computer systems
Stack allocation and synchronization optimizations for Java using escape analysis,2003,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-4544301974&doi=10.1145%2f945885.945892&partnerID=40&md5=34981dbf50c52d3b372eb70ebe9b966a,"This article presents an escape analysis framework for Java to determine (1) if an object is not reachable after its method of creation returns, allowing the object to be allocated on the stack, and (2) if an object is reachable only from a single thread during its lifetime, allowing unnecessary synchronization operations on that object to be removed. We introduce a new program abstraction for escape analysis, the connection graph, that is used to establish reachability relationships between objects and object references. We show that the connection graph can be succinctly summarized for each method such that the same summary information may be used in different calling contexts without introducing imprecision into the analysis. We present an interprocedural algorithm that uses the above property to efficiently compute the connection graph and identify the nonescaping objects for methods and threads. The experimental results, from a prototype implementation of our framework in the IBM High Performance Compiler for Java, are very promising. The percentage of objects that may be allocated on the stack exceeds 70% of all dynamically created objects in the user code in three out of the ten benchmarks (with a median of 19%); 11% to 92% of all mutex lock operations are eliminated in those 10 programs (with a median of 51%), and the overall execution time reduction ranges from 2% to 23% (with a median of 7%) on a 333-MHz PowerPC workstation with 512 MB memory. © 2003 ACM.",Connection graphs; Escape analysis; Points-to graph,Abstracting; Algorithms; Benchmarking; Codes (symbols); Computer workstations; Graph theory; Information retrieval; Optimization; Program compilers; Storage allocation (computer); Connection graphs; Escape analysis; Points-to graph; Stack allocation; Java programming language
Class analyses as abstract interpretations of trace semantics,2003,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-19044400852&doi=10.1145%2f937563.937565&partnerID=40&md5=826fb286c7d8d1a78e3dad4c13c349f3,"We use abstract interpretation to abstract a compositional trace semantics for a simple imperative object-oriented language into its projection over a set of program points called watchpoints. We say that the resulting watchpoint semantics is focused on the watchpoints. Every abstraction of the computational domain of this semantics induces an abstract, still compositional, and focused watchpoint semantics. This establishes a basis for developing static analyses obtaining information pertaining only to the watchpoints. As an example, we consider three domains for class analysis of object-oriented programs derived from three techniques present in the literature, namely, rapid type analysis, a simple dataflow analysis, and a constraint-based analysis. We obtain three static analyses which are provably correct and whose abstract operations are provably optimal. Moreover, we prove that our formalization of the constraint-based analysis is more precise than that of the other two analyses. We have implemented our watchpoint semantics and our three domains for class analysis. This implementation shows that the time and space costs of the analysis are actually proportional to the number of watchpoints, as a consequence of the focused nature of the watchpoint semantics. © 2003 ACM.",Abstract interpretation; Class analysis; Denotational semantics,Abstracting; Computational methods; Constraint theory; Data flow analysis; Information retrieval; Object oriented programming; Program compilers; Abstract interpretations; Class analysis; Constraint-based analysis; Denotational semantics; Semantics
Escape analysis for Java™: Theory and practice,2003,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-23444455434&doi=10.1145%2f945885.945886&partnerID=40&md5=295d43491bd513c9b859a0793b8e74ca,"Escape analysis is a static analysis that determines whether the lifetime of data may exceed its static scope. This paper first presents the design and correctness proof of an escape analysis for Java™. This analysis is interprocedural, context sensitive, and as flow-sensitive as the static single assignment form. So, assignments to object fields are analyzed in a flow-insensitive manner. Since Java is an imperative language, the effect of assignments must be precisely determined. This goal is achieved thanks to our technique using two interdependent analyses, one forward, one backward. We introduce a new method to prove the correctness of this analysis, using aliases as an intermediate step. We use integers to represent the escaping parts of values, which leads to a fast and precise analysis. Our implementation [Blanchet 1999], which applies to the whole Java language, is then presented. Escape analysis is applied to stack allocation and synchronization elimination. In our benchmarks, we stack allocate 13% to 95% of data, eliminate more than 20% of synchronizations on most programs (94% and 99% on two examples) and get up to 43% runtime decrease (21% on average). Our detailed experimental study on large programs shows that the improvement comes more from the decrease of the garbage collection and allocation times than from improvements on data locality, contrary to what happened for ML. This comes from the difference in the garbage collectors. © 2003 ACM.",Java; Optimization; Stack allocation; Static analysis; Synchronization elimination,Benchmarking; Computer aided design; Data reduction; Integer programming; Optimization; Synchronization; Java; Stack allocation; Static analysis; Synchronization elimination; Java programming language
Jam - Designing a Java extension with mixins,2003,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-24644442584&doi=10.1145%2f937563.937567&partnerID=40&md5=0dcf88bb7e8eec362a26f963cf8d3fcb,"In this paper we present Jam, an extension of the Java language supporting mixins, that is, parametric heir classes. A mixin declaration in Jam is similar to a Java heir class declaration, except that it does not extend a fixed parent class, but simply specifies the set of fields and methods a generic parent should provide. In this way, the same mixin can be instantiated on many parent classes, producing different heirs, thus avoiding code duplication and largely improving modularity and reuse. Moreover, as happens for classes and interfaces, mixin names are reference types, and all the classes obtained by instantiating the same mixin are considered subtypes of the corresponding type, and hence can be handled in a uniform way through the common interface. This possibility allows a programming style where different ingredients are ""mixed"" together in defining a class; this paradigm is somewhat similar to that based on multiple inheritance, but avoids its complication. The language has been designed with the main objective in mind to obtain, rather than a new theoretical language, a working and smooth extension of Java. That means, on the design side, that we have faced the challenging problem of integrating the Java overall principles and complex type system with this new notion; on the implementation side, it means that we have developed a Jam-to-Java translator which makes Jam sources executable on every Java Virtual Machine. © 2003 ACM.",Java; Language design,Codes (symbols); Computer aided design; Interfaces (computer); Problem solving; Virtual storage; Code duplication; Complex type systems; Java; Language design; Java programming language
Automatic program specialization for Java,2003,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-3042806118&doi=10.1145%2f778559.778561&partnerID=40&md5=b38033f3364870647ec33cf869ee1343,"The object-oriented style of programming facilitates program adaptation and enhances program genericness, but at the expense of efficiency. We demonstrate experimentally that state-of-the-art Java compilers fail to compensate for the use of object-oriented abstractions in the implementation of generic programs, and that program specialization can eliminate a significant portion of these overheads. We present an automatic program specializer for Java, illustrate its use through detailed case studies, and demonstrate experimentally that it can significantly reduce program execution time. Although automatic program specialization could be seen as being subsumed by existing optimizing compiler technology, we show that specialization and compiler optimization are in fact complementary. © 2003 ACM.",Automatic program specialization; Java; Object-oriented languages; Optimization; Partial evaluation,Abstracting; Optimization; Program compilers; Automatic program specialization; Compiler optimization; Java; Partial evaluation; Java programming language
Continuous program optimization: A case study,2003,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-1542497130&doi=10.1145%2f778559.778562&partnerID=40&md5=9e288c7239eeda9b317d8aebd5a93d67,"Much of the software in everyday operation is not making optimal use of the hardware on which it actually runs. Among the reasons for this discrepancy are hardware/software mismatches, modularization overheads introduced by software engineering considerations, and the inability of systems to adapt to users' behaviors. A solution to these problems is to delay code generation until load time. This is the earliest point at which a piece of software can be fine-tuned to the actual capabilities of the hardware on which it is about to be executed, and also the earliest point at wich modularization overheads can be overcome by global optimization. A still better match between software and hardware can be achieved by replacing the already executing software at regular intervals by new versions constructed on-the-fly using a background code re-optimizer. This not only enables the use of live profiling data to guide optimization decisions, but also facilitates adaptation to changing usage patterns and the late addition of dynamic link libraries. This paper presents a system that provides code generation at load-time and continuous program optimization at run-time. First, the architecture of the system is presented. Then, two optimization techniques are discussed that were developed specifically in the context of continuous optimization. The first of these optimizations continually adjusts the storage layouts of dynamic data structures to maximize data cache locality, while the second performs profile-driven instruction re-scheduling to increase instruction-level parallelism. These two optimizations have very different cost/benefit ratios, presented in a series of benchmarks. The paper concludes with an outlook to future research directions and an enumeration of some remaining research problems. The empirical results presented in this paper make a case in favor of continuous optimization, but indicate that it needs to be applied judiciously. In many situations, the costs of dynamic optimizations outweigh their benefit, so that no break-even point is ever reached. In favorable circumstances, on the other hand, speed-ups of over 120% have been observed. It appears as if the main beneficiaries of continuous optimization are shared libraries, which at different times can be optimized in the context of the currently dominant client application. © 2003 ACM.",Continuous program optimization; Dynamic code generation; Dynamic reoptimization,Benchmarking; Buffer storage; Computer architecture; Computer hardware; Computer software; Cost benefit analysis; Data storage equipment; Data structures; Decision theory; Digital libraries; Optimization; Problem solving; Software engineering; Continuous program optimization; Dynamic code generation; Dynamic reoptimization; Research problems; Program compilers
Stack Inspection: Theory and Variants,2003,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0346902045&doi=10.1145%2f641909.641912&partnerID=40&md5=eaf3ecc416e3db85873fabb12239a683,"Stack inspection is a security mechanism implemented in runtimes such as the JVM and the CLR to accommodate components with diverse levels of trust. Although stack inspection enables the fine-grained expression of access control policies, it has rather a complex and subtle semantics. We present a formal semantics and an equational theory to explain how stack inspection affects program behavior and code optimisations. We discuss the security properties enforced by stack inspection, and also consider variants with stronger, simpler properties.",Access control; Contextual equivalence; Equational reasoning; Operational semantics; Stack inspection,Codes (symbols); Computer programming languages; Context sensitive languages; Data structures; Inspection; Optimization; Security of data; Semantics; Syntactics; Access control methods; Equational reasoning; Software inspection; Software verification; Computer software
Rank 2 intersection types for local definitions and conditional expressions,2003,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-1242308664&doi=10.1145%2f778559.778560&partnerID=40&md5=d530ee6890c9a6308d89f062039a60cb,"We propose a rank 2 intersection type system with new typing rules for local definitions (let-expressions and letrec-expressions) and conditional expressions (if-expressions and match-expressions). This is a further step towards the use of intersection types in ""real"" programming languages. The technique for typing local definitions relies entirely on the principal typing property (i.e. it does not depend on particulars of rank 2 intersection), so it can be applied to any system with principal typings. The technique for typing conditional expressions, which is based on the idea of introducing metrics on types to ""limit the use"" of the intersection type constructor in the types assigned to the branches of the conditionals, is instead tailored to rank 2 intersection. However, the underlying idea might also be useful for other type systems. © 2003 ACM.",Polymorphic recursion; Principal typings; Type inference,Computer simulation; Data reduction; Metric system; Conditional expressions; Polymorphic recursion; Principal typings; Type inference; Computer programming languages
A Foundation for Embedded Languages,2003,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0348162505&doi=10.1145%2f641909.641910&partnerID=40&md5=ec6c37fbda41414524615926982288db,"Recent work on embedding object languages into Haskell use ""phantom types"" (i.e., parameterized types whose parameter does not occur on the right-hand side of the type definition) to ensure that the embedded object-language terms are simply typed. But is it a safe assumption that only simply-typed terms can be represented in Haskell using phantom types? And conversely, can all simply-typed terms be represented in Haskell under the restrictions imposed by phantom types? In this article we investigate the conditions under which these assumptions are true: We show that these questions can be answered affirmatively for an idealized Haskell-like language and discuss to which extent Haskell can be used as a meta-language.",Completeness; Embedded languages; Haskell; Higher-order abstract syntax; Phantom types; Standard ML; Type safety; Type soundness,Abstracting; Computational linguistics; Computer programming languages; Data structures; Mathematical models; Object oriented programming; Semantics; Subroutines; Code generation; Meta-languages; Monomorphic languages; Embedded systems
Eliminating Synchronization Bottlenecks Using Adaptive Replication,2003,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0346271709&doi=10.1145%2f641909.641911&partnerID=40&md5=da9238d2455e9cf70d8a0a9280694fc8,The utilization of adaptive replication technique for automatic elimination of synchronization bottlenecks was analyzed. Adaptive replication involves dynamic detection of contention at each object which eliminates unnecessary replication. The technique was implemented in the context of parallelizing compiler for a subset of C++. The combination of lock coarsening and adaptive replication eliminated synchronization bottlenecks and also reduced the overheads associated with the synchronization and replication processes.,Atomic operations; Commutativity analysis; Parallel computing; Parallelizing compilers; Replication; Synchronization,Adaptive algorithms; Automatic programming; Computational methods; Concurrency control; Data acquisition; Object oriented programming; Program compilers; Synchronization; Atomic operations; Commutativity analysis; Contention detection; Parallel processing systems
Intensional Analysis of Quantified Types,2003,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0346902047&doi=10.1145%2f641888.641889&partnerID=40&md5=33dbf8443f896c56bcb8e7ca0437cbe4,"Compilers for polymorphic languages can use run-time type inspection to support advanced implementation techniques such as tagless garbage collection, polymorphic marshalling, and flattened data structures. Intensional type analysis is a type-theoretic framework for expressing and certifying such type-analyzing computations. Unfortunately, existing approaches to intensional analysis do not work well on quantified types such as existential or polymorphic types. This makes it impossible to code (in a type-safe language) applications such as garbage collection, persistency, or marshalling which must be able to examine the type of any run-time value. We present a typed intermediate language that supports the analysis of quantified types. In particular, we provide both type-level and term-level constructs for analyzing quantified types. Our system supports structural induction on quantified types yet type-checking remains decidable. We also show that our system is compatible with a type-erasure semantics.",Certified code; Intensional type analysis; Runtime type dispatch; Typed intermediate languages,Codes (symbols); Data structures; Distributed computer systems; Program compilers; Program interpreters; Semantics; Theorem proving; Intentional type analysis; Polymorphic languages; Java programming language
Mobile Safe Ambients,2003,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0347532829&doi=10.1145%2f596980.596981&partnerID=40&md5=fb3a25ad0f1fbcdb22a39853eea45f45,"Two forms of interferences are individuated in Cardelli and Gordon's Mobile Ambients (MA): plain interferences, which are similar to the interferences one finds in CCS and π-calculus; and grave interferences, which are more dangerous and may be regarded as programming errors. To control interferences, the MA movement primitives are modified; the resulting calculus is called Mobile Safe Ambients (SA). The modification also has computational significance. In the MA interaction rules, an ambient may enter, exit, or open another ambient. The second ambient undergoes the action; it has no control on when the action takes place. In SA this is rectified: any movement takes place only if both participants agree. Existing type systems for MA can be easily adapted to SA. The type systems for controlling mobility, however, appear to be more powerful in SA, in that (i) type systems for MA may give more precise information when transplanted onto SA, and (ii) new type systems may be defined. Two type systems are presented that remove all grave interferences. Other advantages of SA are: a useful algebraic theory; programs sometimes more robust (they require milder conditions for correctness) and/or simpler. All these points are illustrated in several examples.",Behavioral equivalences; Interferences; Mobility,Algebra; Mathematical models; Mobile telecommunication systems; Semantics; Mobile safe ambients; Programming errors; Computer programming languages
Information Flow Inference for ML,2003,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0348017041&doi=10.1145%2f596980.596983&partnerID=40&md5=72e62eb5e6241bb88fc3f43afbdaf9f0,"This paper presents a type-based information flow analysis for a call-by-value λ-calculus equipped with references, exceptions and let-polymorphism, which we refer to as ML. The type system is constraint-based and has decidable type inference. Its noninterference proof is reasonably light-weight, thanks to the use of a number of orthogonal techniques. First, a syntactic segregation between values and expressions allows a lighter formulation of the type system. Second, noninterference is reduced to subject reduction for a nonstandard language extension. Lastly, a semi-syntactic approach to type soundness allows dealing with constraint-based polymorphism separately.",Constraint-based analysis; Non-interference,Algorithms; Computer programming languages; Mathematical models; Problem solving; Semantics; Functional constructs; Information flow interference; Information science
Pointer Analysis for Structured Parallel Programs,2003,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0346271722&doi=10.1145%2f596980.596982&partnerID=40&md5=79a4c196479944fda6fd5d5f4b99560f,"This paper presents a novel interprocedural, flow-sensitive, and context-sensitive pointer analysis algorithm for multithreaded programs that may concurrently update shared pointers. The algorithm is designed to handle programs with structured parallel constructs, including fork-join constructs, parallel loops, and conditionally spawned threads. For each pointer and each program point, the algorithm computes a conservative approximation of the memory locations to which that pointer may point. The algorithm correctly handles a wide range of programming language constructs, including recursive functions, recursively generated parallelism, function pointers, structures, arrays, nested structures and arrays, pointer arithmetic, casts between different pointer types, heap and stack allocated memory, shared global variables, and thread-private global variables. We have implemented the algorithm in the SUIF compiler system and used the implementation to analyze a set of multithreaded programs written in the Cilk programming language. Our experimental results show that the analysis has good precision and converges quickly for our set of Cilk programs.",Pointer analysis,Algorithms; Approximation theory; Computer programming languages; Recursive functions; Semantics; Storage allocation (computer); Multithread programs; Parallel languages; Structured programming
A Transformational Approach to Binary Translation of Delayed Branches,2003,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0346271707&doi=10.1145%2f641888.641890&partnerID=40&md5=6c14f0575c44ed21db3b6762e45d1013,"A binary translator examines binary code for a source machine and generates code for a target machine. Understanding what to do with delayed branches in binary code can involve tricky case analyses, for example, if there is a branch instruction in a delay slot. This article presents a disciplined method for deriving such case analyses. The method identifies problematic cases, shows the translations for the nonproblematic cases, and gives confidence that all cases are considered. The method supports such common architectures as SPARC, MIPS, and PA-RISC, and it should apply to any tool that analyzes machine instructions. We begin by writing a very simple interpreter for the source machine's code. We then transform the interpreter into an interpreter for a target machine without delayed branches. To maintain the semantics of the program being interpreted, we simultaneously transform the sequence of source-machine instructions into a sequence of target-machine instructions. The transformation of the instructions becomes our algorithm for binary translation.",Binary translation; Program analysis; Program transformation,Algorithms; Codes (symbols); Computer architecture; Program interpreters; Program translators; Semantics; Binary translation; Program transformation; Computer programming languages
PolyTOIL: A Type-Safe Polymorphic Object-Oriented Language,2003,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0346271708&doi=10.1145%2f641888.641891&partnerID=40&md5=c82db6cf5015a6c1737c72887c6ae3e2,"PolyTOIL is a new statically typed polymorphic object-oriented programming language that is provably typesafe. By separating the definitions of subtyping and inheritance, providing a name for the type of self, and carefully defining the type-checking rules, we have obtained a language that is very expressive while supporting modular type-checking of classes. The matching relation on types, which is related to F-bounded quantification, is used both in stating type-checking rules and expressing the bounds on type parameters for polymorphism. The design of PolyTOIL is based on a careful formal definition of type-checking rules and semantics. A proof of type safety is obtained with the aid of a subject reduction theorem.",Hash type; Matching,Computer aided language translation; Data reduction; Recursive functions; Semantics; Theorem proving; Block translation; Object oriented constructs; Object oriented programming
The undecidability of associativity and commutativity analysis,2002,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0039486656&doi=10.1145%2f570886.570889&partnerID=40&md5=8f0a17177698a38103e3ed2ef550499b,"Associativity is required for the use of general scans and reductions in parallel languages. Some systems also require functions used with scans and reductions to be commutative. We prove the undecidability of both associativity and commutativity. Thus, it is impossible in general for a compiler to check for those conditions. We also prove the stronger result that the resulting relations fail to be recursively enumerable. We prove that these results hold for the kind of function subprograms of practical interest in such a situation: function subprograms that, due to syntactical restrictions, are guaranteed to halt. Thus, our results are stronger than one can obtain from Rice's Theorem. We also obtain limitations concerning the construction of functions and limitations concerning compiler-generated run-time checks. In addition, we prove an undecidability result about programmer-constructed run-time checks.","Concurrent Programming - parallel programming; Language Classifications - concurrent, distributed, and parallel languages; Language Constructs and Features - general scan operators, general reduction operators; Mathematical Logic - computability theory",
Information flow vs. resource access in the aqsynchronous pi-calculus,2002,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0038893943&doi=10.1145%2f570886.570890&partnerID=40&md5=6ac1b50ff4128dea927f42c2f0b15bc5,"We propose an extension of the asynchronous π-calculus in which a variety of security properties may be captured using types. These are an extension of the input/output types for the π-calculus in which I/O capabilities are assigned specific security levels. The main innovation is a uniform typing system that, by varying slightly the allowed set of types, captures different notions of security. We first define a typing system that ensures that processes running at security level σ cannot access resources with a security level higher than σ. The notion of access control guaranteed by this system is formalized in terms of a Type Safety Theorem. We then show that, by restricting the allowed types, our system prohibits implicit information flow from high-level to low-level processes. We prove that low-level behavior can not be influenced by changes to high-level behavior. This is formalized as a noninterference theorem with respect to may testing.","General - security and protection (e.g., firewalls); Local and Wide-Area Networks - internet (e.g., CSMA/CS); Network Architecture and Design - distributed networks; Security and Protection - information flow controls",
"On loops, dominators, and dominance frontiers",2002,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0038893942&doi=10.1145%2f570886.570887&partnerID=40&md5=2d87e8f8639354973dc4b7acdd532387,"This article explores the concept of loops and loop nesting forests of control-flow graphs, using the problem of constructing the dominator tree of a graph and the problem of computing the iterated dominance frontier of a set of vertices in a graph as guiding applications. The contributions of this article include: (1) An axiomatic characterization, as well as a constructive characterization, of a family of loop nesting forests that includes various specific loop nesting forests that have been previously defined. (2) The definition of a new loop nesting forest, as well as an efficient, almost linear-time, algorithm for constructing this forest. (3) An illustration of how loop nesting forests can be used to transform arbitrary (potentially irreducible) problem instances into equivalent acylic graph problem instances in the case of the two problems of (a) constructing the dominator tree of a graph, and (b) computing the iterated dominance frontier of a set of vertices in a graph, leading to new, almost linear-time, algorithms for these problems.",Algorithms; Data Structures - graphs and networks; trees; Dominator; Graph Theory - graph algorithms; trees; Graph transformation; Irreducible graph; Languages; Processors - code generation; compilers; interpreters; incremental compilers; optimization,
Practical extraction techniques for Java,2002,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0040671947&doi=10.1145%2f586088.586090&partnerID=40&md5=5bc37b400b469f0f0d20d0dad46cf537,"Reducing application size is important for software that is distributed via the internet, in order to keep download times manageable, and in the domain of embedded systems, where applications are often stored in (Read-Only or Flash) memory. This paper explores extraction techniques such as the removal of unreachable methods and redundant fields, inlining of method calls, and transformation of the class hierarchy for reducing application size. We implemented a number of extraction techniques in Jax, an application extractor for Java, and evaluated their effectiveness on a set of large Java applications. We found that, on average, the class file archives for these benchmarks were reduced to 37.5% of their original size. Modeling dynamic language features such as reflection, and extracting software distributions other than complete applications requires additional user input. We present a uniform approach for supplying this input that relies on MEL, a modular specification language. We also discuss a number of issues and challenges associated with the extraction of embedded systems applications.",Algorithms; Application extraction; Call graph construction; Class hierarchy transformation; Experimentation; Languages; Measurement; Performance; Processors - Compilers; Optimization; Semantics of Programming Languages - Program Analysis,
On the usefulness of type and liveness accuracy for garbage collection and leak detection,2002,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0040078395&doi=10.1145%2f586088.586089&partnerID=40&md5=1f5d8788c899d12ff5d1140410841dd4,"The effectiveness of garbage collectors and leak detectors in identifying dead objects is depends on the accuracy of their reachability traversal. Accuracy has two orthogonal dimensions: (i) whether the reachability traversal can distinguish between pointers and nonpointers (type accuracy), and (ii) whether the reachability traversal can identify memory locations that will be dereferenced in the future (liveness accuracy). This article presents an experimental study of the importance of type and liveness accuracy for reachability traversals. We show that liveness accuracy reduces the reachable heap size by up to 62% for our benchmark programs. However, the simpler liveness schemes (e.g., intraprocedural analysis of local variables) are largely ineffective for our benchmark runs: one must analyze global variables using interprocedural analysis to obtain significant benefits. Type accuracy has an insignificant impact on a garbage collector's ability to find unreachable objects in our benchmark runs. We report results for programs written in C, C++ and Eiffel.",Conservative garbage collection; Experimentation; Languages; Leak detection; Liveness accuracy; Measurement; Performance; Processors - Memory management (garbage collection); Program analysis; Type accuracy,
Data abstraction and information hiding,2002,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0041811822&doi=10.1145%2f570886.570888&partnerID=40&md5=f2ea54afb97db90a060bd8502437f02f,"This article describes an approach for verifying programs in the presence of data abstraction and information hiding, which are key features of modern programming languages with objects and modules. This article draws on our experience building and using an automatic program checker, and focuses on the property of modular soundness: that is, the property that the separate verifications of the individual modules of a program suffice to ensure the correctness of the composite program. We found this desirable property surprisingly difficult to achieve. A key feature of our methodology for modular soundness is a new specification construct: the abstraction dependency, which reveals which concrete variables appear in the representation of a given abstract variable, without revealing the abstraction function itself. This article discusses in detail two varieties of abstraction dependencies: static and dynamic. The article also presents a new technical definition of modular soundness as a monotonicity property of verifiability with respect to scope and uses this technical definition to formally prove the modular soundness of a programming discipline for static dependencies.","Abstract variables; Language Constructs and Features - modules, packages; Languages; Object-Oriented Programming; Requirements/Specifications; Software/Program Verification; Specifying and Verifying and Reasoning about Programs; Verification",
Efficient and effective branch reordering using profile data,2002,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0038893905&doi=10.1145%2f586088.586091&partnerID=40&md5=e2f1ba4b79e54311f68698e0f2898729,"The conditional branch has long been considered an expensive operation. The relative cost of conditional branches has increased as recently designed machines are now relying on deeper pipelines and higher multiple issue. Reducing the number of conditional branches executed often results in a substantial performance benefit. This paper describes a code-improving transformation to reorder sequences of conditional branches that compare a common variable to constants. The goal is to obtain an ordering where the fewest average number of branches in the sequence will be executed. First, sequences of branches that can be reordered are detected in the control flow. Second, profiling information is collected to predict the probability that each branch will transfer control out of the sequence. Third, the cost of performing each conditional branch is estimated. Fourth, the most beneficial ordering of the branches based on the estimated probability and cost is selected. The most beneficial ordering often includes the insertion of additional conditional branches that did not previously exist in the sequence. Finally, the control flow is restructured to reflect the new ordering. The results of applying the transformation are on average reductions of about 8% fewer instructions executed and 13% branches performed, as well as about a 4% decrease in execution time.",Algorithms; Branch reordering; Compilers; optimization; Conditional branches; Languages; Profiling,
Type-safe linking with recursive DLLs and shared libraries,2002,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0038893903&doi=10.1145%2f586088.586093&partnerID=40&md5=f10d36d11d3d20bbf96aeac725fa957d,"Component-based programming is an increasingly prevalent theme in software development, motivating the need for expressive and safe module interconnection languages. Dynamic linking is an important requirement for module interconnection languages, as exemplified by dynamic link libraries (DLLs) and class loaders in operating systems and Java, respectively. A semantics is given for a type-safe module interconnection language that supports shared libraries and dynamic linking, as well as circular import dependencies (recursive modules). The core language requirements of the module interconnection language are compatible with programming languages such as Java and C#.",Dynamic Linking; Formal Definitions and Theory - Semantics; Language Constructs and Features - Data Types and Structures; Languages; Module Interconnection Languages; Recursive Modules; Shared Libraries,
Automatic derivation of compiler machine descriptions,2002,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0038895759&doi=10.1145%2f567097.567100&partnerID=40&md5=54ed78d89103bd261d2e11b1909223d2,"We describe a method designed to significantly reduce the effort required to retarget a compiler to a new architecture, while at the same time producing fast and effective compilers. The basic idea is to use the native C compiler at compiler construction time to discover architectural features of the new architecture. From this information a formal machine description is produced. Given this machine description, a native code-generator can be generated by a back-end generator such as BEG or burg. A prototype automatic Architecture Discovery Tool (called ADT) has been implemented. This tool is completely automatic and requires minimal input from the user. Given the Internet address of the target machine and the command-lines by which the native C compiler, assembler, and linker are invoked, ADT will generate a BEG machine specification containing the register set, addressing modes, instruction set, and instruction timings for the architecture. The current version of ADT is general enough to produce machine descriptions for the integer instruction sets of common RISC and CISC architectures such as the Sun SPARC, Digital Alpha, MIPS, DEC VAX and Intel x86.","D.2.7 [Software Engineering]: Distribution, Maintenance, and Enhancement - Portability; D.3.2 [Programming Languages]: Language Classifications - Macro and assembly languages",Algorithms; C (programming language); Computer aided software engineering; Computer architecture; Computer hardware; Computer operating systems; Encoding (symbols); Integer programming; Program assemblers; Code generators; Instruction sets; Program compilers
Register tiling in nonrectangular iteration spaces,2002,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0038895757&doi=10.1145%2f567097.567101&partnerID=40&md5=bdad89a33e7604df4d9ce0aa961caf36,"Loop tiling is a well-known loop transformation generally used to expose coarse-grain parallelism and to exploit data reuse at the cache level. Tiling can also be used to exploit data reuse at the register level and to improve a program's ILP. However, previous proposals in the literature (as well as commercial compilers) are only able to perform multidimensional tiling for the register level when the iteration space is rectangular. In this article we present a new general algorithm to perform multidimensional tiling for the register level in both rectangular and nonrectangular iteration spaces. We also propose a simple heuristic to determine the tiling parameters at this level. Finally, we evaluate our method using as benchmarks typical linear algebra algorithms having nonrectangular iteration spaces and compare our proposal against hand-optimized vendor-supplied numerical libraries and against commercial compilers able to perform optimizing code transformations such as inner unrolling, unroll-and-jam, and software pipelining. Measurements were taken on three different superscalar microprocessors. Results will show that our method outperforms the native compilers (showing speedups of 2.5 in average) and matches the performance of vendor-supplied numerical libraries. The general conclusion is that compiler technology can make it possible for nonrectangular loop nests to achieve as high performance as hand-optimized codes.",D.3.4 [Programming Languages] Processors; Data reuse; Locality; Loop optimization; Loop tiling; Measurement; Performance; Register level,Algorithms; Codes (symbols); Computer programming languages; Computer software reusability; Data processing; Heuristic methods; Iterative methods; Linear algebra; Microprocessor chips; Program compilers; Data reuse; Register tiling; Shift registers
Parametric shape analysis via 3-valued logic,2002,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0039488517&doi=10.1145%2f514188.514190&partnerID=40&md5=307ba341065715e62bf9a10669dc58f0,"Shape analysis concerns the problem of determining ""shape invariants"" for programs that perform destructive updating on dynamically allocated storage. This article presents a parametric framework for shape analysis that can be instantiated in different ways to create different shape-analysis algorithms that provide varying degrees of efficiency and precision. A key innovation of the work is that the stores that can possibly arise during execution are represented (conservatively) using 3-valued logical structures. The framework is instantiated in different ways by varying the predicates used in the 3-valued logic. The class of programs to which a given instantiation of the framework can be applied is not limited a priori (i.e., as in some work on shape analysis, to programs that manipulate only lists, trees, DAGS, etc.); each instantiation of the framework can be applied to any program, but may produce imprecise results (albeit conservative ones) due to the set of predicates employed. Categories and Subject Descriptors: D.2.5 [Software Engineering]: Testing and Debugging -symbolic execution; D.3.3 [Programming Languages]: Language Constructs and Features -data types and structures; dynamic storage management; E.1 [Data]: Data Structures - graphs; lists; trees; E.2 [Data]: Data Storage Representations - composite structures; linked representations; F.3.1 [Logics and Meanings of Programs]: Specifying and Verifying and Reasoning about Programs - assertions; invariants.",3-valued logic; Abstract interpretation; Algorithms; Alias analysis; Constraint solving; Destructive updating; Languages; Pointer analysis; Shape analysis; Static analysis; Theory; Verification,Algorithms; Constraint theory; Data structures; Formal logic; Program debugging; Program interpreters; Theorem proving; Pointer analysis; Shape analysis; Static analysis; Computer programming languages
Compiling language definitions: The ASF+SDF compiler,2002,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0038895765&doi=10.1145%2f567097.567099&partnerID=40&md5=d490cfba03dda1718e87a5cbd0b5d29e,"The ASF+SDF Meta-Environment is an interactive language development environment whose main application areas are definition and implementation of domain-specific languages, generation of program analysis and transformation tools, and production of software renovation tools. It uses conditional rewrite rules to define the dynamic semantics and other tool-oriented aspects of languages, so the effectiveness of the generated tools is critically dependent on the quality of the rewrite rule implementation. The ASF+SDF rewrite rule compiler generates C code, thus taking advantage of C's portability and the sophisticated optimization capabilities of current C compilers as well as avoiding potential abstract machine interface bottlenecks. It can handle large (10,000+ rule) language definitions and uses an efficient run-time storage scheme capable of handling large (1,000,000+ node) terms. Term storage uses maximal subterm sharing (hash-consing), which turns out to be more effective in the case of ASF+SDF than in Lisp or SML. Extensive benchmarking has shown the time and space performance of the generated code to be as good as or better than that of the best current rewrite rule and functional language compilers.",D.3.1 [Programming Languages]: Formal Definitions and Theory - Semantics; D.3.2 [Programming Languages]: Language Classifications - Specialized application languages; D.3.4 [Programming Languages]: Processors - Code generation; compilers; optimization,C (programming language); Codes (symbols); Computer aided software engineering; Data storage equipment; Graph theory; Interfaces (computer); Probability; Semantics; Compilation; Term rewriting; Program compilers
The apprentice challenge,2002,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0040080218&doi=10.1145%2f514188.514189&partnerID=40&md5=3d4661b83f1f285235179754a596224b,"We describe a mechanically checked proof of a property of a small system of Java programs involving an unbounded number of threads and synchronization, via monitors. We adopt the output of the javac compiler as the semantics and verify the system at the bytecode level under an operational semantics for the JVM. We assume a sequentially consistent memory model and atomicity at the bytecode level. Our operational semantics is expressed in ACL2, a Lisp-based logic of recursive functions. Our proofs are checked with the ACL2 theorem prover. The proof involves reasoning about arithmetic; infinite loops; the creation and modification of instance objects in the heap, including threads; the inheritance of fields from superclasses; pointer chasing and smashing; the invocation of instance methods (and the concomitant dynamic method resolution); use of the start method on thread objects; the use of monitors to attain synchronization between threads; and consideration of all possible interleavings (at the bytecode level) over an unbounded number of threads. Readers familiar with monitor-based proofs of mutual exclusion will recognize our proof as fairly classical. The novelty here comes from (i) the complexity of the individual operations on the abstract machine; (ii) the dependencies between Java threads, heap objects, and synchronization; (iii) the bytecode-level interleaving; (iv) the unbounded number of threads; (v) the presence in the heap of incompletely initialized threads and other objects; and (vi) the proof engineering permitting automatic mechanical verification of code-level theorems. We discuss these issues. The problem posed here is also put forth as a benchmark against which to measure other approaches to formally proving properties of multithreaded Java programs. Categories and Subject Descriptors: D.2.4 [Software Engineering]: Software/Program Verification; D.3.0 [Programming Languages]: General; F.4.0 [Mathematical Logic and Formal Languages]: General.",Java; Java virtual machine; Languages; Mutual exclusion; Operational semantics; Parallel and distributed computation; Theorem proving; Verification,Codes (symbols); Computer monitors; Computer simulation; Distributed computer systems; Parallel processing systems; Program compilers; Semantics; Theorem proving; Bytecode level interleaving; Java virtual machine; Java programming language
Type-preserving compilation of featherweight Java,2002,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0347532841&doi=10.1145%2f514952.514954&partnerID=40&md5=04c4e802027174d9b2b2b5ad9b5a0e2c,"We present an efficient encoding of core Java constructs in a simple, implementable typed intermediate language. The encoding, after type erasure, has the same operational behavior as a standard implementation using viables and self-application for method invocation. Classes inherit super-class methods with no overhead. We support mutually recursive classes while preserving separate compilation. Our strategy extends naturally to a significant subset of Java, including interfaces and privacy. The formal translation using Featherweight Java allows comprehensible type-preservation proofs and serves as a starting point for extending the translation to new features. Our work provides a foundation for supporting certifying compilation of Java-like class-based languages in a type-theoretic framework.",Java; Object encodings; Type systems; Typed intermediate languages,Encoding (symbols); Interfaces (computer); Optimization; Program compilers; Theorem proving; Featherweight Java; Object encodings; Typed intermediate languages; Java programming language
Repairing syntax errors in LR parsers,2002,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-19044381597&doi=10.1145%2f586088.586092&partnerID=40&md5=258c5b41ac636b41f555fc22f6f9bf50,"This article reports on an error-repair algorithm for LR parsers. It locally inserts, deletes or shifts symbols at the positions where errors are detected, thus modifying the right context in order to resume parsing on a valid piece of input. This method improves on others in that it does not require the user to provide additional information about the repair process, it does not require precalculation of auxiliary tables, and it can be easily integrated into existing LR parser generators. A Yacc-based implementation is presented along with some experimental results and comparisons with other well-known methods.",Algorithms; Automatic method; Languages; LR parsing; Processors - Parsing; Syntactic error repair,
Handling irreducible loops: Optimized node splitting versus DJ-graphs,2002,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0039488519&doi=10.1145%2f567097.567098&partnerID=40&md5=0580f0b954e1180b24921b5b1f1d0a93,"This paper addresses the question of how to handle irreducible regions curing optimization, which has become even more relevant for contemporary processors since recent VLIW-like architectures highly rely on instruction scheduling. The contributions of this paper are twofold. First, a method of optimized node splitting to transform irreducible regions of control flow into reducible regions is formally denned and its correctness is shown. This method is superior to approaches previously published since it reduces the number of replicated nodes by comparison. Second, three methods that handle regions of irreducible control flow are evaluated with respect to their impact on compiler optimizations. First, traditional node splitting is evaluated. Second, optimized node splitting is implemented. Third, DJ-Graphs are utilized to recognize nesting of irreducible (and reducible) loops and apply common loop optimizations extended for irreducible loops. Experiments compare the performance of these approaches with unrecognized irreducible loops that cannot be subject to loop optimizations, which is typical for contemporary compilers. Measurements show improvements of 1 to 40% for these methods of handling irreducible loops over the unoptimized case. Optimized node splitting may be chosen to retrofit existing compilers since it has the advantage that it only requires few changes to an optimizing compiler while limiting the code growth of compiled programs compared to traditional node splitting. Recognizing loops via DJ-Graphs should be chosen for new compiler developments since it requires more changes to the optimizer but does not significantly change the code size of compiled programs while yielding comparable improvements. Handling irreducible loops should even yield more benefits for exploiting instruction-level parallelism of modern architectures in the context of global instruction scheduling and optimization techniques that may introduce irreducible loops, such as enhanced modulo scheduling.",Algorithms; Code optimization; Compilation; Control flow graphs; D.3.4 [Programming Languages]: Processors - Compilers; optimization; E.1 [Data Structures]: Graphs and networks; G.2.2 [Discrete Mathematics]: Graph Theory - Graph algorithms; Languages,Algorithms; Codes (symbols); Graph theory; Mathematical transformations; Program compilers; Control flow graphs; Irreducible flowgraphs; Reducible flowgraphs; Computer programming
Automatic data and computation decomposition on distributed memory parallel computers,2002,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0040027475&doi=10.1145%2f509705.509706&partnerID=40&md5=d48d6dce26b4c413662d864ad9cfa3a7,"To exploit parallelism on shared memory parallel computers (SMPCs), it is natural to focus on decomposing the computation (mainly by distributing the iterations of the nested Do-Loops). In contrast, on distributed memory parallel computers (DMPCs), the decomposition of computation and the distribution of data must both be handled - in order to balance the computation load and to minimize the migration of data. We propose and validate experimentally a method for handling computations and data synergistically to minimize the overall execution time on DMPCs. The method is based on a number of novel techniques, also presented in this article. The core idea is to rank the ""importance"" of data arrays in a program and specify some of the dominant. The intuition is that the dominant arrays are the ones whose migration would be the most expensive. Using the correspondence between iteration space mapping vectors and distributed dimensions of the dominant data array in each nested Do-loop, allows us to design algorithms for determining data and computation decompositions at the same time. Based on data distribution, computation decomposition for each nested Do-loop is determined based on either the ""owner computes"" rule or the ""owner stores"" rule with respect to the dominant data array. If all temporal dependence relations across iteration partitions are regular we use tiling to allow pipelining and the overlapping of computation and communication However, in order to use tiling on DMPCs, we needed to extend the existing techniques for determining tiling vectors and tile sizes, as they were originally suited for SMPCs only. The overall method is illustrated on programs for the 2D heat equation, for the Gaussian elimination with pivoting, and for the 2D fast Fourier transform on a linear processor array and on a 2D processor grid.",Algorithms; Computation decomposition; D.3.4 [Programming Languages]: Processors - compilers; Data alignment; Data distribution; Distributed-memory computers; Dominant data array; E.1 [Data Structures]: arrays; Languages; Optimization,Computational methods; Data reduction; Decomposition; Distributed computer systems; Fourier transforms; Iterative methods; Set theory; Vectors; Execution time; Subsets; Parallel processing systems
ACM Transactions on Programming Languages and System: Guest Editorial,2002,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0346271728&doi=10.1145%2f514952.514953&partnerID=40&md5=f0160ee66bb1dbb08bc5a3bf6aa32fba,[No abstract available],,
More dynamic object reclassification: FickleII,2002,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0346902063&doi=10.1145%2f514952.514955&partnerID=40&md5=3ade7748ab11a38df72090acd52ebac9,"Reclassification changes the class membership of an object at run-time while retaining its identity. We suggest language features for object reclassification, which extend an imperative, typed, class-based, object-oriented language. We present our proposal through the language FickleII. The imperative features, combined with the requirement for a static and safe type system, provided the main challenges. We develop a type and effect system for fickleII and prove its soundness with respect to the operational semantics. In particular, even though objects may be reclassified across classes with different members, there will never be an attempt to access nonexisting members.",Object-oriented languages; Type and effect systems,Computer science; Object oriented programming; Semantics; Students; Object reclassification; Operational semantics; Computer programming languages
An assume-guarantee rule for checking simulation,2002,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0040620592&doi=10.1145%2f509705.509707&partnerID=40&md5=b6204db8caae0ff6e308ac4684620170,"The simulation preorder on state transition systems is widely accepted as a useful notion of refinement, both in its own right and as an efficiently checkable sufficient condition for trace containment. For composite systems, due to the exponential explosion of the state space, there is a need for decomposing a simulation check of the form P ≤s Q, denoting ""P is simulated by Q,"" into simpler simulation checks on the components of P and Q. We present an assume-guarantee rule that enables such a decomposition. To the best of our knowledge, this is the first assume-guarantee rule that applies to a refinement relation different from trace containment. Our rule is circular, and its soundness proof requires induction on trace trees. The proof is constructive: given simulation relations that witness the simulation preorder between corresponding components of P and Q, we provide a procedure for constructing a witness relation for P ≤s Q. We also extend our assume-guarantee rule to account for fairness constraints on transition systems.",D.2.4 [Software Engineering]: Software/Program Verification - formal methods; F.3.1 [Logics and Meanings of Programs]: Specifying and Verifying and Reasoning about Programs; Simulation relations; Verification; Verification rules,Computer simulation; Computer software; Constraint theory; Decomposition; Formal logic; Hierarchical systems; Trees (mathematics); Simulation preorders; State transition systems; Computer systems
Efficient and precise array access analysis,2002,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0040620594&doi=10.1145%2f509705.509708&partnerID=40&md5=f169cbe781f23a180f9dddae3e2c291e,"A number of existing compiler techniques hinge on the analysis of array accesses in a program. The most important task in array access analysis is to collect the information about array accesses of interest and summarize it in some standard form. Traditional forms used in array access analysis are sensitive to the complexity of array subscripts; that is, they are usually quite accurate and efficient for simple array subscripting expressions, but lose accuracy or require potentially expensive algorithms for complex subscripts. Our study has revealed that in many programs, particularly numerical applications, many access patterns are simple in nature even when the subscripting expressions are complex. Based on this analysis, we have developed a new, general array region representational form, called the linear memory access descriptor (LMAD). The key idea of the LMAD is to relate all memory accesses to the linear machine memory rather than to the shape of the logical data structures of a programming language. This form helps us expose the simplicity of the actual patterns of array accesses in memory, which may be hidden by complex array subscript expressions. Our recent experimental studies show that our new representation simplifies array access analysis and, thus, enables efficient and accurate compiler analysis.","Algorithms; Array access analysis; D.3.4 [Programming Languages]: Processors - compilers, optimization; F.3.2 [Logics and Meanings of Programs]: Semantics of Programming Languages - program analysis; Internal representation; Languages",Algorithms; Computer programming languages; Data storage equipment; Data structures; Formal logic; Program compilers; Semantics; Array access analysis; Linear memory access descriptors; Arrays
"Lessons learned about one-way, dataflow constraints in the Garnet and Amulet graphical toolkits",2001,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0038843643&doi=10.1145%2f506315.506318&partnerID=40&md5=25aef3fd1b5a568f304db0b8f9890ac5,"One-way, dataflow constraints are commonly used in graphical interface toolkits, programming environments, and circuit applications. Previous papers on dataflow constraints have focused on the design and implementation of individual algorithms. In contrast, this article focuses on the lessons we have learned from a decade of implementing competing algorithms in the Garnet and Amulet graphical interface toolkits. These lessons reveal the design and implementation tradeoffs for different one-way, constraint satisfaction algorithms. The most important lessons we have learned are that (1) mark-sweep algorithms are more efficient than topological ordering algorithms; (2) lazy and eager evaluators deliver roughly comparable performance for most applications; and (3) constraint satisfaction algorithms have more than adequate speed, except that the storage required by these algorithms can be problematic.",D.2.2 [Software Engineering]: Design Tools and Techniques-user interfaces; D.2.6 [Software Engineering]: Programming Environments-graphical,
Efficient Java RMI for parallel programming,2001,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0038223826&doi=10.1145%2f506315.506317&partnerID=40&md5=0c0c66e59fa8e795ea5d667a8e44086d,"Java offers interesting opportunities for parallel computing. In particular, Java Remote Method Invocation (RMI) provides a flexible kind of remote procedure call (RPC) that supports polymorphism. Sun's RMI implementation achieves this kind of flexibility at the cost of a major runtime overhead. The goal of this article is to show that RMI can be implemented efficiently, while still supporting polymorphism and allowing interoperability with Java Virtual Machines (JVMs). We study a new approach for implementing RMI, using a compiler-based Java system called Manta. Manta uses a native (static) compiler instead of a just-in-time compiler. To implement RMI efficiently, Manta exploits compile-time type information for generating specialized serializers. Also, it uses an efficient RMI protocol and fast low-level communication protocols. A difficult problem with this approach is how to support polymorphism and interoperability. One of the consequences of polymorphism is that an RMI implementation must be able to download remote classes into an application during runtime. Manta solves this problem by using a dynamic bytecode compiler, which is capable of compiling and linking bytecode into a running application. To allow interoperability with JVMs, Manta also implements the Sun RMI protocol (i.e., the standard RMI protocol), in addition to its own protocol. We evaluate the performance of Manta using benchmarks and applications that run on a 32-node Myrinet cluster. The time for a null-RMI (without parameters or a return value) of Manta is 35 times lower than for the Sun JDK 1.2, and only slightly higher than for a C-based RPC protocol. This high performance is accomplished by pushing almost all of the runtime overhead of RMI to compile time. We study the performance differences between the Manta and the Sun RMI protocols in detail. The poor performance of the Sun RMI protocol is in part due to an inefficient implementation of the protocol. To allow a fair comparison, we compiled the applications and the Sun RMI protocol with the native Manta compiler. The results show that Mania's null-RMI latency is still eight times lower than for the compiled Sun RMI protocol and that Manta's efficient RMI protocol results in 1.8 to 3.4 times higher speedups for four out of six applications.","D.1.3 [Programming Techniques]: Concurrent Programming - distributed programming, parallel programming; D.3.2. [Programming Languages]: Language Classifications-concurrent, distributed, and parallel languages; D.3.4; Object-oriented languages",
Operator strength reduction,2001,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0039436349&doi=10.1145%2f504709.504710&partnerID=40&md5=95bba18b1a194dbefc70612108253cc3,"Operator strength reduction is a technique that improves compiler-generated code by reformulating certain costly computations in terms of less expensive ones. A common case arises in array addressing expressions used in loops. The compiler can replace the sequence of multiplies generated by a direct translation of the address expression with an equivalent sequence of additions. When combined with linear function test replacement, strength reduction can speed up the execution of loops containing array references. The improvement comes from two sources: a reduction in the number of operations needed to implement the loop and the use of less costly operations. This paper presents a new algorithm for operator strength reduction, called OSR. OSR improves upon an earlier algorithm of Allen, Cocke, and Kennedy [Allen et al. 1981]. OSR operates on the static single assignment (SSA) form of a procedure [Cytron et al. 1991]. By taking advantage of the properties of SSA form, we have derived an algorithm that is simple to understand, quick to implement, and, in practice, fast to run. Its asymptotic complexity is, in the worst case, the same as the Allen, Cocke, and Kennedy algorithm (ACK). OSR achieves optimization results that are equivalent to those obtained with the ACK algorithm. OSR has been implemented in several research and production compilers. Categories and Subject Descriptors: D.3.4 [Programming Languages]: Processors - Compilers, Optimization General Terms: Algorithms, Languages.",Algorithms; Languages; Loops; Static single assignment form; Strength reduction,
Transformations of CCP programs,2001,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0042408128&doi=10.1145%2f503502.503504&partnerID=40&md5=ae707f621351638cb4a5b37b9dea3fbf,"We introduce a transformation system for concurrent constraint programming (CCP). We define suitable applicability conditions for the transformations that guarantee the input/output CCP semantics is also preserved when distinguishing deadlocked computations from successful ones and when considering intermediate results of (possibly) nonterminating computations. The system allows us to optimize CCP programs while preserving their intended meaning: In addition to the usual benefits for sequential declarative languages, the transformation of concurrent programs can also lead to the elimination of communication channels and of synchronization points, to the transformation of nondeterministic computations into deterministic ones, and to the crucial saving of computational space. Furthermore, since the transformation system preserves the deadlock behavior of programs, it can be used for proving deadlock-freeness of a given program with respect to a class of queries. To this aim, it is sometimes sufficient to apply our transformations and to specialize the resulting program with respect to the given queries in such a way that the obtained program is trivially deadlock-free.","D.3.2 [Programming Languages]: Language Classifications - concurrent, distributed, and parallel languages; F.3.2 [Logics and Meanings of Programs]; I.2.2 [Artificial Intelligence]: Automatic Programming - Program transformation",Automatic programming; Communication channels (information theory); Computational methods; Concurrent engineering; Constraint theory; Logic programming; Optimization; Parallel processing systems; Semantics; Synchronization; Concurrent constraint programming (CCP); Computer programming languages
Featherweight Java: A minimal core calculus for Java and GJ,2001,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0012583283&doi=10.1145%2f503502.503505&partnerID=40&md5=5ecb7ddcda56f6740af95512cc452354,"Several recent studies have introduced lightweight versions of Java: reduced languages in which complex features like threads and reflection are dropped to enable rigorous arguments about key properties such as type safety. We carry this process a step further, omitting almost all features of the full language (including interfaces and even assignment) to obtain a small calculus, Featherweight Java, for which rigorous proofs are not only possible but easy. Featherweight Java bears a similar relation to Java as the lambda-calculus does to languages such as ML and Haskell. It offers a similar computational ""feel,"" providing classes, methods, fields, inheritance, and dynamic typecasts with a semantics closely following Java's. A proof of type safety for Featherweight Java thus illustrates many of the interesting features of a safety proof for the full language, while remaining pleasingly compact. The minimal syntax, typing rules, and operational semantics of Featherweight Java make it a handy tool for studying the consequences of extensions and variations. As an illustration of its utility in this regard, we extend Featherweight Java with generic classes in the style of GJ (Bracha, Odersky, Stoutamire, and Wadler) and give a detailed proof of type safety. The extended system formalizes for the first time some of the key features of GJ.",D.3.1 [Programming Languages]: Formal Definitions and Theory; D.3.2 [Programming Languages]: Language Classifications - Object-oriented languages; D.3.3 [Programming Languages]: Language Constructs and Features - Classes and objects,Classification (of information); Computational methods; Data structures; Encoding (symbols); Mathematical models; Object oriented programming; Program compilers; Semantics; Inheritance property; Object calculus; Typecasting property; Java programming language
Mechanizing a theory of program composition for UNITY,2001,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0038843672&doi=10.1145%2f504709.504711&partnerID=40&md5=9fda1102fa4f27715f488c9dd966830d,"Compositional reasoning must be better understood if non-trivial concurrent programs are to be verified. Chandy and Sanders [2000] have proposed a new approach to reasoning about composition, which Charpentier and Chandy [1999] have illustrated by developing a large example in the UNITY formalism. The present paper describes extensive experiments on mechanizing the compositionality theory and the example, using the proof tool Isabelle. Broader issues are discussed, in particular, the formalization of program states. The usual representation based upon maps from variables to values is contrasted with the alternatives, such as a signature of typed variables. Properties need to be transferred from one program component's signature to the common signature of the system. Safety properties can be so transferred, but progress properties cannot be. Using polymorphism, this problem can be circumvented by making signatures sufficiently flexible. Finally the proof of the example itself is outlined. Categories and Subject Descriptors: F.3.1 [Logics and Meanings of Programs]: Specifying and Verifying and Reasoning about Programs - Logics of programs; Mechanical verification General Terms: Theory, Verification.",Compositional reasoning; Concurrency; Isabelle; Theory; UNITY; Verification,
Error repair with validation in LR-based parsing,2001,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0038843679&doi=10.1145%2f504083.504084&partnerID=40&md5=75df76b3c84bda87e5971008e04bf13a,"When the compiler encounters an error symbol in an erroneous input, the local error-repair method repairs the input by either inserting a repair string before the error symbol or deleting the error symbol. Although the extended FMQ of Fischer et al. and the method of McKenzie et al. report the improved quality of diagnostic messages, they suffer from redundant parse stack configurations. This article proposes an efficient LR error-recovery method, with validation-removing repairs that give the same validation result as a previously considered, lower-cost repair. Moreover, its execution speed is proportional to the length of the stack configuration. The algorithm is implemented on a Bison, GNU LALR(1), parser generating system. Experimental results are presented.","Algorithms; Complexity; D.3.4 [programming languages]; Error recovery; Languages; Least cost; LR; Parsing; Processors - Compilers; Theory; Translator writing systems, and compiler generators",Algorithms; Error analysis; Graph theory; Set theory; Syntactics; Error recovery; Parse stack configuration; Program compilers
An indexed model of recursive types for foundational proof-carrying code,2001,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0038559204&doi=10.1145%2f504709.504712&partnerID=40&md5=a1f80bd10fc5a9163c47dd40d151561e,"The proofs of ""traditional"" proof carrying code (PCC) are type-specialized in the sense that they require axioms about a specific type system. In contrast, the proofs of foundational PCC explicitly define all required types and explicitly prove all the required properties of those types assuming only a fixed foundation of mathematics such as higher-order logic. Foundational PCC is both more flexible and more secure than type-specialized PCC. For foundational PCC we need semantic models of type systems on von Neumann machines. Previous models have been either too weak (lacking general recursive types and first-class function-pointers), too complex (requiring machine-checkable proofs of large bodies of computability theory), or not obviously applicable to von Neumann machines. Our new model is strong, simple, and works either in λ-calculus or on Pentiums. Categories and Subject Descriptors: F.3.1 [Logics and Meanings of Programs]: Specifying and Verifying and Reasoning about Programs - Mechanical verification; F.3.2 [Logics and Meanings of Programs]: Semantics of Programming Languages.",Languages; Theory,
A framework for call graph construction algorithms,2001,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0011792814&doi=10.1145%2f506315.506316&partnerID=40&md5=2ca3bdd19ab42c4d32908208478ffd14,"A large number of call graph construction algorithms for object-oriented and functional languages have been proposed, each embodying different tradeoffs between analysis cost and call graph precision. In this article we present a unifying framework for understanding call graph construction algorithms and an empirical comparison of a representative set of algorithms. We first present a general parameterized algorithm that encompasses many well-known and novel call graph construction algorithms. We have implemented this general algorithm in the Vortex compiler infrastructure, a mature, multilanguage, optimizing compiler. The Vortex implementation provides a ""level playing field"" for meaningful cross-algorithm performance comparisons. The costs and benefits of a number of call graph construction algorithms are empirically assessed by applying their Vortex implementation to a suite of sizeable (5,000 to 50,000 lines of code) Cecil and Java programs. For many of these applications, interprocedural analysis enabled substantial speed-ups over an already highly optimized baseline. Furthermore, a significant fraction of these speed-ups can be obtained through the use of a scalable, near-linear time call graph construction algorithm.","D.3.2 [Programming Languages]: Language Classifications - applicative (functional) languages; D.3.3 [Programming Languages]: Language Constructs and Features - Classes and objects; D.3.4; Object-oriented languages; Procedures, functions, and Subroutines",
Model checking of hierarchical state machines,2001,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0042908982&doi=10.1145%2f503502.503503&partnerID=40&md5=6e73121739b640f50cb85d491fb1ff10,"Model checking is emerging as a practical tool for detecting logical errors in early stages of system design. We investigate the model checking of sequential hierarchical (nested) Systems, i.e., finite-state machines whose states themselves can be other machines. This nesting ability is common in various software design methodologies, and is available in several commercial modeling tools. The straightforward way to analyze a hierarchical machine is to flatten it (thus incurring an exponential blow up) and apply a model-checking tool on the resulting ordinary FSM. We show that this flattening can be avoided. We develop algorithms for verifying linear-time requirements whose complexity is polynomial in the size of the hierarchical machine. We also address the verification of branching time requirements and provide efficient algorithms and matching lower bounds.","D.2.2 [Design Tools and Techniques]: State diagrams, Object-oriented design methods; D.2.4 [Software/Program Verification]: Formal methods, Model checking; F.3.1 [Specifying and Verifying and Reasoning about Programs]: Mechanical verification",Algorithms; Boolean functions; Computational complexity; Computer aided software engineering; Error analysis; Finite automata; Mathematical models; Object oriented programming; Finite state machines (FSM); Software verification; Hierarchical systems
Type Elaboration and Subtype Completion for Java Bytecode,2001,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0346271752&doi=10.1145%2f383043.383045&partnerID=40&md5=2e06c8b300cb99ec254a1d6ce55a528f,"Java source code is strongly typed, but the translation from Java source to bytecode omits much of the type information originally contained within methods. Type elaboration is a technique for reconstructing strongly typed programs from incompletely typed bytecode by inferring types for local variables. There are situations where, technically, there are not enough types in the original type hierarchy to type a bytecode program. Subtype completion is a technique for adding necessary types to an arbitrary type hierarchy to make type elaboration possible for all verifiable Java bytecode. Type elaboration with subtype completion has been implemented as part of the Marmot Java compiler.",D.3.4 [Programming Languages]: Processors - Compilers; F.3.3 [Logics and Meanings of Programs]: Studies of Program Constructs - Type Structure; Java compiler; Languages; Lattice completion; Object-oriented type systems; Theory; Type-directed compilation,Boolean algebra; Codes (symbols); Inference engines; Object oriented programming; Program compilers; Set theory; Theorem proving; Bytecodes; Typechecking; Java programming language
Synthesis of Concurrent Programs for an Atomic Read/Write Model of Computation,2001,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0010342818&doi=10.1145%2f383043.383044&partnerID=40&md5=e0dbe7a104a53edc4e8964b24054f681,"Methods for mechanically synthesizing concurrent programs from temporal logic specifications have been proposed by Emerson and Clarke and by Manna and Wolper. An important advantage of these synthesis methods is that they obviate the need to manually compose a program and manually construct a proof of its correctness. A serious drawback of these methods in practice, however, is that they produce concurrent programs for models of computation that are often unrealistic, involving highly centralized system architecture (Manna and Wolper), processes with global information about the system state (Emerson and Clarke), or reactive modules that can read all of their inputs in one atomic step (Anuchitanukul and Manna, and Pnueli and Rosner). Even simple synchronization protocols based on atomic read/write primitives such as Peterson's solution to the mutual exclusion problem have remained outside the scope of practical mechanical synthesis methods. In this paper, we show how to mechanically synthesize in more realistic computational models solutions to synchronization problems. We illustrate the method by synthesizing Peterson's solution to the mutual exclusion problem.",C.2.4 [Computer-Communication Networks]: Distributed Systems; D.1.2 [Programming Techniques]: Automatic Programming; D.1.3 [Programming Techniques]: Concurrent Programming; D.2.1 [Software Engineering]: Requirements/Specifications,Computational methods; Computer architecture; Computer programming languages; Logic programming; Mathematical models; Mathematical operators; Semantics; Shift registers; Synchronization; Systems analysis; Atomic registers; Concurrent programming; Mutual exclusion problems; Concurrent engineering
A Schema for Interprocedural Modification Side-Effect Analysis with Pointer Aliasing,2001,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0002174919&doi=10.1145%2f383043.381532&partnerID=40&md5=5e4581bb704d9979be8dc544209e9c4d,"The first interprocedural modification side-effects analysis for C (MODC) that obtains better than worst-case precision on programs with general-purpose pointer usage is presented with empirical results. The analysis consists of an algorithm schema corresponding to a family of MODC algorithms with two independent phases: one for determining pointer-induced aliases and a subsequent one for propagating interprocedural side effects. These MODC algorithms are parameterized by the aliasing method used. The empirical results compare the performance of two dissimilar MODC algorithms: MODC(FSAlias) uses a flow-sensitive, calling-context-sensitive interprocedural alias analysis; MODC(FIAlias) uses a flow-insensitive, calling-context-insensitive alias analysis which is much faster, but less accurate. These two algorithms were profiled on 45 programs ranging in size from 250 to 30,000 lines of C code, and the results demonstrate dramatically the possible cost-precision trade-offs. This first comparative implementation of MODC analyses offers insight into the differences between flow-/context-sensitive and flow-/context-insensitive analyses. The analysis cost versus precision trade-offs in side-effect information obtained are reported. The results show surprisingly that the precision of flow-sensitive side-effect analysis is not always prohibitive in cost, and that the precision of flow-insensitive analysis is substantially better than worst-case estimates and seems sufficient for certain applications. On average MODC(FSAlias) for procedures and calls is in the range of 20% more precise than MODC(FIAlias); however, the performance was found to be at least an order of magnitude slower than MODC(FIAlias).",D.3.4 [Programming Languages]: Processors - Compilers; F.3.2 [Logics and Meanings of Programs]: Semantics of Programming Languages - Program analysis; Optimization,Algorithms; Context free languages; Context sensitive languages; Data flow analysis; Information analysis; Logic programming; Optimization; Program compilers; Semantics; Parameterizations; Computer programming languages
Parallel execution of Prolog programs: A survey,2001,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0011732410&doi=10.1145%2f504083.504085&partnerID=40&md5=ea3c051d78baee7717f620d847bca073,"Since the early days of logic programming, researchers in the field realized the potential for exploitation of parallelism present in the execution of logic programs. Their high-level nature, the presence of nondeterminism, and their referential transparency, among other characteristics, make logic programs interesting candidates for obtaining speedups through parallel execution. At the same time, the fact that the typical applications of logic programming frequently involve irregular computations, make heavy use of dynamic data structures with logical variables, and involve search and speculation, makes the techniques used in the corresponding parallelizing compilers and run-time systems potentially interesting even outside the field. The objective of this article is to provide a comprehensive survey of the issues arising in parallel execution of logic programming languages along with the most relevant approaches explored to date in the field. Focus is mostly given to the challenges emerging from the parallel execution of Prolog programs. The article describes the major techniques used for shared memory implementation of Or-parallelism, And-parallelism, and combinations of the two. We also explore some related issues, such as memory management, compile-time analysis, and execution visualization.",,Computer programming languages; Constraint theory; Data structures; Parallel processing systems; Program compilers; Automatic parallelization; Prolog programs; Logic programming
Semiring-based constraint logic programming: Syntax and semantics,2001,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0041405781&doi=10.1145%2f383721.383725&partnerID=40&md5=17d2036e605c05358d7e06ab44ff2ef9,"We extend the Constraint Logic Programming (CLP) formalism in order to handle semiring-based constraints. This allows us to perform in the same language both constraint solving and optimization. In fact, constraints based on semirings are able to model both classical constraint solving and more sophisticated features like uncertainty, probability, fuzziness, and optimization. We then provide this class of languages with three equivalent semantics: model-theoretic, fix-point, and proof-theoretic, in the style of classical CLP programs.",D.3.1 [Programming languages]: Formal Definitions and Theory; D.3.2 [Programming languages]: Language Classifications - Constraint and logic languages; F.3.2 [Logics And Meanings Of Programs]: Semantics of Programming Languages - Denotational semantics,Computer software; Constraint theory; Function evaluation; Fuzzy sets; Hierarchical systems; Knowledge representation; Mathematical models; Optimization; Probabilistic logics; Semantics; Theorem proving; Constraint logic programming (CLP); Syntax; Logic programming
Using types to analyze and optimize object-oriented programs,2001,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0002940467&doi=10.1145%2f383721.383732&partnerID=40&md5=c77c918504bbe0374a7671b6b8e73e87,"Object-oriented programming languages provide many software engineering benefits, but these often come at a performance cost. Object-oriented programs make extensive use of method invocations and pointer dereferences, both of which are potentially costly on modern machines. We show how to use types to produce effective, yet simple, techniques that reduce the costs of these features in Modula-3, a statically typed, object-oriented language. Our compiler performs type-based alias analysis to disambiguate memory references. It uses the results of the type-based alias analysis to eliminate redundant memory references and to replace monomorphic method invocation sites with direct calls. Using limit, static, and running time evaluation, we demonstrate that these techniques are effective, and sometimes perfect for a set of Modula-3 benchmarks.",Algorithms; Alias analysis; Classes and objects; D.3.4 [Programming Languages]: Processors - compilers; Languages; Measurement; Method invocation; Object orientation; optimization; Performance; Polymorphism; Redundancy elimination,Algorithms; Benchmarking; Data flow analysis; Optimization; Program compilers; Redundancy; Software engineering; Alias analysis; Parameterizations; Polymorphism; Object oriented programming
Scheduling time-constrained instructions on pipelined processors,2001,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0042908988&doi=10.1145%2f383721.383733&partnerID=40&md5=46c8e62d8f0cfa3322a9411541a0835b,"In this work we investigate the problem of scheduling instructions on idealized microprocessors with multiple pipelines, in the presence of precedence constraints, release-times, deadlines, and latency constraints. A latency of lij specifies that there must be at least lij time-steps between the completion time of instruction i and the start time of instruction j. A latency of lij = -1 can be used to specify that j may be scheduled concurrently with i but not earlier. We present a generic algorithm that runs in O(n2log nα(n) + ne) time, given n instructions and e edges in the precedence DAG, where α(n) is the functional inverse of the Ackermann function. Our algorithm can be used to construct feasible schedules for various classes of instances, including instances with the following configurations: (1) one pipeline, with individual release-times and deadlines and where the latencies between instructions are restricted to 0 and 1; (2) m pipelines, with individual release-times and deadlines, and monotone-interval order precedences; (3) two pipelines with latencies of -1 or 0, and release-times and deadlines; (4) one pipeline, latencies of 0 or 1 and individual processing times that are at least one; (5) m pipelines, intree precedences, constant latencies, and deadlines; (6) m pipelines, outtree precedences, constant latencies, and release-times. For instances with deadlines, optimal schedules that minimize the maximal tardiness can be constructed using binary search, in O(log n) iterations of our algorithm. We obtain our results using backward scheduling, a very general relaxation method, which extends, unifies, and clarifies many previous results on instruction scheduling for pipelined and parallel machines.",Algorithms; D.3.4 [Programming Languages]: Processors - code generation; F.2.2 [Analysis of Algorithms and Problem Complexity]: Nonnumerical Algorithms and Problems - sequencing and scheduling; Optimization,Algorithms; Computational complexity; Concurrency control; Constraint theory; Iterative methods; Mathematical models; Optimization; Reduced instruction set computing; Scheduling; Generalization methods; Optimal greedy solutions; Pipeline processing systems
Understanding class hierarchies using concept analysis,2000,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0041111989&doi=10.1145%2f353926.353940&partnerID=40&md5=fe19a4d6e647df599961dcfbe4d59f33,"A new method is presented for analyzing and reengineering class hierarchies. In our approach, a class hierarchy is processed along with a set of applications that use it, and a fine-grained analysis of the access and subtype relationships between objects, variables, and class members is performed. The result of this analysis is again a class hierarchy, which is guaranteed to be behaviorally equivalent to the original hierarchy, but in which each object only contains the members that are required. Our method is semantically well-founded in concept analysis: the new class hierarchy is a minimal and maximally factorized concept lattice that reflects the access and subtype relationships between variables, objects and class members. The method is primarily intended as a tool for finding imperfections in the design of class hierarchies, and can be used as the basis for tools that largely automate the process of reengineering such hierarchies. The method can also be used as a space-optimizing source-to-source transformation that removes redundant fields from objects. A prototype implementation for Java has been constructed, and used to conduct several case studies. Our results demonstrate that the method can provide valuable insights into the usage of a class hierarchy in a specific context, and lead to useful restructuring proposals.","D.2.7 [Software Engineering]: Distribution, Maintenance and Enhancement - restructuring, reverse engineering, reengineering; D.3.3 [Programming Languages]: Language Constructs and Features - Classes and Objects, Inheritance",Algorithms; Approximation theory; Codes (symbols); Hierarchical systems; Java programming language; Redundancy; Reengineering; Reverse engineering; Semantics; Software engineering; Systems analysis; Virtual reality; Class hierarchy reengineering; Concept analysis; Object oriented programming
Sets and Constraint Logic Programming,2000,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0348203654&doi=10.1145%2f365151.365169&partnerID=40&md5=506aacd832f93b291bf2e41a5f174be7,"In this paper we present a study of the problem of handling constraints made by conjunctions of positive and negative literals based on the predicate symbols =, ∈, ∪, and ∥ (i.e., disjointness of two sets) in a (hybrid) universe of finite sets. We also review and compare the main techniques considered to represent finite sets in the context of logic languages. The resulting constraint algorithms are embedded in a Constraint Logic Programming (CLP) language which provides finite sets - along with basic set-theoretic operations - as first-class objects of the language. The language - called CLP (SεT) - is an instance of the general CLP framework, and as such it inherits all the general features and theoretical results of this scheme. We provide, through programming examples, a taste of the expressive power offered by programming in CLP(SεT).","D.3.2 [Programming Languages]: Constraint and Logic Languages; D.3.3 [Programming Languages]: Language Constructs and Features; F.4 [Mathematical Logic and Formal Languages]: Mathematical Logic - Logic and Constraint Programming, Set Theory",
A Balanced Code Placement Framework,2000,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0346312914&doi=10.1145%2f365151.365161&partnerID=40&md5=ac9fe7268072aab6e4a56b9d68445043,"Give-N-Take is a code placement framework which uses a generic producer-consumer mechanism. An instance of this could be a communication step between a processor that computes (produces) some data, and other processors that subsequently reference (consume) these data in an expression. An advantage of Give-N-Take over traditional partial redundancy elimination techniques is its concept of production regions, instead of single locations, which can be beneficial for general latency hiding. Give-N-Take also guarantees balanced production, i.e., each production will be started and stopped exactly once. The framework can also take advantage of production coming ""for free,"" as induced by side effects, without disturbing balance. Give-N-Take can place production either before or after consumption, and it also provides the option to speculatively hoist code out of potentially zero-trip loop (nest) constructs. Give-N-Take uses a fast elimination method based on Tarjan intervals, with a complexity linear in the program size in most cases. We have implemented Give-N-Take as part of a Fortran D compiler prototype, where it solves various communication generation problems associated with compiling data-parallel languages onto distributed-memory architectures.",Algorithms; Compilers; D.3.4 [Programming Languages]: Processors - code generation; Data-flow analysis; Fortran D; High Performance Fortran; Languages; Latency hiding; Optimization; Partial redundancy elimination; Tarjan Intervals; Theory,
Extending Graham-Glanville techniques for optimal code generation,2000,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0042911757&doi=10.1145%2f371880.371881&partnerID=40&md5=9d38b734df8ad981c74c8a31ed292d25,"We propose a new technique for constructing code-generator generators, which combines the advantages of the Graham-Glanville parsing technique and the bottom-up tree parsing approach. Machine descriptions are similar to Yacc specifications. The construction effectively generates a pushdown automaton as the matching device. This device is able to handle ambiguous grammars, and can be used to generate locally optimal code without the use of heuristics. Cost computations are performed at preprocessing time. The class of regular tree grammars augmented with costs that can be handled by our system properly includes those that can be handled by bottom-up systems based on finite-state tree parsing automata. Parsing time is linear in the size of the subject tree. We have tested the system on specifications for some systems and report table sizes.","Algorithms; Code-generator generator; D.3.4 [Programming Languages]: Processors - Code generation, retargetable compilers, translator writing systems and compiler generators; Languages; Optimal code generation; Tree pattern matching",
Java bytecode compression for low-end embedded systems,2000,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0010605201&doi=10.1145%2f353926.353933&partnerID=40&md5=f2e54c6b9ee5e66ed3c0a62b49b63d3e,"A program executing on a low-end embedded system, such as a smart-card, faces scarce memory resources and fixed execution time constraints. We demonstrate that factorization of common instruction sequences in Java bytecode allows the memory footprint to be reduced, on average, to 85% of its original size, with a minimal execution time penalty. While preserving Java compatibility, our solution requires only a few modifications which are straightforward to implement in any JVM used in a low-end embedded system.",Code compression; D.3.4 [Programming Languages]: Processors - Optimization; Design; Embedded systems; Experimentation; Intepreters; Java bytecode; Run-time environments,Algorithms; Code converters; Constraint theory; Costs; Embedded systems; Smart cards; Storage allocation (computer); Systems analysis; Virtual reality; Code compression; Factorization; Java bytecodes; Run-time environments; Java programming language
The role of commutativity in constraint propagation algorithms,2000,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0013398527&doi=10.1145%2f371880.371884&partnerID=40&md5=5aecb805e9c11db6c6f93c162b55f509,"Constraint propagation algorithms form an important part of most of the constraint programming systems. We provide here a simple, yet very general framework that allows us to explain several constraint propagation algorithms in a systematic way. In this framework we proceed in two steps. First, we introduce a generic iteration algorithm on partial orderings and prove its correctness in an abstract setting. Then we instantiate this algorithm with specific partial orderings and functions to obtain specific constraint propagation algorithms. In particular, using the notions commutativity and semi-commutativity, we show that the AC-3, PC-2, DAC, and DPC algorithms for achieving (directional) arc consistency and (directional) path consistency are instances of a single generic algorithm. The work reported here extends and simplifies that of Apt [1999a].",Algorithms; Commutativity; Constraint propagation; D.3.3 [Language Constructs and Features]: Constraints; Generic algorithms; I.1.2 [Algorithms]: Analysis of Algorithms; I.2.2 [Automatic Programming]: Program Synthesis; Languages; Verification,
Lazy rewriting on eager machinery,2000,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0002988669&doi=10.1145%2f345099.345102&partnerID=40&md5=9d7dcba3a74cb30474e454cf1103a096,"The article introduces a novel notion of lazy rewriting. By annotating argument positions as lazy, redundant rewrite steps are avoided, and the termination behavior of a term-rewriting system can be improved. Some transformations of rewrite rules enable an implementation using the same primitives as an implementation of eager rewriting.",D.3.4 [Programming Languages]: Processors - Compilers; Innermost reduction; Languages; Lazy rewriting; Optimization; Specificity ordering,Data structures; Encoding (symbols); Mathematical transformations; Optimization; Program compilers; Theorem proving; Trees (mathematics); Functional languages; Term-rewriting system (TRS); Computer programming languages
Type-based analysis of uncaught exceptions,2000,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0038037654&doi=10.1145%2f349214.349230&partnerID=40&md5=a6af58b85c70891d731d841300fa8b32,"This article presents a program analysis to estimate uncaught exceptions in ML programs. This analysis relies on unification-based type inference in a nonstandard type system, using rows to approximate both the flow of escaping exceptions (a la effect systems) and the flow of result values (a la control-flow analyses). The resulting analysis is efficient and precise; in particular, arguments carried by exceptions are accurately handled.",D.2.5 [Software Engineering]: Testing and Debugging - Error handling and recovery; D.3.2 [Programming Languages]: Language Classifications - Applicative (functional) languages; F.3.2 [Logics and Meanings of Programs]; ML; Symbolic execution,Ada (programming language); Algorithms; Error analysis; Modula (programming language); Program debugging; Semantics; Program execution; Java programming language
Undecidability of context-sensitive data-dependence analysis,2000,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0005337540&doi=10.1145%2f345099.345137&partnerID=40&md5=c1df1b01d8d4ed17939cd75871e788f6,"A number of program-analysis problems can be tackled by transforming them into certain kinds of graph-reachability problems in labeled directed graphs. The edge labels can be used to filter out paths that are not of interest: a path P from vertex s to vertex t only counts as a ""valid connection"" between s and t if the word spelled out by P is in a certain language. Often the languages used for such filtering purposes are languages of matching parentheses. In some cases, the matched-parenthesis condition is used to filter out paths with mismatched calls and returns. This leads to so-called ""context-sensitive"" program analyses, such as context-sensitive interprocedural slicing and context-sensitive interprocedural dataflow analysis. In other cases, the matched-parenthesis condition is used to capture a graph-theoretic analog of McCarthy's rules: ""car (cons (x,y)) = x"" and ""cdr (cons (x,y)) = y"". That is, in the code fragment c = cons (a, b) ; d = car (c) ; the fact that there is a ""structure-transmitted data-dependence"" from a to d, but not from b to d, is captured in a graph by (1) using a vertex for each variable, (2) an edge from vertex i to vertex j when i is used on the right-hand side of an assignment to j, (3) parentheses that match as the labels on the edges that run from a to c and c to d, and (4) parentheses that do not match as the labels on the edges that run from b to c and c to d. However, structure-transmitted data-dependence analysis is context-insensitive, because there are no constraints that filter out paths with mismatched calls and returns. Thus, a natural question is whether these two kinds of uses of parentheses can be combined to create a context-sensitive analysis for structure-transmitted data-dependences. This article answers the question in the negative: in general, the problem of context-sensitive, structure-transmitted data-dependence analysis is undecidable. The results imply that, in general, both context-sensitive set-based analysis and ∞-CFA (when data constructors and selectors are taken into account) are also undecidable.",,Codes (symbols); Computability and decidability; Computer programming languages; Constraint theory; Data flow analysis; Data reduction; Data structures; Graph theory; Logic programming; Optimization; Program compilers; Set theory; Data constructors; Directed graphs (D-graphs); Discrete mathematics; Context sensitive languages
The Benefits and Costs of DyC's Run-Time Optimizations,2000,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0003289590&doi=10.1145%2f365151.367161&partnerID=40&md5=90f5a45f7078e0225cca74e1c6a2e5b8,"DyC selectively dynamically compiles programs during their execution, utilizing the run-time-computed values of variables and data structures to apply optimizations that are based on partial evaluation. The dynamic optimizations are preplanned at static compile time in order to reduce their run-time cost; we call this staging. DyC's staged optimizations include (1) an advanced binding-time analysis that supports polyvariant specialization (enabling both single-way and multiway complete loop unrolling), polyvariant division, static loads, and static calls, (2) low-cost, dynamic versions of traditional global optimizations, such as zero and copy propagation and dead-assignment elimination, and (3) dynamic peephole optimizations, such as strength reduction. Because of this large suite of optimizations and its low dynamic compilation overhead, DyC achieves good performance improvements on programs that are larger and more complex than the kernels previously targeted by other dynamic compilation systems. This paper evaluates the benefits and costs of applying DyC's optimizations. We assess their impact on the performance of a variety of small to medium-sized programs, both for the regions of code that are actually transformed and for the entire application as a whole. Our study includes an analysis of the contribution to performance of individual optimizations, the performance effect of changing the applications' inputs, and a detailed accounting of dynamic compilation costs.",D.3.4 [Programming Languages]: Processors compilers; Dynamic compilation; Optimization; Performance; Specialization,
Fusion-based register allocation,2000,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0039925072&doi=10.1145%2f353926.353929&partnerID=40&md5=e18e5b271433326bbe8b6fed6fb5199a,"The register allocation phase of a compiler maps live ranges of a program to registers. If there are more candidates than there are physical registers, the register allocator must spill a live range (the home location is in memory) or split a live range (the live range occupies multiple locations). One of the challenges for a register allocator is to deal with spilling and splitting together. Fusion-based register allocation uses the structure of the program to make splitting and spilling decisions, with the goal to move overhead operations to infrequently executed parts of a program. The basic idea of fusion-based register allocation is to build up the interference graph. Starting with some base region (e.g., a basic block, a loop), the register allocator adds basic blocks to the region and incrementally builds the interference graph. When there are more live ranges than registers, the register allocator selects live ranges to split; these live ranges are split along the edge that was most recently added to the region. This article describes fusion-based register allocation in detail and compares it with other approaches to register allocation. For programs from the SPEC92 suite, fusion-based register allocation can improve the execution time (of optimized programs, for the MIPS architecture) by up to 8.4% over Chaitin-style register allocation.",Compilers; D.3.4 [Programming Languages]: Processors - code generation; Languages; Measurement; Optimization; Performance evaluation; Register allocation,Codes (symbols); Computer programming languages; Graph theory; Optimization; Problem solving; Storage allocation (computer); Virtual reality; Code generation; Interference graph; Register allocation; Program compilers
Independence in CLP languages,2000,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0040291313&doi=10.1145%2f349214.349224&partnerID=40&md5=d1d306cd19c7e418f833dcb00d3c3f79,"Studying independence of goals has proven very useful in the context of logic programming. In particular, it has provided a formal basis for powerful automatic parallelization tools, since independence ensures that two goals may be evaluated in parallel while preserving correctness and efficiency. We extend the concept of independence to constraint logic programs (CLP) and prove that it also ensures the correctness and efficiency of the parallel evaluation of independent goals. Independence for CLP languages is more complex than for logic programming as search space preservation is necessary but no longer sufficient for ensuring correctness and efficiency. Two additional issues arise. The first is that the cost of constraint solving may depend upon the order constraints are encountered. The second is the need to handle dynamic scheduling. We clarify these issues by proposing various types of search independence and constraint solver independence, and show how they can be combined to allow different optimizations, from parallelism to intelligent backtracking. Sufficient conditions for independence which can be evaluated ""a priori"" at run-time are also proposed. Our study also yields new insights into independence in logic programming languages. In particular, we show that search space preservation is not only a sufficient but also a necessary condition for ensuring correctness and efficiency of parallel execution.",D.1.2 [Programming Techniques]: Automatic Programming - automatic analysis of algorithms; D.1.3 [Programming Techniques]: Parallel Programming; D.1.6 [Programming Techniques]: Logic Programming; F.3.1; Program transformation,Computer programming languages; Constraint theory; Parallel processing systems; Program compilers; Semantics; Herbrand domains; Intelligent backtracking; Logic programming
Local type inference,2000,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0039925079&doi=10.1145%2f345099.345100&partnerID=40&md5=1d0d873fdb155ca43865867659ec3864,"We study two partial type inference methods for a language combining subtyping and impredicative polymorphism. Both methods are local in the sense that missing annotations are recovered using only information from adjacent nodes in the syntax tree, without long-distance constraints such as unification variables. One method infers type arguments in polymorphic applications using a local constraint solver. The other infers annotations on bound variables in function abstractions by propagating type constraints downward from enclosing application nodes. We motivate our design choices by a statistical analysis of the uses of type inference in a sizable body of existing ML code.",D.3.1 [Programming Languages]: Formal Definitions and Theory; Languages; Polymorphism; Subtyping; Theory; Type inference,Abstracting; Algorithms; Codes (symbols); Computer programming languages; Constraint theory; Data structures; Formal languages; Statistical methods; Trees (mathematics); Polymorphism; Subtyping; Type inference; Inference engines
An automata-theoretic approach to modular model checking,2000,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0039332719&doi=10.1145%2f345099.345104&partnerID=40&md5=42758e7eca248e4f8822c85544b66742,"In modular verification the specification of a module consists of two parts. One part describes the guaranteed behavior of the module. The other part describes the assumed behavior of the system in which the module is interacting. This is called the assume-guarantee paradigm. In this paper we consider assume-guarantee specifications in which the guarantee is specified by branching temporal formulas. We distinguish between two approaches. In the first approach, the assumption is specified by branching temporal formulas too. In the second approach, the assumption is specified by linear temporal logic. We consider guarantees in ∀CTL and ∀CTL*, the universal fragments of CTL and CTL*, and assumptions in LTL, ∀CTL, and ∀CTL*. We develop two fundamental techniques: building maximal models for ∀CTL and ∀CTL* formulas and using alternating automata to obtain space-efficient algorithms for fair model checking. Using these techniques we classify the complexity of satisfiability, validity, implication, and modular verification for ∀CTL and ∀CTL*. We show that modular verification is PSPACE-complete for ∀CTL and is EXPSPACE-complete for ∀CTL*. We prove that when the assumption is linear, these bounds hold also for guarantees in CTL and CTL*. On the other hand, the problem remains EXPSPACE-hard even when we restrict the assumptions to LTL and take the guarantee as a fixed ∀CTL formula.",Algorithms; Automata; D.2.4 [Software Engineering]: Software/Program Verification; Modular verification; Temporal logic; Verification,Algorithms; Computational complexity; Finite automata; Logic programming; Mathematical models; Synchronization; Software modularity; Temporal logics; Computer software
Integrating object-oriented programming and protected objects in Ada 95,2000,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0003357627&doi=10.1145%2f353926.353938&partnerID=40&md5=636202372b4dd223ff673f76d5dd6ad3,"Integrating concurrent and object-oriented programming has been an active research topic since the late 1980's. There is now a plethora of methods for achieving this integration. The majority of approaches have taken a sequential object-oriented language and made it concurrent. A few approaches have taken a concurrent language and made it object-oriented. The most important of this latter class is the Ada 95 language, which is an extension to the object-based concurrent programming language Ada 83. Arguably, Ada 95 does not fully integrate its models of concurrency and object-oriented programming. For example, neither tasks nor protected objects are extensible. This article discusses ways in which protected objects can be made more extensible.",Ada 95; Concurrency; Concurrent object-oriented programming; D.3.3 [Programming Languages]: Language Constructs and Features - Concurrent programming structures and inheritance; Inheritance anomaly; Languages,Ada (programming language); Concurrency control; Data processing; Distributed computer systems; Java programming language; Microcomputers; Real time systems; Software engineering; Synchronization; Concurrent languages; Concurrent programming structures; Inheritance anomaly; Polymorphism; Object oriented programming
Program transformation and runtime support for threaded MPI execution on shared-memory machines,2000,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0003576826&doi=10.1145%2f363911.363920&partnerID=40&md5=4f92a76eaa1f1aae16f4bf6ef224119d,"Parallel programs written in MPI have been widely used for developing high-performance applications on various platforms. Because of a restriction of the MPI computation model, conventional MPI implementations on shared-memory machines map each MPI node to an OS process, which can suffer serious performance degradation in the presence of multiprogramming. This paper studies compile-time and runtime techniques for enhancing performance portability of MPI code running on multiprogrammed shared-memory machines. The proposed techniques allow MPI nodes to be executed safely and efficiently as threads. Compile-time transformation eliminates global and static variables in C code using node-specific data. The runtime support includes an efficient and provably correct communication protocol that uses lock-free data structure and takes advantage of address space sharing among threads. The experiments on SGI Origin 2000 show that our MPI prototype called TMPI using the proposed techniques is competitive with SGI's native MPI implementation in a dedicated environment, and that it has significant performance advantages in a multiprogrammed environment.","B.3.2 [Memory Structures]: Design Styles - Shared-memory; D.1.3 [Programming Techniques]: Concurrent Programming - Parallel programming; D.3.2 [Programming Languages]: Language Classifications - Concurrent, distributed, and parallel languages",
Optimizing Memory Usage in the Polyhedral Model,2000,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0038485313&doi=10.1145%2f365151.365152&partnerID=40&md5=aa50010ea1058682e1ca5a588ac9bb32,"The polyhedral model provides a single unified foundation for systolic array synthesis and automatic parallelization of loop programs. We investigate the problem of memory reuse when compiling Alpha (a functional language based on this model). Direct compilation would require unacceptably large memory (for example O(n3) for matrix multiplication). Researchers have previously addressed the problem of memory reuse, and the analysis that this entails for projective memory allocations. This paper addresses, for a given schedule, the choice of the projections so as to minimize the volume of the residual memory. We prove tight bounds on the number of linearly independent projection vectors. Our method is constructive, yielding an optimal memory allocation. We extend the method to modular functions, and deal with the subsequent problems of code generation. Our ideas are illustrated on a number of examples generated by the current version of the Alpha compiler.",C.1.2 [Processor Architectures]: Multiprocessors - array processors; D.3.4 [Programming Languages]: Processors - compilation; Languages; Memory management; Optimization; SIMD processors,
Generation of LR parsers by partial evaluation,2000,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0039893757&doi=10.1145%2f349214.349219&partnerID=40&md5=9af328f27edb0fe7dbb9edd2bf3acfdb,"The combination of modern programming languages and partial evaluation yields new approaches to old problems. In particular, the combination of functional programming and partial evaluation can turn a general parser into a parser generator. We use an inherently functional approach to implement general LR(k) parsers and specialize them with respect to the input grammars using offline partial evaluation. The functional specification of LR parsing yields a concise implementation of the algorithms themselves. Furthermore, we demonstrate the elegance of the functional approach by incorporating on-the-fly attribute evaluation for S-attributed grammars and two schemes for error recovery, which lend themselves to natural and elegant implementation. The parsers require only minor changes to achieve good specialization results. The generated parsers have production quality and match those produced by traditional parser generators in speed and compactness.",D.1.1 [Programming Techniques]: Applicative (Functional) Programming; D.1.2 [Programming Techniques]: Automatic Programming; D.3.2 [Programming Languages]: Language Classifications - Applicative Languages; D.3.4 [Programming Languages]: Processors,Algorithms; Computational grammars; Computer programming; Error analysis; Natural sciences computing; Program compilers; Semantics; Functional programming; Memoization; Parser generation; Computer programming languages
Efficient and safe-for-space closure conversion,2000,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0005062397&doi=10.1145%2f345099.345125&partnerID=40&md5=0dc60f739b3c9e554ad80ec76a93b1f6,"Modern compilera often implement function calls (or returns) in two steps: first, a ""closure"" environment is properly installed to provide access for free variables in the target program fragment; second, the control is transferred to the target by a ""jump with arguments (or results)."" Closure conversion - which decides where and how to represent closures at runtime - is a crucial step in the compilation of functional languages. This paper presents a new algorithm that exploits the use of compile-time control and data-flow information to optimize function calls. By extensive closure sharing and allocating as many closures in registers as possible, our new closure-conversion algorithm reduces heap allocation by 36% and memory fetches for local and global variables by 43%; and improves the already efficient code generated by an earlier version of the Standard ML of New Jersey compiler by about 17% on a DECstation 5000. Moreover, unlike most other approaches, our new closure-allocation scheme satisfies the strong safe-for-space-complexity rule, thus achieving good asymptotic space usage.","Algorithms; D.3.3 [Programming Languages]: Language Constructs and Features - Procedures, functions, and subroutines; D.3.4 [Programming Languages]: Processors - Compilers; optimization; closure conversion; Languages; Performance; Theory",Algorithms; Codes (symbols); Data flow analysis; Optimization; Program compilers; Shift registers; Subroutines; Closure conversion; Code generation; Functional languages; Computer programming languages
Graph rewrite systems for program optimization,2000,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0005255125&doi=10.1145%2f363911.363914&partnerID=40&md5=fb1872d331c2fa675ac7a907d0d4d8c1,"Graph rewrite systems can be used to specify and generate program optimizations. For termination of the systems several rule-based criteria are developed, defining exhaustive graph, rewrite systems. For nondeterministic systems stratification is introduced which automatically selects single normal forms. To illustrate how far the methodology reaches, parts of the lazy code motion optimization are specified. The resulting graph rewrite system classes can be evaluated by a uniform algorithm, which forms the basis for the optimizer generator OPTIMIX. With this tool several optimizer components have been generated, and some numbers on their speed are presented.",Algorithms; Compiler generators; D.3.4 [Programming Languages]: Processors - Translator writing systems and compiler generators; Graph rewrite systems; Languages; Performance; Program analysis; Program optimization; Program transformation; Specificat; Theory,
Incremental analysis of constraint logic programs,2000,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0037882758&doi=10.1145%2f349214.349216&partnerID=40&md5=d06469eae9629d51b1fa538d2c7bbabd,"Global analyzers traditionally read and analyze the entire program at once, in a nonincremental way. However, there are many situations which are not well suited to this simple model and which instead require reanalysis of certain parts of a program which has already been analyzed. In these cases, it appears inefficient to perform the analysis of the program again from scratch, as needs to be done with current systems. We describe how the fixed-point algorithms used in current generic analysis engines for (constraint) logic programming languages can be extended to support incremental analysis. The possible changes to a program are classified into three types: addition, deletion, and arbitrary change. For each one of these, we provide one or more algorithms for identifying the parts of the analysis that must be recomputed and for performing the actual recomputation. The potential benefits and drawbacks of these algorithms are discussed. Finally, we present some experimental results obtained with an implementation of the algorithms in the PLAI generic abstract interpretation framework. The results show significant benefits when using the proposed incremental analysis algorithms.","D.1.2 [Programming Techniques]: Automatic Programming - Automatic analysis of algorithms, Program transformation; D.1.6 [Programming Techniques]: Logic programming; D.3.4 [Programming Languages]: Compilers; F.3.1 [Logics and Meanings of Programs]",Algorithms; Computer programming languages; Data flow analysis; Data storage equipment; Information analysis; Mathematical transformations; Program compilers; Search engines; Abstract interpretation; Incremental computation; Static analysis; Logic programming
Syntactic type abstraction,2000,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0000865197&doi=10.1145%2f371880.371887&partnerID=40&md5=43c747d5944863200ca337ffe414695a,"Software developers often structure programs in such a way that different pieces of code constitute distinct principals. Types help define the protocol by which these principals interact. In particular abstract types allow a principal to make strong assumptions about how well-typed clients use the facilities that it provides. We show how the notions of principals and type abstraction can be formalized within a language. Different principals can know the implementation of different abstract types. We use additional syntax to track the flow of values with abstract types during the evaluation of a program and demonstrate how this framework supports syntactic proofs (in the style of subject reduction) for type-abstraction properties. Such properties have traditionally required semantic arguments; using syntax avoids the need to build a model for the language We present various typed lambda calculi with principals, including versions that have mutable state and recursive types.",D.2.11 [Software Engineering]: Software Architectures - Information Hiding; Languages; D.3.1 [Programming Languages]: Formal Definitions and Theory - Syntax; Semantics; D.3.3 [Programming Languages]: Language Constructs and Features - Abstract data,
Context-sensitive synchronization-sensitive analysis is undecidable,2000,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0000422707&doi=10.1145%2f349214.349241&partnerID=40&md5=71c70f26584446f8d8b2888538e4616a,"Static program analysis is concerned with the computation of approximations of the runtime behavior of programs. Precise information about a program's runtime behavior is, in general, uncomputable for various different reasons, and each reason may necessitate making certain approximations in the information computed. This article illustrates one source of difficulty in static analysis of concurrent programs. Specifically, the article shows that an analysis that is simultaneously both context-sensitive and synchronization-sensitive (that is, a context-sensitive analysis that precisely takes into account the constraints on execution order imposed by the synchronization statements in the program) is impossible even for the simplest of analysis problems.",D.3.4 [Programming Languages]: Processors - compilers; F.4.1 [Mathematical Logic and Formal Languages]: Formal Languages - Decision problems; F.4.1 [Mathematical Logic and Formal Languages]: Mathematical Logic - Computability Theory; Optimization,Approximation theory; Computational methods; Computer programming; Information analysis; Synchronization; Interprocedural analysis; Static program analysis; Context sensitive languages
Typed memory management via static capabilities,2000,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0000189545&doi=10.1145%2f363911.363923&partnerID=40&md5=0a559bf9b54fc909aa040ebfffb5e74a,"Region-based memory management is an alternative to standard tracing garbage collection that makes operations such as memory deallocation explicit but verifiably safe. In this article, we present a new compiler intermediate language, called the Capability Language (CL), that supports region-based memory management and enjoys a provably safe type system. Unlike previous region-based type systems, region lifetimes need not be lexically scoped, and yet the language may be checked for safety without complex analyses. Therefore, our type system may be deployed in settings such as extensible operating systems where both the performance and safety of untrusted code is important. The central novelty of the language is the use of static capabilities to specify the permissibility of various operations, such as memory access and deallocation. In order to ensure capabilities are relinquished properly, the type system tracks aliasing information using a form of bounded quantification. Moreover, unlike previous work on region-based type systems, the proof of soundness of our type system is relatively simple, employing only standard syntactic techniques. In order to show how our language may be used in practice, we show how to translate a variant of Tofte and Talpin's high-level type-and-effects system for region-based memory management into our language. When combined with known region inference algorithms, this translation provides a way to compile source-level languages to CL.","D.3.1 [Programming Languages]: Formal Definitions and Theory - Semantics, Syntax; D.3.4 [Programming Languages]: Processors - Compilers; F.3.2 [Logics and Meanings of Programs]: Semantics of Programming Languages - Operational Semantics",
Automated data-member layout of heap objects to improve memory-hierarchy performance,2000,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0001495548&doi=10.1145%2f353926.353937&partnerID=40&md5=3404c96fe206f0520a5a2d1bfcbfdd8f,"We present and evaluate a simple, yet efficient optimization technique that improves memory-hierarchy performance for pointer-centric applications by up to 24% and reduces cache misses by up to 35%. This is achieved by selecting an improved ordering for the data members of pointer-based data structures. Our optimization is applicable to all type-safe programming languages that completely abstract from physical storage layout; examples of such languages are Java and Oberon. Our technique does not involve programmers in the optimization process, but runs fully automatically, guided by dynamic profiling information that captures which paths through the program are taken with what frequency. The algorithm first strives to cluster data members that are accessed closely after one another onto the same cache line, increasing spatial locality. Then, the data members that have been mapped to a particular cache line are ordered to minimize load latency in case of a cache miss.",Algorithms; D.3.4 [Programming Languages]: Processors - Optimization; D.3.4 [Programming Languages]: Processors - Runtime environments; Dynamic data structures; Dynamic optimization; Languages; Memory-hierarchy optimization; Performance,Algorithms; Buffer storage; Data acquisition; Data structures; Hierarchical systems; Optimization; Program compilers; Servers; Dynamic data structures; Dynamic optimization; Memory-hierarchy optimization; Java programming language
Standard fixpoint iteration for Java bytecode verification,2000,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0001362437&doi=10.1145%2f363911.363915&partnerID=40&md5=ad93998517c3bfdf799995540f39bbe0,"Java bytecode verification forms the basis for Java-based Internet security and needs a rigorous description. One important aspect of bytecode verification is to check if a Java Virtual Machine (JVM) program is statically well-typed. So far, several formal specifications have been proposed to define what the static well-typedness means. This paper takes a step further and presents a chaotic fixpoint iteration, which represents a family of fixpoint computation strategies to compute a least type for each JVM program within a finite number of iteration steps. Since a transfer function in the iteration is not monotone, we choose to follow the example of a nonstandard fixpoint theorem, which requires that all transfer functions are increasing, and monotone in case the bigger element is already a fixpoint. The resulting least type is the artificial top element if and only if the JVM program is not statically well-typed. The iteration is standard and close to Sun's informal specification and most commercial bytecode verifiers.",Algorithms; Bytecode verification; D.3.1 [Programming Languages]: Formal Definitions and Theory; Dataflow analysis; Fixpoint; Java; Languages,
From flop to megaflops: Java for technical computing,2000,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0000542531&doi=10.1145%2f349214.349222&partnerID=40&md5=ed0b0b6b5790ee480c4eb06da452e1a5,"Although there has been some experimentation with Java as a language for numerically intensive computing, there is a perception by many that the language is unsuited for such work because of performance deficiencies. In this article we show how optimizing array bounds checks and null pointer checks creates loop nests on which aggressive optimizations can be used. Applying these optimizations by hand to a simple matrix-multiply test case leads to Java-compliant programs whose performance is in excess of 500 Mflops on a four-processor 332MHz RS/6000 model F50 computer. We also report in this article the effect that various optimizations have on the performance of six floating-point-intensive benchmarks. Through these optimizations we have been able to achieve with Java at least 80% of the peak Fortran performance on the same benchmarks. Since all of these optimizations can be automated, we conclude that Java will soon be a serious contender for numerically intensive computing.","Arrays; Compilers; D.1.3 [Programming Techniques]: Concurrent Programming - Parallel programming; D.3,4 [Programming Languages]: Processor - compilers; Java; Languages; Performance; Run-time environments",Arrays; Computer graphics; Computer networks; FORTRAN (programming language); Iterative methods; Object oriented programming; Parallel processing systems; Program compilers; Semantics; Loop unrolling; Technical computing; Java programming language
Compiler techniques for code compaction,2000,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0000162467&doi=10.1145%2f349214.349233&partnerID=40&md5=7a73691fbc17a9a7e810677d92341ae0,"In recent years there has been an increasing trend toward the incorporation of computers into a variety of devices where the amount of memory available is limited. This makes it desirable to try to reduce the size of applications where possible. This article explores the use of compiler techniques to accomplish code compaction to yield smaller executables. The main contribution of this article is to show that careful, aggressive, interprocedural optimization, together with procedural abstraction of repeated code fragments, can yield significantly better reductions in code size than previous approaches, which have generally focused on abstraction of repeated instruction sequences. We also show how ""equivalent"" code fragments can be detected and factored out using conventional compiler techniques, and without having to resort to purely linear treatments of code sequences as in suffix-tree-based approaches, thereby setting up a framework for code compaction that can be more flexible in its treatment of what code fragments are considered equivalent. Our ideas have been implemented in the form of a binary-rewriting tool that reduces the size of executables by about 30% on the average.","Code compaction; Code compression; Code size; Compilers; D.3.4 [Programming Languages]: Processors - code generation; E.4 [Coding and Information Theory]: Data Compaction and Compression - Program, representation; Experimentation; Optimization; Performance",Binary codes; Computer programming languages; Computer software; Data storage equipment; Embedded systems; Code compression; Program compilers
From system F to typed assembly language,1999,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0041525084&doi=10.1145%2f319301.319345&partnerID=40&md5=60fc386311738b3e215a60697496ddfe,"We motivate the design of a typed assembly language (TAL) and present a type-preserving translation from System F to TAL. The typed assembly language we present is based on a conventional RISC assembly language, but its static type system provides support for enforcing high-level language abstractions, such as closures, tuples, and user-defined abstract data types. The type system ensures that well-typed programs cannot violate these abstractions. In addition, the typing constructs admit many low-level compiler optimizations. Our translation to TAL is specified as a sequence of type-preserving transformations, including CPS and closure conversion phases; type-correct source programs are mapped to type-correct assembly language. A key contribution is an approach to polymorphic closure conversion that is considerably simpler than previous work. The compiler and typed assembly language provide a fully automatic way to produce certified code, suitable for use in systems where untrusted and potentially malicious code must be checked for safety before execution.",Certified code; Closure conversion; Secure extensible systems; Type-directed compilation; Typed assembly language; Typed intermediate languages,Computer programming languages; Integer programming; Mathematical transformations; Reduced instruction set computing; Static random access storage; Translation (languages); Assembly languages; Register allocation; Computer aided software engineering
Sharing and groundness dependencies in logic programs,1999,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0013164133&doi=10.1145%2f330249.330252&partnerID=40&md5=3e405f001c4f91fd87bd1facfbd54489,"We investigate Jacobs and Langen's Sharing domain, introduced for the analysis of variable sharing in logic programs, and show that it is isomorphic to Marriott and Søndergaard's Pos domain, introduced for the analysis of groundness dependencies. Our key idea is to view the sets of variables in a Sharing domain element as the models of a corresponding Boolean function. This leads to a recasting of sharing analysis in terms of the property of ""not being affected by the binding of a single variable."" Such an ""unaffectedness dependency"" analysis has close connections with groundness dependency analysis using positive Boolean functions. This new view improves our understanding of sharing analysis, and leads to an elegant expression of its combination with groundness dependency analysis based on the reduced product of Sharing and Pos. It also opens up new avenues for the efficient implementation of sharing analysis, for example using reduced order binary decision diagrams, as well as efficient implementation of the reduced product, using domain factorizations.",Abstract interpretation; Boolean functions; Dataflow analysis; Global analysis; Invariants; Languages; Logic programming; Logics of programs; Optimization; Processors - compilers; Specifying and Verifying and Reasoning about Programs - assertions; Theory,
Space-efficient scheduling of nested parallelism,1999,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0040362680&doi=10.1145%2f314602.314607&partnerID=40&md5=1c74f9c5dd0bed225ba725e806ea6bf3,"Many of today's high-level parallel languages support dynamic, fine-grained parallelism. These languages allow the user to expose all the parallelism in the program, which is typically of a much higher degree than the number of processors. Hence an efficient scheduling algorithm is required to assign computations to processors at runtime. Besides having low overheads and good load balancing, it is important for the scheduling algorithm to minimize the space usage of the parallel program. This article presents an on-line scheduling algorithm that is provably space efficient and time efficient for nested-parallel languages. For a computation with depth D and serial space requirement S1, the algorithm generates a schedule that requires at most S1 + O(K · D · p) space (including scheduler space) on p processors. Here, K is a user-adjustable runtime parameter specifying the net amount of memory that a thread may allocate before it is preempted by the scheduler. Adjusting the value of K provides a trade-off between the running time and the memory requirement of a parallel computation. To allow the scheduler to scale with the number of processors, we also parallelize the scheduler and analyze the space and time bounds of the computation to include scheduling costs. In addition to showing that the scheduling algorithm is space and time efficient in theory, we demonstrate that it is effective in practice. We have implemented a runtime system that uses our algorithm to schedule lightweight parallel threads. The results of executing parallel programs on this system show that our scheduling algorithm significantly reduces memory usage compared to previous techniques, without compromising performance.",Algorithms; D.1.3 [Programming Techniques]: Concurrent Programming - Parallel programming; D.3.4 [Programming Languages]: Processors - Runtime environments; F.2.0 [Nonnumerical Algorithms and Problems]: Analysis Of Algorithms and Problem Complexity,Algorithms; Scheduling; Theorem proving; Dynamic scheduling; Multithreading; Nested parallelism; Computer programming languages
Effectiveness of abstract interpretation in automatic parallelization: A case study in logic programming,1999,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0040802388&doi=10.1145%2f316686.316688&partnerID=40&md5=acd5209b5de3e95c1a29bf8970c75634,"We report on a detailed study of the application and effectiveness of program analysis based on abstract interpretation to automatic program parallelization. We study the case of parallelizing logic programs using the notion of strict independence. We first propose and prove correct a methodology for the application in the parallelization task of the information inferred by abstract interpretation, using a parametric domain. The methodology is generic in the sense of allowing the use of different analysis domains. A number of well-known approximation domains are then studied and the transformation into the parametric domain defined. The transformation directly illustrates the relevance and applicability of each abstract domain for the application. Both local and global analyzers are then built using these domains and embedded in a complete parallelizing compiler. Then, the performance of the domains in this context is assessed through a number of experiments. A comparatively wide range of aspects is studied, from the resources needed by the analyzers in terms of time and memory to the actual benefits obtained from the information inferred. Such benefits are evaluated both in terms of the characteristics of the parallelized code and of the actual speedups obtained from it. The results show that data flow analysis plays an important role in achieving efficient parallelizations, and that the cost of such analysis can be reasonable even for quite sophisticated abstract domains. Furthermore, the results also offer significant insight into the characteristics of the domains, the demands of the application, and the trade-offs involved.",,Approximation theory; Automatic programming; Data flow analysis; Logic programming; Mathematical models; Parallel processing systems; Program compilers; Semantics; Abstract interpretation; Functional languages; Abstracting
"On failure of the pruning technique in ""error repair in shift-reduce parsers""",1999,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0043045171&doi=10.1145%2f314602.314603&partnerID=40&md5=385f9b58b6074a074e1341107ef06154,A previous article presented a technique to compute the least-cost error repair by incrementally generating configurations that result from inserting and deleting tokens in a syntactically incorrect input. An additional mechanism to improve the run-time efficiency of this algorithm by pruning some of the configurations was discussed as well. In this communication we show that the pruning mechanism may lead to suboptimal repairs or may block all repairs. Certain grammatical errors in a common construct of the Java programming language also lead to the above kind of failure.,Algorithms; D.3.4 [Programming Languages]; Error repair; Languages; Processors - parsing,Algorithms; C (programming language); Portals; Program compilers; Error repair; Pruning techniques; Java programming language
A provably time-efficient parallel implementation of full speculation,1999,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0041412807&doi=10.1145%2f316686.316690&partnerID=40&md5=23fda0b83d9c4bb23f1ba94b49fcef92,"Speculative evaluation, including leniency and futures, is often used to produce high degrees of parallelism. Understanding the performance characteristics of such evaluation, however, requires having a detailed understanding of the implementation. For example, the particular implementation technique used to suspend and reactivate threads can have an asymptotic effect on performance. With the goal of giving the users some understanding of performance without requiring them to understand the implementation, we present a provable implementation bound for a language based on speculative evaluation. The idea is (1) to supply the users with a semantics for a language that defines abstract costs for measuring or analyzing the performance of computations, (2) to supply the users with a mapping of these costs onto runtimes on various machine models, and (3) to describe an implementation strategy of the language and prove that it meets these mappings. For this purpose we consider a simple language based on speculative evaluation. For every computation, the semantics of the language returns a directed acyclic graph (DAG) in which each node represents a unit of computation, and each edge represents a dependence. We then describe an implementation strategy of the language and show that any computation with w work (the number of nodes in the DAG) and d depth (the length of the longest path in the DAG) will run on a p-processor PRAM in O(w/p + d log p) time. The bounds are work efficient (within a constant factor of linear speedup) when there is sufficient parallelism, w/d ≥ p log p. These are the first time bounds we know of for languages with speculative evaluation. The main challenge is in parallelizing the necessary queuing operations on suspended threads.",Abstract machines; Parallel languages; Profiling semantics; Speculation; Threads,Abstracting; Computer programming languages; Costs; Data flow analysis; Graph theory; Programming theory; Semantics; Abstract machines; Directed acyclic graph (DAG); Parallel processing systems
Dependency analysis for standard ML,1999,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0039251316&doi=10.1145%2f325478.325481&partnerID=40&md5=5bc1d29502f9827ce4ceb0e7007b03d4,"Automatic dependency analysis is a useful addition to a system like CM, our compilation manager for Standard ML of New Jersey. It relieves the programmer from the tedious and error-prone task of having to specify compilation dependencies by hand and thereby makes its usage more user friendly. But dependency analysis is not easy, as the general problem for Standard ML is NP-complete. Therefore, CM has to impose certain restrictions on the programming language to recover tractability. We prove the NP-completeness result, discuss the restrictions on ML that are used by CM, and provide the resulting analysis algorithms.","Algorithms; Compilation management; D.3.3 [Programming Languages]: Language Constructs and Features - Modules, packages; Dependency analysis; F.2.2 [Analysis of Algorithms and Problem Complexity]: Nonnumerical Algorithms and Problems; Languages; Theory",
Partial redundancy elimination in SSA form,1999,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0012575181&doi=10.1145%2f319301.319348&partnerID=40&md5=c18b1d8b24064dad3faac87fe7fd0ba9,"The SSAPRE algorithm for performing partial redundancy elimination based entirely on SSA form is presented. The algorithm is formulated based on a new conceptual framework, the factored redundancy graph, for analyzing redundancy, and represents the first sparse approach to the classical problem of partial redundancy elimination. At the same time, it provides new perspectives on the problem and on methods for its solution. With the algorithm description, theorems and their proofs are given showing that the algorithm produces the best possible code by the criteria of computational optimality and lifetime optimality of the introduced temporaries. In addition to the base algorithm, a practical implementation of SSAPRE that exhibits additional compile-time efficiencies is described. In closing, measurement statistics are provided that characterize the instances of the partial redundancy problem from a set of benchmark programs and compare optimazation time spent by an implementation of SSAPRE against a classical partial redundancy emlimination implementation. The data lend insight into the nature of partial redundancy elimination and demonstrate the expediency of this new approach.",Code motion; Common subexpressions; Data flow analysis; Partial redundancy; Static single assignment form,Algorithms; Computational methods; Computer control; Data flow analysis; Data reduction; Mathematical transformations; Optimization; Problem solving; Redundancy; Statistical methods; Code motion; Partial redundancy; Computer programming languages
Techniques for the translation of MATLAB Programs into Fortran 90,1999,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0037882884&doi=10.1145%2f316686.316693&partnerID=40&md5=0d542c9a71fdff5d66f51ca953cc4540,"This article describes the main techniques developed for FALCON's MATLAB-to-Fortran 90 compiler. FALCON is a programming environment for the development of high-performance scientific programs. It combines static and dynamic inference methods to translate MATLAB programs into Fortran 90. The static inference is supported with advanced value propagation techniques and symbolic algorithms for subscript analysis. Experiments show that FALCON's MATLAB translator can generate code that performs more than 1000 times faster than the interpreted version of MATLAB and substantially faster than commercially available MATLAB compilers on one processor of an SGI Power Challenge. Furthermore, in most cases we have tested, the compiler-generated code is as fast as corresponding hand-written programs.",Array language compilation; Inference; MATLAB,Codes (symbols); FORTRAN (programming language); Mathematical operators; Matrix algebra; Program compilers; Software engineering; Vectors; Array language compilation; Dynamic inference methods; Computer software
Reasoning about Grover's quantum search algorithm using probabilistic wp,1999,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0041412799&doi=10.1145%2f319301.319303&partnerID=40&md5=1688d0aab19b04926ea65874d0cf7a5e,"Grover's search algorithm is designed to be executed on a quantum-mechanical computer. In this article, the probabilistic wp-calculus is used to model and reason about Grover's algorithm. It is demonstrated that the calculus provides a rigorous programming notation for modeling this and other quantum algorithms and that it also provides a systematic framework of analyzing such algorithms.",Quantum computation; Quantum mechanics,Algorithms; Computational methods; Computer program listings; Computer simulation; Parallel processing systems; Probability; Search engines; Statistical methods; Quantum computation; Quantum search algorithms; Computer programming languages
"Model-checking concurrent systems with unbounded integer variables: Symbolic representations, approximations, and experimental results",1999,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0012085727&doi=10.1145%2f325478.325480&partnerID=40&md5=58c73fc7e213aae91c73d75839c20d1d,"Model checking is a powerful technique for analyzing large, finite-state systems. In an infinite-state system, however, many basic properties are undecidable. In this article, we present a new symbolic model checker which conservatively evaluates safety and liveness properties on programs with unbounded integer variables. We use Presburger formulas to symbolically encode a program's transition system, as well as its model-checking computations. All fixpoint calculations are executed symbolically, and their convergence is guaranteed by using approximation techniques. We demonstrate the promise of this technology on some well-known infinite-state concurrency problems.",D.2.4 [Software Engineering]: Software/Program Verification - Formal Methods; F.3.1 [Logics and Meanings of Programs]: Specifying and Verifying and Reasoning about Programs - Invariants; Mechanical verification; Model Checking; Preand post-conditions,
Compile-time memory reuse in logic programming languages through update in place,1999,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0041914113&doi=10.1145%2f319301.319309&partnerID=40&md5=a639e31b82e601b81ebf1028cce73b29,"Standard implementation techniques for single-assignment languages modify a data structure without destroying the original, which may subsequently be accessed. Instead a variant structure is created by using newly allocated cells to represent the changed portion and to replace any cell that references a newly allocated cell. The rest of the original structure is shared by the variant. The effort required to leave the original uncorrupted is unnecessary when the program will never reference the original again. This effort includes allocating and initializing new cells, as well as garbage collecting replaced cells. This article specifies a transformation system that introduces update-in-place operations, making Prolog programs update recursive data structures much as an imperative program would. The article introduces the notion of a reuse map, which formalizes reallocation decisions. Because optimal memory reuse is intractable, a heuristical method is presented that performs well in practice. Small Prolog programs that manipulate recursive data structures have their speed increased up to about five times (naive reverse), not counting any speedup obtained by avoiding garbage collection. Quicksort is about three times as fast, merge-sort about one and a half, matrix transposition about twice, and Gaussian elimination is about 1.2 times as fast.",Compile-time garbage collection; Local reuse; Prolog; Reuse map; Update in place,Automatic programming; Data storage equipment; Data structures; Heuristic methods; Mathematical transformations; PROLOG (programming language); Garbage collection; Matrix transposition; Logic programming
Dynamic typing for distributed programming in polymorphic languages,1999,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0002166787&doi=10.1145%2f314602.314604&partnerID=40&md5=2213d890b92cc68b287d648c83ac6fbe,"While static typing is widely accepted as being necessary for secure program execution, dynamic typing is also viewed as being essential in some applications, particularly for distributed programming environments. Dynamics have been proposed as a language construct for dynamic typing, based on experience with languages such as CLU, Cedar/Mesa, and Modula-3. However proposals for incorporating dynamic typing into languages with parametric polymorphism have serious shortcomings. A new approach is presented to extending polymorphic languages with dynamic typing. At the heart of the approach is the use of dynamic type dispatch, where polymorphic functions may analyze the structure of their type arguments. This approach solves several open problems with the traditional approach to adding dynamic typing to polymorphic languages. An explicitly typed language XMLdyn is presented; this language uses refinement kinds to ensure that dynamic type dispatch does not fail at run-time. Safe dynamics are a new form of dynamics that use refinement kinds to statically check the use of run-time dynamic typing. Run-time errors are isolated to a separate construct for performing run-time type checks.",D.3.1 [Programming Languages]: Formal Definitions and Theory - semantics; D.3.3 [Programming Languages]: Language Constructs and Features - data types; Dynamic typing; Languages; Marshalling; Parametric polymorphism; Static typing; Structures,Data structures; Distributed computer systems; Program compilers; Semantics; XML; Polymorphic languages; Computer programming languages
Static correlated branch prediction,1999,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0003339553&doi=10.1145%2f330249.330255&partnerID=40&md5=18d7392118ace4a0e53ac3937c97b1a5,"Recent work in history-based branch prediction uses novel hardware structures to capture branch correlation and increase branch prediction accuracy. Branch correlation occurs when the outcome of a conditional branch can be accurately predicted by observing the outcomes of previously executed branches in the dynamic instruction stream. In this article, we show how to instrument a program so that it is practical to collect run-time statistics that indicate where branch correlation occurs, and we then show how to use these statistics to transform the program so that its static branch prediction accuracy is improved. The run-time information that we gather is called a path profile, and it summarizes how often each executed sequence of program points occurs in the program trace. Our path profiles are more general than those previously proposed. The code transformation that we present is called static correlated branch prediction (SCBP). It exhibits better branch prediction accuracy than previously thought possible for static prediction techniques. Furthermore, through the use of an overpruning heuristic, we show that it is possible to determine automatically an appropriate trade-off between code expansion and branch predictability so that our transformation improves the performance of multiple-issue, deeply pipelined microprocessors like those being built today.",Algorithms; Branch correlation; Branch prediction; Languages; Optimization; Path profiling; Performance; Processors - compilers; Profile-driven optimization; Single Data Stream Architectures - RISC/CISC; VLIW architectures,
"Specification and verification of fault-tolerance, timing, and scheduling",1999,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0002619624&doi=10.1145%2f314602.314605&partnerID=40&md5=23b6918aef0b47c4956c237ed25301da,"Fault-tolerance and timing have often been considered to be implementation issues of a program, quite distinct from the functional safety and liveness properties. Recent work has shown how these non-functional and functional properties can be verified in a similar way. However, the more practical question of determining whether a real-time program will meet its deadlines, i.e., showing that there is a feasible schedule, is usually done using scheduling theory, quite separately from the verification of other properties of the program. This makes it hard to use the results of scheduling analysis in the design, or redesign, of fault-tolerant and real-time programs. This article shows how fault-tolerance, timing, and schedulability can be specified and verified using a single notation and model. This allows a unified view to be taken of the functional and nonfunctional properties of programs and a simple transformational method to be used to combine these properties. It also permits results from scheduling theory to be interpreted and used within a formal proof framework. The notation and model are illustrated using a simple example.",D.1.3 [Programming Techniques]: Concurrent Programming; D.2.4 [Software Engineering]: Software/Program Verification; D.4.5 [Operating Systems]: Fault-Tolerance and Reliability; D.4.7 [Organization and Design]: Real-Time Systems,Computer programming languages; Process control; Real time systems; Scheduling; Security systems; Safety-critical computer systems; Fault tolerant computer systems
Hierarchical modularity,1999,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0005042679&doi=10.1145%2f325478.325518&partnerID=40&md5=6ec06125dc4d9f65d2457d74e06ac490,"To cope with the complexity of very large systems, it is not sufficient to divide them into simple pieces because the pieces themselves will either be too numerous or too large. A hierarchical modular structure is the natural solution. In this article we explain how that approach can be applied to software. Our compilation manager provides a language for specifying where individual modules fit into a hierarchy and how they are related semantically. We pay particular attention to the structure of the global name space of program identifiers that are used for module linkage because any potential for name clashes between otherwise unrelated parts of a program can negatively affect modularity. We discuss the theoretical issues in building software hierarchically, and we describe our implementation of CM, the compilation manager for Standard ML of New Jersey.",Compilation management; D.2.2 [Software Engineering]: Tools and Techniques -Modules and interfaces; D.3.3 [Programming Languages]: Language Constructs and Features - Modules; Design; Languages; Linking; Management; Modularity; Modules; Packages,
Linear scan register allocation,1999,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0001228239&doi=10.1145%2f330249.330250&partnerID=40&md5=4b61d421174fa5e4db2cea7951cdf59d,"We describe a new algorithm for fast global register allocation called linear scan. This algorithm is not based on graph coloring, but allocates registers to variables in a single linear-time scan of the variables' live ranges. The linear scan algorithm is considerably faster than algorithms based on graph coloring, is simple to implement, and results in code that is almost as efficient as that obtained using more complex and time-consuming register allocators based on graph coloring. The algorithm is of interest in applications where compile time is a concern, such as dynamic compilation systems, ""just-in-time"" compilers, and interactive development environments.",Algorithms; Code generation; Code optimization; Compilers; Optimazation; Performance; Processors - compilers; Register allocation,
Cache miss equations: A compiler framework for analyzing and tuning memory behavior,1999,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0001714824&doi=10.1145%2f325478.325479&partnerID=40&md5=0574e7b8406140095bcccebf709b78f7,"With the ever-widening performance gap between processors and main memory, cache memory, which is used to bridge this gap, is becoming more and more significant. Caches work well for programs that exhibit sufficient locality. Other programs, however, have reference patterns that fail to exploit the cache, thereby suffering heavily from high memory latency. In order to get high cache efficiency and achieve good program performance, efficient memory accessing behavior is necessary. In fact, for many programs, program transformations or source-code changes can radically alter memory access patterns, significantly improving cache performance. Both hand-tuning and compiler optimization techniques are often used to transform codes to improve cache utilization. Unfortunately, cache conflicts are difficult to predict and estimate, precluding effective transformations. Hence, effective transformations require detailed knowledge about the frequency and causes of cache misses in the code. This article describes methods for generating and solving Cache Miss Equations (CMEs) that give a detailed representation of cache behavior, including conflict misses, in loop-oriented scientific code. Implemented within the SUIF compiler framework, our approach extends traditional compiler reuse analysis to generate linear Diophantine equations that summarize each loop's memory behavior. While solving these equations is in general difficult, we show that is also unnecessary, as mathematical techniques for manipulating Diophantine equations allow us to relatively easily compute and/or reduce the number of possible solutions, where each solution corresponds to a potential cache miss. The mathematical precision of CMEs allows us to find true optimal solutions for transformations such as blocking or padding. The generality of CMEs also allows us to reason about interactions between transformations applied in concert. The article also gives examples of their use to determine array padding and offset amounts that minimize cache misses, and to determine optimal blocking factors for tiled code. Overall, these equations represent an analysis framework that offers the generality and precision needed for detailed compiler optimizations.",C.1.0 [Processor Architectures]: General; C.4 [Performance of Systems]: Measurement Techniques; Cache memories; Compilation; D.3.4 [Programming Languages]: Processors - compilers; Design; Experimentation; optimization; Optimization; Performance,
Efficient logic variables for distributed computing,1999,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0004392517&doi=10.1145%2f319301.319347&partnerID=40&md5=906f40f9554b8cf03fb3df4cc6e93ef6,"We define a practical algorithm for distributed rational tree unification and prove its correctness in both the off-line and on-line cases. We derive the distributed algorithm from a centralized one, showing clearly the trade-offs between local and distributed execution. The algorithm is used to realize logic variables in the Mozart Programming System, which implements the Oz language (see http://www.mozart-oz.org). Oz appears to the programmer as a concurrent object-oriented language with dataflow synchronization. Logic variables implement the dataflow behavior. We show that logic variables can easily be added to the more restricted models of Java and ML, thus providing an alternative way to do concurrent programming in these languages. We present common distributed programming idioms in a network-transparent way using logic variables. We show that in common cases the algorithm maintains the same message latency as explicit message passing. In addition, it is able to handle uncommon cases that arise from the properties of latency tolerance and third-party independence. This is evidence that using logic variables in distributed computing is beneficial at both the system and language levels. At the system level, they improve latency tolerance and third-party independence. At the language level, they help make network-transparent distribution practical.",,Algorithms; Approximation theory; Computational methods; Computer programming; Data reduction; Formal logic; Java programming language; Object oriented programming; Semantics; Distributed algorithms; Logic variables; Distributed computer systems
Componential set-based analysis,1999,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0001257686&doi=10.1145%2f316686.316703&partnerID=40&md5=ae61a43e34499b9614d9abda82fe85d9,"Set-based analysis (SBA) produces good predictions about the behavior of functional and object-oriented programs. The analysis proceeds by inferring constraints that characterize the data flow relationships of the analyzed program. Experiences with MrSpidey, a static debugger based on SBA, indicate that SBA can adequately deal with programs of up to a couple of thousand lines of code. SBA fails, however, to cope with larger programs because it generates systems of constraints that are at least linear, and possibly quadratic, in the size of the analyzed program. This article presents theoretical and practical results concerning methods for reducing the size of constraint systems. The theoretical results include a proof-theoretic characterization of the observable behavior of constraint systems for program components, and a complete algorithm for deciding the observable equivalence of constraint systems. In the course of this development we establish a close connection between the observable equivalence of constraint systems and the equivalence of regular-tree grammars. We then exploit this connection to adapt a variety of algorithms for simplifying grammars to the problem of simplifying constraint systems. Based on the resulting algorithms, we have developed componential set-based analysis, a modular and polymorphic variant of SBA. Experimental results verify the effectiveness of the simplification algorithms and the componential analysis. The simplified constraint systems are typically an order of magnitude smaller than the original systems. These reductions in size produce significant gains in the speed of the analysis.",Constraint-based analysis; Program analysis; Scheme; Soft typing; Static debugging,Algorithms; Computer programming languages; Computer software selection and evaluation; Object oriented programming; Program debugging; Set theory; Constraint-based analysis; Soft typing; Computer aided software engineering
'C and tcc: A language and compiler for dynamic code generation,1999,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0001379889&doi=10.1145%2f316686.316697&partnerID=40&md5=cc82f0c22eeccf71f16af9369b9eff82,"Dynamic code generation allows programmers to use run-time information in order to achieve performance and expressiveness superior to those of static code. The 'C (Tick C) language is a superset of ANSI C that supports efficient and high-level use of dynamic code generation. 'C provides dynamic code generation at the level of C expressions and statements and supports the composition of dynamic code at run time. These features enable programmers to add dynamic code generation to existing C code incrementally and to write important applications (such as ""just-in-time"" compilers) easily. The article presents many examples of how 'C can be used to solve practical problems. The tcc compiler is an efficient, portable, and freely available implementation of 'C. tcc allows programmers to trade dynamic compilation speed for dynamic code quality: in some applications, it is most important to generate code quickly, while in others code quality matters more than compilation speed. The overhead of dynamic compilation is on the order of 100 to 600 cycles per generated instruction, depending on the level of dynamic optimization. Measurements show that the use of dynamic code generation can improve performance by almost an order of magnitude; two- to four-fold speedups are common. In most cases, the overhead of dynamic compilation is recovered in under 100 uses of the dynamic code; sometimes it can be recovered within one use.",ANSI C; Compilers; Dynamic code generation; Dynamic code optimization,Algorithms; Binary codes; Computational grammars; Program compilers; Dynamic code generation; Dynamic compilation; C (programming language)
A type system for Java bytecode subroutines,1999,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0002335588&doi=10.1145%2f314602.314606&partnerID=40&md5=9983e91cc6a57339073e2bea0d0532a6,"Java is typically compiled into an intermediate language, JVML, that is interpreted by the Java Virtual Machine. Because mobile JVML code is not always trusted, a bytecode verifier enforces static constraints that prevent various dynamic errors. Given the importance of the bytecode verifier for security, its current descriptions are inadequate. This article proposes using typing rules to describe the bytecode verifier because they are more precise than prose, clearer than code, and easier to reason about than either. JVML has a subroutine construct which is used for the compilation of Java's try-finally statement. Subroutines are a major source of complexity for the bytecode verifier because they are not obviously last-in/first-out and because they require a kind of polymorphism. Focusing on subroutines, we isolate an interesting, small subset of JVML. We give typing rules for this subset and prove their correctness. Our type system constitutes a sound basis for bytecode verification and a rational reconstruction of a delicate part of Sun's bytecode verifier.",D.3.1 [Programming Languages]: Formal Definitions and Theory - semantics; D.4.6 [Operating Systems]: Security and Protection - invasive software; F.3.2 [Logics and Meanings of Programs]: Semantics of Programming Languages - operational semantics,Virtual storage; Web browsers; XML; Java virtual machine language (JVML); Polymorphism; Java programming language
Procedure placement using temporal-ordering information,1999,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0005329615&doi=10.1145%2f330249.330254&partnerID=40&md5=7b10eeeaffb458d65ec980cd276cd9e7,"Instruction cache performance is important to instruction fetch efficiency and overall processor performance. The layout of an executable has a substantial effect on the cache miss rate and the instruction working set size during execution. This means that the performance of an executable can be improved by applying a code-placement algorithm that minimizes instruction cache conflicts and improves spatial locality. We describe an algorithm for procedure placement, one type of code placement, that significantly differs from previous approaches in the type of information used to drive the placement algorithm. In particular, we gather temporal-ordering information that summarizes the interleaving of procedures in a program trace. Our algorithm uses this information along with cache configuration and procedure size information to better estimate the conflict cost of a potential procedure ordering. It optimizes the procedure placement for single level and multilevel caches. In addition to reducing instruction cache conflicts, the algorithm simultaneously minimizes the instruction working set size of the program. We compare the performance of our algorithm with a particularly successful procedure-placement algorithm and show noticeable improvements in the instruction cache behavior, while maintaining the same instruction working set size.",Algorithms; Code placement; Conflict misses; Optimization; Performance; Performance Analysis and Design Aids - simulation; Processors - compilers; Temporal profiling; Working-set optimization,
Linearity and the pi-calculus,1999,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0001065208&doi=10.1145%2f330249.330251&partnerID=40&md5=c7ef5269ba1585561412be577e6f960c,"The economy and flexibility of the pi-calculus make it an attractive object of theoretical study and a clean basis for concurrent language design and implementation. However, such generality has a cost: encoding higher-level features like functional computation in pi-calculus throws away potentially useful information. We show how a linear type system can be used to recover important static information about a process's behavior. In particular, we can guarantee that two processes communicating over a linear channel cannot interfere with other communicating processes. After developing standard results such as soundness of typing, we focus on equivalences, adapting the standard notion of barbed bisimulation to the linear setting and showing how reductions on linear channels induce a useful ""partial confluence"" of process behaviors. For an extended example of the theory, we prove the validity of a tail-call optimization for higher-order functions represented as processes.",Concurrency; Confluence; Formal Definations and Theory; Languages; Linear types; Pi-calculus; Process calculi; Theory,
Interprocedural pointer alias analysis,1999,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0000998787&doi=10.1145%2f325478.325519&partnerID=40&md5=223fce57184a44ab26d53ec5c38fe128,"We present practical approximation methods for computing and representing interprocedural aliases for a program written in a language that includes pointers, reference parameters, and recursion. We present the following contributions: (1) a framework for interprocedural pointer alias analysis that handles function pointers by constructing the program call graph while alias analysis is being performed; (2) a flow-sensitive interprocedural pointer alias analysis algorithm; (3) a flow-insensitive interprocedural pointer alias analysis algorithm; (4) a flow-insensitive interprocedural pointer alias analysis algorithm that incorporates kill information to improve precision; (5) empirical measurements of the efficiency and precision of the three interprocedural alias analysis algorithms.",Algorithms; D.3.4 [Programming Languages]: Processors - Optimization; Interprocedural analysis; Pointer aliasing; Program analysis,
Identifying loops in almost linear time,1999,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0005083632&doi=10.1145%2f316686.316687&partnerID=40&md5=961779080a9acaf191460302c398fee3,"Loop identification is an essential step in performing various loop optimizations and transformations. The classical algorithm for identifying loops is Tarjan's interval-finding algorithm, which is restricted to reducible graphs. More recently, several people have proposed extensions to Tarjan's algorithm to deal with irreducible graphs. Havlak presents one such extension, which constructs a loop-nesting forest for an arbitrary flow graph. We show that the running time of this algorithm is quadratic in the worst-case, and not almost linear as claimed. We then show how to modify the algorithm to make it run in almost linear time. We next consider the quadratic algorithm presented by Sreedhar et al. which constructs a loop-nesting forest different from the one constructed by the Havlak algorithm. We show that this algorithm too can be adapted to run in almost linear time. We finally consider an algorithm due to Steensgaard, which constructs yet another loop-nesting forest. We show how this algorithm can be made more efficient by borrowing ideas from the other algorithms discussed earlier.",Irreducible flowgraphs; Loops,Algorithms; Computer programming languages; Data structures; Optimization; Program compilers; Trees (mathematics); Loop identification; Identification (control systems)
A type system for object initialization in the Java bytecode language,1999,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0000877763&doi=10.1145%2f330643.330646&partnerID=40&md5=4057e26163cd0a9fd41abdbc31438d95,"In the standard Java implementation, a Java language program is compiled to Java bytecode. This bytecode may be sent across the network to another site, where it is then executed by the Java Virtual Machine. Since bytecode may be written by hand, or corrupted during network transmission, the Java Virtual Machine contains a bytecode verifier that performs a number of consistency checks before code is run. These checks include type correctness and, as illustrated by previous attacks on the Java Virtual Machine, are critical for system security. In order to analyze existing bytecode verifiers and to understand the properties that should be verified, we develop a precise specification of statically correct Java bytecode, in the form of a type system. Our focus in this article is a subset of the bytecode language dealing with object creation and initialization. For this subset, we prove, that, for every Java bytecode program that satisfies our typing constraints, every object is initialized before it is used. The type system is easily combined with a previous system developed by Stata and Abadi for bytecode subroutines. Our analysis of subroutines and object initialization reveals a previously unpublished bug in the Sun JDK bytecode verifier.",D.3.1 [Programming Languages]: Formal Definitions and Theory; F.3.1 [Theory of Computation]: Specifying and Verifying and Reasoning about Programs - mechanical verification; F.3.3 [Theory of Computation]: Studies of Program Constructs - type structure,
A global communication optimization technique based on data-flow analysis and linear algebra,1999,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0000529068&doi=10.1145%2f330643.330647&partnerID=40&md5=7ff72e41a1ef5ecbca9ff15929ca7c72,"Reducing communication overhead is extremely important in distributed-memory message-passing architectures. In this article, we present a technique to improve communication that considers data access patterns of the entire program. Our approach is based on a combination of traditional data-flow analysis and a linear algebra framework, and it works on structured programs with conditional statements and nested loops but without arbitrary goto statements. The distinctive features of the solution are the accuracy in keeping communication set information, support for general alignments and distributions including block-cyclic distributions, and the ability to simulate some of the previous approaches with suitable modifications. We also show how optimizations such as message vectorization, message coalescing, and redundancy elimination are supported by our framework. Experimental results on several benchmarks show that our technique is effective in reducing the number of messages (an average of 32% reduction), the volume of the data communicated (an average of 37% reduction), and the execution time (an average of 26% reduction).",,
Should your specification language be typed?,1999,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0003897323&doi=10.1145%2f319301.319317&partnerID=40&md5=b4a15b6225686e0816361af2f0095cc6,"Most specification languages have a type system. Type systems are hard to get right, and getting them wrong can lead to inconsistencies. Set theory can serve as the basis for a specification language without types. This possibility, which has been widely overlooked, offers many advantages. Untyped set theory is simple and is more flexible than any simple typed formalism. Polymorphism, overloading, and subtyping can make a type system more powerful, but at the cost of increased complexity, and such refinements can never attain the flexibility of having no types at all. Typed formalisms have advantages too, stemming from the power of mechanical type checking. While types serve little purpose in hand proofs, they do help with mechanized proofs. In the absence of verification, type checking can catch errors in specifications. It may be possible to have the best of both worlds by adding typing annotations to an untyped specification language. We consider only specification languages, not programming languages.",Set theory; Specification; Types,Computational complexity; Computer aided software engineering; Computer software; Error analysis; Mathematical models; Problem solving; Set theory; Polymorphism; Computer programming languages
Specificational functions,1999,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0000554020&doi=10.1145%2f319301.319350&partnerID=40&md5=48d9d8d07a50012e0e1a70dce125ff1f,"Mathematics supplies us with various operators for creating functions from relations, sets, known functions, and so on. Function inversion is a simple example. These operations are useful in specifying programs. However, many of them have strong constraints on their arguments to ensure that the result is indeed a function. For example, only functions that are bijective may be inverted. This is a serious impediment to their use in specifications, because at best it limits the specifier's expressive power, and at worst it imposes strong proof obligations on the programmer. We propose to loosen the definition of functions so that the constraints on operations such as inversion can be greatly relaxed. The specificational functions that emerge generalize traditional functions in that their application to some arguments may yield no good outcome, while for other arguments their application may yield any of several outcomes unpredictably. While these functions are not in general algorithmic, they can serve as specifications of traditional functions as embodied in programming languages. The idea of specificational functions is not new, but accommodating them in all their generality without falling foul of a myriad of anomalies has proved elusive. We investigate the technical problems that have hindered their use, and propose solutions. In particular, we develop a formal axiomatization for reasoning about specificational functions, and we prove its consistency by constructing a model. Categories and Subject Descriptors: F.3.1 [Logics and Meanings of Programs]: Specifying and Verifying and Reasoning about Programs - Logics of programs; F.4.1 [Mathematical Logic and Formal Languages]: Mathematical Logic - Lambda calculus and related systems; D.2.4 [Software Engineering]: Software/Program Verification - Correctness proofs; formal methods.",Expression; Function; Logic; Nondeterminacy; Refinement calculus,Computer aided software engineering; Distributed computer systems; Formal logic; Functions; Integer programming; Semantics; Axiomatization; Computer programming languages
Constraint-based termination analysis of logic programs,1999,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0001004312&doi=10.1145%2f330643.330645&partnerID=40&md5=aca2398182b889647116106c4841e5e8,"Current norm-based automatic termination analysis techniques for logic programs can be split up into different components: inference of mode or type information, derivation of models, generation of well-founded orders, and verification of the termination conditions themselves. Although providing high-precision results, these techniques suffer from an efficiency point of view, as several of these analyses are often performed through abstract interpretation. We present a new termination analysis which integrates the various components and produces a set of constraints that, when solvable, identifies successful termination proofs. The proposed method is both efficient and precise. The use of constraint sets enables the propagation of information over all different phases while the need for multiple analyses is considerably reduced.",1.2.2 [Artificial Intelligence]: Automatic Programming; 1.2.3 [Artificial Intelligence]: Deduction and Theorem Proving; Constraint solving; Languages; Logic programming; Termination analysis; Verification,
Specification and dialogue control of visual interaction through visual rewriting systems,1999,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0000895208&doi=10.1145%2f330643.330644&partnerID=40&md5=062c524b88388a5ec89922d2de0b09d5,"Computers are increasingly being seen not only as computing tools but more so as communication tools, thus placing special emphasis on human-computer interaction (HCI). In this article, the focus is on visual HCI, where the messages exchanged between human and computer are images appearing on the computer screen, as usual in current popular user interfaces. We formalize interactive sessions of a human-computer dialogue as a structured set of legal visual sentences, i.e., as a visual language, and show how rewriting systems can be generalized to specify both the pictorial and the computational aspects of visual languages. To this end, Visual Conditional Attributed Rewriting (VCARW) systems are introduced, and used for specification of visual languages. These specifications are given as inputs to a procedure illustrated in the article as a system of algorithms, which automatically generates control mechanisms of the interaction, thus favoring the design of more reliable and usable systems.",D.1.7 [Visual Programming]; D.2.2 [Design Tools and Techniques]: User Interfaces; D.2.6 [Programming Environment]: Interactive Environments; F.4.2 [Grammars and Other Rewriting Systems],
Optimizing compilation of CLP(R),1998,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0032256524&doi=10.1145%2f295656.295661&partnerID=40&md5=54ac7f6cd3110726256b2db81b9e7cfe,"Constraint Logic Programming (CLP) languages extend logic programming by allowing the use of constraints from different domains such as real numbers or Boolean functions. They have proved to be ideal for expressing problems that require interactive mathematical modeling and complex combinatorial optimization problems. However, CLP languages have mainly been considered as research systems, useful for rapid prototyping, but not really competitive with more conventional programing languages where efficiency is a more important consideration. One promising approach to improving the performance of CLP systems is the use of powerful program optimizations to reduce the cost of constraint solving. We extend work in this area by describing a new optimizing compiler for the CLP language CLP(R). The compiler implements six powerful optimizations: reordering of constraints, bypass of the constraint solver, splitting and dead-code elimination, removal of redundant constraints, removal of redundant variables, and specialization of constraints which cannot fail. Each program optimization is designed to remove the overhead of constraint solving when possible and keep the number of constraints in the store as small as possible. We systematically evaluate the effectiveness of each optimization in isolation and in combination. Our empirical evaluation of the compiler verifies that optimizing compilation can be made efficient enough to allow compilation of real-world programs and that it is worth performing such compilation because it gives significant time and space performance improvements.",,Constraint theory; Optimization; Program compilers; Programming theory; Constraint logic programming (CLP); Source-to-source program transformation; Logic programming
Space/time-efficient scheduling and execution of parallel irregular computations,1998,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0032255043&doi=10.1145%2f295656.295660&partnerID=40&md5=7ae1d7b74abafbb078b38364c249a8d9,"In this article we investigate the trade-off between time and space efficiency in scheduling and executing parallel irregular computations on distributed-memory machines. We employ acyclic task dependence graphs to model irregular parallelism with mixed granularity, and we use direct remote memory access to support fast communication. We propose new scheduling techniques and a run-time active memory management scheme to improve memory utilization while retaining good time efficiency, and we provide a theoretical analysis on correctness and performance. This work is implemented in the context of the RAPID system which uses an inspector/executor approach to parallelize irregular computations at run-time. We demonstrate the effectiveness of the proposed techniques on several irregular applications such as sparse matrix code and the fast multipole method for particle simulation. Our experimental results on Cray-T3E show that problems of large sizes can be solved under limited space capacity, and that the loss of execution efficiency caused by the extra memory management overhead is reasonable.",,Algorithms; Data structures; Parallel processing systems; Storage allocation (computer); Direct remote memory access; Parallel irregular computations; Run time support; Multiprogramming
"New, simpler linear-time dominators algorithm",1998,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0032266860&doi=10.1145%2f295656.295663&partnerID=40&md5=6da98beba066e0460021e4214dbe551e,"We present a new linear-time algorithm to find the immediate dominators of all vertices in a flowgraph. Our algorithm is simpler than previous linear-time algorithms: rather than employ complicated data structures, we combine the use of microtrees and memorization with new observations on a restricted class of path compressions. We have implemented our algorithm, and we report experimental results that show that the constant factors are low. Compared to the standard, slightly superlinear algorithm of Lengauer and Tarjan, which has much less overhead, our algorithm runs 10-20% slower on real flowgraphs of reasonable size and only a few percent slower on very large flowgraphs.",,Algorithms; Data structures; Graph theory; Program compilers; Path compression; Data flow analysis
Cost-optimal code motion,1998,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0032267445&doi=10.1145%2f295656.295664&partnerID=40&md5=eedee84f7f70ac456162e8bba6b50250,"We generalize Knoop et al.'s Lazy Code Motion (LCM) algorithm for partial redundancy elimination so that the generalized version also performs strength reduction. Although Knoop et al. have themselves extended LCM to strength reduction with their Lazy Strength Reduction algorithm, our approach differs substantially from theirs and results in a broader class of candidate expressions, stronger safety guarantees, and the elimination of the potential for performance loss instead of gain. Also, our general framework is not limited to traditional strength reduction, but rather can also handle a wide variety of optimizations in which data, flow information enables the replacement of a computation with a less expensive one. As a simple example, computations can be hoisted to points where they are constant foldable. Another example we sketch is the hoisting of polymorphic operations to points where type analysis provides leverage for optimization. Our general approach consists of placing computations so as to minimize their cost, rather than merely their number. So long as the cost differences between flowgraph nodes obey a certain natural constraint, a cost-optimal code motion transformation that does not unnecessarily prolong the lifetime of temporary variables can be found using techniques completely analogous to LCM. Specifically, the cost differences can be discovered using a wide variety of forward data-flow analyses in a manner which we described.",,Algorithms; Programming theory; Lazy code motion (LCM) algorithm; Data flow analysis
Dynamic currency determination in optimized programs,1998,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0032255186&doi=10.1145%2f295656.295657&partnerID=40&md5=a93659c7857d6ca3be403f041c2a11e1,"Compiler optimizations pose many problems to source-level debugging of an optimized program due to reordering, insertion, and deletion of code. One such problem is to determine whether the value of a variable is current at a breakpoint - that is, whether its actual value is the same as its expected value. We use the notion of dynamic currency of a variable in source-level debugging and propose the use of a minimal unrolled graph to reduce the run-time overhead of dynamic currency determination. We prove that the minimal unrolled graph is an adequate basis for performing bit-vector data flow analyses at a breakpoint. This property is used to perform dynamic currency determination. It is also shown to help in recovery of a dynamically noncurrent variable.",,Algorithms; Data flow analysis; Optimization; Program compilers; Programming theory; Dynamic concurrency determination; Program debugging
Equality-based flow analysis versus recursive types,1998,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0032269062&doi=10.1145%2f295656.295662&partnerID=40&md5=88941b92547676596caf6bad6282cd19,"Equality-based control-flow analysis has been studied by Henglein, Bondoff and Jorgensen, De-Fouw, Grove, and Chambers, and others. It is faster than the subset-based 0-CFA, but also more approximate. Heintze asserted in 1995 that a program can be safety checked with an equality-based control-flow analysis if and only if it can be typed with recursive types. In this article we falsify Heintze's assertion, and we present a type system equivalent to equality-based control-flow analysis. The new type system contains both recursive types and an unusual notion of subtyping. We have s≤t if s and t unfold to the same regular tree, and we have ⊥≤t≤T where t is a function type. In particular, there is no nontrivial subtyping between function types.",,Algorithms; Programming theory; Recursive functions; Trees (mathematics); Equality-based control-flow analysis; Data flow analysis
Finitary fairness,1998,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0032267451&doi=10.1145%2f295656.295659&partnerID=40&md5=3da8ef984715b1c2347945e6d3ab9357,"Fairness is a mathematical abstraction: in a multiprogramming environment, fairness abstracts the details of admissible (`fair') schedulers; in a distributed environment, fairness abstracts the relative speeds of processors. We argue that the standard definition of fairness often is unnecessarily weak and can be replaced by the stronger, yet still abstract, notion of finitary fairness. While standard weak fairness requires that no enabled transition is postponed forever, finitary weak fairness requires that for every computation of a system there is an unknown bound k such that no enabled transition is postponed more than k consecutive times. In general, the finitary restriction fin(F) of any given fairness requirement F is the union of all ω-regular safety properties contained in F. The adequacy of the proposed abstraction is shown in two ways. Suppose we prove a program property under the assumption of finitary fairness. In a multiprogramming environment, the program then satisfies the property for all fair finite-state schedulers. In a distributed environment, the program then satisfies the property for all choices of lower and upper bounds on the speeds (or timings) of processors. The benefits of finitary fairness are twofold. First, the proof rules for verifying liveness properties of concurrent programs are simplified: well-founded induction over the natural numbers is adequate to prove termination under finitary fairness. Second, the fundamental problem of consensus in a faulty asynchronous distributed environment can be solved assuming finitary fairness.",,Program diagnostics; Programming theory; Finitary fairness; Program verification; Multiprogramming
Task- and data-parallel programming language based on shared objects,1998,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0032256749&doi=10.1145%2f295656.295658&partnerID=40&md5=a2b94192b98f60f645d252fabfd1f691,"Many programming languages support either task parallelism or data parallelism, but few languages provide a uniform framework for writing applications that need both types of parallelism. We present a programming language and system that integrates task and data parallelism using shared objects. Shared objects may be stored on one processor or may be replicated. Objects may also be partitioned and distributed on several processors. Task parallelism is achieved by forking processes remotely and have them communicate and synchronize through objects. Data parallelism is achieved by executing operations on partitioned objects in parallel. Writing task- and data-parallel applications with shared objects has several advantages. Programmers use the objects as if they were stored in a memory common to all processors. On distributed-memory machines, if objects are remote, replicated, or partitioned, the system takes care of many low-level details such as data transfers and consistency semantics. In this article, we show how to write task- and data-parallel programs with our shared object model. We also describe a portable implementation of the model. To assess the performance of the system, we wrote several applications that use task and data parallelism and executed them on a collection of Pentium Pros connected by Myrinet. The performance of the applications is also discussed in this articles.",,Computer hardware description languages; Data structures; Data parallelism; Object oriented programming
"The Design, Implementation, and Evaluation of Jade",1998,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0032058018&doi=10.1145%2f291889.291893&partnerID=40&md5=adb5231fd26f59707776c36d8eb6953c,"Jade is a portable, implicitly parallel language designed for exploiting task-level concurrency. Jade programmers start with a program written in a standard serial, imperative language, then use Jade constructs to declare how parts of the program access data. The Jade implementation uses this data access information to automatically extract the concurrency and map the application onto the machine at hand. The resulting parallel execution preserves the semantics of the original serial program. We have implemented Jade as an extension to C, and Jade implementations exist for shared-memory multiprocessors, homogeneous message-passing machines, and heterogeneous networks of workstations. In this article we discuss the design goals and decisions that determined the final form of Jade and present an overview of the Jade implementation. We also present our experience using Jade to implement several complete scientific and engineering applications. We use this experience to evaluate how the different Jade language features were used in practice and how well Jade as a whole supports the process of developing parallel applications. We find that the basic idea of preserving the serial semantics simplifies the program development process, and that the concept of using data access specifications to guide the parallelization offers significant advantages over more traditional control-based approaches. We also find that the Jade data model can interact poorly with concurrency patterns that write disjoint pieces of a single aggregate data structure, although this problem arises in only one of the applications.",D.1.3 [Programming Techniques]: Concurrent Programming - parallel programming; D.3.2 [Programming Languages]: Language Constructs and Features - concurrent programming structures; Languages; Parallel computing; Parallel programming languages; Performance,Computational linguistics; Computer aided software engineering; Computer networks; Computer programming; Computer workstations; Data acquisition; Data structures; Parallel processing systems; Abstracting; Algorithms; Binary codes; Computer aided design; Computer hardware; Computer software; Computer software portability; Concurrency control; Mathematical models; Problem solving; Semantics; Concurrent programming; Data access specifications; Concurrent programming; Parallel programming languages; Software Package Jade; Computer programming languages
Constraint-Based Array Dependence Analysis,1998,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0032058019&doi=10.1145%2f291889.291900&partnerID=40&md5=f5496b42f69400a4a711cf79082909c9,"Traditional array dependence analysis, which detects potential memory aliasing of array references, is a key analysis technique for automatic parallelization. Recent studies of benchmark codes indicate that limitations of analysis cause many compilers to overlook large amounts of potential parallelism, and that exploiting this parallelism requires algorithms to answer new questions about array references, not just get better answers to the old question of aliasing. We need to ask about the flow of values in arrays, to check the legality of array privatization, and about the conditions under which a dependence exists, to obtain information about conditional parallelism. In some cases, we must answer these questions about code containing nonlinear terms in loop bounds or subscripts. This article describes techniques for phrasing these questions in terms of systems of constraints. Conditional dependence analysis can be performed with a constraint operation we call the ""gist"" operation. When subscripts and loop bounds are affine, questions about the flow of values in array variables can be phrased in terms of a subset of Presburger Arithmetic. When the constraints describing a dependence are not affine, we introduce uninterpreted function symbols to represent the nonaffine terms. Our constraint language also provides a rich language for communication with the dependence analyzer, by either the programmer or other phases of the compiler. This article also documents our investigations of the practicality of our approach. The worst-case complexity of Presburger Arithmetic indicates that it might be unsuitable for any practical application. However, we have found that analysis of benchmark programs does not cause the exponential growth in the number of constraints that could occur in the worst case. We have studied the constraints produced during our analysis, and identified characteristics that keep our algorithms free of exponential behavior in practice.",Algorithms; Array dataflow analysis; D.3.4 [Programming Languages]: Processors - compilers; optimization; Dependence abstraction; Dependence analysis; Languages; Parallelization; Presburger arithmetic; Static analysis,Algorithms; Binary codes; Computational complexity; Constraint theory; Data flow analysis; Data storage equipment; Iterative methods; Parallel processing systems; Set theory; Vectors; Arrays; Computer programming; Computer programming languages; Digital arithmetic; Program compilers; Array dependence analysis; Static analysis; Array dependence analysis; Benchmark programs; Presburger arithmetic; Computer programming languages; Data flow analysis
Within ARM's Reach: Compilation of Left-Linear Rewrite Systems via Minimal Rewrite Systems,1998,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0032057966&doi=10.1145%2f291889.291903&partnerID=40&md5=a9184f5a525f9cbee519578aa05c53fe,"A new compilation technique for left-linear term-rewriting systems is presented, where rewrite rules are transformed into so-called minimal rewrite rules. These minimal rules have such a simple form that they can be viewed as instructions for an abstract rewriting machine (ARM).",Abstract machine; Automata; D.3.4 [Programming Languages]: Processors - compilers; optimization; Languages; Specificity ordering; Term rewriting,Abstracting; Automata theory; Codes (symbols); Data storage equipment; Formal languages; Graph theory; Machine oriented languages; Mathematical transformations; Semantics; Syntactics; Computer programming languages; Computer systems; Program compilers; Abstract rewriting machines (ARM); Functional languages; Term-rewriting systems (TRS); Abstract rewriting machine; Term rewriting systems; Program compilers; Computer programming
A Region Inference Algorithm,1998,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0032108162&doi=10.1145%2f291891.291894&partnerID=40&md5=8b9ebd3ae0667ce49099f3b57a803ca0,"Region Inference is a program analysis which infers lifetimes of values. It is targeted at a runtime model in which the store consists of a stack of regions and memory management predominantly consists of pushing and popping regions, rather than performing garbage collection. Region Inference has previously been specified by a set of inference rules which formalize when regions may be allocated and deallocated. This article presents an algorithm which implements the specification. We prove that the algorithm is sound with respect to the region inference rules and that it always terminates even though the region inference rules permit polymorphic recursion in regions. The algorithm is the result of several years of experiments with region inference algorithms in the ML Kit, a compiler from Standard ML to assembly language. We report on practical experience with the algorithm and give hints on how to implement it.",Algorithms; D.3.3 [Programming Languages]: Language Constructs and Features - Dynamic storage management; F.3.3 [Logics and Meanings of Programs]: Studies of Program Constructs - Functional constructs; Regions; Reliability; Theory; Type structure,Algorithms; Computer programming languages; Computer simulation; Data storage equipment; Program compilers; Recursive functions; Response time (computer systems); Data structures; Inference engines; Reliability theory; Storage allocation (computer); Syntactics; Assembly languages; Polymorphic systems; Polymorphic recursion; Program analysis; Region inference algorithm; Program diagnostics; Computer programming languages
Partial Evaluation of Functional Logic Programs,1998,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0032108041&doi=10.1145%2f291891.291896&partnerID=40&md5=bcfd435ceaf85294f90add6ee20da6c3,"Languages that integrate functional and logic programming with a complete operational semantics are based on narrowing, a unification-based goal-solving mechanism which subsumes the reduction principle of functional languages and the resolution principle of logic languages. In this article, we present a partial evaluation scheme for functional logic languages based on an automatic unfolding algorithm which builds narrowing trees. The method is formalized within the theoretical framework established by Lloyd and Shepherdson for the partial deduction of logic programs, which we have generalized for dealing with functional computations. A generic specialization algorithm is proposed which does not depend on the eager or lazy nature of the narrower being used. To the best of our knowledge, this is the first generic algorithm for the specialization of functional logic programs. We study the semantic properties of the transformation and the conditions under which the technique terminates, is sound and complete, and is generally applicable to a wide class of programs. We also discuss the relation to work on partial evaluation in functional programming, term-rewriting systems, and logic programming. Finally, we present some experimental results with an implementation of the algorithm which show in practice that the narrowing-driven partial evaluator effectively combines the propagation of partial data structures (by means of logical variables and unification) with better opportunities for optimization (thanks to the functional dimension).",D.1.1 [Programming Techniques]: Applicative (Functional) Programming; D.1.6 [Programming Techniques]: Logic Programming; D.3.3 [Language Classifications]: Multiparadigm Languages,Algorithms; Computational linguistics; Data structures; Logic programming; Optimization; Artificial intelligence; Automatic programming; Computer programming languages; Performance; Problem solving; Semantics; Functional programming; Functional logic languages; Functional programming; Program transformation; Program diagnostics; Logic programming
A Practical and Flexible Flow Analysis for Higher-Order Languages,1998,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0032108284&doi=10.1145%2f291891.291898&partnerID=40&md5=9fed9c65243b0a689f4d8311125d539a,"A flow analysis collects data-flow and control-flow information about programs. A compiler can use this information to enable optimizations. The analysis described in this article unifies and extends previous work on flow analyses for higher-order languages supporting assignment and control operators. The analysis is abstract interpretation based and is parameterized over two polyvariance operators and a projection operator. These operators are used to regulate the speed and accuracy of the analysis. An implementation of the analysis is incorporated into and used in a production Scheme compiler. The analysis can process any legal Scheme program without modification. Others have demonstrated that a OCFA analysis can enable optimizations, but a OCFA analysis is O(n3). An O(n) instantiation of our analysis successfully enables the optimization of closure representations and procedure calls. Experiments with the cheaper instantiation show that it is as effective as OCFA for these optimizations.",Abstract interpretation; Algorithms; D.3.4 [Programming; Design; Higher-order languages; Languages: Processors - Optimization,Abstracting; Data flow analysis; Data structures; Mathematical operators; Optimization; Program compilers; Algorithms; High level languages; Control-flow analysis; Parameterizations; Polyvariance operators; Projection operator; High level languages; Data flow analysis
Compositional verification of concurrent systems using Petri-net-based condensation rules,1998,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0032153859&doi=10.1145%2f293677.293681&partnerID=40&md5=b2c31fbd3f5cc55ad21d4d24eb7aa1ee,"The state-explosion problem of formal verification has obstructed its application to large-scale software systems. In this article, we introduce a set of new condensation theories: IOT-failure equivalence, IOT-state equivalence, and firing-dependence theory to cope with this problem. Our condensation theories are much weaker than current theories used for the compositional verification of Petri nets. More significantly, our new condensation theories can eliminate the interleaved behaviors caused by asynchronously sending actions. Therefore, our technique provides a much more powerful means for the compositional verification of asynchronous processes. Our technique can efficiently analyze several state-based properties: boundedness, reachable markings, reachable submarkings, and deadlock states. Based on the notion of our new theories, we develop a set of condensation rules for efficient verification of large-scale software systems. The experimental results show a significant improvement in the analysis of large-scale concurrent systems. © 1998 ACM.",Algorithms; Boundedness; D.2.4[Software Engineering]: Program Verification; Experimentation; F.3.1[Logics and Meanings of Programs]: Specifying and Verifying and Reasoning about Programs - mechanical verification; Reliability; Theory; Verification,Large scale systems; Petri nets; Deadlock states; Reachability graphs; Software engineering
A logical model for relational abstract domains,1998,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0032153905&doi=10.1145%2f293677.293680&partnerID=40&md5=4eb48b4eae5c11b96c810cbacc64882d,"In this article we introduce the notion of Heyting completion in abstract interpretation. We prove that Heyting completion provides a model for Cousot's reduced cardinal power of abstract domains and that it supplies a logical basis to specify relational domains for program analysis and abstract interpretation. We study the algebraic properties of Heyting completion in relation with other well-known domain transformers, like reduced product and disjunctive completion. This provides a uniform algebraic setting where complex abstract domains can be specified by simple logic formulas or as solutions of recursive abstract domain equations, involving few basic operations for domain construction, all characterized by a clean logical interpretation. We apply our framework to characterize directionality and condensing in downward closed analysis of (constraint) logic programs. © 1998 ACM.",D.3.4[Programming Languages]: Processors - optimization; F.3.2[Logics and Meanings of Programs]: Semantics of Programming Languages - program analysis; F.4.1[Mathematic Logic and Formal Languages]: Mathematical Logic; Languages; Theory,Algebra; Constraint theory; Logic programming; Mathematical models; Heyting completion; Programming theory
An Abstract Machine for Tabled Execution of Fixed-Order Stratified Logic Programs,1998,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0032058020&doi=10.1145%2f291889.291897&partnerID=40&md5=f637e02bc0280ed30c2d25b64c77d428,"SLG resolution uses tabling to evaluate nonfloundering normal logic programs according to the well-founded semantics. The SLG-WAM, which forms the engine of the XSB system, can compute in-memory recursive queries an order of magnitude faster than current deductive databases. At the same time, the SLG-WAM tightly integrates Prolog code with tabled SLG code, and executes Prolog code with minimal overhead compared to the WAM. As a result, the SLG-WAM brings to logic programming important termination and complexity properties of deductive databases. This article describes the architecture of the SLG-WAM for a powerful class of programs, the class of fixed-order dynamically stratified programs. We offer a detailed description of the algorithms, data structures, and instructions that the SLG-WAM adds to the WAM, and a performance analysis of engine overhead due to the extensions.",D.1.2 [Programming Techniques]: Automatic Programming; D.1.6 [Programming Techniques]: Logic Programming; D.3.4 [Programming Languages]: Processors - compilers; interpreters; optimization,Algorithms; Artificial intelligence; Automatic programming; Binary codes; Computational complexity; Computer architecture; Data structures; Database systems; Formal languages; Program compilers; Semantics; Theorem proving; Program interpreters; PROLOG (programming language); Interpreters; Memoing; Automatic programming; Deductive database; Stratified logic programs; Logic programming
Efficient and flexible incremental parsing,1998,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0032153740&doi=10.1145%2f293677.293678&partnerID=40&md5=065d2d1d13501670bc7c522862652e69,"Previously published algorithms for LR(k) incremental parsing are inefficient, unnecessarily restrictive, and in some cases incorrect. We present a simple algorithm based on parsing LR(k) sentential forms that can incrementally parse an arbitrary number of textual and/or structural modifications in optimal time and with no storage overhead. The central role of balanced sequences in achieving truly incremental behavior from analysis algorithms is described, along with automated methods to support balancing during parse table generation and parsing. Our approach extends the theory of sentential-form parsing to allow for ambiguity in the grammar, exploiting it for notational convenience, to denote sequences, and to construct compact (""abstract"") syntax trees directly. Combined, these techniques make the use of automatically generated incremental parsers in interactive software development environments both practical and effective. In addition, we address information preservation in these environments: Optimal node reuse is defined; previous definitions are shown to be insufficient; and a method for detecting node reuse is provided that is both simpler and faster than existing techniques. A program representation based on self-versioning documents is used to detect changes in the program, generate efficient change reports for subsequent analyses, and allow the parsing transformation itself to be treated as a reversible modification in the edit log. © 1998 ACM.",D.2.6 [Software Engineering]: Programming Environments - interactive; D.2.7 [Software Engineering]: Distribution and Maintenance - version control,Algorithmic languages; Program compilers; Program translators; Incremental parsing; Software engineering
Automatic Data Layout for Distributed-Memory Machines,1998,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0032108102&doi=10.1145%2f291891.291901&partnerID=40&md5=05a625ac89280e6716c04f5fbbf1d1af,"The goal of languages like Fortran D or High Performance Fortran (HPF) is to provide a simple yet efficient machine-independent parallel programming model. After the algorithm selection, the data layout choice is the key intellectual challenge in writing an efficient program in such languages. The performance of a data layout depends on the target compilation system, the target machine, the problem size, and the number of available processors. This makes the choice of a good layout extremely difficult for most users of such languages. If languages such as HPF are to find general acceptance, the need for data layout selection support has to be addressed. We believe that the appropriate way to provide the needed support is through a tool that generates data layout specifications automatically. This article discusses the design and implementation of a data layout selection tool that generates HPF-style data layout specifications automatically. Because layout is done in a tool that is not embedded in the target compiler and hence will be run only a few times during the tuning phase of an application, it can use techniques such as integer programming that may be considered too computationally expensive for inclusion in production compilers. The proposed framework for automatic data layout selection builds and examines search spaces of candidate data layouts. A candidate layout is an efficient layout for some part of the program. After the generation of search spaces, a single candidate layout is selected for each program part, resulting in a data layout for the entire program. A good overall data layout may require the remapping of arrays between program parts. A performance estimator based on a compiler model, an execution model, and a machine model are needed to predict the execution time of each candidate layout and the costs of possible remappings between candidate data layouts. In the proposed framework, instances of NP-complete problems are solved during the construction of candidate layout search spaces and the final selection of candidate layouts from each search space. Rather than resorting to heuristics, the framework capitalizes on state-of-the-art 0-1 integer programming technology to compute optimal solutions of these NP-complete problems. A prototype data layout assistant tool based on our framework has been implemented as part of the D system currently under development at Rice University. The article reports preliminary experimental results. The results indicate that the framework is efficient and allows the generation of data layouts of high quality.",,Computational complexity; Computer simulation; Data structures; Distributed computer systems; FORTRAN (programming language); Integer programming; Program compilers; Response time (computer systems); Algorithms; Computer architecture; Computer software portability; Data reduction; Embedded systems; Graph theory; Mathematical models; Code generation; Mathematical mappings; Data layout; Distributed memory machine; Production compilers; Computer aided software engineering; Parallel processing systems
Static Caching for Incremental Computation,1998,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0032057980&doi=10.1145%2f291889.291895&partnerID=40&md5=bd998705ab5fc0d5b8bf244539046adf,"A systematic approach is given for deriving incremental programs that exploit caching. The cache-and-prune method presented in the article consists of three stages: (I) the original program is extended to cache the results of all its intermediate subcomputations as well as the final result, (II) the extended program is incrementalized so that computation on a new input can use all intermediate results on an old input, and (III) unused results cached by the extended program and maintained by the incremental program are pruned away, leaving a pruned extended program that caches only useful intermediate results and a pruned incremental program that uses and maintains only the useful results. All three stages utilize static analyses and semantics-preserving transformations. Stages I and III are simple, clean, and fully automatable. The overall method has a kind of optimally with respect to the techniques used in Stage II. The method can be applied straightforwardly to provide a systematic approach to program improvement via caching.",D.1 [Programming Techniques]: Automatic Programming - automatic analysis of algorithms; program transformation; D.3.3 [Programming Languages] Language Constructs and Features; D.3.4 [Programming Languages]: Processors - optimization,Cache memory; Computation theory; Computer aided design; Computer program listings; Optimization; Problem solving; Program compilers; Semantics; Algorithms; Computational linguistics; Computational methods; Computer programming; Computer programming languages; Program translators; Caching; Incremental programs; Memoization; Cache and prune method; Program transformations; Static caching; Computer programming; Computer software
Proofs about a Folklore Let-Polymorphic Type Inference Algorithm,1998,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0032108250&doi=10.1145%2f291891.291892&partnerID=40&md5=c96592150f2be9efe1e5f92cda6dd794,"The Hindley/Milner let-polymorphic type inference system has two different elgorithms: one is the de facto standard Algorithm W that is bottom-up (or context-insensitive), and the other is a ""folklore"" algorithm that is top-down (or context-sensitive). Because the latter algorithm has not been formally presented with its soundness and completeness proofs, and its relation with the W algorithm has not been rigorously investigated, its use in place of (or in combination with) W is not well founded. In this article, we formally define the context-sensitive, top-down type inference algorithm (named ""M""), prove its soundness and completeness, and show a distinguishing property that M always stops earlier than W if the input program is ill typed. Our proofs can be seen as theoretical justifications for various type-checking strategies being used in practice.",Algorithms; D.3.3 [Programming Languages]: Language Constructs and Features - Data types and structures; F.3.3 [Logics and Meaning of Programs]: Studies of Program Constructs - Type structure; Languages; Theory; Type error; Type inference algorithm,Algorithms; Computer programming; Data structures; Program compilers; Context free languages; Context sensitive languages; Error analysis; Inference engines; Recursive functions; Theorem proving; Inference algorithm; Polymorphic type algorithm; Polymorphic systems; Type-checkings; Context sensitive languages; Computer programming languages
A Study of the Applicability of Existing Exception-Handling Techniques to Component-Based Real-Time Software Technology,1998,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0032010350&doi=10.1145%2f276393.276395&partnerID=40&md5=1f5dab782a60fa9ca8e180849b145354,"This study focuses on the current state of error-handling technology and concludes with recommendations for further research in error handling for component-based real-time software. With real-time programs growing in size and complexity, the quality and cost of developing and maintaining them are still deep concerns to embedded software industries. Component-based software is a promising approach in reducing development cost while increasing quality and reliability. As with any other real-time software, component-based software needs exception detection and handling mechanisms to satisfy reliability requirements. The current lack of suitable error-handling techniques can make an application composed of reusable software nondeterministic and difficult to understand in the presence of errors.",D.2.10 [Software Engineering]: Design; D.2.3 [Software Engineering]: Coding Tools and Techniques; D.2.6 [Software Engineering]: Programming Environments; D.3.3 [Programming Languages]: Language Constructs and Features,Computer programming; Computer programming languages; Computer software reusability; Computer system recovery; Error detection; Real time systems; Standardization; Embedded systems; Error handling; Reconfigurable software; Error handling; Real time software; Software industry; Software engineering; Computer software
Alma-0: An imperative language that supports declarative programming,1998,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0032153940&doi=10.1145%2f293677.293679&partnerID=40&md5=e54d6bbd24eb5bc4914d1165ac0f39a1,"We describe here an implemented small programming language, called Alma-0, that augments the expressive power of imperative programming by a limited number of features inspired by the logic programming paradigm. These additions encourage declarative programming and make it a more attractive vehicle for problems that involve search. We illustrate the use of Alma-0 by presenting solutions to a number of classical problems, including α-β search, STRIPS planning, knapsack, and Eight Queens. These solutions are substantially simpler than their counterparts written in the imperative or in the logic programming style and can be used for different purposes without any modification. We also discuss here the implementation of Alma-0 and an operational, executable, semantics of a large subset of the language. © 1998 ACM.","1.2.8 [Problem Solving, Control Methods and Search]: Backtracking; D.3.2 [Language Classifications]: Nondeterministic Languages; F.3.2 [Semantics of Programming Languages]: Operational Semantics; F.3.3 [Studies of Program Constructs]: Control Primitives",Logic programming; Programming theory; Declarative programming; Programming language Alma-0; Problem oriented languages
A Partially Deadlock-Free Typed Process Calculus,1998,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0032010343&doi=10.1145%2f276393.278524&partnerID=40&md5=4dd57d3387f97bc5a6e472159152b9d1,"We propose a novel static type system for a process calculus, which ensures both partial deadlock-freedom and partial confluence. The key novel ideas are (1) introduction of the order of channel use as type information, and (2) classification of communication channels into reliable and unreliable channels based on their usage and a guarantee of the usage by the type system. We can ensure that communication on reliable channels never causes deadlock and also that certain reliable channels never introduce nondeterminism. After presenting the type system and formal proofs of its correctness, we show encodings of the λ-calculus and typical concurrent objects in the deadlock-free fragment of the calculus and demonstrate how type information can be used for reasoning about program behavior.",Concurrency; D.3.1 [Programming Languages]: Formal Definitions and Theory; D.3.3 [Programming Languages]: Language Constructs and Features General Terms: Languages; Deadlock-freedom; Type theory,Communication channels (information theory); Computer programming; Computer system recovery; Encoding (symbols); Algorithms; Problem solving; Theorem proving; Concurrent programming languages; Process calculus; Calculus; Deadlock-freedom; Type theory; Computer programming languages
A Systematic Study of Functional Language Implementations,1998,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0032010344&doi=10.1145%2f276393.276397&partnerID=40&md5=202790c5bcb6d328050149da568eb4ba,"We introduce a unified framework to describe, relate, compare, and classify functional language implementations. The compilation process is expressed as a succession of program transformations in the common framework. At each step, different transformations model fundamental choices. A benefit of this approach is to structure and decompose the implementation process. The correctness proofs can be tackled independently for each step and amount to proving program transformations in the functional world. This approach also paves the way to formal comparisons by making it possible to estimate the complexity of individual transformations or compositions of them. Our study aims at covering the whole known design space of sequential functional language implementations. In particular, we consider call-by-value, call-by-name, and call-by-need reduction strategies as well as environment- and graph-based implementations. We describe for each compilation step the diverse alternatives as program transformations. In some cases, we illustrate how to compare or relate compilation techniques, express global optimizations, or hybrid implementations. We also provide a classification of well-known abstract machines.",D.1.1 [Programming Techniques]: Functional Programming; D.2.4 [Software Engineering]: Program Verification - correctness proofs; D.2.8 [Software Engineering]: Metrics - complexity measures; D.3.4 [Programming Languages]: Processors - code generation,Abstracting; Graph theory; Program compilers; Program translators; Computational complexity; Computer programming; Data structures; Optimization; Program diagnostics; Abstract machines; Functional programing; Program transformations; Functional languages; Functional programming; Program verification; Computer programming languages
Synthesis of Concurrent Systems with Many Similar Processes,1998,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031598901&doi=10.1145%2f271510.271519&partnerID=40&md5=31db6b6aa24a6a7a3fcd66a71af6e61f,"Methods for synthesizing concurrent programs from temporal logic specifications based on the use of a decision procedure for testing temporal satisfiability have been proposed by Emerson and Clarke and by Manna and Wolper. An important advantage of these synthesis methods is that they obviate the need to manually compose a program and manually construct a proof of its correctness. One only has to formulate a precise problem specification; the synthesis method then mechanically constructs a correct solution. A serious drawback of these methods in practice, however, is that they suffer from the state explosion problem. To synthesize a concurrent system consisting of K sequential processes, each having N states in its local transition diagram, requires construction of the global product-machine having about NK global states in general. This exponential growth in K makes it infeasible to synthesize systems composed of more than 2 or 3 processes. In this article, we show how to synthesize concurrent systems consisting of many (i.e., a finite but arbitrarily large number K of) similar sequential processes. Our approach avoids construction of the global product-machine for K processes; instead, it constructs a two-process product-machine for a single pair of generic sequential processes. The method is uniform in K, providing a simple template that can be instantiated for each process to yield a solution for any fixed K. The method is also illustrated on synchronization problems from the literature.",C.2.4 [Computer-Communication Networks]: Distributed Systems; D.1.2 [Programming Techniques]: Automatic Programming; D.1.3 [Programming Techniques]: Concurrent Programming; D.2.4 [Software Engineering]: Program Verification,Automation; Computer programming; Decision theory; Graph theory; Synchronization; Artificial intelligence; Computational methods; Distributed computer systems; Program diagnostics; Software engineering; Concurrent programs; Program synthesis; Temporal logic; Concurrent program; Program synthesis; Program verification; State explosion problem; Concurrency control; Computer programming
Solving Shape-Analysis Problems in Languages with Destructive Updating,1998,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031598986&doi=10.1145%2f271510.271517&partnerID=40&md5=5181430e292f88178807dddeea55b960,"This article concerns the static analysis of programs that perform destructive updating on heap-allocated storage. We give an algorithm that uses finite shape graphs to approximate conservatively the possible ""shapes"" that heap-allocated structures in a program can take on. For certain programs, our technique is able to determine such properties as (1) when the input to the program is a list, the output is also a list and (2) when the input to the program is a tree, the output is also a tree. For example, the method can determine that ""listness"" is preserved by (1) a program that performs list reversal via destructive updating of the input list and (2) a program that searches a list and splices a new element into the list. None of the previously known methods that use graphs to model the program's store are capable of determining that ""listness"" is preserved on these examples (or examples of similar complexity). In contrast with most previous work, our shape analysis algorithm is even accurate for certain programs that update cyclic data structures; that is, it is sometimes able to show that when the input to the program is a circular list, the output is also a circular list. For example, the shape-analysis algorithm can determine that an insertion into a circular list preserves ""circular listness."".",,Abstracting; Algorithms; Data flow analysis; Data structures; Graph theory; Vectors; Approximation theory; Computer program listings; Computer programming languages; Data storage equipment; Mathematical models; Alias analysis; Pointer analysis; Shape analysis algorithm; Shape graphs; Destructive updating; Finite shape graphs; Program analysis; Computer programming languages; Program diagnostics
Fast Algorithms for Compressed Multimethod Dispatch Table Generation,1998,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031598987&doi=10.1145%2f271510.271521&partnerID=40&md5=416c95d064fa7da94f873140d2d06f6d,"The efficiency of dynamic dispatch is a major impediment to the adoption of multimethods in object-oriented languages. In this article, we propose a simple multimethod dispatch scheme based on compressed dispatch tables. This scheme is applicable to any object-oriented language using a method precedence order that satisfies a specific monotonous property (e.g., as Cecil and Dylan) and guarantees that dynamic dispatch is performed in constant time, the latter being a major requirement for some languages and applications. We provide efficient algorithms to build the dispatch tables, provide their worst-case complexity, and demonstrate the effectiveness of our scheme by real measurements performed on two large object-oriented applications. Finally, we provide a detailed comparison of our technique with other existing techniques.","D.3.3 [Programming Languages]: Language Constructs and Features - control structures; D.3.4 [Programming Languages]: Processors - optimization; E.1.3 [Data]: Data Structures - tables; Procedures, functions and subroutines",Algorithms; Computational complexity; Data compression; Data storage equipment; Data structures; Object oriented programming; Optimization; Subroutines; Computer programming languages; Graph theory; Heuristic methods; Vectors; Dispatch tables; Multimethods; Pole types; Run-time dispatch; Dispatch table; Multimethod dispatch scheme; Object oriented languages; Computer programming languages; Object oriented programming
Controlling Generalization and Polyvariance in Partial Deduction of Normal Logic Programs,1998,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031598967&doi=10.1145%2f271510.271525&partnerID=40&md5=b1f64e5881494214fdd583cf75568399,"Given a program and some input data, partial deduction computes a specialized program handling any remaining input more efficiently. However, controlling the process well is a rather difficult problem. In this article, we elaborate global control for partial deduction: for which atoms, among possibly infinitely many, should specialized relations be produced, meanwhile guaranteeing correctness as well as termination? Our work is based on two ingredients. First, we use the concept of a characteristic tree, encapsulating specialization behavior rather than syntactic structure, to guide generalization and polyvariance, and we show how this can be done in a correct and elegant way. Second, we structure combinations of atoms and associated characteristic trees in global trees registering ""causal"" relationships among such pairs. This allows us to spot looming nontermination and perform proper generalization in order to avert the danger, without having to impose a depth bound on characteristic trees. The practical relevance and benefits of the work are illustrated through extensive experiments. Finally, a similar approach may improve upon current (on-line) control strategies for program transformation in general such as (positive) supercompilation of functional programs. It also seems valuable in the context of abstract interpretation to handle infinite domains of infinite height with more precision.",D.1.2 [Programming Techniques]: Automatic Programming; D.1.6 [Programming Techniques]: Logic Programming; I.2.3 [Artificial Intelligence]: Deduction and Theorem Proving - logic programming,Artificial intelligence; Computational linguistics; Data flow analysis; Data structures; Formal languages; Online systems; Program translators; Abstracting; Algorithms; Syntactics; Trees (mathematics); Characteristic tree; Partial deduction; Polyvariance; Program transformation; Flow analysis; Partial evaluation; Logic programming
Isomorph-Free Model Enumeration: A New Method for Checking Relational Specifications,1998,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0032010255&doi=10.1145%2f276393.276396&partnerID=40&md5=40e9c21d4fc558307cb77db82a78efcf,"Software specifications often involve data structures with huge numbers of values, and consequently they cannot be checked using standard state exploration or model-checking techniques. Data structures can be expressed with binary relations, and operations over such structures can be expressed as formulae involving relational variables. Checking properties such as preservation of an invariant thus reduces to determining the validity of a formula or, equivalently, finding a model (of the formula's negation). A new method for finding relational models is presented. It exploits the permutation invariance of models - if two interpretations are isomorphic, then neither is a model, or both are - by partitioning the space into equivalence classes of symmetrical interpretations. Representatives of these classes are constructed incrementally by using the symmetry of the partial interpretation to limit the enumeration of new relation values. The notion of symmetry depends on the type structure of the formula; by picking the weakest typing, larger equivalence classes (and thus fewer representatives) are obtained. A more refined notion of symmetry that exploits the meaning of the relational operators is also described. The method typically leads to exponential reductions; in combination with other, simpler, reductions it makes automatic analysis of relational specifications possible for the first time.",D.2.1 [Software Engineering]: Requirements/Specifications; D.2.4 [Software Engineering]: Program Verification; D.2.5 [Software Engineering]: Testing and Debugging,Algorithms; Computer programming languages; Data structures; Equivalence classes; Mathematical models; Program debugging; Program diagnostics; Binary sequences; Formal languages; Pruning; Relational calculus; Relational specifications; Program verification; Relational models; Software specifications; Software engineering; Computer software
"""Maximal-Munch"" Tokenization in Linear Time",1998,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0032010256&doi=10.1145%2f276393.276394&partnerID=40&md5=07ad0d63c0244caeeb77d42f532d7904,"The lexical-analysis (or scanning) phase of a compiler attempts to partition an input string into a sequence of tokens. The convention in most languages is that the input is scanned left to right, and each token identified is a ""maximal munch"" of the remaining input - the longest prefix of the remaining input that is a token of the language. Although most of the standard compiler textbooks present a way to perform maximal-munch tokenization, the algorithm they describe is one that, for certain sets of token definitions, can cause the scanner to exhibit quadratic behavior in the worst case. In this article, we show that maximal-munch tokenization can always be performed in time linear in the size of the input.",D.3.4 [Programming Languages]: Processors - compilers; F.1.1 [Computation by Abstract Devices]: Models of Computation - automata; F.2.2 [Analysis of Algorithms and Problem Complexity]: Nonnumerical Algorithms and Problems - pattern matching,Dynamic programming; Finite automata; Quadratic programming; Algorithms; Automata theory; Computational complexity; Computer programming languages; Pattern matching; Text processing; Backtracking; Memoization; Tokenization; Lexical analysis; Maximal munch tokenization; Computer programming languages; Program compilers
Polymorphic Splitting: An Effective Polyvariant Flow Analysis,1998,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031598899&doi=10.1145%2f271510.271523&partnerID=40&md5=7d915c0ee903338c8cb8e6c8a3aa9d0a,"This article describes a general-purpose program analysis that computes global control-flow and data-flow information for higher-order, call-by-value languages. The analysis employs a novel form of polyvariance called polymorphic splitting that uses let-expressions as syntactic clues to gain precision. The information derived from the analysis is used both to eliminate run-time checks and to inline procedures. The analysis and optimizations have been applied to a suite of Scheme programs. Experimental results obtained from the prototype implementation indicate that the analysis is extremely precise and has reasonable cost. Compared to monovariant flow analyses such as 0CFA, or analyses based on type inference such as soft typing, the analysis eliminates significantly more run-time checks. Run-time check elimination and inlining together typically yield a 20 to 40% performance improvement for the programs in the benchmark suite, with some programs running four times as fast.",D.3.2 [Programming Languages]: Language classifications - applicative languages; D.3.4 [Programming Languages]: Processors - optimization; Experimentation; Flow analysis; Inlining; Performance; Polyvariance; Run-time checks,Abstracting; Computation theory; Data flow analysis; Optimization; Program compilers; Computational linguistics; Computer programming languages; Computer software; Polymorphic splitting; Polyvariance; Monovariant flow analyses; Polymorphic splitting; Run time checks; Soft typing; Computer programming languages; Data flow analysis
A New Framework for Elimination-Based Data Flow Analysis Using DJ Graphs,1998,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0032010237&doi=10.1145%2f276393.278523&partnerID=40&md5=08281cd5a79ebfa83f0e302768c6d211,"In this article we present a new framework for elimination-based exhaustive and incremental data flow analysis using the DJ graph representation of a program. Unlike previous approaches to elimination-based incremental data flow analysis, our approach can handle arbitrary structural and nonstructural changes to program flowgraphs, including irreducibility. We show how our approach is related to dominance frontiers, and we exploit this relationship to establish the complexity of our exhaustive analysis and to aid the design of our incremental analysis.",Algorithms; D.3.4 [Programming Languages]: Processors - compilers; DJ graphs; E.1 [Data]: Data Structures - graphs; Exhaustive and incremental data flow analysis; Irreducible flowgraphs; Languages; Optimization; Reducible flowgraphs; Tarjan's interval; Trees,Algorithms; Computational complexity; Computer programming languages; Data structures; Graph theory; Optimization; Program compilers; Computer program listings; Irreducible flowgraphs; Reducible flowgraphs; DJ graphs; Program flowgraphs; Tarjan interval; Data flow analysis
A Practical Framework for Demand-Driven Interprocedural Data Flow Analysis,1997,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031269332&doi=10.1145%2f267959.269970&partnerID=40&md5=8de1f604bef81b1911c0b226f4560e8c,"The high cost and growing importance of interprocedural data flow analysis have led to an increased interest in demand-driven algorithms. In this article, we present a general framework for developing demand-driven interprocedural data flow analyzers and report our experience in evaluating the performance of this approach. A demand for data flow information is modeled as a set of queries. The framework includes a generic demand-driven algorithm that determines the response to a query by iteratively applying a system of query propagation rules. The propagation rules yield precise responses for the class of distributive finite data flow problems. We also describe a two-phase framework variation to accurately handle nondistributive problems. A performance evaluation of our demand-driven approach is presented for two data flow problems, namely, reaching-definitions and copy constant propagation. Our experiments show that demand-driven analysis performs well in practice, reducing both time and space requirements when compared with exhaustive analysis.",Algorithms; Copy constant propagation; Data flow analysis; Def-use chains; Demand-driven algorithms; Distributive data flow frameworks; Experimentation; Interprocedural data flow analysis; Performance; Program optimizations,Algorithms; Computer programming languages; Computer software; Information retrieval systems; Program compilers; Program processors; Query languages; Software engineering; Copy constant propagation; Def use chains; Demand driven algorithms; Interprocedural data flow analysis; Program optimizations; Data reduction
Verifying Parameterized Networks,1997,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031221834&doi=10.1145%2f265943.265960&partnerID=40&md5=0540ee0e2de09f0c13572efcabe01345,This article describes a technique based on network grammars and abstraction to verify families of state-transition systems. The family of state-transition systems is represented by a context-free network grammar. Using the structure of the network grammar our technique constructs a process invariant that simulates all the state-transition systems in the family. A novel idea introduced in this article is the use of regidar languages to express state properties. We have implemented our techniques and verified two nontrivial examples.,B.6.2 [Logics Design]: Reliability and Testing; D.2.4 [Software Engineering]: Program Verification; F.3.1 [Logics and Meanings of Programs]: Specifying and Verifying and Reasoning about Programs; Theory; Verification,Computer simulation; Formal languages; Formal logic; State transition systems; Temporal logic; Context free grammars
An Approach for Exploring Code-Improving Transformations,1997,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031270677&doi=10.1145%2f267959.267960&partnerID=40&md5=edf5773dbf4b24858ab9f78bd2ef8cca,"Although code transformations are routinely applied to improve the performance of programs for both scalar and parallel machines, the properties of code-improving transformations are n not well understood. In this article we present a framework that enables the explorations, both analytically and experimentally, of properties of code-improving transformations. The major component of the framework is a specification language, Gospel, for expressing the conditions needed to safely apply a transformation and the actions required to change the code to implement the transformation. The framework includes a technique that facilitates an analytical investigation of code-improving transformations using the Gospel specifications. It also contains a tool Genesis, that automatically produces a transformer that implements the transformations specified in Gospel. We demonstrate the usefulness of the framework by exploring the enabling and disabling properties of transformations. We first present analytical result on the enabling and disabling properties of a set of code transformations, including both traditional and parallelizing transformations, and then describe experimental results showing the types of transformations and the enabling and disabling interactions actually found in a set of programs.",Algorithms; Automatic generation of optimizers; Code-improving tansformations; Enabling and disabling of optimizations; Experimentation; Parallelizing transformations; Performance; Specification of program optimizations,Algorithms; Codes (symbols); Computational linguistics; Computer aided software engineering; Computer programming languages; Parallel processing systems; Program processors; Code generation; Code improving transformations; Parallelizing transformations; Semantics; Syntax; Program compilers
Commutativity Analysis: A New Analysis Technique for Parallelizing Compilers,1997,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031274872&doi=10.1145%2f267959.269969&partnerID=40&md5=06286eeba2d0b84c462d78afc4e5ed86,"This article presents a new analysis technique, commutativity analysis, for automatically parallelizing computations that manipulate dynamic, pointer-based data structures. Commutativity analysis views the computation as composed of operations on objects. It then analyzes the program at this granularity to discover when operations commute (i.e., generate the same final result regardless of the order in which they execute). If all of the operations required to perform a given computation commute, the compiler can automatically generate parallel code. We have implemented a prototype compilation system that uses commutativity analysis as its primary analysis technique We have used this system to automatically parallelize three complete scientific computations: the Barnes-Hut N-body solver, the Water liquid simulation code, and the String seismic simulation code. This article presents performance results for the generated parallel code running on the Stanford DASH machine. These results provide encouraging evidence that commutativity analysis can serve as the basis for a successful parallelizing compiler.",Compilers; Parallel computing; Parallelizing compilers,Codes (symbols); Computer operating procedures; Data structures; Program compilers; Barnes-Hut N body solver; Commutativity analysis; String seismic simulation code; Water liquid simulation code; Parallel processing systems
Mobile Objects in Distributed Oz,1997,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031220218&doi=10.1145%2f265943.265972&partnerID=40&md5=0e7c9eea310402d89f89aae6c90b4df7,"Some of the most difficult questions to answer when designing a distributed application are related to mobility: what information to transfer between sites and when and how to transfer it. Network-transparent distribution, the property that a program's behavior is independent of how it is partitioned among sites, does not directly address these questions. Therefore we propose to extend all language entities with a network behavior that enables efficient distributed programming by giving the programmer a simple and predictable control over network communication patterns. In particular, we show how to give objects an arbitrary mobility behavior that is independent of the object's definition. In this way, the syntax and semantics of objects are the same regardless of whether they are used as stationary servers, mobile agents, or simply as caches. These ideas have been implemented in Distributed Oz, a concurrent object-oriented language that is state aware and has dataflow synchronization. We prove that the implementation of objects in Distributed Oz is network transparent. To satisfy the predictability condition, the implementation avoids forwarding chains through intermediate sites. The implementation is an extension to the publicly available DFKI Oz 2.0 system.","D.1.3 [Programming Techniques]: Concurrent Programming - distributed programming; D.3.2 [Programming Languages]: Language Classifications - concurrent, distributed, and parallel languages; Data-flow languages; Object-oriented languages",Computational linguistics; Computer networks; Computer systems programming; Distributed computer systems; Flowcharting; Multiprocessing programs; Distributed Oz programming language; Distributed programming; Object oriented programming
Combinatory Formulations of Concurrent Languages,1997,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031275167&doi=10.1145%2f267959.269967&partnerID=40&md5=db9212f91bcb861d1f6d75e80fa6f945,"We design a sytem with six Basic Combinators and prove that it is powerful enough to embed the full asynchronous π-calculus, including replication. Our theory for constructing Combinatory Versions of concurrent languages is based on a method, used by Quine and Bernays, for the general elimination of variables in linguistic formalism. Our combinators are designed to eliminate the requirement of names that are bound by an input prefix. They also eliminate the need for input prefix, output prefix, and the accompanying mechanism of substitution. We define a notion of bisimulation for the combinatory version and show that the combinatory version preserves the semantics of the original calculus. One of the distinctive features of the approach is that it can be used to rework several process algebras in order to derive equivalent combinatory versions.",Functional completeness; Languages; Quine-Bernays combinators; Theory,Algebra; Combinatorial mathematics; Concurrency control; Differentiation (calculus); Formal logic; Parallel processing systems; Systems analysis; Basic combinators; Combinatory formulations; Combinatory versions; Concurrent languages; Input prefix; Quine-Bernays combinators; Formal languages
"A Note on ""on the Conversion of Indirect to Direct Recursion""",1997,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031269638&doi=10.1145%2f267959.269973&partnerID=40&md5=1e887bf80368d5ffec9371704b967cb4,"In the article, ""On the Conversion of Indirect to Direct Recursion"" (ACM Lett. Program. Lang. 2, 1-4. pp. 151-164), a method was introduced to convert indirect to direct recursion. It was claimed that for any call graph, there is a mutual-recursion elimination sequence if and only if no strongly connected component contains two node-disjoint circuits. We first give a counterexample and then provide a correction.",Call graphs; Inline substitution; Procedure inlining; Theory,Computer operating procedures; Computer programming languages; Program compilers; Program processors; Call graphs; Direct recursion; Inline substitution; Procedure inlining; Programming theory
A Reflection on Call-by-Value,1997,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031274017&doi=10.1145%2f267959.269968&partnerID=40&md5=1241de6243b248284a5f3ff92de0ee72,"One way to model a sound and complete translation from a source calculus into a target calculus is with an adjoint or a Galois connection. In the special case of a reflection, one also has that the target calculus is isomorphic to a subset of the source. We show that three widely studied translations form reflections. We use as our source language Moggi's computational lambda calculus, which is an extension of Plotkin's call-by-value calculus. We show that Plotkin's CPS translation, Moggi's monad translation, and Girard's translation to linear logic can all be regarded as reflections from this source language, and we put forward the computational lambda calculus as a model of call-by-value computation that improves on the traditional call-by-value calculus. Our work strengthens Plotkin's and Moggi's original results and improves on recent work based on equational correspondence, which uses equations rather than reductions.",Category theory; Compiling; Continuations; Galois connections; Languages; Theory,Computational linguistics; Computer programming; Computer programming languages; Differentiation (calculus); Formal languages; Program compilers; Program processors; Call by value; Category theory; Computational lambda calculus; Equational correspondence; Galois connections; Formal logic
Proving Concurrent Constraint Programs Correct,1997,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031220274&doi=10.1145%2f265943.265954&partnerID=40&md5=b51e8ad6f3fb85da4611fd70dc4af34f,"We introduce a simple compositional proof system for proving (partial) correctness of concurrentconstraint programs (CCP). The proof system is based on a denotational approximation of the strongest postcondition semantics of CCP programs. The proof system is proved to be correct for full CCP and complete for the class of programs in which the denotational semantics characterizes exactly the strongest postcondition. This class includes the so-called confluent CCP, a special case of which is constraint logic programming with dynamic scheduling.",D.1.3 [Programming Techniques]: Concurrent Programming; D.3.1 [Programming Languages]: Formal Definitions and Theory - semantics; F.3.1 [Logics and Meanings of Programs]: Specifying and Verifying and Reasoning about Programs - logics of programs,Computational linguistics; Constraint theory; Logic programming; Theorem proving; Concurrent constraint programs (CCP); Multiprocessing programs
Interprocedural Control Flow Analysis of First-Order Programs with Tail-Call Optimization,1997,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031191619&doi=10.1145%2f262004.262006&partnerID=40&md5=f1fef444335e62b59bd4adc54b504ff2,"Knowledge of low-level control flow is essential for many compiler optimizations. In systems with tail-call optimization, the determination of interprocedural control flow is complicated by the fact that because of tail-call optimization, control flow at procedure returns is not readily evident from the call graph of the program. This article shows how interprocedural control-flow analysis of first-order programs can be carried out using well-known concepts from parsing theory. In particular, we show that context-insensitive (or zeroth-order) control-flow analysis corresponds to the notion of FOLLOW sets in context-free grammars, while context-sensitive (or first-order) control-flow analysis corresponds to the notion of LR(1) items. The control-flow information so obtained can be used to improve the precision of interprocedural dataflow analyses as well as to extend certain low-level code optimizations across procedure boundaries.",Algorithms; Control-flow analysis; D.3.4 [Programming Languages]; Languages; Processors - optimization; Theory,Computer programming languages; Context free grammars; Optimization; First order programs; Interprocedural control flow analysis; Parsing theory; Tail call optimization; Program processors
Making Graphs Reducible with Controlled Node Splitting,1997,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031274168&doi=10.1145%2f267959.269971&partnerID=40&md5=24b607eaaab2776aea5d180e67e81509,"Several compiler optimizations, such as data flow analysis, the exploitation of instruction-level parallelism (ILP), loop transformations, and memory disambiguation, require programs; with reducible control flow graphs. However, not all programs satisfy this property. A new method for transforming irreducible control flow graphs to reducible control flow graphs, called Controlled Node Splitting (CNS), is presented. CNS duplicates nodes of the control flow graph to obtain reducible control flow graphs. CNS results in a minimum number of splits and a minimum number of duplicates. Since the computation time to find the optimal split sequence is large a heuristic has been developed. The results of this heuristic are close to the optimum. Straightforward application of node splitting resulted in an average code size increase of 235% per procedure of our benchmark programs. CNS with the heuristic limits this increase to only 3%. The impact on the total code size of the complete programs is 13.6% for a straightforward application of node splitting. However, when CNS is used, with the heuristic the average growth in code size of a complete program dramatically reduces to 0.2%.",Algorithms; Compilation; Control flow graphs; Instruction-level parallelism; Irreducibility; Languages; Node splitting; Reducibility,Algorithms; Computer architecture; Computer programming languages; Data reduction; Data structures; Parallel processing systems; Program processors; Control flow graphs; Controlled node splitting; Data flow analysis; Instruction level parallelism; Loop transformations; Program compilers
Parallelizing Nonnumerical Code with Selective Scheduling and Software Pipelining,1997,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031274169&doi=10.1145%2f267959.269966&partnerID=40&md5=fbc1fd152cb740559e5aa263720ab957,"Instruction-level parallelism (ILP) in nonnumerical code is regarded as scarce and hard to exploit due to its irregularity. In this article, we introduce a new code-scheduling technique for irregular ILP called ""selective scheduling"" which can be used as a component for superscalar and VLIW compilers. Selective scheduling can compute a wide set of independent operations across all execution paths based on renaming and forward-substitution and can compute available operations across loop iterations if combined with software pipelining. This scheduling approach has better heuristics for determining the usefulness of moving one operation versus moving another and can successfully find useful code motions without resorting to branch profiling. The compile-time overhead of selective scheduling is low due to its incremental computation technique and its controlled code duplication. We parallelized the SPEC integer benchmarks and five AIX utilities without using branch probabilities. The experiments indicate that a fivefold speedup is achievable on realistic resources with a reasonable overhead in compilation time and code expansion and that a solid speedup increase is also obtainable on machines with fewer resources. These results improve previously known characteristics of irregular ILP.",Algorithms; Experimentation; Global instruction scheduling; Instruction-level parallelism; Languages; Software pipelining; Speculative code motion; Superscalar; VLIW,Algorithms; Codes (symbols); Computer architecture; Computer operating procedures; Computer programming languages; Pipeline processing systems; Program compilers; Program processors; Instruction level parallelism; Nonnumerical code; Selective scheduling; Software pipelining; Speculative code motion; Superscalar; Parallel processing systems
Utilizing Symmetry when Model-Checking under Fairness Assumptions: An Automata-Theoretic Approach,1997,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031191101&doi=10.1145%2f262004.262008&partnerID=40&md5=aae3381f9b42ad5f3d847be2b7de1860,"One useful technique for combating the state explosion problem is to exploit symmetry when performing temporal logic model checking. In previous work it is shown how, using some basic notions of group theory, symmetry may be exploited for the full range of correctness properties expressible in the very expressive temporal logic CTL*. Surprisingly, while fairness properties are readily expressible in CTL*, these methods are not powerful enough to admit any amelioration of state explosion, when fairness assumptions are involved. We show that it is nonetheless possible to handle fairness efficiently by trading some group theory for automata theory. Our automatatheoretic approach depends on detecting fair paths subtly encoded in a quotient structure whose arcs are annotated with permutations, by using a threaded structure that reflects coordinate shifts caused by the permutations.",Abstraction; Algorithms; Automata; Concurrent Programming; D.2.4 [Software Engineering]; D. 1.3 [Programming Techniques]; Model-checking; Program Verification; F.3.1 [Logics and Meanings of Programs]verification; Verification,Algorithms; Automata theory; Computer aided software engineering; State explosion problem; Temporal logic model checking; Computer programming
Disjunctive Program Analysis for Algebraic Data Types,1997,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031222533&doi=10.1145%2f265943.265966&partnerID=40&md5=69d04a0a827e82f8fce2f7c476b7864c,"We describe how binding-time, data-flow, and strictness analyses for languages with higher-order functions and algebraic data types can be obtained by instantiating a generic program logic and axiomatization of the properties analyzed for. A distinctive feature of the analyses is that disjunctions of program properties are represented exactly. This yields analyses of high precision and provides a logical characterization of abstract interpretations involving tensor products and uniform properties of recursive data structures. An effective method for proving properties of a program based on fixed-point iteration is obtained by grouping logically equivalent formulae of the same type into equivalence classes, obtaining a lattice of properties of that type, and then denning an abstract interpretation over these lattices. We demonstrate this in the case of strictness analysis by proving that the strictness abstract interpretation of a program is the equivalence class containing the strongest property provable of the program in the strictness logic.",D.2.7 [Software Engineering]: Program Verification - assertion checkers; D.3.1 [Programming Languages]: Formal Definitions and Theory - semantics; D.3.2 [Programming Languages]: Language Classifications - applicative languages,Data structures; Equivalence classes; Iterative methods; Logic programming; Recursive functions; Tensors; Theorem proving; Binding time; Lambda calculus; Strictness abstract interpretation; Strictness analysis; Flowcharting
Toward a Complete Transformational Toolkit for Compilers,1997,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031223628&doi=10.1145%2f265943.265944&partnerID=40&md5=0efbc3400d2144e22172d3e5e9058d8c,"PIM is an equational logic designed to function as a ""transformational toolkit"" for compilers and other programming tools that analyze and manipulate imperative languages. It has been applied to such problems as program slicing, symbolic evaluation, conditional constant propagation, and dependence analysis. PIM consists of the untyped lambda calculus extended with an algebraic data type that characterizes the behavior of lazy stores and generalized conditionals. A graph form of PIM terms is by design closely related to several intermediate representations commonly used in optimizing compilers. In this article, we show that PIM's core algebraic component, PIMt, possesses a complete equational axiomatization (under the assumption of certain reasonable restrictions on term formation). This has the practical consequence of guaranteeing that every semantics-preserving transformation on a program representable in PIMt can be derived by application of PIMt rules. We systematically derive the complete PIMt logic as the culmination of a sequence of increasingly powerful equational systems starting from a straightforward ""interpreter"" for closed PIMt terms. This work is an intermediate step in a larger program to develop a set of well-founded tools for manipulation of imperative programs by compilers and other systems that perform program analysis.",,Algebra; Computational linguistics; Data structures; Formal logic; Graph theory; Logic programming; Mathematical transformations; Program interpreters; Equational axiomatization; Program compilers
Nesting of Reducible and Irreducible Loops,1997,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031186224&doi=10.1145%2f262004.262005&partnerID=40&md5=a6d9ad43664ad5278078b1425378e85d,"Recognizing and transforming loops are essential steps in any attempt to improve the running. time of a program. Aggressive restructuring techniques have been developed for single-entry (reducible) loops, but restructures and the dataflow and dependence analysis they rely on often give up in the presence of multientry (irreducible) loops. Thus one irreducible loop can prevent the improvement of all loops in a procedure. This article gives an algorithm to build a loop nesting tree for a procedure with arbitrary control flow. The algorithm uses definitions of reducible and irreducible loops which allow either kind of loop to be nested in the other. The tree construction algorithm, an extension of Tarjan's algorithm for testing reducibility, runs in almost linear time. In the presence of irreducible loops, the loop nesting tree can depend on the depth-first spanning tree used to build it. In particular, the header node representing a reducible loop in one version of the loop nesting tree can be the representative of an irreducible loop in another. We give a normalization method that maximizes the set of reducible loops discovered, independent of the depth-first spanning tree used. The normalization requires the insertion of at most one node and one edge per reducible loop.",Algorithms; D.3.4 [Programming Languages]; Languages; Processors - compilers; Reducible loops; Strongly-connected regions,Algorithms; Program compilers; Trees (mathematics); Loop nesting trees; Computer programming languages
Formally Based Profiling for Higher-Order Functional Languages,1997,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031099099&doi=10.1145%2f244795.244802&partnerID=40&md5=aa3bca908d94c07472173112691feea8,"We present the first source-level profiler for a compiled, nonstrict, higher-order, purely functional language capable of measuring time as well as space usage. Our profiler is implemented in a production-quality optimizing compiler for Haskell and can successfully profile large applications. A unique feature of our approach is that we give a formal specification of the attribution of execution costs to cost centers. This specification enables us to discuss our design decisions in a precise framework, prove properties about the attribution of costs, and examine the effects of different program transformations on the attribution of costs. Since it is not obvious how to map this specification onto a particular implementation, we also present an implementation-oriented operational semantics, and prove it equivalent to the specification.",D.2.5 [Software Engineering]: Testing and Debugging - debugging aids; D.3.2 [Programming Languages]: Language Classifications - applicative languages; D.3.4 [Programming Languages]: Processors - compilers; Optimization,Computational linguistics; Costs; High level languages; Optimization; Program debugging; Program transformations; Source level profiler; Program compilers
Kleene Algebra with Tests,1997,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031141317&doi=10.1145%2f256167.256195&partnerID=40&md5=6a04a4f2f0432661198f8aa1753a5804,"We introduce Kleene algebra with tests, an equational system for manipulating programs. We give a purely equational proof, using Kleene algebra with tests and commutativity conditions, of the following classical result: every while program can be simulated by a while program with at most one while loop. The proof illustrates the use of Kleene algebra with tests and commutativity conditions in program equivalence proofs.",D.2.2 [Software Engineering]: Tools and Techniques -structured programming; D.2.4 [Software Engineering]: Program Verification - correctness proofs; D.3.3 [Software Engineering]: Language Constructs and Features - control structures,Algebra; Computer simulation; Equivalence classes; Formal logic; Software engineering; Theorem proving; Kleene algebra; Programming theory
Incremental Computation of Dominator Trees,1997,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031102751&doi=10.1145%2f244795.244799&partnerID=40&md5=294dc4c56665f4c651892fcd27dde29e,"In this article, we present a new algorithm for incrementally maintaining the dominator tree of an arbitrary flowgraph. Previous work most relevant to this article includes only the Carroll-Ryder algorithm and the Ramalingam-Reps algorithm. Both these methods are restricted to reducible flowgraphs. By contrast, our approach can handle irreducible as well as reducible flowgraphs. For the case where an edge is inserted, our incremental algorithm is also faster than previous incremental algorithms in the worst case. For the deletion case, our algorithm has a quadratic time complexity in the worst case.",Algorithms; D.3.4 [Programming Languages]: Processors - compilers; DJ graphs; Dominance frontiers; Dominator trees; E.1 [Data]: Data Structures - graphs; Incremental compilation; Languages; Optimization; Trees,Algorithms; Computational complexity; Data structures; Trees (mathematics); Caroll Ryder algorithm; Dominator trees; Flowgraphs; Ramalingam Reps algorithm; Program compilers
Abstract Interpretation of Reactive Systems,1997,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031102750&doi=10.1145%2f244795.244800&partnerID=40&md5=86db9c009415b6e9c49719919f7670e7,"The advent of ever more complex reactive systems in increasingly critical areas calls for the development of automated verification techniques. Model checking is one such technique, which has proven quite successful. However, the state-explosion problem remains a major stumbling block. Recent experience indicates that solutions are to be found in the application of techniques for property-preserving abstraction and successive approximation of models. Most such applications have so far been based solely on the property-preserving characteristics of simulation relations. A major drawback of all these results is that they do not offer a satisfactory formalization of the notion of precision of abstractions. The theory of Abstract Interpretation offers a framework for the definition and justification of property-preserving abstractions. Furthermore, it provides a method for the effective computation of abstract models directly from the text of a program, thereby avoiding the need for intermediate storage of a full-blown model. Finally, it formalizes the notion of optimality, while allowing to trade precision for speed by computing suboptimal approximations. For a long time, applications of Abstract Interpretation have mainly focused on the analysis of universal safety properties, i.e., properties that hold in all states along every possible execution path. In this article, we extend Abstract Interpretation to the analysis of both existential and universal reactive properties, as expressible in the modal μ-calculus. It is shown how abstract models may be constructed by symbolic execution of programs. A notion of approximation between abstract models is defined while conditions are given under which optimal models can be constructed. Examples are given to illustrate this. We indicate conditions under which also falsehood of formulae is preserved. Finally, we compare our approach to those based on simulation relations.",D.2.4 [Software Engineering]: Program Verification -validation; F.3.1 [Logics and Meanings of Programs]: Specifying and Verifying and Reasoning about Programs - mechanical verification; Logics of programs; Reliability; Theory; Verification,Approximation theory; Formal logic; Logic programming; Abstract interpretation; Reactive systems; Program diagnostics
Precise Flow-Insensitive May-Alias Analysis is NP-Hard,1997,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030777732&doi=10.1145%2f239912.239913&partnerID=40&md5=20c96866f4dd9c706203ec7177e7f15b,"Determining aliases is one of the fundamental static analysis problems, in part because the precision with which this problem is solved can affect the precision of other analyses such as live variables, available expressions, and constant propagation. Previous work has investigated the complexity of flow-sensitive alias analysis. In this article we show that precise flow-insensitive may-alias analysis is NP-hard given arbitrary levels of pointers and arbitrary pointer dereferencing.",D.3.4 [Programming Languages]: Processors - compilers; optimization; F.4.1 [Mathematical Logic and Formal Languages]: Mathematical logic - computability theory; F.4.3 [Mathematical Logic and Formal Languages]: Formal Languages - decision problems,Computation theory; Computer programming languages; Formal languages; Formal logic; Optimization; Program compilers; Program processors; Programming theory; Available expressions; Constant propagation; Live variables; Precise flow insensitive may alias analysis; Static analysis; Computational complexity
A Refinement Calculus for the Synthesis of Verified Hardware Descriptions in VHDL,1997,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031188429&doi=10.1145%2f262004.262007&partnerID=40&md5=37f792fc1557c90a60e60f3e584f12b9,"A formal refinement calculus targeted at system-level descriptions in the IEEE standard hardware description language VHDL is described here. Refinement can be used to develop hardware description code that is ""correct by construction."" The calculus is closely related to a Hoare-style programming logic for VHDL and real-time systems in general. That logic and a semantics for a core subset of VHDL are described. The programming logic and the associated refinement calculus are shown to be complete. This means that if there is a code that can be shown to implement a given specification, then it will be derivable from the specification via the calculus.",B.1.2 [Hardware]; Control structures and microprogramming - formal models; B.1.4 [Hardware]; Control structures and Microprogramming - languages and compilers; D.3.3 [Programming Languages]; Design; Language semantics; Languages; Theory; Verification,Computational linguistics; Digital circuits; Denotational semantics; Formal verification; Refinement calculus; Computer hardware description languages
A Practical Soft Type System for Scheme,1997,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030777956&doi=10.1145%2f239912.239917&partnerID=40&md5=ae2c507a52b3a13c793ad3a28cdfecf9,"A soft type system infers types for the procedures and data structures of dynamically typed programs. Like conventional static types, soft types express program invariants and thereby provide valuable information for program optimization and debugging. A soft type checker uses the types inferred by a soft type system to eliminate run-time checks that are provably unnecessary; any remaining run-time checks are flagged as potential program errors. Soft Scheme is a practical soft type checker for R4RS Scheme. Its underlying type system generalizes conventional Hindley-Milner type inference by incorporating recursive types and a limited form of union type. Soft Scheme accommodates all of R4RS Scheme including uncurried procedures of fixed and variable arity, assignment, and continuations.",D.3.2 [Programming Languages]: Language Classifications - applicative languages; D.3.4 [Programming Languages]: Processors - optimization; F.3.3 [Logics and Meanings of Programs]: Studies of Program Constructs - type structure; Performance; Reliability,Computer programming languages; Errors; Optimization; Program debugging; Program processors; Checker; Program errors; Runtime checks; Soft type system; Data structures
Slicing Real-Time Programs for Enhanced Schedulability,1997,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031141014&doi=10.1145%2f256167.256394&partnerID=40&md5=fde618eec4eef5c4c9e6cfbe0d36960a,"In this article we present a compiler-based technique to help develop correct real-time systems. The domain we consider is that of multiprogrammed real-time applications, in which periodic tasks control physical systems via interacting with external sensors and actuators. While a system is up and running, these operations must be performed as specified - otherwise the system may fail. Correctness depends not only on each program individually, but also on the time-multiplexed behavior of all of the programs running together. Errors due to overloaded resources are exposed very late in a development process, and often at runtime. They are usually remedied by human-intensive activities such as instrumentation, measurement, code tuning and redesign. We describe a static alternative to this process, which relies on well-accepted technologies from optimizing compilers and fixed-priority scheduling. Specifically, when a set of tasks are found to be overloaded, a scheduling analyzer determines candidate tasks to be transformed via program slicing. The slicing engine decomposes each of the selected tasks into two fragments: one that is ""time critical"" and the other ""unobservable."" The unobservable part is then spliced to the end of the time-critical code, with the external semantics being maintained. The benefit is that the scheduler may postpone the unobservable code beyond its original deadline, which can enhance overall schedulability. While the optimization is completely local, the improvement is realized globally, for the entire task set.",C.3 [Computer Systems Organization]: Special-Purpose and Application-Based Systems - real time systems; D.3.4 [Programming Languages]: Processors - compilers; optimization; Design; J.7 [Computer Applications]: Computers in Other Systems - real time,Codes (symbols); Error analysis; Multiprogramming; Optimization; Program compilers; Resource allocation; Program slicing; Static priority scheduling; Real time systems
On the Use of Regular Expressions for Searching Text,1997,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031140972&doi=10.1145%2f256167.256174&partnerID=40&md5=71cb76fa93bdf725acb175f40408734b,"The use of regular expressions for text search is widely known and well understood. It is then surprising that the standard techniques and tools prove to be of limited use for searching structured text formatted with SGML or similar markup languages. Our experience with structured text search has caused us to reexamine the current practice. The generally accepted rule of ""leftmost longest match"" is an unfortunate choice and is at the root of the difficulties. We instead propose a rule which is semantically cleaner. This rule is generally applicable to a variety of text search applications, including source code analysis, and has interesting properties in its own right. We have written a publicly available search tool implementing the theory in the article, which has proved valuable in a variety of circumstances.",Algorithms; D.3.2 [Programming Languages]: Language Classifications - specialized application languages; Regular expressions; Regular languages; SGML,Algorithms; Computer programming languages; Computer systems programming; Knowledge based systems; Regular expressions; Similar markup language (SGML); Source code analysis; Text search applications; Formal languages
Implementing Signatures for C++,1997,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030834564&doi=10.1145%2f239912.239922&partnerID=40&md5=2650a0ae4555dcbfbbd18ea3c712eb86,"We outline the design and detail the implementation of a language extension for abstracting types and for decoupling subtyping and inheritance in C++. This extension gives the user more of the flexibility of dynamic typing while retaining the efficiency and security of static typing. After a brief discussion of syntax and semantics of this language extension and examples of its use, we present and analyze three different implementation techniques: a preprocessor to a C++ compiler, an implementation in the front end of a C++ compiler, and a low-level implementation with back-end support. We follow with an analysis of the performance of the three implementation techniques and show that our extension actually allows subtype polymorphism to be implemented more efficiently than with virtual functions. We conclude with a discussion of the lessons we learned for future programming language design.",D.1.5 [Programming Techniques]: Object-Oriented Programming; D.2.2 [Software Engineering]: Tools and Techniques - modules and interfaces; D.3.3 [Programming Languages]: Language Constructs and Features - abstract data types,Design; Interfaces (computer); Object oriented programming; Program compilers; Program processors; Abstract data types; Dynamic typing; Polymorphism; Semantics; Static typing; Subtyping; Syntax; C (programming language)
Formal Derivation of Efficient Parallel Programs by Construction of List Homomorphisms,1997,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031144375&doi=10.1145%2f256167.256201&partnerID=40&md5=0c9e6a94f781dbc67f498c109726e949,"It has been attracting much attention to make use of list homomorphisms in parallel programming because they ideally suit the divide-and-conquer parallel paradigm. However, they have been usually treated rather informally and ad hoc in the development of efficient parallel programs. What is worse is that some interesting functions, e.g., the maximum segment sum problem, are basically not list homomorphisms. In this article, we propose a systematic and formal way for the construction of a list homomorphism for a given problem so that an efficient parallel program is derived. We show, with several well-known but nontrivial problems, how a straightforward, and ""obviously"" correct, but quite inefficient solution to the problem can be successfully turned into a semantically equivalent ""almost list homomorphism."" The derivation is based on two transformations, namely tupling and fusion, which are defined according to the specific recursive structures of list homomorphisms.",D.1.1 [Programming Techniques]: Applicative (Functional) Programming; D.1.3 [Programming Techniques]: Concurrent Programming - parallel programming; F.3.3 [Logics and Meanings of Programs]: Studies of Program Constructs -program and recursion schemes,Data structures; Mathematical transformations; Problem solving; Recursive functions; List homomorphism; Parallel functional programming; Parallel processing systems
Complementation in Abstract Interpretation,1997,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030778191&doi=10.1145%2f239912.239914&partnerID=40&md5=b35e0c074a709c1e09a120d236861347,"Reduced product of abstract domains is a rather well-known operation for domain composition in abstract interpretation. In this article, we study its inverse operation, introducing a notion of domain complementation in abstract interpretation. Complementation provides a systematic way to design new abstract domains, and it allows to systematically decompose domains. Also, such an operation allows to simplify domain verification problems, and it yields space-saving representations for complex domains. We show that the complement exists in most cases, and we apply complementation to three well-known abstract domains, notably to Cousot and Cousot's interval domain for integer variable analysis, to Cousot and Cousot's domain for comportment analysis of functional languages, and to the domain Sharing for aliasing analysis of logic languages.",Abstract domain; Abstract interpretation; D.3.1 [Programming Languages]: Formal Definitions and Theory - semantics; F.3.1 [Logics and Meanings of Programs]: Specifying and Verifying and Reasoning about Programs - logics of programs; Languages; Theory,Analysis; Computer programming; Computer programming languages; Design; Programming theory; Abstract domain; Abstract interpretation; Aliasing analysis; Closure operator; Complementation; Integer variable analysis; Program analysis; Logic programming
Symbolic Model Checking for Event-Driven Real-Time Systems,1997,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031096502&doi=10.1145%2f244795.244803&partnerID=40&md5=0336b8c851ea3da1c4234f06d4c15760,"In this article, we consider symbolic model checking for event-driven real-time systems. We first propose a Synchronous Real-Time Event Logic (SREL) for capturing the formal semantics of synchronous, event-driven real-time systems. The concrete syntax of these systems is given in terms of a graphical programming language called Modechart, by Jahanian and Mok, which can be translated into SREL structures. We then present a symbolic model-checking algorithm for SREL. In particular, we give an efficient algorithm for constructing OBDDs (Ordered Binary Decision Diagrams) for linear constraints among integer variables. This is very important in a BDD-based symbolic model checker for real-time systems, since timing and event occurrence constraints are used very often in the specification of these systems. We have incorporated our construction algorithm into the SMV v2.3 from Carnegie-Mellon University and have been able to achieve one to two orders of magnitude in speedup and space saving when compared to the implementation of timing and event-counting functions by integer arithmetics provided by SMV.",F.3.1 [Logics and Meanings of Programs]: Specifying and Verifying and Reasoning about Programs - mechanical verification; F.4.1 [Mathematical Logic and Formal Languages]: Mathematical Logic-computational logic; Languages; Specification techniques,Algorithms; Computational linguistics; Computer programming languages; Constraint theory; Data structures; Formal logic; Modechart programming language; Ordered binary decision diagrams (OBDD); Symbolic model checking; Synchronous real time event logic (SREL); Real time systems
Specifying Representations of Machine Instructions,1997,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031141881&doi=10.1145%2f256167.256225&partnerID=40&md5=c9f8442b502f6ea969d58d9963709d7b,"We present SLED, a Specification Language for Encoding and Decoding, which describes abstract, binary, and assembly-language representations of machine instructions. Guided by a SLED specification, the New Jersey Machine-Code Toolkit generates bit-manipulating code for use in applications that process machine code. Programmers can write such applications at an assembly-language level of abstraction, and the toolkit enables the applications to recognize and emit the binary representations used by the hardware. SLED is suitable for describing both CISC and RISC machines; we have specified representations of MIPS R3000, SPARC, Alpha, and Intel Pentium instructions, and toolkit users have written specifications for the Power PC and Motorola 68000. The article includes representative excerpts from our SPARC and Pentium specifications. SLED uses four elements; fields and tokens describe parts of instructions; patterns describe binary representations of instructions or groups of instructions; and constructors map between the abstract and binary levels. By combining the elements in different ways, SLED supports machine-independent implementations of machine-level concepts like conditional assembly, span-dependent instructions, relocatable addresses, object code, sections, and relocation. SLED specifications can be checked automatically for consistency with existing assemblers. The implementation of the toolkit is largely determined by our representations of patterns and constructors. We use a normal form that facilitates construction of encoders and decoders. The article describes the normal form and its use. The toolkit has been used to help build several applications. We have built a retargetable debugger and a retargetable, optimizing linker. Colleagues have built a dynamic code generator, a decompiler, and an execution-time analyzer. The toolkit generates efficient code; for example, the linker emits binary up to 15% faster than it emits assembly language, making it 1.7-2 times faster to produce an a.out directly than by using the assembler.",C.0 [Computer Systems Organization]: General - systems specification methodology; D.3.2 [Programming Languages]: Language Classifications -specialized application languages,Binary sequences; Codes (symbols); Decoding; Encoding (symbols); Program assemblers; Program compilers; Program debugging; Reduced instruction set computing; Machine instructions; Program linkers; Specifying language for encoding and decoding (SLED); Computer hardware description languages
Protocol Specifications and Component Adaptors,1997,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031102173&doi=10.1145%2f244795.244801&partnerID=40&md5=e98d295b78ec006bde8a9e424d659922,"In this article we examine the augmentation of application interfaces with enhanced specifications that include sequencing constraints called protocols. Protocols make explicit the relationship between messages (methods) supported by the application. These relationships are usually only given implicitly, either in the code or in textual comments. We define notions of interface compatibility based upon protocols and show how compatibility can be checked, discovering a class of errors that cannot be discovered via the type system alone. We then define software adaptors that can be used to bridge the difference between applications that have functionally compatible but type- and protocol-incompatible interfaces. We discuss what it means for an adaptor to be well formed. Leveraging the information provided by protocols, we show how adaptors can be automatically generated from a high-level description, called an interface mapping.",C.0 [Computer Systems Organization]: General - systems specification methodology; D.1.3 [Programming Techniques]: Concurrent Programming; D.2.1 [Software Engineering]: Requirements/Specifications,Computer operating systems; Computer software; Error detection; Interfaces (computer); Network protocols; Interface compatibility; Interface mapping; Software component adaptors; Computer hardware description languages
Pure versus Impure Lisp,1997,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031100714&doi=10.1145%2f244795.244798&partnerID=40&md5=8dcac0d26b8134b26fdcb39969b849ef,"The aspect of purity versus impurity that we address involves the absence versus presense of mutation: the use of primitives (RPLACA and RPLACD in Lisp, set-car! and set-car! in Scheme) that change the state of pairs without creating new pairs. It is well known that cyclic list structures can be created by impure Lisp, but not by pure Lisp. In this sense, impure Lisp is ""more powerful"" than pure Lisp. If the inputs and outputs are restricted to be sequences of atomic symbols, however, this difference in computability disappears. We show that if the temporal sequence of input and output operations must be maintained (that is, if computations must be ""on-line""), then a difference in complexity remains. We do this by comparing the power of pure and impure ""Lisp machines."" We show that what an impure Lisp machine does in n steps (executions of primitive operations), a pure Lisp machine can do in O(n log n) steps, and that in some cases Ω(n log n) steps are necessary.",D.3.3 [Programming Languages]: Language Constructs and Features - Lisp; F.3.3 [Logics and Meanings of Programs]: Studies of Program Constructs - program and recursion schemes; Languages; Online computation; Schematology; Theory,Computability and decidability; Computational complexity; Program diagnostics; Recursive functions; Online computation; LISP (programming language)
Optimal Control Dependence Computation and the Roman Chariots Problem,1997,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031147607&doi=10.1145%2f256167.256217&partnerID=40&md5=6fb285ac98f6a88c8230c5a48dd0f487,"The control dependence relation plays a fundamental role in program restructuring and optimization. The usual representation of this relation is the control dependence graph (CDG), but the size of the CDG can grow quadratically with the input program, even for structured programs. In this article, we introduce the augmented postdominator tree (APT), a data structure which can be constructed in space and time proportional to the size of the program and which supports enumeration of a number of useful control dependence sets in time proportional to their size. Therefore, APT provides an optimal representation of control dependence. Specifically, the APT data structure supports enumeration of the set cd(e), which is the set of statements control dependent on control-flow edge e, of the set conds(w), which is the set of edges on which statement w is dependent, and of the set cdequiv(w), which is the set of statements having the same control dependences as w. Technically, APT can be viewed as a factored representation of the CDG where queries are processed using an approach known as filtering search. Categories and Subject Descriptors: D.3.4 [Programming Languages]: Processors - compilers and optimization; I.1.2 [Algebraic Manipulation]: Algorithms - analysis of algorithms.",Algorithms; Compilers; Control dependence; D.3.4 [Programming Languages]: Processors - compilers and optimization; I.1.2 [Algebraic Manipulation]: Algorithms - analysis of algorithms; Languages; Program optimization; Program transformation; Theory,Data structures; Optimization; Parallel algorithms; Program compilers; Set theory; Structured programming; Trees (mathematics); Augmented postdominator tree (APT); Control dependence graph (CDG); Programming theory
Lightweight Closure Conversion,1997,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030784722&doi=10.1145%2f239912.239915&partnerID=40&md5=573053ecece27e2ce08627b9d30117bb,"We consider the problem of lightweight closure conversion, in which multiple procedure call protocols may coexist in the same code. A lightweight closure omits bindings for some of the free variables of the procedure that it represents. Flow analysis is used to match the protocol expected by each procedure and the protocol used at its possible call sites. We formulate the flow analysis as a deductive system that generates a labeled transition system and a set of constraints. We show that any solution to the constraints justifies the resulting transformation. Some of the techniques used are similar to those of abstract interpretation, but others appear to be novel.","D.3.3 [Programming Languages]: Language Constructs and Features - procedures, functions, and subroutines; D.3.4 [Programming Languages]: Processors - compilers; optimization",Codes (symbols); Logic programming; Network protocols; Optimization; Program processors; Subroutines; Abstract interpretation; Deductive system; Flow analysis; Lightweight closure conversion; Program compilers
Evidence-Based Static Branch Prediction Using Machine Learning,1997,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030784721&doi=10.1145%2f239912.239923&partnerID=40&md5=c5524e5509c82f725757f8b225712e3d,"Correctly predicting the direction that branches will take is increasingly important in today's wide-issue computer architectures. The name program-based branch prediction is given to static branch prediction techniques that base their prediction on a program's structure. In this article, we investigate a new approach to program-based branch prediction that uses a body of existing programs to predict the branch behavior in a new program. We call this approach to program-based branch prediction evidence-based static prediction, or ESP. The main idea of ESP is that the behavior of a corpus of programs can be used to infer the behavior of new programs. In this article, we use neural networks and decision trees to map static features associated with each branch to a prediction that the branch will be taken. ESP shows significant advantages over other prediction mechanisms. Specifically, it is a program-based technique; it is effective across a range of programming languages and programming styles; and it does not rely on the use of expert-defined heuristics. In this article, we describe the application of ESP to the problem of static branch prediction and compare our results to existing program-based branch predictors. We also investigate the applicability of ESP across computer architectures, programming languages, compilers, and run-time systems. We provide results showing how sensitive ESP is to the number and type of static features and programs included in the ESP training sets, and we compare the efficacy of static branch prediction for subroutine libraries. Averaging over a body of 43 C and Fortran programs, ESP branch prediction results in a miss rate of 20%, as compared with the 25% miss rate obtained using the best existing program-based heuristics.",C.4 [Computer Systems Organization]: Performance of Systems - measurement techniques; D.3.4 [Programming Languages]: Processors - compilers; optimization; I.2.6 [Artificial Intelligence]: Learning - connectionism and neural nets; parameter learning,Computer architecture; Computer programming languages; Decision tables; Neural networks; Optimization; Program compilers; Subroutines; Decision trees; Evidence based static branch prediction; Program optimization; Runtime systems; Learning systems
Symbolic Analysis for Parallelizing Compilers,1996,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030190872&doi=10.1145%2f233561.233568&partnerID=40&md5=f7ab78ead566344f7bae40fee5bef4e8,"The notion of dependence captures the most important properties of a program for efficient execution on parallel computers. The dependence structure of a program defines the necessary constraints of the order of execution of the program components and provides sufficient information for the exploitation of the available parallelism. Static discovery and management of the dependence structure of programs save a tremendous amount of execution time, and dynamic utilization of dependence information results in a significant performance gain on parallel computers. However, experiments with parallel computers indicate that existing multiprocessing environments are unable to deliver the desired performance over a wide range of real applications, mainly due to lack of precision of their dependence information. This calls for an effective compilation scheme capable of understanding the dependence structure of complicated application programs. This article describes a methodology for capturing and analyzing program properties that are essential in the effective detection and efficient exploitation of parallelism on parallel computers. Based on this methodology, a symbolic analysis framework is developed for the Parafrase-2 parallelizing compiler. This framework extends the scope of a variety of important program analysis problems and solves them in a unified way. The attained solution space of these problems is much larger than that handled by existing compiler technology. Such a powerful approach is required for the effective compilation of a large class of application programs.",C.1.2 [Processor Architectures]: Multiple Data Stream Architectures - array and vector processors; D.2.6 [Software Engineering]: Programming Environments; D.3.4 [Programming Languages]: Processors - compilers; Optimization; Parallel processors,Algorithms; Computer systems programming; Multiprocessing systems; Optimization; Parallel processing systems; Performance; Problem solving; Programming theory; Systems analysis; Dependence analysis; Experimentation; Parafrase 2 parallelizing compiler; Parallelization; Symbolic analysis; Program compilers
Using Dataflow Analysis Techniques to Reduce Ownership Overhead in Cache Coherence Protocols,1996,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030284364&doi=10.1145%2f236114.236116&partnerID=40&md5=406ff651e2f7432c1cecabb144ec925c,"In this article we explore the potential of classical dataflow analysis techniques in removing overhead in write-invalidate cache coherence protocols for shared-memory multiprocessors. We construct three compiler algorithms with varying degree of sophistication that detect loads followed by stores to the same address. Such loads are marked and constitute a hint to the cache to obtain an exclusive copy of the block so that the subsequent store does not introduce access penalties. The simplest of the three compiler algorithms analyzes the existence of load-store sequences within each basic block of code whereas the other two analyze load-store sequences across basic blocks at the intraprocedural level. The algorithms have been incorporated into an optimizing C compiler, and we have evaluated their efficiencies by compiling and executing seven parallel programs on a simulated multiprocessor. Our results show that the detection efficiency of the most aggressive algorithm is 96% or higher for four of the seven programs studied. We also compare the efficiency of these static algorithms with that of dynamic hardware-based algorithms that reduce ownership overhead. We find that the static analysis using classical dataflow analysis results in similar performance improvements as dynamic hardware-based approaches.","B.3.2 [Memory Structures]: Design styles - Cache memories; C.1.2 [Processor Architectures]: Multiprocessors - Multiple-inttruction-stream, multiple-data-stream processors (MIMD); D.3.4 [Programming Languages]: Processors - Compilers; optimization",Algorithms; Block codes; Data storage equipment; Multiprocessing programs; Optimization; Program processors; Cache coherence protocols; Dataflow analysis; Dynamic hardware; Intraprocedural level; Multiple data stream processors; Multiple instruction stream; Program compilers
Principles and Practice of Unification Factoring,1996,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030246077&doi=10.1145%2f232706.232722&partnerID=40&md5=c4909b2f98e77adcc02b2f86967cfa2d,"The efficiency of resolution-based logic programming languages, such as Prolog, depends critically on selecting and executing sets of applicable clause heads to resolve against subgoals. Traditional approaches to this problem have focused on using indexing to determine the smallest possible applicable set. Despite their usefulness, these approaches ignore the nondeterminism inherent in many programming languages to the extent that they do not attempt to optimize execution after the applicable set has been determined. Unification factoring seeks to rectify this omission by regarding the indexing and unification phases of clause resolution as a single process. This article formalizes that process through the construction of factoring automata. A polynomial-time algorithm is given for constructing optimal factoring automata that preserve the clause selection strategy of Prolog. More generally, when the clause selection strategy is not fixed, constructing such an optimal automaton is shown to be NP-complete, solving an open trie minimization problem. Unification factoring is implemented through a source code transformation that preserves the full semantics of Prolog. This transformation is specified in the article, and using it, several well-known programs show significant performance improvements across several different systems. A prototype of unification factoring is available by anonymous ftp.",D.3.4 [Programming Languages]: Processors - compilers; optimization; F.1.1 [Computation by Abstract Devices]: Models of Computation - automata,Algorithms; Automata theory; Computational complexity; Computational linguistics; Indexing (of information); Mathematical models; Optimization; Problem solving; Program compilers; Nonnumerical algorithms; Pattern matching; Problem complexity; Trie minimization; Unification factoring; PROLOG (programming language)
Probabilistic Predicate Transformers,1996,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030143516&doi=10.1145%2f229542.229547&partnerID=40&md5=598e1104056c7a8b87f9b1e9d6d66875,"Probabilistic predicates generalize standard predicates over a state space; with probabilistic predicate transformers one thus reasons about imperative programs in terms of probabilistic pre- and postconditions. Probabilistic healthiness conditions generalize the standard ones, characterizing ""real"" probabilistic programs, and are based on a connection with an underlying relational model for probabilistic execution; in both contexts demonic nondeterminism coexists with probabilistic choice. With the healthiness conditions, the associated weakest-precondition calculus seems suitable for exploring the rigorous derivation of small probabilistic programs.",D.2.4 [Program Verification]: Correctness Proofs; D.3.1 [Programming Languages]: Formal Definitions and Theory; F.1.2 [Modes of Computation]: Probabilistic Computation,Algorithms; Computational linguistics; Computational methods; Computer hardware description languages; Formal languages; Probability; Programming theory; Statistics; Galois connection; Nondeterminism; Predicate transformers; Probabilistic computation; Program derivation; Program reasoning; Program verification; Refinement; Weakest preconditions; Computer programming languages
Storage Assignment to Decrease Code Size,1996,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030149574&doi=10.1145%2f229542.229543&partnerID=40&md5=06782621bee7871e08bb9988d23a858e,"DSP architectures typically provide indirect addressing modes with autoincrement and decrement. In addition, indexing mode is generally not available, and there are usually few, if any, general-purpose registers. Hence, it is necessary to use address registers and perform address arithmetic to access automatic variables. Subsuming the address arithmetic into autoincrement and decrement modes improves the size of the generated code. In this article we present a formulation of the problem of optimal storage assignment such that explicit instructions for address arithmetic are minimized. We prove that for the case of a single address register the decision problem is NP-complete, even for a single basic block. We then generalize the problem to multiple address registers. For both cases heuristic algorithms are given, and experimental results are presented.",Algorithms; Code size; Compilation; D.3.4 [Programming Languages]: Processors - compilers; Experimentation; Optimization; Storage assignment,Algorithms; Computational complexity; Computer architecture; Computer programming languages; Digital arithmetic; Digital signal processing; Heuristic methods; Optimization; Program compilers; Shift registers; Autoincrement; Code size; Compilation; Decrement; Indirect addressing modes; Storage assignment; Digital storage
Polymorphic Typing of Variables and References,1996,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030146184&doi=10.1145%2f229542.229544&partnerID=40&md5=a24cb74b5f52f5535d2cd56cb7b6f0ca,"In this article we consider the polymorphic type checking of an imperative language. Our language contains variables, first-class references (pointers), and first-class functions. Variables, as in traditional imperative languages, are implicitly dereferenced, and their addresses (L-values) are not first-class values. Variables are easier to type check than references and, in many cases, lead to more general polymorphic types. We present a polymorphic type system for our language and prove that it is sound. Programs that use variables sometimes require weak types, as in Tofte's type system for Standard ML, but such weak types arise far less frequently with variables than with references.",Assignment; D.3.3 [Programming Languages]: Language constructs and features; F.3.3 [Logics and Meanings of Programs]: Studies of program constructs - type structure; Languages; References; Theory; Variables; Verification,Computer software; Formal languages; Logic programming; Programming theory; Assignment; Imperative language; Language construct; Language feature; Polymorphic typing; References; Studies of program constructs; Type structure; Variables; Verification; Computer programming languages
Indirect Distributed Garbage Collection: Handling Object Migration,1996,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030247992&doi=10.1145%2f232706.232711&partnerID=40&md5=6d50649048e7dffe1e8906a46680501e,"In new distributed systems, object mobility is usually allowed and is sometimes used by the underlying object manager system to benefit from object access locality. On the other hand, in-transit references to objects can exist at any moment in asynchronous distributed systems. In the presence of object mobility and in-transit references, many garbage collector (GC) algorithms fail to operate correctly. Others need to use the system's object finder to find the objects while performing their work. As a general principle, a GC should never interfere with object manager policies (such as forcing migration or fixing an object to a given processor). However, if the GC uses the object finder, it will change the access pattern of the system, and eventually it could foul the global allocation policy. In this article we propose a new GC family, Indirect Garbage Collectors, allowing to separate the problems of object management (placement, replication, and retrieval) from garbage collection. This property allows our algorithms to be implemented on top of almost any existent distributed object system, without having to use the object finder.",Algorithms; C.2.4 [Computer-Communication Networks]: Distributed Systems - network operating systems; D.1.3 [Programming Techniques]: Concurrent Programming; D.4.2 [Operating Systems]: Storage Management - distributed memories; Performance; Verification,Algorithms; Computer operating systems; Concurrent engineering; Data storage equipment; Performance; Distributed memories; Indirect distributed garbage collection; Network operating systems; Object finder; Object man; Object mobility; Verification; Distributed computer systems
Identifying Loops Using DJ Graphs,1996,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030286489&doi=10.1145%2f236114.236115&partnerID=40&md5=cd31bcd606f86f6bf0f79ce4dcf607b5,"Loop identification is a necessary step in loop transformations for high-performance architectures. One classical technique for detecting loops is Tarjan's interval-finding algorithm. The intervals identified by Tarjan's method are single-entry, strongly connected subgraphs that closely reflect a program's loop structure. We present a simple algorithm for identifying both reducible and irreducible loops using DJ graphs. Our method is a generalization of Tarjan's method, as it identifies nested intervals (or loops) even in the presence of irreducibility.",Algorithms; D.3.4 [Programming Languages]: Processors - Compilers; optimization; DJ graphs; Dominator trees; E.1 [Data]: Data Structures - Graphs; Irreducible flowgraphs; Languages; Reducible flowgraphs; Tarjan's interval; Trees,Algorithms; Computer architecture; Flowcharting; Graphic methods; Optimization; Program compilers; Program processors; DJ graphs; Dominator trees; Irreducible flowgraphs; Loop identification; Loops; Reducible flowgraphs; Tarjan interval; Data structures
Global Analysis of Constraint Logic Programs,1996,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030246828&doi=10.1145%2f232706.232734&partnerID=40&md5=e17f602cee01b176ccb99a16e172335a,"This article presents and illustrates a practical approach to the dataflow analysis of constraint logic programming languages using abstract interpretation. It is first argued that, from the framework point of view, it suffices to propose relatively simple extensions of traditional analysis methods which have already been proved useful and practical and for which efficient fixpoint algorithms exist. This is shown by proposing a simple extension of Bruynooghe's traditional framework which allows it to analyze constraint logic programs. Then, and using this generalized framework, two abstract domains and their required abstract functions are presented: the first abstract domain approximates definiteness information and the second one freeness. Finally, an approach for combining those domains is proposed. The two domains and their combination have been implemented and used in the analysis of CLP(script R sign) and Prolog-III applications. Results from this implementation showing its performance and accuracy are also presented.",D.1.6 [Programming Techniques]: Logic Programming; D.1.m [Programming Techniques]: Miscellaneous - constraint logic programming; D.3.2 [Programming Languages]: Language classifications - nonprocedural languages,Abstracting; Algorithms; Approximation theory; Constraint theory; Performance; Program compilers; PROLOG (programming language); Abstract interpretation; Constraint logic programming; Global program analysis; Nonprocedural languages; Program analysis; Logic programming
Compositional Parallel Programming Languages,1996,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030194286&doi=10.1145%2f233561.233565&partnerID=40&md5=694a2b6d5e2b94099cc33949d21a2985,"In task-parallel programs, diverse activities can take place concurrently, and communication and synchronization patterns are complex and not easily predictable. Previous work has identified compotitionality as an important design principle for task-parallel programs. In this article, we discuss alternative approaches to the realization of this principle, which holds that properties of program components should be preserved when those components are composed in parallel with other program components. We review two programming languages, Strand and Program Composition Notation, that support compositionality via a small number of simple concepts, namely, monotone operations on shared objects, a uniform addressing mechanism, and parallel composition. Both languages have been used extensively for large-scale application development, allowing us to provide an informed assessment of both their strengths and their weaknesses. We observe that while compositionality simplifies development of complex applications, the use of specialized languages hinders reuse of existing code and tools and the specification of domain decomposition strategies. This suggests an alternative approach based on small extensions to existing sequential languages. We conclude the article with a discussion of two languages that realize this strategy.","Compositionality; D.3.2 [Programming Languages]: Language Classifications - concurrent, distributed, and parallel languages; D.3.3 [Programming Languages]: Language Constructs and Features - concurrent programming structures; Languages",Computer systems programming; Concurrency control; Parallel processing systems; Synchronization; Compositionality; Monotone operations; Parallel composition; Parallel languages; Parallel programming; Program components; Program Composition Notation programming language; Strand programming language; Task parallel programs; Uniform addressing mechanism; Computer programming languages
On Subtyping and Matching,1996,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030190712&doi=10.1145%2f233561.233563&partnerID=40&md5=84045b8bb46ccc9a7e654a46ba0a171d,"A relation between recursive object types, called matching, has been proposed as a generalization of subtyping. Unlike subtyping, matching does not support subsumption, but it does support inheritance of binary methods. We argue that matching is a good idea, but that it should not be regarded as a form of F-bounded subtyping (as was originally intended). We show that a new interpretation of matching as higher-order subtyping has better properties. Matching turns out to be a third-order construction, possibly the only one to have been proposed for general use in programming.",D.1.5 [Programming Techniques]: Object-Oriented Programming; D.3.3 [Programming Languages]: Languages Constructs and Features; F-bounded; Languages; Matching; Subtyping; Type operators; Type theory,Computer programming languages; Data processing; Programming theory; Recursive functions; Binary methods; F bounded subtyping; Higher order subtyping; Matching; Recursive object types; Subtyping; Type operators; Type theory; Object oriented programming
A Natural Semantics for Eiffel Dynamic Binding,1996,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030289053&doi=10.1145%2f236114.236118&partnerID=40&md5=bc162400b4a7f733f0b9fa25139df291,"This article formally defines Eiffel dynamic binding in presence of renaming and redefinition. Message passing, inheritance, and polymorphism are expressed in an operational style using Natural Semantics. From the formal specification, we derive an algorithm to determine the appropriate version of a feature to apply to a given object. This algorithm, based only on the class hierarchy and not using any intermediate structure, gives a practical approach to the understanding of inheritance, renaming, and redefinition in Eiffel.",D.1.5 [Programming Techniques]: Object-Oriented Programming; D.2.1 [Software Engineering]: Requirements/Specifications - Languages; D.3.1 [Programming Languages]: Formal Definitions and Theory - Semantics,Algorithms; Computer programming languages; Programming theory; Software engineering; Eiffel dynamic binding; Inheritance; Message passing; Polymorphism; Redefinition; Renaming; Semantics; Object oriented programming
Eta-Expansion Does the Trick,1996,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030289450&doi=10.1145%2f236114.236119&partnerID=40&md5=56aec013d75f9a50a4a59c88d2255336,"Partial-evaluation folklore has it that massaging one's source programs can make them specialize better. In Jones, Gomard, and Sestoft's recent textbook, a whole chapter is dedicated to listing such ""binding-time improvements"": nonstandard use of continuation-passing style, eta-expansion, and a popular transformation called ""The Trick."" We provide a unified view of these binding-time improvements, from a typing perspective. Just as a proper treatment of product values in partial evaluation requires partially static values, a proper treatment of disjoint sums requires moving static contexts across dynamic case expressions. This requirement precisely accounts for the nonstandard use of continuation-passing style encountered in partial evaluation. Eta-expansion thus acts as a uniform binding-time coercion between values and contexts, be they of function type, product type, or disjoint-sum type. For the latter case, it enables ""The Trick."" In this article, we extend Gomard and Jones' partial evaluator for the λ-calculus, λ-Mix, with products and disjoint sums; we point out how eta-expansion for (finite) disjoint sums enables The Trick; we generalize our earlier work by identifying that eta-expansion can be obtained in the binding-time analysis simply by adding two coercion rules; and we specify and prove the correctness of our extension to λ-Mix.","D.1.1 [Programming Techniques]: Applicative (Functional) Programming; D.3.3 [Programming Languages]: Language Constructs and Features - Procedures, functions, and subroutines",Algorithms; Formal languages; Formal logic; Function evaluation; Program compilers; Program processors; Subroutines; Automatic programming; Binding time coercion; Binding time improvements; Compiler generators; Eta expansion; Functional constructs; Operational semantics; Partial evaluation; Program transformation; Translator writing systems; Computer programming languages
Parameter Passing and Control Stack Management in Prolog Implementation Revisited,1996,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030284443&doi=10.1145%2f236114.236120&partnerID=40&md5=ce3077a33da4b3dc43106bb059d7b85a,"Parameter passing and control stack management are two of the crucial issues in Prolog implementation. In the Warren Abstract Machine (WAM), the most widely used abstract machine for Prolog implementation, arguments are passed through argument registers, and the information associated with procedure calls is stored in possibly two frames. Although accessing registers is faster than accessing memory, this scheme requires the argument registers to be saved and restored for backtracking and makes it difficult to implement full tail recursion elimination. These disadvantages may far outweigh the advantage in emulator-based implementations because registers are actually simulated by using memory. In this article, we reconsider the two crucial issues and describe a new abstract machine called ATOAM (yet Another Tree-Oriented Abstract Machine). The ATOAM differs from the WAM mainly in that (1) arguments are passed directly into stack frames, (2) only one frame is used for each procedure call, and (3) procedures are translated into matching trees if possible, and clauses in each procedure are indexed on all input arguments. The above-mentioned inefficiencies of the WAM do not exist in the ATOAM because backtracking requires less bookkeeping operations, and tail recursion can be handled in most cases like a loop statement in procedural languages. An ATOAM-emulator-based Prolog system called B-Prolog has been implemented, which is available through anonymous ftp from ftp.kyutech.ac.jp (131.206.1.101) in the directory pub/Language/prolog. B-Prolog is comparable in performance with and can sometimes be significantly faster than emulated SICStus-Prolog. By measuring the numbers of memory and register references made in both systems, we found that passing arguments in stack is no worse than passing arguments in registers even if accessing memory is four times as expensive as accessing registers.",Abstract machine; D.1.6 [Programming Techniques]: Logic Programming; D.3.4 [Programming Languages]: Processors - Compilers; Experimentation; Languages; Prolog,Data storage equipment; Logic programming; Program compilers; Program processors; Another tree oriented abstract machine; Control stack management; Parameter passing; Warren abstract machine; PROLOG (programming language)
Iterated Register Coalescing,1996,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030143433&doi=10.1145%2f229542.229546&partnerID=40&md5=fe9c2a619f66ec706ad8653c47fc49a5,"An important function of any register allocator is to target registers so as to eliminate copy instructions. Graph-coloring register allocation is an elegant approach to this problem. If the source and destination of a move instruction do not interfere, then their nodes can be coalesced in the interference graph. Chaitin's coalescing heuristic could make a graph uncolorable (i.e., introduce spills); Briggs et al. demonstrated a conservative coalescing heuristic that preserves colorability. But Briggs's algorithm is too conservative and leaves too many move instructions in our programs. We show how to interleave coloring reductions with Briggs's coalescing heuristic, leading to an algorithm that is safe but much more aggressive.",Algorithms; Copy propagation; D.3.4 [Programming Languages]: Processors - Code generation; G.2 [Discrete Mathematics]: Graph Theory - Graph algorithms; Graph coloring; Languages; Optimization; Register allocation; Register coalescing,Algorithms; Computer programming languages; Graph theory; Heuristic methods; Optimization; Code generation; Copy propagation; Graph coloring; Register allocation; Register coalescing; Shift registers
Type Classes in Haskell,1996,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030103375&doi=10.1145%2f227699.227700&partnerID=40&md5=57dff12d33221d96eb3dd08b877fad15,"This article defines a set of type inference rules for resolving overloading introduced by type classes, as used in the functional programming language Haskell. Programs including type classes are transformed into ones which may be typed by standard Hindley-Milner inference rules. In contrast to other work on type classes, the rules presented here relate directly to Haskell programs. An innovative aspect of this work is the use of second-order lambda calculus to record type information in the transformed program.",D.1.1 [Programming Techniques]: Language Classifications - applicative languages; F.3.3 [Logics and Meanings of Programs]: Studies of Program Constructs - type structure; Functional programming; Haskell; Languages; Theory; Type classes; Types,Computer software; Logic programming; Functional programming language Haskell; Hindley-Milner inference rules; Second order lambda calculus; Transformed program; Type classes; Computer programming languages
Parallelism for Free: Efficient and Optimal Bitvector Analyses for Parallel Programs,1996,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030146262&doi=10.1145%2f229542.229545&partnerID=40&md5=cd66a73d9b46790b83fd1338b0d78ee5,"We consider parallel programs with shared memory and interleaving semantics, for which we show how to construct for unidirectional bitvector problems optimal analysis algorithms that are as efficient as their purely sequential counterparts and that can easily be implemented. Whereas the complexity result is rather obvious, our optimality result is a consequence of a new Kam/Ullman-style Coincidence Theorem. Thus using our method, the standard algorithms for sequential programs computing liveness, availability, very busyness, reaching definitions, definition-use chains, or the analyses for performing code motion, assignment motion, partial dead-code elimination or strength reduction, can straightforward be transferred to the parallel setting at almost no cost.","Algorithms; Assignment motion; Compilers; D.3.2 [Programming Languages]: Language classifications - Concurrent, distributed, and parallel languages; D.3.4 [Programming Techniques]: Processors - Code generation; Languages; Optimization; Performance",Algorithms; Computational linguistics; Data storage equipment; Optimization; Parallel processing systems; Program compilers; Theorem proving; Assignment motion; Bitvector problems; Code motion; Data flow analysis; Definition use chains; Interleaving semantics; Kam-Ullman style coincidence theorem; Parallelism; Partial dead code elimination; Shared memory; Computer programming languages
Improving Data Locality with Loop Transformations,1996,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030190854&doi=10.1145%2f233561.233564&partnerID=40&md5=058f6c74ef42fd14506d9fae372f0d3d,"In the past decade, processor speed has become significantly faster than memory speed. Small, fast cache memories are designed to overcome this discrepancy, but they are only effective when programs exhibit data locality. In this article, we present compiler optimizations to improve data locality based on a simple yet accurate cost model. The model computes both temporal and spatial reuse of cache lines to find desirable loop organizations. The cost model drives the application of compound transformations consisting of loop permutation, loop fusion, loop distribution, and loop reversal. We demonstrate that these program transformations are useful for optimizing many programs. To validate our optimization strategy, we implemented our algorithms and ran experiments on a large collection of scientific programs and kernels. Experiments illustrate that for kernels our model and algorithm can select and achieve the best loop structure for a nest. For over 30 complete applications, we executed the original and transformed versions and simulated cache hit rates. We collected statistics about the inherent characteristics of these programs and our ability to improve their data locality. To our knowledge, these studies are the first of such breadth and depth. We found performance improvements were difficult to achieve because benchmark programs typically have high hit rates even for small data caches; however, our optimizations significantly improved several programs.",Cache; Compiler optimization; D.3.4 [Programming Languages]: Processors - compilers; Data locality; Languages; Loop distribution; Loop fusion; Loop permutation; Loop reversal; Loop transformations; Microprocessors; Optimization; Performance; Simulation,Algorithms; Buffer storage; Computational methods; Computer operating systems; Computer simulation; Mathematical models; Microcomputers; Optimization; Performance; Program compilers; Compiler optimization; Cost model; Data locality; Kernels; Loop distribution; Loop fusion; Loop permutation; Loop reversal; Loop transformations; Scientific programs; Programming theory
Demand-Driven Register Allocation,1996,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030287935&doi=10.1145%2f236114.236117&partnerID=40&md5=595872b9479478fb16e194eba3e4bd8b,"A new global register allocation technique, demand-driven register allocation, is described. Demand-driven register allocation quantifies the costs and benefits of allocating variables to registers over live ranges so that high-quality allocations can be made. Local allocation is done first, and then global allocation is done iteratively beginning in the the most deeply nested loops. Because local allocation precedes global allocation, demand-driven allocation does not interfere with the use of well-known, high-quality local register allocation and instruction-scheduling techniques.",Algorithms; D.3.4 [Programming Languages]: Processors - Code generation;optimization; Languages; Optimizing compiler; Performance; Theory,Algorithms; Block codes; Optimization; Program compilers; Code generation; Demand driven register allocation; Global allocation; Local allocation; Program processors
Constrained Types and Their Expressiveness,1996,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030246571&doi=10.1145%2f232706.232715&partnerID=40&md5=1bfc0d63f565e371f63e398863028f6f,"A constrained type consists of both a standard type and a constraint set. Such types enable efficient type inference for object-oriented languages with polymorphism and subtyping, as demonstrated by Eifrig, Smith, and Trifonov. Until now, it has been unclear how expressive constrained types are. In this article we study constrained types without universal quantification. We prove that they accept the same programs as the type system of Amadio and Cardelli with subtyping and recursive types. This result gives a precise connection between constrained types and the standard notion of type.",Constraints; D.3.2 [Programming Languages]: Language Classifications - applicative languages; F.3.3 [Logics and Meanings of Programs]: Studies of Program Constructs - type Structure; Languages; Theory,Computer software; Constraint theory; Formal languages; Logic programming; Applicative languages; Language classifications; Program constructs; Type structure; Object oriented programming
Handling Floating-Point Exceptions in Numeric Programs,1996,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030100969&doi=10.1145%2f227699.227701&partnerID=40&md5=a00b2b996eda95bb05e94a1b3b8b24ac,"There are a number of schemes for handling arithmetic exceptions that can be used to improve the speed (or alternatively the reliability) of numeric code. Overflow and underflow are the most troublesome exceptions, and depending on the context in which the exception can occur, they may be addressed either: (1) through a ""brute force"" reevaluation with extended range, (2) by reevaluating using a technique known as scaling, (3) by substituting an infinity or zero, or (4) in the case of underflow, with gradual underflow. In the first two of these cases, the offending computation is simply reevaluated using a safer but slower method. The latter two cases are cheaper, more automated schemes that ideally are built in as options within the computer system. Other arithmetic exceptions can be handled with similar methods. These and some other techniques are examined with an eye toward determining the support programming languages and computer systems ought to provide for floating-point exception handling. It is argued that the cheapest short-term solution would be to give full support to most of the required (as opposed to recommended) special features of the IEC/IEEE Standard for Binary Floating-Point Arithmetic. An essential part of this support would include standardized access from high-level languages to the exception flags defined by the standard. Some possibilities outside the IEEE Standard are also considered, and a few thoughts on possible better-structured support within programming languages are discussed.",Algorithms; D.3.0 [Programming Languages]: General - standards; D.3.2 [Programming Languages]: Language Constructs and Features - control structures; Design; G.1.0 [Numerical Analysis]: General - computer arithmetic; Languages; Numerical algorithms,Algorithms; Computational complexity; Computer systems programming; Digital arithmetic; Numerical analysis; Floating point exceptions; Numeric code; Overflow exceptions; Underflow exceptions; High level languages
Total Correctness by Local Improvement in the Transformation of Functional Programs,1996,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030102464&doi=10.1145%2f227699.227716&partnerID=40&md5=b95a7cc6dcda73c3ced4cce3d706a7dc,"The goal of program transformation is to improve efficiency while preserving meaning. One of the best-known transformation techniques is Burstall and Darlington's unfold-fold method. Unfortunately the unfold-fold method itself guarantees neither improvement in efficiency nor total correctness. The correctness problem for unfold-fold is an instance of a strictly more general problem: transformation by locally equivalence-preserving steps does not necessarily preserve (global) equivalence. This article presents a condition for the total correctness of transformations on recursive programs, which, for the first time, deals with higher-order functional languages (both strict and nonstrict) including lazy data structures. The main technical result is an improvement theorem which says that if the local transformation steps are guided by certain optimization concerns (a fairly natural condition for a transformation), then correctness of the transformation follows. The improvement theorem makes essential use of a formalized improvement theory; as a rather pleasing corollary it also guarantees that the transformed program is a formal improvement over the original. The theorem has immediate practical consequences: it is a powerful tool for proving the correctness of existing transformation methods for higher-order functional programs, without having to ignore crucial factors such as memoization or folding, and it yields a simple syntactic method for guiding and constraining the unfold-fold method in the general case so that total correctness (and improvement) is always guaranteed.",D.1.1 [Programming Techniques]: Applicative (Functional) Programming; D.2.4 [Software Engineering]: Program Verification - correctness proofs; F.3.1 [Logics and Meanings of Programs]: Specifying and Verifying and Reasoning about Programs; Languages,Data structures; Equivalence classes; Program translators; Software engineering; Theorem proving; Higher order functional language; Program transformation; Unfold fold methods; Computer programming languages
Reconciling Responsiveness with Performance in Pure Object-Oriented Languages,1996,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030195171&doi=10.1145%2f233561.233562&partnerID=40&md5=2d109d98b7fb6d261d1ef48511de48c0,"Dynamically dispatched calls often limit the performance of object-oriented programs, since object-oriented programming encourages factoring code into small, reusable units, thereby increasing the frequency of these expensive operations. Frequent calls not only slow down execution with the dispatch overhead per se, but more importantly they hinder optimization by limiting the range and effectiveness of standard global optimizations. In particular, dynamically dispatched calls prevent standard interprocedural optimizations that depend on the availability of a static call graph. The SELF implementation described here offers two novel approaches to optimization. Type feedback speculatively inlines dynamically dispatched calls based on profile information that predicts likely receiver classes. Adaptive optimization reconciles optimizing compilation with interactive performance by incrementally optimizing only the frequently executed parts of a program. When combined, these two techniques result in a system that can execute programs significantly faster than previous systems while retaining much of the interactiveness of an interpreted system.",D.1.5 [Programming Languages]: Object-Oriented Programming - SELF; D.2.6 [Programming Languages]: Programming Environments - interactive programming environments; Exploratory programming environments,Feedback; Interactive computer systems; Object oriented programming; Optimization; Performance; Program compilers; Program interpreters; Rapid prototyping; Adaptive optimization; Dynamically dispatched calls; Interprocedural optimization; Object oriented languages; Pause clustering; Profile based optimization; Run time compilation; Type feedback; Computer programming languages
Conditional Attribute Grammars,1996,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0029706401&doi=10.1145%2f225540.225544&partnerID=40&md5=af63cb509e08118dfa63b3c878b3a4f9,"Attribute grammars are a useful formalism for the specification of computations on structured terms. The classical definition of attribute grammars, however, has no way of treating conditionals nonstrictly. Consequently, the natural way of expressing many otherwise well-behaved computations involves a circularity. This article presents conditional attribute grammars, an extension of attribute grammars that enables more precise analysis of conditionals. In conditional attribute grammars, attribute equations may have guards. Equations are active only when their guards are satisfied. The standard attribute grammar evaluation classes are definable for conditional attribute grammars, and the corresponding evaluation techniques can be easily adapted. However, determining membership in standard evaluation classes such as 1-SWEEP, OAG, and SNC is NP-hard.",Categories and Subject Descriptors: D.3.4 [Programming Languages]: Processors - code generation; compilers; translator writing systems and compiler generators; F.4.2 [Mathematical Logic and Formal Languages]: Grammars and Other Rewriting Systems,Computational complexity; Computer programming languages; Data structures; Function evaluation; Attribute grammars; Conditional attribute grammars; Computational grammars
An Incremental Algorithm for Satisfying Hierarchies of Multiway Dataflow Constraints,1996,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0029706399&doi=10.1145%2f225540.225543&partnerID=40&md5=7dc367296f7d7080eb65677725bf208b,"One-way dataflow constraints have gained popularity in many types of interactive systems because of their simplicity, efficiency, and manageability. Although it is widely acknowledged that multiway dataflow constraints could make it easier to specify certain relationships in these applications, concerns about their predictability and efficiency have impeded their acceptance. Constraint hierarchies have been developed to address the predictability problem, and incremental algorithms have been developed to address the efficiency problem. However, existing incremental algorithms for satisfying constraint hierarchies encounter two difficulties: (1) they are incapable of guaranteeing an acyclic solution if a constraint hierarchy has one or more cyclic solutions and (2) they require worst-case exponential time to satisfy systems of multioutput constraints. This article surmounts these difficulties by presenting an incremental algorithm called QuickPlan that satisfies in worst-case O(N2) time any hierarchy of multiway, multioutput dataflow constraints that has at least one acyclic solution, where N is the number of constraints. With benchmarks and real problems that can be solved efficiently using existing algorithms, its performance is competitive or superior. With benchmarks and real problems that cannot be solved using existing algorithms or that cannot be solved efficiently, QuickPlan finds solutions and does so efficiently, typically in O(N) time or less. QuickPlan is based on the strategy of propagation of degrees of freedom. The only restriction it imposes is that every constraint method must use all of the variables in the constraint as either an input or an output variable. This requirement is met in every constraint-based, interactive application that we have developed or seen.",D.2.2 [Software Engineering]: Tools and Techniques -user interfaces; D.2.6 [Software Engineering]: Programming Environments; I.1.2 [Computing Methodologies]: Algorithms - nonalgebraic algorithms,Algorithms; Computational linguistics; Computer software; Hierarchical systems; Interactive computer systems; Problem solving; Constraint hierarchies; Incremental algorithm; Multiway dataflow constraints; One way dataflow constraints; Programming language QuickPlan; Constraint theory
Iteration Abstraction in Sather,1996,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0029697571&doi=10.1145%2f225540.225541&partnerID=40&md5=cc762c96a4915fad2146c7b08fdcb9a9,"Sather extends the notion of an iterator in a powerful new way. We argue that iteration abstractions belong in class interfaces on an equal footing with routines. Sather iterators were derived from CLU iterators but are much more flexible and better suited for object-oriented programming. We retain the property that iterators are structured, i.e., strictly bound to a controlling structured statement. We motivate and describe the construct along with several simple examples. We compare it with iteration based on CLU iterators, cursors, riders, streams, series, generators, coroutines, blocks, closures, and lambda expressions. Finally, we describe experiences with iterators in the Sather compiler and libraries.",Coroutines; D.1.5 [Programming Techniques]: Object-Oriented Programming; D.3.3 [Programming Languages]: Language Constructs and Features - control structures; F.3.3 [Logics and Meanings of Programs]: Studies of Program Constructs - control primitives,Computer programming languages; Data structures; Interfaces (computer); Program compilers; User interfaces; Iteration abstraction; Programming language Sather; Object oriented programming
On the Optimality of Change Propagation for Incremental Evaluation of Hierarchical Attribute Grammars,1996,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0029721953&doi=10.1145%2f225540.225542&partnerID=40&md5=6b98fc1707de4cdb5d60c5ac77c4b6f7,"Several new attribute grammar dialects have recently been developed, all with the common goal of allowing large, complex language translators to be specified through a modular composition of smaller attribute grammars. We refer to the class of dialects as hierarchical attribute grammars. In this short article, we present a characterization of optimal incremental evaluation that indicates the unsuitability of change propagation as the basis of an optimal incremental evaluator for hierarchical attribute grammars. This result lends strong support to the use of incremental evaluators based on more applicative approaches to attribute evaluation, such as Carle and Pollock's evaluator based on caching of partially attributed subtrees, Pugh's evaluator based on function caching of semantic functions, and Swierstra and Vogt's evaluator based on function caching of visit sequences.",Algorithms; Attribute grammar; D 3.4 [Programming Languages]: Processors - translator writing systems and compiler generators; D.2.3 [Software Engineering]: Coding - program editors; D.2.6 [Software Engineering]: Programming Environments; Languages,Algorithms; Hierarchical systems; Optimization; Change propagation; Hierarchical attribute grammars; Optimal incremental evaluation; Computational grammars
Fully abstract semantics for a first-order functional language with logic variables,1991,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0026242264&partnerID=40&md5=a1132a92c96fc2ce4cf7dad05aeeae8f,"There is much interest in combining the functional and logic programming paradigms; in particular, there have been several proposals for adding logic variables to functional languages, since that permits incremental construction of data structures through constraint intersection. While it is straightforward to give an abstract semantics for functional languages and for logic languages, it has proved surprisingly difficult to give a proper semantic account of functional languages with logic variables. We solve this problem in this paper. First, we present a first-order functional language with logic variables and give its meaning using a structural operational semantics. Then, we give it a denotational semantics, using a novel technique involving closure operators on a Scott domain. Finally, we show that these two semantics correspond in the strongest possible way: we show that the detonational semantics is fully abstract with respect to the operational semantics. The techniques developed in this paper are quite general and can be used to give semantics to any constraint-based logic programming language. Our results can also be interpreted as a generalization of Kalm semantics for dataflow networks in which processes not only exchange messages, but have access to a shared global address space in which variables are bound through constraint intersection.",,Computer Programming - Logic Programming; Abstract Semantics; Denotational Semantics; Functional Languages; Logic Variables; Computer Programming Languages
Automatic generation and use of abstract structure operators,1991,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0026242636&doi=10.1145%2f115372.214523&partnerID=40&md5=701a1468853b442b91246c8789691f2e,"Abstract structures are those structures definable by parametric and recursive type equations. Manipulation of the instances of such structures is often expressed as recursive functions. These functions can be quite complex and tedious to write, especially for types needed to model complex objects found in many modern applications. We define a set of operators for computing over abstract structures that provide a clean interface with large functionality. These operations have many of the good properties found in the relational algebra such as abstraction, algebraic manipulation, and specification, but operate over a much larger class of values. Concrete definitions of these operators for specific types can be automatically generated as a by-product of type declaration and are thus made available to the user at no programming cost.",,Computer Programming - Theory; Computer Programming Languages; Abstract Structure Operators; Automatic Programming; Computer Metatheory
First-come-first-served mutual-exclusion algorithm with small communication variables,1991,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0026241959&partnerID=40&md5=7ffcae28704bbd25564907e7d5b5916a,We present an algorithm for the mutual-exclusion problem that satisfies the 'first-come-first-served' property and require only five shared bits per participant. The algorithm works in a model of concurrency that does not assume atomic operations.,,"Computer Programming - Algorithms; Concurrency; Mutual Exclusion; Nonatomic Operations; Computer Systems, Digital"
INCREMENTAL DATA-FLOW ANALYSIS ALGORITHMS.,1988,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0023825991&doi=10.1145%2f42192.42193&partnerID=40&md5=8c61d98dcedee789597df67a57dee788,"An incremental update algorithm modifies the solution of a problem that has been changed, rather than re-solving the entire problem. ACINCF and ACINCB are incremental update algorithms for forward and backward data-flow analysis, respectively, based on our equations model of Allen-Cocke interval analysis. In addition, we have studied their performance on a 'nontoy' structured programming language L. Given a set of localized program changes in a program written in L, we identify a priori the nodes in its flow graph whose corresponding data-flow equations may be affected by the changes. We characterize these possibly affected nodes by their corresponding program structures and their relation to the original change sites, and do so without actually performing the incremental updates. Our results can be refined to characterize the reduced equations possibly affected if structured loop exit mechanisms are used, either singly or together, thereby relating richness of programming-language usage to the ease of incremental updating.",,COMPUTER OPERATING SYSTEMS - Program Processors; COMPUTER PROGRAMMING - Program Debugging; COMPUTER PROGRAMMING LANGUAGES - Design; COMPLEXITY OF ALGORITHMS; DATA-FLOW ANALYSIS; ELIMINATION METHODS; COMPUTER SOFTWARE
NOTE ON THE DRINKING PHILOSOPHERS PROBLEM.,1988,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0023825394&partnerID=40&md5=297023596505e9341290cbb49202f42d,"A nonprobabilistic solution with symmetric rules exists to the drinking philosophers problem. The solution is fair, given an implementation which eventually executes any guarded command that is infinitely often enabled. We present modified versions of the solution that are fair under a weaker implementation: namely, one which eventually executes any guarded command that is continuously enabled. In order for the modified algorithms not to have increased delays in satisfying requests for resources, we find it necessary to either violate a nice read-only property of the existing solution or restrict the right of a philosopher to request resources.",,COMPUTER SYSTEMS PROGRAMMING - Multiprocessing; CONCURRENT PROGRAMMING; DINING PHILOSOPHERS PROBLEM; DRINKING PHILOSOPHERS PROBLEM; COMPUTER OPERATING SYSTEMS
CLARIFICATION OF 'FEEDING INPUTS ON DEMAND' IN EFFICIENT DEMAND-DRIVEN EVALUATION. PART 1.,1986,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0022593826&partnerID=40&md5=d807fbaa4a6cfeb084ca9ad643b7d904,"Part 1 discussed a scheme whereby a compiler could propagate demands through programs in a powerful stream language called L. A data-driven evaluation of the transformed program performed exactly the same computation as a demand-driven computation of the original program. In this clarification, the authors change the meaning of 'feed inputs on demand' to 'place a gate on each input data line and control it with the associated demand line'.",,COMPUTER PROGRAMMING LANGUAGES - Design; CONCURRENT PROGRAMMING; DATA FLOW LANGUAGES; FUNCTIONAL PROGRAMMING; COMPUTER PROGRAMMING
A Type Discipline for Message Passing Parallel Programs,2022,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146418248&doi=10.1145%2f3552519&partnerID=40&md5=a8c18d464254de9d04833ad059176ddd,"We present ParTypes, a type discipline for parallel programs. The model we have in mind comprises a fixed number of processes running in parallel and communicating via collective operations or point-to-point synchronous message exchanges. A type describes a protocol to be followed by each processes in a given program. We present the type theory, a core imperative programming language and its operational semantics, and prove that type checking is decidable (up to decidability of semantic entailment) and that well-typed programs do not deadlock and always terminate. The article is accompanied by a large number of examples drawn from the literature on parallel programming. © 2022 Copyright held by the owner/author(s).",dependent types; message passing computation; Parallel programs; termination,Computability and decidability; Formal languages; Parallel programming; Semantics; Collective operations; Dependent types; Fixed numbers; Message exchange; Message passing computation; Message-passing; Message-passing parallel programs; Parallel program; Running-in; Termination; Message passing
Two Parametricities Versus Three Universal Types,2022,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146434215&doi=10.1145%2f3539657&partnerID=40&md5=9974d02a11d02b38d550a7fe78fbb773,"The formal calculus System F models the essence of polymorphism and abstract data types, features that exist in many programming languages. The calculus' core property is parametricity: a theorem expressing the language's abstractions and validating important principles like information hiding and modularity. When System F is combined with features like recursive types, mutable state, continuations or exceptions, the formulation of parametricity needs to be adapted to follow suit, for example using techniques like step-indexing, Kripke world-indexing or biorthogonality. However, it is less clear how this formulation should change when System F is combined with untyped languages, gradual types, dynamic sealing and runtime type analysis (typecase) alongside type generation. Extensions of System F with these features have been proven to satisfy forms of parametricity (with Kripke worlds carrying semantic interpretations of types). However, the relative power of the modified formulations of parametricity with respect to others and the relative expressiveness of System F with and without these extensions are unknown. In this paper, we explain that the aforementioned different settings have a common characteristic: they do not enforce or preserve the lexical scope of System F's type variables. Formally, this results in the existence of a universal type (note: this is not the same as a universally-quantified type). We explain why standard parametricity is incompatible with such a type and how type worlds resolve this. Building on these insights, we answer two open conjectures from the literature, negatively, and we point out a deficiency in current proposals for combining System F with gradual types. © 2022 Association for Computing Machinery.",Fully abstract compilation; parametricity; sealing; System F; universal type,Differentiation (calculus); Indexing (of information); Semantics; Biorthogonality; Formal calculi; Fully abstract compilation; Information hiding; Parametricity; Property; Recursive types; Sealing; System F; Universal type; Abstract data types
Revisiting Iso-Recursive Subtyping,2022,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144353731&doi=10.1145%2f3549537&partnerID=40&md5=859288557f5419ea085a8b1961c89988,"The Amber rules are well-known and widely used for subtyping iso-recursive types. They were first briefly and informally introduced in 1985 by Cardelli in a manuscript describing the Amber language. Despite their use over many years, important aspects of the metatheory of the iso-recursive style Amber rules have not been studied in depth or turn out to be quite challenging to formalize. This article aims to revisit the problem of subtyping iso-recursive types. We start by introducing a novel declarative specification for Amber-style iso-recursive subtyping. Informally, the specification states that two recursive types are subtypes if all their finite unfoldings are subtypes. The Amber rules are shown to have equivalent expressive power to this declarative specification. We then show two variants of sound, complete and decidable algorithmic formulations of subtyping with respect to the declarative specification, which employ the idea of double unfoldings. Compared to the Amber rules, the double unfolding rules have the advantage of: (1) being modular; (2) not requiring reflexivity to be built in; (3) leading to an easy proof of transitivity of subtyping; and (4) being easily applicable to subtyping relations that are not antisymmetric (such as subtyping relations with record types). This work sheds new insights on the theory of subtyping iso-recursive types, and the new rules based on double unfoldings have important advantages over the original Amber rules involving recursive types. All results are mechanically formalized in the Coq theorem prover. © 2022 Association for Computing Machinery.",formalization; Iso-recursive types; subtyping,Amber; Formal languages; Theorem proving; Algorithmics; Expressive power; Formalisation; Iso-recursive type; Meta-theory; Modulars; Recursive types; Subtyping relation; Subtypings; Unfoldings; Specifications
Deep Dive into ZGC: A Modern Garbage Collector in OpenJDK,2022,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145778102&doi=10.1145%2f3538532&partnerID=40&md5=cffc6aa06a259fda78c87019bd3213f7,"ZGC is a modern, non-generational, region-based, mostly concurrent, parallel, mark-evacuate collector recently added to OpenJDK. It aims at having GC pauses that do not grow as the heap size increases, offering low latency even with large heap sizes. The ZGC C++ source code is readily accessible in the OpenJDK repository, but reading it (25 KLOC) can be very intimidating, and one might easily get lost in low-level implementation details, obscuring the key concepts. To make the ZGC algorithm more approachable, this work provides a thorough description on a high-level, focusing on the overall design with moderate implementation details. To explain the concurrency aspects, we provide a SPIN model that allows studying races between mutators and GC threads, and how they are resolved in ZGC. Such a model is not only useful for learning the current design (offering a deterministic and interactive experience) but also beneficial for prototyping new ideas and extensions. Our hope is that our detailed description and the SPIN model will enable the use of ZGC as a building block for future GC research, and research ideas implemented on top of it could even be adopted in the industry more readily, bridging the gap between academia and industry in the context of GC research. © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Garbage collection; model checking; SPIN; ZGC,C++ (programming language); C# source code; Deep dives; Garbage collection; Garbage collectors; Low latency; Models checking; Overall design; Region-based; SPIN; ZGC; Model checking
Containerless Plurals: Separating Number from Type in Object-Oriented Programming,2022,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146430002&doi=10.1145%2f3527635&partnerID=40&md5=bafe23d293e8d176919959d7fc29113f,"To let expressions evaluate to no or many objects, most object-oriented programming languages require the use of special constructs that encode these cases as single objects or values. While the requirement to treat these standard situations idiomatically seems to be broadly accepted, I argue that its alternative, letting expressions evaluate to any number of objects directly, has several advantages that make it worthy of consideration. As a proof of concept, I present a core object-oriented programming language, dubbed Num, which separates number from type so that the type of an expression is independent of the number of objects it may evaluate to, thus removing one major obstacle to using no, one, and many objects uniformly. Furthermore, Num abandons null references, replaces the nullability of reference types with the more general notion of countability, and allows methods to be invoked on any number of objects, including no object. To be able to adapt behavior to the actual number of receivers, Num complements instance methods with plural methods, that is, with methods that operate on a number of objects jointly and that replace static methods known from other languages. An implementation of Num in Prolog and accompanying type and number safety proofs are presented. © 2022 Copyright held by the owner/author(s).",bunches; collections; Multiplicities in programming; null-safety; object-relational programming,PROLOG (programming language); Bunch; Collection; Containerless; Multiplicity in programming; Null-safety; Object-oriented programming languages; Object-relational; Object-relational programming; Objectoriented programming (OOP); Single object; Object oriented programming
CHAD: Combinatory Homomorphic Automatic Differentiation,2022,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137614210&doi=10.1145%2f3527634&partnerID=40&md5=fe1e63f216581895fc31a36cad19dfa9,"We introduce Combinatory Homomorphic Automatic Differentiation (CHAD), a principled, pure, provably correct define-then-run method for performing forward and reverse mode automatic differentiation (AD) on programming languages with expressive features. It implements AD as a compositional, type-respecting source-code transformation that generates purely functional code. This code transformation is principled in the sense that it is the unique homomorphic (structure preserving) extension to expressive languages of Elliott's well-known and unambiguous definitions of AD for a first-order functional language. Correctness of the method follows by a (compositional) logical relations argument that shows that the semantics of the syntactic derivative is the usual calculus derivative of the semantics of the original program.In their most elegant formulation, the transformations generate code with linear types. However, the code transformations can be implemented in a standard functional language lacking linear types: While the correctness proof requires tracking of linearity, the actual transformations do not. In fact, even in a standard functional language, we can get all of the type-safety that linear types give us: We can implement all linear types used to type the transformations as abstract types by using a basic module system.In this article, we detail the method when applied to a simple higher-order language for manipulating statically sized arrays. However, we explain how the methodology applies, more generally, to functional languages with other expressive features. Finally, we discuss how the scope of CHAD extends beyond applications in AD to other dynamic program analyses that accumulate data in a commutative monoid.  © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Automatic differentiation; denotational semantics; functional programming; software correctness,Abstracting; Application programs; Calculations; Cosine transforms; Differentiation (calculus); Functional programming; Linear transformations; Automatic differentiations; Denotational semantics; Forward mode; Functional languages; Linear types; Purely functional; Reverse mode; Software correctness; Source code transformation; Semantics
Introduction to the Special Issue on ESOP 2021,2022,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137635896&doi=10.1145%2f3524730&partnerID=40&md5=0951fd0c20e674c72eac3e8d31409d24,[No abstract available],,
Runtime Complexity Bounds Using Squeezers,2022,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137834696&doi=10.1145%2f3527632&partnerID=40&md5=db560b9ebc2300a6f608977c62e00652,"Determining upper bounds on the time complexity of a program is a fundamental problem with a variety of applications, such as performance debugging, resource certification, and compile-time optimizations. Automated techniques for cost analysis excel at bounding the resource complexity of programs that use integer values and linear arithmetic. Unfortunately, they fall short when the complexity depends more intricately on the evolution of data during execution. In such cases, state-of-the-art analyzers have shown to produce loose bounds, or even no bound at all.We propose a novel technique that generalizes the common notion of recurrence relations based on ranking functions. Existing methods usually unfold one loop iteration and examine the resulting arithmetic relations between variables. These relations assist in establishing a recurrence that bounds the number of loop iterations. We propose a different approach, where we derive recurrences by comparing whole traces with whole traces of a lower rank, avoiding the need to analyze the complexity of intermediate states. We offer a set of global properties, defined with respect to whole traces, that facilitate such a comparison and show that these properties can be checked efficiently using a handful of local conditions. To this end, we adapt state squeezers, an induction mechanism previously used for verifying safety properties. We demonstrate that this technique encompasses the reasoning power of bounded unfolding, and more. We present some seemingly innocuous, yet intricate, examples that previous tools based on cost relations and control flow analysis fail to solve, and that our squeezer-powered approach succeeds.  © 2022 Copyright held by the owner/author(s).",recurrence equations; Runtime complexity analysis; simulation; squeezers; synthesis,Application programs; Integer programming; Iterative methods; Program debugging; Complexity analysis; Complexity bounds; Loop iteration; Recurrence equation; Run time complexity; Runtime complexity analyse; Simulation; Squeezer; Time complexity; Upper Bound; Cost benefit analysis
Strong-separation Logic,2022,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137767109&doi=10.1145%2f3498847&partnerID=40&md5=8eb35cd7a43536b64f6470ebab0e2f99,"Most automated verifiers for separation logic are based on the symbolic-heap fragment, which disallows both the magic-wand operator and the application of classical Boolean operators to spatial formulas. This is not surprising, as support for the magic wand quickly leads to undecidability, especially when combined with inductive predicates for reasoning about data structures. To circumvent these undecidability results, we propose assigning a more restrictive semantics to the separating conjunction. We argue that the resulting logic, strong-separation logic, can be used for symbolic execution and abductive reasoning just like ""standard""separation logic, while remaining decidable even in the presence of both the magic wand and inductive predicates (we consider a list-segment predicate and a tree predicate)-a combination of features that leads to undecidability for the standard semantics.  © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",(bi-)abduction; Decision procedure; magic wand,Boolean algebra; Computer circuits; Formal logic; Separation; (bi-)abduction; Abductive reasoning; Boolean operators; Decision procedure; Magic wand; Separation logic; Strong separations; Symbolic execution; Undecidability; Semantics
For a Few Dollars More: Verified Fine-Grained Algorithm Analysis Down to LLVM,2022,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137789243&doi=10.1145%2f3486169&partnerID=40&md5=835b3b4262858acf00d30ee82d87370a,"We present a framework to verify both, functional correctness and (amortized) worst-case complexity of practically efficient algorithms. We implemented a stepwise refinement approach, using the novel concept of resource currencies to naturally structure the resource analysis along the refinement chain, and allow a fine-grained analysis of operation counts. Our framework targets the LLVM intermediate representation. We extend its semantics from earlier work with a cost model. As case studies, we verify the amortized constant time push operation on dynamic arrays and the O(nlog n) introsort algorithm, and refine them down to efficient LLVM implementations. Our sorting algorithm performs on par with the state-of-the-art implementation found in the GNU C++ Library, and provably satisfies the complexity required by the C++ standard.  © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Algorithm analysis; Isabelle/HOL; program verification; refinement,C++ (programming language); Open source software; Algorithm analysis; Fine grained; Functional correctness; Isabelle; Isabelle/HOL; Novel concept; Program Verification; Refinement; Stepwise refinement; Worst-case complexity; Semantics
Session Coalgebras: A Coalgebraic View on Regular and Context-free Session Types,2022,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137828760&doi=10.1145%2f3527633&partnerID=40&md5=1c8731bf87c7f50085d2fc646275a1d4,"Compositional methods are central to the verification of software systems. For concurrent and communicating systems, compositional techniques based on behavioural type systems have received much attention. By abstracting communication protocols as types, these type systems can statically check that channels in a program interact following a certain protocol-whether messages are exchanged in the intended order.In this article, we put on our coalgebraic spectacles to investigate session types, a widely studied class of behavioural type systems. We provide a syntax-free description of session-based concurrency as states of coalgebras. As a result, we rediscover type equivalence, duality, and subtyping relations in terms of canonical coinductive presentations. In turn, this coinductive presentation enables us to derive a decidable type system with subtyping for the π-calculus, in which the states of a coalgebra will serve as channel protocols. Going full circle, we exhibit a coalgebra structure on an existing session type system, and show that the relations and type system resulting from our coalgebraic perspective coincide with existing ones. We further apply to session coalgebras the coalgebraic approach to regular languages via the so-called rational fixed point, inspired by the trinity of automata, regular languages, and regular expressions with session coalgebras, rational fixed point, and session types, respectively. We establish a suitable restriction on session coalgebras that determines a similar trinity, and reveals the mismatch between usual session types and our syntax-free coalgebraic approach. Furthermore, we extend our coalgebraic approach to account for context-free session types, by equipping session coalgebras with a stack.  © 2022 Copyright held by the owner/author(s).",coalgebra; coinduction; process calculi; Session types,Abstracting; Calculations; Computer programming languages; Object oriented programming; Syntactics; Verification; Behavioral type system; Coalgebraic; Coalgebras; Coinduction; Compositional method; Context-free; Fixed points; Process calculi; Session types; Type systems; Formal languages
Nested Session Types,2022,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137826214&doi=10.1145%2f3539656&partnerID=40&md5=085609f42eacca6dfb86ea41ea39b808,"Session types statically describe communication protocols between concurrent message-passing processes. Unfortunately, parametric polymorphism even in its restricted prenex form is not fully understood in the context of session types. In this article, we present the metatheory of session types extended with prenex polymorphism and, as a result, nested recursive datatypes. Remarkably, we prove that type equality is decidable by exhibiting a reduction to trace equivalence of deterministic first-order grammars. Recognizing the high theoretical complexity of the latter, we also propose a novel type equality algorithm and prove its soundness. We observe that the algorithm is surprisingly efficient and, despite its incompleteness, sufficient for all our examples. We have implemented our ideas by extending the Rast programming language with nested session types. We conclude with several examples illustrating the expressivity of our enhanced type system.  © 2022 Copyright held by the owner/author(s).",Nested types; polymorphism; type equality,Message passing; % reductions; Communications protocols; Datatypes; Message passing process; Meta-theory; Nested type; Parametric polymorphism; Prenex forms; Session types; Type equality; Polymorphism
Types for Complexity of Parallel Computation in Pi-calculus,2022,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137815211&doi=10.1145%2f3495529&partnerID=40&md5=f7f86ea1191b103cc56c633f3e4eeb70,"Type systems as a technique to analyse or control programs have been extensively studied for functional programming languages. In particular, some systems allow one to extract from a typing derivation a complexity bound on the program. We explore how to extend such results to parallel complexity in the setting of pi-calculus, considered as a communication-based model for parallel computation. Two notions of time complexity are given: the total computation time without parallelism (the work) and the computation time under maximal parallelism (the span). We define operational semantics to capture those two notions and present two type systems from which one can extract a complexity bound on a process. The type systems are inspired both by sized types and by input/output types, with additional temporal information about communications.  © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",complexity analysis; implicit computational complexity; pi-calculus; process calculi; sized types; Type systems,Calculations; Functional programming; Complexity analysis; Complexity bounds; Control program; Functional programming languages; Implicit computational complexity; Parallel Computation; Pi calculus; Process calculi; Sized type; Type systems; Semantics
What's Decidable about Causally Consistent Shared Memory?,2022,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135247852&doi=10.1145%2f3505273&partnerID=40&md5=deeb1bda3ccb19411a23c1aae49063df,"While causal consistency is one of the most fundamental consistency models weaker than sequential consistency, the decidability of safety verification for (finite-state) concurrent programs running under causally consistent shared memories is still unclear. In this article, we establish the decidability of this problem for two standard and well-studied variants of causal consistency. To do so, for each variant, we develop an equivalent ""lossy""operational semantics, whose states track possible futures, rather than more standard semantics that record the history of the execution. We show that these semantics constitute well-structured transition systems, thus enabling decidable verification. Based on a key observation, which we call the ""shared-memory causality principle,""the two novel semantics may also be of independent use in the investigation of weakly consistent models and their verification. Interestingly, our results are in contrast to the undecidability of this problem under the Release/Acquire fragment of the C/C++11 memory model, which forms another variant of causally consistent memory that, in terms of allowed outcomes, lies strictly between the two models studied here. Nevertheless, we show that all these three variants coincide for write/write-race-free programs, which implies the decidability of verification for such programs under Release/Acquire.  © 2022 Association for Computing Machinery.",causal consistency; concurrency; decidability; release/acquire; shared-memory; verification; Weak memory models; well-structured transition systems,Memory architecture; Semantics; Causal consistency; Concurrency; Consistency model; Finite-state; Release/acquire; Safety verification; Sequential consistency; Shared memory; Weak memory models; Well-structured transition systems; Computability and decidability
TF-Coder: Program Synthesis for Tensor Manipulations,2022,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135214616&doi=10.1145%2f3517034&partnerID=40&md5=7ddf3e2bf146b10845d10a979c6fadee,"The success and popularity of deep learning is on the rise, partially due to powerful deep learning frameworks such as TensorFlow and PyTorch, which make it easier to develop deep learning models. However, these libraries also come with steep learning curves, since programming in these frameworks is quite different from traditional imperative programming with explicit loops and conditionals. In this work, we present a tool called TF-Coder for programming by example in TensorFlow. TF-Coder uses a bottom-up weighted enumerative search, with value-based pruning of equivalent expressions and flexible type- A nd value-based filtering to ensure that expressions adhere to various requirements imposed by the TensorFlow library. We train models to predict TensorFlow operations from features of the input and output tensors and natural language descriptions of tasks to prioritize relevant operations during search. TF-Coder solves 63 of 70 real-world tasks within 5 minutes, sometimes finding simpler solutions in less time compared to experienced human programmers.  © 2022 Copyright held by the owner/author(s).",PBE; Program synthesis; programming by example; tensor manipulation; tensor transformation; TensorFlow,Deep learning; Learning frameworks; Learning models; PBE; Program synthesis; Programming by Example; Steep learning curve; Tensor manipulation; Tensor transformation; Tensorflow; Value-based; Tensors
Prophecy Made Simple,2022,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135224630&doi=10.1145%2f3492545&partnerID=40&md5=f7f06f5a2ff191e915b8174719a0de06,"Prophecy variables were introduced in the article ""The Existence of Refinement Mappings""by Abadi and Lamport. They were difficult to use in practice. We describe a new kind of prophecy variable that we find much easier to use. We also reformulate ideas from that article in a more mathematical way.  © 2022 held by the owner/author(s).",auxiliary variable; Formal specification; refinement; state machine,Auxiliary variables; Prophecy variables; Refinement; Refinement mapping; Simple++; State-machine; Formal specification
Solving Program Sketches with Large Integer Values,2022,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135243798&doi=10.1145%2f3532849&partnerID=40&md5=dfe8f0d5ddb9dbf24fd7947c507bb39c,"Program sketching is a program synthesis paradigm in which the programmer provides a partial program with holes and assertions. The goal of the synthesizer is to automatically find integer values for the holes so that the resulting program satisfies the assertions. The most popular sketching tool, Sketch, can efficiently solve complex program sketches but uses an integer encoding that often performs poorly if the sketched program manipulates large integer values. In this article, we propose a new solving technique that allows Sketch to handle large integer values while retaining its integer encoding. Our technique uses a result from number theory, the Chinese Remainder Theorem, to rewrite program sketches to only track the remainders of certain variable values with respect to several prime numbers. We prove that our transformation is sound and the encoding of the resulting programs are exponentially more succinct than existing Sketch encodings. We evaluate our technique on a variety of benchmarks manipulating large integer values. Our technique provides speedups against both existing Sketch solvers and can solve benchmarks that existing Sketch solvers cannot handle.  © 2022 Association for Computing Machinery.",chinese remainder theorem; program sketching; Program synthesis,Encoding (symbols); Integer programming; Signal encoding; Chinese remainder theorem; Complex programs; Integer encoding; Integer values; Prime number; Program sketching; Program synthesis; Sketching tools; Sketchings; Number theory
Gradualizing the Calculus of Inductive Constructions,2022,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130232502&doi=10.1145%2f3495528&partnerID=40&md5=5cf22ba0e8281fc366ef1af8606f291c,"We investigate gradual variations on the Calculus of Inductive Construction (CIC) for swifter prototyping with imprecise types and terms. We observe, with a no-go theorem, a crucial trade-off between graduality and the key properties of normalization and closure of universes under dependent product that CIC enjoys. Beyond this Fire Triangle of Graduality, we explore the gradualization of CIC with three different compromises, each relaxing one edge of the Fire Triangle. We develop a parametrized presentation of Gradual CIC (GCIC) that encompasses all three variations, and develop their metatheory. We first present a bidirectional elaboration of GCIC to a dependently-typed cast calculus, CastCIC, which elucidates the interrelation between typing, conversion, and the gradual guarantees. We use a syntactic model of CastCIC to inform the design of a safe, confluent reduction, and establish, when applicable, normalization. We study the static and dynamic gradual guarantees as well as the stronger notion of graduality with embedding-projection pairs formulated by New and Ahmed, using appropriate semantic model constructions. This work informs and paves the way towards the development of malleable proof assistants and dependently-typed programming languages.  © 2022 Association for Computing Machinery.",dependent types; Gradual typing; proof assistants,Economic and social effects; Semantics; Syntactics; % reductions; Calculus of inductive constructions; Dependent types; Gradual typing; Meta-theory; No-go theorem; Normalisation; Proof assistant; Property; Trade off; Calculations
Fast Graph Simplification for Interleaved-Dyck Reachability,2022,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135241278&doi=10.1145%2f3492428&partnerID=40&md5=cc1f38fa68a2dc5c1e25d18ad9a7b0fe,"Many program-analysis problems can be formulated as graph-reachability problems. Interleaved Dyck language reachability (InterDyck-reachability) is a fundamental framework to express a wide variety of program-analysis problems over edge-labeled graphs. The InterDyck language represents an intersection of multiple matched-parenthesis languages (i.e., Dyck languages). In practice, program analyses typically leverage one Dyck language to achieve context-sensitivity, and other Dyck languages to model data dependencies, such as field-sensitivity and pointer references/dereferences. In the ideal case, an InterDyck-reachability framework should model multiple Dyck languages simultaneously.Unfortunately, precise InterDyck-reachability is undecidable. Any practical solution must over-approximate the exact answer. In the literature, a lot of work has been proposed to over-approximate the InterDyck-reachability formulation. This article offers a new perspective on improving both the precision and the scalability of InterDyck-reachability: We aim at simplifying the underlying input graph G. Our key insight is based on the observation that if an edge is not contributing to any InterDyck-paths, we can safely eliminate it from G. Our technique is orthogonal to the InterDyck-reachability formulation and can serve as a pre-processing step with any over-approximating approach for InterDyck-reachability. We have applied our graph simplification algorithm to pre-processing the graphs from a recent InterDyck-reachability-based taint analysis for Android. Our evaluation of three popular InterDyck-reachability algorithms yields promising results. In particular, our graph-simplification method improves both the scalability and precision of all three InterDyck-reachability algorithms, sometimes dramatically.  © 2022 Association for Computing Machinery.",CFL-reachability; Static analysis,Scalability; Analysis problems; AS graph; CFL-reachability; Dyck language; Graph simplification; Labeled graphs; Parenthesis languages; Program analysis; Reachability; Reachability problem; Static analysis
Armada: Automated Verification of Concurrent Code with Sound Semantic Extensibility,2022,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135238444&doi=10.1145%2f3502491&partnerID=40&md5=ab895323dfeb9ee82a0f490ed2769884,"Safely writing high-performance concurrent programs is notoriously difficult. To aid developers, we introduce Armada, a language and tool designed to formally verify such programs with relatively little effort. Via a C-like language and a small-step, state-machine-based semantics, Armadagives developers the flexibility to choose arbitrary memory layout and synchronization primitives so that they are never constrained in their pursuit of performance. To reduce developer effort, Armadaleverages SMT-powered automation and a library of powerful reasoning techniques, including rely-guarantee, TSO elimination, reduction, and pointer analysis. All of these techniques are proven sound, and Armadacan be soundly extended with additional strategies over time. Using Armada, we verify five concurrent case studies and show that we can achieve performance equivalent to that of unverified code.  © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Refinement; weak memory models; x86-TSO,C (programming language); Codes (symbols); Automated verification; Concurrents programs; Developer efforts; Memory layout; Performance; Refinement; State-machine; Synchronization primitive; Weak memory models; X86-TSO; Semantics
Conditional Independence by Typing,2022,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123424387&doi=10.1145%2f3490421&partnerID=40&md5=bd4da97e004222b865494f1d6f797607,"A central goal of probabilistic programming languages (PPLs) is to separate modelling from inference. However, this goal is hard to achieve in practice. Users are often forced to re-write their models to improve efficiency of inference or meet restrictions imposed by the PPL. Conditional independence (CI) relationships among parameters are a crucial aspect of probabilistic models that capture a qualitative summary of the specified model and can facilitate more efficient inference.We present an information flow type system for probabilistic programming that captures conditional independence (CI) relationships and show that, for a well-typed program in our system, the distribution it implements is guaranteed to have certain CI-relationships. Further, by using type inference, we can statically deduce which CI-properties are present in a specified model.As a practical application, we consider the problem of how to perform inference on models with mixed discrete and continuous parameters. Inference on such models is challenging in many existing PPLs, but can be improved through a workaround, where the discrete parameters are used implicitly, at the expense of manual model re-writing. We present a source-to-source semantics-preserving transformation, which uses our CI-type system to automate this workaround by eliminating the discrete parameters from a probabilistic program. The resulting program can be seen as a hybrid inference algorithm on the original program, where continuous parameters can be drawn using efficient gradient-based inference methods, while the discrete parameters are inferred using variable elimination.We implement our CI-type system and its example application in SlicStan: A compositional variant of Stan. © 2021 Copyright held by the owner/author(s).",compiler correctness; conditional independence; information flow types; Probabilistic programming; static analysis,Computer programming languages; Inference engines; Modeling languages; Probability distributions; Semantics; Compiler correctness; Conditional independences; Continuous parameters; Discrete parameters; Flow type; Information flow type; Information flows; Probabilistic programming; Probabilistic programming language; Type systems; Static analysis
On Time-sensitive Control Dependencies,2022,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123398675&doi=10.1145%2f3486003&partnerID=40&md5=e508260ac59d9e7ba35dca8121f028ca,"We present efficient algorithms for time-sensitive control dependencies (CDs). If statement y is time-sensitively control dependent on statement x, then x decides not only whether y is executed but also how many timesteps after x. If y is not standard control dependent on x, but time-sensitively control dependent, then y will always be executed after x, but the execution time between x and y varies. This allows us to discover, e.g., timing leaks in security-critical software.We systematically develop properties and algorithms for time-sensitive CDs, as well as for nontermination-sensitive CDs. These work not only for standard control flow graphs (CFGs) but also for CFGs lacking a unique exit node (e.g., reactive systems). We show that Cytron's efficient algorithm for dominance frontiers [10] can be generalized to allow efficient computation not just of classical CDs but also of time-sensitive and nontermination-sensitive CDs. We then use time-sensitive CDs and time-sensitive slicing to discover cache timing leaks in an AES implementation. Performance measurements demonstrate scalability of the approach. © 2021 Association for Computing Machinery.",Control dependency; program slicing; timing dependency; timing leak,Computational efficiency; Data flow analysis; Flow graphs; Control dependency; Control-flow graphs; Critical software; Non terminations; Program slicing; Property; Security-critical; Time step; Timing dependency; Timing leak; Timing circuits
Bounded Verification of Multi-threaded Programs via Lazy Sequentialization,2022,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123441720&doi=10.1145%2f3478536&partnerID=40&md5=af614aada5bf1be1fea107f0b4b8752d,"Bounded verification techniques such as bounded model checking (BMC) have successfully been used for many practical program analysis problems, but concurrency still poses a challenge. Here, we describe a new approach to BMC of sequentially consistent imperative programs that use POSIX threads. We first translate the multi-threaded program into a nondeterministic sequential program that preserves reachability for all round-robin schedules with a given bound on the number of rounds. We then reuse existing high-performance BMC tools as backends for the sequential verification problem. Our translation is carefully designed to introduce very small memory overheads and very few sources of nondeterminism, so it produces tight SAT/SMT formulae, and is thus very effective in practice: Our Lazy-CSeq tool implementing this translation for the C programming language won several gold and silver medals in the concurrency category of the Software Verification Competitions (SV-COMP) 2014-2021 and was able to find errors in programs where all other techniques (including testing) failed. In this article, we give a detailed description of our translation and prove its correctness, sketch its implementation using the CSeq framework, and report on a detailed evaluation and comparison of our approach. © 2021 Association for Computing Machinery.",Concurrent program analysis; sequentialization,Model checking; Program translators; Software testing; Translation (languages); Analysis problems; Bounded model checking; Bounded verifications; Concurrent program analyse; Concurrents programs; Multi-threaded programs; Practical projects; Program analysis; Sequentialization; Verification techniques; C (programming language)
Bounded Abstract Effects,2022,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123450457&doi=10.1145%2f3492427&partnerID=40&md5=597d13684b1bc97abb07d5604eba21e3,"Effect systems have been a subject of active research for nearly four decades, with the most notable practical example being checked exceptions in programming languages such as Java. While many exception systems support abstraction, aggregation, and hierarchy (e.g., via class declaration and subclassing mechanisms), it is rare to see such expressive power in more generic effect systems. We designed an effect system around the idea of protecting system resources and incorporated our effect system into the Wyvern programming language. Similar to type members, a Wyvern object can have effect members that can abstract lower-level effects, allow for aggregation, and have both lower and upper bounds, providing for a granular effect hierarchy. We argue that Wyvern's effects capture the right balance of expressiveness and power from the programming language design perspective. We present a full formalization of our effect-system design, showing that it allows reasoning about authority and attenuation. Our approach is evaluated through a security-related case study. © 2022 Copyright held by the owner/author(s).",abstraction; effect system; Effects; expressiveness; language-based security,Ada (programming language); Abstraction; Effect; Effect system; Expressive power; Expressiveness; Generic effects; Language-based security; Subclassing; System resources; Systems support; Abstracting
The Systematic Design of Responsibility Analysis by Abstract Interpretation,2022,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123416988&doi=10.1145%2f3484938&partnerID=40&md5=bffd2f39b9725445208ca8c53c63910f,"Given a behavior of interest, automatically determining the corresponding responsible entity (i.e., the root cause) is a task of critical importance in program static analysis. In this article, a novel definition of responsibility based on the abstraction of trace semantics is proposed, which takes into account the cognizance of observer, which, to the best of our knowledge, is a new innovative idea in program analysis. Compared to current dependency and causality analysis methods, the responsibility analysis is demonstrated to be more precise on various examples.However, the concrete trace semantics used in defining responsibility is uncomputable in general, which makes the corresponding concrete responsibility analysis undecidable. To solve this problem, the article proposes a sound framework of abstract responsibility analysis, which allows a balance between cost and precision. Essentially, the abstract analysis builds a trace partitioning automaton by an iteration of over-approximating forward reachability analysis with trace partitioning and under/over-approximating backward impossible failure accessibility analysis, and determines the bounds of potentially responsible entities along paths in the automaton. Unlike the concrete responsibility analysis that identifies exactly a single action as the responsible entity along every concrete trace, the abstract analysis may lose some precision and find multiple actions potentially responsible along each automaton path. However, the soundness is preserved, and every responsible entity in the concrete is guaranteed to be also found responsible in the abstract. © 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM.",abstract interpretation; backward accessibility analysis; causality; cognizance; dependency; forward reachability analysis; Responsibility analysis; trace partitioning,Concretes; Iterative methods; Model checking; Semantics; Static analysis; Trace analysis; Abstract interpretations; Backward accessibility analyse; Causality; Cognizance; Dependency; Forward reachability analyse; Reachability analysis; Responsibility analyse; Trace partitioning; Trace semantics; Abstracting
Completeness and Complexity of Reasoning about Call-by-Value in Hoare Logic,2021,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119470671&doi=10.1145%2f3477143&partnerID=40&md5=9b6fb88eb5d997be9c311d90ba99130c,"We provide a sound and relatively complete Hoare logic for reasoning about partial correctness of recursive procedures in presence of local variables and the call-by-value parameter mechanism and in which the correctness proofs support contracts and are linear in the length of the program. We argue that in spite of the fact that Hoare logics for recursive procedures were intensively studied, no such logic has been proposed in the literature.  © 2021 Association for Computing Machinery.",call-by-value; completeness; Hoare logic; Recursive procedures; soundness,Call-by-value; Call-by-value parameters; Completeness; Correctness proofs; Hoare Logic; Local variables; Parameter mechanisms; Partial correctness; Recursive procedure; Soundness; Computer circuits
RustHorn: CHC-based Verification for Rust Programs,2021,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119521319&doi=10.1145%2f3462205&partnerID=40&md5=b4a619eb0287788075b65e0ed6094f68,"Reduction to satisfiability of constrained Horn clauses (CHCs) is a widely studied approach to automated program verification. Current CHC-based methods, however, do not work very well for pointer-manipulating programs, especially those with dynamic memory allocation. This article presents a novel reduction of pointer-manipulating Rust programs into CHCs, which clears away pointers and memory states by leveraging Rust's guarantees on permission. We formalize our reduction for a simplified core of Rust and prove its soundness and completeness. We have implemented a prototype verifier for a subset of Rust and confirmed the effectiveness of our method.  © 2021 Association for Computing Machinery.",automated verification; CHC; ownership; permission; pointer; Rust,Storage allocation (computer); % reductions; Automated program verification; Automated verification; Constrained horn clause; Horn clause; Ownership; Permission; Pointer; Rust; Satisfiability; Logic programming
An Extended Account of Trace-relating Compiler Correctness and Secure Compilation,2021,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119491281&doi=10.1145%2f3460860&partnerID=40&md5=988184382e0fedba797a1146bd587823,"Compiler correctness, in its simplest form, is defined as the inclusion of the set of traces of the compiled program in the set of traces of the original program. This is equivalent to the preservation of all trace properties. Here, traces collect, for instance, the externally observable events of each execution. However, this definition requires the set of traces of the source and target languages to be the same, which is not the case when the languages are far apart or when observations are fine-grained. To overcome this issue, we study a generalized compiler correctness definition, which uses source and target traces drawn from potentially different sets and connected by an arbitrary relation. We set out to understand what guarantees this generalized compiler correctness definition gives us when instantiated with a non-trivial relation on traces. When this trace relation is not equality, it is no longer possible to preserve the trace properties of the source program unchanged. Instead, we provide a generic characterization of the target trace property ensured by correctly compiling a program that satisfies a given source property, and dually, of the source trace property one is required to show to obtain a certain target property for the compiled code. We show that this view on compiler correctness can naturally account for undefined behavior, resource exhaustion, different source and target values, side channels, and various abstraction mismatches. Finally, we show that the same generalization also applies to many definitions of secure compilation, which characterize the protection of a compiled program linked against adversarial code.  © 2021 Copyright held by the owner/author(s).",compiler correctness; formal languages; Galois connection; hyperproperties; programming languages; property-preserving compilation; secure compilation; Trace properties,Codes (symbols); Computer programming languages; Program compilers; Compiled programs; Compiler correctness; Galois connection; Hyperproperty; Programming language; Property; Property-preserving; Property-preserving compilation; Secure compilation; Trace property; Formal languages
TaDA Live: Compositional Reasoning for Termination of Fine-grained Concurrent Programs,2021,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119494041&doi=10.1145%2f3477082&partnerID=40&md5=1531184f16661634161ad365cb35d819,"We present TaDA Live, a concurrent separation logic for reasoning compositionally about the termination of blocking fine-grained concurrent programs. The crucial challenge is how to deal with abstract atomic blocking: that is, abstract atomic operations that have blocking behaviour arising from busy-waiting patterns as found in, for example, fine-grained spin locks. Our fundamental innovation is with the design of abstract specifications that capture this blocking behaviour as liveness assumptions on the environment. We design a logic that can reason about the termination of clients that use such operations without breaking their abstraction boundaries, and the correctness of the implementations of the operations with respect to their abstract specifications. We introduce a novel semantic model using layered subjective obligations to express liveness invariants and a proof system that is sound with respect to the model. The subtlety of our specifications and reasoning is illustrated using several case studies.  © 2021 Copyright held by the owner/author(s).",busy-waiting; concurrent separation logics; Fine-grained concurrency; linearisability; liveness; termination,Computer circuits; Formal logic; Semantics; Specifications; Blockings; Busy-waiting; Concurrent separation logic; Concurrents programs; Fine grained; Fine-grained concurrency; Linearisability; Liveness; Separation logic; Termination; Abstracting
Introduction to the Special Section on ESOP 2020,2021,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119475865&doi=10.1145%2f3484490&partnerID=40&md5=c1ed3c57b0500a571c2e76b12d2f0f2c,[No abstract available],,
On Polymorphic Sessions and Functions: A Tale of Two (Fully Abstract) Encodings,2021,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111456477&doi=10.1145%2f3457884&partnerID=40&md5=be9a8b4e7a1d4426ed51ba13d725a9a9,"This work exploits the logical foundation of session types to determine what kind of type discipline for the -calculus can exactly capture, and is captured by, -calculus behaviours. Leveraging the proof theoretic content of the soundness and completeness of sequent calculus and natural deduction presentations of linear logic, we develop the first mutually inverse and fully abstract processes-as-functions and functions-as-processes encodings between a polymorphic session I€-calculus and a linear formulation of System F. We are then able to derive results of the session calculus from the theory of the -calculus: (1) we obtain a characterisation of inductive and coinductive session types via their algebraic representations in System F; and (2) we extend our results to account for value and process passing, entailing strong normalisation. © 2021 ACM.",full abstraction; I€-calculus; linear logic; Session types; system F,Differentiation (calculus); Encoding (symbols); Formal logic; Abstract process; Algebraic representations; Linear formulation; Logical foundations; Natural deduction; Process-passing; Sequent calculus; Soundness and completeness; Calculations
Armed Cats: Formal Concurrency Modelling at Arm,2021,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111441141&doi=10.1145%2f3458926&partnerID=40&md5=02e4bf405b65c1f36aebdcffd072082c,"We report on the process for formal concurrency modelling at Arm. An initial formal consistency model of the Arm achitecture, written in the cat language, was published and upstreamed to the herd+diy tool suite in 2017. Since then, we have extended the original model with extra features, for example, mixed-size accesses, and produced two provably equivalent alternative formulations. In this article, we present a comprehensive review of work done at Arm on the consistency model. Along the way, we also show that our principle for handling mixed-size accesses applies to x86: We confirm this via vast experimental campaigns. We also show that our alternative formulations are applicable to any model phrased in a style similar to the one chosen by Arm. © 2021 Owner/Author.",arm architecture; Concurrency Weak memory models; linux; mixed-size accesses,Software engineering; Consistency model; Experimental campaign; Mixed Size; Original model; Toolsuite; Linguistics
A Programming Language for Data Privacy with Accuracy Estimations,2021,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111456404&doi=10.1145%2f3452096&partnerID=40&md5=2f3b5356c9912c1493ef8fcacca45900,"Differential privacy offers a formal framework for reasoning about the privacy and accuracy of computations on private data. It also offers a rich set of building blocks for constructing private data analyses. When carefully calibrated, these analyses simultaneously guarantee the privacy of the individuals contributing their data, and the accuracy of the data analysis results, inferring useful properties about the population. The compositional nature of differential privacy has motivated the design and implementation of several programming languages to ease the implementation of differentially private analyses. Even though these programming languages provide support for reasoning about privacy, most of them disregard reasoning about the accuracy of data analyses. To overcome this limitation, we present DPella, a programming framework providing data analysts with support for reasoning about privacy, accuracy, and their trade-offs. The distinguishing feature of DPella is a novel component that statically tracks the accuracy of different data analyses. To provide tight accuracy estimations, this component leverages taint analysis for automatically inferring statistical independence of the different noise quantities added for guaranteeing privacy. We evaluate our approach by implementing several classical queries from the literature and showing how data analysts can calibrate the privacy parameters to meet the accuracy requirements, and vice versa. © 2021 ACM.",Accuracy; concentration bounds; databases; differential privacy; Haskell,Economic and social effects; Privacy by design; Accuracy estimation; Design and implementations; Differential privacies; Privacy and accuracy; Private data analysis; Programming framework; Statistical independence; Useful properties; Population statistics
Ranking and Repulsing Supermartingales for Reachability in Randomized Programs,2021,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111463619&doi=10.1145%2f3450967&partnerID=40&md5=cd306bdecc071835058347bc385f758c,"Computing reachability probabilities is a fundamental problem in the analysis of randomized programs. This article aims at a comprehensive and comparative account of various martingale-based methods for over-and under-approximating reachability probabilities. Based on the existing works that stretch across different communities (formal verification, control theory, etc.), we offer a unifying account. In particular, we emphasize the role of order-theoretic fixed points-a classic topic in computer science-in the analysis of randomized programs. This leads us to two new martingale-based techniques, too. We also make an experimental comparison using our implementation of template-based synthesis algorithms for those martingales. © 2021 Owner/Author.",fixed point; martingale; Randomized program; reachability,Linguistics; Software engineering; Experimental comparison; Fixed points; Reachability; Supermartingales; Template-based synthesis; Computation theory
"Robustly safe compilation, an efficient form of secure compilation",2021,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105011877&doi=10.1145%2f3436809&partnerID=40&md5=de42a65fa56025ee3bd5fb4eaa8ddceb,"Security-preserving compilers generate compiled code that withstands target-level attacks such as alteration of control flow, data leaks, or memory corruption. Many existing security-preserving compilers are proven to be fully abstract, meaning that they reflect and preserve observational equivalence. Fully abstract compilation is strong and useful but, in certain cases, comes at the cost of requiring expensive runtime constructs in compiled code. These constructs may have no relevance for security, but are needed to accommodate differences between the source and target languages that fully abstract compilation necessarily needs. As an alternative to fully abstract compilation, this article explores a different criterion for secure compilation called robustly safe compilation or RSC. Briefly, this criterion means that the compiled code preserves relevant safety properties of the source program against all adversarial contexts interacting with the compiled program. We show that RSC can be proved more easily than fully abstract compilation and also often results in more efficient code. We also present two different proof techniques for establishing that a compiler attains RSC and, to illustrate them, develop three illustrative robustly safe compilers that rely on different target-level protection mechanisms. We then proceed to turn one of our compilers into a fully abstract one and through this example argue that proving RSC can be simpler than proving full abstraction. To better explain and clarify notions, this article uses syntax highlighting in a way that colourblind and black-8-white readers can benefit from Reference [58]. For a better experience, please print or view this article in colour.1  © 2021 Owner/Author.",formal languages; fully abstract compilation; programming languages; robust safety; robustly-safe compilation; Secure compilation,Program compilers; Security of data; Compiled programs; Control flows; Full abstraction; Memory corruption; Observational equivalences; Protection mechanisms; Safety property; Target language; Codes (symbols)
A lightweight formalism for reference lifetimes and borrowing in rust,2021,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104956171&doi=10.1145%2f3443420&partnerID=40&md5=3e73a72a739fd6d7548e75d152b40553,"Rust is a relatively new programming language that has gained significant traction since its v1.0 release in 2015. Rust aims to be a systems language that competes with C/C++. A claimed advantage of Rust is a strong focus on memory safety without garbage collection. This is primarily achieved through two concepts, namely, reference lifetimes and borrowing. Both of these are well-known ideas stemming from the literature on region-based memory management and linearity/uniqueness. Rust brings both of these ideas together to form a coherent programming model. Furthermore, Rust has a strong focus on stack-allocated data and, like C/C++ but unlike Java, permits references to local variables. Type checking in Rust can be viewed as a two-phase process: First, a traditional type checker operates in a flow-insensitive fashion; second, a borrow checker enforces an ownership invariant using a flow-sensitive analysis. In this article, we present a lightweight formalism that captures these two phases using a flow-sensitive type system that enforces ""type and borrow safety.""In particular, programs that are type and borrow safe will not attempt to dereference dangling pointers. Our calculus core captures many aspects of Rust, including copy- and move-semantics, mutable borrowing, reborrowing, partial moves, and lifetimes. In particular, it remains sufficiently lightweight to be easily digested and understood and, we argue, still captures the salient aspects of reference lifetimes and borrowing. Furthermore, extensions to the core can easily add more complex features (e.g., control-flow, tuples, method invocation). We provide a soundness proof to verify our key claims of the calculus. We also provide a reference implementation in Java with which we have model checked our calculus using over 500B input programs. We have also fuzz tested the Rust compiler using our calculus against 2B programs and, to date, found one confirmed compiler bug and several other possible issues. © 2021 ACM.",model checking; ownership; Rust; type theory,Calculations; Java programming language; Program compilers; Semantics; Dangling pointers; Flow-sensitive analysis; Flow-sensitive type system; Garbage collection; Method invocation; Programming models; Reference implementation; Region-based memory management; C++ (programming language)
CSim2: Compositional top-down verification of concurrent systems using rely-guarantee,2021,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105010983&doi=10.1145%2f3436808&partnerID=40&md5=ab5ee48cad3d2f08a407d0a35e565c1f,"To make feasible and scalable the verification of large and complex concurrent systems, it is necessary the use of compositional techniques even at the highest abstraction layers. When focusing on the lowest software abstraction layers, such as the implementation or the machine code, the high level of detail of those layers makes the direct verification of properties very difficult and expensive. It is therefore essential to use techniques allowing to simplify the verification on these layers. One technique to tackle this challenge is top-down verification where by means of simulation properties verified on top layers (representing abstract specifications of a system) are propagated down to the lowest layers (that are an implementation of the top layers). There is no need to say that simulation of concurrent systems implies a greater level of complexity, and having compositional techniques to check simulation between layers is also desirable when seeking for both feasibility and scalability of the refinement verification. In this article, we present CSim2 a (compositional) rely-guarantee-based framework for the top-down verification of complex concurrent systems in the Isabelle/HOL theorem prover. CSim2 uses CSimpl, a language with a high degree of expressiveness designed for the specification of concurrent programs. Thanks to its expressibility, CSimpl is able to model many of the features found in real world programming languages like exceptions, assertions, and procedures. CSim2 provides a framework for the verification of rely-guarantee properties to compositionally reason on CSimpl specifications. Focusing on top-down verification, CSim2 provides a simulation-based framework for the preservation of CSimpl rely-guarantee properties from specifications to implementations. By using the simulation framework, properties proven on the top layers (abstract specifications) are compositionally propagated down to the lowest layers (source or machine code) in each concurrent component of the system. Finally, we show the usability of CSim2 by running a case study over two CSimpl specifications of an Arinc-653 communication service. In this case study, we prove a complex property on a specification, and we use CSim2 to preserve the property on lower abstraction layers. © 2021 ACM.",compositional verification; concurrency verification; isabelle/HOL; operating systems verification; Rely-guarantee; simulation and refinement,Abstracting; Specifications; Abstract specifications; Communication service; Complex concurrent systems; Complex properties; Concurrent program; Concurrent systems; Simulation framework; Software abstractions; Verification
Polymorphic iterable sequential effect systems,2021,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104934923&doi=10.1145%2f3450272&partnerID=40&md5=0d86cc60975f8a9c2a9f9a9c9a4943e7,"Effect systems are lightweight extensions to type systems that can verify a wide range of important properties with modest developer burden. But our general understanding of effect systems is limited primarily to systems where the order of effects is irrelevant. Understanding such systems in terms of a semilattice of effects grounds understanding of the essential issues and provides guidance when designing new effect systems. By contrast, sequential effect systems - where the order of effects is important - lack an established algebraic structure on effects. We present an abstract polymorphic effect system parameterized by an effect quantale - an algebraic structure with well-defined properties that can model the effects of a range of existing sequential effect systems. We define effect quantales, derive useful properties, and show how they cleanly model a variety of known sequential effect systems. We show that for most effect quantales, there is an induced notion of iterating a sequential effect; that for systems we consider the derived iteration agrees with the manually designed iteration operators in prior work; and that this induced notion of iteration is as precise as possible when defined. We also position effect quantales with respect to work on categorical semantics for sequential effect systems, clarifying the distinctions between these systems and our own in the course of giving a thorough survey of these frameworks. Our derived iteration construct should generalize to these semantic structures, addressing limitations of that work. Finally, we consider the relationship between sequential effects and Kleene Algebras, where the latter may be used as instances of the former. © 2021 ACM.",effect systems; polymorphism; quantales; Type systems,Semantics; Algebraic structures; Categorical semantics; Kleene algebras; Polymorphic effects; Position effect; Semantic structures; Sequential effects; Useful properties; Algebra
"Semantic Correctness of Dependence-based Slicing for Interprocedural, Possibly Nonterminating Programs",2021,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100787081&doi=10.1145%2f3434489&partnerID=40&md5=ad49bafacb6252f1c09222c2de1c5147,"Existing proofs of correctness for dependence-based slicing methods are limited either to the slicing of intraprocedural programs [2, 39], or the proof is only applicable to a specific slicing method [4, 41]. We contribute a general proof of correctness for dependence-based slicing methods such as Weiser [50, 51], or Binkley et al. [7, 8], for interprocedural, possibly nonterminating programs. The proof uses well-formed weak and strong control closure relations, which are the interprocedural extensions of the generalised weak/strong control closure provided by Danicic et al. [13], capturing various nonterminating-insensitive and nontermination-sensitive control-dependence relations that have been proposed in the literature. Thus, our proof framework is valid for a whole range of existing control-dependence relations. We have provided a definition of semantically correct (SC) slice. We prove that SC slices agree with Weiser slicing, that deterministic SC slices preserve termination, and that nondeterministic SC slices preserve the nondeterministic behavior of the original programs. © 2021 ACM.",bisimulation; Correctness of slicing; interprocedural program; nondeterminism; nontermination; semi-equivalence; simulation; static slicing,Linguistics; Software engineering; Dependence relation; Inter-procedural; Non terminations; Nondeterministic behavior; Proof of correctness; Slicing methods; Semantics
Chocola: Composable Concurrency Language,2021,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100719582&doi=10.1145%2f3427201&partnerID=40&md5=6fe289909c1baa4cb0bd04e4958ab8ca,"Programmers often combine different concurrency models in a single program, in each part of the program using the model that fits best. Many programming languages, such as Clojure, Scala, and Java, cater to this need by supporting different concurrency models. However, existing programming languages often combine concurrency models in an ad hoc way, and the semantics of the combinations are not always well defined. This article studies the combination of three concurrency models: futures, transactions, and actors. We show that a naive combination of these models invalidates the guarantees they normally provide, thereby breaking the assumptions of programmers. Hence, we present Chocola: a unified language of futures, transactions, and actors that maintains the guarantees of all three models wherever possible, even when they are combined. We describe and formalize the semantics of this language and prove the guarantees it provides. We also provide an implementation as an extension of Clojure and demonstrated that it can improve the performance of three benchmark applications for relatively little effort from the developer. © 2021 ACM.",actor model; Futures; software transactional memory,Semantics; Benchmark applications; Clojure; Composable; Three models; Unified language; Benchmarking
Interprocedural Context-Unbounded Program Analysis Using Observation Sequences,2021,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100825518&doi=10.1145%2f3418583&partnerID=40&md5=12a2ee60713126dcc28adae0c9b1c361,"A classical result by Ramalingam about synchronization-sensitive interprocedural program analysis implies that reachability for concurrent threads running recursive procedures is undecidable. A technique proposed by Qadeer and Rehof, to bound the number of context switches allowed between the threads, leads to an incomplete solution that is, however, believed to catch ""most bugs""in practice, as errors tend to occur within few contexts. The question of whether the technique can also prove the absence of bugs at least in some cases has remained largely open. Toward closing this gap, we introduce in this article the generic verification paradigm of observation sequences for resource-parameterized programs. Such a sequence observes how increasing the resource parameter affects the reachability of states satisfying a given property. The goal is to show that increases beyond some ""cutoff""parameter value have no impact on the reachability - the sequence has converged. This allows us to conclude that the property holds for all parameter values. We applied this paradigm to the context-unbounded program analysis problem, choosing the resource to be the number of permitted thread context switches. The result is a partially correct interprocedural reachability analysis technique for concurrent shared-memory programs. Our technique may not terminate but is able to both refute and prove context-unbounded safety for such programs. We demonstrate the effectiveness and efficiency of the technique using a variety of benchmark programs. The safe instances cannot be proved safe by earlier, context-bounded methods. © 2020 ACM.",concurrency; context bound; Interprocedural analysis; recursion; stack,Software engineering; Concurrent threads; Effectiveness and efficiencies; Generic verification; Interprocedural program analysis; Reachability analysis; Recursive procedure; Resource parameters; Shared-memory programs; Linguistics
Active Learning for Inference and Regeneration of Applications that Access Databases,2021,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100818180&doi=10.1145%2f3430952&partnerID=40&md5=822c1a6b299ec3991dce339f6fe600ac,"We present Konure, a new system that uses active learning to infer models of applications that retrieve data from relational databases. Konure comprises a domain-specific language (each model is a program in this language) and associated inference algorithm that infers models of applications whose behavior can be expressed in this language. The inference algorithm generates inputs and database contents, runs the application, then observes the resulting database traffic and outputs to progressively refine its current model hypothesis. Because the technique works with only externally observable inputs, outputs, and database contents, it can infer the behavior of applications written in arbitrary languages using arbitrary coding styles (as long as the behavior of the application is expressible in the domain-specific language). Konure also implements a regenerator that produces a translated Python implementation of the application that systematically includes relevant security and error checks. © 2021 ACM.",Active learning; program inference; program regeneration,Inference engines; Problem oriented languages; ACCESS database; Active Learning; Arbitrary languages; Current modeling; Database contents; Domain specific languages; Inference algorithm; Relational Database; Application programs
Securing Interruptible Enclaved Execution on Small Microprocessors,2021,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130328964&doi=10.1145%2f3470534&partnerID=40&md5=861f1d7508f34d2c7da9da485b4193cc,"Computer systems often provide hardware support for isolation mechanisms such as privilege levels, virtual memory, or enclaved execution. Over the past years, several successful software-based side-channel attacks have been developed that break, or at least significantly weaken, the isolation that these mechanisms offer. Extending a processor with new architectural or micro-architectural features brings a risk of introducing new software-based side-channel attacks. This article studies the problem of extending a processor with new features without weakening the security of the isolation mechanisms that the processor offers. Our solution is heavily based on techniques from research on programming languages. More specifically, we propose to use the programming language concept of full abstraction as a general formal criterion for the security of a processor extension. We instantiate the proposed criterion to the concrete case of extending a microprocessor that supports enclaved execution with secure interruptibility. This is a very relevant instantiation, as several recent papers have shown that interruptibility of enclaves leads to a variety of software-based side-channel attacks. We propose a design for interruptible enclaves and prove that it satisfies our security criterion. We also implement the design on an open-source enclave-enabled microprocessor and evaluate the cost of our design in terms of performance and hardware size. © 2021 Association for Computing Machinery.",enclaves; full abstraction; Language-based security; secure compilation,Abstracting; Computer hardware; Hardware security; Integrated circuit design; Network security; Open source software; Architectural features; Enclave; Full abstraction; Hardware supports; Interruptibility; Language-based security; Secure compilation; Security criterion; Side-channel attacks; Virtual memory; Side channel attack
Obsidian: Typestate and assets for safer blockchain programming,2020,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097251634&doi=10.1145%2f3417516&partnerID=40&md5=22b3c995daeaf1b662e98566403c87b2,"Blockchain platforms are coming into use for processing critical transactions among participants who have not established mutual trust. Many blockchains are programmable, supporting smart contracts, which maintain persistent state and support transactions that transform the state. Unfortunately, bugs in many smart contracts have been exploited by hackers. Obsidian is a novel programming language with a type system that enables static detection of bugs that are common in smart contracts today. Obsidian is based on a core calculus, Silica, for which we proved type soundness. Obsidian uses typestate to detect improper state manipulation and uses linear types to detect abuse of assets. We integrated a permissions system that encodes a notion of ownership to allow for safe, flexible aliasing. We describe two case studies that evaluate Obsidian's applicability to the domains of parametric insurance and supply chain management, finding that Obsidian's type system facilitates reasoning about high-level states and ownership of resources. We compared our Obsidian implementation to a Solidity implementation, observing that the Solidity implementation requires much boilerplate checking and tracking of state, whereas Obsidian does this work statically. © 2020 Owner/Author.",alias control; blockchain; linearity; ownership; permissions; smart contracts; type systems; Typestate,Calculations; Personal computing; Program debugging; Silica; Supply chain management; Case-studies; CoRE calculus; Linear types; Mutual trust; Persistent state; Static detections; Type soundness; Type systems; Blockchain
An effective fusion and tile size model for polyMage,2020,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097243505&doi=10.1145%2f3404846&partnerID=40&md5=60c5495c1ae56d60035b977b9f5ed381,"Effective models for fusion of loop nests continue to remain a challenge in both general-purpose and domain-specific language (DSL) compilers. The difficulty often arises from the combinatorial explosion of grouping choices and their interaction with parallelism and locality. This article presents a new fusion algorithm for high-performance domain-specific compilers for image processing pipelines. The fusion algorithm is driven by dynamic programming and explores spaces of fusion possibilities not covered by previous approaches, and it is also driven by a cost function more concrete and precise in capturing optimization criteria than prior approaches. The fusion model is particularly tailored to the transformation and optimization sequence applied by PolyMage and Halide, two recent DSLs for image processing pipelines. Our model-driven technique when implemented in PolyMage provides significant improvements (up to 4.32×) over PolyMage's approach (which uses auto-tuning to aid its model) and over Halide's automatic approach (by up to 2.46×) on two state-of-the-art shared-memory multicore architectures. © 2020 ACM.",Fusion; image processing pipelines; locality; parallelism; tiling,Cost functions; Digital subscriber lines; Dynamic programming; Image processing; Memory architecture; Pipeline processing systems; Pipelines; Problem oriented languages; Program compilers; Software architecture; Automatic approaches; Combinatorial explosion; Domain specific languages; Image processing pipeline; Model-driven techniques; Multicore architectures; Optimization criteria; Parallelism and localities; Image fusion
Type Inference for C: Applications to the static analysis of incomplete programs,2020,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097237469&doi=10.1145%2f3421472&partnerID=40&md5=d6a15ca6d5337890fb2e30a1a5b590a4,"Type inference is a feature that is common to a variety of programming languages. While, in the past, it has been prominently present in functional ones (e.g., ML and Haskell), today, many object-oriented/multi-paradigm languages such as C# and C++ offer, to a certain extent, such a feature. Nevertheless, type inference still is an unexplored subject in the realm of C. In particular, it remains open whether it is possible to devise a technique that encompasses the idiosyncrasies of this language. The first difficulty encountered when tackling this problem is that parsing C requires, not only syntactic, but also semantic information. Yet, greater challenges emerge due to C's intricate type system. In this work, we present a unification-based framework that lets us infer the missing struct, union, enum, and typedef declarations in a program. As an application of our technique, we investigate the reconstruction of partial programs. Incomplete source code naturally appears in software development: during design and while evolving, testing, and analyzing programs; therefore, understanding it is a valuable asset. With a reconstructed well-typed program, one can: (i) enable static analysis tools in scenarios where components are absent; (ii) improve precision of ""zero setup""static analysis tools; (iii) apply stub generators, symbolic executors, and testing tools on code snippets; and (iv) provide engineers with an assortment of compilable benchmarks for performance and correctness validation. We evaluate our technique on code from a variety of C libraries, including GNU's Coreutils and on snippets from popular projects such as CPython, FreeBSD, and Git. © 2020 ACM.",C language; parsing; Partial programs; type inference,Application programs; Benchmarking; Object oriented programming; Open source software; Semantics; Software design; Software testing; Static analysis; Syntactics; Well testing; FreeBSD; Haskell; Object oriented; Semantic information; Source codes; Testing tools; Type inferences; Type systems; C++ (programming language)
Inferring lower runtime bounds for integer programs,2020,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097215480&doi=10.1145%2f3410331&partnerID=40&md5=b9f75e8429bf7761dbbc4ac1e89d7667,"We present a technique to infer lower bounds on the worst-case runtime complexity of integer programs, where in contrast to earlier work, our approach is not restricted to tail-recursion. Our technique constructs symbolic representations of program executions using a framework for iterative, under-approximating program simplification. The core of this simplification is a method for (under-approximating) program acceleration based on recurrence solving and a variation of ranking functions. Afterwards, we deduce asymptotic lower bounds from the resulting simplified programs using a special-purpose calculus and an SMT encoding. We implemented our technique in our tool LoAT and show that it infers non-trivial lower bounds for a large class of examples. © 2020 Owner/Author.",automated complexity analysis; Integer programs; lower bounds; runtime complexity,Calculations; Iterative methods; Integer program; Lower bounds; Non-trivial; Program execution; Program simplification; Ranking functions; Run time complexity; Symbolic representation; Integer programming
A Theory of Slicing for Imperative Probabilistic Programs,2020,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088822313&doi=10.1145%2f3372895&partnerID=40&md5=0c5f015deddf3c205dacf9e6bc93a2c4,"Dedicated to the memory of Sebastian Danicic. We present a theory for slicing imperative probabilistic programs containing random assignments and ""observe"" statements for conditioning. We represent such programs as probabilistic control-flow graphs (pCFGs) whose nodes modify probability distributions. This allows direct adaptation of standard machinery such as data dependence, postdominators, relevant variables, and so on, to the probabilistic setting. We separate the specification of slicing from its implementation: (1) first, we develop syntactic conditions that a slice must satisfy (they involve the existence of another disjoint slice such that the variables of the two slices are probabilistically independent of each other); (2) next, we prove that any such slice is semantically correct; (3) finally, we give an algorithm to compute the least slice. To generate smaller slices, we may in addition take advantage of knowledge that certain loops will terminate (almost) always. Our results carry over to the slicing of structured imperative probabilistic programs, as handled in recent work by Hur et al. For such a program, we can define its slice, which has the same ""normalized"" semantics as the original program; the proof of this property is based on a result proving the adequacy of the semantics of pCFGs w.r.t. the standard semantics of structured imperative probabilistic programs. © 2020 ACM.",probabilistic control-flow graphs; Probabilistic programming; program slicing,Flow graphs; Machinery; Semantics; Control flow graphs; Data dependence; Probabilistic programs; Random assignment; Sebastian; Probability distributions
Symbolic Disintegration with a Variety of Base Measures,2020,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088839678&doi=10.1145%2f3374208&partnerID=40&md5=bf46f13657297fa69e05c5fdb3f009b6,"Disintegration is a relation on measures and a transformation on probabilistic programs that generalizes density calculation and conditioning, two operations widely used for exact and approximate inference. Existing program transformations that find a disintegration or density automatically are limited to a fixed base measure that is an independent product of Lebesgue and counting measures, so they are of no help in practical cases that require tricky reasoning about other base measures. We present the first disintegrator that handles variable base measures, including discrete-continuous mixtures, dependent products, and disjoint sums. By analogy with type inference, our disintegrator can check a given base measure as well as infer an unknown one that is principal. We derive the disintegrator and prove it sound by equational reasoning from semantic specifications. It succeeds in a variety of applications where disintegration and density calculation had not been previously mechanized. © 2020 Owner/Author.",conditional distributions; density functions; measure kernels; Probabilistic programs,Geodesy; Semantics; Continuous mixtures; Equational reasoning; Exact and approximate inferences; Fixed base; Probabilistic programs; Program transformations; Semantic specification; Type inferences; Disintegration
A Principled Approach to Selective Context Sensitivity for Pointer Analysis,2020,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088822416&doi=10.1145%2f3381915&partnerID=40&md5=0276bae5def490e0ef850787a649a906,"Context sensitivity is an essential technique for ensuring high precision in static analyses. It has been observed that applying context sensitivity partially, only on a select subset of the methods, can improve the balance between analysis precision and speed. However, existing techniques are based on heuristics that do not provide much insight into what characterizes this method subset. In this work, we present a more principled approach for identifying precision-critical methods, based on general patterns of value flows that explain where most of the imprecision arises in context-insensitive pointer analysis. Using this theoretical foundation, we present an efficient algorithm, ZIPPER, to recognize these flow patterns in a given program and employ context sensitivity accordingly. We also present a variant, ZIPPERe, that additionally takes into account which methods are disproportionally costly to analyze with context sensitivity. Our experimental results on standard benchmark and real-world Java programs show that ZIPPER preserves effectively all of the precision (98.8%) of a highly precise conventional context-sensitive pointer analysis (2-object-sensitive with a context-sensitive heap, 2obj for short), with a substantial speedup (on average, 3.4× and up to 9.4×), and that ZIPPERe preserves 94.7% of the precision of 2obj, with an order-of-magnitude speedup (on average, 25.5× and up to 88×). In addition, for 10 programs that cannot be analyzed by 2obj within a three-hour time limit, on average ZIPPERe can guide 2obj to finish analyzing them in less than 11 minutes with high precision compared to context-insensitive and introspective context-sensitive analyses. © 2020 ACM.",Java; points-to analysis; Static analysis,Computer software; Heuristic methods; Static analysis; Context sensitive; Context sensitivity; Context-sensitive analysis; General patterns; High-precision; Java program; Pointer analysis; Theoretical foundations; Fasteners
Generalized Points-to Graphs: A Precise and Scalable Abstraction for Points-to Analysis,2020,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088836103&doi=10.1145%2f3382092&partnerID=40&md5=f4197137da491f278dc888d170a4ea9e,"Computing precise (fully flow-and context-sensitive) and exhaustive (as against demand-driven) points-to information is known to be expensive. Top-down approaches require repeated analysis of a procedure for separate contexts. Bottom-up approaches need to model unknown pointees accessed indirectly through pointers that may be defined in the callers and hence do not scale while preserving precision. Therefore, most approaches to precise points-to analysis begin with a scalable but imprecise method and then seek to increase its precision. We take the opposite approach in that we begin with a precise method and increase its scalability. In a nutshell, we create naive but possibly non-scalable procedure summaries and then use novel optimizations to compact them while retaining their soundness and precision. For this purpose, we propose a novel abstraction called the generalized points-to graph (GPG), which views points-to relations as memory updates and generalizes them using the counts of indirection levels leaving the unknown pointees implicit. This allows us to construct GPGs as compact representations of bottom-up procedure summaries in terms of memory updates and control flow between them. Their compactness is ensured by strength reduction (which reduces the indirection levels), control flow minimization (which removes control flow edges while preserving soundness and precision), and call inlining (which enhances the opportunities of these optimizations). The effectiveness of GPGs lies in the fact that they discard as much control flow as possible without losing precision. This is the reason GPGs are very small even for main procedures that contain the effect of the entire program. This allows our implementation to scale to 158 kLoC for C programs. At a more general level, GPGs provide a convenient abstraction to represent and transform memory in the presence of pointers. Future investigations can try to combine it with other abstractions for static analyses that can benefit from points-to information. © 2020 ACM.",bottom-up interprocedural analysis; Flow-and context-sensitive interprocedural analysis; points-to analysis; procedure summaries,Abstracting; Static analysis; Bottom up approach; Compact representation; Context sensitive; Points-to analysis; Points-to graph; Precise method; Strength reduction; Top down approaches; C (programming language)
Erratum: Type-driven Gradual Security with References (ACM Transactions on Programming Languages and Systems 40:4 (16) DOI: 10.1145/3229061),2020,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088821353&doi=10.1145%2f3387725&partnerID=40&md5=24a05d0cb23771b8f7e94d0880a3e2f7,"In TOPLAS Volume 40, Issue 4, Article No. 16 (Toro et al., Type-driven Gradual Security with References), Figure 6 has an erroneous reduction rule.1 The corrected version of rule (r7) is as follows (the change is highlighted in gray): (Farmula Presented) As a consequence, the logical relations presented in Figures 9 and 10 have to be slightly modified and refactored as shown in Figures 1 and 2 (changes highlighted). The updated definition makes use of the following definition: (Farmula Presented) The proof of noninterference is updated accordingly; it is available in the updated companion technical report TR/DCC-2018-4/v2 (https://www.dcc.uchile.cl/reportes). © 2020 ACM.",,
Debugging Large-scale Datalog: A Scalable Provenance Evaluation Strategy,2020,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088828661&doi=10.1145%2f3379446&partnerID=40&md5=280079ff3d2a303eff874e3686dd63c1,"Logic programming languages such as Datalog have become popular as Domain Specific Languages (DSLs) for solving large-scale, real-world problems, in particular, static program analysis and network analysis. The logic specifications that model analysis problems process millions of tuples of data and contain hundreds of highly recursive rules. As a result, they are notoriously difficult to debug. While the database community has proposed several data provenance techniques that address the Declarative Debugging Challenge for Databases, in the cases of analysis problems, these state-of-the-art techniques do not scale. In this article, we introduce a novel bottom-up Datalog evaluation strategy for debugging: Our provenance evaluation strategy relies on a new provenance lattice that includes proof annotations and a new fixed-point semantics for semi-naïve evaluation. A debugging query mechanism allows arbitrary provenance queries, constructing partial proof trees of tuples with minimal height. We integrate our technique into Soufflé, a Datalog engine that synthesizes C++ code, and achieve high performance by using specialized parallel data structures. Experiments are conducted with DOOP/DaCapo, producing proof annotations for tens of millions of output tuples. We show that our method has a runtime overhead of 1.31× on average while being more flexible than existing state-of-the-art techniques. © 2020 ACM.",datalog; provenance; Static analysis,Computer circuits; Intelligent databases; Logic programming; Problem oriented languages; Query processing; Semantics; XML; Database community; Declarative debugging; Domain specific languages; Evaluation strategies; Logic specifications; Parallel data structures; State-of-the-art techniques; Static program analysis; C++ (programming language)
Reasoning about a machine with local capabilities: Provably safe stack and return pointer management,2019,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077757057&doi=10.1145%2f3363519&partnerID=40&md5=ae69080df3af7b97c1758547b446e16c,Capability machines provide security guarantees at machine level which makes them an interesting target for secure compilation schemes that provably enforce properties such as control-flow correctness and encapsulation of local state. We provide a formalization of a representative capability machine with local capabilities and study a novel calling convention. We provide a logical relation that semantically captures the guarantees provided by the hardware (a form of capability safety) and use it to prove control-flow correctness and encapsulation of local state. The logical relation is not specific to our calling convention and can be used to reason about arbitrary programs. © 2019 Association for Computing Machinery.,Capability machines; CHERI; Local capabilities; Logical relations; Secure compilation; Stack encapsulation; Well-bracketed control flow,Software engineering; CHERI; Control flows; Local capabilities; Logical relations; Secure compilation; Linguistics
Introduction to the special issue on ESOP 2018,2019,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077741028&doi=10.1145%2f3368252&partnerID=40&md5=8d8827a97624002c84a30caccfc9aa45,[No abstract available],,
Consistent subtyping for all,2019,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075715866&doi=10.1145%2f3310339&partnerID=40&md5=449ba45accb8ee8509bd515a9554d6d4,"Consistent subtyping is employed in some gradual type systems to validate type conversions. The original definition by Siek and Taha serves as a guideline for designing gradual type systems with subtyping. Polymorphic types à la System F also induce a subtyping relation that relates polymorphic types to their instantiations. However, Siek and Taha’s definition is not adequate for polymorphic subtyping. The first goal of this article is to propose a generalization of consistent subtyping that is adequate for polymorphic subtyping and subsumes the original definition by Siek and Taha. The new definition of consistent subtyping provides novel insights with respect to previous polymorphic gradual type systems, which did not employ consistent subtyping. The second goal of this article is to present a gradually typed calculus for implicit (higher-rank) polymorphism that uses our new notion of consistent subtyping. We develop both declarative and (bidirectional) algorithmic versions for the type system. The algorithmic version employs techniques developed by Dunfield and Krishnaswami for higher-rank polymorphism to deal with instantiation. We prove that the new calculus satisfies all static aspects of the refined criteria for gradual typing. We also study an extension of the type system with static and gradual type parameters, in an attempt to support a variant of the dynamic criterion for gradual typing. Assuming a coherence conjecture for the extended calculus, we show that the dynamic gradual guarantee of our source language can be reduced to that of λB, which, at the time of writing, is still an open question. Most of the metatheory of this article, except some manual proofs for the algorithmic type system and extensions, has been mechanically formalized using the Coq proof assistant. © 2019 Association for Computing Machinery.",And Phrases: Gradual typing; Consistent subtyping; Dynamic gradual guarantee; Implicit polymorphism,Calculations; Natural language processing systems; Polymorphism; Theorem proving; And Phrases: Gradual typing; Coq proof assistant; Higher-rank polymorphism; Polymorphic types; Source language; Subtyping relation; Subtypings; Type systems; Formal languages
Behavioural equivalence via modalities for algebraic effects,2019,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075744611&doi=10.1145%2f3363518&partnerID=40&md5=2e4463658233d690384d2759045fbeaf,"The article investigates behavioural equivalence between programs in a call-by-value functional language extended with a signature of (algebraic) effect-triggering operations. Two programs are considered as being behaviourally equivalent if they enjoy the same behavioural properties. To formulate this, we define a logic whose formulas specify behavioural properties. A crucial ingredient is a collection of modalities expressing effect-specific aspects of behaviour. We give a general theory of such modalities. If two conditions, openness and decomposability, are satisfied by the modalities, then the logically specified behavioural equivalence coincides with a modality-defined notion of applicative bisimilarity, which can be proven to be a congruence by a generalisation of Howe’s method. We show that the openness and decomposability conditions hold for several examples of algebraic effects: nondeterminism, probabilistic choice, global store, and input/output. © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Algebraic effects; And Phrases: Program equivalence; Applicative bisimilarity; Behavioural logic; Call-by-value; Functional programming; Howe’s method; Modalities,Computer circuits; Functional programming; Algebraic effects; Applicative bisimilarity; Behavioural logic; Call-by-value; Modalities; Program equivalence; S-method; Algebra
Modular product programs,2019,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075709491&doi=10.1145%2f3324783&partnerID=40&md5=0bf52a31d0d3b74a55d5df3ddfd40a98,"Many interesting program properties like determinism or information flow security are hyperproperties, that is, they relate multiple executions of the same program. Hyperproperties can be verified using relational logics, but these logics require dedicated tool support and are difficult to automate. Alternatively, constructions such as self-composition represent multiple executions of a program by one product program, thereby reducing hyperproperties of the original program to trace properties of the product. However, existing constructions do not fully support procedure specifications, for instance, to derive the determinism of a caller from the determinism of a callee, making verification non-modular. We present modular product programs, a novel kind of product program that permits hyperproperties in procedure specifications and, thus, can reason about calls modularly. We provide a general formalization of our product construction and prove it sound and complete. We demonstrate its expressiveness by applying it to information flow security with advanced features such as declassification and termination-sensitivity. Modular product programs can be verified using off-the-shelf verifiers; we have implemented our approach for both secure information flow and general hyperproperties using the Viper verification infrastructure. Our evaluation demonstrates that modular product programs can be used to prove hyperproperties for challenging examples in reasonable time. © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.","And Phrases: Hyperproperties, product programs",Security of data; Dedicated tools; Information flow security; Modular products; Program properties; Relational logic; Secure information flow; Sound and complete; Specifications
Faster algorithms for dynamic algebraicqueries in basic RSMs with constant treewidth,2019,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075619719&doi=10.1145%2f3363525&partnerID=40&md5=348a95e27075a4b31e921e831574b8c1,"Interprocedural analysis is at the heart of numerous applications in programming languages, such as alias analysis, constant propagation, and so on. Recursive state machines (RSMs) are standard models for interprocedural analysis. We consider a general framework with RSMs where the transitions are labeled from a semiring and path properties are algebraic with semiring operations. RSMs with algebraic path properties can model interprocedural dataflow analysis problems, the shortest path problem, the most probable path problem, and so on. The traditional algorithms for interprocedural analysis focus on path properties where the starting point is fixed as the entry point of a specific method. In this work, we consider possible multiple queries as required in many applications such as in alias analysis. The study of multiple queries allows us to bring in an important algorithmic distinction between the resource usage of the one-time preprocessing vs for each individual query. The second aspect we consider is that the control flow graphs for most programs have constant treewidth. Our main contributions are simple and implementable algorithms that support multiple queries for algebraic path properties for RSMs that have constant treewidth. Our theoretical results show that our algorithms have small additional one-time preprocessing but can answer subsequent queries significantly faster as compared to the current algorithmic solutions for interprocedural dataflow analysis. We have also implemented our algorithms and evaluated their performance for performing on-demand interprocedural dataflow analysis on various domains, such as for live variable analysis and reaching definitions, on a standard benchmark set. Our experimental results align with our theoretical statements and show that after a lightweight preprocessing, on-demand queries are answered much faster than the standard existing algorithmic approaches. © 2019 Copyright held by the owner/author(s).",Constant treewidth graphs; Dataflow analysis; Interprocedural analysis; Reachability and distance oracles; Reachability and shortest path,Algebra; Benchmarking; Flow graphs; Problem oriented languages; Query processing; Algorithmic solutions; Inter-procedural analysis; Interprocedural dataflow analysis; Reachability; Recursive state machines; Shortest path; Shortest path problem; Tree-width; Data flow analysis
Environmental bisimulations for probabilistic higher-order languages,2019,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073680603&doi=10.1145%2f3350618&partnerID=40&md5=91285b1e373f81250caf0d70eccbcd54,"Environmental bisimulations for probabilistic higher-order languages are studied. In contrastwith applicative bisimulations, environmental bisimulations are known to be more robust and do not require sophisticated techniques such as Howe's in the proofs of congruence. As representative calculi, call-by-name and call-by-value λ-calculus, and a (call-by-value) λ-calculus extended with references (i.e., a store) are considered. In each case, full abstraction results are derived for probabilistic environmental similarity and bisimilarity with respect to contextual preorder and contextual equivalence, respectively. Some possible enhancements of the (bi)simulations, as ""up-to techniques,"" are also presented. Probabilities force a number of modifications to the definition of environmental bisimulations in nonprobabilistic languages. Some of thesemodifications are specific to probabilities, others may be seen as general refinements of environmental bisimulations, applicable also to non-probabilistic languages. Several examples are presented, to illustrate the modifications and the differences. © 2019 Association for Computing Machinery.",Contextual equivalence; Environmental bisimulation; Imperative languages; Probabilistic lambda calculus,Biomineralization; Differentiation (calculus); Bisimulations; Call-by-value lambda calculus; Contextual equivalence; Environmental bisimulations; Higher-order languages; Imperative languages; Lambda calculus; Non-probabilistic; Calculations
On the impact of programming languages on code quality: A reproduction study,2019,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073681260&doi=10.1145%2f3340571&partnerID=40&md5=d292e06996af4ce4abf13f672a106c1a,"In a 2014 article, Ray, Posnett, Devanbu, and Filkov claimed to have uncovered a statistically significant association between 11 programming languages and software defects in 729 projects hosted on GitHub. Specifically, their work answered four research questions relating to software defects and programming languages. With data and code provided by the authors, the present article first attempts to conduct an experimental repetition of the original study. The repetition is only partially successful, due to missing code and issues with the classification of languages. The second part of this work focuses on their main claim, the association between bugs and languages, and performs a complete, independent reanalysis of the data and of the statistical modeling steps undertaken by Ray et al. in 2014. This reanalysis uncovers a number of serious flaws that reduce the number of languages with an association with defects down from 11 to only 4. Moreover, the practical effect size is exceedingly small. These results thus undermine the conclusions of the original study. Correcting the record is important, as many subsequent works have cited the 2014 article and have asserted, without evidence, a causal link between the choice of programming language for a given task and the number of software defects. Causation is not supported by the data at hand; and, in our opinion, even after fixing the methodological flaws we uncovered, too many unaccounted sources of bias remain to hope for a meaningful comparison of bug rates across languages. © 2019 Association for Computing Machinery.",Programming Languages on Code Quality,Cell proliferation; Codes (symbols); Defects; Code quality; Effect size; Main claims; Missing codes; Reanalysis; Research questions; Software defects; Statistical modeling; Modeling languages
Non-polynomial worst-case analysis of recursive programs,2019,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073684023&doi=10.1145%2f3339984&partnerID=40&md5=d5377d142ab698610af70cf633259519,"We study the problem of developing efficient approaches for proving worst-case bounds of non-deterministic recursive programs. Ranking functions are sound and complete for proving termination and worst-case bounds of non-recursive programs. First, we apply ranking functions to recursion, resulting in measure functions. We show that measure functions provide a sound and complete approach to prove worst-case bounds of non-deterministic recursive programs. Our second contribution is the synthesis of measure functions in nonpolynomial forms.We show that non-polynomial measure functions with logarithm and exponentiation can be synthesized through abstraction of logarithmic or exponentiation terms, Farkas Lemma, and Handelman's Theorem using linear programming. While previous methods obtain polynomial worst-case bounds, our approach can synthesize bounds of various forms including O(n logn) and O(nr ), where r is not an integer.We present experimental results to demonstrate that our approach can efficiently obtain worst-case bounds of classical recursive algorithms such as (i) Merge sort, Heap sort, and the divide-and-conquer algorithm for the Closest Pair problem, where we obtain O(n logn) worst-case bound, and (ii) Karatsuba's algorithm for polynomial multiplication and Strassen's algorithm for matrix multiplication, for which we obtain O(nr ) bounds such that r is not an integer and is close to the best-known bound for the respective algorithm. Besides the ability to synthesize non-polynomial bounds, we also show that our approach is equally capable of obtaining polynomial worst-case bounds for classical programs such as Quick sort and the dynamic programming algorithm for computing Fibonacci numbers. © 2019 Association for Computing Machinery.",Recursive programs; Worst-case analysis,Computation theory; Dynamic programming; Information retrieval; Linear programming; Number theory; Closest pair problem; Divide-and-conquer algorithm; Dynamic programming algorithm; MAtrix multiplication; Polynomial multiplication; Recursive algorithms; Recursive programs; Worst-case analysis; Polynomials
Analysis and optimization of task granularity on the Java virtual machine,2019,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069530660&doi=10.1145%2f3338497&partnerID=40&md5=97a370add07640402ed69911e78e2c07,"Task granularity, i.e., the amount of work performed by parallel tasks, is a key performance attribute of parallel applications. On the one hand, fine-grained tasks (i.e., small tasks carrying out few computations) may introduce considerable parallelization overheads. On the other hand, coarse-grained tasks (i.e., large tasks performing substantial computations) may not fully utilize the available CPU cores, leading to missed parallelization opportunities. In this article, we provide a better understanding of task granularity for task-parallel applications running on a single Java Virtual Machine in a shared-memory multicore. We present a new methodology to accurately and efficiently collect the granularity of each executed task, implemented in a novel profiler (available open-source) that collects carefully selected metrics from the whole system stack with low overhead, and helps developers locate performance and scalability problems. We analyze task granularity in the DaCapo, ScalaBench, and Spark Perf benchmark suites, revealing inefficiencies related to fine-grained and coarse-grained tasks in several applications. We demonstrate that the collected task-granularity profiles are actionable by optimizing task granularity in several applications, achieving speedups up to a factor of 5.90×. Our results highlight the importance of analyzing and optimizing task granularity on the Java Virtual Machine. © 2019 Association for Computing Machinery.",Actionable profiles; Java virtual machine; Performance analysis and optimization; Task granularity; Task parallelism; Vertical profiling,Benchmarking; Network security; Open source software; Open systems; Virtual machine; Actionable profiles; Java virtual machines; Performance analysis and optimizations; Task granularity; Task parallelism; Vertical profiling; Java programming language
Higher-order Demand-driven program analysis,2019,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068453150&doi=10.1145%2f3310340&partnerID=40&md5=e8a3bac66c3c503db1343ce5639f5986,"Developing accurate and efficient program analyses for languages with higher-order functions is known to be difficult. Here we define a new higher-order program analysis, Demand-Driven Program Analysis (DDPA), which extends well-known demand-driven lookup techniques found in first-order program analyses to higher-order programs. This task presents several unique challenges to obtain good accuracy, including the need for a new method for demand-driven lookup of non-local variable values. DDPA is flow- and context-sensitive and provably polynomial-time. To efficiently implement DDPA, we develop a novel pushdown automaton metaprogram-ming framework, the Pushdown Reachability automaton. The analysis is formalized and proved sound, and an implementation is described. © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Context-sensitive; Demand-driven; Flow-sensitive; Functional programming; Polynomial-time; Program analysis; Pushdown system,Polynomial approximation; Context sensitive; Demand-driven; Flow sensitive; Polynomial-time; Program analysis; Pushdown systems; Functional programming
Combinatorial register allocation and instruction scheduling,2019,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068443848&doi=10.1145%2f3332373&partnerID=40&md5=1bfb094d73ac0701d5dc0595aad83e47,"This article introduces a combinatorial optimization approach to register allocation and instruction scheduling, two central compiler problems. Combinatorial optimization has the potential to solve these problems optimally and to exploit processor-specific features readily. Our approach is the first to leverage this potential in practice: it captures the complete set of program transformations used in state-of-the-art compilers, scales to medium-sized functions of up to 1,000 instructions, and generates executable code. This level of practicality is reached by using constraint programming, a particularly suitable combinatorial optimization technique. Unison, the implementation of our approach, is open source, used in industry, and integrated with the LLVM toolchain. An extensive evaluation confirms that Unison generates better code than LLVM while scaling to medium-sized functions. The evaluation uses systematically selected benchmarks from MediaBench and SPEC CPU2006 and different processor architectures (Hexagon, ARM, MIPS). Mean estimated speedup ranges from 1.1% to 10% and mean code size reduction ranges from 1.3% to 3.8% for the different architectures. A significant part of this improvement is due to the integrated nature of the approach. Executing the generated code on Hexagon confirms that the estimated speedup results in actual speedup. Given a fixed time limit, Unison solves optimally functions of up to 946 instructions, nearly an order of magnitude larger than previous approaches. The results show that our combinatorial approach can be applied in practice to trade compilation time for code quality beyond the usual compiler optimization levels, identify improvement opportunities in heuristic algorithms, and fully exploit processor-specific features. © 2019 Association for Computing Machinery.",Combinatorial optimization; Instruction scheduling; Register allocation,Codes (symbols); Combinatorial optimization; Computer programming; Constraint theory; Function evaluation; Heuristic algorithms; Open source software; Optimization; Scheduling; Combinatorial approach; Combinatorial optimization techniques; Compiler optimizations; Constraint programming; Instruction scheduling; Processor architectures; Program transformations; Register allocation; Program compilers
Failure recovery in resilient X10,2019,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068438364&doi=10.1145%2f3332372&partnerID=40&md5=470febd869d5ef4ed4a25d6c4a3b9880,"Cloud computing has made the resources needed to execute large-scale in-memory distributed computations widely available. Specialized programming models, e.g., MapReduce, have emerged to offer transparent fault tolerance and fault recovery for specific computational patterns, but they sacrifice generality. In contrast, the Resilient X10 programming language adds failure containment and failure awareness to a general purpose, distributed programming language. A Resilient X10 application spans over a number of places. Its formal semantics precisely specify how it continues executing after a place failure. Thanks to failure awareness, the X10 programmer can in principle build redundancy into an application to recover from failures. In practice, however, correctness is elusive, as redundancy and recovery are often complex programming tasks. This article further develops Resilient X10 to shift the focus from failure awareness to failure recovery, from both a theoretical and a practical standpoint. We rigorously define the distinction between recoverable and catastrophic failures. We revisit the happens-before invariance principle and its implementation. We shift most of the burden of redundancy and recovery from the programmer to the runtime system and standard library. We make it easy to protect critical data from failure using resilient stores and harness elasticity—dynamic place creation—to persist not just the data but also its spatial distribution. We demonstrate the flexibility and practical usefulness of Resilient X10 by building several representative high-performance in-memory parallel application kernels and frameworks. These codes are 10× to 25× larger than previous Resilient X10 benchmarks. For each application kernel, the average runtime overhead of resiliency is less than 7%. By comparing application kernels written in the Resilient X10 and Spark programming models, we demonstrate that Resilient X10’s more general programming model can enable significantly better application performance for resilient in-memory distributed computations. © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.","X10, APGAS","Computer programming languages; Fault tolerance; Formal methods; Redundancy; Semantics; Application performance; Catastrophic failures; Computational patterns; Distributed computations; Distributed programming languages; General programming model; Parallel application; X10, APGAS; Recovery"
Pye: A framework for preciSE-yet-efficient just-in-time analyses for Java programs,2019,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068484351&doi=10.1145%2f3337794&partnerID=40&md5=024002e9df76344cf4061811d5a8d499,"Languages like Java and C# follow a two-step process of compilation: static compilation and just-in-time (JIT) compilation. As the time spent in JIT compilation gets added to the execution-time of the application, JIT compilers typically sacrifice the precision of program analyses for efficiency. The alternative of performing the analysis for the whole program statically ignores the analysis of libraries (available only at runtime), and thereby generates imprecise results. To address these issues, in this article, we propose a two-step (static+JIT) analysis framework called precise-yet-efficient (PYE) that helps generate precise analysis-results at runtime at a very low cost. PYE achieves the twin objectives of precision and performance during JIT compilation by using a two-pronged approach: (i) It performs expensive analyses during static compilation, while accounting for the unavailability of the runtime libraries by generating partial results, in terms of conditional values, for the input application. (ii) During JIT compilation, PYE resolves the conditions associated with these values, using the pre-computed conditional values for the libraries, to generate the final results. We have implemented the static and the runtime components of PYE in the Soot optimization framework and the OpenJDK HotSpot Server Compiler (C2), respectively. We demonstrate the usability of PYE by instantiating it to perform two context-, flow-, and field-sensitive heap-based analyses: (i) points-to analysis for null-dereference-check elimination; and (ii) escape analysis for synchronization elimination. We evaluate these instantiations against their corresponding state-of-the-art implementations in C2 over a wide range of benchmarks. The extensive evaluation results show that our strategy works quite well and fulfills both the promises it makes: enhanced precision while maintaining efficiency during JIT compilation. © 2019 Association for Computing Machinery.",Efficient program analysis; Java program analysis; Precise,Application programs; Efficiency; Just in time production; Libraries; Petroleum reservoir evaluation; Program compilers; Analysis frameworks; Corresponding state; Java program; Just-in-time compilation; Optimization framework; Precise; Program analysis; Synchronization elimination; Java programming language
Static identification of injection attacks in Java,2019,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068440875&doi=10.1145%2f3332371&partnerID=40&md5=619d2af5c9cbcaaa3fac9594a13eab97,"The most dangerous security-related software errors, according to the OWASP Top Ten 2017 list, affect web applications. They are potential injection attacks that exploit user-provided data to execute undesired operations: database access and updates (SQL injection); generation of malicious web pages (cross-site scripting injection); redirection to user-specified web pages (redirect injection); execution of OS commands and arbitrary scripts (command injection); loading of user-specified, possibly heavy or dangerous classes at run time (reflection injection); access to arbitrary files on the file system (path-traversal); and storing user-provided data into heap regions normally assumed to be shielded from the outside world (trust boundary violation). All these attacks exploit the same weakness: unconstrained propagation of data from sources that the user of a web application controls into sinks whose activation might trigger dangerous operations. Although web applications are written in a variety of languages, Java remains a frequent choice, in particular for banking applications, where security has tangible relevance. This article defines a unified, sound protection mechanism against such attacks, based on the identification of all possible explicit flows of tainteddata in Java code. Such flows can be arbitrarily complex, passing through dynamically allocated data structures in the heap. The analysis is based on abstract interpretation and is interprocedural, flow-sensitive, and context-sensitive. Its notion of taint applies to reference (non-primitive) types dynamically allocated in the heap and is object-sensitive and field-sensitive. The analysis works by translating the program into Boolean formulas that model all possible data flows. Its implementation, within the Julia analyzer for Java and Android, found injection security vulnerabilities in the Internet banking service and in the customer relationship management of large Italian banks, as well as in a set of open-source third-party applications. It found the command injection, which is at the origin of the 2017 Equifax data breach, one of the worst data breaches ever. For objective, repeatable results, this article also evaluates the implementation on two open-source security benchmarks: the Juliet Suite and the OWASP Benchmark for the automatic comparison of static analyzers for cybersecurity. We compared this technique against more than 10 other static analyzers, both free and commercial. The result of these experiments is that ours is the only analysis for injection that is sound (up to well-stated limitations such as multithreading and native code) and works on industrial code, and it is also much more precise than other tools. © 2019 Association for Computing Machinery.",Abstract interpretation; SQL injection attack; Static analysis; Taint analysis; Web application security; XSS,Abstracting; Application programs; Boolean algebra; Codes (symbols); Data flow analysis; Electronic document exchange; Model checking; Network security; Open source software; Program translators; Public relations; Static analysis; Websites; Abstract interpretations; Customer relationship management; Security vulnerabilities; Sql injection attacks; Static identifications; Taint analysis; Third party application (Apps); Web application security; Java programming language
Editorial,2019,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075732924&doi=10.1145%2f3324782&partnerID=40&md5=f71ac6e8a208d9b889ccfb40d98ac288,[No abstract available],,
CSS minification via constraint solving,2019,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075692548&doi=10.1145%2f3310337&partnerID=40&md5=b9a31af86ab4e57027b0830fb5a0593c,"Minification is a widely accepted technique that aims at reducing the size of the code transmitted over the web. This article concerns the problem of semantic-preserving minification of Cascading Style Sheets (CSS)-the de facto language for styling web documents-based on merging similar rules. The cascading nature of CSS makes the semantics of CSS files sensitive to the ordering of rules in the file. To automatically identify rule-merging opportunities that best minimise file size, we reduce the rule-merging problem to a problem concerning ""CSS-graphs,"" i.e., node-weighted bipartite graphs with a dependency ordering on the edges, where weights capture the number of characters. Constraint solving plays a key role in our approach. Transforming a CSS file into a CSS-graph problem requires us to extract the dependency ordering on the edges (an NP-hard problem), which requires us to solve the selector intersection problem. To this end, we provide the first full formalisation of CSS3 selectors (the most stable version of CSS) and reduce their selector intersection problem to satisfiability of quantifier-free integer linear arithmetic, for which highly optimised SMT-solvers are available. To solve the above NP-hard graph optimisation problem, we show how Max-SAT solvers can be effectively employed. We have implemented our rule-merging algorithm and tested it against approximately 70 real-world examples (including examples from each of the top 20 most popular websites). We also used our benchmarks to compare our tool against six well-known minifiers (which implement other optimisations). Our experiments suggest that our tool produced larger savings. A substantially better minification rate was shown when our tool is used together with these minifiers. © 2019 ACM.",Cascading style sheets; Max-SAT; Semantics; Web-optimisation,Computational complexity; Graph theory; Logic programming; Merging; Semantic Web; Semantics; Cascading style sheets; Constraint Solving; Intersection problem; Linear arithmetic; Max-SAT; Merging algorithms; Optimisation problems; Optimisations; Optimization
A machine-learning algorithm with disjunctive model for data-driven program analysis,2019,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075699476&doi=10.1145%2f3293607&partnerID=40&md5=56ebcbf230ee9d92fa0e820110f0ae10,"We present a new machine-learning algorithm with disjunctive model for data-driven program analysis. One major challenge in static program analysis is a substantial amount of manual effort required for tuning the analysis performance. Recently, data-driven program analysis has emerged to address this challenge by automatically adjusting the analysis based on data through a learning algorithm. Although this new approach has proven promising for various program analysis tasks, its effectiveness has been limited due to simpleminded learning models and algorithms that are unable to capture sophisticated, in particular disjunctive, program properties. To overcome this shortcoming, this article presents a new disjunctive model for datadriven program analysis aswell as a learning algorithm to find the model parameters. Ourmodel uses Boolean formulas over atomic features and therefore is able to express nonlinear combinations of program properties. A key technical challenge is to efficiently determine a set of good Boolean formulas, as brute-force search would simply be impractical. We present a stepwise and greedy algorithm that efficiently learns Boolean formulas. We show the effectiveness and generality of our algorithm with two static analyzers: Contextsensitive points-to analysis for Java and flow-sensitive interval analysis for C. Experimental results show that our automated technique significantly improves the performance of the state-of-the-art techniques including ones hand-crafted by human experts. © 2019 ACM.",Context-sensitivity; Data-driven program analysis; Flow-sensitivity; Static analysis,Boolean algebra; Machine learning; Static analysis; Automated techniques; Context sensitivity; Flow sensitivity; Nonlinear combination; Program analysis; State-of-the-art techniques; Static program analysis; Technical challenges; Learning algorithms
"ML, visibly pushdown class memory automata, and extended branching vector addition systems with states",2019,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065707879&doi=10.1145%2f3310338&partnerID=40&md5=ca4ec236d8890f3c6d7e685905e20930,"We prove that the observational equivalence problem for a finitary fragment of the programming langauge ML is recursively equivalent to the reachability problem for extended branching vector addition systems with states (EBVASS). This result has two natural and independent parts. We first prove that the observational equivalence problem is equivalent to the emptiness problem for a new class of class memory automata equipped with a visibly pushdown stack, called Visibly Pushdown Class Memory Automata (VPCMA). Our proof uses the fully abstract game semantics of the language. We then prove that the VPCMA emptiness problem is equivalent to the reachability problem for EBVASS. The results of this article complete our programme to give an automata classification of the ML types with respect to the observational equivalence problem for closed terms. © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Automata over infinite alphabets; Full abstraction; Game semantics; Higher-order types; Vector addition systems,Abstracting; Automata theory; Petri nets; Semantics; Vectors; Automata over infinite alphabets; Full abstraction; Game semantics; Higher order types; Vector addition systems; Equivalence classes
Rethinking incremental and parallel pointer analysis,2019,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062636939&doi=10.1145%2f3293606&partnerID=40&md5=ebb07684e480ec0c07a9ec1bda87dfad,"Pointer analysis is at the heart of most interprocedural program analyses.However, scaling pointer analysis to large programs is extremely challenging. In this article, we study incremental pointer analysis and present a new algorithm for computing the points-to information incrementally (i.e., upon code insertion, deletion, and modification). Underpinned by new observations of incremental pointer analysis, our algorithm significantly advances the state of the art in that it avoids redundant computations and the expensive graph reachability analysis, and preserves precision as the corresponding whole program exhaustive analysis. Moreover, it is parallel within each iteration of fixed-point computation. We have implemented our algorithm, IPA, for Java based on the WALA framework and evaluated its performance extensively on real-world large, complex applications. Experimental results show that IPA achieves more than 200X speedups over existing incremental algorithms, two to five orders of magnitude faster than whole program pointer analysis, and also improves the performance of an incremental data race detector by orders of magnitude. Our IPA implementation is open source and has been adopted by WALA. © 2019 Copyright held by the owner/author(s).",Dynamic graph algorithms; Incremental pointer analysis; Parallelization,Open source software; Complex applications; Dynamic graph algorithms; Fixed-point computation; Incremental algorithm; Parallelizations; Pointer analysis; Reachability analysis; Redundant computation; Iterative methods
Practical subtyping for curry-style languages,2019,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062601652&doi=10.1145%2f3285955&partnerID=40&md5=eb6aebfd72d9ddc8ad3d99e508e6bccc,"We present a new, syntax-directed framework for Curry-style type systems with subtyping. It supports a rich set of features, and allows for a reasonably simple theory and implementation. The system we consider has sum and product types, universal and existential quantifiers, and inductive and coinductive types. The latter two may carry size invariants that can be used to establish the termination of recursive programs. For example, the termination of quicksort can be derived by showing that partitioning a list does not increase its size. The system deals with complex programs involving mixed induction and coinduction, or even mixed polymorphism and (co-)induction. One of the key ideas is to separate the notion of size from recursion.We do not check the termination of programs directly, but rather show that their (circular) typing proofs are wellfounded. Termination is then obtained using a standard (semantic) normalisation proof. To demonstrate the practicality of the system, we provide an implementation accepting all the examples discussed in the article. ©2019 Copyright held by the owner/author(s).",Choice operators; Circular proofs; Curry-style quantifiers; Dot notation for abstract types; Existential types; Inductive and coinductive sized types; Polymorphism; Realizability semantics; Reducibility candidates; Size change principle; Subtyping; Syntax-directed type system,Abstracting; Polymorphism; Semantics; Syntactics; Abstract types; Circular proofs; Curry-style quantifiers; Existential types; Inductive and coinductive sized types; Realizability semantics; Reducibility candidates; Size-change principle; Subtypings; Type systems; Formal languages
Context-free session type inference,2019,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063233033&doi=10.1145%2f3229062&partnerID=40&md5=b3537356a16763dbc0d30101851c2217,"Some interesting communication protocols can be precisely described only by context-free session types, an extension of conventional session types supporting a general form of sequential composition. The complex metatheory of context-free session types, however, hinders the definition of corresponding checking and inference algorithms. In this work, we study a new syntax-directed type system for context-free session types that is easy to embed into a host programming language.We also detail 2 OCaml embeddings that allow us to piggyback on OCaml's type system to check and infer context-free session types. © 2019 Association for Computing Machinery.",Context-free session types; Existential types; OCaml; Session type inference,Linguistics; Software engineering; Context-free; Existential types; Inference algorithm; Meta-theory; OCaml; Sequential compositions; Session types; Type systems; Inference engines
A classical sequent calculus with dependent types,2019,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063213368&doi=10.1145%2f3230625&partnerID=40&md5=4dc1008fdfb0cae5a361e46165d8ff30,"Dependent types are a key feature of the proof assistants based on the Curry-Howard isomorphism. It is well known that this correspondence can be extended to classical logic by enriching the language of proofs with control operators. However, they are known to misbehave in the presence of dependent types, unless dependencies are restricted to values. Moreover, while sequent calculi naturally support continuation-passing-style interpretations, there is no such presentation of a language with dependent types. The main achievement of this article is to give a sequent calculus presentation of a call-by-value language with a control operator and dependent types, and to justify its soundness through a continuation-passing-style translation. We start from the call-by-value version of the λμμ-calculus. We design a minimal language with a value restriction and a type system that includes a list of explicit dependencies to maintain type safety. We then show how to relax the value restriction and introduce delimited continuations to directly prove the consistency by means of a continuation-passing-style translation. Finally, we relate our calculus to a similar system by Lepigre and present a methodology to transfer properties from this system to our own. © 2019 Copyright held by the owner/author(s).",Callby-value; Classical logic; Continuation-passing-style translation; Control operators; Delimited continuations; Dependent types; Sequent calculus; Value restriction,Biomineralization; Computer circuits; Differentiation (calculus); Translation (languages); Callby-value; Classical logic; Continuation-passing style; Control operators; Delimited continuations; Dependent types; Sequent calculus; Value restriction; Calculations
Probabilistic termination by monadic affine sized typing,2019,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063190102&doi=10.1145%2f3293605&partnerID=40&md5=ac80e52b13cb407b5ac886ebdaa7d8f9,"We introduce a system of monadic affine sized types, which substantially generalizes usual sized types and allows in this way to capture probabilistic higher-order programs that terminate almost surely. Going beyond plain, strong normalization without losing soundness turns out to be a hard task, which cannot be accomplished without a richer, quantitative notion of types, but also without imposing some affinity constraints. The proposed type system is powerful enough to type classic examples of probabilistically terminating programs such as random walks. The way typable programs are proved to be almost surely terminating is based on reducibility but requires a substantial adaptation of the technique. © 2019 Association for Computing Machinery.",Affine types; Almost-sure termination; Parametrized reducibility; Probabilistic lambda-calculus; Sized types,Calculations; Affine types; Almost-sure termination; Lambda calculus; Parametrized reducibility; Sized types; Differentiation (calculus)
Interconnectability of session-based logical processes,2018,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061257360&doi=10.1145%2f3242173&partnerID=40&md5=39f4ebc56026985318684e81caaddfb2,"In multiparty session types, interconnection networks identify which roles in a session engage in communication (i.e., two roles are connected if they exchange a message). In session-based interpretations of linear logic the analogue notion corresponds to determining which processes are composed, or cut, using compatible channels typed by linear propositions. In this work, we show that well-formed interactions represented in a session-based interpretation of classical linear logic (CLL) form strictly less-expressive interconnection networks than those of a multiparty session calculus. To achieve this result, we introduce a new compositional synthesis property dubbed partial multiparty compatibility (PMC), enabling us to build a global type denoting the interactions obtained by iterated composition of well-typed CLL threads. We then show that CLL composition induces PMC global types without circular interconnections between three (or more) participants. PMC is then used to define a new CLL composition rule that can form circular interconnections but preserves the deadlock-freedom of CLL. © 2018 Copyright is held by the owner/author(s). Publication rights licensed to ACM.",Classical linear logic; Multiparty sessions; Session types; Synthesis,Calculations; Computer circuits; Linear algebra; Synthesis (chemical); Classical linear logic; Composition rule; Compositional synthesis; Deadlock freedom; Interconnectability; Logical process; Multiparty sessions; Session types; Interconnection networks (circuit switching)
Optimal choice of when to garbage collect,2019,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059983568&doi=10.1145%2f3282438&partnerID=40&md5=83e725ea4fb73c4d83b057bc68fc29ef,"We consider the ultimate limits of program-specific garbage collector (GC) performance for real programs. We first characterize the GC schedule optimization problem. Based on this characterization, we develop a linear-time dynamic programming solution that, given a program run and heap size, computes an optimal schedule of collections for a non-generational collector. Using an analysis of a heap object graph of the program, we compute a property of heap objects that we call their pre-birth time. This information enables us to extend the non-generational GC schedule problem to the generational GC case in a way that also admits a dynamic programming solution with cost quadratic in the length of the trace (number of objects allocated). This improves our previously reported approximately optimal result. We further extend the two-generation dynamic program to any number of generations, allowing other generalizations as well. Our experimental results for two generations on traces from Java programs of the DaCapo benchmark suite show that there is considerable promise to reduce garbage collection costs for some programs by developing program-specific collection policies. For a given space budget, optimal schedules often obtain modest but useful time savings, and for a given time budget, optimal schedules can obtain considerable space savings. © 2019 Copyright held by the owner/author(s).",Automatic storage management; Garbage collection; Optimal schedules,Budget control; Computer software; Refuse collection; Storage management; Benchmark suites; Garbage collection; Garbage collectors; Generational collector; Optimal schedule; Programming solutions; Schedule optimizations; Schedule problems; Dynamic programming
"Transactional sapphire: Lessons in high-performance, on-the-fly garbage collection",2018,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061227939&doi=10.1145%2f3226225&partnerID=40&md5=306ce4cc84c5550e4ed243e11206ea2c,"Constructing a high-performance garbage collector is hard. Constructing a fully concurrent 'on-the-fly' compacting collector is much more so. We describe our experience of implementing the Sapphire algorithm as the first on-the-fly, parallel, replication copying, garbage collector for the Jikes RVM Java virtual machine (JVM). In part, we explain our innovations such as copying with hardware and software transactions, on-the-fly management of Java's reference types, and simple, yet correct, lock-free management of volatile fields in a replicating collector. We fully evaluate, for the first time, and using realistic benchmarks, Sapphire's performance and suitability as a low latency collector. An important contribution of this work is a detailed description of our experience of building an on-the-fly copying collector for a complete JVM with some assurance that it is correct. A key aspect of this is model checking of critical components of this complicated and highly concurrent system. © ACM 2018.",Concurrent garbage collection; Java; Model checking; Parallel garbage collection; Reference objects; Replicating garbage collection; Transactional memory,Benchmarking; Computer software; Java programming language; Refuse collection; Sapphire; Concurrent garbage collection; Garbage collection; Java; Parallel garbage collection; Reference objects; Transactional memory; Model checking
Type-driven gradual security with references,2018,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061239230&doi=10.1145%2f3229061&partnerID=40&md5=c2d624b0e48e80361639508fd324c615,"In security-typed programming languages, types statically enforce noninterference between potentially conspiring values, such as the arguments and results of functions. But to adopt static security types, like other advanced type disciplines, programmers face a steep wholesale transition, often forcing them to refactor working code just to satisfy their type checker. To provide a gentler path to security typing that supports safe and stylish but hard-to-verify programming idioms, researchers have designed languages that blend static and dynamic checking of security types. Unfortunately, most of the resulting languages only support static, type-based reasoning about noninterference if a program is entirely statically secured. This limitation substantially weakens the benefits that dynamic enforcement brings to static security typing. Additionally, current proposals are focused on languages with explicit casts and therefore do not fulfill the vision of gradual typing, according to which the boundaries between static and dynamic checking only arise from the (im)precision of type annotations and are transparently mediated by implicit checks. In this article, we present GSLRef, a gradual security-typed higher-order language with references. As a gradual language, GSLRef supports the range of static-to-dynamic security checking exclusively driven by type annotations, without resorting to explicit casts. Additionally, GSLRef lets programmers use types to reason statically about termination-insensitive noninterference in all programs, even those that enforce security dynamically. We prove that GSLRef satisfies all but one of Siek et al.'s criteria for gradually-typed languages, which ensure that programs can seamlessly transition between simple typing and security typing. A notable exception regards the dynamic gradual guarantee, which some specific programs must violate if they are to satisfy noninterference; it remains an open question whether such a language could fully satisfy the dynamic gradual guarantee. To realize this design, we were led to draw a sharp distinction between syntactic type safety and semantic type soundness, each of which constrains the design of the gradual language. © 2018 Association for Computing Machinery.",Gradual typing; Language-based security; Noninterference,Linguistics; Software engineering; Based reasonings; Dynamic checking; Dynamic security; Gradual typing; Higher-order languages; Language-based security; Noninterference; Programming idioms; Semantics
Feature-specific profiling,2018,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059645608&doi=10.1145%2f3275519&partnerID=40&md5=35e563b63743887eb2d904a8bec0c5df,"While high-level languages come with significant readability and maintainability benefits, their performance remains difficult to predict. For example, programmers may unknowingly use language features inappropriately, which cause their programs to run slower than expected. To address this issue, we introduce feature-specific profiling, a technique that reports performance costs in terms of linguistic constructs. Feature-specific profilers help programmers find expensive uses of specific features of their language. We describe the architecture of a profiler that implements our approach, explain prototypes of the profiler for two languages with different characteristics and implementation strategies, and provide empirical evidence for the approach's general usefulness as a performance debugging tool. © 2018 Copyright held by the owner/author(s).",Performance monitoring; Profiling,Program debugging; Implementation strategies; Language features; Performance costs; Performance debugging; Performance monitoring; Profiling; High level languages
Rigorous estimation of floating-point round-off errors with symbolic Taylor expansions,2018,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058786854&doi=10.1145%2f3230733&partnerID=40&md5=0f4e0a29cc55295f8886d4a865b07b56,"Rigorous estimation of maximum floating-point round-off errors is an important capability central to many formal verification tools. Unfortunately, available techniques for this task often provide very pessimistic overestimates, causing unnecessary verification failure. We have developed a new approach called Symbolic Taylor Expansions that avoids these problems, and implemented a new tool called FPTaylor embodying this approach. Key to our approach is the use of rigorous global optimization, instead of the more familiar interval arithmetic, affine arithmetic, and/or SMT solvers. FPTaylor emits per-instance analysis certificates in the form of HOL Light proofs that can be machine checked. In this article, we present the basic ideas behind Symbolic Taylor Expansions in detail. We also survey as well as thoroughly evaluate six tool families, namely, Gappa (two tool options studied), Fluctuat, PRECiSA, Real2Float, Rosa, and FPTaylor (two tool options studied) on 24 examples, running on the same machine, and taking care to find the best options for running each of these tools. This study demonstrates that FPTaylor estimates round-off errors within much tighter bounds compared to other tools on a significant number of case studies. We also release FPTaylor along with our benchmarks, thus contributing to future studies and tool development in this area. © Copyright is held by the owner/author(s). Publication rights licensed to ACM.",Floating-point arithmetic; Formal verification; Global optimization; IEEE floating-point standard; Mixed-precision arithmetic; Round-off error,Errors; Expansion; Formal verification; Global optimization; Taylor series; Affine arithmetic; Formal verification tools; IEEE floating point; Interval arithmetic; Mixed precision; Rigorous global optimizations; Round-off errors; Taylor expansions; Digital arithmetic
Dynamic deadlock verification for general barrier synchronisation,2018,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058797864&doi=10.1145%2f3229060&partnerID=40&md5=5d515eb6f5813a625b9c2cd1c4f6e65b,"We present Armus, a verification tool for dynamically detecting or avoiding barrier deadlocks. The core design of Armus is based on phasers, a generalisation of barriers that supports split-phase synchronisation, dynamic membership, and optional-waits. This allows Armus to handle the key barrier synchronisation patterns found in modern languages and libraries. We implement Armus for X10 and Java, giving the first sound and complete barrier deadlock verification tools in these settings. Armus introduces a novel event-based graph model of barrier concurrency constraints that distinguishes task-event and event-task dependencies. Decoupling these two kinds of dependencies facilitates the verification of distributed barriers with dynamic membership, a challenging feature of X10. Further, our base graph representation can be dynamically switched between a task-to-task model, Wait-for Graph (WFG), and an event-to-event model, State Graph (SG), to improve the scalability of the analysis. Formally, we show that the verification is sound and complete with respect to the occurrence of deadlock in our core phaser language, and that switching graph representations preserves the soundness and completeness properties. These results are machine checked with the Coq proof assistant. Practically, we evaluate the runtime overhead of our implementations using three benchmark suites in local and distributed scenarios. Regarding deadlock detection, distributed scenarios show negligible overheads and local scenarios show overheads below 1.15×. Deadlock avoidance is more demanding, and highlights the potential gains from dynamic graph selection. In one benchmark scenario, the runtime overheads vary from 1.8× for dynamic selection, 2.6× for SG-static selection, and 5.9× for WFG-static selection. © Copyright 2018 held by Owner/Author",Barrier synchronisation; Deadlock avoidance; Deadlock detection; Java; Phasers; X10,Synchronization; Theorem proving; Concurrency constraints; Deadlock avoidance; Deadlock detection; Graph representation; Java; Phasers; Soundness and completeness; Synchronisation patterns; Java programming language
Erratum: Cross-Language Interoperability in a Multi-Language Runtime (ACM Transactions on Programming Languages and Systems 40:2 DOI: 10.1145/3201898),2018,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185257866&doi=10.1145%2f3283723&partnerID=40&md5=d4f7e0d081ca43765085eb469a9a9194,"In TOPLAS Volume 40, Issue 2, Article No. 8 (Grimmer et al., Cross-Language Interoperability in a Multi-Language Runtime) the name of the last author, Prof. Hanspeter M ssenb ck, Johannes Kepler University, was erroneously left off the article. © 2018 Association for Computing Machinery. All rights reserved.",,
Adaptive static analysis via learning with Bayesian optimization,2018,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057210497&doi=10.1145%2f3121135&partnerID=40&md5=30b600705774ff89880674e1fd82e5bf,"Building a cost-effective static analyzer for real-world programs is still regarded an art. One key contributor to this grim reputation is the difficulty in balancing the cost and the precision of an analyzer. An ideal analyzer should be adaptive to a given analysis task and avoid using techniques that unnecessarily improve precision and increase analysis cost. However, achieving this ideal is highly nontrivial, and it requires a large amount of engineering efforts. In this article, we present a new learning-based approach for adaptive static analysis. In our approach, the analysis includes a sophisticated parameterized strategy that decides, for each part of a given program, whether to apply a precision-improving technique to that part or not. We present a method for learning a good parameter for such a strategy from an existing codebase via Bayesian optimization. The learnt strategy is then used for new, unseen programs. Using our approach, we developed partially flow- and context-sensitive variants of a realistic C static analyzer. The experimental results demonstrate that using Bayesian optimization is crucial for learning from an existing codebase. Also, they show that among all program queries that require flow- or context-sensitivity, our partially flow- and context-sensitive analysis answers 75% of them, while increasing the analysis cost only by 3.3× of the baseline flow- and context-insensitive analysis, rather than 40× or more of the fully sensitive version. © 2018 Association for Computing Machinery.",Bayesian optimization; Data-driven program analysis; Static program analysis,Balancing; Cost effectiveness; Static analysis; Bayesian optimization; Context sensitivity; Context-insensitive analysis; Context-sensitive analysis; Improving techniques; Learning-based approach; Program analysis; Static program analysis; Cost benefit analysis
Corrigendum: Cross-language interoperability in a multi-language runtime (ACM Transactions on Programming Languages and Systems 40:2 (8) DOI: 10.1145/3201898),2018,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057146098&doi=10.1145%2f3283723&partnerID=40&md5=a15663989d77fd43602a2411ad58f85f,"In TOPLAS Volume 40, Issue 2, Article No. 8 (Grimmer et al., Cross-Language Interoperability in a Multi-Language Runtime) the name of the last author, Prof. Hanspeter Mössenböck, Johannes Kepler University, was erroneously left off the article. © 2018 Copyright is held by the owner/author(s).",,
"Editor's foreword to ""static Backward Slicing of Non-Deterministic Programs and Systems""",2018,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053401221&doi=10.1145%2f3243871&partnerID=40&md5=2c4fa2c725d89a705b93eacc1770e73a,[No abstract available],,
Static backward slicing of non-deterministic programs and systems,2018,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053406238&doi=10.1145%2f2886098&partnerID=40&md5=7babd78056fcb7732e244e3a24d83339,"A theory of slicing non-deterministic programs and systems is developed. Non-deterministic programs and systems are represented as non-deterministic program graphs (NDPGs) that allowarbitrary non-deterministic branching to be expressed. Structural and semantic relationships that must exist between an NDPG and (1) its non-termination insensitive (weak) slices and (2) its non-termination sensitive (strong) slices are defined. Weak and strong commitment closure are introduced. These are the NDPG equivalents of being closed under non-termination sensitive and non-termination insensitive control dependence; properties defined on subsets of vertices of the equivalent deterministic structure: The control flow graph. It is proved that if a subset of the vertices of an NDPG is both data dependence closed and (weak/strong) commitment closed, then the resulting induced graphwill, indeed, satisfy our structural and semantic requirements. O(n3) algorithms for computing minimal data andweak/strong commitment closed sets are given. The resulting induced graphs are thus guaranteed to be weak and strong slices, respectively. It is demonstrated, with examples, that programs written in Dijkstra's non-deterministic guarded command language (DNGCL) can be converted to NDPGs to which our slicing algorithms can then be applied. It is proved that the resulting slices (NDPGs) can always be converted back to valid DNGCL programs, highlighting the applicability of our approach to slicing at the source code level. © 2018 ACM.",Control Dependence; Program Slicing,Computation theory; Data flow analysis; Semantics; Backward slicing; Control flow graphs; Guarded commands; Non terminations; Nondeterministic programs; Program slicing; Semantic relationships; Slicing algorithms; Flow graphs
POP-PL: A patient-oriented prescription programming language,2018,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053415971&doi=10.1145%2f3210256&partnerID=40&md5=a84e56567ea4c0264a1e02fe14f11f6e,"A medical prescription is a set of health care instructions that govern the plan of care for an individual patient, which may include orders for drug therapy, diet, clinical assessment, and laboratory testing. Clinicians have long used algorithmic thinking to describe and implement prescriptions but without the benefit of a formal programming language. Instead, medical algorithms are expressed using a natural language patois, flowcharts, or as structured data in an electronic medical record system. The lack of a prescription programming language inhibits expressiveness; results in prescriptions that are difficult to understand, hard to debug, and awkward to reuse; and increases the risk of fatal medical error. This article reports on the design and evaluation of Patient-Oriented Prescription Programming Language (POP-PL), a domain-specific programming language designed for expressing prescriptions. The language is based around the idea that programs and humans have complementary strengths that, when combined properly, can make for safer, more accurate performance of prescriptions. Use of POP-PL facilitates automation of certain low-level vigilance tasks, freeing up human cognition for abstract thinking, compassion, and human communication. We implemented this language and evaluated its design attempting to write prescriptions in the new language and evaluated its usability by assessing whether clinicians can understand and modify prescriptions written in the language. We found that some medical prescriptions can be expressed in a formal domainspecific programming language, and we determined that medical professionals can understand and correctly modify programs written in POP-PL. We also discuss opportunities for refining and further developing POP-PL. © 2018 ACM.",DSL design; Empirical evaluation; Medical prescriptions; Medical programming languages,Abstracting; Ada (programming language); Drug therapy; Medical computing; Natural language processing systems; Clinical assessments; Design and evaluations; Domain specific programming languages; Electronic medical record system; Empirical evaluations; Human communications; Medical prescription; Medical professionals; Algorithmic languages
Algorithms for algebraic path properties in concurrent systems of constant treewidth components,2018,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053432531&doi=10.1145%2f3210257&partnerID=40&md5=022bc2ff7b9663278228982369b161b5,"We study algorithmic questions wrt algebraic path properties in concurrent systems, where the transitions of the system are labeled from a complete, closed semiring. The algebraic path properties can model dataflow analysis problems, the shortest path problem, andmany other natural problems that arise in program analysis. We consider that each component of the concurrent system is a graph with constant treewidth, a property satisfied by the controlflow graphs of most programs. We allow for multiple possible queries, which arise naturally in demand driven dataflow analysis. The study of multiple queries allows us to consider the tradeoff between the resource usage of the one-time preprocessing and for each individual query. The traditional approach constructs the product graph of all components and applies the best-known graph algorithm on the product. In this approach, even the answer to a single query requires the transitive closure (i.e., the results of all possible queries), which provides no room for tradeoff between preprocessing and query time. Our main contributions are algorithms that significantly improve the worst-case running time of the traditional approach, and provide various tradeoffs depending on the number of queries. For example, in a concurrent system of two components, the traditional approach requires hexic time in the worst case for answering one query as well as computing the transitive closure, whereas we show that with one-time preprocessing in almost cubic time, each subsequent query can be answered in at most linear time, and even the transitive closure can be computed in almost quartic time. Furthermore, we establish conditional optimality results showing that the worst-case running time of our algorithms cannot be improved without achieving major breakthroughs in graph algorithms (i.e., improving the worst-case bound for the shortest path problem in general graphs). Preliminary experimental results show that our algorithms perform favorably on several benchmarks. © 2018 ACM.",Algebraic path properties; Concurrent systems; Constant-treewidth graphs; Dataflow analysis; Shortest path,Algebra; Data flow analysis; Query processing; Concurrent systems; Control-flow graphs; Path properties; Shortest path; Shortest path problem; Traditional approaches; Transitive closure; Tree-width; Graph theory
A theoretical foundation of sensitivity in an abstract interpretation framework,2018,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053391621&doi=10.1145%2f3230624&partnerID=40&md5=b038910e48e5d1546542ae2af7be3db8,"Program analyses often utilize various forms of sensitivity such as context sensitivity, call-site sensitivity, and object sensitivity. These techniques all allowfor more precise program analyses, that are able to compute more precise program invariants, and to verify stronger properties. Despite the fact that sensitivity techniques are nowpart of the standard toolkit of static analyses designers and implementers, no comprehensive frameworks allow the description of all common forms of sensitivity. As a consequence, the soundness proofs of static analysis tools involving sensitivity often rely on ad hoc formalization, which are not always carried out in an abstract interpretation framework. Moreover, this also means that opportunities to identify similarities between analysis techniques to better improve abstractions or to tune static analysis tools can easily be missed. In this article, we present and formalize a framework for the description of sensitivity in static analysis. Our framework is based on a powerful abstract domain construction, and utilizes reduced cardinal power to tie basic abstract predicates to the properties analyses are sensitive to. We formalize this abstraction, and the main abstract operations that are needed to turn it into a generic abstract domain construction. We demonstrate that our approach can allow for a more precise description of program states, and that it can also describe a large set of sensitivity techniques, both when sensitivity criteria are static (known before the analysis) or dynamic (inferred as part of the analysis), and sensitive analysis tuning parameters. Last, we show that sensitivity techniques used in state-of-the-art static analysis tools can be described in our framework. © 2018 ACM.",Abstract interpretation; Analysis framework; Analysis sensitivity; Program analysis,Abstracting; Model checking; Static analysis; Abstract interpretations; Analysis frameworks; Analysis sensitivity; Analysis techniques; Context sensitivity; Program analysis; Sensitive analysis; Theoretical foundations; Sensitivity analysis
Modular termination verification of single-threaded and multithreaded programs,2018,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053370766&doi=10.1145%2f3210258&partnerID=40&md5=0cba7fdfdb873702862b5e3b629428df,"We propose an approach for the modular specification and verification of total correctness properties of object-oriented programs. The core of our approach is a specification style that prescribes a way to assign a level expression to each method such that each callee's level is below the caller's, even in the presence of dynamic binding. The specification style yields specifications that properly hide implementation details. The main idea is to use multisets of method names as levels, and to associate with each object levels that abstractly reflect the way the object is built from other objects. A method's level is then defined in terms of the method's own name and the levels associated with the objects passed as arguments. We first present the specification style in the context of programs that do not modify object fields.We then combine it with separation logic and abstract predicate families to obtain an approach for programs with heap mutation. In a third step, we address concurrency, by incorporating an existing approach for verifying deadlock freedom of channels and locks. Our main contribution here is to achieve information hiding by using the proposed termination levels for lock ordering as well. Also, we introduce call permissions to enable elegant verification of termination of programs where threads cause work in other threads, such as in thread pools or fine-grained concurrent algorithms involving compare-and-swap loops. We explain how our approach can be used also to verify the liveness of nonterminating programs. © 2018 ACM.",Modular program verification; Module specifications; Program termination; Separation logic,Computer circuits; Formal logic; Locks (fasteners); Multitasking; Specifications; Concurrent algorithms; Correctness properties; Modular programs; Modular specifications; Multi-threaded programs; Object-oriented program; Program termination; Separation logic; Object oriented programming
Program synthesis for program analysis,2018,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056865633&doi=10.1145%2f3174802&partnerID=40&md5=69bc32f07e5d746cd2d5391a745e94aa,"In this article, we propose a unified framework for designing static analysers based on program synthesis. For this purpose, we identify a fragment of second-order logic with restricted quantification that is expressive enough to model numerous static analysis problems (e.g., safety proving, bug finding, termination and non-termination proving, refactoring). As our focus is on programs that use bit-vectors, we build a decision procedure for this fragment over finite domains in the form of a program synthesiser. We provide instantiations of our framework for solving a diverse range of program verification tasks such as termination, non-termination, safety and bug finding, superoptimisation, and refactoring. Our experimental results show that our program synthesiser compares positively with specialised tools in each area as well as with general-purpose synthesisers. © 2018 Copyright is held by the owner/author(s). Publication rights licensed to ACM.",Program synthesis; Program termination,Linguistics; Software engineering; Analysis problems; Decision procedure; Non terminations; Program synthesis; Program termination; Program Verification; Second-order logic; Unified framework; Static analysis
Algorithmic analysis of qualitative and quantitative termination problems for affine probabilistic programs,2018,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056905036&doi=10.1145%2f3174800&partnerID=40&md5=ffbf20e9cca1b08f41a5db18e133bb12,"In this article, we consider the termination problem of probabilistic programs with real-valued variables. The questions concerned are: qualitative ones that ask (i) whether the program terminates with probability 1 (almost-sure termination) and (ii) whether the expected termination time is finite (finite termination); and quantitative ones that ask (i) to approximate the expected termination time (expectation problem) and (ii) to compute a bound B such that the probability not to terminate after B steps decreases exponentially (concentration problem). To solve these questions, we utilize the notion of ranking supermartingales, which is a powerful approach for proving termination of probabilistic programs. In detail, we focus on algorithmic synthesis of linear ranking-supermartingales over affine probabilistic programs (Apps) with both angelic and demonic non-determinism. An important subclass of Apps is LRApp which is defined as the class of all Apps over which a linear ranking-supermartingale exists. Our main contributions are as follows. Firstly, we show that the membership problem of LRApp (i) can be decided in polynomial time for Apps with at most demonic non-determinism, and (ii) is NP-hard and in PSPACE for Apps with angelic non-determinism. Moreover, the NP-hardness result holds already for Apps without probability and demonic non-determinism. Secondly, we show that the concentration problem over LRApp can be solved in the same complexity as for the membership problem of LRApp. Finally, we show that the expectation problem over LRApp can be solved in 2EXPTIME and is PSPACE-hard even for Apps without probability and non-determinism (i.e., deterministic programs). Our experimental results demonstrate the effectiveness of our approach to answer the qualitative and quantitative questions over Apps with at most demonic non-determinism. © 2018 ACM.",Concentration; Probabilistic programs; Ranking supermartingale; Termination,Concentration (process); High level synthesis; Polynomial approximation; Algorithmic analysis; Algorithmic synthesis; Deterministic programs; Probabilistic programs; Ranking supermartingale; Real-valued variables; Termination; Termination problems; Probability
A logical analysis of framing for specifications with pure method calls,2018,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056840127&doi=10.1145%2f3174801&partnerID=40&md5=683bc00b231029264cbd530da4adb894,"For specifying and reasoning about object-based programs, it is often attractive for contracts to be expressed using calls to pure methods. It is useful for pure methods to have contracts, including read effects, to support local reasoning based on frame conditions. This leads to puzzles such as the use of a pure method in its own contract. These ideas have been explored in connection with verification tools based on axiomatic semantics, guided by the need to avoid logical inconsistency, and focusing on encodings that cater for first-order automated provers. This article adds pure methods and read effects to region logic, a first-order program logic that features frame-based local reasoning and provides modular reasoning principles for end-to-end correctness. Modular reasoning is embodied in a proof rule for linking a module's method implementations with a client that relies on the method contracts. Soundness is proved with respect to conventional operational semantics and uses an extensional (i.e, relational) interpretation of read effects. Applicability to tools based on SMT solvers is demonstrated through machine-checked verification of examples. The developments in this article can guide the implementations of linking as used in modular verifiers and serve as a basis for studying observationally pure methods and encapsulation. © 2018 ACM.",Framing; Local reasoning; Pure methods; Read effect; Specifications,Semantics; Axiomatic semantics; End-to-end correctness; Framing; Local reasoning; Method implementations; Operational semantics; Pure methods; Read effect; Specifications
Cross-language interoperability in a multi-language runtime,2018,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054095374&doi=10.1145%2f3201898&partnerID=40&md5=d5030462d55a7ef70cdca600e2f2a3f8,"In large-scale software applications, programmers often combine different programming languages because this allows them to use the most suitable language for a given problem, to gradually migrate existing projects from one language to another, or to reuse existing source code. However, different programming languages have fundamentally different implementations, which are hard to combine. The composition of language implementations often results in complex interfaces between languages, insufficient flexibility, or poor performance. We propose TruffleVM, a virtual machine (VM) that can execute different programming languages and is able to compose them in a seamless way. TruffleVM supports dynamically-typed languages (e.g., JavaScript and Ruby) as well as statically typed low-level languages (e.g., C). It consists of individual language implementations, which translate source code to an intermediate representation that is executed by a shared VM. TruffleVM composes these different language implementations via generic access. Generic access is a languageagnostic mechanism that language implementations use to access foreign data or call foreign functions. It features language-agnostic messages that the TruffleVM resolves to efficient foreign-language-specific operations at runtime. Generic access supports multiple languages, enables an efficient multi-language development, and ensures high performance. We evaluate generic access with two case studies. The first one explains the transparent composition of JavaScript, Ruby, and C. The second one shows an implementation of the C extensions application programming interface (API) for Ruby. We show that generic access guarantees good runtime performance. It avoids conversion or marshalling of foreign objects at the language boundary and allows the dynamic compiler to perform its optimizations across language boundaries. C 2018 ACM. Copyright © 2017 1nternational lntormation lnstitute.",Cross-language; Language implementation; Language interoperability; Optimization; Virtual machine,Application programming interfaces (API); Application programs; Computer software reusability; Computer systems programming; Interoperability; Linguistics; Network security; Optimization; Ruby; Speech recognition; Virtual machine; Cross languages; Dynamically typed languages; Intermediate representations; Language boundaries; Language implementations; Language interoperability; Large-scale software applications; Run-time performance; C (programming language)
Conditioning in probabilistic programming,2018,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040306627&doi=10.1145%2f3156018&partnerID=40&md5=14b32dd014aa7ba70bb5fb2561e06eac,"This article investigates the semantic intricacies of conditioning, a main feature in probabilistic programming. Our study is based on an extension of the imperative probabilistic guarded command language pGCL with conditioning.We provide a weakest precondition (wp) semantics and an operational semantics. To deal with possibly diverging program behavior, we consider liberal preconditions. We show that diverging program behavior plays a key role when defining conditioning.We establish that weakest preconditions coincide with conditional expected rewards in Markov chains-the operational semantics-and that the wp-semantics conservatively extends the existing semantics of pGCL (without conditioning). An extension of these results with nondeterminism turns out to be problematic: although an operational semantics using Markov decision processes is rather straightforward, we show that providing an inductive wp-semantics in this setting is impossible. Finally, we present two program transformations that eliminate conditioning from any program. The first transformation hoists conditioning while updating the probabilistic choices in the program, while the second transformation replaces conditioning-in the same vein as rejection sampling-by a program with loops. In addition, we present a last program transformation that replaces an independent identically distributed loop with conditioning. © 2018 ACM.",Conditioning; Operational semantics; Probabilistic programming; Weakest pre-condition semantics,Computer programming languages; Markov processes; Natural gas conditioning; Markov Decision Processes; Operational semantics; Probabilistic choices; Probabilistic programming; Program behavior; Program transformations; Rejection samplings; Weakest precondition; Semantics
Constraint-based refactoring,2018,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040307335&doi=10.1145%2f3156016&partnerID=40&md5=495bd37e587b88fb795e2bbab005e63f,"Constraint-based refactoring generalizes constraint-based type refactoring as introduced by Tip et al. [61] by extending the coverage of change from types to names, locations, accessibilities, and other properties of program elements. Starting with a generic specification of refactoring tools, we systematically develop constraint-based refactoring as a generic solution to a certain class of refactoring problems and provide a condition under which constraint-based refactoring tools are proven correct for any given target language. Although compliance with this correctness condition is hard to prove for target languages whose semantics is not formally defined, we show how the condition gives rise to automated testing procedures.We present a novel algorithm based on constraint-logic programming for the generation of constraints from a program to be refactored, and demonstrate its time and space requirements by using it in the application of refactorings to open source programs. Summarizing earlier work, we show how the principles underlying constraint-based refactoring tools extend to ad hoc refactoring, cross-language refactoring, and model/code co-refactoring. © 2018 ACM.",Constraint-based repair; Refactoring,Application programs; Logic programming; Open source software; Semantics; Software testing; Constraint Logic Programming; Constraint-based; Correctness conditions; Generic solutions; Generic specifications; Open source projects; Refactorings; Space requirements; Computer programming languages
Verifying reliability properties using the hyperball abstract domain,2017,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040309811&doi=10.1145%2f3156017&partnerID=40&md5=4638fd749372ccf491bc48cbd4a0a33c,"Modern systems are increasingly susceptible to soft errors that manifest themselves as bit flips and possibly alter the semantics of an application. We would like to measure the quality degradation on semantics due to such bit flips, and thus we introduce a Hyperball abstract domain that allows us to determine the worst-case distance between expected and actual results. Similar to intervals, hyperballs describe a connected and dense space. The semantics of low-level code in the presence of bit flips is hard to accurately describe in such a space. We therefore combine the Hyperball domain with an existing affine system abstract domain that we extend to handle bit flips, which are introduce as disjunctions. Bit-flips can reduce the precision of our analysis, and we therefor introduce the Scale domain as a disjunctive refinement to minimize precision loss. This domain bounds the number of disjunctive elements by quantifying the over-approximation of different partitions and uses submodular optimization to find a good partitioning (within a bound of optimal).We evaluate these domains to show benefits and potential problems. For the application we examine here, adding the Scale domain to the Hyperball abstraction improves accuracy by up to two orders of magnitude. Our initial results demonstrate the feasibility of this approach, although we would like to further improve execution efficiency. © 2017 ACM.",Abstract interpretation; Disjunctive refinement; Numerical abstraction; Reliability analysis,Abstracting; Radiation hardening; Semantics; Abstract interpretations; Disjunctive refinement; Numerical abstraction; Orders of magnitude; Potential problems; Quality degradation; Reliability properties; Submodular optimizations; Reliability analysis
Bit-precise procedure-modular termination analysis,2017,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85038950933&doi=10.1145%2f3121136&partnerID=40&md5=90c233c49545fc9b721767912466cd64,"Non-termination is the root cause of a variety of program bugs, such as hanging programs and denial-of-service vulnerabilities. This makes an automated analysis that can prove the absence of such bugs highly desirable. To scale termination checks to large systems, an interprocedural termination analysis seems essential. This is a largely unexplored area of research in termination analysis, where most effort has focussed on small but difficult single-procedure problems. We present a modular termination analysis for C programs using template-based interprocedural summari-sation. Our analysis combines a context-sensitive, over-approximating forward analysis with the inference of under-approximating preconditions for termination. Bit-precise termination arguments are synthesised over lexicographic linear ranking function templates. Our experimental results show the advantage of interprocedural reasoning over monolithic analysis in terms of efficiency, while retaining comparable precision. © 2017 Copyright is held by the owner/author(s).",Bit-precise analysis; Interprocedural analysis; Templates; Termination analysis,C (programming language); Automated analysis; Inter-procedural analysis; Lexicographic linear ranking; Monolithic analysis; Precise analysis; Templates; Termination analysis; Termination arguments; Program debugging
"A simple, possibly correct LR parser for C11",2017,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85033232387&doi=10.1145%2f3064848&partnerID=40&md5=929b59c4fd6f732fdeba74e09957b64e,"The syntax of the C programming language is described in the C11 standard by an ambiguous context-free grammar, accompanied with English prose that describes the concept of ""scope"" and indicates how certain ambiguous code fragments should be interpreted. Based on these elements, the problem of implementing a compliant C11 parser is not entirely trivial. We review the main sources of dificulty and describe a relatively simple solution to the problem. Our solution employs the well-known technique of combining an LALR(1) parser with a ""lexical feedback"" mechanism. It draws on folklore knowledge and adds several original aspects, including a twist on lexical feedback that allows a smooth interaction with lookahead; a simplified and powerful treatment of scopes; and a few amendments in the grammar. Although not formally verified, our parser avoids several pitfalls that other implementations have fallen prey to. We believe that its simplicity, its mostly declarative nature, and its high similarity with the C11 grammar are strong informal arguments in favor of its correctness. Our parser is accompanied with a small suite of ""tricky"" C11 programs. We hope that it may serve as a reference or a starting point in the implementation of compilers and analysis tools. © 2017 ACM.",Ambiguity; C11; C89; C99; Compilation; Lexical feedback; Parsing,Context free grammars; Context free languages; Problem oriented languages; Syntactics; Ambiguity; Ambiguous contexts; Analysis tools; Code fragments; Compilation; LR parsers; Parsing; C (programming language)
Scaling reliably: Improving the scalability of the Erlang distributed actor platform,2017,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028507134&doi=10.1145%2f3107937&partnerID=40&md5=5904bb1d0e4f3acb862eb58b1c7e5dca,"Distributed actor languages are an effective means of constructing scalable reliable systems, and the Erlang programming language has a well-established and influential model. While the Erlang model conceptually provides reliable scalability, it has some inherent scalability limits and these force developers to depart from the model at scale. This article establishes the scalability limits of Erlang systems and reports the work of the EU RELEASE project to improve the scalability and understandability of the Erlang reliable distributed actor model. We systematically study the scalability limits of Erlang and then address the issues at the virtual machine, language, and tool levels. More specifically: (1) We have evolved the Erlang virtual machine so that it can work effectively in large-scale single-host multicore and NUMA architectures. We have made important changes and architectural improvements to the widely used Erlang/OTP release. (2) We have designed and implemented Scalable Distributed (SD) Erlang libraries to address language-level scalability issues and provided and validated a set of semantics for the new language constructs. (3) To make large Erlang systems easier to deploy, monitor, and debug, we have developed and made open source releases of five complementary tools, some specific to SD Erlang. Throughout the article we use two case studies to investigate the capabilities of our new technologies and tools: a distributed hash table based Orbit calculation and Ant Colony Optimisation (ACO). Chaos Monkey experiments show that two versions of ACO survive random process failure and hence that SD Erlang preserves the Erlang reliability model. While we report measurements on a range of NUMA and cluster architectures, the key scalability experiments are conducted on the Athos cluster with 256 hosts (6,144 cores). Even for programs with no global recovery data to maintain, SD Erlang partitions the network to reduce network traffic and hence improves performance of the Orbit and ACO benchmarks above 80 hosts. ACO measurements show that maintaining global recovery data dramatically limits scalability; however, scalability is recovered by partitioning the recovery data. We exceed the established scalability limits of distributed Erlang, and do not reach the limits of SD Erlang for these benchmarks at this scale (256 hosts, 6,144 cores). © 2017 ACM.",Erlang; Reliability; Scalability,Ant colony optimization; Artificial intelligence; Benchmarking; Computer system recovery; Network architecture; Network security; Open source software; Open systems; Program debugging; Random processes; Recovery; Reliability; Scalability; Semantics; Virtual addresses; Virtual machine; Architectural improvements; Cluster architecture; Complementary tools; Distributed Hash Table; Erlang; Erlang programming language; Language constructs; Monkey experiments; Distributed computer systems
Precise predictive analysis for discovering communication deadlocks in MPI programs,2017,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028517083&doi=10.1145%2f3095075&partnerID=40&md5=02acb5b9fc2497852834847aa2def760,"The Message Passing Interface (MPI) is the standard API for parallelization in high-performance and scientific computing. Communication deadlocks are a frequent problem in MPI programs, and this article addresses the problem of discovering such deadlocks. We begin by showing that if an MPI program is single path, the problem of discovering communication deadlocks is NP-complete. We then present a novel propositional encoding scheme that captures the existence of communication deadlocks. The encoding is based onmodeling executions with partial orders and implemented in a tool called MOPPER. The tool executes an MPI program, collects the trace, builds a formula from the trace using the propositional encoding scheme, and checks its satisfiability. Finally, we present experimental results that quantify the benefit of the approach in comparison to other analyzers and demonstrate that it offers a scalable solution for single-path programs. © 2017 the owner/author(s).",Deadlock discovery; MPI programs; Single-path verification,Application programming interfaces (API); Encoding (symbols); Signal encoding; Deadlock discovery; Encoding schemes; Message passing interface; MPI programs; Parallelizations; Satisfiability; Scalable solution; Single path; Message passing
Sound non-statistical clustering of static analysis alarms,2017,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028504442&doi=10.1145%2f3095021&partnerID=40&md5=fbd36c4624d047b9eef0dba3ee7f35af,"We present a sound method for clustering alarms from static analyzers. Our method clusters alarms by discovering sound dependencies between them such that if the dominant alarms of a cluster turns out to be false, all the other alarms in the same cluster are guaranteed to be false. We have implemented our clustering algorithm on top of a realistic buffer-overflow analyzer and proved that our method reduces 45% of alarm reports. Our framework is applicable to any abstract interpretation-based static analysis and orthogonal to abstraction refinements and statistical ranking schemes. © 2017 ACM.",Abstract interpretation; False alarms; Static analysis,Abstracting; Alarm systems; Model checking; Static analysis; Abstract interpretations; Abstraction refinement; Buffer overflows; False alarms; Static analyzers; Statistical clustering; Statistical ranking; Clustering algorithms
SHErrLoc: A static holistic error locator,2017,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028501444&doi=10.1145%2f3121137&partnerID=40&md5=523228eb9afe759e931f7f6cbf8584d8,"We introduce a general way to locate programmer mistakes that are detected by static analyses. The program analysis is expressed in a general constraint language that is powerful enough to model type checking, information flow analysis, dataflow analysis, and points-to analysis. Mistakes in program analysis result in unsatisfiable constraints. Given an unsatisfiable system of constraints, both satisfiable and unsatisfiable constraints are analyzed to identify the program expressions most likely to be the cause of unsatisfiability. The likelihood of different error explanations is evaluated under the assumption that the programmer's code is mostly correct, so the simplest explanations are chosen, following Bayesian principles. For analyses that rely on programmer-stated assumptions, the diagnosis also identifies assumptions likely to have been omitted. The new error diagnosis approach has been implemented as a tool called SHErrLoc, which is applied to three very different program analyses, such as type inference for a highly expressive type system implemented by the Glasgow Haskell Compiler-including type classes, Generalized Algebraic Data Types (GADTs), and type families. The effectiveness of the approach is evaluated using previously collected programs containing errors. The results show that when compared to existing compilers and other tools, SHErrLoc consistently identifies the location of programmer errors significantly more accurately, without any language-specific heuristics. © 2017 ACM.",Error diagnosis; Haskell; Information flow; Jif; OCaml; Static program analysis; Type inference,Computer programming; Data flow analysis; Errors; Program diagnostics; Static analysis; Error diagnosis; Haskell; Information flows; OCaml; Static program analysis; Type inferences; Program compilers
Don't sit on the fence: A static analysis approach to automatic fence insertion,2017,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85023189138&doi=10.1145%2f2994593&partnerID=40&md5=08b8386133289fc3c5a4e161634b80c1,"Modern architectures rely on memory fences to prevent undesired weakenings of memory consistency. As the fences' semantics may be subtle, the automation of their placement is highly desirable. But precise methods for restoring consistency do not scale to deployed systems' code. We choose to trade some precision for genuine scalability: our technique is suitable for large code bases. We implement it in our new musketeer tool and report experiments on more than 700 executables from packages found in Debian GNU/Linux 7.1, including memcached with about 10,000 L°C. © 2017 ACM.",Concurrency; Program synthesis; Static analysis; Weak memory,Fences; Open source software; Semantics; Analysis approach; Concurrency; Deployed systems; Large code basis; Memory consistency; Modern architectures; Precise method; Program synthesis; Static analysis
From clarity to efficiency for distributed algorithms,2017,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027019485&doi=10.1145%2f2994595&partnerID=40&md5=d8881969b2e96a6d9825a64037d369b5,"This article describes a very high-level language for clear description of distributed algorithms and optimizations necessary for generating efficient implementations. The language supports high-level control flows in which complex synchronization conditions can be expressed using high-level queries, especially logic quantifications, over message history sequences. Unfortunately, the programs would be extremely inefficient, including consuming unbounded memory, if executed straightforwardly. We present new optimizations that automatically transform complex synchronization conditions into incremental updates of necessary auxiliary values as messages are sent and received. The core of the optimizations is the first general method for efficient implementation of logic quantifications. We have developed an operational semantics of the language, implemented a prototype of the compiler and the optimizations, and successfully used the language and implementation on a variety of important distributed algorithms. © 2017 ACM.",Distributed algorithms; High-level queries and updates; Incrementalization; Logic quantifications; Message histories; Synchronization conditions; Yield points,Computer circuits; Computer programming languages; High level languages; Parallel algorithms; Semantics; Synchronization; Complex synchronization; Efficient implementation; High-level queries; Incremental updates; Incrementalization; Logic quantifications; Operational semantics; Yield points; Optimization
The chemical approach to typestate-oriented programming,2017,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85022342871&doi=10.1145%2f3064849&partnerID=40&md5=0dd9d36c85b43227993f2dc34632d9b5,"We introduce a novel approach to typestate-oriented programming based on the chemical metaphor: state and operations on objects are molecules of messages, and state transformations are chemical reactions. This approach allows us to investigate typestate in an inherently concurrent setting, whereby objects can be accessed and modified concurrently by several processes, each potentially changing only part of their state. We introduce a simple behavioral type theory to express in a uniform way both the private and the public interfaces of objects; describe and enforce structured object protocols consisting of possibilities, prohibitions, and obligations; and control object sharing. © 2017 ACM.",Behavioral types; Concurrency; Join calculus; Typestate,Linguistics; Software engineering; Behavioral types; Chemical metaphor; Concurrency; Control objects; Join calculus; State transformation; Typestate; Chemical operations
Multiple facets for dynamic information flow with exceptions,2017,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019959132&doi=10.1145%2f3024086&partnerID=40&md5=c5bfc1a74454cd976a7ba0d4ca34271f,"JavaScript is the source of many security problems, including cross-site scripting attacks and malicious advertising code. Central to these problems is the fact that code from untrusted sources runs with full privileges. Information flow controls help prevent violations of data confidentiality and integrity. This article explores faceted values, a mechanism for providing information flow security in a dynamic manner that avoids the stuck executions of some prior approaches, such as the no-sensitive-upgrade technique. Faceted values simultaneously simulate multiple executions for different security levels to guarantee termination-insensitive noninterference. We also explore the interaction of faceted values with exceptions, declassification, and clearance. © 2017 ACM.",Dynamic analysis; Information flow control; JavaScript; Web security pages.,Dynamic analysis; High level languages; Security of data; Cross Site Scripting Attacks; Data confidentiality; Dynamic information; Information flow control; Information flow security; Javascript; Security problems; WEB security; Flow control
Verifying invariants of lock-free data structures with rely-guarantee and refinement types,2017,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019873752&doi=10.1145%2f3064850&partnerID=40&md5=192dc980bf42478a9405929772351d18,"Verifying invariants of fine-grained concurrent data structures is challenging, because interference from other threads may occur at any time. We propose a new way of proving invariants of fine-grained concurrent data structures: Applying rely-guarantee reasoning to references in the concurrent setting. Rely-guarantee applied to references can verify bounds on thread interference without requiring a whole program to be verified. This article provides three new results. First, it provides a new approach to preserving invariants and restricting usage of concurrent data structures. Our approach targets a space between simple type systems and modern concurrent program logics, offering an intermediate point between unverified code and full verification. Furthermore, it avoids sealing concurrent data structure implementations and can interact safely with unverified imperative code. Second, we demonstrate the approach's broad applicability through a series of case studies, using two implementations: An axiomatic COQ domain-specific language and a library for Liquid Haskell. Third, these two implementations allow us to compare and contrast verifications by interactive proof (COQ) and a weaker form that can be expressed using automatically-discharged dependent refinement types (Liquid Haskell). © 2017 ACM.",Concurrency; Refinement types; Rely-guarantee; Type systems; Verification,Computer programming languages; Data structures; Problem oriented languages; Theorem proving; Verification; Concurrency; Concurrent data structures; Concurrent program; Domain specific languages; Lock-free data structures; Refinement types; Rely guarantees; Type systems; Concurrency control
"On subtyping-relation completeness, with an application to iso-recursive types",2017,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016489106&doi=10.1145%2f2994596&partnerID=40&md5=56d457d1847396d97b6886c27692665d,"Well-known techniques exist for proving the soundness of subtyping relations with respect to type safety. However, completeness has not been treated with widely applicable techniques, as far as we're aware. This article develops techniques for stating and proving that a subtyping relation is complete with respect to type safety and applies the techniques to the study of iso-recursive subtyping. A new proof technique, induction on failing derivations, is provided that may be useful in other domains as well. The common subtyping rules for iso-recursive types-the ""Amber rules""-are shown to be incomplete with respect to type safety. That is, there exist iso-recursive types τ1 and τ2 such that τ1 can safely be considered a subtype of τ2, but τ1≤τ2 is not derivable with the Amber rules. New, algorithmic rules are defined for subtyping iso-recursive types, and the rules are proved sound and complete with respect to type safety. The fully implemented subtyping algorithm is optimized to run in O(mn) time, where m is the number of μ-terms in the types being considered and n is the size of the types being considered. © 2017 ACM.",Completeness; Preciseness; Recursive types; Subtyping,Amber; Completeness; Preciseness; Recursive types; Sound and complete; Subtyping algorithms; Subtyping relation; Subtypings; Type safety; Formal languages
Programs from proofs: A framework for the safe execution of untrusted software,2017,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015712449&doi=10.1145%2f3014427&partnerID=40&md5=0f44f7b458a0caf51a3c75b9394830e2,"Today, software is traded worldwide on global markets, with apps being downloaded to smartphones within minutes or seconds. This poses, more than ever, the challenge of ensuring safety of software in the face of (1) unknown or untrusted software providers together with (2) resource-limited software consumers. The concept of Proof-Carrying Code (PCC), years ago suggested by Necula, provides one framework for securing the execution of untrusted code. PCC techniques attach safety proofs, constructed by software producers, to code. Based on the assumption that checking proofs is usually much simpler than constructing proofs, software consumers should thus be able to quickly check the safety of software. However, PCC techniques often suffer from the size of certificates (i.e., the attached proofs), making PCC techniques inefficient in practice. In this article, we introduce a new framework for the safe execution of untrusted code called Programs from Proofs (PfP). The basic assumption underlying the PfP technique is the fact that the structure of programs significantly influences the complexity of checking a specific safety property. Instead of attaching proofs to program code, the PfP technique transforms the program into an efficiently checkable form, thus guaranteeing quick safety checks for software consumers. For this transformation, the technique also uses a producer-side automatic proof of safety. More specifically, safety proving for the software producer proceeds via the construction of an abstract reachability graph (ARG) unfolding the control-flow automaton (CFA) up to the degree necessary for simple checking. To this end, we combine different sorts of software analysis: expensive analyses incrementally determining the degree of unfolding, and cheap analyses responsible for safety checking. Out of the abstract reachability graph we generate the new program. In its CFA structure, it is isomorphic to the graph and hence another, this time consumer-side, cheap analysis can quickly determine its safety. Like PCC, Programs from Proofs is a general framework instantiable with different sorts of (expensive and cheap) analysis. Here, we present the general framework and exemplify it by some concrete examples. We have implemented different instantiations on top of the configurable program analysis tool CPACHECKER and report on experiments, in particular on comparisons with PCC techniques. © 2017 ACM.",Abstract interpretation; Program certification; Program extraction; Program transformation; Programs from proofs; Proof-carrying code,Abstracting; Codes (symbols); Commerce; Flow graphs; International trade; Abstract interpretations; Program certification; Program extraction; Program transformations; Proof-carrying code; Network function virtualization
Limitations of partial compaction: Towards practical bounds,2017,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016507028&doi=10.1145%2f2994597&partnerID=40&md5=44c2e7166ab328feef960c9c908ad7ba,"Compaction of a managed heap is a costly operation to be avoided as much as possible in commercial runtimes. Instead, partial compaction is often used to defragment parts of the heap and avoid space blowup. Previous study of compaction limitation provided some initial asymptotic bounds but no implications for practical systems. In this work, we extend the theory to obtain better bounds and make them strong enough to become meaningful for modern systems. © 2017 ACM.",Compaction; Fragmentation; Lower bounds; Memory management; Theory,Linguistics; Software engineering; Asymptotic bounds; Fragmentation; Lower bounds; Memory management; Practical systems; Runtimes; Theory; Compaction
SPL: An extensible language for distributed stream processing,2017,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016448382&doi=10.1145%2f3039207&partnerID=40&md5=829c7ad20a86dd85ad17e5c9ef554976,"Big data is revolutionizing how all sectors of our economy do business, including telecommunication, transportation, medical, and finance. Big data comes in two flavors: data at rest and data in motion. Processing data in motion is stream processing. Stream processing for big data analytics often requires scale that can only be delivered by a distributed system, exploiting parallelism on many hosts and many cores. One such distributed stream processing system is IBM Streams. Early customer experience with IBM Streams uncovered that another core requirement is extensibility, since customers want to build high-performance domain-specific operators for use in their streaming applications. Based on these two core requirements of distribution and extensibility, we designed and implemented the Streams Processing Language (SPL). This article describes SPL with an emphasis on the language design, distributed runtime, and extensibility mechanism. SPL is now the gateway for the IBM Streams platform, used by our customers for stream processing in a broad range of application domains. © 2017 ACM.",Stream processing,Computer hardware description languages; Data handling; Distributed parameter control systems; Sales; Customer experience; Distributed runtime; Distributed stream processing; Distributed systems; Extensible languages; Language design; Stream processing; Streaming applications; Big data
Foreword,2017,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016515058&doi=10.1145%2f3052720&partnerID=40&md5=9cd3b3e46f9ef3308a17ec8754bc8da1,[No abstract available],,
Newtonian program analysis via tensor product,2017,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017120774&doi=10.1145%2f3024084&partnerID=40&md5=425ef41833ea7e360e3b12b73f361836,"Recently, Esparza et al. generalized Newton's method-a numerical-analysis algorithm for finding roots of real-valued functions- to a method for finding fixed-points of systems of equations over semirings. Their method provides a new way to solve interprocedural dataflow-analysis problems. As in its real-valued counterpart, each iteration of their method solves a simpler ""linearized"" problem. One of the reasons this advance is exciting is that some numerical analysts have claimed that ""'all' effective and fast iterative [numerical] methods are forms (perhaps very disguised) of Newton's method."" However, there is an important difference between the dataflow-analysis and numerical-analysis contexts: When Newton's method is used in numerical-analysis problems, commutativity of multiplication is relied on to rearrange an expression of the form ""a ∗ X ∗ b + c ∗ X ∗ d"" into ""(a ∗ b + c ∗ d) ∗ X."" Equations with such expressions correspond topath problems described by regular languages. In contrast, when Newton's method is used for interprocedural dataflow analysis, the ""multiplication"" operation involves function composition and hence is non-commutative: ""a∗ X∗b+c ∗ X∗d"" cannot be rearranged into ""(a∗b+c ∗d)∗ X."" Equations with such expressions correspond to path problems described by linear context-free languages (LCFLs). In this article, we present an improved technique for solving the LCFL sub-problems produced during successive rounds of Newton's method. Our method applies to predicate abstraction, on which most of today's software model checkers rely. © 2017 ACM.",Interprocedural program analysis; Newton's method; Polynomial fixed-point equation; Regular expression; Semiring; Tensor product,Context free languages; Data flow analysis; Iterative methods; Model checking; Newton-Raphson method; Numerical analysis; Numerical methods; Tensors; Fixed point equation; Interprocedural program analysis; Newton's methods; Regular expressions; Semi-ring; Tensor products; C (programming language)
Towards a compiler for reals,2017,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015608067&doi=10.1145%2f3014426&partnerID=40&md5=0dc77961a8f4e27205604da5ae7cf66c,"Numerical software, common in scientific computing or embedded systems, inevitably uses a finite-precision approximation of the real arithmetic in whichmost algorithms are designed. Inmany applications, the roundoff errors introduced by finite-precision arithmetic are not the only source of inaccuracy, and measurement and other input errors further increase the uncertainty of the computed results. Adequate tools are needed to help users select suitable data types and evaluate the provided accuracy, especially for safety-critical applications. We present a source-to-source compiler called Rosa that takes as input a real-valued program with error specifications and synthesizes code over an appropriate floating-point or fixed-point data type. The main challenge of such a compiler is a fully automated, sound, and yet accurate-enough numerical error estimation. We introduce a unified technique for bounding roundoff errors from floating-point and fixed-point arithmetic of various precisions. The technique can handle nonlinear arithmetic, determine closed-form symbolic invariants for unbounded loops, and quantify the effects of discontinuities on numerical errors. We evaluate Rosa on a number of benchmarks from scientific computing and embedded systems and, comparing it to the state of the art in automated error estimation, show that it presents an interesting tradeoff between accuracy and performance. © 2017 ACM.",Compilation; Discontinuity error; Fixed-point arithmetic; Floating-point arithmetic; Loops; Roundoff error; Sensitivity analysis; Verification,Approximation algorithms; Benchmarking; Digital arithmetic; Embedded systems; Errors; Program compilers; Safety engineering; Sensitivity analysis; Uncertainty analysis; Verification; Compilation; Finite precision; Finite-precision arithmetic; Loops; Numerical errors; Numerical software; Round-off errors; Safety critical applications; Fixed point arithmetic
"Polymorphic manifest contracts, revised and resolved",2017,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85012284859&doi=10.1145%2f2994594&partnerID=40&md5=3d593bde51764459091f6d9b64a106ef,"Manifest contracts track precise program properties by refining types with predicates-for example, {x:Int | x > 0} denotes the positive integers. Contracts and polymorphism make a natural combination: programmers can give strong contracts to abstract types, precisely stating pre- and post conditions while hiding implementation details-for instance, an abstract type of stacks might specify that the pop operation has input type {x:αStack | not (empty x)}. This article studies a polymorphic calculus with manifest contracts and establishes fundamental properties including type soundness and relational parametricity. Indeed, this is not the first work on polymorphic manifest contracts, but existing calculi are not very satisfactory. Gronski et al. developed the SAGE language, which introduces polymorphism through the Type:Type discipline, but they do not study parametricity. Some authors of this article have produced two separate works: Belo et al. [2011] and Greenberg [2013] studied polymorphic manifest contracts and parametricity, but their calculi have metatheoretical problems in the type conversion relations. Indeed, they depend on a few conjectures, which turn out to be false. Our calculus is the first polymorphic manifest calculus with parametricity, depending on no conjectures-it resolves the issues in prior calculi with delayed substitution on casts. © 2017 ACM.",Abstract datatypes; Contracts; Corrections; Dynamic checking; Logical relations; Parametric polymorphism; Postconditions; Preconditions; Refinement types; Runtime verification; Syntactic proof,Abstracting; Biomineralization; Contracts; Integer programming; Pathology; Corrections; Datatypes; Dynamic checking; Logical relations; Parametric polymorphism; Postconditions; Preconditions; Refinement types; Run-time verification; Calculations
"The truth, the whole truth, and nothing but the truth: A pragmatic guide to assessing empirical evaluations",2016,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994531582&doi=10.1145%2f2983574&partnerID=40&md5=8633e04c07d2c8b09dd6b52fd491dc51,"An unsound claim can misdirect a field, encouraging the pursuit of unworthy ideas and the abandonment of promising ideas. An inadequate description of a claim can make it difficult to reason about the claim, for example, to determine whether the claim is sound. Many practitioners will acknowledge the threat of unsound claims or inadequate descriptions of claims to their field. We believe that this situation is exacerbated, and even encouraged, by the lack of a systematic approach to exploring, exposing, and addressing the source of unsound claims and poor exposition. This article proposes a framework that identifies three sins of reasoning that lead to unsound claims and two sins of exposition that lead to poorly described claims and evaluations. Sins of exposition obfuscate the objective of determining whether or not a claim is sound, while sins of reasoning lead directly to unsound claims. Our framework provides practitioners with a principled way of critiquing the integrity of their own work and the work of others. We hope that this will help individuals conduct better science and encourage a cultural shift in our research community to identify and promulgate sound claims. © 2016 ACM.",Experimental evaluation; Experimentation; Observation study,Software engineering; Cultural shift; Empirical evaluations; Experimental evaluation; Experimentation; nocv1; Observation study; Research communities; Linguistics
"DRFx: An understandable, high performance, and flexible memory model for concurrent languages",2016,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84989318609&doi=10.1145%2f2925988&partnerID=40&md5=abdab7e4f8f69998086e9d2ba53549be,"The most intuitive memory model for shared-memory multi-threaded programming is sequential consistency (SC), but it disallows the use of many compiler and hardware optimizations and thus affects performance. Data-race-free (DRF) models, such as the C++11 memory model, guarantee SC execution for data-race-free programs. But these models provide no guarantee at all for racy programs, compromising the safety and debuggability of such programs. To address the safety issue, the Java memory model, which is also based on the DRF model, provides a weak semantics for racy executions. However, this semantics is subtle and complex, making it difficult for programmers to reason about their programs and for compiler writers to ensure the correctness of compiler optimizations. We present the DRFx memory model, which is simple for programmers to understand and use while still supporting many common optimizations. We introduce a memory model (MM) exception that can be signaled to halt execution. If a program executes without throwing this exception, then DRFx guarantees that the execution is SC. If a program throws an MM exception during an execution, then DRFx guarantees that the program has a data race. We observe that SC violations can be detected in hardware through a lightweight form of conflict detection. Furthermore, our model safely allows aggressive compiler and hardware optimizations within compiler-designated program regions. We formalize ourmemorymodel, prove several properties of this model, describe a compiler and hardware design suitable for DRFx, and evaluate the performance overhead due to our compiler and hardware requirements. © 2016 ACM.",DRFx; Memory models; Sequential consistency,C (programming language); Concurrency control; Hardware; Java programming language; Reconfigurable hardware; Semantics; Compiler optimizations; Concurrent languages; Conflict detection; DRFx; Hardware optimization; Java Memory model; Memory models; Sequential consistency; Program compilers
Analyzing runtime and size complexity of integer programs,2016,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041433983&doi=10.1145%2f2866575&partnerID=40&md5=5bcbc2794d3914fa0b77a7fb68227ac9,"We present a modular approach to automatic complexity analysis of integer programs. Based on a novel alternation between finding symbolic time bounds for program parts and using these to infer bounds on the absolute values of program variables, we can restrict each analysis step to a small part of the program while maintaining a high level of precision. The bounds computed by our method are polynomial or exponential expressions that depend on the absolute values of input parameters. We show how to extend our approach to arbitrary cost measures, allowing the use of our technique to find upper bounds for other expended resources, such as network requests or memory consumption. Our contributions are implemented in the open-source tool KoAT, and extensive experiments show the performance and power of our implementation in comparison with other tools. © 2016 ACM.",Automated complexity analysis; Integer programs; Runtime complexity,Complex networks; Complexity analysis; Exponential expression; Integer program; Memory consumption; Modular approach; Open source tools; Program variables; Run time complexity; Integer programming
"The design and formalization of Mezzo, a permission-based programming language",2016,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018693073&doi=10.1145%2f2837022&partnerID=40&md5=3159b41c35460ce57bdb530e7709cf88,"The programming language Mezzo is equipped with a rich type system that controls aliasing and access to mutable memory. We give a comprehensive tutorial overview of the language. Then we present a modular formalization of Mezzo's core type system, in the form of a concurrent λ-calculus, which we successively extend with references, locks, and adoption and abandon, a novel mechanism that marries Mezzo's static ownership discipline with dynamic ownership tests. We prove that well-typed programs do not go wrong and are data-race free. Our definitions and proofs are machine checked. © 2016 ACM.",Aliasing; Concurrency; Ownership; Side effects; Static type systems,Calculations; Differentiation (calculus); Aliasing; Concurrency; Ownership; Side effect; Static type systems; Computer programming languages
A practical approach for model checking C/C++11 code,2016,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84970021180&doi=10.1145%2f2806886&partnerID=40&md5=38f3bcf57e7325222684aa0c8f0d2832,"Writing low-level concurrent software has traditionally required intimate knowledge of the entire toolchain and often has involved coding in assembly. New language standards have extended C and C++ with support for low-level atomic operations and a weak memory model, enabling developers to write portable and efficient multithreaded code. In this article, we present CDSCHECKER, a tool for exhaustively exploring the behaviors of concurrent code under the C/C++ memory model. We have used CDSCHECKER to exhaustively unit test concurrent data structure implementations and have discovered errors in a published implementation of a work-stealing queue and a single producer, single consumer queue. © 2016 ACM.",Model checking; Relaxed memory model,C++ (programming language); Codes (symbols); Concurrency control; Java programming language; Atomic operation; Concurrent data structures; Concurrent software; Language standards; Memory modeling; Multithreaded; Relaxed memory models; Weak memory models; Model checking
The pluto+algorithm: A practical approach for parallelization and locality optimization of affine loop nests,2016,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84966312140&doi=10.1145%2f2896389&partnerID=40&md5=c56e8092aa98ba2559cec5b087d3594e,"Affine transformations have proven to be powerful for loop restructuring due to their ability to model a very wide range of transformations. A single multidimensional affine function can represent a long and complex sequence of simpler transformations. Existing affine transformation frameworks such as the Pluto algorithm, which include a cost function for modern multicore architectures for which coarse-grained parallelism and locality are crucial, consider only a subspace of transformations to avoid a combinatorial explosion in finding transformations. The ensuing practical trade-offs lead to the exclusion of certain useful transformations: in particular, transformation compositions involving loop reversals and loop skewing by negative factors. In addition, there is currently no proof that the algorithm successfully finds a tree of permutable loop bands for all affine loop nests. In this article, we propose an approach to address these two issues (1) bymodeling a much larger space of practically useful affine transformations in conjunction with the existing cost function of Pluto, and (2) by extending the Pluto algorithm in a way that allows a proof for its soundness and completeness for all affine loop nests. We perform an experimental evaluation of both, the effect on compilation time, and performance of generated codes. The evaluation shows that our new framework, Pluto+, provides no degradation in performance for any benchmark from Polybench. For the Lattice Boltzmann Method (LBM) simulations with periodic boundary conditions, it provides a mean speedup of 1.33× over Pluto. We also show that Pluto+ does not increase compilation time significantly. Experimental results on Polybench show that Pluto+ increases overall polyhedral source-to-source optimization time by only 15%. In cases in which it improves execution time significantly, it increased polyhedral optimization time by only 2.04×. © 2016 ACM.",Affine transformations; Automatic parallelization; Locality optimization; Loop transformations; Polyhedral model; Tiling,Algorithms; Benchmarking; Computational fluid dynamics; Cost functions; Economic and social effects; Function evaluation; Software architecture; Trees (mathematics); Affine transformations; Automatic Parallelization; Locality optimization; Loop transformation; Polyhedral modeling; Tiling; Optimization
Thinking inside the box: Compartmentalized garbage collection,2016,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84966297215&doi=10.1145%2f2866576&partnerID=40&md5=2f4e8c849460ae4e395efd368eb70203,"The web browser is the ""new desktop."" Not only do many users spend most of their time using the browser, the browser has also become host to rich and dynamic applications that were previously tailored to each individual operating system. The lingua franca of web scripting, JavaScript, was pivotal in this development. Imagine that all desktop applications allocated memory from a single heap managed by the operating system. To reclaim memory upon application shutdown, all processes would then be garbage collected- not just the one being quit. While operating systems improved upon this approach long ago, this was how browsers managed memory until recently. This article explores compartmentalized memory management, an approach tailored specifically to web browsers. The idea is to partition the JavaScript heap into compartments and allocate objects to compartments based on their origin. All objects in the same compartment reference each other direct, whereas cross-origin references go through wrapper objects. We carefully evaluate our techniques using Mozilla's Firefox browser-which now ships with our enhancements-and demonstrate the benefits of collecting each compartment independently. This simultaneously improves runtime performance (up to 36%) and reduces garbage collection pause times (up to 75%) as well as the memory footprint of the browser. In addition, enforcing the same-origin security policy becomes simple and efficient with compartments. © 2016 ACM.",Garbage collection; Isolation; Memory management; Web-browser architecture,High level languages; Java programming language; Refuse collection; World Wide Web; Desktop applications; Dynamic applications; Garbage collection; Isolation; Memory footprint; Memory management; Run-time performance; Security policy; Web browsers
ThisType for object-oriented languages: From theory to practice,2016,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84966447595&doi=10.1145%2f2888392&partnerID=40&md5=806631c7a6bf1c37940c643a5b16598c,"In object-oriented programs, objects often provide methods whose parameter types or return types are the object types themselves. For example, the parameter types of binary methods are the types of their receiver objects, and the return types of some factory methods are the types of their enclosing objects. However, most object-oriented languages do not support such methods precisely because their type systems do not support explicit recursive types, which lead to a mismatch between subclassing and subtyping. This mismatch means that an expression of a subclass may not always be usable in a context where an expression of a superclass is expected, which is not intuitive in an object-oriented setting. Researchers have proposed various type-sound approaches to support methods with types of their enclosing object types denoted by some variants of ThisType, but they reject reasonable and useful methods due to unpermissive type systems or they use less precise declared inexact types rather than runtime exact types. In this article, we present a thorough approach to support methods with ThisTypes: from a new encoding of objects in a typed lambda calculus that allows subtyping by subclassing to an open-source implementation as an extension of the Java programming language. We first provide real-world examples that motivate the need for ThisTyped methods to precisely describe desired properties of programs. We define a new object encoding that enables subtyping by subclassing even in the presence of negative occurrences of type recursion variables by distinguishing object types from existential object types. Based on this object encoding, we formalize language features to support ThisTyped methods with a core calculus CoreThisJava, and prove its type soundness. Finally, we provide ThisJava, a prototype implementation of the calculus, to show its backward compatibility, and we make it publicly available. We believe that our approach theoretically expands the long pursuit of an object-oriented language with ThisTypes to support more useful methods with more precise types. © 2016 ACM.",Binary methods; Exact types; Generic factory methods; Object-oriented languages; ThisType; Virtual constructors,Bins; Calculations; Computational linguistics; Computer programming; Differentiation (calculus); Encoding (symbols); Formal languages; Industrial plants; Java programming language; Open source software; Binary method; Exact types; Factory methods; ThisType; Virtual constructors; Object oriented programming
Automatic storage optimization for arrays,2016,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84966377293&doi=10.1145%2f2845078&partnerID=40&md5=ba5b18a6b148e779f2caf98f7a4a0a97,"Efficient memory allocation is crucial for data-intensive applications, as a smaller memory footprint ensures better cache performance and allows one to run a larger problem size given a fixed amount of main memory. In this article, we describe a new automatic storage optimization technique to minimize the dimensionality and storage requirements of arrays used in sequences of loop nests with a predetermined schedule.We formulate the problem of intra-array storage optimization as one of finding the right storage partitioning hyperplanes: each storage partition corresponds to a single storage location. Our heuristic is driven by a dual-objective function that minimizes both the dimensionality of the mapping and the extents along those dimensions. The technique is dimension optimal for most codes encountered in practice. The storage requirements of the mappings obtained also are asymptotically better than those obtained by any existing scheduledependent technique. Storage reduction factors and other results that we report from an implementation of our technique demonstrate its effectiveness on several real-world examples drawn from the domains of image processing, stencil computations, high-performance computing, and the class of tiled codes in general. © 2016 ACM.",Array contraction; Compilers; Memory optimization; Polyhedral framework; Storage mapping optimization,Cache memory; Image processing; Mapping; Optimization; Program compilers; Array contraction; Data-intensive application; Dual objective function; High performance computing; Mapping optimization; Memory optimization; Polyhedral framework; Stencil computations; Digital storage
Verifying custom synchronization constructs using higher-order separation logic,2016,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84954327216&doi=10.1145%2f2818638&partnerID=40&md5=61c7ebec3524bb67fed824735378c66f,"Synchronization constructs lie at the heart of any reliable concurrent program. Many such constructs are standard (e.g., locks, queues, stacks, and hash-tables). However, many concurrent applications require custom synchronization constructs with special-purpose behavior. These constructs present a significant challenge for verification. Like standard constructs, they rely on subtle racy behavior, but unlike standard constructs, they may not have well-understood abstract interfaces. As they are custom built, such constructs are also far more likely to be unreliable. This article examines the formal specification and verification of custom synchronization constructs. Our target is a library of channels used in automated parallelization to enforce sequential behavior between program statements. Our high-level specification captures the conditions necessary for correct execution; these conditions reflect program dependencies necessary to ensure sequential behavior. We connect the high-level specification with the low-level library implementation to prove that a client's requirements are satisfied. Significantly, we can reason about program and library correctness without breaking abstraction boundaries. To achieve this, we use a program logic called iCAP (impredicative Concurrent Abstract Predicates) based on separation logic. iCAP supports both high-level abstraction and low-level reasoning about races. We use this to show that our high-level channel specification abstracts three different, increasingly complex lowlevel implementations of the library. iCAP's support for higher-order reasoning lets us prove that sequential dependencies are respected, while iCAP's next-generation semantic model lets us avoid ugly problems with cyclic dependencies. Copyright © 2016 is held by the owner/author(s). Publication rights licensed to ACM.",Concurrency; Concurrent abstract predicates; Separation logic,Abstracting; Formal logic; Formal specification; Reconfigurable hardware; Semantics; Separation; Specifications; Synchronization; Concurrency; Concurrent abstract predicates; Cyclic dependencies; Formal specification and verification; High level specification; High-level abstraction; Separation logic; Sequential dependencies; Computer circuits
An abstract interpretation-based model of tracing just-in-time compilation,2016,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84954308032&doi=10.1145%2f2853131&partnerID=40&md5=ce0491d6efeea1dc5473cab89f0398be,"Tracing just-in-time compilation is a popular compilation technique for the efficient implementation of dynamic languages, which is commonly used for JavaScript, Python, and PHP. It relies on two key ideas. First, it monitors program execution in order to detect so-called hot paths, that is, the most frequently executed program paths. Then, hot paths are optimized by exploiting some information on program stores that is available and therefore gathered at runtime. The result is a residual program where the optimized hot paths are guarded by sufficient conditions ensuring some form of equivalence with the original program. The residual program is persistently mutated during its execution, for example, to add new optimized hot paths or to merge existing paths. Tracing compilation is thus fundamentally different from traditional static compilation. Nevertheless, despite the practical success of tracing compilation, very little is known about its theoretical foundations. We provide a formal model of tracing compilation of programs using abstract interpretation. The monitoring phase (viz., hot path detection) corresponds to an abstraction of the trace semantics of the program that captures the most frequent occurrences of sequences of program points together with an abstraction of their corresponding stores, for example, a type environment. The optimization phase (viz., residual program generation) corresponds to a transform of the original program that preserves its trace semantics up to a given observation asmodeled by some abstraction. We provide a generic framework to express dynamic optimizations along hot paths and to prove them correct. We instantiate it to prove the correctness of dynamic type specialization and constant variable folding. We show that our framework is more general than the model of tracing compilation introduced by Guo and Palsberg [2011], which is based on operational bisimulations. In our model, we can naturally express hot path reentrance and common optimizations like dead-store elimination, which are either excluded or unsound in Guo and Palsberg's framework. © 2016 ACM.",Abstract interpretation; Trace semantics; Tracing JIT compilation,High level languages; Just in time production; Model checking; Semantics; Abstract interpretations; Compilation techniques; Dynamic optimization; Efficient implementation; Jit compilations; Just-in-time compilation; Theoretical foundations; Trace semantics; Abstracting
Decomposition-based Synthesis for Applying Divide-and-Conquer–like Algorithmic Paradigms,2024,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191262549&doi=10.1145%2f3648440&partnerID=40&md5=25df9c75f53b36e4cb554f68ebdb304d,"Algorithmic paradigms such as divide-and-conquer (D&C) are proposed to guide developers in designing efficient algorithms, but it can still be difficult to apply algorithmic paradigms to practical tasks. To ease the usage of paradigms, many research efforts have been devoted to the automatic application of algorithmic paradigms. However, most existing approaches to this problem rely on syntax-based program transformations and thus put significant restrictions on the original program. In this article, we study the automatic application of D&C and several similar paradigms, denoted as D&C-like algorithmic paradigms, and aim to remove the restrictions from syntax-based transformations. To achieve this goal, we propose an efficient synthesizer, named AutoLifter, which does not depend on syntax-based transformations. Specifically, the main challenge of applying algorithmic paradigms is from the large scale of the synthesized programs, and AutoLifter addresses this challenge by applying two novel decomposition methods that do not depend on the syntax of the input program, component elimination and variable elimination, to soundly divide the whole problem into simpler subtasks, each synthesizing a sub-program of the final program and being tractable with existing synthesizers. We evaluate AutoLifter on 96 programming tasks related to six different algorithmic paradigms. AutoLifter solves 82/96 tasks with an average time cost of 20.17 s, significantly outperforming existing approaches. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",algorithm synthesis; decomposition methods for program synthesis tasks; Inductive program synthesis,Algorithm synthesis; Algorithmics; Automatic application; Decomposition method for program synthesis task; Decomposition methods; Divide-and-conquer; Inductive Program Synthesis; Program synthesis; Program transformations; Research efforts; Syntactics
CFLOBDDs: Context-Free-Language Ordered Binary Decision Diagrams,2024,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85194006297&doi=10.1145%2f3651157&partnerID=40&md5=a055256268bc4251b7746cbcbbb5f48c,"This article presents a new compressed representation of Boolean functions, called CFLOBDDs (for Context-Free-Language Ordered Binary Decision Diagrams). They are essentially a plug-compatible alternative to BDDs (Binary Decision Diagrams), and hence are useful for representing certain classes of functions, matrices, graphs, relations, and so forth in a highly compressed fashion. CFLOBDDs share many of the good properties of BDDs, but—in the best case—the CFLOBDD for a Boolean function can be exponentially smaller than any BDD for that function. Compared with the size of the decision tree for a function, a CFLOBDD—again, in the best case—can give a double-exponential reduction in size. They have the potential to permit applications to (i) execute much faster and (ii) handle much larger problem instances than has been possible heretofore. We applied CFLOBDDs in quantum-circuit simulation and found that for several standard problems, the improvement in scalability, compared to BDDs, is quite dramatic. With a 15-minute timeout, the number of qubits that CFLOBDDs can handle are 65,536 for Greenberger-Horne-Zellinger, 524,288 for Bernstein-Vazirani, 4,194,304 for Deutsch-Jozsa, and 4,096 for Grover’s algorithm, besting BDDs by factors of 128×, 1,024×, 8,192×, and 128×, respectively. © 2024 Copyright held by the owner/author(s).",best-case double-exponential compression; Decision diagram; matched paths; quantum simulation,Binary decision diagrams; Context free languages; Decision trees; Quantum chemistry; Best-case double-exponential compression; Context-free languages; Decision diagram; Double exponential; Function matrixes; Matched path; Matrix graphs; Ordered binary decision diagrams; Property; Quantum simulations; Boolean functions
Homeostasis: Design and Implementation of a Self-Stabilizing Compiler,2024,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85196654910&doi=10.1145%2f3649308&partnerID=40&md5=838bcafff794a7a8bf34d8db0eeae0c3,"Mainstream compilers perform a multitude of analyses and optimizations on the given input program. Each analysis (such as points-to analysis) may generate a program-abstraction (such as points-to graph). Each optimization is typically composed of multiple alternating phases of inspection of such program-abstractions and transformations of the program. Upon transformation of a program, the program-abstractions generated by various analyses may become inconsistent with the modified program. Consequently, the correctness of the downstream inspection (and consequent transformation) phases cannot be ensured until the relevant program-abstractions are stabilized; that is, the program-abstractions are either invalidated or made consistent with the modified program. In general, the existing compiler frameworks do not perform automated stabilization of the program-abstractions and instead leave it to the compiler pass writers to deal with the complex task of identifying the relevant program-abstractions to be stabilized, the points where the stabilization is to be performed, and the exact procedure of stabilization. In this article, we address these challenges by providing the design and implementation of a novel compiler-design framework called Homeostasis.Homeostasis automatically captures all the program changes performed by each transformation phase, and later, triggers the required stabilization using the captured information, if needed. We also provide a formal description of Homeostasis and a correctness proof thereof. To assess the feasibility of using Homeostasis in compilers of parallel programs, we have implemented our proposed idea in IMOP, a compiler framework for OpenMP C programs. Furthermore, to illustrate the benefits of using Homeostasis, we have implemented a set of standard data-flow passes, and a set of involved optimizations that are used to remove redundant barriers in OpenMP C programs. Implementations of none of these optimizations in IMOP required any additional lines of code for stabilization of the program-abstractions. We present an evaluation in the context of these optimizations and analyses, which demonstrates that Homeostasis is efficient and easy to use. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Compiler design; multi-pass compilation; self-stabilization,Abstracting; Application programming interfaces (API); C (programming language); Data flow analysis; Program compilers; C programs; Compiler design; Design and implementations; Homoeostasis; Input programs; Multi-pass; Multi-pass compilation; Optimisations; Self stabilization; Transformation phasis; Stabilization
Adversities in Abstract Interpretation - Accommodating Robustness by Abstract Interpretation,2024,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85196642881&doi=10.1145%2f3649309&partnerID=40&md5=0e646345c1783601c5c8fdc864239429,"Robustness is a key and desirable property of any classifying system, in particular, to avoid the ever-rising threat of adversarial attacks. Informally, a classification system is robust when the result is not affected by the perturbation of the input. This notion has been extensively studied, but little attention has been dedicated to how the perturbation affects the classification. The interference between perturbation and classification can manifest in many different ways, and its understanding is the main contribution of the present article. Starting from a rigorous definition of a standard notion of robustness, we build a formal method for accommodating the required degree of robustness - depending on the amount of error the analyst may accept on the classification result. Our idea is to precisely model this error as an abstraction. This leads us to define weakened forms of robustness also in the context of programming languages, particularly in language-based security, e.g., information-flow policies, and in program verification. The latter is possible by moving from a quantitative (standard) model of perturbation to a novel qualitative model, given by means of the notion of abstraction. As in language-based security, we show that it is possible to confine adversities, which means to characterize the degree of perturbation (and/or the degree of class generalization) for which the classifier may be deemed adequately robust. We conclude with an experimental evaluation of our ideas, showing how weakened forms of robustness apply to state-of-the-art image classifiers. © 2024 Copyright held by the owner/author(s).",abstract interpretation; domain completeness; information-flow policies; machine learning; Robustness of classifying systems,Abstracting; Formal methods; Machine learning; Model checking; Abstract interpretations; Classification results; Classification system; Degree of robustness; Domain completeness; Information flow policies; Language-based security; Machine-learning; Property; Robustness of classifying system; Classification (of information)
Interactive Abstract Interpretation with Demanded Summarization,2024,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189431869&doi=10.1145%2f3648441&partnerID=40&md5=ae388b1f767704d78173092541be2085,"We consider the problem of making expressive, interactive static analyzers compositional. Such a technique could help bring the power of server-based static analyses to integrated development environments (IDEs), updating their results live as the code is modified. Compositionality is key for this scenario, as it enables reuse of already-computed analysis results for unmodified code. Previous techniques for interactive static analysis either lack compositionality, cannot express arbitrary abstract domains, or are not from-scratch consistent. We present demanded summarization, the first algorithm for incremental compositional analysis in arbitrary abstract domains that guarantees from-scratch consistency. Our approach analyzes individual procedures using a recent technique for demanded analysis, computing summaries on demand for procedure calls. A dynamically updated summary dependency graph enables precise result invalidation after program edits, and the algorithm is carefully designed to guarantee from-scratch-consistent results after edits, even in the presence of recursion and in arbitrary abstract domains. We formalize our technique and prove soundness, termination, and from-scratch consistency. An experimental evaluation of a prototype implementation on synthetic and real-world program edits provides evidence for the feasibility of this theoretical framework, showing potential for major performance benefits over non-demanded compositional analyses.  © 2024 Copyright held by the owner/author(s).",Abstract interpretation; Incremental computation,Abstracting; Codes (symbols); Model checking; Abstract domains; Abstract interpretations; Compositional analysis; Compositionality; Incremental computation; Integrated development environment; Power; Reuse; Server-based; Static analyzers; Static analysis
"Locally Abstract, Globally Concrete Semantics of Concurrent Programming Languages",2024,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189238395&doi=10.1145%2f3648439&partnerID=40&md5=ba942b00568280fcf44e25d65ee1f29f,"Formal, mathematically rigorous programming language semantics are the essential prerequisite for the design of logics and calculi that permit automated reasoning about concurrent programs. We propose a novel modular semantics designed to align smoothly with program logics used in deductive verification and formal specification of concurrent programs. Our semantics separates local evaluation of expressions and statements performed in an abstract, symbolic environment from their composition into global computations, at which point they are concretised. This makes incremental addition of new language concepts possible, without the need to revise the framework. The basis is a generalisation of the notion of a program trace as a sequence of evolving states that we enrich with event descriptors and trailing continuation markers. This allows to postpone scheduling constraints from the level of local evaluation to the global composition stage, where well-formedness predicates over the event structure declaratively characterise a wide range of concurrency models. We also illustrate how a sound program logic and calculus can be defined for this semantics.  © 2024 Copyright held by the owner/author(s).",compositionality; concurrent programming languages; continuations; Denotational semantics; program calculus; program logics; trace semantics,Biomineralization; Computer programming languages; Concretes; Petroleum reservoir evaluation; Automated reasoning; Compositionality; Concurrent programming languages; Concurrents programs; Continuation; Denotational semantics; Program calculus; Program logic; Programming language semantics; Trace semantics; Semantics
Choral: Object-oriented Choreographic Programming,2024,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189437159&doi=10.1145%2f3632398&partnerID=40&md5=c43a3c4d39e527d37837e5505af2601c,"Choreographies are coordination plans for concurrent and distributed systems, which define the roles of the involved participants and how they are supposed to work together. In the paradigm of choreographic programming, choreographies are programs that can be compiled into executable implementations. In this article, we present Choral, the first choreographic programming language based on mainstream abstractions. The key idea in Choral is a new notion of data type, which allows for expressing that data is distributed over different roles.We use this idea to reconstruct the paradigm of choreographic programming through object-oriented abstractions. Choreographies are classes, and instances of choreographies are objects with states and behaviours implemented collaboratively by roles. Choral comes with a compiler that, given a choreography, generates an implementation for each of its roles. These implementations are libraries in pure Java, whose types are under the control of the Choral programmer. Developers can then modularly compose these libraries in their programs, to participate correctly in choreographies. Choral is the first incarnation of choreographic programming offering such modularity, which finally connects more than a decade of research on the paradigm to practical software development. The integration of choreographic and object-oriented programming yields other powerful advantages, where the features of one paradigm benefit the other in ways that go beyond the sum of the parts. On the one hand, the high-level abstractions and static checks from the world of choreographies can be used to write concurrent and distributed object-oriented software more concisely and correctly. On the other hand, we obtain a much more expressive choreographic language from object-oriented abstractions than in previous work. This expressivity allows for writing more reusable and flexible choreographies. For example, object passing makes Choral the first higher-order choreographic programming language, whereby choreographies can be parameterised over other choreographies without any need for central coordination. We also extend method overloading to a new dimension: specialisation based on data location. Together with subtyping and generics, this allows Choral to elegantly support user-defined communication mechanisms and middleware.  © 2024 Copyright held by the owner/author(s).",Choreographies; communication; higher-kinded types,Abstracting; Computer systems programming; High level languages; Libraries; Middleware; Software design; Choreography; Concurrent and distributed systems; Concurrent objects; Datatypes; Executables; High-kinded type; High-level abstraction; Higher-level abstraction; Object oriented; Objectoriented programming (OOP); Object oriented programming
LoRe: A Programming Model for Verifiably Safe Local-first Software,2024,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189434414&doi=10.1145%2f3633769&partnerID=40&md5=29e5a5e085cc7fdf09f41504e17039be,"Local-first software manages and processes private data locally while still enabling collaboration between multiple parties connected via partially unreliable networks. Such software typically involves interactions with users and the execution environment (the outside world). The unpredictability of such interactions paired with their decentralized nature make reasoning about the correctness of local-first software a chal- lenging endeavor. Yet, existing solutions to develop local-first software do not provide support for automated safety guarantees and instead expect developers to reason about concurrent interactions in an environment with unreliable network conditions. We propose LoRe, a programming model and compiler that automatically verifies developer-supplied safety properties for local-first applications. LoRe combines the declarative data flow of reactive programming with static analysis and verification techniques to precisely determine concurrent interactions that violate safety invariants and to selectively employ strong consistency through coordination where required. We propose a formalized proof principle and demonstrate how to automate the process in a prototype implementation that outputs verified executable code. Our evaluation shows that LoRe simplifies the development of safe local-first software when compared to state-of-the-art approaches and that verification times are acceptable.  © 2024 Copyright held by the owner/author(s).",automatic verification; consistency; invariants; Local-first software; reactive programming,Data flow analysis; Program compilers; Verification; Automatic verification; Concurrent interactions; Consistency; Execution environments; Invariant; Local-first software; Private data; Programming models; Reactive programming; Unreliable network; Static analysis
Focusing on Refinement Typing,2023,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181754793&doi=10.1145%2f3610408&partnerID=40&md5=7554f89151ed7429391fb9ff5c35fc42,"We present a logically principled foundation for systematizing, in a way that works with any computational effect and evaluation order, SMT constraint generation seen in refinement type systems for functional programming languages. By carefully combining a focalized variant of call-by-push-value, bidirectional typing, and our novel technique of value-determined indexes, our system generates solvable SMT constraints without existential (unification) variables. We design a polarized subtyping relation allowing us to prove our logically focused typing algorithm is sound, complete, and decidable. We prove type soundness of our declarative system with respect to an elementary domain-theoretic denotational semantics. Type soundness implies, relatively simply, the total correctness and logical consistency of our system. The relative ease with which we obtain both algorithmic and semantic results ultimately stems from the proof-theoretic technique of focalization.  © 2023 Copyright held by the owner/author(s).",bidirectional typechecking; call-by-push-value; polarity; Refinement types,Functional programming; Bidirectional typechecking; Call by push values; Computational effects; Computational evaluation; Constraints generation; Evaluation order; Polarity; Refinement type; Type soundness; Typechecking; Semantics
Exploiting Partially Context-sensitive Profiles to Improve Performance of Hot Code,2023,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181655127&doi=10.1145%2f3612937&partnerID=40&md5=1f8b6eec14f80b80c08e29f4ba9e747b,"Availability of profiling information is a major advantage of just-in-time (JIT) compilation. Profiles guide the compilation order and optimizations, thus substantially improving program performance. Ahead-of-time (AOT) compilation can also utilize profiles, obtained during separate profiling runs of the programs. Profiles can be context-sensitive, i.e., each profile entry is associated with a call-stack. To ease profile collection and reduce overheads, many systems collect partially context-sensitive profiles, which record only a call-stack suffix. Despite prior related work, partially context-sensitive profiles have the potential to further improve compiler optimizations.In this article, we describe a novel technique that exploits partially context-sensitive profiles to determine which portions of code are hot and compile them with additional compilation budget. This technique is applicable to most AOT compilers that can access partially context-sensitive profiles, and its goal is to improve program performance without significantly increasing code size. The technique relies on a new hot-code-detection algorithm to reconstruct hot regions based on the partial profiles. The compilation ordering and the inlining of the compiler are modified to exploit the information about the hot code. We formally describe the proposed algorithm and its heuristics and then describe our implementation inside GraalVM Native Image, a state-of-the-art AOT compiler for Java. Evaluation of the proposed technique on 16 benchmarks from DaCapo, Scalabench, and Renaissance suites shows a performance improvement between 22% and 40% on 4 benchmarks, and between 2.5% and 10% on 5 benchmarks. Code-size increase ranges from 0.8%-9%, where 10 benchmarks exhibit an increase of less than 2.5%.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Ahead-of-time compilation; inline substitution; inlining,Benchmarking; Budget control; Codes (symbols); Just in time production; Program compilers; Ahead-of-time compilation; Code size; Context-sensitive; Improve performance; Inline substitution; Inlining; Just-in-time compilation; Optimisations; Profiling informations; Program performance; Optimization
Capturing Types,2023,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85177873802&doi=10.1145%2f3618003&partnerID=40&md5=92247e1774170215fc61ff87275a9047,"Type systems usually characterize the shape of values but not their free variables. However, many desirable safety properties could be guaranteed if one knew the free variables captured by values. We describe CC< :ĝ-», a calculus where such captured variables are succinctly represented in types, and show it can be used to safely implement effects and effect polymorphism via scoped capabilities. We discuss how the decision to track captured variables guides key aspects of the calculus, and show that CC< :ĝ-» admits simple and intuitive types for common data structures and their typical usage patterns. We demonstrate how these ideas can be used to guide the implementation of capture checking in a practical programming language.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",capabilities; effects; resources; Scala; type systems,Capability; Common datum; Effect; Free variable; Resource; Safety property; Scala; Simple++; Type systems; Usage patterns
A Verified Optimizer for Quantum Circuits,2023,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171615744&doi=10.1145%2f3604630&partnerID=40&md5=df00b8a4d7548c00959cd705e3254df2,"We present voqc, the first verified optimizer for quantum circuits, written using the Coq proof assistant. Quantum circuits are expressed as programs in a simple, low-level language called sqir, a small quantum intermediate representation, which is deeply embedded in Coq. Optimizations and other transformations are expressed as Coq functions, which are proved correct with respect to a semantics of sqir programs. sqir programs denote complex-valued matrices, as is standard in quantum computation, but we treat matrices symbolically to reason about programs that use an arbitrary number of quantum bits. sqir's careful design and our provided automation make it possible to write and verify a broad range of optimizations in voqc, including full-circuit transformations from cutting-edge optimizers. © 2023 Copyright held by the owner/author(s).",certified compilation; circuit optimization; Formal verification; quantum computing,Formal verification; Matrix algebra; Quantum computers; Quantum optics; Semantics; Theorem proving; Certified compilation; Circuit optimization; Coq proof assistant; Low-level language; matrix; Optimisations; Optimizers; Quantum circuit; Quantum Computing; Simple++; Timing circuits
Prisma : A Tierless Language for Enforcing Contract-client Protocols in Decentralized Applications,2023,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173611847&doi=10.1145%2f3604629&partnerID=40&md5=1d045cb9276e5256c8888f7f38e8cdf3,"Decentralized applications (dApps) consist of smart contracts that run on blockchains and clients that model collaborating parties. dApps are used to model financial and legal business functionality. Today, contracts and clients are written as separate programs - in different programming languages - communicating via send and receive operations. This makes distributed program flow awkward to express and reason about, increasing the potential for mismatches in the client-contract interface, which can be exploited by malicious clients, potentially leading to huge financial losses.In this article, we present Prisma , a language for tierless decentralized applications, where the contract and its clients are defined in one unit and pairs of send and receive actions that ""belong together""are encapsulated into a single direct-style operation, which is executed differently by sending and receiving parties. This enables expressing distributed program flow via standard control flow and renders mismatching communication impossible. We prove formally that our compiler preserves program behavior in presence of an attacker controlling the client code. We systematically compare Prisma with mainstream and advanced programming models for dApps and provide empirical evidence for its expressiveness and performance.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesDomain-specific languages; Scala; smart contracts,Losses; Program compilers; Additional key word and phrasesdomain-specific language; Block-chain; Contract interfaces; Decentralised; Distributed projects; Financial loss; Key words; Program flow; Scala; Specific languages; Smart contract
A Model Checker for Operator Precedence Languages,2023,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173624993&doi=10.1145%2f3608443&partnerID=40&md5=63c24ef46d6dfb6c3980b3f684da390a,"The problem of extending model checking from finite state machines to procedural programs has fostered much research toward the definition of temporal logics for reasoning on context-free structures. The most notable of such results are temporal logics on Nested Words, such as CaRet and NWTL. Recently, Precedence Oriented Temporal Logic (POTL) has been introduced to specify and prove properties of programs coded trough an Operator Precedence Language (OPL). POTL is complete w.r.t. the FO restriction of the MSO logic previously defined as a logic fully equivalent to OPL. POTL increases NWTL's expressive power in a perfectly parallel way as OPLs are more powerful that nested words.In this article, we produce a model checker, named POMC, for OPL programs to prove properties expressed in POTL. To the best of our knowledge, POMC is the first implemented and openly available model checker for proving tree-structured properties of recursive procedural programs. We also report on the experimental evaluation we performed on POMC on a nontrivial benchmark.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesLinear temporal logic; input-driven languages; model checking; operator precedence languages; precedence oriented temporal logic; visibly pushdown languages,Computer circuits; Context free languages; Temporal logic; Additional key word and phraseslinear temporal logic; Finite states machine; Input-driven language; Key words; Model checker; Models checking; Operator precedence language; Precedence oriented temporal logic; Property; Visibly pushdown languages; Model checking
Optimizing Homomorphic Evaluation Circuits by Program Synthesis and Time-bounded Exhaustive Search,2023,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173276725&doi=10.1145%2f3591622&partnerID=40&md5=f5d75819d415fe1fd7579545f5fe6a31,"We present a new and general method for optimizing homomorphic evaluation circuits. Although fully homomorphic encryption (FHE) holds the promise of enabling safe and secure third party computation, building FHE applications has been challenging due to their high computational costs. Domain-specific optimizations require a great deal of expertise on the underlying FHE schemes and FHE compilers that aim to lower the hurdle, generate outcomes that are typically sub-optimal, as they rely on manually-developed optimization rules. In this article, based on the prior work of FHE compilers, we propose a method for automatically learning and using optimization rules for FHE circuits. Our method focuses on reducing the maximum multiplicative depth, the decisive performance bottleneck, of FHE circuits by combining program synthesis, term rewriting, and equality saturation. It first uses program synthesis to learn equivalences of small circuits as rewrite rules from a set of training circuits. Then, we perform term rewriting on the input circuit to obtain a new circuit that has lower multiplicative depth. Our rewriting method uses the equational matching with generalized version of the learned rules, and its soundness property is formally proven. Our optimizations also try to explore every possible alternative order of applying rewrite rules by time-bounded exhaustive search technique called equality saturation. Experimental results show that our method generates circuits that can be homomorphically evaluated 1.08×-3.17× faster (with the geometric mean of 1.56×) than the state-of-the-art method. Our method is also orthogonal to existing domain-specific optimizations.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",equality saturation; Homomorphic encryption circuit; program synthesis; term rewriting,Cryptography; Program compilers; Domain specific; Encryption circuits; Equality saturation; Fully homomorphic encryption; Ho-momorphic encryptions; Homomorphic encryption circuit; Homomorphic-encryptions; Optimisations; Program synthesis; Term rewriting; Timing circuits
SSProve: A Foundational Framework for Modular Cryptographic Proofs in Coq,2023,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173587757&doi=10.1145%2f3594735&partnerID=40&md5=2a40452177229b2d324aee78333dfa8d,"State-separating proofs (SSP) is a recent methodology for structuring game-based cryptographic proofs in a modular way, by using algebraic laws to exploit the modular structure of composed protocols. While promising, this methodology was previously not fully formalized and came with little tool support. We address this by introducing SSProve, the first general verification framework for machine-checked state-separating proofs. SSProve combines high-level modular proofs about composed protocols, as proposed in SSP, with a probabilistic relational program logic for formalizing the lower-level details, which together enable constructing machine-checked cryptographic proofs in the Coq proof assistant. Moreover, SSProve is itself fully formalized in Coq, including the algebraic laws of SSP, the soundness of the program logic, and the connection between these two verification styles.To illustrate SSProve, we use it to mechanize the simple security proofs of ElGamal and pseudo-random-function-based encryption. We also validate the SSProve approach by conducting two more substantial case studies: First, we mechanize an SSP security proof of the key encapsulation mechanism-data encryption mechanism (KEM-DEM) public key encryption scheme, which led to the discovery of an error in the original paper proof that has since been fixed. Second, we use SSProve to formally prove security of the sigma-protocol zero-knowledge construction, and we moreover construct a commitment scheme from a sigma-protocol to compare with a similar development in CryptHOL. We instantiate the security proof for sigma-protocols to give concrete security bounds for Schnorr's sigma-protocol. © 2023 Copyright held by the owner/author(s).",formal verification; game-based proofs; High-assurance cryptography; machine-checked proofs; modular proofs; probabilistic relational program logic; state-separating proofs,Algebra; Computer circuits; Formal logic; Formal verification; Theorem proving; Game-Based; Game-based proof; High assurance; High-assurance cryptography; Machine-checked proofs; Modular proof; Modulars; Probabilistic relational program logic; Probabilistics; Program logic; State-separating proof; Public key cryptography
Multiple Input Parsing and Lexical Analysis,2023,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85175996625&doi=10.1145%2f3594734&partnerID=40&md5=135100638e2304625e91b33289cab3e7,"This article introduces two new approaches in the areas of lexical analysis and context-free parsing. We present an extension, MGLL, of generalised parsing which allows multiple input strings to be parsed together efficiently, and we present an enhanced approach to lexical analysis which exploits this multiple parsing capability. The work provides new power to formal language specification and disambiguation, and brings new techniques into the historically well-studied areas of lexical and syntax analysis. It encompasses character-level parsing at one extreme and the classical LEX/YACC style division at the other, allowing the advantages of both approaches. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Lexical analysis; lexical disambiguation; programming language syntax specification,Computer programming languages; Context free languages; Syntactics; Context-free; Input string; Language syntax; Lexical analysis; Lexical contexts; Lexical disambiguation; Multiple inputs; New approaches; Power; Programming language syntax specification; Specifications
Optimization-Aware Compiler-Level Event Profiling,2023,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85165691195&doi=10.1145%2f3591473&partnerID=40&md5=e698595db782ae71bdc486eddf4ea874,"Tracking specific events in a program's execution, such as object allocation or lock acquisition, is at the heart of dynamic analysis. Despite the apparent simplicity of this task, quantifying these events is challenging due to the presence of compiler optimizations. Profiling perturbs the optimizations that the compiler would normally do - a profiled program usually behaves differently than the original one.In this article, we propose a novel technique for quantifying compiler-internal events in the optimized code, reducing the profiling perturbation on compiler optimizations. Our technique achieves this by instrumenting the program from within the compiler, and by delaying the instrumentation until the point in the compilation pipeline after which no subsequent optimizations can remove the events. We propose two different implementation strategies of our technique based on path-profiling, and a modification to the standard path-profiling algorithm that facilitates the use of the proposed strategies in a modern just-in-time (JIT) compiler. We use our technique to analyze the behaviour of the optimizations in Graal, a state-of-the-art compiler for the Java Virtual Machine, identifying the reasons behind a performance improvement of a specific optimization, and the causes behind an unexpected slowdown of another. Finally, our evaluation results show that the two proposed implementations result in a significantly lower execution-time overhead w.r.t. a naive implementation.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",code optimization; compiler-IR instrumentation; debugging; Dynamic analysis; just-in-time compilers; profiling,Codes (symbols); Program compilers; Code optimization; Compiler optimizations; Compiler-IR instrumentation; Debugging; Dynamics analysis; IR instrumentation; Just-in-time compiler; Optimisations; Path profiling; Profiling; Just in time production
Passport: Improving Automated Formal Verification Using Identifiers,2023,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85165665086&doi=10.1145%2f3593374&partnerID=40&md5=306eaf7bd456234bf00997e82b563ee5,"Formally verifying system properties is one of the most effective ways of improving system quality, but its high manual effort requirements often render it prohibitively expensive. Tools that automate formal verification by learning from proof corpora to synthesize proofs have just begun to show their promise. These tools are effective because of the richness of the data the proof corpora contain. This richness comes from the stylistic conventions followed by communities of proof developers, together with the powerful logical systems beneath proof assistants. However, this richness remains underexploited, with most work thus far focusing on architecture rather than on how to make the most of the proof data. This article systematically explores how to most effectively exploit one aspect of that proof data: identifiers.We develop the Passport approach, a method for enriching the predictive Coq model used by an existing proof-synthesis tool with three new encoding mechanisms for identifiers: category vocabulary indexing, subword sequence modeling, and path elaboration. We evaluate our approach's enrichment effect on three existing base tools: ASTactic, Tac, and Tok. In head-to-head comparisons, Passport automatically proves 29% more theorems than the best-performing of these base tools. Combining the three tools enhanced by the Passport approach automatically proves 38% more theorems than combining the three base tools. Finally, together, these base tools and their enhanced versions prove 45% more theorems than the combined base tools. Overall, our findings suggest that modeling identifiers can play a significant role in improving proof synthesis, leading to higher-quality software.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",machine learning; Proof assistants; proof engineering; proof synthesis,Formal verification; Theorem proving; Automated formal verification; Improving systems; Logical system; Machine-learning; Proof assistant; Proof engineering; Proof synthesis; Synthesis tool; System property; System quality; Machine learning
Synchronous Deterministic Parallel Programming for Multi-Cores with ForeC,2023,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85165701562&doi=10.1145%2f3591594&partnerID=40&md5=042d9c50c7c1990ecee64a607d625d96,"Embedded real-time systems are tightly integrated with their physical environment. Their correctness depends both on the outputs and timeliness of their computations. The increasing use of multi-core processors in such systems is pushing embedded programmers to be parallel programming experts. However, parallel programming is challenging because of the skills, experiences, and knowledge needed to avoid common parallel programming traps and pitfalls. This article proposes the ForeC synchronous multi-threaded programming language for the deterministic, parallel, and reactive programming of embedded multi-cores. The synchronous semantics of ForeC is designed to greatly simplify the understanding and debugging of parallel programs. ForeC ensures that ForeC programs can be compiled efficiently for parallel execution and be amenable to static timing analysis. ForeC's main innovation is its shared variable semantics that provides thread isolation and deterministic thread communication. All ForeC programs are correct by construction and deadlock free because no non-deterministic constructs are needed. We have benchmarked our ForeC compiler with several medium-sized programs (e.g., a 2.274-line ForeC program with up to 26 threads and distributed on up to 10 cores, which was based on a 2.155-line non-multi-threaded C program). These benchmark programs show that ForeC can achieve better parallel performance than Esterel, a widely used imperative synchronous language for concurrent safety-critical systems, and is competitive in performance to OpenMP, a popular desktop solution for parallel programming (which implements classical multi-threading, hence is intrinsically non-deterministic). We also demonstrate that the worst-case execution time of ForeC programs can be estimated to a high degree of precision.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",code generation; determinism; multi-core; parallelism; Programming language; reactive programming; semantics; synchronous; worst-case execution time,Application programming interfaces (API); Benchmarking; C (programming language); Embedded systems; Interactive computer systems; Multicore programming; Program compilers; Program debugging; Real time systems; Safety engineering; Semantics; Codegeneration; Determinism; Deterministics; Embedded real time systems; Multi-cores; Multithreaded; Parallelism; Reactive programming; Synchronoi; Worst-case execution time; Parallel programming
Side-channel Elimination via Partial Control-flow Linearization,2023,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85165628047&doi=10.1145%2f3594736&partnerID=40&md5=feb8d481336bbff8fb6f22d05a10550d,"Partial control-flow linearization is a code transformation conceived to maximize work performed in vectorized programs. In this article, we find a new service for it. We show that partial control-flow linearization protects programs against timing attacks. This transformation is sound: Given an instance of its public inputs, the partially linearized program always runs the same sequence of instructions, regardless of secret inputs. Incidentally, if the original program is publicly safe, then accesses to the data cache will be data oblivious in the transformed code. The transformation is optimal: Every branch that depends on some secret data is linearized; no branch that depends on only public data is linearized. Therefore, the transformation preserves loops that depend exclusively on public information. If every branch that leaves a loop depends on secret data, then the transformed program will not terminate. Our transformation extends previous work in non-trivial ways. It handles C constructs such as ""goto,""""break,""""switch,""and ""continue,""which are absent in the FaCT domain-specific language (2018). Like Constantine (2021), our transformation ensures operation invariance but without requiring profiling information. Additionally, in contrast to SC-Eliminator (2018) and Lif (2021), it handles programs containing loops whose trip count is not known at compilation time.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",compiler; cryptography; Side channel,C (programming language); Cosine transforms; Metadata; Problem oriented languages; Program compilers; Side channel attack; Code transformation; Compiler; Control-flow; Data caches; Linearisation; New services; Partial control; Public data; Secret data; Side-channel; Linearization
A Derivative-based Parser Generator for Visibly Pushdown Grammars,2023,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85165651217&doi=10.1145%2f3591472&partnerID=40&md5=5e468b200d4722214ac33ce13c8d4ee8,"In this article, we present a derivative-based, functional recognizer and parser generator for visibly pushdown grammars. The generated parser accepts ambiguous grammars and produces a parse forest containing all valid parse trees for an input string in linear time. Each parse tree in the forest can then be extracted also in linear time. Besides the parser generator, to allow more flexible forms of the visibly pushdown grammars, we also present a translator that converts a tagged CFG to a visibly pushdown grammar in a sound way, and the parse trees of the tagged CFG are further produced by running the semantic actions embedded in the parse trees of the translated visibly pushdown grammar. The performance of the parser is compared with popular parsing tools, including ANTLR, GNU Bison, and other popular hand-crafted parsers. The correctness and the time complexity of the core parsing algorithm are formally verified in the proof assistant Coq.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",derivative-based parsing; formal verification; Parser generators,Computational linguistics; Formal languages; Semantics; Theorem proving; Derivative-based parsing; Input string; Linear time; Parse forest; Parse trees; Parser generators; Performance; Pushdown; Semantic action; Time complexity; Formal verification
Contextual Linear Types for Differential Privacy,2023,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85165653768&doi=10.1145%2f3589207&partnerID=40&md5=5c1bd4fb1f69921be478dbc7916979f9,"Language support for differentially private programming is both crucial and delicate. While elaborate program logics can be very expressive, type-system-based approaches using linear types tend to be more lightweight and amenable to automatic checking and inference, and in particular in the presence of higher-order programming. Since the seminal design of Fuzz, which is restricted to μ-differential privacy in its original design, significant progress has been made to support more advanced variants of differential privacy, like ( μ, ϵ)-differential privacy. However, supporting these advanced privacy variants while also supporting higher-order programming in full has proven to be challenging. We present Jazz, a language and type system that uses linear types and latent contextual effects to support both advanced variants of differential privacy and higher-order programming. Latent contextual effects allow delaying the payment of effects for connectives such as products, sums, and functions, yielding advantages in terms of precision of the analysis and annotation burden upon elimination, as well as modularity. We formalize the core of Jazz, prove it sound for privacy via a logical relation for metric preservation, and illustrate its expressive power through a number of case studies drawn from the recent differential privacy literature.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",differential privacy; Type systems,Contextual effects; Differential privacies; High order programming; Linear types; Logical relations; Metric preservations; Original design; Product functions; Program logic; Type systems
A First-order Logic with Frames,2023,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85165634224&doi=10.1145%2f3583057&partnerID=40&md5=661c55d734fd33002f83ef3466e06636,"We propose a novel logic, Frame Logic (FL), that extends first-order logic and recursive definitions with a construct Sp(·) that captures the implicit supports of formulas - the precise subset of the universe upon which their meaning depends. Using such supports, we formulate proof rules that facilitate frame reasoning elegantly when the underlying model undergoes change. We show that the logic is expressive by capturing several data-structures and also exhibit a translation from a precise fragment of separation logic to frame logic. Finally, we design a program logic based on frame logic for reasoning with programs that dynamically update heaps that facilitates local specifications and frame reasoning. This program logic consists of both localized proof rules as well as rules that derive the weakest tightest preconditions in frame logic.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",first-order logic; first-order logic with recursive definitions; Frame reasoning; heap verification; program logics; program verification,Computer circuits; First order logic; First-order logic with recursive definition; Frame logic; Frame reasoning; Heap verification; Program logic; Program Verification; Proof rules; Recursive definitions; Formal logic
Omnisemantics: Smooth Handling of Nondeterminism,2023,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150298168&doi=10.1145%2f3579834&partnerID=40&md5=6a77b84ce166887171d56ef0462699b1,"This article gives an in-depth presentation of the omni-big-step and omni-small-step styles of semantic judgments. These styles describe operational semantics by relating starting states to sets of outcomes rather than to individual outcomes. A single derivation of these semantics for a particular starting state and program describes all possible nondeterministic executions (hence the name omni), whereas in traditional small-step and big-step semantics, each derivation only talks about one single execution. This restructuring allows for straightforward modeling of both nondeterminism and undefined behavior as commonly encountered in sequential functional and imperative programs. Specifically, omnisemantics inherently assert safety (i.e., they guarantee that none of the execution branches gets stuck), while traditional semantics need either a separate judgment or additional error markers to specify safety in the presence of nondeterminism.Omnisemantics can be understood as an inductively defined weakest-precondition semantics (or more generally, predicate-transformer semantics) that does not involve invariants for loops and recursion but instead uses unrolling rules like in traditional small-step and big-step semantics. Omnisemantics were previously described in association with several projects, but we believe the technique has been underappreciated and deserves a well-motivated, extensive, and pedagogical presentation of its benefits. We also explore several novel aspects associated with these semantics, in particular, their use in type-safety proofs for lambda calculi, partial-correctness reasoning, and forward proofs of compiler correctness for terminating but potentially nondeterministic programs being compiled to nondeterministic target languages. All results in this article are formalized in Coq.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesNondeterminism; compiler correctness proofs; termination,Biomineralization; Program compilers; Theorem proving; Additional key word and phrasesnondeterminism; Big-step semantics; Compiler correctness; Compiler correctness proof; Correctness proofs; Functional programs; Key words; Non Determinism; Operational semantics; Termination; Semantics
Typed-Untyped Interactions: A Comparative Analysis,2023,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150056985&doi=10.1145%2f3579833&partnerID=40&md5=c3241b3d6f5cd8553791ed3733782c56,The literature presents many strategies for enforcing the integrity of types when typed code interacts with untyped code. This article presents a uniform evaluation framework that characterizes the differences among some major existing semantics for typed-untyped interaction. Type system designers can use this framework to analyze the guarantees of their own dynamic semantics. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.,Additional Key Words and PhrasesComplete monitoring; blame completeness; blame soundness,Additional key word and phrasescomplete monitoring; Blame completeness; Blame soundness; Comparative analyzes; Dynamic semantic; Evaluation framework; Key words; System designers; Type systems; Semantics
Immutability and Encapsulation for Sound OO Information Flow Control,2023,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150267141&doi=10.1145%2f3573270&partnerID=40&md5=99614dbff51a93e3a651081521e53bf0,"Security-critical software applications contain confidential information which has to be protected from leaking to unauthorized systems. With language-based techniques, the confidentiality of applications can be enforced. Such techniques are for example type systems that enforce an information flow policy through typing rules. The precision of such type systems, especially in object-oriented languages, is an area of active research: an appropriate system should not reject too many secure programs while soundly preserving noninterference. In this work, we introduce the language SIFO which supports information flow control for an object-oriented language with type modifiers. Type modifiers increase the precision of the type system by utilizing immutability and uniqueness properties of objects for the detection of information leaks. We present SIFO informally by using examples to demonstrate the applicability of the language, formalize the type system, prove noninterference, implement SIFO as a pluggable type system in the programming language L42, and evaluate it with a feasibility study and a benchmark.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesSecurity; confidentiality; information flow; integrity; mutation control; type system,Application programs; Flow control; Object detection; Additional key word and phrasessecurity; Confidentiality; Information flow control; Information flows; Integrity; Key words; Mutation control; Object-oriented languages; Security-critical; Type systems; Object oriented programming
"The Tortoise and the Hare Algorithm for Finite Lists, Compositionally",2023,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150275669&doi=10.1145%2f3564619&partnerID=40&md5=fb720bb2713a12ca6105d5f741503227,"In the tortoise-and-hare algorithm, when the fast pointer reaches the end of a finite list, the slow pointer points to the middle of this list. In the early 2000's, this property was found to make it possible to program a palindrome detector for immutable lists that operates in one recursive traversal of the given list and performs the smallest possible number of comparisons, using the ""There And Back Again""(TABA) recursion pattern. In this article, this palindrome detector is reconstructed in OCaml, formalized with the Coq Proof Assistant, and proved to be correct. More broadly, this article presents a compositional account of the tortoise-and-hare algorithm for finite lists. Concretely, compositionality means that programs that use a fast and a slow pointer can be expressed with an ordinary fold function for lists and reasoned about using ordinary structural induction on the given list. This article also contains a dozen new applications of the TABA recursion pattern and of its tail-recursive variant, ""There and Forth Again"".  © 2023 Association for Computing Machinery.",Additional Key Words and PhrasesThe tortoise-and-hare algorithm; compositionality; continuations; defunctionalization; fold functions for lists; Gallina; immutable lists; lightweight fusion by fixed-point promotion; OCaml; pure and total functions; slow pointers and fast pointers; structural recursion; TABA (There and Back Again); TAFA (There and Forth Again); the Coq Proof Assistant,Theorem proving; Additional key word and phrasesthe tortoise-and-hare algorithm; Compositionality; Continuation; Defunctionalizations; Fixed points; Fold function for list; Gallina; Immutable list; Key words; Lightweight fusion by fixed-point promotion; Ocaml; Pure and total function; Slow pointer and fast pointer; Structural recursion; The coq proof assistant; There and back again; There and forth again; Linguistics
Towards Porting Operating Systems with Program Synthesis,2023,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150072185&doi=10.1145%2f3563943&partnerID=40&md5=37f9a719485ee7c5ca4b6905cafcebcd,"The end of Moore's Law has ushered in a diversity of hardware not seen in decades. Operating system (OS) (and system software) portability is accordingly becoming increasingly critical. Simultaneously, there has been tremendous progress in program synthesis. We set out to explore the feasibility of using modern program synthesis to generate the machine-dependent parts of an operating system. Our ultimate goal is to generate new ports automatically from descriptions of new machines.One of the issues involved is writing specifications, both for machine-dependent operating system functionality and for instruction set architectures. We designed two domain-specific languages: Alewife for machine-independent specifications of machine-dependent operating system functionality and Cassiopea for describing instruction set architecture semantics. Automated porting also requires an implementation. We developed a toolchain that, given an Alewife specification and a Cassiopea machine description, specializes the machine-independent specification to the target instruction set architecture and synthesizes an implementation in assembly language with a customized symbolic execution engine. Using this approach, we demonstrate the successful synthesis of a total of 140 OS components from two pre-existing OSes for four real hardware platforms. We also developed several optimization methods for OS-related assembly synthesis to improve scalability.The effectiveness of our languages and ability to synthesize code for all 140 specifications is evidence of the feasibility of program synthesis for machine-dependent OS code. However, many research challenges remain; we also discuss the benefits and limitations of our synthesis-based approach to automated OS porting. © 2023 Copyright held by the owner/author(s).",Additional Key Words and PhrasesProgram synthesis; assembly languages; operating systems,Computer architecture; Computer software portability; Problem oriented languages; Semantics; Additional key word and phrasesprogram synthesis; Assembly-language; Instruction set architecture; Key words; Moore Law; Operating system; Porting operating systems; Program synthesis; System functionality; System softwares; Specifications
"Satisfiability Modulo Ordering Consistency Theory for SC, TSO, and PSO Memory Models",2023,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150052298&doi=10.1145%2f3579835&partnerID=40&md5=b2754fb8e64f7ff0480901f835e46e72,"Automatically verifying multi-threaded programs is difficult because of the vast number of thread interleavings, a problem aggravated by weak memory consistency. Partial orders can help with verification because they can represent many thread interleavings concisely. However, there is no dedicated decision procedure for solving partial-order constraints.In this article, we propose a novel ordering consistency theory for concurrent program verification that is applicable not only under sequential consistency, but also under the TSO and PSO weak memory models. We further develop an efficient theory solver, which checks consistency incrementally, generates minimal conflict clauses, and includes a custom propagation procedure. We have implemented our approach in a tool, called Zord, and have conducted extensive experiments on the SV-COMP 2020 ConcurrencySafety benchmarks. Our experimental results show a significant improvement over the state-of-the-art. © 2023 Copyright held by the owner/author(s).",Additional Key Words and PhrasesProgram verification; concurrency; satisfiability modulo theory; weak memory models,Additional key word and phrasesprogram verification; Concurrency; Consistency theory; Interleavings; Key words; Memory modeling; Partial order; Satisfiability; Satisfiability modulo Theories; Weak memory models; Formal logic
A Relational Program Logic with Data Abstraction and Dynamic Framing,2023,ACM Transactions on Programming Languages and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146418751&doi=10.1145%2f3551497&partnerID=40&md5=d968acd241374387f8f2ed7c915055a9,"Dedicated to Tony Hoare. In a paper published in 1972, Hoare articulated the fundamental notions of hiding invariants and simulations. Hiding: invariants on encapsulated data representations need not be mentioned in specifications that comprise the API of a module. Simulation: correctness of a new data representation and implementation can be established by proving simulation between the old and new implementations using a coupling relation defined on the encapsulated state. These results were formalized semantically and for a simple model of state, though the paper claimed this could be extended to encompass dynamically allocated objects. In recent years, progress has been made toward formalizing the claim, for simulation, though mainly in semantic developments. In this article, hiding and simulation are combined with the idea in Hoare's 1969 paper: a logic of programs. For an object-based language with dynamic allocation, we introduce a relational Hoare logic with stateful frame conditions that formalizes encapsulation, hiding of invariants, and couplings that relate two implementations. Relations and other assertions are expressed in first-order logic. Specifications can express a wide range of relational properties such as conditional equivalence and noninterference with declassification. The proof rules facilitate relational reasoning by means of convenient alignments and are shown sound with respect to a conventional operational semantics. A derived proof rule for equivalence of linked programs directly embodies representation independence. Applicability to representative examples is demonstrated using an SMT-based implementation. © 2022 Association for Computing Machinery.",automated verification; data abstraction; logics of programs; product programs; Relational properties; relational verification; representation independence,Abstracting; Computer circuits; Formal logic; Semantics; Automated verification; Data abstraction; Data representations; Logic of programs; Product program; Program logic; Proof rules; Relational properties; Relational verification; Representation independence; Specifications
