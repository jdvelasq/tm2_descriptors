Title,Year,Source title,Link,Abstract,Author Keywords,Index Keywords
Compensating the effect of communication delay in client-server-based shared haptic virtual environments,2015,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84954134894&doi=10.1145%2f2835176&partnerID=40&md5=e62cbdab75a7ae1e5c04b63486a2eb60,"Shared haptic virtual environments can be realized using a client-server architecture. In this architecture, each client maintains a local copy of the virtual environment (VE). A centralized physics simulation running on a server calculates the object states based on haptic device position information received from the clients. The object states are sent back to the clients to update the local copies of the VE, which are used to render interaction forces displayed to the user through a haptic device. Communication delay leads to delayed object state updates and increased force feedback rendered at the clients. In this article, we analyze the effect of communication delay on the magnitude of the rendered forces at the clients for cooperative multi-user interactions with rigid objects. The analysis reveals guidelines on the tolerable communication delay. If this delay is exceeded, the increased force magnitude becomes haptically perceivable. We propose an adaptive force rendering scheme to compensate for this effect, which dynamically changes the stiffness used in the force rendering at the clients. Our experimental results, including a subjective user study, verify the applicability of the analysis and the proposed scheme to compensate the effect of time-varying communication delay in a multi-user SHVE. Copyright © 2015 ACM.",Collaboration; Communication delay; Haptic rendering; Multi-user; Perceived transparency; Shared haptic virtual environment,Client server computer systems; Feedback; Virtual reality; Collaboration; Communication delays; Haptic rendering; Haptic virtual environments; Multi-user; Cooperative communication
Gaze transition entropy,2015,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84954108136&doi=10.1145%2f2834121&partnerID=40&md5=2df2796f50ae44f762eef6af5dfe8a9a,"This article details a two-step method of quantifying eye movement transitions between areas of interest (AOIs). First, individuals' gaze switching patterns, represented by fixated AOI sequences, are modeled as Markov chains. Second, Shannon's entropy coefficient of the fit Markov model is computed to quantify the complexity of individual switching patterns. To determine the overall distribution of attention over AOIs, the entropy coefficient of individuals' stationary distribution of fixations is calculated. The novelty of the method is that it captures the variability of individual differences in eye movement characteristics, which are then summarized statistically. The method is demonstrated on gaze data collected from two studies, during free viewing of classical art paintings. Normalized Shannon's entropy, derived from individual transition matrices, is related to participants' individual differences as well as to either their aesthetic impression or recognition of artwork. Low transition and high stationary entropies suggest greater curiosity mixed with a higher subjective aesthetic affinity toward artwork, possibly indicative of visual scanning of the artwork in a more deliberate way. Meanwhile, both high transition and stationary entropies may be indicative of recognition of familiar artwork. Copyright © 2015 ACM.",Entropy; Eye movement transitions; Markov chain,Chains; Entropy; Markov processes; Entropy coefficients; Individual Differences; Movement characteristics; Shannon's entropy; Stationary distribution; Switching patterns; Transition entropies; Transition matrices; Eye movements
"Strutting hero, sneaking villain: Utilizing body motion cues to predict the intentions of others",2015,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84946575271&doi=10.1145%2f2791293&partnerID=40&md5=140a1bb812e41980cff3d2a39a2bc683,"A better understanding of how intentions and traits are perceived from body movements is required for the design of more effective virtual characters that behave in a socially realistic manner. For this purpose, realistic body motion, captured from human movements, is being used more frequently for creating characters with natural animations in games and entertainment. However, it is not always clear for programmers and designers which specific motion parameters best convey specific information such as certain emotions, intentions, or traits. We conducted two experiments to investigate whether the perceived traits of actors could be determined from their body motion, and whether these traits were associated with their perceived intentions. We first recorded body motions from 26 professional actors, who were instructed to move in a ""hero""-like or a ""villain""-like manner. In the first experiment, 190 participants viewed individual video recordings of these actors and were required to provide ratings to the body motion stimuli along a series of different cognitive dimensions (intentions, attractiveness, dominance, trustworthiness, and distinctiveness). The intersubject ratings across observers were highly consistent, suggesting that social traits are readily determined from body motion. Moreover, correlational analyses between these ratings revealed consistent associations across traits, for example, that perceived ""good"" intentions were associated with higher ratings of attractiveness and dominance. Experiment 2 was designed to elucidate the qualitative body motion cues that were critical for determining specific intentions and traits from the hero- and villain-like body movements. The results revealed distinct body motions that were readily associated with the perception of either ""good"" or ""bad"" intentions. Moreover, regression analyses revealed that these ratings accurately predicted the perception of the portrayed character type. These findings indicate that intentions and social traits are communicated effectively via specific sets of body motion features. Furthermore, these results have important implications for the design of the motion of virtual characters to convey desired social information. © 2015 ACM.",Experimentation; Human factors,Human engineering; Regression analysis; Video recording; Cognitive dimensions; Correlational analysis; Experimentation; Human movements; Motion parameters; Social information; Specific information; Virtual character; Animation
Perception of Emotion and Personality through Full-Body Movement Qualities: A Sport Coach Case Study,2015,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84946567342&doi=10.1145%2f2791294&partnerID=40&md5=509eaebec9638698f561d565e3e244e7,"Virtual sport coaches guide users through their physical activity and provide motivational support. Users' motivation can rapidly decay if the movements of the virtual coach are too stereotyped. Kinematic patterns generated while performing a predefined fitness movement can elicit and help to prolong users' interaction and interest in training. Human body kinematics has been shown to convey various social attributes such as gender, identity, and acted emotions. To date, no study provides information regarding how spontaneous emotions and personality traits together are perceived from full-body movements. In this article, we study how people make reliable inferences regarding spontaneous emotional dimensions and personality traits of human coaches from kinematic patterns they produced when performing a fitness sequence. Movements were presented to participants via a virtual mannequin to isolate the influence of kinematics on perception. Kinematic patterns of biological movement were analyzed in terms of movement qualities according to the effort-shape [Dell 1977] notation proposed by Laban [1950]. Three studies were performed to provide an analysis of the process leading to perception: from coaches' states and traits through bodily movements to observers' social perception. Thirty-two participants (i.e., observers) were asked to rate the movements of the virtual mannequin in terms of conveyed emotion dimensions, personality traits (five-factor model of personality), and perceived movement qualities (effort-shape) from 56 fitness movement sequences. The results showed high reliability for most of the evaluated dimensions, confirming interobserver agreement from kinematics at zero acquaintance. A large expressive halo merging emotional (e.g., perceived intensity) and personality aspects (e.g., extraversion) was found, driven by perceived kinematic impulsivity and energy. Observers' perceptions were partially accurate for emotion dimensions and were not accurate for personality traits. Together, these results contribute to both the understanding of dimensions of social perception through movement and the design of expressive virtual sport coaches.",Experimentation; Human factors,Health; Human engineering; Kinematics; Sports; Biological movements; Emotional dimensions; Emotions and personality; Experimentation; Five-factor model of personality; Full-body movement; Interobserver agreement; Virtual mannequins; Behavioral research
"The effects of visuomotor calibration to the perceived space and body, through embodiment in immersive virtual reality",2015,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84946571784&doi=10.1145%2f2818998&partnerID=40&md5=58ac4acff9b6d4c0beb61a056733bd6b,"We easily adapt to changes in the environment that involve cross-sensory discrepancies (e.g., between vision and proprioception). Adaptation can lead to changes in motor commands so that the experienced sensory consequences are appropriate for the new environment (e.g., we program a movement differently while wearing prisms that shift our visual space). In addition to these motor changes, perceptual judgments of space can also be altered (e.g., how far can I reach with my arm?). However, in previous studies that assessed perceptual judgments of space after visuomotor adaptation, the manipulation was always a planar spatial shift, whereas changes in body perception could not directly be assessed. In this study, we investigated the effects of velocity-dependent (spatiotemporal) and spatial scaling distortions of arm movements on space and body perception, taking advantage of immersive virtual reality. Exploiting the perceptual illusion of embodiment in an entire virtual body, we endowed subjects with new spatiotemporal or spatial 3D mappings between motor commands and their sensory consequences. The results imply that spatiotemporal manipulation of 2 and 4 times faster can significantly change participants' proprioceptive judgments of a virtual object's size without affecting the perceived body ownership, although it did affect the agency of the movements. Equivalent spatial manipulations of 11 and 22 degrees of angular offset also had a significant effect on the perceived virtual object's size; however, the mismatched information did not affect either the sense of body ownership or agency. We conclude that adaptation to spatial and spatiotemporal distortion can similarly change our perception of space, although spatiotemporal distortions can more easily be detected. © 2015 ACM.",Experimentation; Human factors,Human engineering; Virtual reality; Body perceptions; Experimentation; Immersive virtual reality; Sensory discrepancy; Spatial scaling; Spatio-temporal distortions; Velocity-dependent; Virtual objects; Sensory perception
The perception of lighting inconsistencies in composite outdoor scenes,2015,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84941560902&doi=10.1145%2f2810038&partnerID=40&md5=9b21653b2274a1250ad9874595f5735d,"It is known that humans can be insensitive to large changes in illumination. For example, if an object of interest is extracted from one digital photograph and inserted into another, we do not always notice the differences in illumination between the object and its new background. This inability to spot illumination inconsistencies is often the key to success in digital ""doctoring"" operations.We present a set of experiments in which we explore the perception of illumination in outdoor scenes. Our results can be used to predict when and why inconsistencies go unnoticed. Applications of the knowledge gained from our studies include smarter digital ""cut-and-paste"" and digital ""fake"" detection tools, and image-based composite scene backgrounds for layout and previsualization. © 2015 ACM.",Images; Lighting; Perception,Psychology computing; Sensory perception; Cut-and-paste; Detection tools; Digital photographs; Image-based; Images; Outdoor scenes; Previsualization; Spot illumination; Lighting
Introduction to special issue SAP 2015,2015,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84941559057&doi=10.1145%2f2815623&partnerID=40&md5=7bfcb53b0d54cdf56e30c884dd0af121,[No abstract available],,
Evidence that viewers prefer higher frame-rate film,2015,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84941553448&doi=10.1145%2f2810039&partnerID=40&md5=c89c7e9c6547997185efd65cded2362b,"High frame-rate (HFR) movie-making refers to the capture and projection of movies at frame rates several times higher than the traditional 24 frames per second. This higher frame rate theoretically improves the quality of motion portrayed in movies, and helps avoid motion blur, judder, and other undesirable artifacts. However, there is considerable debate in the cinema industry regarding the acceptance of HFR content given anecdotal reports of hyper-realistic imagery that reveals too much set and costume detail. Despite the potential theoretical advantages, there has been little empirical investigation of the impact of high frame-rate techniques on the viewer experience. In this study, we use stereoscopic 3D content, filmed and projected at multiple frame rates (24, 48, and 60 fps), with shutter angles ranging from 180. to 358., to evaluate viewer preferences. In a pairedcomparison paradigm, we assessed preferences along a set of five attributes (realism, motion smoothness, blur/clarity, quality of depth, and overall preference). The resulting data show a clear preference for higher frame rates, particularly when contrasting 24 fps with 48 or 60 fps. We found little impact of shutter angle on viewers' choices, with the exception of one measure (motion smoothness) for one clip type. These data are the first empirical evidence of the advantages afforded by high frame-rate capture and presentation in a cinema context. © 2015 ACM.",Cinema; high Frame rate; Perception; Preference; Stereoscopic 3D,Sensory perception; Stereo image processing; Cinema; Empirical investigation; Frames per seconds; High frame rate; Movie-making; Multiple-frame; Preference; Realistic imagery; Motion pictures
Big foot: Using the size of a virtual foot to scale gap width,2015,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84941554264&doi=10.1145%2f2811266&partnerID=40&md5=24fec0a29f987bd9afcce53766b1f297,"Spatial perception research in the real world and in virtual environments suggests that the body (e.g., hands) plays a role in the perception of the scale of the world. However, little research has closely examined how varying the size of virtual body parts may influence judgments of action capabilities and spatial layout. Here, we questioned whether changing the size of virtual feet would affect judgments of stepping over and estimates of the width of a gap. Participants viewed their disembodied virtual feet as small or large and judged both their ability to step over a gap and the size of gaps shown in the virtual world. Foot size affected both affordance judgments and size estimates such that those with enlarged virtual feet estimated they could step over larger gaps and that the extent of the gap was smaller. Shrunken feet led to the perception of a reduced ability to step over a gap and smaller estimates of width. The results suggest that people use their visually perceived foot size to scale virtual spaces. Regardless of foot size, participants felt that they owned the feet rendered in the virtual world. Seeing disembodied, but motion-tracked, virtual feet affected spatial judgments, suggesting that the presentation of a single tracked body part is sufficient to produce similar effects on perception, as has been observed with the presence of fully co-located virtual self-avatars or other body parts in the past. © 2015 ACM.",Affordances; Avatars; Body perception; Partial self-avatars; Perception; Virtual environments,Sensory perception; Affordances; Avatars; Body perceptions; Partial self-avatars; Spatial judgments; Spatial perception; Virtual selves; Virtual worlds; Virtual reality
Multimodal affect: Perceptually evaluating an affective talking head,2015,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84941557357&doi=10.1145%2f2811265&partnerID=40&md5=6fbc4ab2555afaa36e9203af3a52f086,"Many tasks such as driving or rapidly sorting items can be best achieved by direct actions. Other tasks such as giving directions, being guided through a museum, or organizing a meeting are more easily solved verbally. Since computers are increasingly being used in all aspects of daily life, it would be of great advantage if we could communicate verbally with them. Although advanced interactions with computers are possible, a vast majority of interactions are still based on the WIMP (Window, Icon, Menu, Point) metaphor [Hevner and Chatterjee 2010] and are, therefore, via simple text and gesture commands. The field of affective interfaces is working toward making computers more accessible by giving them (rudimentary) natural-language abilities, including using synthesized speech, facial expressions, and virtual body motions. Once the computer is granted a virtual body, however, it must be given the ability to use it to nonverbally convey socio-emotional information (such as emotions, intentions, mental state, and expectations) or it will likely be misunderstood. Here, we present a simple affective talking head along with the results of an experiment on the multimodal expression of emotion. The results show that although people can sometimes recognize the intended emotion from the semantic content of the text even when the face does not convey affect, they are considerably better at it when the face also shows emotion. Moreover, when both face and text convey emotion, people can detect different levels of emotional intensity. © 2015 ACM.",Affective interfaces; Emotion; Facial animation; Speech,Semantics; Speech; Affective interfaces; Emotion; Emotional information; Facial animation; Facial Expressions; Multimodal expressions; Natural languages; Synthesized speech; Character recognition
Evaluating the color fidelity of ITMOs and HDR color appearance models,2015,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84941554367&doi=10.1145%2f2808232&partnerID=40&md5=f90387c79b7b4e8857c98adf00198b1c,"With the increasing availability of high-dynamic-range (HDR) displays comes the need to remaster existing content in a way that takes advantage of the extended range of luminance and contrast that such displays offer. At the same time, it is crucial that the creative intent of the director is preserved through such changes as much as possible. In this article, we compare several approaches for dynamic range extension to assess their ability to correctly reproduce the color appearance of standard dynamic range (SDR) images on HDR displays. A number of state-of-the-art inverse tone mapping operators (ITMOs) combined with a standard chromatic adaptation transform (CAT) as well as some HDR color appearance models have been evaluated through a psychophysical study, making use of an HDR display as well as HDR ground-truth data. We found that global ITMOs lead to the most reliable performance when combined with a standard CAT, while more complex methods were found to be more scene dependent, and often less preferred than the unprocessed SDR image. HDR color appearance models, albeit being the most complete solutions for accurate color reproduction, were found to not be well suited to the problem of dynamic range expansion, suggesting that further research may be necessary to provide accurate color management in the context of inverse tone mapping. © 2015 ACM.",Color appearance modeling; High-dynamic-range imaging; ITMO; Subjective evaluation,Cell proliferation; Inverse problems; Mapping; Color appearance models; Dynamic range expansions; Dynamic range extension; High dynamic range imaging; Inverse tone mappings; ITMO; Psychophysical studies; Subjective evaluations; Color
"Discrete versus solid: Representing quantity using linear, area, and volume glyphs",2015,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84939780417&doi=10.1145%2f2767129&partnerID=40&md5=7d17466a87632a0557dd71fb5e89e3e8,"It is common in infographics for quantities to be represented by stacks of discrete blocks. For example, a magazine illustration showing automobile production in different countries might use stacks of blocks with each block representing a thousand cars. This is unlike what is done to represent quantity in the charts used by statisticians, or for quantitative glyphs used in maps. In these cases, solid bars or solid area glyphs such as circles are commonly used to represent quantity. This raises the question of whether breaking bars, area, or volume glyphs into discrete blocks can improve the rapid estimation of quantity. We report on a study where participants compared quantities represented using bar, area, and volume glyphs in both solid and discrete variants. The discrete variants used up to 4,4 × 4, and 4 × 4 × 4 blocks or 10,10 × 10, and 10 × 10 × 10 blocks for bar, area, and volume, respectively. The results show that people are significantly more accurate in estimating quantities using the discrete versions, but they take somewhat longer. For both areas and volumes, the accuracy gains were considerable. © 2015 ACM 1544-3558/2015/07-ART12 $15.00.",Glyphs; Representing quantity; Subitizing; Visualization,Flow visualization; Psychology computing; Sensory perception; Automobile production; Glyphs; Infographics; Rapid estimation; Representing quantity; Subitizing; Automotive industry
Exploring the effect of motion type and emotions on the perception of gender in virtual humans,2015,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84939811929&doi=10.1145%2f2767130&partnerID=40&md5=3b85a6a4d2c0512c7b3a40b8fdb9d38f,"In this article, we investigate the perception of gender from the motion of virtual humans under different emotional conditions and explore the effect of emotional bias on gender perception (e.g., anger being attributed to males more than females). As motion types can present different levels of physiological cues, we also explore how two types of motion (walking and conversations) are affected by emotional bias. Walking typically displays more physiological cues about gender (e.g., hip sway) and therefore is expected to be less affected by emotional bias. To investigate these effects, we used a corpus of captured facial and body motions from four male and four female actors, performing basic emotions through conversation and walk. We expected that the appearance of the model would also influence gender perception; therefore, we displayed both male and female motions on two virtual models of different sex. Two experiments were then conducted to assess gender judgments from these motions. In both experiments, participants were asked to rate how male or female they considered the motions to be under different emotional states, then classified the emotions to determine how accurately they were portrayed by actors. Overall, both experiments showed that gender ratings were affected by the displayed emotion. However, we found that conversations were influenced by gender stereotypes to a greater extent than walking motions. This was particularly true for anger, which was perceived as male on both male and female motions, and sadness, which was perceived as less male when portrayed by male actors. We also found a slight effect of the model when observing gender on different types of virtual models. These results have implications for the design and animation of virtual humans. © 2015 ACM 1544-3558/2015/07-ART11 $15.00.",Emotions; Facial animation; Gender,Animation; Physiology; Basic emotions; Emotional state; Emotions; Facial animation; Gender; Gender stereotypes; Virtual humans; Walking motion; Behavioral research
Simulating fixations when looking at visual arts,2015,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84939815073&doi=10.1145%2f2736286&partnerID=40&md5=6bf7a3df610c6125632999451242852f,"When people look at pictures, they fixate on specific areas. The sequences of such fixations are so characteristic for certain pictures that metrics can be derived that allow successful grouping of similar pieces of visual art. However, determining enough fixation sequences by eye tracking is not practically feasible for large groups of people and pictures. In order to get around this limitation, we present a novel algorithm that simulates eye movements by calculating scan paths for images and time frames in real time. The basis of our algorithm is an attention model that combines and optimizes rectangle features with Adaboost. The model is adapted to the characteristics of the retina, and its input is dependent on a few earlier fixations. This method results in significant improvements compared to previous approaches. Our simulation process delivers the same data structures as an eye tracker, thus can be analyzed by standard eye-tracking software. A comparison with recorded data from eye tracking experiments shows that our algorithm for simulating fixations has a very good prediction quality for the stimulus areas on which many subjects focus. We also compare the results with those from earlier works. Finally, we demonstrate how the presented algorithm can be used to calculate the similarity of pictures in terms of human perception. © 2015 ACM 1544-3558/2015/06-ART9 $15.00.",Eye movement; Perception; Visual attention,Adaptive boosting; Algorithms; Behavioral research; Computer software; Sensory perception; Target tracking; Attention model; Human perception; Novel algorithm; Prediction quality; Rectangle features; Simulation process; Specific areas; Visual Attention; Eye movements
Perceptual tolerance to stereoscopic 3D image distortion,2015,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84939783045&doi=10.1145%2f2770875&partnerID=40&md5=7b26c0b5b8c695d95deb3e0621dd2f28,"An intriguing aspect of picture perception is the viewer's tolerance to variation in viewing position, perspective, and display size. These factors are also present in stereoscopic media, where there are additional parameters associated with the camera arrangement (e.g., separation, orientation). The predicted amount of depth from disparity can be obtained trigonometrically; however, perceived depth in complex scenes often differs from geometric predictions based on binocular disparity alone. To evaluate the extent and the cause of deviations from geometric predictions of depth from disparity in naturalistic scenes, we recorded stereoscopic footage of an indoor scene with a range of camera separations (camera interaxial (IA) ranged from 3 to 95 mm) and displayed them on a range of screen sizes. In a series of experiments participants estimated 3D distances in the scene relative to a reference scene, compared depth between shots with different parameters, or reproduced the depth between pairs of objects in the scene using reaching or blind walking. The effects of IA and screen size were consistently and markedly smaller than predicted from the binocular viewing geometry, suggesting that observers are able to compensate for the predicted distortions. We conclude that the presence of multiple realistic monocular depth cues drives normalization of perceived depth from binocular disparity. It is not clear to what extent these differences are due to cognitive as opposed to perceptual factors. However, it is notable that these normalization processes are not task specific; they are evident in both perception- and action-oriented tasks. © 2015 ACM 1544-3558/2015/07-ART10 $15.00.",Convergence; Film; Interaxial; Scaling; Stereoscopic 3D,Binoculars; Cameras; Films; Forecasting; Geometry; Screen printing; Binocular disparity; Convergence; Geometric predictions; Interaxial; Normalization process; Perception and actions; Perceptual factors; Scaling; Stereo image processing
Investigating perceived emotional correlates of rhythmic density in algorithmic music composition,2015,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84939808267&doi=10.1145%2f2749466&partnerID=40&md5=b447baed86297376f064ef3d1244fbbd,"Affective algorithmic composition is a growing field that combines perceptually motivated affective computing strategies with novel music generation. This article presents work toward the development of one application. The long-term goal is to develop a responsive and adaptive system for inducing affect that is both controlled and validated by biophysical measures. Literature documenting perceptual responses to music identifies a variety of musical features and possible affective correlations, but perceptual evaluations of these musical features for the purposes of inclusion in a music generation system are not readily available. A discrete feature, rhythmic density (a function of note duration in each musical bar, regardless of tempo), was selected because it was shown to be well-correlated with affective responses in existing literature. A prototype system was then designed to produce controlled degrees of variation in rhythmic density via a transformative algorithm. A two-stage perceptual evaluation of a stimulus set created by this prototype was then undertaken. First, listener responses from a pairwise scaling experiment were analyzed via Multidimensional Scaling Analysis (MDS). The statistical best-fit solution was rotated such that stimuli with the largest range of variation were placed across the horizontal plane in two dimensions. In this orientation, stimuli with deliberate variation in rhythmic density appeared farther from the source material used to generate them than from stimuli generated by random permutation. Second, the same stimulus set was then evaluated according to the order suggested in the rotated two-dimensional solution in a verbal elicitation experiment. A Verbal Protocol Analysis (VPA) found that listener perception of the stimulus set varied in at least two commonly understood emotional descriptors, which might be considered affective correlates of rhythmic density. Thus, these results further corroborate previous studies wherein musical parameters are monitored for changes in emotional expression and that some similarly parameterized control of perceived emotional content in an affective algorithmic composition system can be achieved and provide a methodology for evaluating and including further possible musical features in such a system. Some suggestions regarding the test procedure and analysis techniques are also documented here. © 2015 ACM 1544-3558/2015/06-ART8$15.00.",Affect; Algorithmic composition; Music perception; Rhythm,Adaptive systems; Testing; Affect; Algorithmic compositions; Algorithmic music compositions; Multidimensional scaling analysis; Music perception; Rhythm; Two-dimensional solutions; Verbal protocol analysis; Behavioral research
Welcome message from the new editors-in-chief,2015,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84939810025&doi=10.1145%2f2798732&partnerID=40&md5=33dfb7d3e13ac4d8e3b6752de4cdbcf6,[No abstract available],,
Affordance judgments in HMD-based virtual environments: Stepping over a pole and stepping off a ledge,2015,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929164477&doi=10.1145%2f2720020&partnerID=40&md5=894c7eef3c32705c3bbfa2911e7c20d1,"People judge what they can and cannot do all the time when acting in the physical world. Can I step over that fence or do I need to duck under it? Can I step off of that ledge or do I need to climb off of it? These qualities of the environment that people perceive that allow them to act are called affordances. This article compares people's judgments of affordances on two tasks in both the real world and in virtual environments presented with head-mounted displays. The two tasks were stepping over or ducking under a pole, and stepping straight off of a ledge. Comparisons between the real world and virtual environments are important because they allow us to evaluate the fidelity of virtual environments. Another reason is that virtual environment technologies enable precise control of the myriad perceptual cues at work in the physical world and deepen our understanding of how people use vision to decide how to act. In the experiments presented here, the presence or absence of a self-avatar - an animated graphical representation of a person embedded in the virtual environment - was a central factor. Another important factor was the presence or absence of action, that is, whether people performed the task or reported that they could or could not perform the task. The results show that animated self-avatars provide critical information for people deciding what they can and cannot do in virtual environments, and that action is significant in people's affordance judgments. © 2015 ACM.",Affordances; Head-mounted displays; Height perception; Virtual reality,Poles; Sensory perception; Virtual reality; Affordances; Environment technologies; Graphical representations; Head mounted displays; Physical world; Precise control; Real-world; Step-overs; Helmet mounted displays
Auditory distance presentation in an urban augmented Reality Environment,2015,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84925737370&doi=10.1145%2f2723568&partnerID=40&md5=c13359de51a53f4616a32713bb06a86e,"Presenting points of interest in the environment by means of audio augmented reality offers benefits compared with traditional visual augmented reality and map-based approaches. However, presentation of distant virtual sound sources is problematic. This study looks at combining well-known auditory distance cues to convey the distance of points of interest. The results indicate that although the provided cues are intuitively mapped to relatively short distances, users can with only little training learn to map these cues to larger distances. © 2015 ACM.",Auditory display; Distance perception; Localization; Spatial sound,Depth perception; Audio augmented reality; Auditory display; Distance perception; Localization; Map-based approach; Points of interest; Spatial sound; Virtual sound sources; Augmented reality
Visual enhancement of MR angiography images to facilitate planning of arteriovenous malformation interventions,2015,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929173416&doi=10.1145%2f2701425&partnerID=40&md5=29c99ab3274479a565ecf8c297c93e77,"The primary purpose of medical image visualization is to improve patient outcomes by facilitating the inspection, analysis, and interpretation of patient data. This is only possible if the users' perceptual and cognitive limitations are taken into account during every step of design, implementation, and evaluation of interactive displays. Visualization of medical images, if executed effectively and efficiently, can empower physicians to explore patient data rapidly and accurately with minimal cognitive effort. This article describes a specific case study in biomedical visualization system design and evaluation, which is the visualization of MR angiography images for planning arteriovenous malformation (AVM) interventions. The success of an AVM intervention greatly depends on the surgeon gaining a full understanding of the anatomy of the malformation and its surrounding structures. Accordingly, the purpose of this study was to investigate the usability of visualization modalities involving contour enhancement and stereopsis in the identification and localization of vascular structures using objective user studies. Our preliminary results indicate that contour enhancement, particularly when combined with stereopsis, results in improved performance enhancement of the perception of connectivity and relative depth between different structures. © 2015 ACM.",Arteriovenous malformation; Cel shading; Contour enhancement; MR angiography; Stereopsis,Angiography; Hospital data processing; Medical imaging; Visualization; Arteriovenous malformation; Biomedical visualization; Cel shading; Cognitive limitations; Design and evaluations; MR angiography; Performance enhancements; Stereopsis; Data visualization
The influence of the stereo base on blind and sighted reaches in a virtual environment,2015,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84925696610&doi=10.1145%2f2724716&partnerID=40&md5=070b0b4f8c3c1b9e8e150e3db7a0f239,"In virtual environments, perceived distances are frequently reported to be shorter than intended. One important parameter for spatial perception in a stereoscopic virtual environment is the stereo base-that is, the distance between the two viewing cameras. We systematically varied the stereo base relative to the interpupillary distance (IPD) and examined influences on distance and size perception. Furthermore, we tested whether an individual adjustment of the stereo base through an alignment task would reduce the errors in distance estimation. Participants performed reaching movements toward a virtual tennis ball either with closed eyes (blind reaches) or open eyes (sighted reaches). Using the participants' individual IPD, the stereo base was set to (a) the IPD, (b) proportionally smaller, (c) proportionally larger, or (d) adjusted according to the individual performance in an alignment task that was conducted beforehand. Overall, consistent with previous research, distances were underestimated. As expected, with a smaller stereo base, the virtual object was perceived as being farther away and bigger, in contrast to a larger stereo base, where the virtual object was perceived to be nearer and smaller. However, the manipulation of the stereo base influenced blind reaching estimates to a smaller extent than expected, which might be due to a combination of binocular disparity and pictorial depth cues. In sighted reaching, when visual feedback was available, presumably the use of disparity matching led to a larger effect of the stereo base. The use of an individually adjusted stereo base diminished the average underestimation but did not reduce interindividual variance. Interindividual differences were task specific and could not be explained through differences in stereo acuity or fixation disparity. © 2015 ACM.",Depth perception; Distance estimation; Stereoscopic displays; Virtual environments,Alignment; Depth perception; Eye movements; Stereo image processing; Visual communication; Binocular disparity; Distance estimation; Individual performance; Inter-individual differences; Perceived distances; Spatial perception; Stereoscopic display; Stereoscopic virtual environments; Virtual reality
Depth artifacts caused by spatial interlacing in stereoscopic 3D displays,2015,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84923468629&doi=10.1145%2f2699266&partnerID=40&md5=300ad8acef979a685be373f482e5e09f,"Most spatially interlacing stereoscopic 3D displays display odd and even rows of an image to either the left or the right eye of the viewer. The visual system then fuses the interlaced image into a single percept. This row-based interlacing creates a small vertical disparity between the images; however, interlacing may also induce horizontal disparities, thus generating depth artifacts. Whether people perceive the depth artifacts and, if so, what is the magnitude of the artifacts are unknown. In this study, we hypothesized and tested if people perceive interlaced edges on different depth levels. We tested oblique edge orientations ranging from 2 degrees to 32 degrees and pixel sizes ranging from 16 to 79 arcsec of visual angle in a depth probe experiment. Five participants viewed the visual stimuli through a stereoscope under three viewing conditions: noninterlaced, interlaced, and row averaged (i.e., where even and odd rows are averaged). Our results indicated that people perceive depth artifacts when viewing interlaced stereoscopic images and that these depth artifacts increase with pixel size and decrease with edge orientation angle. A pixel size of 32 arcsec of visual angle still evoked depth percepts, whereas 16 arcsec did not. Row-averaging images effectively eliminated these depth artifacts. These findings have implications for display design, content production, image quality studies, and stereoscopic games and software. © 2015 ACM.",Binocular perception; Depth artifacts; Passive 3D displays; Stereoscopic displays,Photography; Pixels; Stereo image processing; 3-D displays; Binocular perception; Content production; Depth artifacts; Horizontal disparity; Stereoscopic 3-D display; Stereoscopic display; Viewing conditions; Three dimensional displays
Eye height manipulations: A possible solution to reduce underestimation of egocentric distances in head-mounted displays,2015,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84923493932&doi=10.1145%2f2699254&partnerID=40&md5=d7e7f7d4cdab20f678a76a09b738d786,"Virtual reality technology can be considered a multipurpose tool for diverse applications in various domains, for example, training, prototyping, design, entertainment, and research investigating human perception. However, for many of these applications, it is necessary that the designed and computer-generated virtual environments are perceived as a replica of the real world. Many research studies have shown that this is not necessarily the case. Specifically, egocentric distances are underestimated compared to real-world estimates regardless of whether the virtual environment is displayed in a head-mounted display or on an immersive large-screen display. While the main reason for this observed distance underestimation is still unknown, we investigate a potential approach to reduce or even eliminate this distance underestimation. Building up on the angle of declination below the horizon relationship for perceiving egocentric distances, we describe how eye height manipulations in virtual reality should affect perceived distances. In addition, we describe how this relationship could be exploited to reduce distance underestimation for individual users. In a first experiment, we investigate the influence of a manipulated eye height on an action-based measure of egocentric distance perception. We found that eye height manipulations have similar predictable effects on an action-based measure of egocentric distance as we previously observed for a cognitive measure. This might make this approach more useful than other proposed solutions across different scenarios in various domains, for example, for collaborative tasks. In three additional experiments, we investigate the influence of an individualized manipulation of eye height to reduce distance underestimation in a sparse-cue and a rich-cue environment. In these experiments, we demonstrate that a simple eye height manipulation can be used to selectively alter perceived distances on an individual basis, which could be helpful to enable every user to have an experience close to what was intended by the content designer.",Distance perception; Distance underestimation; Eye height; Virtual reality,Depth perception; Helmet mounted displays; Street traffic control; User experience; Additional experiments; Distance perception; Distance underestimation; Diverse applications; Eye heights; Head mounted displays; Large screen displays; Virtual reality technology; Virtual reality
"The effect of wrinkles, presentation mode, and intensity on the perception of facial actions and full-face expressions of laughter",2015,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84923487895&doi=10.1145%2f2699255&partnerID=40&md5=617cb220468ea524236e14f31a524696,"This article focuses on the identification and perception of facial action units displayed alone as well as the meaning decoding and perception of full-face synthesized expressions of laughter. We argue that the adequate representation of single action units is important in the decoding and perception of full-face expressions. In particular, we focus on three factors that may influence the identification and perception of single actions and full-face expressions: their presentation mode (static vs. dynamic), their intensity, and the presence of wrinkles. For the purpose of this study, we used a hybrid approach for animation synthesis that combines data-driven and procedural animations with synthesized wrinkles generated using a bump mapping method. Using such animation technique, we created animations of single action units and full-face movements of two virtual characters. Next, we conducted two studies to evaluate the role of presentation mode, intensity, and wrinkles in single actions and full-face context-free expressions. Our evaluation results show that intensity and presentation mode influence (1) the identification of single action units and (2) the perceived quality of the animation. At the same time, wrinkles (3) are useful in the identification of a single action unit and (4) influence the perceived meaning attached to the animation of full-face expressions. Thus, all factors are important for successful communication of expressions displayed by virtual characters. © 2015 ACM.",Action units; Facial expressions; Laughter; Wrinkles,Animation; Decoding; Action Unit; Animation synthesis; Animation techniques; Facial Expressions; Laughter; Presentation modes; Procedural animation; Wrinkles; Quality control
Analysis of stereoscopic images as a new method for daylighting studies,2015,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84925622567&doi=10.1145%2f2665078&partnerID=40&md5=2315dde252f28ff63fd18b2eab92a1fe,"This article presents the comparison analysis and results of an experiment designed with two presentation modes: real environments and stereoscopic images. The aim of this article is of a methodological nature, with a main objective of analyzing the usability of stereoscopic image presentation as a research tool to evaluate the daylight impact on the perceived architectural quality of small rooms. Twenty-six participants evaluated 12 different stimuli, divided in equal parts between real rooms and stereoscopic images. The stimuli were two similar rooms of different achromatic-colored surfaces (white and black) with three different daylight openings in each room. The participants assessed nine architectural quality attributes on a semantic differential scale. A pragmatic statistical approach (Bland-Altman Approach) for assessing agreement between two methods was used. Results suggest that stereoscopic image presentation is an accurate method to be used when evaluating all nine attributes in the white room and nearly all attributes in the black room. © 2015 Copyright is held by the owner/author(s). Publication rights licensed to ACM.",Achromatic color; Aesthetic perception; Daylighting studies; Method comparison; Stereoscopic imaging; Visualization,Daylighting; Flow visualization; Image analysis; Semantics; Stereo image processing; Three dimensional computer graphics; Achromatic colors; Aesthetic perception; Architectural quality; Comparison analysis; Method comparison; Semantic differential scale; Statistical approach; Stereoscopic imaging; Quality control
On the benefits of using constant visual angle glyphs in interactive exploration of 3D scatterplots,2014,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84916926345&doi=10.1145%2f2677971&partnerID=40&md5=7067c15d50ab5f7cc6e90ffc44b22b1b,"Visual exploration of clouds of data points is an important application of virtual environments. The common goal of this activity is to use the strengths of human perception to identify interesting structures in data, which are often not detected using traditional, computational analysis methods. In this article, we seek to identify some of the parameters that affect how well structures in visualized data clouds can be identified by a human observer. Two of the primary factors tested are the volumetric densities of the visualized structures and the presence/absence of clutter around the displayed structures. Furthermore, we introduce a new approach to glyph visualization-constant visual angle (CVA) glyphs-which has the potential to mitigate the effect of clutter at the cost of dispensing with the common real-world depth cue of relative size. In a controlled experiment where test subjects had to locate and select visualized structures in an immersive virtual environment, we identified several significant results. One result is that CVA glyphs ease perception of structures in cluttered environments while not deteriorating it when clutter is absent. Another is the existence of threshold densities, above which perception of structures becomes easier and more precise. © 2014 ACM 1544-3558/2014/12-ART19 $15.00.",3D shape perception; Glyphs; Scatterplots; Visualization,Clutter (information theory); Flow visualization; Radar clutter; Visualization; 3D shape perception; Cluttered environments; Computational analysis; Controlled experiment; Glyphs; Immersive virtual environments; Interactive exploration; Scatter plots; Three dimensional computer graphics
Depth of field affects perceived depth in stereographs,2014,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84916939240&doi=10.1145%2f2667227&partnerID=40&md5=d7dcad839ac128a37ea6daea93d01f35,"Although it has been reported that depth of field influences depth perception in nonstereo photographs, it remains unclear how depth of field affects depth perception under stereo viewing conditions. We showed participants stereo photographs with different depths of field using a Wheatstone stereoscope and a commercially available 3D TV. The depicted scene contained a floor, a background, and a measuring probe at different locations. Participants drew a floor plan of the depicted scene to scale. We found that perceived depth decreased with decreasing depth of field for shallow depths of field in scenes containing a heightin-the-field cue. For larger depths of field, different effects were found depending on the display system and the viewing distance. There was no effect on perceived depth using the 3D TV, but perceived depth decreased with increasing depth of field using the Wheatstone stereoscope. However, in the 3D TV case, we found that the perceived depth decreased with increasing depth of field in scenes in which the height-in-the-field cue was removed. This indicates that the effect of depth of field on perceived depth may be influenced by other depth cues in the scene, such as height-in-the-field cues. © 2014 ACM 1544-3558/2014/12-ART18 $15.00.",3D TV; Binocular disparity; Depth of field; Depth perception; Stereoscope,Depth perception; Display devices; Floors; Stereo image processing; Binocular disparity; Depth of field; Different effects; Shallow depths; Stereo-photographs; Stereoscope; Viewing conditions; Viewing distance; Photography
An automated high-level saliency predictor for smart game balancing,2014,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84916942010&doi=10.1145%2f2637479&partnerID=40&md5=b1f7eeebb4c9cdb6e21c26367199c80a,"Successfully predicting visual attention can significantly improve many aspects of computer graphics: scene design, interactivity and rendering. Most previous attention models are mainly based on low-level image features, and fail to take into account highlevel factors such as scene context, topology, or task. Low-level saliency has previously been combined with task maps, but only for predetermined tasks. Thus, the application of these methods to graphics (e.g., for selective rendering) has not achieved its full potential. In this article, we present the first automated high-level saliency predictor incorporating two hypotheses from perception and cognitive science that can be adapted to different tasks. The first states that a scene is comprised of objects expected to be found in a specific context as well objects out of context which are salient (scene schemata) while the other claims that viewer's attention is captured by isolated objects (singletons). We propose a new model of attention by extending Eckstein's Differential Weighting Model. We conducted a formal eye-tracking experiment which confirmed that object saliency guides attention to specific objects in a game scene and determined appropriate parameters for a model. We present a GPU-based system architecture that estimates the probabilities of objects to be attended in real-time. We embedded this tool in a game level editor to automatically adjust game level difficulty based on object saliency, offering a novel way to facilitate game design. We perform a study confirming that game level completion time depends on object topology as predicted by our system. © 2014 ACM 1544-3558/2014/12-ART17 $15.00.",Computer graphics; Game balancing; Scene schemata,Behavioral research; Computer games; Computer graphics; Eye tracking; Object tracking; Topology; Visual communication; Cognitive science; Game balancing; High-level factors; Low-level image features; Model of attention; Scene schemata; Selective rendering; System architectures; Rendering (computer graphics)
Design and analysis of predictive sampling of haptic signals,2014,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84916877336&doi=10.1145%2f2670533&partnerID=40&md5=0d358e52b835f3dc53a548c19a8bd240,"In this article, we identify adaptive sampling strategies for haptic signals. Our approach relies on experiments wherein we record the response of several users to haptic stimuli. We then learn different classifiers to predict the user response based on a variety of causal signal features. The classifiers that have good prediction accuracy serve as candidates to be used in adaptive sampling. We compare the resultant adaptive samplers based on their rate-distortion tradeoff using synthetic as well as natural data. For our experiments, we use a haptic device with a maximum force level of 3 N and 10 users. Each user is subjected to several piecewise constant haptic signals and is required to click a button whenever he perceives a change in the signal. For classification, we not only use classifiers based on level crossings and Weber's law but also random forests using a variety of causal signal features. The random forest typically yields the best prediction accuracy and a study of the importance of variables suggests that the level crossings and Weber's classifier features are most dominant. The classifiers based on level crossings and Weber's law have good accuracy (more than 90%) and are only marginally inferior to random forests. The level crossings classifier consistently outperforms the one based on Weber's law even though the gap is small. Given their simple parametric form, the level crossings and Weber's law-based classifiers are good candidates to be used for adaptive sampling. We study their rate-distortion performance and find that the level crossing sampler is superior. For example, for haptic signals obtained while exploring various rendered objects, for an average sampling rate of 10 samples per second, the level crossings adaptive sampler has a mean square error about 3dB less than the Weber sampler. © 2014 ACM 1544-3558/2014/12-ART16 $15.00.",Adaptive sampling; Decision tree; Level crossings; Linear regression; Random forest; Rate-distortion curve; Weber's law,Classification (of information); Decision trees; Electric distortion; Forecasting; Image coding; Linear regression; Mean square error; Railroad crossings; Random forests; Signal distortion; Adaptive sampling; Adaptive sampling strategies; Level crossing; Piece-wise constants; Rate distortion curves; Rate distortion performance; Rate distortion trade-off; Weber's law; Signal sampling
Biometrics via oculomotor plant characteristics: Impact of parameters in oculomotor plant model,2014,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84916896949&doi=10.1145%2f2668891&partnerID=40&md5=06e343df77ef12bdb83a0069bc77247f,"This article proposes and evaluates a novel biometric approach utilizing the internal, nonvisible, anatomical structure of the human eye. The proposed method estimates the anatomical properties of the human oculomotor plant from the measurable properties of human eye movements, utilizing a two-dimensional linear homeomorphic model of the oculomotor plant. The derived properties are evaluated within a biometric framework to determine their efficacy in both verification and identification scenarios. The results suggest that the physical properties derived from the oculomotor plant model are capable of achieving 20.3% equal error rate and 65.7% rank-1 identification rate on high-resolution equipment involving 32 subjects, with biometric samples taken over four recording sessions; or 22.2% equal error rate and 12.6% rank-1 identification rate on low-resolution equipment involving 172 subjects, with biometric samples taken over two recording sessions. © 2014 ACM 1544-3558/2014/12-ART20 $15.00.",Biological system modeling; Human oculomotor system; Mathematical model; Security and protection,Biometrics; Eye movements; Mathematical models; Anatomical properties; Anatomical structures; Biological system modeling; Derived properties; Human oculomotor system; Identification rates; Plant characteristics; Security and protection; Biological systems
The role of individual difference in judging expressiveness of computer-assisted music performances by experts,2014,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84916897776&doi=10.1145%2f2668124&partnerID=40&md5=a6f202bf8e5926bc98903139d5115f28,"Computational systems for generating expressive musical performances have been studied for several decades now. These models are generally evaluated by comparing their predictions with actual performances, both from a performance parameter and a subjective point of view, often focusing on very specific aspects of the model. However, little is known about how listeners evaluate the generated performances and what factors influence their judgement and appreciation. In this article, we present two studies, conducted during two dedicated workshops, to start understanding how the audience judges entire performances employing different approaches to generating musical expression. In the preliminary study, 40 participants completed a questionnaire in response to five different computer-generated and computer-assisted performances, rating preference and describing the expressiveness of the performances. In the second, ""GATM"" (Gruppo di Analisi e Teoria Musicale) study, 23 participants also completed the Music Cognitive Style questionnaire. Results indicated that music systemizers tend to describe musical expression in terms of the formal aspects of the music, and music empathizers tend to report expressiveness in terms of emotions and characters. However, high systemizers did not differ from high empathizers in their mean preference score across the five pieces. We also concluded that listeners tend not to focus on the basic technical aspects of playing when judging computer-assisted and computer-generated performances. Implications for the significance of individual differences in judging musical expression are discussed. © 2014 ACM 1544-3558/2014/12-ART22 $15.00.",Cognitive styles; Human-centered computing; Music performance,Psychology computing; Sensory perception; Cognitive styles; Computational system; Computer generated; Human-centered computing; Individual Differences; Music performance; Musical performance; Performance parameters; Surveys
Feel effects: Enriching storytelling with haptic feedback,2014,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84907937334&doi=10.1145%2f2641570&partnerID=40&md5=ee016a05753d95493985a0884060b8e0,"Despite a long history of use in communication, haptic feedback is a relatively new addition to the toolbox of special effects. Unlike artists who use sound or vision, haptic designers cannot simply access libraries of effects that map cleanly to media content, and they lack even guiding principles for creating such effects. In this article, we make progress toward both capabilities: we generate a foundational library of usable haptic vocabulary and do so with a methodology that allows ongoing additions to the library in a principled and effective way. We define a feel effect as an explicit pairing between a meaningful linguistic phrase and a rendered haptic pattern. Our initial experiment demonstrates that users who have only their intrinsic language capacities, and no haptic expertise, can generate a core set of feel effects that lend themselves via semantic inference to the design of additional effects. The resulting collection of more than 40 effects covers a wide range of situations (including precipitation, animal locomotion, striking, and pulsating events) and is empirically shown to produce the named sensation for the majority of our test users in a second experiment. Our experiments demonstrate a unique and systematic approach to designing a vocabulary of haptic sensations that are related in both the semantic and parametric spaces. © 2014 ACM.",Authoring tools; Feel effect; Haptic vocabulary; Haptics media; Vibrotactile feedback,Authoring tool; Feel effect; Haptic vocabulary; Haptics; Vibro-tactile feedbacks
Stereo day-for-night: Retargeting disparity for scotopic vision,2014,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84907810029&doi=10.1145%2f2644813&partnerID=40&md5=09a4a6f760c432ae08551d95f1762650,"Several approaches attempt to reproduce the appearance of a scotopic low-light night scene on a photopic display (""day-fornight"") by introducing color desaturation, loss of acuity, and the Purkinje shift toward blue colors. We argue that faithful stereo reproduction of night scenes on photopic stereo displays requires manipulation of not only color but also binocular disparity. To this end, we performed a psychophysics experiment to devise a model of disparity at scotopic luminance levels. Using this model, we can match binocular disparity of a scotopic stereo content displayed on a photopic monitor to the disparity that would be perceived if the scene was actually scotopic. The model allows for real-time computation of common stereo content as found in interactive applications such as simulators or computer games. © 2014 ACM.",Night vision; Scotopic vision; Stereoscopic 3D,Binoculars; Bins; Color; Computer games; Stereo image processing; Binocular disparity; Desaturation; Interactive applications; Luminance levels; Night vision; Real-time computations; Stereo displays; Stereo reproductions; Stereo vision
Spatial and temporal linearities in posed and spontaneous smiles,2014,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84907854563&doi=10.1145%2f2641569&partnerID=40&md5=567921c9ed20724ace4c3ccf2d1f110c,"Creating facial animations that convey an animator's intent is a difficult task because animation techniques are necessarily an approximation of the subtle motion of the face. Some animation techniques may result in linearization of the motion of vertices in space (blendshapes, for example), and other, simpler techniques may result in linearization of the motion in time. In this article, we consider the problem of animating smiles and explore how these simplifications in space and time affect the perceived genuineness of smiles. We create realistic animations of spontaneous and posed smiles from high-resolution motion capture data for two computer-generated characters. The motion capture data is processed to linearize the spatial or temporal properties of the original animation. Through perceptual experiments, we evaluate the genuineness of the resulting smiles. Both space and time impact the perceived genuineness. We also investigate the effect of head motion in the perception of smiles and show similar results for the impact of linearization on animations with and without head motion. Our results indicate that spontaneous smiles are more heavily affected by linearizing the spatial and temporal properties than posed smiles. Moreover, the spontaneous smiles were more affected by temporal linearization than spatial linearization. Our results are in accordance with previous research on linearities in facial animation and allow us to conclude that a model of smiles must include a nonlinear model of velocities. © 2014 ACM.",Smile animation,Linearization; Animation techniques; Computer generated characters; Facial animation; High resolution; Motion capture data; Non-linear model; Subtle motions; Temporal property; Animation
Gaze-to-object mapping during visual search in 3D virtual environments,2014,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84907811480&doi=10.1145%2f2644812&partnerID=40&md5=1ab488ce25eaa74af50348c5eaee7c08,"Stimuli obtained from highly dynamic 3D virtual environments and synchronous eye-tracking data are commonly used by algorithms that strive to correlate gaze to scene objects, a process referred to as gaze-to-object mapping (GTOM). We propose to address this problem with a probabilistic approach using Bayesian inference. The desired result of the inference is a predicted probability density function (PDF) specifying for each object in the scene a probability to be attended by the user. To evaluate the quality of a predicted attention PDF, we present a methodology to assess the information value (i.e., likelihood) in the predictions of different approaches that can be used to infer object attention. To this end, we propose an experiment based on a visual search task, which allows us to determine the object of attention at a certain point in time under controlled conditions. We perform this experiment with a wide range of static and dynamic visual scenes to obtain a ground-truth evaluation dataset, allowing us to assess GTOM techniques in a set of 30 particularly challenging cases. © 2014 ACM.",Eye-tracking; Gaze analysis; Gaze processing; Gaze-to-object mapping; Object-based attention; Video games; Virtual environments; Visual attention,Eye-tracking; Gaze analysis; Object-based attention; Video game; Visual Attention; Virtual reality
Can i recognize my body's weight? the influence of shape and texture on the perception of self,2014,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84907812589&doi=10.1145%2f2641568&partnerID=40&md5=caaaca3ea94a69e951c3dafcc90fc78b,"The goal of this research was to investigate women's sensitivity to changes in their perceived weight by altering the body mass index (BMI) of the participants' personalized avatars displayed on a large-screen immersive display. We created the personalized avatars with a full-body 3D scanner that records the participants' body geometry and texture. We altered the weight of the personalized avatars to produce changes in BMI while keeping height, arm length, and inseam fixed and exploited the correlation between body geometry and anthropometric measurements encapsulated in a statistical body shape model created from thousands of body scans. In a 2 × 2 psychophysical experiment, we investigated the relative importance of visual cues, namely shape (own shape vs. an average female body shape with equivalent height and BMI to the participant) and texture (own photorealistic texture or checkerboard pattern texture) on the ability to accurately perceive own current body weight (by asking the participant, ""Is it the same weight as you?""). Our results indicate that shape (where height and BMI are fixed) had little effect on the perception of body weight. Interestingly, the participants perceived their body weight veridically when they saw their own photo-realistic texture. As compared to avatars with photo-realistic texture, the avatars with checkerboard texture needed to be significantly thinner in order to represent the participants' current weight. This suggests that in general the avatars with checkerboard texture appeared bigger. The range that the participants accepted as their own current weight was approximately a 0.83% to ?6.05% BMI change tolerance range around their perceived weight. Both the shape and the texture had an effect on the reported similarity of the body parts and the whole avatar to the participant's body. This work has implications for new measures for patients with body image disorders, as well as researchers interested in creating personalized avatars for games, training applications, or virtual reality. © 2014 ACM.",3D shape; Avatar; BMI; Body perception; Human perception and performance; Texture; Virtual environments; Visual psychophysics,Virtual reality; 3-D shape; Avatar; BMI; Body perceptions; Human perception; Visual psychophysics; Textures
Introduction to special issue SAP 2014,2014,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84912034513&doi=10.1145%2f2670636&partnerID=40&md5=678bd8c3c010e826e37f877f062c2755,[No abstract available],,
Human perception of visual realism for photo and computer-generated face images,2014,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84906848880&doi=10.1145%2f2620030&partnerID=40&md5=b3c9b92825673db0de57a7d72a8032a7,"Computer-generated (CG) face images are common in video games, advertisements, and other media. CG faces vary in their degree of realism, a factor that impacts viewer reactions. Therefore, efficient control of visual realism of face images is important. Efficient control is enabled by a deep understanding of visual realism perception: The extent to which viewers judge an image as a real photograph rather than a CG image. Across two experiments, we explored the processes involved in visual realism perception of face images. In Experiment 1, participants made visual realism judgments on original face images, inverted face images, and images of faces that had the top and bottom halves misaligned. In Experiment 2, participants made visual realism judgments on original face images, scrambled faces, and images that showed different parts of faces. Our findings indicate that both holistic and piecemeal processing are involved in visual realism perception of faces, with holistic processing becoming more dominant when resolution is lower. Our results also suggest that shading information is more important than color for holistic processing, and that inversion makes visual realism judgments harder for realistic images but not for unrealistic images. Furthermore, we found that eyes are the most influential face part for visual realism, and face context is critical for evaluating realism of face parts. To the best of our knowledge, this work is a first realism-centric study attempting to bridge the human perception of visual realism on face images with general face perception tasks. © 2014 ACM.",Holistic face perception; Human perception; Piecemeal face perception; Visual realism,Psychology computing; Sensory perception; Efficient control; Face images; Face perceptions; Human perception; Is-enabled; Realistic images; Video game; Visual realism; Experiments
Towards the temporally perfect virtual button: Touch-feedback simultaneity and perceived quality in mobile touchscreen press interactions,2014,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84906855971&doi=10.1145%2f2611387&partnerID=40&md5=d7e92d93d5827ba97910f00ee94ad6d8,"Pressing a virtual button is still the major interaction method in touchscreen mobile phones. Although phones are becoming more and more powerful, operating system software is getting more and more complex, causing latency in interaction. We were interested in gaining insight into touch-feedback simultaneity and the effects of latency on the perceived quality of touchscreen buttons. In an experiment, we varied the latency between touch and feedback between 0 and 300 ms for tactile, audio, and visual feedback modalities. We modelled the proportion of simultaneity perception as a function of latency for each modality condition. We used a Gaussian model fitted with the maximum likelihood estimation method to the observations. These models showed that the point of subjective simultaneity (PSS) was 5ms for tactile, 19ms for audio, and 32ms for visual feedback. Our study included the scoring of perceived quality for all of the different latency conditions. The perceived quality dropped significantly between latency conditions 70 and 100 ms when the feedback modality was tactile or audio, and between 100 and 150 ms when the feedback modality was visual. When the latency was 300ms for all feedback modalities, the quality of the buttons was rated significantly lower than in all of the other latency conditions, suggesting that a long latency between a touch on the screen and feedback is problematic for users. Together with PSS and these quality ratings, a 75% threshold was established to define a guideline for the recommended latency range between touch and feedback. Our guideline suggests that tactile feedback latency should be between 5 and 50 ms, audio feedback latency between 20 and 70 ms, and visual feedback latency between 30 and 85 ms. Using these values will ensure that users will perceive the feedback as simultaneous with the finger's touch. These values also ensure that the users do not perceive reduced quality. These results will guide engineers and designers of touchscreen interactions by showing the trade-offs between latency and user preference and the effects that their choices might have on the quality of the interactions and feedback they design. © 2014 ACM.",Audio; Feedback; Mobile device; Simultaneity; Tactile; Temporal perception; Touch; Touchscreen,Fasteners; Feedback; Maximum likelihood estimation; Mobile devices; Visual communication; Audio; Simultaneity; Tactile; Temporal perception; Touch; Touchscreen; Touch screens
A new hypothesis on facial beauty perception,2014,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84906877146&doi=10.1145%2f2622655&partnerID=40&md5=064bc376cb9c2df78c467e2397f547dd,"In this article, a new hypothesis on facial beauty perception is proposed: The weighted average of two facial geometric features is more attractive than the inferior one between them. Extensive evidences support the new hypothesis. We collected 390 well-known beautiful face images (e.g., Miss Universe, movie stars, and super models) as well as 409 common face images from multiple sources. Dozens of volunteers rated the face images according to their attractiveness. Statistical regression models are trained on this database. Under the empirical risk principle, the hypothesis is tested on 318,801 pairs of images and receives consistently supportive results. A corollary of the hypothesis is attractive facial geometric features construct a convex set. This corollary derives a convex hull based face beautification method, which guarantees attractiveness and minimizes the before-after difference. Experimental results show its superiority to state-of-the-art geometric based face beautification methods. Moreover, the mainstream hypotheses on facial beauty perception (e.g., the averageness, symmetry, and golden ratio hypotheses) are proved to be compatible with the proposed hypothesis. © 2014 ACM.",Convex hull; Face beautification; Face perception; Hypothesis; Machine learning; Regression,Artificial intelligence; Computational geometry; Learning systems; Convex hull; Face beautification; Face perceptions; Hypothesis; Regression; Set theory
A comparative perceptual study of soft-shadow algorithms,2014,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84906858670&doi=10.1145%2f2620029&partnerID=40&md5=61ad9b5f23e1447a63058d3f41c88436,"We performed a perceptual user study of algorithms that approximate soft shadows in real time. Although a huge body of soft shadow algorithms have been proposed, to our knowledge this is the first methodical study for comparing different real-time shadow algorithms with respect to their plausibility and visual appearance. We evaluated soft-shadow properties like penumbra overlap with respect to their relevance to shadow perception in a systematic way, and we believe that our results can be useful to guide future shadow approaches in their methods of evaluation. In this study, we also capture the predominant case of an inexperienced user observing shadows without comparing to a reference solution, such as when watching a movie or playing a game. One important result of this experiment is to scientifically verify that real-time soft-shadow algorithms, despite having become physically based and very realistic, can nevertheless be intuitively distinguished from a correct solution by untrained users. © 2014 ACM.",Perception; Soft shadows,Psychology computing; Sensory perception; Correct solution; Physically based; Real time; Real-time shadows; Reference solution; Soft shadow; User study; Visual appearance; Algorithms
Perceptual evaluation of motion editing for realistic throwing animations,2014,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84906857438&doi=10.1145%2f2617916&partnerID=40&md5=5ac985b9367c90d20f0bb3460b4f2ffe,"Animation budget constraints during the development of a game often call for the use of a limited set of generic motions. Editing operations are thus generally required to animate virtual characters with a sufficient level of variety. Evaluating the perceptual plausibility of edited animations can therefore contribute greatly towards producing visually plausible animations. In this article, we study observers' sensitivity to manipulations of overarm and underarm biological throwing animations. In the first experiment, we modified the release velocity of the ball while leaving the motion of the virtual thrower and the angle of release of the ball unchanged. In the second experiment, we evaluated the possibility of further modifying throwing animations by simultaneously editing the motion of the thrower and the release velocity of the ball, using dynamic time warping. In both experiments, we found that participants perceived shortened underarm throws to be particularly unnatural. We also found that modifying the thrower's motion in addition to modifying the release velocity of the ball does not significantly improve the perceptual plausibility of edited throwing animations. In the third experiment, we modified the angle of release of the ball while leaving the magnitude of release velocity and the motion of the thrower unchanged, and found that this editing operation is efficient for improving the perceptual plausibility of shortened underarm throws. Finally, in Experiment 4, we replaced the virtual human thrower with a mechanical throwing device (a ramp) and found the opposite pattern of sensitivity to modifications of the release velocity, indicating that biological and physical throws are subject to different perceptual rules. Our results provide valuable guidelines for developers of games and virtual reality applications by specifying thresholds for the perceptual plausibility of throwing manipulations while also providing several interesting insights for researchers in visual perception of biological motion. © 2014 ACM.",Graphics; Motion capture; Motion editing; Perception; Physics,Budget control; Experiments; Motion picture editing machines; Physics; Sensory perception; Velocity; Virtual reality; Dynamic time warping; Editing operations; Graphics; Motion capture; Motion editing; Perceptual evaluation; Release velocities; Virtual character; Animation
Olfactory adaptation in virtual environments,2014,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84906849919&doi=10.1145%2f2617917&partnerID=40&md5=b09971198bb714a33e177b5abd1b77f2,"Visual perception is becoming increasingly important in computer graphics. Research on human visual perception has led to the development of perception-driven computer graphics techniques, where knowledge of the human visual system (HVS) and, in particular, its weaknesses are exploited when rendering and displaying 3D graphics. Findings on limitations of the HVS have been used to maintain high perceived quality but reduce the computed quality of some of the image without this quality difference being perceived. This article investigates the amount of time for which (if at all) such limitations could be exploited in the presence of smell. The results show that for our experiment, adaptation to smell does indeed affect participants' ability to determine quality difference in the animations. Having been exposed to a smell before undertaking the experiment, participants were able to determine the quality in a similar fashion to the ""no smell"" condition, whereas without adaptation, participants were not able to distinguish the quality difference. © 2014 ACM.",Adaptation to smell; High-fidelity graphics,Virtual reality; Vision; 3D graphics; Adaptation to smell; Exposed to; High-fidelity; Human visual perception; Human visual systems; Perceived quality; Visual perception; Experiments
The role of sound source perception in gestural sound description,2014,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84899735486&doi=10.1145%2f2536811&partnerID=40&md5=383440cc802f3c84b26c816aba07d956,"We investigated gesture description of sound stimuli performed during a listening task. Our hypothesis is that the strategies in gestural responses depend on the level of identification of the sound source and specifically on the identification of the action causing the sound. To validate our hypothesis, we conducted two experiments. In the first experiment, we built two corpora of sounds. The first corpus contains sounds with identifiable causal actions. The second contains sounds for which no causal actions could be identified. These corpora properties were validated through a listening test. In the second experiment, participants performed arm and hand gestures synchronously while listening to sounds taken from these corpora. Afterward, we conducted interviews asking participants to verbalize their experience while watching their own video recordings. They were questioned on their perception of the listened sounds and on their gestural strategies. We showed that for the sounds where causal action can be identified, participants mainly mimic the action that has produced the sound. In the other case, when no action can be associated with the sound, participants trace contours related to sound acoustic features. We also found that the interparticipants' gesture variability is higher for causal sounds compared to noncausal sounds. Variability demonstrates that, in the first case, participants have several ways of producing the same action, whereas in the second case, the sound features tend to make the gesture responses consistent. © 2014 ACM.",Cross-modal relationships; Embodied cognition; Environmental sound perception; Gesture; Sound mimicry; Sound source identification; Sound tracing,Experiments; Video recording; Cross-modal; Embodied cognition; Environmental sounds; Gesture; Sound source identification; Sound tracings; Acoustic generators
Online 3D gaze localization on stereoscopic displays,2014,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84899744100&doi=10.1145%2f2593689&partnerID=40&md5=91f17728d0913ccbcf230b442bca4a53,"This article summarizes our previous work on developing an online system to allow the estimation of 3D gaze depth using eye tracking in a stereoscopic environment. We report on recent extensions allowing us to report the full 3D gaze position. Our system employs a 3D calibration process that determines the parameters of a mapping from a naive depth estimate, based simply on triangulation, to a refined 3D gaze point estimate tuned to a particular user. We show that our system is an improvement on the geometry-based 3D gaze estimation returned by a proprietary algorithm provided with our tracker. We also compare our approach with that of the Parameterized Self-Organizing Map (PSOM) method, due to Essig and colleagues, which also individually calibrates to each user. We argue that our method is superior in speed and ease of calibration, is easier to implement, and does not require an iterative solver to produce a gaze position, thus guaranteeing computation at the rate of tracker acquisition. In addition, we report on a user study that indicates that, compared with PSOM, our method more accurately estimates gaze depth, and is nearly as accurate in estimating horizontal and vertical position. Results are verified on two different 4D eye tracking systems, a high accuracy Wheatstone haploscope and a medium accuracy active stereo display. Thus, it is the recommended method for applications that primarily require gaze depth information, while its ease of use makes it suitable for many applications requiring full 3D gaze position. © 2014 ACM.",3D eye tracking; Eye tracking; Stereoscopic displays; Vergence,Conformal mapping; Display devices; Estimation; Iterative methods; 3D gaze positions; Depth information; Eye tracking systems; Eye-tracking; Parameterized self-organizing maps; Stereoscopic display; Vergences; Vertical positions; Three dimensional
Effects of long-term exposure on sensitivity and comfort with stereoscopic displays,2014,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84899747986&doi=10.1145%2f2536810&partnerID=40&md5=cd52a30f099f331fe0725aa70703f591,"Stereoscopic 3Dmedia has recently increased in appreciation and availability. This popularity has led to concerns over the health effects of habitual viewing of stereoscopic 3D content; concerns that are largely hypothetical. Here we examine the effects of repeated, long-term exposure to stereoscopic 3D in the workplace on several measures of stereoscopic sensitivity (discrimination, depthmatching, and fusion limits) along with reported negative symptoms associated with viewing stereoscopic 3D.We recruited a group of adult stereoscopic 3D industry experts and compared their performance with observers who were (i) inexperienced with stereoscopic 3D, (ii) researchers who study stereopsis, and (iii) vision researchers with little or no experimental stereoscopic experience. Unexpectedly, we found very little difference between the four groups on all but the depth discrimination task, and the differences that did occur appear to reflect task-specific training or experience. Thus, we found no positive or negative consequences of repeated and extended exposure to stereoscopic 3D in these populations. © 2014 ACM.",Depth discrimination; S3D; Stereo displays; Stereopsis; Stereoscopic 3D,Psychology computing; Sensory perception; Depth discrimination; S3D; Stereo displays; Stereopsis; Stereoscopic 3D; Three dimensional
Improving transparency in teleoperation by means of cutaneous tactile force feedback,2014,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84897143020&doi=10.1145%2f2604969&partnerID=40&md5=6299d3e38727f38dc57867b147ca826b,"A study on the role of cutaneous and kinesthetic force feedback in teleoperation is presented. Cutaneous cues provide less transparency than kinesthetic force, but they do not affect the stability of the teleoperation system. On the other hand, kinesthesia provides a compelling illusion of telepresence but affects the stability of the haptic loop. However, when employing common grounded haptic interfaces, it is not possible to independently control the cutaneous and kinesthetic components of the interaction. For this reason, many control techniques ensure a stable interaction by scaling down both kinesthetic and cutaneous force feedback, even though acting on the cutaneous channel is not necessary. We discuss here the feasibility of a novel approach. It aims at improving the realism of the haptic rendering, while preserving its stability, by modulating cutaneous force to compensate for a lack of kinesthesia. We carried out two teleoperation experiments, evaluating (1) the role of cutaneous stimuli when reducing kinesthesia and (2) the extent to which an overactuation of the cutaneous channel can fully compensate for a lack of kinesthetic force feedback. Results showed that, to some extent, it is possible to compensate for a lack of kinesthesia with the aforementioned technique, without significant performance degradation. Moreover, users showed a high comfort level in using the proposed system. © 2014 ACM.",Cutaneous force feedback; Robotic teleoperation; Tactile force feedback; Wearable haptics,Haptic interfaces; Transparency; Visual communication; Control techniques; Force feedback; Haptic rendering; Haptics; Performance degradation; Robotic teleoperation; Tactile forces; Teleoperation systems; Remote control
Models of gaze control for manipulation tasks,2013,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891790947&doi=10.1145%2f2536764.2536767&partnerID=40&md5=6a00efe5228fb04a50632e493844d7b2,"Human studies have shown that gaze shifts are mostly driven by the current task demands. In manipulation tasks, gaze leads action to the next manipulation target. One explanation is that fixations gather information about task relevant properties, where task relevance is signalled by reward. This work presents new computational models of gaze shifting, where the agent imagines ahead in time the informational effects of possible gaze fixations. Building on our previous work, the contributions of this article are: (i) the presentation of two new gaze control models, (ii) comparison of their performance to our previous model, (iii) results showing the fit of all these models to previously published human data, and (iv) integration of a visual search process. The first new model selects the gaze that most reduces positional uncertainty of landmarks (Unc), and the second maximises expected rewards by reducing positional uncertainty (RU). Our previous approach maximises the expected gain in cumulative reward by reducing positional uncertainty (RUG). In experiment ii the models are tested on a simulated humanoid robot performing a manipulation task, and each model's performance is characterised by varying three environmental variables. This experiment provides evidence that the RUG model has the best overall performance. In experiment iii, we compare the hand-eye coordination timings of the models in a robot simulation to those obtained from human data. This provides evidence that only the models that incorporate both uncertainty and reward (RU and RUG) match human data. © 2013 ACM.",Decision making; Gaze control; Reinforcement learning,Anthropomorphic robots; Decision making; Experiments; Reinforcement learning; Computational model; Environmental variables; Gaze control; Hand eye coordination; Humanoid robot; Manipulation task; Robot simulations; Visual search; Computer simulation
Walking pace affected by interactive sounds simulating stepping on different terrains,2013,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891788109&doi=10.1145%2f2536764.2536770&partnerID=40&md5=cb280a5531ecf4de748f689839991bc2,"This article investigates whether auditory feedback affects natural locomotion patterns. Individuals were provided with footstep sounds simulating different surface materials. The sounds were interactively generated using shoes with pressure sensors. Results showed that subjects' walking speed changed as a function of the type of simulated ground material. This effect may arise due to the presence of conflicting information between the auditory and foot-haptic modality, or because of an adjustment of locomotion to the physical properties evoked by the sounds simulating the ground materials. The results reported in this study suggest that auditory feedback may be more important in the regulation of walking in natural environments than has been acknowledged. Furthermore, auditory feedback could be used to develop novel approaches to the design of therapeutic and rehabilitation procedures for locomotion. © 2013 ACM.",Gait patterns; Interactive auditory feedback; Walking,Sensory perception; Auditory feedback; Different terrains; Gait pattern; Interactive sounds; Natural environments; Natural locomotions; Rehabilitation procedures; Walking; Psychology computing
Space perception in virtual environments: Displacement from the center of projection causes less distortion than predicted by cue-based models,2013,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891767530&doi=10.1145%2f2536764.2536765&partnerID=40&md5=d56ad901404e68431afcbadd225b87e1,"Virtual reality systems commonly include both monocular and binocular depth cues, which have the potential to provide viewers with a realistic impression of spatial properties of the virtual environment. However, when multiple viewers share the same display, only one viewer typically receives the projectively correct images. All other viewers experience the same images despite displacement from the center of projection (CoP). Three experiments evaluated perceptual distortions caused by displacement from the CoP and compared those percepts to predictions of models based on monocular and binocular viewing geometry. Leftward and rightward displacement from the CoP caused virtual angles on the ground plane to be judged as larger and smaller, respectively, compared to judgments from the CoP. Backward and forward displacement caused rectangles on the ground plane to be judged as larger and smaller in depth, respectively, compared to judgments from the CoP. Judgment biases were in the same direction as cue-based model predictions but of smaller magnitude. Displacement from the CoP had asymmetric effects on perceptual judgments, unlike model predictions. Perceptual distortion occurred with monocular cues alone but was exaggerated when binocular cues were added. The results are grounded in terms of practical implications for multiuser virtual environments. © 2013 ACM.",Depth perception; Stereoscopic displays; Virtual environments,Binoculars; Depth perception; Forecasting; Asymmetric effects; Center of projections; Forward displacements; Multi-user virtual environment; Perceptual distortion; Spatial properties; Stereoscopic display; Virtual reality system; Virtual reality
Virtual travel collisions: Response method influences perceived realism of virtual environments,2013,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891788519&doi=10.1145%2f2536764.2536772&partnerID=40&md5=9539996bef47edd74db28b4aba452f50,"Travel methods are the most basic and widespread interaction method with virtual environments. They are the primary and often the only way the user interactively experiences the environment. We present a study composed of three experiments that investigates how virtual collisions methods and feedback impact user perception of the realism of collisions and the virtual environment. A wand-based virtual travel method was used to navigate maze environments in an immersive projective system. The results indicated that the introduction of collision handling significantly improved the user's perception of the realism of the environment and collisions. An effect of feedback on the perceived level of realism of collisions and solidity of the environment was also found. Our results indicate that feedback should be context appropriate, e.g. fitting to a collision with the object; yet, the modality and richness of feedback were only important in that traditional color change feedback did not perform as well as audio or haptic feedback. In combination, the experiments indicated that in immersive virtual environments the stop collision handling method produced a more realistic impression than the slide method that is popular in games. In total, the study suggests that feedback fitting the collision context, coupled with the stop handling method, provides the best perceived realism of collisions and scene. © 2013 ACM.",Collision feedback; Presence; Realism; Virtual collisions; Virtual environments; Virtual travel,Experiments; Immersive virtual environments; Interaction methods; Perceived realisms; Presence; Realism; User's perceptions; Virtual collisions; Virtual travel; Virtual reality
Saliency-maximized audio visualization and efficient audio-visual browsing for faster-than-real-time human acoustic event detection,2013,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891803843&doi=10.1145%2f2536764.2536773&partnerID=40&md5=a93ecbc6c859800ed16d980327470c78,"Browsing large audio archives is challenging because of the limitations of human audition and attention. However, this task becomes easier with a suitable visualization of the audio signal, such as a spectrogram transformed to make unusual audio events salient. This transformation maximizes the mutual information between an isolated event's spectrogram and an estimate of how salient the event appears in its surrounding context. When such spectrograms are computed and displayed with fluid zooming over many temporal orders of magnitude, sparse events in long audio recordings can be detected more quickly and more easily. In particular, in a 1/10-real-time acoustic event detection task, subjects who were shown saliency-maximized rather than conventional spectrograms performed significantly better. Saliency maximization also improves the mutual information between the ground truth of nonbackground sounds and visual saliency, more than other common enhancements to visualization. © 2013 ACM.",Acoustic event detection; Audio visualization; Visual salience/saliency,Spectrographs; Visualization; Acoustic event detections; Audio archives; Audio events; Audio visualization; Mutual informations; Temporal order; Visual salience; Visual saliency; Audio acoustics
A framework for applying the principles of depth perception to information visualization,2013,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891804130&doi=10.1145%2f2536764.2536766&partnerID=40&md5=a0a9397462659b2cbd3e5e282076bb71,"During the visualization of 3D content, using the depth cues selectively to support the design goals and enabling a user to perceive the spatial relationships between the objects are important concerns. In this novel solution, we automate this process by proposing a framework that determines important depth cues for the input scene and the rendering methods to provide these cues. While determining the importance of the cues, we consider the user's tasks and the scene's spatial layout. The importance of each depth cue is calculated using a fuzzy logic-based decision system. Then, suitable rendering methods that provide the important cues are selected by performing a cost-profit analysis on the rendering costs of the methods and their contribution to depth perception. Possible cue conflicts are considered and handled in the system. We also provide formal experimental studies designed for several visualization tasks. A statistical analysis of the experiments verifies the success of our framework. © 2013 ACM.",Cue combination; Depth cues; Depth perception; Fuzzy logic; Information visualization,Depth perception; Fuzzy logic; Information analysis; Information systems; Visualization; Cost-profit analysis; Cue combination; Decision systems; Depth cue; Information visualization; Rendering methods; Spatial layout; Spatial relationships; Three dimensional computer graphics
2D linear oculomotor plant mathematical model: Verification and biometric applications,2013,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891796388&doi=10.1145%2f2536764.2536774&partnerID=40&md5=4aebd808dc235d4cc75bf1c1363f63cd,"This article assesses the ability of a two-dimensional (2D) linear homeomorphic oculomotor plant mathematical model to simulate normal human saccades on a 2D plane. The proposed model is driven by a simplified pulse-step neuronal control signal and makes use of linear simplifications to account for the unique characteristics of the eye globe and the extraocular muscles responsible for horizontal and vertical eye movement. The linear nature of the model sacrifices some anatomical accuracy for computational speed and analytic tractability, and may be implemented as two one-dimensional models for parallel signal simulation. Practical applications of the model might include improved noise reduction and signal recovery facilities for eye tracking systems, additional metrics from which to determine user effort during usability testing, and enhanced security in biometric identification systems. The results indicate that the model is capable of produce oblique saccades with properties resembling those of normal human saccades and is capable of deriving muscle constants that are viable as biometric indicators. Therefore, we conclude that sacrifice in the anatomical accuracy of the model produces negligible effects on the accuracy of saccadic simulation on a 2D plane and may provide a usable model for applications in computer science, human-computer interaction, and related fields. © 2013 ACM.",Biological system modeling; Biometrics; Human visual system; Mathematical model,Biometrics; Eye movements; Mathematical models; Biological system modeling; Biometric applications; Biometric identification systems; Computational speed; Extraocular muscles; Eye tracking systems; Human Visual System; One-dimensional model; Computer simulation
Intuitiveness of vibrotactile speed regulation cues,2013,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891757032&doi=10.1145%2f2536764.2536771&partnerID=40&md5=9c31cd9e2292a39337a58604f1149844,"Interpretations of vibrotactile stimulations were compared between two participant groups. In both groups, the task was to evaluate specifically designed tactile stimulations presented to the wrist or chest. Ascending, constant, and descending vibration frequency profiles of the stimuli represented information for three different speed regulation instructions: ""accelerate your speed,"" ""keep your speed constant,"" and ""decelerate your speed,"" respectively. The participants were treated differently so that one of the groups was first taught (i.e., primed) the meanings of the stimuli, whereas the other group was not taught (i.e., unprimed). The results showed that the stimuli were evaluated nearly equally in the primed and the unprimed groups. The best performing stimuli communicated the three intended meanings in the rate of 88% to 100% in the primed group and in the unprimed group in the rate of 71% to 83%. Both groups performed equally in evaluating ""keep your speed constant"" and ""decelerate your speed"" information. As the unprimed participants performed similarly to the primed participants, the results suggest that vibrotactile stimulation can be intuitively understood. The results suggest further that carefully designed vibrotactile stimulations could be functional in delivering easy-to-understand feedback on how to regulate the speed of movement, such as in physical exercise and rehabilitation applications. © 2013 ACM.",Haptic feedback; Heart rate monitor; Human-computer interaction; Iconic information; Intuitive decision making; Priming,Human computer interaction; Patient monitoring; Haptic feedbacks; Heart-rate monitors; Iconic information; Physical exercise; Priming; Speed regulation; Tactile stimulation; Vibration frequency; Speed
Perceptual impact of gesture control of spatialization,2013,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891783807&doi=10.1145%2f2536764.2536769&partnerID=40&md5=3657c3e7b8fd0e23d072af63ac5585bf,"In two experiments, visual cues from gesture control of spatialization were found to affect auditory movement perception depending on the identifiability of auditory motion trajectories, the congruency of audiovisual stimulation, the sensory focus of attention, and the attentional process involved. Visibility of the performer's gestures improved spatial audio trajectory identification, but it shifted the listeners' attention to vision, impairing auditory motion encoding in the case of incongruent stimulation. On the other hand, selectively directing attention to audition resulted in interference from the visual cues for acoustically ambiguous trajectories. Auditory motion information was poorly preserved when dividing attention between auditory and visual movement feedback from performance gestures. An auditory focus of attention is a listener strategy that maximizes performance, due to the improvement caused by congruent visual stimulation and its robustness to interference from incongruent stimulation for acoustically unambiguous trajectories. Attentional strategy and auditory motion calibration are two aspects that need to be considered when employing gesture control of spatialization. © 2013 ACM.",3D audio; Auditory movement; Gesture control; Multimodal integration,Sensory perception; 3D audio; Audio-visual stimulation; Auditory movement; Focus of Attention; Gesture control; Multimodal integration; Visual movement; Visual stimulation; Trajectories
Clustering approach to characterize haptic expressions of emotions,2013,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891788308&doi=10.1145%2f2536764.2536768&partnerID=40&md5=6ee2360130ccb252a1f082c3542999c6,"Several studies have investigated the relevance of haptics to physically convey various types of emotion. However, they use basic analysis approaches to identify the relevant features for an effective communication of emotion. This article presents an advanced analysis approach, based on the clustering technique, that enables the extraction of the general features of affective haptic expressions as well as the identification of specific features in order to discriminate between close emotions that are difficult to differentiate. This approach was tested in the context of affective communication through a virtual handshake. It uses a haptic device, which enables the expression of 3D movements. The results of this research were compared to those of the standard Analysis of Variance method in order to highlight the advantages and limitations of each approach. © 2013 ACM.",Emotion; Haptic,Psychology computing; Sensory perception; Affective communication; Analysis approach; Clustering approach; Clustering techniques; Effective communication; Emotion; Haptic; Relevant features; Communication
Analyzing correspondence between sound objects and body motion,2013,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885636283&doi=10.1145%2f2465780.2465783&partnerID=40&md5=b287b17090b06cf18c2cdac6b7c16e01,"Links between music and body motion can be studied through experiments called sound-tracing. One of the main challenges in such research is to develop robust analysis techniques that are able to deal with the multidimensional data that musical sound and body motion present. The article evaluates four different analysis methods applied to an experiment in which participants moved their hands following perceptual features of short sound objects. Motion capture data has been analyzed and correlated with a set of quantitative sound features using four different methods: (a) a pattern recognition classifier, (b) t-tests, (c) Spearman's ρ correlation, and (d) canonical correlation. This article shows how the analysis methods complement each other, and that applying several analysis techniques to the same data set can broaden the knowledge gained from the experiment. © 2013 ACM.",Music-related motion; Sound-tracing,Pattern recognition; Analysis techniques; Canonical correlations; Motion capture data; Multidimensional data; Music-related motion; Pattern recognition classifiers; Perceptual feature; Sound-tracing; Experiments
Measuring the degree of face familiarity based on extended NMF,2013,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885597870&doi=10.1145%2f2465780.2465782&partnerID=40&md5=75828a92f66910bbd9ca274c221e31fb,"Getting familiar with a face is an important cognitive process in human perception of faces, but little study has been reported on how to objectively measure the degree of familiarity. In this article, a method is proposed to quantitatively measure the familiarity of a face with respect to a set of reference faces that have been seen previously. The proposed method models the context-free and context-dependent forms of familiarity suggested by psychological studies and accounts for the key factors, namely exposure frequency, exposure intensity and similar exposure, that affect human perception of face familiarity. Specifically, the method divides the reference set into nonexclusive groups and measures the familiarity of a given face by aggregating the similarities of the face to the individual groups. In addition, the nonnegative matrix factorization (NMF) is extended in this paper to learn a compact and localized subspace representation for measuring the similarities of the face with respect to the individual groups. The proposed method has been evaluated through experiments that follow the protocols commonly used in psychological studies and has been compared with subjective evaluation. Results have shown that the proposed measurement is highly consistent with the subjective judgment of face familiarity. Moreover, a face recognition method is devised using the concept of face familiarity and the results on the standard FERET evaluation protocols have further verified the efficacy of the proposed familiarity measurement. © 2013 ACM.",Face analysis; Face familiarity; Face recognition; NMF,Computer science; Psychology computing; Evaluation protocol; Face analysis; Face familiarity; Face recognition methods; NMF; Nonnegative matrix factorization; Subjective evaluations; Subspace representation; Face recognition
Cueing multimedia search with audiovisual blur,2013,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885630800&doi=10.1145%2f2465780.2465781&partnerID=40&md5=fece781a3992753bab90d8e57b18a4a5,"Situated in the context of multimedia browsing, this study concerns perceptual processes involved in searching for an audiovisual object displayed among several distractors. The aim of the study is to increase the perceptual saliency of the target in order to enhance the search process. As blurring distractors and maintaining the target sharp has proved to be a great facilitator of visual search, we propose combining visual blur with an audio blur analogue to improve multimodal search. Three perceptual experiments were performed in which participants had to retrieve an audiovisual object from a set of six competing stimuli. The first two experiments explored the effect of blur level on unimodal search tasks. A third experiment investigated the influence of an audio and visual modality combination with both modalities cued on an audiovisual search task. Results showed that both visual and audio blurs render stimuli distractors less prominent and thus helped users focus on a sharp target more easily. Performances were also faster and more accurate in the bimodal condition than in either unimodal search task, auditory or visual. Our work suggests that audio and audiovisual interfaces dedicated to multimedia search could benefit from different uses of blur on presentation strategies. © 2013 ACM.",Audio and visual blurs; Audiovisual search; Multimedia browsing; Multisensory integration,Computer science; Psychology computing; Audio-visual objects; Audiovisual interfaces; Audiovisual search; Multimedia browsing; Multimodal search; Multisensory integration; Presentation strategies; Visual blur; Experiments
Graph-based joint clustering of fixations and visual entities,2013,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885618190&doi=10.1145%2f2465780.2465784&partnerID=40&md5=14a02a2b33c339f73dd43622a2669991,"We present a method that extracts groups of fixations and image regions for the purpose of gaze analysis and image understanding. Since the attentional relationship between visual entities conveys rich information, automatically determining the relationship provides us a semantic representation of images. We show that, by jointly clustering human gaze and visual entities, it is possible to build meaningful and comprehensive metadata that offer an interpretation about how people see images. To achieve this, we developed a clustering method that uses a joint graph structure between fixation points and over-segmented image regions to ensure a cross-domain smoothness constraint. We show that the proposed clustering method achieves better performance in relating attention to visual entities in comparison with standard clustering techniques. © 2013 ACM.",Clustering; Gaze analysis; Gaze visualization; Image segmentation; ROI detection,Graphic methods; Image segmentation; Better performance; Clustering; Clustering methods; Clustering techniques; Gaze analysis; Graph structures; Semantic representation; Smoothness constraints; Cluster analysis
Learning to walk in virtual reality,2013,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880710719&doi=10.1145%2f2465780.2465785&partnerID=40&md5=fc5e35de49e1d04324b570ca04d9e984,"This article provides longitudinal data for when participants learned to travel with a walking metaphor through virtual reality (VR) worlds, using interfaces that ranged from joystick-only, to linear and omnidirectional treadmills, and actual walking in VR. Three metrics were used: travel time, collisions (a measure of accuracy), and the speed profile. The time that participants required to reach asymptotic performance for traveling, and what that asymptote was, varied considerably between interfaces. In particular, when a world had tight turns (0.75 m corridors), participants who walked were more proficient than those who used a joystick to locomote and turned either physically or with a joystick, even after 10 minutes of training. The speed profile showed that this was caused by participants spending a notable percentage of the time stationary, irrespective of whether or not they frequently played computer games. The study shows how speed profiles can be used to help evaluate participants' proficiency with travel interfaces, highlights the need for training to be structured to addresses specific weaknesses in proficiency (e.g., start-stop movement), and for studies to measure and report that proficiency. © 2013 ACM.",Metrics; Navigation; Travel; Virtual reality interfaces,Computer science; Navigation; Psychology computing; 10 minutes; Asymptotic performance; Longitudinal data; Metrics; Speed profile; Travel; Virtual reality interfaces; Virtual reality
Sound sample detection and numerosity estimation using auditory display,2013,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874891621&doi=10.1145%2f2422105.2422109&partnerID=40&md5=bb0f1572ae4c395e30aaf71ceb359617,"This article investigates the effect of various design parameters of auditory information display on user performance in two basic information retrieval tasks. We conducted a user test with 22 participants in which sets of sound samples were presented. In the first task, the test participants were asked to detect a given sample among a set of samples. In the second task, the test participants were asked to estimate the relative number of instances of a given sample in two sets of samples. We found that the stimulus onset asynchrony (SOA) of the sound samples had a significant effect on user performance in both tasks. For the sample detection task, the average error rate was about 10% with an SOA of 100 ms. For the numerosity estimation task, an SOA of at least 200 ms was necessary to yield average error rates lower than 30%. Other parameters, including the samples' sound type (synthesized speech or earcons) and spatial quality (multichannel loudspeaker or diotic headphone playback), had no substantial effect on user performance. These results suggest that diotic, or indeed monophonic, playback with appropriately chosen SOA may be sufficient in practical applications for users to perform the given information retrieval tasks, if information about the sample location is not relevant. If location information was provided through spatial playback of the samples, test subjects were able to simultaneously detect and localize a sample with reasonable accuracy. © 2013 ACM.",Experimentation; Human factors,Human engineering; Information retrieval; Auditory display; Design parameters; Experimentation; Information display; Location information; Reasonable accuracy; Synthesized speech; User performance; Estimation
Generating stereoscopic HDR images using HDR-LDR image pairs,2013,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874845561&doi=10.1145%2f2422105.2422108&partnerID=40&md5=12f591ab2afb52c73de8b693058cb538,"A number of novel imaging technologies have been gaining popularity over the past few years. Foremost among these are stereoscopy and high dynamic range (HDR) Imaging. While a large body of research has looked into each of these imaging technologies independently, very little work has attempted to combine them. This is mostly due to the current limitations in capture and display. In this article, we mitigate problems of capturing Stereoscopic HDR (SHDR) that would potentially require two HDR cameras, by capturing an HDR and LDR pair and using it to generate 3D stereoscopic HDR content. We ran a detailed user study to compare four different methods of generating SHDR content. The methods investigated were the following: two based on expanding the luminance of the LDR image, and two utilizing stereo correspondence methods, which were adapted for our purposes. Results demonstrate that one of the stereo correspondence methods may be considered perceptually indistinguishable from the ground truth (image pair captured using two HDR cameras), while the other methods are all significantly distinct from the ground truth. © 2013 ACM.",Design; Experimentation; Performance,Design; Imaging techniques; Current limitation; Experimentation; Ground truth; High dynamic range; Image pairs; Imaging technology; Performance; Stereo correspondences; Cameras
Perceptual characterization of motion evoked by sounds for synthesis control purposes,2013,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874885023&doi=10.1145%2f2422105.2422106&partnerID=40&md5=46b2b1109e8d49d67136e6824ae2d25a,"This article addresses the question of synthesis and control of sound attributes from a perceptual point of view. We focused on an attribute related to the general concept of motion evoked by sounds. To investigate this concept, we tested 40 monophonic abstract sounds on listeners via a questionnaire and drawings, using a parametrized custom interface. This original procedure, which was defined with synthesis and control perspectives in mind, provides an alternative means of determining intuitive control parameters for synthesizing sounds evoking motion. Results showed that three main shape categories (linear, with regular oscillations, and with circular oscillations) and three types of direction (rising, descending, and horizontal) were distinguished by the listeners. In addition, the subjects were able to perceive the low-frequency oscillations (below 8 Hz) quite accurately. Three size categories (small, medium, and large) and three levels of randomness (none, low amplitude irregularities, and high amplitude irregularities) and speed (constant speed and speeds showing medium and large variations) were also observed in our analyses of the participants' drawings. We further performed a perceptual test to confirm the relevance of the contribution of some variables with synthesized sounds combined with visual trajectories. Based on these results, a general typology of evoked motion was drawn up and an intuitive control strategy was designed, based on a symbolic representation of continuous trajectories (provided by devices such as motion capture systems, pen tablets, etc.). These generic tools could be used in a wide range of applications such as sound design, virtual reality, sonification, and music. © 2013 ACM.",Design; Experimentation; Performance; Theory,Design; Virtual reality; Experimentation; Intuitive control strategy; Low-frequency oscillations; Motion capture system; Performance; Regular oscillations; Symbolic representation; Theory; Abstracting
Perceptual importance of lighting phenomena in rendering of animated water,2013,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874917498&doi=10.1145%2f2422105.2422107&partnerID=40&md5=3f8863451738d938396063c06f41dd87,"Recent years have seen increasing research in perceptually-driven reductions in the costs of realistically rendered imagery. Water is complex and recognizable, and continues to be in the forefront of research. However, the contribution of individual lighting phenomena to the perceived realism of virtual water has not been addressed. All these phenomena have costs associated with their rendering, but does the visual benefit outweigh these costs? This study investigates the human perception of various illumination components found in water-rich virtual environments. The investigation uses a traditional psychophysical analysis to examine viewer perception of these lighting phenomena as they relate to the rendering cost, and ultimately reveals common trends in perceptual value. Five different scenes with a wide range of water and lighting dynamics were tested for perceptual value by one hundred participants. Our results provide an importance comparison for lighting phenomena in the rendering of water, and cost reductions can be made with little or no effect on the perceived quality of the imagery if viewed in a scenario similar to our testing. © 2013 ACM.",Design; Experimentation; Performance,Cost reduction; Design; Lighting; Virtual reality; Experimentation; Human perception; Illumination components; Perceived quality; Perceived realisms; Performance; Psychophysical; Virtual water; Cost benefit analysis
Improving relative depth judgments in augmented reality with auxiliary augmentations,2013,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874857381&doi=10.1145%2f2422105.2422111&partnerID=40&md5=b47ba076e2f19fbf374fae149fd3c044,"Significant depth judgment errors are common in augmented reality. This study presents a visualization approach for improving relative depth judgments in augmented reality. The approach uses auxiliary augmented objects in addition to the main augmentation to support ordinal and interval depth judgment tasks. The auxiliary augmentations are positioned spatially near real-world objects, and the location of the main augmentation can be deduced based on the relative depth cues between the augmented objects. In the experimental part, the visualization approach was tested in the ""X-ray"" visualization case with a video see-through system. Two relative depth cues, in addition to motion parallax, were used between graphical objects: relative size and binocular disparity. The results show that the presence of auxiliary objects significantly reduced errors in depth judgment. Errors in judging the ordinal location with respect to a wall (front, at, or behind) and judging depth intervals were reduced. In addition to reduced errors, the presence of auxiliary augmentation increased the confidence in depth judgments, and it was subjectively preferred. The visualization approach did not have an effect on the viewing time. © 2013 ACM.",Design; Experimentation; Human factors; Performance,Augmented reality; Design; Errors; Geometrical optics; Human engineering; Binocular disparity; Depth cue; Experimentation; Graphical objects; Motion parallax; Performance; Real-world objects; Relative sizes; Visualization
Abstract painting with interactive control of perceptual entropy,2013,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874855894&doi=10.1145%2f2422105.2422110&partnerID=40&md5=89c8cfdc641854230f5330fbf794cd6a,"This article presents a framework for generating abstract art from photographs. The aesthetics of abstract art is largely attributed to its greater perceptual ambiguity than photographs. According to psychological theories [Berlyne 1971], the ambiguity tends to invoke moderate mental effort in the viewer for interpreting the underlying contents, and this process is usually accompanied by subtle aesthetic pleasure. We study this phenomenon through human experiments comparing the subjects' interpretations of abstract art and photographs, and quantitatively verify, the increased perceptual ambiguities in terms of recognition accuracy and response time. Based on the studies, we measure the level of perceptual ambiguity using entropy, as it measures uncertainty levels in information theory, and propose a painterly rendering method with interactive control of the ambiguity levels. Given an input photograph, we first segment it into regions corresponding to different objects and parts in an interactive manner and organize them into a hierarchical parse tree representation. Then we execute a painterly rendering process with image obscuring operators to transfer the photograph into an abstract painting style with increased perceptual ambiguities in both the scene and individual objects. Finally, using kernel density estimation and message-passing algorithms, we compute and control the ambiguity levels numerically to the desired levels, during which we may predict and control the viewer's perceptual path among the image contents by assigning different ambiguity levels to different objects. We have evaluated the rendering results using a second set of human experiments, and verified that they achieve similar abstract effects to original abstract paintings. © 2013 ACM.",Algorithms; Experimentation; Human factors,Algorithms; Experiments; Human engineering; Information theory; Photography; Experimentation; Kernel Density Estimation; Measures uncertainties; Message passing algorithm; Painterly Rendering; Perceptual ambiguities; Psychological theory; Recognition accuracy; Abstracting
Evaluation of monocular depth cues on a high-dynamic-range display for visualization,2013,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896920350&doi=10.1145%2f2504568&partnerID=40&md5=c407b02e598418f45a373e0cd2f7eb80,"The aim of this work is to identify the depth cues that provide intuitive depth-ordering when used to visualize abstract data. In particular we focus on the depth cues that are effective on a high-dynamic-range (HDR) display: contrast and brightness. In an experiment participants were shown a visualization of the volume layers at different depths with a single isolated monocular cue as the only indication of depth. The observers were asked to identify which slice of the volume appears to be closer. The results show that brightness, contrast and relative size are the most effective monocular depth cues for providing an intuitive depth ordering. © 2013 ACM.",Brightness; Contrast; Depth ordering; Depth perception; Monocular depth cues; Phrases: HDR; Transparency; Visualization,Depth perception; Flow visualization; Luminance; Transparency; Abstract data; Contrast; Depth cue; Depth ordering; High dynamic range; Phrases: HDR; Relative sizes; Visualization
Shape perception of thin transparent objects with stereoscopic viewing,2013,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896984055&doi=10.1145%2f2506206.2506208&partnerID=40&md5=046309c2968fe3af0e50a96cd77acbca,"Many materials, including water surfaces, jewels, and glassware exhibit transparent refractions. The human visual system can somehow recover 3D shape from refracted images. While previous research has elucidated various visual cues that can facilitate visual perception of transparent objects, most of them focused on monocular material perception. The question of shape perception of transparent objects is much more complex and few studies have been undertaken, particular in terms of binocular vision. In this article, we first design a system for stereoscopic surface orientation estimation with photo-realistic stimuli. It displays pre-rendered stereoscopic images and a real-time S3D (Stereoscopic 3D) shape probe simultaneously. Then we estimate people's perception of the shape of thin transparent objects using a gauge figure task. Our results suggest that people can consistently perceive the surface orientation of thin transparent objects, and stereoscopic viewing improves the precision of estimates. To explain the results, we present an edge-aware orientation map based on image gradients and structure tensors to illustrate the orientation information in images. We also decomposed the normal direction of the surface into azimuth angle and slant angle to explain why additional depth information can improve the accuracy of perceived normal direction. © 2013 ACM.",Phrases: 3D shape perception; Shape-from-texture; Stereoscopic; Transparent; ZMATERIAL perception,Binocular vision; Estimation; Surfaces; 3D shape perception; Human Visual System; Orientation information; Shape-from-texture; Stereoscopic; Stereoscopic viewing; Transparent; Transparent objects; Three dimensional
Introduction to special issue SAP 2013,2013,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896905251&doi=10.1145%2f2506206.2506207&partnerID=40&md5=99d706789cd4257c6114d214975ca19e,[No abstract available],,
Elastic images: Perceiving local elasticity of images through a novel pseudo-haptic deformation effect,2013,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896910369&doi=10.1145%2f2501599&partnerID=40&md5=eeecc02bc30ceef4cb123f2f71ca29e8,"We introduce the Elastic Images, a novel pseudo-haptic feedback technique which enables the perception of the local elasticity of images without the need of any haptic device. The proposed approach focus on whether visual feedback is able to induce a sensation of stiffness when the user interacts with an image using a standard mouse. The user, when clicking on a Elastic Image, is able to deform it locally according to its elastic properties. To reinforce the effect, we also propose the generation of procedural shadows and creases to simulate the compressibility of the image and several mouse cursors replacements to enhance pressure and stiffness perception. A psychophysical experiment was conducted to quantify this novel pseudo-haptic perception and determine its perceptual threshold (or its Just Noticeable Difference). The results showed that users were able to recognize up to eight different stiffness values with our proposed method and confirmed that it provides a perceivable and exploitable sensation of elasticity. The potential applications of the proposed approach range from pressure sensing in product catalogs and games, or its usage in graphical user interfaces for increasing the expressiveness of widgets. © 2013 ACM.",Elasticity; Phrases: Pseudo-haptic; Stiffness; Texture,Graphical user interfaces; Haptic interfaces; Mammals; Stiffness; Textures; Visual communication; Deformation effects; Elastic properties; Feedback techniques; Just-noticeable difference; Perceptual threshold; Pseudo-haptics; Psychophysical experiments; Stiffness perception; Elasticity
Preference and artifact analysis for video transitions of places,2013,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896939989&doi=10.1145%2f2501601&partnerID=40&md5=5dfb3bcffcc4b8fe58c1cadfa028a258,"Emerging interfaces for video collections of places attempt to link similar content with seamless transitions. However, the automatic computer vision techniques that enable these transitions have many failure cases which lead to artifacts in the final rendered transition. Under these conditions, which transitions are preferred by participants and which artifacts are most objectionable? We perform an experiment with participants comparing seven transition types, from movie cuts and dissolves to image-based warps and virtual camera transitions, across five scenes in a city. This document describes how we condition this experiment on slight and considerable view change cases, and how we analyze the feedback from participants to find their preference for transition types and artifacts. We discover that transition preference varies with view change, that automatic rendered transitions are significantly preferred even with some artifacts, and that dissolve transitions are comparable to lesssophisticated rendered transitions. This leads to insights into what visual features are important to maintain in a rendered transition, and to an artifact ordering within our transitions. © 2013 ACM.",Phrases: Video-based rendering; Video transition artifacts,Psychology computing; Sensory perception; Computer vision techniques; Dissolve transitions; Seamless transition; Video collections; Video transition; Video-based rendering; Virtual camera; Visual feature; Experiments
Surface perception of planar abstractions,2013,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896984112&doi=10.1145%2f2501853&partnerID=40&md5=23369e899b61d90b83526c764cbe307f,"Various algorithms have been proposed to create planar abstractions of 3D models, but there has been no systematic effort to evaluate the effectiveness of such abstractions in terms of perception of the abstracted surfaces. In this work, we perform a large crowd-sourced study involving approximately 70k samples to evaluate how well users can orient gauges on planar abstractions of commonly occurring models.We test four styles of planar abstractions against ground truth surface representations, and analyze the data to discover a wide variety of correlations between task error and measurements relating to surface-specific properties such as curvature, local thickness and medial axis distance, and abstraction-specific properties. We use these discovered correlations to create linear models to predict error in surface understanding at a given point, for both surface representations and planar abstractions. Our predictive models reveal the geometric causes most responsible for error, and we demonstrate their potential use to build upon existing planar abstraction techniques in order to improve perception of the abstracted surface. © 2013 ACM.",Perception; Phrases: Crowd-sourced user study; Planar abstraction; Surface representation,Abstracting; Computer programming languages; Sensory perception; Thickness measurement; Abstraction techniques; Ground truth; Planar abstraction; Predict errors; Predictive models; Surface perception; Surface representation; User study; Surface properties
"Audio, visual, and audio-visual egocentric distance perception by moving subjects in virtual environments",2012,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878503285&doi=10.1145%2f2355598.2355602&partnerID=40&md5=3e7133ddb8d095474f8182a9e4a52ad1,"We present a study on audio, visual, and audio-visual egocentric distance perception by moving subjects in virtual environments. Audio-visual rendering is provided using tracked passive visual stereoscopy and acoustic wave field synthesis (WFS). Distances are estimated using indirect blind-walking (triangulation) under each rendering condition. Experimental results show that distances perceived in the virtual environment are systematically overestimated for rendered distances closer than the position of the audio-visual rendering system and underestimated for farther distances. Interestingly, subjects perceived each virtual object at a modality-independent distance when using the audio modality, the visual modality, or the combination of both. WFS was able to synthesise perceptually meaningful sound fields. Dynamic audio-visual cues were used by subjects when estimating the distances in the virtual world. Moving may have provided subjects with a better visual distance perception of close distances than if they were static. No correlation between the feeling of presence and the visual distance underestimation has been found. To explain the observed perceptual distance compression, it is proposed that, due to conflicting distance cues, the audio-visual rendering system physically anchors the virtual world to the real world. Virtual objects are thus attracted by the physical audio-visual rendering system. © 2012 ACM.",Distance estimation; Large-screen immersive displays; Spatial perception; Spatialized audio; Virtual environments; Wave field synthesis,Acoustic fields; Depth perception; Sound reproduction; Speech recognition; Distance estimation; Large-screen immersive displays; Spatial perception; Spatialized audio; Wave field synthesis; Virtual reality
Comparing face recognition algorithms to humans on challenging tasks,2012,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878476400&doi=10.1145%2f2355598.2355599&partnerID=40&md5=4bafd06329f8eb938d42e8dea6f97714,"We compared face identification by humans and machines using images taken under a variety of uncontrolled illumination conditions in both indoor and outdoor settings. Natural variations in a person's day-to-day appearance (e.g., hair style, facial expression, hats, glasses, etc.) contributed to the difficulty of the task. Both humans and machines matched the identity of people (same or different) in pairs of frontal view face images. The degree of difficulty introduced by photometric and appearancebased variability was estimated using a face recognition algorithm created by fusing three top-performing algorithms from a recent international competition. The algorithm computed similarity scores for a constant set of same-identity and differentidentity pairings from multiple images. Image pairs were assigned to good, moderate, and poor accuracy groups by ranking the similarity scores for each identity pairing, and dividing these rankings into three strata. This procedure isolated the role of photometric variables from the effects of the distinctiveness of particular identities. Algorithm performance for these constant identity pairings varied dramatically across the groups. In a series of experiments, humans matched image pairs from the good, moderate, and poor conditions, rating the likelihood that the images were of the same person (1: sure same - 5: sure different). Algorithms were more accurate than humans in the good and moderate conditions, but were comparable to humans in the poor accuracy condition. To date, these are the most variable illumination- and appearance-based recognition conditions on which humans and machines have been compared. The finding that machines were never less accurate than humans on these challenging frontal images suggests that face recognition systems may be ready for applications with comparable difficulty. We speculate that the superiority of algorithms over humans in the less challenging conditions may be due to the algorithms' use of detailed, view-specific identity information. Humans may consider this information less important due to its limited potential for robust generalization in suboptimal viewing conditions. © 2012 ACM.",Face recognition; Human-machine comparisons,Algorithms; Competition; Rapid thermal annealing; Algorithm performance; Appearance-based recognition; Face recognition algorithms; Face recognition systems; Human-machine; International competitions; Uncontrolled illumination; Variable illumination; Face recognition
On the limits of resolution and visual angle in visualization,2012,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878508415&doi=10.1145%2f2355598.2355603&partnerID=40&md5=2736ae8f4ade92c077c5efd09184ce85,"This article describes a perceptual level-of-detail approach for visualizing data. Properties of a dataset that cannot be resolved in the current display environment need not be shown, for example, when too few pixels are used to render a data element, or when the element's subtended visual angle falls below the acuity limits of our visual system. To identify these situations, we asked: (1) What type of information can a human user perceive in a particular display environment? (2) Can we design visualizations that control what they represent relative to these limits? and (3) Is it possible to dynamically update a visualization as the display environment changes, to continue to effectively utilize our perceptual abilities? To answer these questions, we conducted controlled experiments that identified the pixel resolution and subtended visual angle needed to distinguish different values of luminance, hue, size, and orientation. This information is summarized in a perceptual display hierarchy, a formalization describing how many pixels-resolution-and how much physical area on a viewer's retina-visual angle-is required for an element's visual properties to be readily seen. We demonstrate our theoretical results by visualizing historical climatology data from the International Panel for Climate Change. © 2012 ACM.",Hue; Luminance; Orientation; Resolution; Size; Visual acuity; Visual angle; Visual perception; Visualization,Climate change; Crystal orientation; Flow visualization; Luminance; Optical resolving power; Vision; Visualization; Hue; Size; Visual acuity; Visual angle; Visual perception; Pixels
Curve shape and curvature perception through interactive sonification,2012,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878482723&doi=10.1145%2f2355598.2355600&partnerID=40&md5=f45f999fba43e629ca71cd4f45dd626f,"In this article we present an approach that uses sound to communicate geometrical data related to a virtual object. This has been developed in the framework of a multimodal interface for product design. The interface allows a designer to evaluate the quality of a 3-D shape using touch, vision, and sound. Two important considerations addressed in this article are the nature of the data that is sonified and the haptic interaction between the user and the interface, which in fact triggers the sound and influences its characteristics. Based on these considerations, we present a number of sonification strategies that are designed to map the geometrical data of interest into sound. The fundamental frequency of various sounds was used to convey the curve shape or the curvature to the listeners. Two evaluation experiments are described, one involves partipants with a varied background, the other involved the intended users, i.e. participants with a background in industrial design. The results show that independent of the sonification method used and independent of whether the curve shape or the curvature were sonified, the sonification was quite successful. In the first experiment participants had a success rate of about 80% in a multiple choice task, in the second experiment it took the participants on average less than 20 seconds to find the maximum, minimum or inflection points of the curvature of a test curve. © 2012 ACM.",Haptics; Modal synthesis; Sonification; Sound synthesis,Data processing; Experiments; Modal analysis; Product design; Evaluation experiments; Fundamental frequencies; Haptics; Interactive sonification; Modal synthesis; Multi-modal interfaces; Sonifications; Sound synthesis; Quality control
How real is real enough? optimal reality sampling for fast recognition of mobile imagery,2012,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878500557&doi=10.1145%2f2355598.2355604&partnerID=40&md5=c8fe6ca2f640b1bc9b043e918edabf14,"We present the first study to discover optimal reality sampling for mobile imagery. In particular, we identify the minimum information required for fast recognition of images of directly perceivable real-world buildings displayed on a mobile device. Resolution, image size, and JPEG compression of images of facades were manipulated in a same-different recognition task carried out in the field. Best-effort performance is shown to be reachable with significantly lower detail granularity than previously thought. For best user performance, we recommend presenting images as large as possible on the screen and decreasing resolution accordingly. © 2012 ACM.",Mobile imagery; Recognition,Mobile devices; Best-effort; Fast recognition; JPEG compression; Minimum information; Mobile imagery; Real-world; Recognition; User performance; Optimization
Perception and replication of planar sonic gestures,2012,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878521109&doi=10.1145%2f2355598.2355601&partnerID=40&md5=598d959e85f39a94a63fbb683bfa33e6,"As tables, boards, and walls become surfaces where interaction can be supported by auditory displays, it becomes important to know how accurately and effectively a spatial gesture can be rendered by means of an array of loudspeakers embedded in the surface. Two experiments were designed and performed to assess: (i) how sequences of sound pulses are perceived as gestures when the pulses are distributed in space and time along a line; (ii) how the timing of pulses affects the perceived and reproduced continuity of sequences; and (iii) how effectively a second parallel row of speakers can extend sonic gestures to a two-dimensional space. Results show that azimuthal trajectories can be effectively replicated and that switching between discrete and continuous gestures occurs within the range of inter-pulse interval from 75 to 300ms. The vertical component of sonic gestures cannot be reliably replicated. © 2012 ACM.",Auditory localization; Sonic gestures,Computer science; Sensory perception; Auditory display; Auditory localization; Sonic gestures; Sound pulse; Space and time; Spatial gestures; Two dimensional spaces; Vertical component; Loudspeakers
Which facial profile do humans expect after seeing a frontal view? A comparison with a linear face model,2012,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864851581&doi=10.1145%2f2325722.2325724&partnerID=40&md5=2ad95aacbed1177a862fca37a33b1bd9,"Manipulated versions of three-dimensional faces that have different profiles, but almost the same appearance in frontal views, provide a novel way to investigate if and how humans use class-specific knowledge to infer depth from images of faces. After seeing a frontal view, participants have to select the profile that matches that view. The profiles are original (ground truth), average, random other, and two solutions computed with a linear face model (3D Morphable Model). One solution is based on 2D vertex positions, the other on pixel colors in the frontal view. The human responses demonstrate that humans neither guess nor just choose the average profile. The results also indicate that humans actually use the information from the front view, and not just rely on the plausibility of the profiles per se. All our findings are perfectly consistent with a correlation-based inference in a linear face model. The results also verify that the 3D reconstructions from our computational algorithms (stimuli 4 and 5) are similar to what humans expect, because they are chosen to be the true profile equally often as the ground-truth profiles. Our experiments shed new light on the mechanisms of human face perception and present a new quality measure for 3D reconstruction algorithms. © 2012 ACM.",Depth; Faces; Morphable Model; Shape perception,Algorithms; Image matching; 3-D reconstruction algorithms; 3D Morphable model; 3D reconstruction; Average profiles; Computational algorithm; Depth; Face models; Faces; Facial profiles; Ground truth; Human faces; Human response; Morphable model; Pixel color; Quality measures; Shape perception; Three dimensional
Introduction to special issue SAP 2012,2012,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864842800&doi=10.1145%2f2325722.2325723&partnerID=40&md5=2fe7145c4255530ed1485dac62d72ce3,[No abstract available],,
Perception of blending in stereo motion panoramas,2012,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864839123&doi=10.1145%2f2325722.2325728&partnerID=40&md5=8d6bd22688ce2e0572f171f48204b264,"Most methods for synthesizing panoramas assume that the scene is static. A few methods have been proposed for synthesizing stereo or motion panoramas, but there has been little attempt to synthesize panoramas that have both stereo and motion. One faces several challenges in synthesizing stereo motion panoramas, for example, to ensure temporal synchronization between left and right views in each frame, to avoid spatial distortion of moving objects, and to continuously loop the video in time. We have recently developed a stereo motion panorama method that tries to address some of these challenges. The method blends spacetime regions of a video XYT volume, such that the blending regions are distinct and translate over time. This article presents a perception experiment that evaluates certain aspects of the method, namely how well observers can detect such blending regions. We measure detection time thresholds for different blending widths and for different scenes, and for monoscopic versus stereoscopic videos. Our results suggest that blending may be more effective in image regions that do not contain coherent moving objects that can be tracked over time. For example, we found moving water and partly transparent smoke were more effectively blended than swaying branches. We also found that performance in the task was roughly the same for mono versus stereo videos. © 2012 ACM.",Blending; Motion; Omnistereo; Perception; Stereo; Visual perception,Computer science; Sensory perception; Detection time; Image regions; Monoscopic; Motion; Moving objects; Moving water; Omnistereo; Perception experiment; Space-time region; Spatial distortion; Stereo; Stereo motion; Stereo video; Stereoscopic video; Temporal synchronization; Visual perception; Blending
Visual and emotional salience influence eye movements,2012,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864830150&doi=10.1145%2f2325722.2325726&partnerID=40&md5=9a45f76a6afd24e9baa0ffd8e952b548,"In natural vision both stimulus features and cognitive/affective factors influence an observer's attention. However, the relationship between stimulus-driven (bottom-up) and cognitive/affective (top-down) factors remains controversial: How well does the classic visual salience model account for gaze locations? Can emotional salience counteract strong visual stimulus signals and shift attention allocation irrespective of bottom-up features? Here we compared Itti and Koch's [2000] and Spectral Residual (SR) visual salience model and explored the impact of visual salience and emotional salience on eye movement behavior, to understand the competition between visual salience and emotional salience and how they affect gaze allocation in complex scenes viewing. Our results show the insufficiency of visual salience models in predicting fixation. Emotional salience can override visual salience and can determine attention allocation in complex scenes. These findings are consistent with the hypothesis that cognitive/affective factors play a dominant role in active gaze control. © 2012 ACM.",Attention; Bottom-up; Emotional salience; Eye movements; Top-down; Visual salience,Computer science; Sensory perception; Attention; Bottom-up; Emotional salience; Topdown; Visual salience; Eye movements
Minification affects verbal- and action-based distance judgments differently in head-mounted displays,2012,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864831169&doi=10.1145%2f2325722.2325727&partnerID=40&md5=d23c68faf64dcbbe5e78fea41caaf7aa,"Numerous studies report that people underestimate egocentric distances in Head-Mounted Display (HMD) virtual environments compared to real environments as measured by direct blind walking. Geometric minification, or rendering graphics with a larger field of view than the display's field of view, has been shown to eliminate this underestimation in a virtual hallway environment [Kuhl et al. 2006, 2009]. This study demonstrates that minification affects blind walking in a sparse classroom and does not influence verbal reports of distance. Since verbal reports of distance have been reported to be compressed in real environments, we speculate that minification in an HMD replicates peoples' real-world blind walking and verbal report distance judgments. We also demonstrate a new method for quantifying any unintentional miscalibration in our experiments. This process involves using the HMD in an augmented reality configuration and having each participant indicate where the targets and horizon appeared after each experiment. More work is necessary to understand how and why minification changes verbal- and walking-based egocentric distance judgments differently.© 2012 ACM.",Distance judgments; Head-mounted display; Minification; Perception; Virtual environments,Augmented reality; Experiments; Sensory perception; Virtual reality; Blind walking; Direct blind walking; Distance judgments; Field of views; Head mounted displays; Larger fields; Minification; Miscalibration; Real environments; Helmet mounted displays
Single-trial EEG classification of artifacts in videos,2012,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864857932&doi=10.1145%2f2325722.2325725&partnerID=40&md5=de800f72c3ff909d74a326c332f46c52,"In this article we use an ElectroEncephaloGraph (EEG) to explore the perception of artifacts that typically appear during rendering and determine the perceptual quality of a sequence of images. Although there is an emerging interest in using an EEG for image quality assessment, one of the main impediments to the use of an EEG is the very low Signal-to-Noise Ratio (SNR) which makes it exceedingly difficult to distinguish neural responses from noise. Traditionally, event-related potentials have been used for analysis of EEG data. However, they rely on averaging and so require a large number of participants and trials to get meaningful data. Also, due the the low SNR ERP's are not suited for single-trial classification. We propose a novel wavelet-based approach for evaluating EEG signals which allows us to predict the perceived image quality from only a single trial. Our wavelet-based algorithm is able to filter the EEG data and remove noise, eliminating the need for many participants or many trials. With this approach it is possible to use data from only 10 electrode channels for single-trial classification and predict the presence of an artifact with an accuracy of 85%. We also show that it is possible to differentiate and classify a trial based on the exact type of artifact viewed. Our work is particularly useful for understanding how the human visual system responds to different types of degradations in images and videos. An understanding of the perception of typical image-based rendering artifacts forms the basis for the optimization of rendering and masking algorithms. © 2012 ACM.",EEG; Human visual system; Perception; Perception of rendering artifacts; Rendering; SVM; Wavelets,Algorithms; Image quality; Image reconstruction; Sensory perception; Signal to noise ratio; EEG signals; Event related potentials; Human Visual System; Image quality assessment; Image-Based Rendering; Low signal-to-noise ratio; Low SNR; Neural response; Perceptual quality; REmove noise; Rendering; Sequence of images; Single trial; Single-trial EEG; SVM; Wavelet-based algorithms; Wavelet-based approach; Wavelets; Electroencephalography
Real-time adaptive blur for reducing eye strain in stereoscopic displays,2012,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863712411&doi=10.1145%2f2207216.2207220&partnerID=40&md5=5e8c49da075b3e5ceaf6dac92edf45d6,"Stereoscopic devices are widely used (immersion-based working environments, stereoscopically-viewed movies, auto-stereoscopic screens). In some instances, exposure to stereoscopic immersion techniques can be lengthy, and so eye strain sets in. We propose a method for reducing eye strain induced by stereoscopic vision. After reviewing sources of eye strain linked to stereoscopic vision, we focus on one of these sources: images with high frequency content associated with large disparities. We put forward an algorithm for removing the irritating high frequencies in high horizontal disparity zones (i.e., for virtual objects appearing far from the real screen level). We elaborate on our testing protocol to establish that our image processing method reduces eye strain caused by stereoscopic vision, both objectively and subjectively. We subsequently quantify the positive effects of our algorithm on the relief of eye strain and discuss further research perspectives. © 2012 ACM.",Blur; Stereoscopy; Tracked point of view; Virtual reality; Visual fatigue,Image processing; Virtual reality; Blur; Eye strain; High frequency; Horizontal disparity; Image processing - methods; Immersion technique; Large disparity; Stereoscopic display; Stereoscopic vision; Stereoscopy; Testing protocols; Tracked point of view; Virtual objects; Visual fatigue; Working environment; Algorithms
Gaze-contingent visual presentation technique with electro-ocular-graph- based saccade detection,2012,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863697553&doi=10.1145%2f2207216.2207217&partnerID=40&md5=9d9b6619e0cbcfa9bc7097a6fbc49400,"When a single column of light sources flashes quickly in a temporal pattern during a horizontal saccade eye movement, twodimensional images can be perceived in the space neighboring the light source. This perceptual phenomenon has been applied to light devices for visual arts and entertainment. However, a serious drawback in exploiting this perceptual phenomenon for a visual information display is that a two-dimensional image cannot be viewed if there is any discrepancy between the ocular motility and the flicker timing. We overcame this drawback by combining the saccade-based display with an electro-ocular-graphbased sensor for detecting the saccade. The saccade onset is measured with the electro-ocular-graph-based sensor in real time and the saccade-based display is activated instantaneously as the saccade begins. The psychophysical experiments described in this article demonstrates that the method that we used can detect saccades with low latency and allows the saccade-based display to convey visual information more effectively than when the light sources continuously blink regardless of the observer's eye movements. © 2012 ACM.",Character recognition; Electro-ocular-graph; Saccade detection; Saccade-based display; Visual display,Character recognition; Graphic methods; Light sources; Sensors; Electro-ocular-graph; Gaze-contingent; Low latency; Psychophysical experiments; Real time; Temporal pattern; Two-dimensional images; Visual arts; Visual display; Visual information; Visual presentation; Eye movements
Evaluating the effectiveness of orientation indicators with an awareness of individual differences,2012,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863652466&doi=10.1145%2f2207216.2207218&partnerID=40&md5=5aa4159c4f399a9adf88b1e5a5289c96,"Understanding how users perceive 3D geometric objects can provide a basis for creating more effective tools for visualization in applications such as CAD or medical imaging. This article examines how orientation indicators affect users' accuracy in perceiving the shape of a 3D object shown as multiple views. Multiple views force users to infer the orientation of an object and recognize corresponding features between distinct vantage points. These are difficult tasks, and not all users are able to carry them out accurately. We use a cognitive experimental paradigm to evaluate the effectiveness of two types of orientation indicators on a person's ability to compare views of objects presented in different orientations. The orientation indicators implemented were colocated, which shared a center-point with the 3D object, or noncolocated with (displaced from) the 3D object. The study accounts for additional factors including object complexity, axis of rotation, and users' individual differences in spatial abilities. Our results show that an orientation indicator helps users in comparing multiple views, and that the effect is influenced by the type of aid, a person's spatial ability, and the difficulty of the task. In addition to establishing an effect of an orientation indicator, this article helps demonstrate the application of a particular experimental paradigm and analysis, as well as the importance of considering individual differences when designing interface aids. © 2012 ACM.",Cognitive support; Mental rotation; Methodology; Visualization,Computer aided design; Flow visualization; Medical imaging; Three dimensional computer graphics; Visualization; 3D object; Axis of rotation; Co-located; Cognitive support; Effective tool; Geometric objects; Individual Differences; Mental rotation; Methodology; Multiple views; Spatial abilities; Three dimensional
Parametric time-frequency representation of spatial sound in virtual worlds,2012,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863688923&doi=10.1145%2f2207216.2207219&partnerID=40&md5=490d6a782e9405bd296920a0f8740ef2,Directional audio coding (DirAC) is a parametric time-frequency domain method for processing spatial audio based on psychophysical assumptions and on energetic analysis of the sound field. Methods to use DirAC in spatial sound synthesis for virtual worlds are presented in this article. Formal listening tests are used to show that DirAC can be used to position and to control the spatial extent of virtual sound sources with good audio quality. It is also shown that DirAC can be used to generate reverberation for N-channel horizontal listening with only two monophonic reverberators without a prominent loss in quality when compared with quality obtained with N-channel reverberators. © 2012 ACM.,Spatial sound; Time-frequency processing,Acoustic fields; Frequency domain analysis; Sound reproduction; Virtual reality; Audio Coding; Audio quality; Energetic analysis; Listening tests; N-channel; Psychophysical; Spatial audio; Spatial extent; Spatial sound; Time frequency domain; Time-frequency processing; Time-frequency representations; Virtual sound sources; Virtual worlds; Quality control
Reinforcement learning utilizes proxemics: An avatar learns to manipulate the position of people in immersive virtual reality,2012,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859474974&doi=10.1145%2f2134203.2134206&partnerID=40&md5=83e08cb9da9427c493cfb59a2296e9f2,"A reinforcement learning (RL) method was used to train a virtual character to move participants to a specified location. The virtual environment depicted an alleyway displayed through a wide field-of-view head-tracked stereo head-mounted display. Based on proxemics theory, we predicted that when the character approached within a personal or intimate distance to the participants, they would be inclined to move backwards out of the way. We carried out a between-groups experiment with 30 female participants, with 10 assigned arbitrarily to each of the following three groups: In the Intimate condition the character could approach within 0.38m and in the Social condition no nearer than 1.2m. In the Random condition the actions of the virtual character were chosen randomly from among the same set as in the RL method, and the virtual character could approach within 0.38m. The experiment continued in each case until the participant either reached the target or 7 minutes had elapsed. The distributions of the times taken to reach the target showed significant differences between the three groups, with 9 out of 10 in the Intimate condition reaching the target significantly faster than the 6 out of 10 who reached the target in the Social condition. Only 1 out of 10 in the Random condition reached the target. The experiment is an example of applied presence theory: we rely on the many findings that people tend to respond realistically in immersive virtual environments, and use this to get people to achieve a task of which they had been unaware. This method opens up the door for many such applications where the virtual environment adapts to the responses of the human participants with the aim of achieving particular goals. © 2012 ACM 1544-3558/2012/03- ART3 ©10.00.",Experimentation; Human Factors,Experiments; Human computer interaction; Human engineering; Reinforcement learning; Experimentation; Head mounted displays; Head-tracked; Immersive virtual environments; Immersive virtual reality; Random condition; Reinforcement learning method; Social conditions; Virtual character; Wide field-of-view; Virtual reality
Scene-Motion Thresholds During Head Yaw for Immersive Virtual Environments,2012,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025412209&doi=10.1145%2f2134203.2134207&partnerID=40&md5=88e8630041e77bb74303541e9b6c3b81,"In order to better understand how scene motion is perceived in immersive virtual environments, we measured scene-motion thresholds under different conditions across three experiments. Thresholds were measured during quasi-sinusoidal head yaw, single left-to-right or right-to-left head yaw, different phases of head yaw, slow to fast head yaw, scene motion relative to head yaw, and two scene-illumination levels.We found that across various conditions (1) thresholds are greater when the scene moves with head yaw (corresponding to gain <1.0) than when the scene moves against head yaw (corresponding to gain >1.0), and (2) thresholds increase as head motion increases. © 2012, ACM. All rights reserved.",Experimentation; head motion; Human Factors; latency; Measurement; Psychophysics; redirected walking; scene-motion thresholds,Human engineering; Condition; Experimentation; Head motion; Illumination levels; Immersive virtual environments; Virtual reality
Enhancing visuospatial map learning through action on cellphones,2012,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859470823&doi=10.1145%2f2134203.2134208&partnerID=40&md5=8e7bd1ad2becd3bac5ddf013df14ccd2,"The visuospatial learning of a map on cellphone displays was examined. The spatial knowledge of human participants was assessed after they had learned the relative positions of London Underground stations on a map via passive, marginally active, or active exploration. Following learning, the participants were required to answer questions in relation to the spatial representation and distribution of the stations on the map. Performances were compared between conditions involving (1) without auditory cues versus continuous auditory cues; (2) without auditory cues versus noncontinuous auditory cues; and (3) continuous auditory cues versus noncontinuous auditory cues. Results showed that the participants perfomed better following active and marginally-active explorations, as compared to purely passive learning. These results also suggest that under specific conditions (i.e., continuous sound with extremely fast tempo) there is no benefit to spatial abilities from active exploration over passive observation; while continuous sound with moderate to fast tempo is effective for simple actions (i.e., key press). © 2012 ACM 1544-3558/2012/03-ART5 $10.00.",Experimentation; Human Factors; Performance,Human engineering; Subway stations; Active explorations; Auditory cues; Cell phone; Experimentation; London Underground; Map learning; Passive learning; Performance; Relative positions; Spatial abilities; Spatial knowledge; Spatial representations; Telephone sets
Voluntary facial activations regulate physiological arousal and subjective experiences during virtual social stimulation,2012,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859475112&doi=10.1145%2f2134203.2134204&partnerID=40&md5=0024b3bde8269f374ef347b70d3896ca,"Exposure to distressing computer-generated stimuli and feedback of physiological changes during exposure have been effective in the treatment of anxiety disorders (e.g., social phobia). Here we studied voluntary facial activations as a method for regulating more spontaneous physiological changes during virtual social stimulation. Twenty-four participants with a low or high level of social anxiety activated either the corrugator supercilii (used in frowning) or the zygomaticus major (used in smiling) facial muscle to keep a female or a male computer character walking towards them. The more socially anxious participants had a higher level of skin conductance throughout the trials as compared to less anxious participants. Within both groups, short-term skin conductance responses were enhanced both during and after facial activations; and corrugator supercilii activations facilitated longer term electrodermal relaxation. Zygomaticus major activations had opposite effects on subjective emotional ratings of the less and the more socially anxious. In sum, voluntary facial activations were effective in regulating emotional arousal during virtual social exposure. Corrugator supercilii activation was found an especially promising method for facilitating autonomic relaxation. © 2012 ACM 1544-3558/2012/03-ART1 $10.00.",Experimentation; Human Factors,Human engineering; Physiology; Corrugator; Experimentation; Facial muscles; Skin conductance; Chemical activation
Multimodal recognition of reading activity in transit using body-worn sensors,2012,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859448311&doi=10.1145%2f2134203.2134205&partnerID=40&md5=33d2483e7980d181c16755ae95e89dd9,"Reading is one of the most well-studied visual activities. Vision research traditionally focuses on understanding the perceptual and cognitive processes involved in reading. In this work we recognize reading activity by jointly analyzing eye and head movements of people in an everyday environment. Eye movements are recorded using an electrooculography (EOG) system; body movements using body-worn inertial measurement units. We compare two approaches for continuous recognition of reading: String matching (STR) that explicitly models the characteristic horizontal saccades during reading, and a support vector machine (SVM) that relies on 90 eye movement features extracted from the eye movement data. We evaluate both methods in a study performed with eight participants reading while sitting at a desk, standing, walking indoors and outdoors, and riding a tram. We introduce a method to segment reading activity by exploiting the sensorimotor coordination of eye and head movements during reading. Using person-independent training, we obtain an average precision for recognizing reading of 88.9% (recall 72.3%) using STR and of 87.7% (recall 87.9%) using SVM over all participants. We show that the proposed segmentation scheme improves the performance of recognizing reading events by more than 24%. Our work demonstrates that the joint analysis of eye and body movements is beneficial for reading recognition and opens up discussion on the wider applicability of a multimodal recognition approach to other visual and physical activities. © 2012 ACM 1544-3558/2012/03-ART2 $10.00.",Algorithms; Experimentation; Measurement,Algorithms; Feature extraction; Measurements; Support vector machines; Units of measurement; Body movements; Body-worn sensors; Cognitive process; Experimentation; Head movements; Inertial measurement unit; Joint analysis; Multimodal recognition; Person-independent; Physical activity; Segmentation scheme; String matching; Support vector machine (SVM); Vision research; Visual activity; Eye movements
Modeling human aesthetic perception of visual textures,2011,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855228714&doi=10.1145%2f2043603.2043609&partnerID=40&md5=f4c8b67ace9e5f94f97588b8b65636b5,"Texture is extensively used in areas such as product design and architecture to convey specific aesthetic information. Using the results of a psychological experiment, we model the relationship between computational texture features and aesthetic properties of visual textures. Contrary to previous approaches, we build a layered model, which provides insights into hierarchical relationships involved in human aesthetic texture perception. This model uses a set of intermediate judgements to link computational texture features with aesthetic texture properties. We pursue two different approaches for modeling. (1) Supervised machine-learning methods are used to generate linear and nonlinear models from the experimental data automatically. The quality of these models is discussed, mainly focusing on interpretability and accuracy. (2) We apply a psychological-based approach that models the processing pathways in human perception of naturalness, introducing judgement dimensions (principal components) mediating the relationship between texture features and naturalness judgements. This multiple mediator model serves as a verification of the machine-learning approach. We conclude with a comparison of these two approaches, highlighting the similarities and discrepancies in terms of identified relationships between computational texture features and aesthetic properties of visual textures. © 2011 ACM.",Aesthetic space; Human judgments; Human perception; Machine learning; Mediator analysis; Modeling; Texture analysis,Learning systems; Models; Product design; Quality control; Aesthetic space; Human judgments; Human perception; Machine-learning; Mediator analysis; Texture analysis; Textures
Bimodal task-facilitation in a virtual traffic scenario through spatialized sound rendering,2011,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855213891&doi=10.1145%2f2043603.2043606&partnerID=40&md5=d0c12e5e5510b519150a7ac34b38c028,"Audio rendering is generally used to increase the realism of virtual environments (VE). In addition, audio rendering may also improve the performance in specific tasks carried out in interactive applications such as games or simulators. In this article we investigate the effect of the quality of sound rendering on task performance in a task which is inherently vision-dominated. The task is a virtual traffic gap-crossing scenario with two elements: first, to discriminate crossable and uncrossable gaps in oncoming traffic, and second, to find the right timing to start crossing the street without an accident. A study was carried out with 48 participants in an immersive virtual environment setup with a large screen and headphones. Participants were grouped into three different scenarios. In the first one, spatialized audio rendering with head-related transfer function (HRTF) filtering was used. The second group was tested with conventional stereo rendering, and the remaining group ran the experiment in a mute condition. Our results give a clear evidence that spatialized audio improves task performance compared to the unimodal mute condition. Since all task-relevant information was in the participants' field-of-view, we conclude that an enhancement of task performance results from a bimodal advantage due to the integration of visual and auditory spatial cues. © 2011 ACM.",Audio-visual perception; Bimodal; Human-computer interaction; Pedestrian safety; Pedestrian simulator; Spatialized audio rendering; Task facilitation; Virtual environments,Human computer interaction; Pedestrian safety; Sensory perception; Sound reproduction; Virtual reality; Audio rendering; Bimodal; Field of views; Head related transfer function; Immersive virtual environments; Interactive applications; Large screen; Quality of sounds; Second group; Spatial cues; Spatialized audio; Spatialized sound; Specific tasks; Task facilitation; Task performance; Unimodal; Virtual environments; Audio acoustics
CyberWalk: Enabling unconstrained omnidirectional walking through virtual environments,2011,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855228569&doi=10.1145%2f2043603.2043607&partnerID=40&md5=2c9ad5f505b4838887a251f14fa511b1,"Despite many recent developments in virtual reality, an effective locomotion interface which allows for normal walking through large virtual environments was until recently still lacking. Here, we describe the new CyberWalk omnidirectional treadmill system, which makes it possible for users to walk endlessly in any direction, while never leaving the confines of the limited walking surface. The treadmill system improves on previous designs, both in its mechanical features and in the control system employed to keep users close to the center of the treadmill. As a result, users are able to start walking, vary their walking speed and direction, and stop walking as they would on a normal, stationary surface. The treadmill system was validated in two experiments, in which both the walking behavior and the performance in a basic spatial updating task were compared to that during normal overground walking. The results suggest that walking on the CyberWalk treadmill is very close to normal walking, especially after some initial familiarization. Moreover, we did not find a detrimental effect of treadmill walking in the spatial updating task. The CyberWalk system constitutes a significant step forward to bringing the real world into the laboratory or workplace. © 2011 ACM.",Control system; Locomotion; Spatial navigation; Treadmill; Virtual reality,Biped locomotion; Control systems; Virtual reality; Detrimental effects; Locomotion interfaces; Mechanical feature; Normal walking; Spatial navigation; Spatial updating; Treadmill; Treadmill walking; Virtual environments; Walking behavior; Walking speed; Walking surfaces; Walking through; Sporting goods
Attentional gradient for crossmodal proximal-distal tactile cueing of visual spatial attention,2011,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855215426&doi=10.1145%2f2043603.2043605&partnerID=40&md5=71b4516b8afd4118baafcfc96244a978,"Past studies have established a crossmodal spatial attentional link among vision, audition, and touch. The present study examined the dependence of visual attention on the distance between a distal visual target (a changing element among static distractors) and the quadrant of the visual display cued by a proximal tactile stimulus. The distance between the center of the cued visual quadrant and the visual target was one of six values: 0, 90, 180, 350, 450, and 550 pixels. The distances of 0, 90, and 180 corresponded to the valid tactile cueing condition, where the tactile cue and the visual target occurred in the same quadrant. The distances of 350, 450, and 550 corresponded to the invalid tactile cueing condition, where the tactilely-cued quadrant did not match that of the visual change. Results from 10 young adults showed that mean response time increased with respect to the cue-target distance, thereby confirming a gradient of visual attention for proximal-distal tactile cueing. In addition, the response times for valid tactile cues were shorter than those for invalid tactile cues, confirming earlier findings that valid tactile cues facilitate visual search and invalid tactile cues interfere with visual search. The findings of the present study have implications for the design of multimodal attention-cueing systems for practical applications such as collision warning systems in automobiles. © 2011 ACM.",Attention gradient; Crossmodal attention cueing; Eye Gaze; Proximal-distal cueing; Tactile cueing,Alarm systems; Attention gradient; Crossmodal attention; Eye-gaze; Proximal-distal cueing; Tactile cueing; Response time (computer systems)
Effects of scale change on distance perception in virtual environments,2011,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855244057&doi=10.1145%2f2043603.2043608&partnerID=40&md5=c8c291fd22eec8146f12cd80ad9823f2,"We conducted a series of experiments to investigate effects of scale changes on distance perception in virtual environments. All experiments were carried out in an HMD. Participants first made distance estimates with feedback in a virtual tunnel (adaptation) and then made distance estimates without feedback in a differently-scaled virtual environment (test).We examined several types of scale changes, including changing the size of (1) the tunnel, (2) the targets, and (3) the separation of the two targets. Changes in target size always affected distance estimates at test. When the targets became smaller, participants overshot distance and when the targets became larger, participants undershot distance. Changes in the size of the tunnel or the separation between the targets (without a change in the size of the targets) had a minimal effect on distance estimates. These results indicate that distance estimates at test were strongly influenced by familiar size cues for distance. The discussion focuses on the stability of calibration processes and mechanisms for cue integration for perceiving distance in virtual environments. © 2011 ACM.",Distance estimation; Head-mounted displays; Perception; Scale; Virtual environments,Depth perception; Estimation; Experiments; Helmet mounted displays; Sensory perception; Calibration process; Cue integration; Distance estimation; Distance perception; Effects of scale; Head mounted displays; Investigate effects; Minimal effects; Scale; Size cues; Target size; Virtual environments; Virtual reality
Redirected walking to explore virtual environments: Assessing the potential for spatial interference,2011,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855243871&doi=10.1145%2f2043603.2043604&partnerID=40&md5=849f0a43f0fe124fd20cbdb756064998,"Redirected walking has gained popularity in recent years as a way of enhancing the safety of users immersed in a virtual reality simulation and of extending the amount of space that can be simulated in a virtual environment (VE). Limits imposed by the available physical space and functional tracking area are overcome by inducing immersed users to veer imperceptibly in a way that prevents them from leaving the confines of the tracking space. Redirected walking has been shown to be feasible at levels below noticeable thresholds and to function without increasing the incidence of simulator sickness. The present studies demonstrate that redirected walking can function without negatively impacting memory for spatial locations of landmarks in a VE, despite introducing discrepancies between various spatial senses and distorting the spatial mapping of movement onto the environment. Additionally, the present studies implement what, to our knowledge, is the first generalized redirected walking algorithm that is independent of any task or environment structure, and can adaptively steer users in real time as they engage in spontaneous, unconstrained navigation. The studies also demonstrate that such an algorithm can be implemented successfully in a gymnasium-sized space. © 2011 ACM.",3D interfaces; Human computer interaction; Multimodal sensory integration; Redirected walking; Spatial cognition; Spatial memory; Spatial senses; Virtual reality,Algorithms; Human computer interaction; 3D interface; Multi-modal; Redirected walking; Spatial cognition; Spatial memory; Spatial senses; Virtual reality
Integrating multiple views with virtual mirrors to facilitate scene understanding,2011,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80054845836&doi=10.1145%2f2043603.2043610&partnerID=40&md5=393725d0fdcd3bec628bc60c84b3ed04,"In this article, an image integration technique called Virtual Mirroring (VM) is evaluated. VM is a technique that combines multiple 2D views of a 3D scene into a single composite image by overlaying views onto virtual mirrors. Given multiple views of a scene, one view is augmented with the remaining views by placing virtual mirrors on the first view and overlaying onto them the corresponding remaining views. Unlike a standard array presentation, where 2D views are not integrated and simply placed adjacent to one another, the VM presentation preserves the relative location, orientation, and scale between views. As such, it is our contention that humans will fare better at performing certain visual tasks, such as scene identification, when viewing a 3D scene via a VM presentation than when viewing an array presentation. We performed an experiment on 12 participants, where participants were required to identify 96 scenes both with a VM and an array presentation and we compared their % correctness and response times. Moreover, we studied the effects of adding an auditory attentional load on performance. We found that regardless of load, participants were able to identify scenes using VM presentation with greater accuracy and at greater speeds. © 2011 ACM.",Attentional Load; Image integration; Psychophysics; Scene identification; Virtual mirror; Visual task,Integration; Mirrors; Virtual reality; Vision; Attentional load; Image integration; Psychophysics; Virtual mirror; Visual tasks; Three dimensional
Near-field distance perception in real and virtual environments using both verbal and action responses,2011,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80052371388&doi=10.1145%2f2010325.2010328&partnerID=40&md5=2000b882c05d905c89880a487efbe384,"Few experiments have been performed to investigate near-field egocentric distance estimation in an Immersive Virtual Environment (IVE) as compared to the Real World (RW). This article investigates near-field distance estimation in IVEs and RW conditions using physical reach and verbal report measures, by using an apparatus similar to that used by Bingham and Pagano [1998]. Analysis of our experiment shows distance compression in both the IVE and RW conditions in participants' perceptual judgments to targets. This is consistent with previous research in both action space in an IVE and reach space with Augmented Reality (AR). Analysis of verbal responses from participants revealed that participants underestimated significantly less in the virtual world as compared to the RW. We also found that verbal reports and reaches provided different results in both IVEs and RW environments. © 2011 ACM.",Depth perception; Distance estimation; Human factors and usability; Immersive virtual environments; Virtual reality,Augmented reality; Depth perception; Estimation; Experiments; Human computer interaction; Human engineering; Action spaces; Distance estimation; Distance perception; Immersive virtual environments; Near-field; Virtual environments; Virtual worlds; Virtual reality
Perceptual considerations for motion blur rendering,2011,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80052359030&doi=10.1145%2f2010325.2010330&partnerID=40&md5=dc837840f03d91f6d87fbd8a7696b89a,"Motion blur is a frequent requirement for the rendering of high-quality animated images. However, the computational resources involved are usually higher than those for images that have not been temporally antialiased. In this article we study the influence of high-level properties such as object material and speed, shutter time, and antialiasing level. Based on scenes containing variations of these parameters, we design different psychophysical experiments to determine how influential they are in the perception of image quality. This work gives insights on the effects these parameters have and exposes certain situations where motion blurred stimuli may be indistinguishable from a gold standard. As an immediate practical application, images of similar quality can be produced while the computing requirements are reduced. Algorithmic efforts have traditionally been focused on finding new improved methods to alleviate sampling artifacts by steering computation to the most important dimensions of the rendering equation. Concurrently, rendering algorithms can take advantage of certain perceptual limits to simplify and optimize computations. To our knowledge, none of them has identified nor used these limits in the rendering of motion blur. This work can be considered a first step in that direction. © 2011 ACM.",Motion blur; Perceptual rendering; Temporal antialiasing,Anti-aliasing; Image quality; Computational resources; Gold standards; High quality; Improved methods; Motion blur; Perceptual rendering; Psychophysical experiments; Rendering algorithms; Rendering equation; Sampling artifacts; Temporal antialiasing; Algorithms
Modeling and animating eye blinks,2011,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80052386773&doi=10.1145%2f2010325.2010327&partnerID=40&md5=2cb3ed643ccec1ce3e1a2acada0c4baf,"Facial animation often falls short in conveying the nuances present in the facial dynamics of humans. In this article, we investigate the subtleties of the spatial and temporal aspects of eye blinks. Conventional methods for eye blink animation generally employ temporally and spatially symmetric sequences; however, naturally occurring blinks in humans show a pronounced asymmetry on both dimensions. We present an analysis of naturally occurring blinks that was performed by tracking data from high-speed video using active appearance models. Based on this analysis, we generate a set of key-frame parameters that closely match naturally occurring blinks. We compare the perceived naturalness of blinks that are animated based on real data to those created using textbook animation curves. The eye blinks are animated on two characters, a photorealistic model and a cartoon model, to determine the influence of character style. We find that the animated blinks generated from the human data model with fully closing eyelids are consistently perceived as more natural than those created using the various types of blink dynamics proposed in animation textbooks. © 2011 ACM.",Eye blinks,Textbooks; Active appearance models; Conventional methods; Eye blink; Facial animation; High-speed video; Human data; Key frames; Naturally occurring; Photo-realistic; Temporal aspects; Tracking data; Animation
Online-Only Introduction,2011,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025404183&doi=10.1145%2f2010325.2010331&partnerID=40&md5=647a9b89347e421d594625d59b586f8b,[No abstract available],,
Evaluation of walking in place on a Wii balance board to explore a virtual environment,2011,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80052376644&doi=10.1145%2f2010325.2010329&partnerID=40&md5=128358d3ac7facff98f02b220b8dac35,"In this work, we present a method of ""Walking In Place"" (WIP) on the Nintendo Wii Fit Balance Board to explore a virtual environment. We directly compare our method to joystick locomotion and normal walking. The joystick proves inferior to physically walking and to WIP on the Wii Balance Board (WIP-Wii). Interestingly, we find that physically exploring an environment on foot is equivalent in terms of spatial orientation to exploring an environment using our WIP-Wii method. This implies that the WIP-Wii is a good inexpensive alternative to exploring a virtual environment and it may be well-suited for exploring large virtual environments. © 2011 ACM.",Graphics; Human-computer interaction; Space perception; Virtual Reality (VR),Human computer interaction; Interactive computer graphics; Knowledge management; Graphics; Human-computer; Nintendo WII; Normal walking; Space perception; Spatial orientations; Virtual environments; Walking-in-place; Virtual reality
Do predictions of visual perception aid design?,2011,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79551673696&doi=10.1145%2f1870076.1870080&partnerID=40&md5=ef592ca548c434336da200e5af2aa763,"Understanding and exploiting the abilities of the human visual system is an important part of the design of usable user interfaces and information visualizations. Designers traditionally learn qualitative rules of thumb for how to enable quick, easy, and veridical perception of their design. More recently, work in human and computer vision has produced more quantitative models of human perception, which take as input arbitrary, complex images of a design. In this article, we ask whether models of perception aid the design process, using our tool DesignEye as a working example of a perceptual tool incorporating such models. Through a series of interactions with designers and design teams, we find that the models can help, but in somewhat unexpected ways. DesignEye was capable of facilitating A/B comparisons between designs, and judgments about the quality of a design. However, overall ""goodness"" values were not very useful, showed signs of interfering with a natural process of trading off perceptual vs. other design issues, and would likely interfere with acceptance of a perceptual tool by professional designers. Perhaps most surprisingly, DesignEye, by providing in essence a simple visualization of the design, seemed to facilitate communication about not only perceptual aspects of design, but also about design goals and how to achieve those goals. We discuss resulting design principles for making perceptual tools useful in general.© 2011 ACM.",Clutter; Design tool; Field study; Human-computer interaction; Saliency; Visual perception,Clutter (information theory); Computer vision; Equipment; Human computer interaction; Knowledge management; User interfaces; Vision; Visualization; Clutter; Design tool; Field study; Saliency; Visual perception; Design
Object selection in gaze controlled systems: What you don't look at is what you get,2011,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79551670150&doi=10.1145%2f1870076.1870081&partnerID=40&md5=d782096b30263f4ac412576b56a2d9a1,"Controlling computers using eye movements can provide a fast and efficient alternative to the computer mouse. However, implementing object selection in gaze-controlled systems is still a challenge. Dwell times or fixations on a certain object typically used to elicit the selection of this object show several disadvantages. We studied deviations of critical thresholds by an individual and task-specific adaptation method. This demonstrated an enormous variability of optimal dwell times. We developed an alternative approach using antisaccades for selection. For selection by antisaccades, highlighted objects are copied to one side of the object. The object is selected when fixating to the side opposed to that copy requiring to inhibit an automatic gaze shift toward new objects. Both techniques were compared in a selection task. Two experiments revealed superior performance in terms of errors for the individually adapted dwell times. Antisaccades provide an alternative approach to dwell time selection, but they did not show an improvement over dwell time. We discuss potential improvements in the antisaccade implementation with which antisaccades might become a serious alternative to dwell times for object selection in gaze-controlled systems.© 2011 ACM.",Gaze control,Speech recognition; Alternative approach; Computer mouse; Controlled system; Controlling computers; Critical threshold; Dwell time; Gaze control; Gaze shifts; Object selection; Task-specific adaptation; Eye movements
Perception-motivated interpolation of image sequences,2011,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79551714517&doi=10.1145%2f1870076.1870079&partnerID=40&md5=780504f3620e85c37f9d8a067001dd95,"We present a method for image interpolation that is able to create high-quality, perceptually convincing transitions between recorded images. By implementing concepts derived from human vision, the problem of a physically correct image interpolation is relaxed to that of image interpolation which is perceived as visually correct by human observers. We find that it suffices to focus on exact edge correspondences, homogeneous regions and coherent motion to compute convincing results. A user study confirms the visual quality of the proposed image interpolation approach. We show how each aspect of our approach increases perceived quality of the result. We compare the results to other methods and assess achievable quality for different types of scenes.© 2011 ACM.",Image interpolation; Morphing; Perception,Coherent motion; High quality; Homogeneous regions; Human observers; Human vision; Image interpolations; Image sequence; Morphing; Perceived quality; Perception; User study; Visual qualities; Interpolation
Emulating human observers with bayesian binning: Segmentation of action streams,2011,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80052381935&doi=10.1145%2f2010325.2010326&partnerID=40&md5=8e9fae229ce1e5679a8539feeee4a37d,"Natural body movements arise in the form of temporal sequences of individual actions. During visual action analysis, the human visual system must accomplish a temporal segmentation of the action stream into individual actions. Such temporal segmentation is also essential to build hierarchical models for action synthesis in computer animation. Ideally, such segmentations should be computed automatically in an unsupervised manner. We present an unsupervised segmentation algorithm that is based on Bayesian Binning (BB) and compare it to human segmentations derived from psychophysical data. BB has the advantage that the observation model can be easily exchanged. Moreover, being an exact Bayesian method, BB allows for the automatic determination of the number and positions of segmentation points. We applied this method to motion capture sequences from martial arts and compared the results to segmentations provided by humans from movies that showed characters that were animated with the motion capture data. Human segmentation was then assessed by an interactive adjustment paradigm, where participants had to indicate segmentation points by selection of the relevant frames. Results show a good agreement between automatically generated segmentations and human performance when the trajectory segments between the transition points were modeled by polynomials of at least third order. This result is consistent with theories about differential invariants of human movements. © 2011 ACM.",Action segmentation; Bayesian methods; Motion capture; Unsupervised learning,Animation; Bayesian networks; Hierarchical systems; Image segmentation; Unsupervised learning; Action segmentation; Automatic determination; Automatically generated; Bayesian methods; Differential invariants; Motion capture; Temporal segmentations; Unsupervised segmentation; Barium compounds
Modeling locomotor control: The advantages of mobile gaze,2011,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79551664240&doi=10.1145%2f1870076.1870077&partnerID=40&md5=9440c0f57dc5a292e124c5c9f3ee89b1,"In 1958, JJ Gibson put forward proposals on the visual control of locomotion. Research in the last 50 years has served to clarify the sources of visual and nonvisual information that contribute to successful steering, but has yet to determine how this information is optimally combined under conditions of uncertainty. Here, we test the conditions under which a locomotor robot with a mobile camera can steer effectively using simple visual and extra-retinal parameters to examine how such models cope with the noisy real-world visual and motor estimates that are available to humans. This applied modeling gives us an insight into both the advantages and limitations of using active gaze to sample information when steering.© 2011 ACM.",Active vision; Eye movements; Gaze; Locomotion; Robot; Steering,Aldehydes; Robots; Active Vision; Gaze; Locomotion; Locomotor control; Mobile camera; Real-world; Sample information; Visual control of locomotion; Eye movements
Perception of mechanically and optically simulated bumps and holes,2010,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883754712&doi=10.1145%2f%2f1670671.1670674&partnerID=40&md5=86efcd3749c5e19dfe2c16c5e1ffc509,"In this article, we investigate the perception of optically simulated haptic feedback. The perception of optically and mechanically simulated bumps and holes was tested experimentally. In an earlier article, we have described the active cursor technique, a method to simulate haptic feedback optically without resorting to special mechanical force feedback devices, commonly applied to produce haptic percepts in computer interfaces. The operation of the force feedback device is substituted by tiny displacements on the cursor position relative to the intended force. This method exploits the domination of the visual over the haptic modality. Results show that people can recognize optically simulated bump and hole structures and that active cursor displacements influence the haptic perception of bumps and holes. Depending on the simulated strength of the force, optically simulated haptic feedback can take precedence over mechanically simulated haptic feedback and also the other way around. When optically simulated and mechanically simulated haptic feedback counteract each other, however, the weight attributed to each source of haptic information differs from user to user. It is concluded that active cursor displacements can be used to simulate the operation of mechanical force feedback devices. © 2010 ACM.",Experimentation; Force feedback; Multisensory perception; Optically simulated haptic feedback; Simulation,Mechanical engineering; Experimentation; Force feedback; Haptic feedbacks; Multisensory perceptions; Simulation; Haptic interfaces
An other-race effect for face recognition algorithms,2011,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79551667954&doi=10.1145%2f1870076.1870082&partnerID=40&md5=39b6125194ce71140d91f9895f1ac05b,"Psychological research indicates that humans recognize faces of their own race more accurately than faces of other races. This ""other-race effect"" occurs for algorithms tested in a recent international competition for state-of-the-art face recognition algorithms. We report results for a Western algorithm made by fusing eight algorithms from Western countries and an East Asian algorithm made by fusing five algorithms from East Asian countries. At the low false accept rates required for most security applications, the Western algorithm recognized Caucasian faces more accurately than East Asian faces and the East Asian algorithm recognized East Asian faces more accurately than Caucasian faces. Next, using a test that spanned all false alarm rates, we compared the algorithms with humans of Caucasian and East Asian descent matching face identity in an identical stimulus set. In this case, both algorithms performed better on the Caucasian faces-the ""majority"" race in the database. The Caucasian face advantage, however, was far larger for the Western algorithm than for the East Asian algorithm. Humans showed the standard other-race effect for these faces, but showed more stable performance than the algorithms over changes in the race of the test faces. State-of-the-art face recognition algorithms, like humans, struggle with ""other-race face"" recognition.© 2011 ACM.",Face recognition; Human-machine comparisons,Algorithms; Competition; International trade; Caucasians; East Asian countries; Face recognition algorithms; False accept rate; False alarm rate; Human-machine; International competitions; Other-race effects; Psychological research; Security application; Western countries; Face recognition
Human-inspired search for redundancy in automatic sign language recognition,2011,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79551659377&doi=10.1145%2f1870076.1870083&partnerID=40&md5=0cbaafae31777fc53c6e9fb7b6855c43,"Human perception of sign language can serve as inspiration for the improvement of automatic recognition systems. Experiments with human signers show that sign language signs contain redundancy over time. In this article, experiments are conducted to investigate whether comparable redundancies also exist for an automatic sign language recognition system. Such redundancies could be exploited, for example, by reserving more processing resources for the more informative phases of a sign, or by discarding uninformative phases. In the experiments, an automatic system is trained and tested on isolated fragments of sign language signs. The stimuli used were similar to those of the human signer experiments, allowing us to compare the results. The experiments show that redundancy over time exists for the automatic recognizer. The central phase of a sign is the most informative phase, and the first half of a sign is sufficient to achieve a recognition performance similar to that of the entire sign. These findings concur with the results of the human signer studies. However, there are differences as well, most notably the fact that human signers score better on the early phases of a sign than the automatic system. The results can be used to improve the automatic recognizer, by using only the most informative phases of a sign as input.© 2011 ACM.",Automatic sign language recognition; Sign language perception,Quality assurance; Redundancy; Automatic recognition system; Automatic systems; Human perception; Processing resources; Recognition performance; Sign language; Sign Language recognition; Experiments
Making virtual walking real: Perceptual evaluation of a new treadmill control algorithm,2010,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79951634469&doi=10.1145%2f%2f1670671.1670675&partnerID=40&md5=f82db772f5e6badc8d6dd7f4adb507c4,"For us humans, walking is our most natural way of moving through the world. One of the major challenges in present research on navigation in virtual reality is to enable users to physically walk through virtual environments. Although treadmills, in principle, allow users to walk for extended periods of time through large virtual environments, existing setups largely fail to produce a truly immersive sense of navigation. Partially, this is because of inadequate control of treadmill speed as a function of walking behavior. Here, we present a new control algorithm that allows users to walk naturally on a treadmill, including starting to walk from standstill, stopping, and varying walking speed. The treadmill speed control consists of a feedback loop based on the measured user position relative to a given reference position, plus a feed-forward term based on online estimation of the user's walking velocity. The purpose of this design is to make the treadmill compensate fully for any persistent walker motion, while keeping the accelerations exerted on the user as low as possible. We evaluated the performance of the algorithm by conducting a behavioral experiment in which we varied its most important parameters. Participants walked at normal walking speed and then, on an auditory cue, abruptly stopped. After being brought back to the center of the treadmill by the control algorithm, they rated how smoothly the treadmill had changed its velocity in response to the change in walking speed. Ratings, in general, were quite high, indicating good control performance. Moreover, ratings clearly depended on the control algorithm parameters that were varied. Ratings were especially affected by the way the treadmill reversed its direction of motion. In conclusion, controlling treadmill speed in such a way that changes in treadmill speed are unobtrusive and do not disturb VR immersiveness is feasible on a normal treadmill with a straightforward control algorithm. © 2010 ACM.",Motion control; Treadmill walking; Virtual reality,Algorithms; Motion control; Parameter estimation; Speed; Sporting goods; Virtual reality; Algorithm parameters; Behavioral experiment; Control performance; Direction of motion; Inadequate controls; Large virtual environments; Perceptual evaluation; Treadmill walking; Exercise equipment
Perceptual effects of scene context and viewpoint for virtual pedestrian crowds,2011,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79551664681&doi=10.1145%2f1870076.1870078&partnerID=40&md5=15f1ac230ebf56123854f168265e6ef6,"In this article, we evaluate the effects of position, orientation, and camera viewpoint on the plausibility of pedestrian formations. In a set of three perceptual studies, we investigated how humans perceive characteristics of virtual crowds in static scenes reconstructed from annotated still images, where the orientations and positions of the individuals have been modified. We found that by applying rules based on the contextual information of the scene, we improved the perceived realism of the crowd formations when compared to random formations. We also examined the effect of camera viewpoint on the plausibility of virtual pedestrian scenes, and we found that an eye-level viewpoint is more effective for disguising random behaviors, while a canonical viewpoint results in these behaviors being perceived as less realistic than an isometric or top-down viewpoint. Results from these studies can help in the creation of virtual crowds, such as computer graphics pedestrian models or architectural scenes, and identify situations when users' perception is less accurate.© 2011 ACM.",Perception; Virtual crowd formation,Behavioral research; Cameras; Architectural scenes; Contextual information; Pedestrian models; Perception; Perceptual effects; Random behavior; Rules based; Still images; Topdown; Users' perception; Virtual crowd formation; Virtual crowds; Virtual reality
Perceptual organization of affective and sensorial expressive intentions in music performance,2010,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77949345736&doi=10.1145%2f%2f1670671.1670678&partnerID=40&md5=5caa5744bb9876d27127e5cbdf7e2e4c,"Expression communication is the added value of a musical performance. It is part of the reason why music is interesting to listen to and sounds alive. Previous work on the analysis of acoustical features yielded relevant features for the recognition of different expressive intentions, inspired both by emotional and sensorial adjectives. In this article, machine learning techniques are employed to understand how expressive performances represented by the selected features are clustered on a low-dimensional space, and to define a measure of acoustical similarity. Being that expressive intentions are similar according to the features used for the recognition, and since recognition implies subjective evaluation, we hypothesized that performances are similar also from a perceptual point of view. We then compared and integrated the clustering of acoustical features with the results of two listening experiments. A first experiment aims at verifying whether subjects can distinguish different categories of expressive intentions, and a second experiment aims at understanding which expressions are perceptually clustered together in order to derive common evaluation criteria adopted by listeners, and to obtain the perceptual organization of affective and sensorial expressive intentions. An interpretation of the resulting spatial representation based on action is proposed and discussed. © 2010 ACM.",Expression; Music performance,Learning systems; Expression; Expression communication; Expressive performance; Low-dimensional spaces; Machine learning techniques; Music performance; Perceptual organization; Spatial representations; Experiments
User-based evaluation of data-driven haptic rendering,2010,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78649245685&doi=10.1145%2f1857893.1857900&partnerID=40&md5=d15a149debeae7365081e3bcf1ccc774,"In this article, the data-driven haptic rendering approach presented in our earlier work is assessed. The approach relies on recordings from real objects from which a data-driven model is derived that captures the haptic properties of the object. We conducted two studies. In the first study, the Just Noticeable Difference (JND) for small forces, as encountered in our set-up, was determined. JNDs were obtained both for active and passive user interaction. A conservative threshold curve was derived that was then used to guide the model generation in the second study. The second study examined the achievable rendering fidelity for two objects with different stiffnesses. Subjects directly compared data-driven virtual feedback with the real objects. Results indicated that it is crucial to include dynamic material effects to achieve haptic feedback that cannot be distinguished from real objects. Results also showed that the fidelity is considerably decreased for stiffer objects due to limits of the display hardware. © 2010 ACM.",Deformable models; Discrimination study; Force discrimination; Haptic rendering,Data-driven; Data-driven model; Deformable models; Discrimination study; Display hardware; Dynamic materials; Force discrimination; Haptic feedbacks; Haptic rendering; Just-noticeable difference; Model generation; Real objects; User interaction; Virtual feedback; Deformation
An empirical pipeline to derive gaze prediction heuristics for 3D action games,2010,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78649302726&doi=10.1145%2f1857893.1857897&partnerID=40&md5=2e9db826d27ef8a4d78e11eb190e7b1d,"Gaze analysis and prediction in interactive virtual environments, such as games, is a challenging topic since the 3D perspective and variations of the viewpoint as well as the current task introduce many variables that affect the distribution of gaze. In this article, we present a novel pipeline to study eye-tracking data acquired from interactive 3D applications. The result of the pipeline is an importance map which scores the amount of gaze spent on each object. This importance map is then used as a heuristic to predict a user's visual attention according to the object properties present at runtime. The novelty of this approach is that the analysis is performed in object space and the importance map is defined in the feature space of high-level properties. High-level properties are used to encode task relevance and other attributes, such as eccentricity, which may have an impact on gaze behavior. The pipeline has been tested with an exemplary study on a first-person shooter game. In particular, a protocol is presented describing the data acquisition procedure, the learning of different importance maps from the data, and finally an evaluation of the performance of the derived gaze predictors. A metric measuring the degree of correlation between attention predicted by the importance map and the actual gaze yielded clearly positive results. The correlation becomes particularly strong when the player is attentive to an in-game task. © 2010 ACM.",Eye-tracking; Gaze analysis; Gaze predictor; High-level properties; Importance map; Video games; Virtual environments; Visual attention,Forecasting; Pipelines; Three dimensional; Virtual reality; Eye-tracking; Gaze analysis; Gaze predictor; High-level properties; Importance map; Video game; Virtual environments; Visual Attention; Human computer interaction
Evaluation of force and torque magnitude discrimination thresholds on the human hand-arm system,2010,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78649338581&doi=10.1145%2f1857893.1857894&partnerID=40&md5=523ef679f9c18bdab40f244d9e4c2238,"This article reports on experiments about haptic perception aimed at measuring the force/torque differential thresholds applied to the hand-arm system. The experimental work analyzes how force is sent back to the user by means of a 6 degrees-of-freedom haptic device. Our findings on force perception indicate that the just-noticeable-difference is generally higher than previously reported in the literature and not constant along the stimulus continuum. We found evidence that the thresholds change also among the different directions. Furthermore, asymmetries in force perceptions, which were not described in previous reports, can be evinced for most of the directions. These findings support our claim that human beings perceive forces differently along different directions, thus suggesting that perception can also be enhanced by suitable signal processing, that is, with a manipulation of the force signal before it reaches the haptic device. We think that the improvement of the user perception can have a great impact in many applications and in particular we are focusing on surgical teleoperation scenarios. © 2010 ACM.",Force thresholds; Perceptual asymmetries; Perceptual-based signal processing,Differential threshold; Discrimination thresholds; Experimental works; Force perception; Force signal; Force thresholds; Hand-arm system; Haptic devices; Haptic perception; Human being; Human hands; Perceptual asymmetries; Perceptual-based signal processing; Tele-operations; User perceptions; Signal processing
Perceptually guided high-fidelity rendering exploiting movement bias in visual attention,2010,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78649286955&doi=10.1145%2f1857893.1857899&partnerID=40&md5=e55ce2c9eea6b0a4ae64ad60267de456,"A major obstacle for real-time rendering of high-fidelity graphics is computational complexity. A key point to consider in the pursuit of ""realism in real time"" in computer graphics is that the Human Visual System (HVS) is a fundamental part of the rendering pipeline. The human eye is only capable of sensing image detail in a 2° foveal region, relying on rapid eye movements, or saccades, to jump between points of interest. These points of interest are prioritized based on the saliency of the objects in the scene or the task the user is performing. Such ""glimpses"" of a scene are then assembled by the HVS into a coherent, but inevitably imperfect, visual perception of the environment. In this process, much detail, that the HVS deems unimportant, may literally go unnoticed. Visual science research has identified that movement in the background of a scene may substantially influence how subjects perceive foreground objects. Furthermore, recent computer graphics work has shown that both fixed viewpoint and dynamic scenes can be selectively rendered without any perceptual loss of quality, in a significantly reduced time, by exploiting knowledge of any high-saliency movement that may be present. A high-saliency movement can be generated in a scene if an otherwise static objects starts moving. In this article, we investigate, through psychophysical experiments, including eye-tracking, the perception of rendering quality in dynamic complex scenes based on the introduction of a moving object in a scene. Two types of object movement are investigated: (i) rotation in place and (ii) rotation combined with translation. These were chosen as the simplest movement types. Future studies may include movement with varied acceleration. The object's geometry and location in the scene are not salient. We then use this information to guide our high-fidelity selective renderer to produce perceptually high- quality images at significantly reduced computation times. We also show how these results can have important implications for virtual environment and computer games applications. © 2010 ACM.",Attention; Movement saliency; Perception; Saliency map; Selective rendering,Color photography; Computational complexity; Computer applications; Image processing; Rotation; Virtual reality; Attention; Movement saliency; Perception; Saliency map; Selective rendering; Eye movements
A supervised combination strategy for illumination chromaticity estimation,2010,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78649260498&doi=10.1145%2f1857893.1857898&partnerID=40&md5=06bc3a49f7a095b0ee148d5213c1c6a3,"Color constancy is an important perceptual ability of humans to recover the color of objects invariant of light information. It is also necessary for a robust machine vision system. Until now, a number of color constancy algorithms have been proposed in the literature. In particular, the edge-based color constancy uses the edge of an image to estimate light color. It is shown to be a rich framework that can represent many existing illumination estimation solutions with various parameter settings. However, color constancy is an ill-posed problem; every algorithm is always given out under some assumptions and can only produce the best performance when these assumptions are satisfied. In this article, we have investigated a combination strategy relying on the Extreme Learning Machine (ELM) technique that integrates the output of edge-based color constancy with multiple parameters. Experiments on real image data sets show that the proposed method works better than most single-color constancy methods and even some current state-of-the-art color constancy combination strategies. © 2010 ACM.",Color constancy; Combination strategy; Extreme learning machine; Illumination estimation,Color; Computer vision; Estimation; Learning systems; Color constancy; Combination strategies; Edge-based; Extreme learning machine; Ill posed problem; Illumination estimation; Light color; Machine vision systems; Multiple parameters; Parameter setting; Real image data; Color computer graphics
Proxemics with multiple dynamic characters in an immersive virtual environment,2010,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78649259650&doi=10.1145%2f1857893.1857896&partnerID=40&md5=247f5cda6d9e42f541ff09eb8fe1e141,"An experiment was carried out to examine the impact on electrodermal activity of people when approached by groups of one or four virtual characters at varying distances. It was premised on the basis of proxemics theory that the closer the approach of the virtual characters to the participant, the greater the level of physiological arousal. Physiological arousal was measured by the number of skin conductance responses within a short time period after the approach, and the maximum change in skin conductance level 5 s after the approach. The virtual characters were each either female or a cylinder of human size, and one or four characters approached each subject a total of 12 times. Twelve male subjects were recruited for the experiment. The results suggest that the number of skin conductance responses after the approach and the change in skin conductance level increased the closer the virtual characters approached toward the participants. Moreover, these response variables were inversely correlated with the number of visits, showing a typical adaptation effect. There was some evidence to suggest that the number of characters who simultaneously approached (one or four) was positively associated with the responses. Surprisingly there was no evidence of a difference in response between the humanoid characters and cylinders on the basis of this physiological data. It is suggested that the similarity in this quantitative arousal response to virtual characters and virtual objects might mask a profound difference in qualitative response, an interpretation supported by questionnaire and interview results. Overall the experiment supported the premise that people exhibit heightened physiological arousal the closer they are approached by virtual characters. © 2010 ACM.",Avatars; Human-computer interaction; Proxemics; Virtual characters,Cylinders (shapes); Experiments; Knowledge management; Physiological models; Physiology; Virtual reality; Avatars; Dynamic character; Electrodermal activity; Human-size; Immersive virtual environments; Level 5; Physiological data; Proxemics; Qualitative response; Skin conductance; Time-periods; Virtual character; Virtual objects; Human computer interaction
Investigating the performance of path-searching tasks in depth on multiview displays,2010,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78649232333&doi=10.1145%2f1857893.1857901&partnerID=40&md5=0200debf50ac80fd7927c3c37e38b88a,"Multiview auto-stereoscopic displays support both stereopsis and head motion parallax depth cues and could be superior for certain tasks. Previous work suggests that a high viewpoint density (100 views/10cm at the eye) is required to convincingly support motion parallax. However, it remains unclear how viewpoint density affects task performance, and this factor is critical in determining display and system design requirements. Therefore, we present a simulated multiview display apparatus to undertake experiments using a path-searching task in which we control two independent variables: the stereoscopic depth and the viewpoint density. In the first experiment, we varied both cues and found that even small amounts of stereo depth (2cm) reliably improved task accuracy and reduced latency, whereas there was no evidence of dependence on viewpoint density. In the second experiment, we switched off the stereoscopic cue and varied viewpoint density alone. We found that for these monoscopic images increasing viewpoint density resulted in some reduction in response latency (up to eight views/10cm) but had no effect on accuracy. We conclude for cases where occlusion is not an overriding factor that low viewpoint densities may be sufficient to enable effective path-searching task performance. © 2010 ACM.",Motion parallax; Multiview displays; Stereopsis; Viewpoint density,Experiments; Geometrical optics; Auto-stereoscopic display; Depth cue; Head motion; Independent variables; Monoscopic; Motion parallax; Multi-views; Multiview displays; Searching task; Stereopsis; Support motion; System design; Task performance; Viewpoint density; Astrophysics
Quantifying fidelity for virtual environment simulations employing memory schema assumptions,2010,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78649243439&doi=10.1145%2f1857893.1857895&partnerID=40&md5=bcf431fa6cc77637e935fb3fb719eeae,"In a virtual environment (VE), efficient techniques are often needed to economize on rendering computation without compromising the information transmitted. The reported experiments devise a functional fidelity metric by exploiting research on memory schemata. According to the proposed measure, similar information would be transmitted across synthetic and real-world scenes depicting a specific schema. This would ultimately indicate which areas in a VE could be rendered in lower quality without affecting information uptake. We examine whether computationally more expensive scenes of greater visual fidelity affect memory performance after exposure to immersive VEs, or whether they are merely more aesthetically pleasing than their diminished visual quality counterparts. Results indicate that memory schemata function in VEs similar to real-world environments. ""High- level"" visual cognition related to late visual processing is unaffected by ubiquitous graphics manipulations such as polygon count and depth of shadow rendering; ""normal"" cognition operates as long as the scenes look acceptably realistic. However, when the overall realism of the scene is greatly reduced, such as in wireframe, then visual cognition becomes abnormal. Effects that distinguish schema-consistent from schema-inconsistent objects change because the whole scene now looks incongruent. We have shown that this effect is not due to a failure of basic recognition. © 2010 ACM.",Computer graphics; Human-computer interaction; Visual cognition,Knowledge management; Virtual reality; Immersive; Memory performance; Polygon count; Real world environments; Real-world; Virtual environments; Visual Cognition; Visual fidelity; Visual qualities; Visual-processing; Human computer interaction
How does presentation method and measurement protocol affect distance estimation in real and virtual environments?,2010,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77955214779&doi=10.1145%2f1823738.1823744&partnerID=40&md5=1f8463c923a55f3f2c421b272453dc0e,"We conducted two experiments that compared distance perception in real and virtual environments in six visual presentation methods using either timed imagined walking or direct blindfolded walking, while controlling for several other factors that could potentially impact distance perception. Our presentation conditions included unencumbered real world, real world seen through an HMD, virtual world seen through an HMD, augmented reality seen through an HMD, virtual world seen on multiple, large immersive screens, and photo-based presentation of the real world seen on multiple, large immersive screens. We found that there was a similar degree of underestimation of distance in the HMD and large-screen presentations of virtual environments. We also found that while wearing the HMD can cause some degree of distance underestimation, this effect depends on the measurement protocol used. Finally, we found that photo-based presentation did not help to improve distance perception in a large-screen immersive display system. The discussion focuses on points of similarity and difference with previous work on distance estimation in real and virtual environments. © 2010 ACM 1544-3558/2010/07-ART26 $10.00.",Distance estimation; Egocentric depth perception; Head-mounted displays; Large-screen immersive displays; Perception; Virtual environments,Augmented reality; Depth perception; Estimation; Virtual reality; Distance estimation; Distance perception; Head mounted displays; Immersive; Large-screen immersive displays; Measurement protocol; Similar degree; Virtual environments; Virtual worlds; Visual presentation; Helmet mounted displays
Visually Significant Edges,2010,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77955200642&doi=10.1145%2f1823738.1823745&partnerID=40&md5=4a3391b4ce78be683e0e4a99aca27c01,"Numerous image processing and computer graphics methods make use of either explicitly computed strength of image edges, or an implicit edge strength definition that is integrated into their algorithms. In both cases, the end result is highly affected by the computation of edge strength. We address several shortcomings of the widely used gradient magnitude-based edge strength model through the computation of a hypothetical Human Visual System (HVS) response at edge locations. Contrary to gradient magnitude, the resulting ""visual significance"" values account for various HVS mechanisms such as luminance adaptation and visual masking, and are scaled in perceptually linear units that are uniform across images. The visual significance computation is implemented in a fast multiscale second-generation wavelet framework which we use to demonstrate the differences in image retargeting, HDR image stitching, and tone mapping applications with respect to the gradient magnitude model. Our results suggest that simple perceptual models provide qualitative improvements on applications utilizing edge strength at the cost of a modest computational burden. © 2010 ACM 1544-3558/2010/07-ART27 $10.00.",Edge strength; HDR; Visual perception,Computer graphics; Image processing; Vision; Wavelet transforms; Computational burden; Edge location; Gradient magnitude; HDR image; Human visual systems; Image edge; Image retargeting; Multiscales; Perceptual model; Second generation wavelet; Strength models; Tone mapping; Visual masking; Visual perception; Edge detection
Perceptually motivated guidelines for voice synchronization in film,2010,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77955210600&doi=10.1145%2f1823738.1823741&partnerID=40&md5=fb17d8de4d985f21685d22da360fb414,"We consume video content in a multitude of ways, including in movie theaters, on television, on DVDs and Blu-rays, online, on smart phones, and on portable media players. For quality control purposes, it is important to have a uniform viewing experience across these various platforms. In this work, we focus on voice synchronization, an aspect of video quality that is strongly affected by current post-production and transmission practices. We examined the synchronization of an actor's voice and lip movements in two distinct scenarios. First, we simulated the temporal mismatch between the audio and video tracks that can occur during dubbing or during broadcast. Next, we recreated the pitch changes that result from conversions between formats with different frame rates. We show, for the first time, that these audio visual mismatches affect viewer enjoyment. When temporal synchronization is noticeably absent, there is a decrease in the perceived performance quality and the perceived emotional intensity of a performance. For pitch changes, we find that higher pitch voices are not preferred, especially for male actors. Based on our findings, we advise that mismatched audio and video signals negatively affect viewer experience. © 2010 ACM 1544-3558/2010/07-ART23 $10.00.",Auditory perceptual research; Human perception and performance; Multisensory perception and integration; Visual psychophysics,Customer satisfaction; Motion picture theaters; Physics; Psychophysiology; Total quality management; Visual communication; Human perception; Multisensory; Multisensory perception and integration; Visual psychophysics; Synchronization
Pointing in pictorial space: Quantifying the perceived relative depth structure in mono and stereo images of natural scenes,2010,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77955190237&doi=10.1145%2f1823738.1823742&partnerID=40&md5=adfc1663fe2b3b76c70ba632c6dde6b4,"Although there has recently been a large increase in commercial 3D applications, relatively little is known about the quantitative perceptual improvement from binocular disparity. In this study we developed a method to measure the perceived relative depth structure of natural scenes. Observers were instructed to adjust the direction of a virtual pointer from one object to another. The pointing data was used to reconstruct the relative logarithmic depths of the objects in pictorial space. The results showed that the relative depth structure is more similar between observers for stereo images than for mono images in two out of three scenes. A similar result was found for the depth range: for the same two scenes the stereo images were perceived as having more depth than the monocular images. In addition, our method allowed us to determine the subjective center of projection. We found that the pointing settings fitted the reconstructed depth best for substantially wider fields of view than the veridical center of projection for both mono and stereo images. The results indicate that the improvement from binocular disparity depends on the scene content: scenes with sufficient monocular information may not profit much from binocular disparity. copy; 2010 ACM 1544-3558/2010/07-ART24 $10.00.",Binocular disparity; Depth perception; Natural scenes,Binoculars; Depth perception; Profitability; 3D application; Binocular disparity; Center of projections; Depth range; Depth structure; Fields of views; Monocular image; Natural scenes; Pictorial space; Stereo-image; Optical instruments
Analysis of disparity distortions in omnistereoscopic displays,2010,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77955193137&doi=10.1145%2f1823738.1823743&partnerID=40&md5=bdfc9eb120483257965028c326ed5d2d,"An omnistereoscopic image is a pair of panoramic images that enables stereoscopic depth perception all around an observer. An omnistereo projection on a cylindrical display does not require tracking of the observer's viewing direction. However, such a display introduces stereo distortions. In this article, we investigate two projection models for rendering 3D scenes in omnistereo. The first is designed to give zero disparity errors at the center of the visual field. The second is the well-known slit-camera model. For both models, disparity errors are shown to increase gradually in the periphery, as visual stereo acuity decreases. We use available data on human stereoscopic acuity limits to argue that depth distortions caused by these models are so small that they cannot be perceived. © 2010 ACM 1544-3558/2010/07-ART25 $10.00.",Depth acuity; Median plane; Panorama; Perception; Stereo,Depth perception; Errors; Sensory perception; 3D scenes; Camera model; Depth acuity; Panoramic images; Projection models; Stereoscopic depth perception; Visual fields; Three dimensional
The saliency of anomalies in animated human characters,2010,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77955193661&doi=10.1145%2f1823738.1823740&partnerID=40&md5=f1d09671bf66355e4f8607eff5072676,"Virtual characters are much in demand for animated movies, games, and other applications. Rapid advances in performance capture and advanced rendering techniques have allowed the movie industry in particular to create characters that appear very human-like. However, with these new capabilities has come the realization that such characters are yet not quite ""right."" One possible hypothesis is that these virtual humans fall into an ""Uncanny Valley"", where the viewer's emotional response is repulsion or rejection, rather than the empathy or emotional engagement that their creators had hoped for. To explore these issues, we created three animated vignettes of an arguing couple with detailed motion for the face, eyes, hair, and body. In a set of perceptual experiments, we explore the relative importance of different anomalies using two different methods: a questionnaire to determine the emotional response to the full-length vignettes, with and without facial motion and audio; and a 2AFC (two alternative forced choice) task to compare the performance of a virtual ""actor"" in short clips (extracts from the vignettes) depicting a range of different facial and body anomalies. We found that the facial anomalies are particularly salient, even when very significant body animation anomalies are present. © 2010 ACM 1544-3558/2010/07-ART22 $10.00.",Eye tracking; Human animation; Motion capture; Perception of human motion; Virtual characters,Animation; Eye-tracking; Human animation; Human motions; Motion capture; Virtual character; Virtual reality
Editorial - Apgv 2010 special issue,2010,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77955208847&doi=10.1145%2f1823738.1823739&partnerID=40&md5=eaa1dbebe517f3a8a0ca7faa798db44d,[No abstract available],,
Pilot gaze and glideslope control,2010,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77954007464&doi=10.1145%2f1773965.1773969&partnerID=40&md5=0eca89485efb0f4dd6bbd6c90bf7c447,"We examined the eye movements of pilots as they carried out simulated aircraft landings under day and night lighting conditions. Our five students and five certified pilots were instructed to quickly achieve and then maintain a constant 3-degree glideslope relative to the runway. However, both groups of pilots were found to make significant glideslope control errors, especially during simulated night approaches. We found that pilot gaze was directed most often toward the runway and to the ground region located immediately in front of the runway, compared to other visual scene features. In general, their gaze was skewed toward the near half of the runway and tended to follow the runway threshold as it moved on the screen. Contrary to expectations, pilot gaze was not consistently directed at the aircraft's simulated aimpoint (i.e., its predicted future touchdown point based on scene motion). However, pilots did tend to fly the aircraft so that this point was aligned with the runway threshold. We conclude that the supplementary out-of-cockpit visual cues available during day landing conditions facilitated glideslope control performance. The available evidence suggests that these supplementary visual cues are acquired through peripheral vision, without the need for active fixation. © 2010 ACM.",,Aircraft landing; Eye movements; Aimpoint; Control errors; Control performance; Glideslope; Landing conditions; Lighting conditions; Peripheral vision; Runway threshold; Touchdown points; Visual cues; Visual scene; Aircraft
Neural modeling of flow rendering effectiveness,2010,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77953977332&doi=10.1145%2f1773965.1773971&partnerID=40&md5=df9bc4a4196c4707a8fbd399473aa101,"It has been previously proposed that understanding the mechanisms of contour perception can provide a theory for why some flow rendering methods allow for better judgments of advection pathways than others. In this article, we develop this theory through a numerical model of the primary visual cortex of the brain (Visual Area 1) where contour enhancement is understood to occur according to most neurological theories. We apply a two-stage model of contour perception to various visual representations of flow fields evaluated using the advection task of Laidlaw et al. In the first stage, contour enhancement is modeled based on Li's cortical model. In the second stage, a model of streamline tracing is proposed, designed to support the advection task. We examine the predictive power of the model by comparing its performance to that of human subjects on the advection task with four different visualizations. The results show the same overall pattern for humans and the model. In both cases, the best performance was obtained with an aligned streamline based method, which tied with a LIC-based method. Using a regular or jittered grid of arrows produced worse results. The model yields insights into the relative strengths of different flow visualization methods for the task of visualizing advection pathways. © 2010 ACM.",,Flow visualization; Contour perception; Cortical models; Human subjects; Model yields; Neural modeling; Numerical models; Predictive power; Primary visual cortex; Relative strength; Rendering methods; Streamline tracing; Two stage model; Visual areas; Visual representations; Advection
Writing with music: Exploring the use of auditory feedback in gesture interfaces,2010,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77954090550&doi=10.1145%2f1773965.1773968&partnerID=40&md5=53d11aaab77c9d973ee23b6b46fb80cf,"We investigate the use of auditory feedback in pen-gesture interfaces in a series of informal and formal experiments. Initial iterative exploration showed that gaining performance advantage with auditory feedback was possible using absolute cues and state feedback after the gesture was produced and recognized. However, gaining learning or performance advantage from auditory feedback tightly coupled with the pen-gesture articulation and recognition process was more difficult. To establish a systematic baseline, Experiment 1 formally evaluated gesture production accuracy as a function of auditory and visual feedback. Size of gestures and the aperture of the closed gestures were influenced by the visual or auditory feedback, while other measures such as shape distance and directional difference were not, supporting the theory that feedback is too slow to strongly influence the production of pen stroke gestures. Experiment 2 focused on the subjective aspects of auditory feedback in pen-gesture interfaces. Participants' rating on the dimensions of being wonderful and stimulating was significantly higher with musical auditory feedback. Several lessons regarding pen gestures and auditory feedback are drawn from our exploration: a few simple functions such as indicating the pen-gesture recognition results can be achieved, gaining performance and learning advantage through tightly coupled process-based auditory feedback is difficult, pen-gesture sets and their recognizers can be designed to minimize visual dependence, and people's subjective experience of gesture interaction can be influenced using musical auditory feedback. These lessons may serve as references and stepping stones toward future research and development in pen-gesture interfaces with auditory feedback. © 2010 ACM.",,Experiments; Feedback; Gesture recognition; Visual communication; Wearable computers; Auditory feedback; Formal Experiments; Gesture interaction; Gesture interfaces; Pen gesture; Recognition process; Research and development; Shape distances; Stepping stone; Subjective aspects; Tightly-coupled; Visual feedback; State feedback
Identifying the role of Proprioception in upper-limb prosthesis control: Studies on targeted motion,2010,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77953966212&doi=10.1145%2f1773965.1773966&partnerID=40&md5=4650a17997f76f92bb62de3b35ffd90d,"Proprioception plays a crucial role in enabling humans to move purposively and interact with their physical surroundings. Current technology in upper-limb prostheses, while beginning to incorporate some haptic feedback, does not provide amputees with proprioceptive information about the state of the limb. Thus, the wearer must visually monitor the limb, which is often inconvenient or even impossible for some tasks. This work seeks to quantify the potential benefits of incorporating proprioceptive motion feedback into upper-limb prosthesis designs. We apply a noninvasive method for controlling the availability of proprioceptive motion feedback in unimpaired individuals in a human subject study to compare the benefits of visual and proprioceptive motion feedback in targeted motion tasks. Combined results of the current study and our previous study using a different task indicate that the addition of proprioceptive motion feedback improves targeting accuracy under nonsighted conditions and, for some tasks, under sighted conditions as well. This work motivates the development of methods for providing artificial proprioceptive feedback to a prosthesis wearer. © 2010 ACM.",,Biological organs; Feedback; State feedback; Current technology; Haptic feedbacks; Human subjects; Motion tasks; Noninvasive methods; Potential benefits; Proprioception; Targeting accuracy; Upper limbs; Prosthetics
Evaluating the multivariate visual quality performance of image-processing components,2010,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77954002833&doi=10.1145%2f1773965.1773967&partnerID=40&md5=ecea9842a20bd00d6b9759b7b21a9d23,"The estimation of image quality is a demanding task, especially when estimating different high-quality imaging products or their components. The challenge is the multivariate nature of image quality as well as the need to use naïve observers as test subjects, since they are the actual end-users of the products. Here, we use a subjective approach suitable for estimating the quality performance of different imaging device components with naïve observers-the interpretation-based quality (IBQ) approach. From two studies with 61 naïve observers, 17 natural image contents, and 13 different camera image signal processor pipelines, we determined the subjectively crucial image quality attributes and dimensions and the description of each pipeline's perceived image quality performance. We found that the subjectively most important image quality dimensions were color shift/naturalness, darkness, and sharpness. The first dimension, which was related to naturalness and colors, distinguished the good-quality pipelines from the middle- and low-quality groups, and the dimensions of darkness and sharpness described why the quality failed in the low-quality pipelines. The study suggests that the high-level concept naturalness is a requirement for high-quality images, whereas quality can fail for other reasons in low-quality images, and this failure can be described by low-level concepts, such as darkness and sharpness. © 2010 ACM.",,Estimation; Pipeline processing systems; Pipelines; Signal processing; Camera images; End-users; High quality images; High-quality imaging; Imaging device; Low qualities; Natural images; Quality performance; Visual qualities; Image quality
Evaluating 2D and 3D visualizations of spatiotemporal information,2010,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77953974104&doi=10.1145%2f1773965.1773970&partnerID=40&md5=2a7606b3d1c9bc5f06ab8d118a1564c8,"Time-varying geospatial data presents some specific challenges for visualization. Here, we report the results of three experiments aiming at evaluating the relative efficiency of three existing visualization techniques for a class of such data. The class chosen was that of object movement, especially the movements of vehicles in a fictitious landscape. Two different tasks were also chosen. One was to predict where three vehicles will meet in the future given a visualization of their past movement history. The second task was to estimate the order in which four vehicles arrived at a specific place. Our results reveal that previous findings had generalized human perception in these situations and that large differences in user efficiency exist for a given task between different types of visualizations depicting the same data. Furthermore, our results are in line with earlier general findings on the nature of human perception of both object shape and scene changes. Finally, the need for new taxonomies of data and tasks based on results from perception research is discussed. © 2010 ACM.",,Data visualization; Efficiency; Three dimensional computer graphics; Vehicles; 2D and 3D visualization; Geo-spatial data; Human perception; Movement history; Object movements; Relative efficiency; Spatiotemporal information; Time varying; User efficiencies; Visualization technique; Visualization
Differences in head orientation behavior for speakers and listeners: An experiment in a virtual environment,2010,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77955598945&doi=10.1145%2f1658349.1658351&partnerID=40&md5=c76e35415f1328fab5ef2ae4dfd193df,"An experiment was conducted to investigate whether human observers use knowledge of the differences in focus of attention in multiparty interaction to identify the speaker amongst the meeting participants. A virtual environment was used to have good stimulus control. Head orientations were displayed as the only cue for focus attention. The orientations were derived from a corpus of tracked head movements. We present some properties of the relation between head orientations and speaker-listener status, as found in the corpus. With respect to the experiment, it appears that people use knowledge of the patterns in focus of attention to distinguish the speaker from the listeners. However, the human speaker identification results were rather low. Head orientations (or focus of attention) alone do not provide a sufficient cue for reliable identification of the speaker in a multiparty setting. © 2010 ACM.",Focus of attention; Gaze behavior; Head orientation; Multiparty conversation; Perception of gaze; Virtual environments,Experiments; Focus of Attention; Gaze behavior; Head orientation; Multi-party conversations; Perception of gaze; Virtual environments; Virtual reality
Simulating believable forward accelerations on a stewart motion platform,2010,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77955637956&doi=10.1145%2f1658349.1658354&partnerID=40&md5=78d96ea191d8e263afafa2e35f133bfa,"It is still an unsolved problem how to optimally simulate self-motion using motion simulators. We investigated how a forward acceleration can be simulated as believably as possible on a hexapod motion platform equipped with a projection screen. Human participants rated the believability of brief forward accelerations. These were simulated as visual forward accelerations over a ground plane with people as size cues, presented together with brief forward surge translations and backward pitches of the platform, and synchronous random up-down movements of the camera in the visual scene and the platform. The magnitudes of all of the parameters were varied independently across trials. Even though variability between participants was high, the most believable simulation occurred when visual accelerations were combined with backward pitches of the platform, which changed the gravitoinertial vector direction approximately consistent with the visual acceleration. However, a wide range of platform pitches was accepted as believable.With high visual acceleration cues most participants reported trials as realistic even when the platform tilt rate was above vestibular canal thresholds reported in other works. Other manipulated parameters had only a mild influence on the responses. These results can be used to optimize motion-cueing algorithms for simulating linear accelerations in motion simulators. © 2010 ACM.",Linear vection; Multisensory integration; Self-motion perception; Simulator design; Vestibular; Virtual reality,Projection screens; Simulators; Virtual reality; Vision; Multisensory integration; Self-motion perception; Simulator design; Vection; Vestibular; Acceleration
Multidimensional scaling analysis of haptic exploratory procedures,2010,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77955608548&doi=10.1145%2f1658349.1658356&partnerID=40&md5=328c3d5c76b13b85793aef5afc6b8dc6,"Previous work in real and virtual settings has shown that the way in which we interact with objects plays a fundamental role in the way we perceive them. This article uses multidimensional scaling (MDS) analysis to further characterize and quantify the effects of using different haptic exploratory procedures (EPs) on perceptual similarity spaces. In Experiment 1, 20 participants rated similarity on a set of nine novel, 3D objects varying in shape and texture after either following their contours, laterally rubbing their centers, gripping them, or sequentially touching their tips.MDSanalysiswas used to recover perceptual maps of the objects and relative weights of perceptual dimensions from similarity data. Both the maps and relative weights of shape/texture properties were found to vary as a function of the EP used. In addition, large individual differences in the relative weight of shape/texture were observed. In Experiment 2, 17 of the previous participants repeated Experiment 1 after an average of 105 days. The same patterns of raw similarity ratings, perceptual maps, dimension weights, and individual differences were observed, indicating that perceptual similarities remained stable over time. The findings underscore the role of hand movements and individual biases in shaping haptic perceptual similarity. A framework for validating multimodal virtual displays based on the approach used in the study is also presented. © 2010 ACM.",Exploratory procedure; Haptic; Multidimensional scaling; Shape; Similarity; Texture,Experiments; Textures; Exploratory procedure; Haptic; Multi-dimensional scaling; Shape; Similarity; Three dimensional
Mesh saliency and human eye fixations,2010,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77956256957&doi=10.1145%2f1670671.1670676&partnerID=40&md5=ec429f9c1c62c1b988ab16026c390a2f,"Mesh saliency has been proposed as a computational model of perceptual importance for meshes, and it has been used in graphics for abstraction, simplification, segmentation, illumination, rendering, and illustration. Even though this technique is inspired by models of low-level human vision, it has not yet been validated with respect to human performance. Here, we present a user study that compares the previous mesh saliency approaches with human eye movements. To quantify the correlation between mesh saliency and fixation locations for 3D rendered images, we introduce the normalized chance-adjusted saliency by improving the previous chance-adjusted saliency measure. Our results show that the current computational model of mesh saliency can model human eye movements significantly better than a purely random model or a curvature-based model. © 2010 ACM.",Eye-tracker; Mesh saliency; Visual perception,Computation theory; Computational methods; Eye tracking; Image enhancement; Mesh generation; Rendering (computer graphics); Computational model; Eye trackers; Human performance; Mesh saliencies; Random Model; Rendered images; Saliency measure; Visual perception; Eye movements
Bimodal perception of audio-visual material properties for virtual environments,2010,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77955615235&doi=10.1145%2f1658349.1658350&partnerID=40&md5=ff40bbbcac2e1f98a39b45a3d9f337ed,"High-quality rendering of both audio and visual material properties is very important in interactive virtual environments, since convincingly rendered materials increase realism and the sense of immersion. We studied how the level of detail of auditory and visual stimuli interact in the perception of audio-visual material rendering quality. Our study is based on perception of material discrimination, when varying the levels of detail of modal synthesis for sound, and bidirectional reflectance distribution functions for graphics. We performed an experiment for two different models (a Dragon and a Bunny model) and two material types (plastic and gold). The results show a significant interaction between auditory and visual level of detail in the perception of material similarity, when comparing approximate levels of detail to a high-quality audio-visual reference rendering. We show how this result can contribute to significant savings in computation time in an interactive audio-visual rendering system. To our knowledge, this is the first study that shows interaction of audio and graphics representation in a material perception task. © 2010 ACM.",Audio-visual rendering; Bimodal perception; Crossmodal perception; Material perception,Knowledge representation; Materials properties; Modal analysis; Virtual reality; Audio-visual; Audio-visual material; Bidirectional reflectance distribution functions; Bimodal perception; Computation time; Cross-modal; High quality; High-quality audio; Interactive virtual environments; Level of detail; Levels of detail; Material discrimination; Material property; Modal synthesis; Rendering quality; Rendering system; Two-materials; Virtual environments; Visual levels; Visual stimulus; Distribution functions
Computational visual attention systems and their cognitive foundations: A survey,2010,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77955637257&doi=10.1145%2f1658349.1658355&partnerID=40&md5=afd8b159b17f4338c2592dff97df1b7f,"Based on concepts of the human visual system, computational visual attention systems aim to detect regions of interest in images. Psychologists, neurobiologists, and computer scientists have investigated visual attention thoroughly during the last decades and profited considerably from each other. However, the interdisciplinarity of the topic holds not only benefits but also difficulties: Concepts of other fields are usually hard to access due to differences in vocabulary and lack of knowledge of the relevant literature. This article aims to bridge this gap and bring together concepts and ideas from the different research areas. It provides an extensive survey of the grounding psychological and biological research on visual attention as well as the current state of the art of computational systems. Furthermore, it presents a broad range of applications of computational attention systems in fields like computer vision, cognitive systems, and mobile robotics. We conclude with a discussion on the limitations and open questions in the field. © 2010 ACM.",Biologically motivated computer vision; Regions of interest; Robot vision; Saliency; Visual attention,Computer vision; Robots; Surveys; Biologically motivated computer vision; Regions of interest; Robot vision; Saliency; Visual Attention; Cognitive systems
Cognitive transfer of spatial awareness states from immersive virtual environments to reality,2010,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872409581&doi=10.1145%2f1670671.1670673&partnerID=40&md5=98491225a5534b5c6b72286852e4c6e7,"An individual's prior experience will influence how new visual information in a scene is perceived and remembered. Accuracy of memory performance per se is an imperfect reflection of the cognitive activity (awareness states) that underlies performance in memory tasks. The aim of this research is to investigate the effect of varied visual fidelity of training environments on the transfer of training to the real world after exposure to immersive simulations representing a real-world scene. A between groups experiment was carried out to explore the effect of rendering quality on measurements of location-based recognition memory for objects and associated states of awareness. The immersive simulation consisted of one room that was either rendered flat-shaded or using radiosity rendering. The simulation was displayed on a stereo head-tracked head mounted display. Post exposure to the synthetic simulation, participants completed a memory recognition task conducted in a real-world scene by physically arranging objects in their physical form in a real-world room. Participants also reported one of four states of awareness following object recognition. They were given several options of awareness states that reflected the level of visual mental imagery involved during retrieval, the familiarity of the recollection and related guesses. The scene incorporated objects that ""fitted"" into the specific context of the real-world scene, referred to as consistent objects, and objects that were not related to the specific context of the real-world scene, referred to as inconsistent objects. A follow-up studywas conducted a week after the initial test. Interestingly, results revealed a higher proportion of correct object recognition associated with mental imagery when participants were exposed to low-fidelity flat-shaded training scenes rather than the radiosity rendered ones. Memory psychology indicates that awareness states based on visual imagery require stronger attentional processing in the first instance than those based on familiarity. A tentative claim would, therefore, be that those immersive environments that are distinctive because of their variation from ""real,"" such as flat-shaded environments, recruit stronger attentional resources. This additional attentional processing may bring about a change in participants' subjective experiences of ""remembering"" when they later transfer the training from that environment into a real-world situation. © 2010 ACM.",Human-computer interaction; Perceptual graphics,Helmet mounted displays; Human computer interaction; Object recognition; Stereo image processing; Virtual reality; Head mounted displays; Immersive environment; Immersive virtual environments; Perceptual graphics; Real world situations; Subjective experiences; Synthetic simulation; Transfer of trainings; Rendering (computer graphics)
Spatial learning and navigation using: A virtual verbal display,2010,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77955593968&doi=10.1145%2f1658349.1658352&partnerID=40&md5=354e131570aee0c2e72323e38ab3164f,"We report on three experiments that investigate the efficacy of a new type of interface called a virtual verbal display (VVD) for nonvisual learning and navigation of virtual environments (VEs). Although verbal information has been studied for routeguidance, little is known about the use of context-sensitive, speech-based displays (e.g., the VVD) for supporting free exploration and wayfinding behavior. During training, participants used the VVD (Experiments I and II) or a visual display (Experiment III) to search the VEs and find four hidden target locations. At test, all participants performed a route-finding task in the corresponding real environment, navigating with vision (Experiments I and III) or from verbal descriptions (Experiment II). Training performance between virtual display modes was comparable, but wayfinding in the real environment was worse after VVD learning than visual learning, regardless of the testing modality. Our results support the efficacy of the VVD for searching computer-based environments but indicate a difference in the cognitive maps built up between verbal and visual learning, perhaps due to lack of physical movement in the VVD. © 2010 ACM.",Human-computer interaction; Navigation; Verbal learning; Virtual environments; Virtual verbal display; Wayfinding,Experiments; Human computer interaction; Knowledge management; Navigation; Cognitive maps; Context-sensitive; Physical movements; Real environments; Searching computers; Spatial learning; Target location; Verbal information; Verbal learning; Virtual displays; Virtual environments; Virtual verbal display; Visual display; Visual learning; Way-finding; Virtual reality
Comparing lighting quality evaluations of real scenes with those from high dynamic range and conventional images,2010,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886993418&doi=10.1145%2f1670671.1670677&partnerID=40&md5=72bbf3a1d02299e7d1fc7772678149cd,"Thirty-nine participants viewed six interior scenes in an office/laboratory building and rated them for brightness, uniformity, pleasantness, and glare. The scenes were viewed in three presentation modes: participants saw the real space and images of the spaces on a 17-inch computer monitor in both conventional and high dynamic range (HDR) mode. HDR mode allowed the high range of luminances in the real scene to be accurately reproduced, with maximum luminances more than 10 times higher than those in the conventional images. For those participants who saw the images before the real spaces (the most relevant order for practical applications), the HDR images were rated as significantly more realistic than the conventional images. However, this effect was limited to scenes with relatively large areas of high luminance, which in this study was represented by scenes with windows and daylight. Ratings of the HDR images were significantly related to simple photometric descriptors of the images in the expected manner: Brightness and glare ratings were positively correlated with overall and elevated luminance, and nonuniformity ratings were positively correlated with luminance variability. These results suggest that for evaluations of visual appearance of interior scenes featuring large areas of high luminance, the HDR method may be used as a surrogate for experiencing a real space both for lighting quality research, and in the design process. © 2010 ACM.",High dynamic range; Lighting; Luminance; Virtual reality,Glare; Lighting; Quality control; Virtual reality; Design process; High dynamic range; High luminances; Lighting quality; Luminance variability; Maximum luminance; Presentation modes; Visual appearance; Luminance
Volume composition and evaluation using eye-tracking data,2010,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77955630828&doi=10.1145%2f1658349.1658353&partnerID=40&md5=b556d94b2cb7c4aab7bd9c97e3b7e2df,"This article presents a method for automating rendering parameter selection to simplify tedious user interaction and improve the usability of visualization systems. Our approach acquires the important/interesting regions of a dataset through simple user interaction with an eye tracker. Based on this importance information, we automatically compute reasonable rendering parameters using a set of heuristic rules, which are adapted from visualization experience and psychophysical experiments. A user study has been conducted to evaluate these rendering parameters, and while the parameter selections for a specific visualization result are subjective, our approach provides good preliminary results for general users while allowing additional control adjustment. Furthermore, our system improves the interactivity of a visualization system by significantly reducing the required amount of parameter selections and providing good initial rendering parameters for newly acquired datasets of similar types. © 2010 ACM.",Eye tracker; Illustrative visualization; Interaction; Usability and human factors in visualization; Volume rendering,Color photography; Eye controlled devices; Human engineering; Volume rendering; Additional control; Data sets; Eye trackers; Eye-tracking; Heuristic rules; Human factors; Illustrative visualization; Interaction; Interactivity; Parameter selection; Psychophysical experiments; User interaction; User study; Visualization system; Visualization
"Impressionism, expressionism, surrealism: Automated recognition of painters and schools of art",2010,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857744855&doi=10.1145%2f1670671.1670672&partnerID=40&md5=15bf8e80201664314c692a5f2fe0b7af,"We describe a method for automated recognition of painters and schools of art based on their signature styles and studied the computer-based perception of visual art. Paintings of nine artists, representing three different schools of art - impressionism, surrealism and abstract expressionism - were analyzed using a large set of image features and image transforms. The computed image descriptors were assessed using Fisher scores, and the most informative features were used for the classification and similarity measurements of paintings, painters, and schools of art. Experimental results show that the classification accuracy when classifying paintings into nine painter classes is 77%, and the accuracy of associating a given painting with its school of art is 91%. An interesting feature of the proposed method is its ability to automatically associate different artists that share the same school of art in an unsupervised fashion. The source code used for the image classification and image similarity described in this article is available for free download. © 2010 ACM.",Art; Image similarity; Painting; Perceptual reasoning,Abstracting; Image analysis; Image classification; Painting; Automated recognition; Classification accuracy; Image descriptors; Image features; Image similarity; Image transforms; Perceptual reasoning; Similarity measurements; Arts computing
Search task performance using subtle gaze direction with the presence of distractions,2009,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-70349084187&doi=10.1145%2f1577755.1577760&partnerID=40&md5=71bb2b307427204efd3828ef668d6754,"A new experiment is presented that demonstrates the usefulness of an image space modulation technique called subtle gaze direction (SGD) for guiding the user in a simple searching task. SGD uses image space modulations in the luminance channel to guide a viewer's gaze about a scene without interrupting their visual experience. The goal of SGD is to direct a viewer's gaze to certain regions of a scene without introducing noticeable changes in the image. Using a simple searching task, we compared performance using no modulation, using subtle modulation, and using obvious modulation. Results from the experiments show improved performance when using subtle gaze direction, without affecting the user's perception of the image. We then extend the experiment to evaluate performance with the presence of distractors. The distractors took the form of extra modulations, which do not correspond to a target in the image. Experimentation shows, that, even in the presence of distractors, more accurate results are returned on a simple search task using SGD, as compared to results returned when no modulation at all is used. Results establish the potential of the method for a wide range of applications including gaming, perceptually based rendering, navigation in virtual environments, and medical search tasks. © 2009 ACM.",Eye tracking; Gaze direction; Image manipulation; Luminance; Psychophysics,Modulation; Physics; Psychophysiology; Virtual reality; Eye tracking; Gaze direction; Image manipulation; Image space; Modulation techniques; Navigation in virtual en-vironments; Psychophysics; Search tasks; Searching task; Experiments
Investigating the role of body shape on the perception of emotion,2009,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-70349102533&doi=10.1145%2f1577755.1577757&partnerID=40&md5=620d56cce4a28b0cfb09a783fc55a8f9,"In order to analyze the emotional content of motions portrayed by different characters, we created real and virtual replicas of an actor exhibiting six basic emotions: sadness, happiness, surprise, fear, anger, and disgust. In addition to the video of the real actor, his actions were applied to five virtual body shapes: a low- and high-resolution virtual counterpart, a cartoon-like character, a wooden mannequin, and a zombie-like character (Figures 1 and 2). In a point light condition, we also tested whether the absence of a body affected the perceived emotion of the movements. Participants were asked to rate the actions based on a list of 41 more complex emotions. We found that the perception of emotional actions is highly robust and to the most part independent of the character's body, so long as form is present. When motion alone is present, emotions were generally perceived as less intense than in the cases where form was present. © 2009 ACM.",Graphics; Motion capture; Perception,Basic emotions; Body shapes; Graphics; High resolution; Light conditions; Motion capture; Perception; Behavioral research
On uniform resampling and gaze analysis of bidirectional texture functions,2009,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-70349246632&doi=10.1145%2f1577755.1577761&partnerID=40&md5=8b4deb6547473e0ad2fc687c1ffac3eb,"The use of illumination and view-dependent texture information is recently the best way to capture the appearance of real-world materials accurately. One example is the Bidirectional Texture Function. The main disadvantage of these data is their massive size. In this article, we employ perceptually-based methods to allow more efficient handling of these data. In the first step, we analyse different uniform resampling by means of a psychophysical study with 11 subjects, comparing original data with rendering of a uniformly resampled version over the hemisphere of illumination and view-dependent textural measurements. We have found that down-sampling in view and illumination azimuthal angles is less apparent than in elevation angles and that illumination directions can be down-sampled more than view directions without loss of visual accuracy. In the second step, we analyzed subjects gaze fixation during the experiment. The gaze analysis confirmed results from the experiment and revealed that subjects were fixating at locations aligned with direction of main gradient in rendered stimuli. As this gradient was mostly aligned with illumination gradient, we conclude that subjects were observing materials mainly in direction of illumination gradient. Our results provide interesting insights in human perception of real materials and show promising consequences for development of more efficient compression and rendering algorithms using these kind of massive data. © 2009 ACM.",BTF; Eye tracking; Phychophysical experiment; Texture compression; Uniform resampling; Visual degradation,Data handling; Degradation; Textures; BTF; Eye tracking; Phychophysical experiment; Texture compression; Uniform resampling; Visual degradation; Experiments
Transactions on Applied Perception: Guest editorial,2009,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-70349143171&doi=10.1145%2f1577755.1577756&partnerID=40&md5=db12ad5a4aa202c077749b717496dc90,[No abstract available],,
Handling occluders in transitions from panoramic images: A perceptual study,2009,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-70350092993&doi=10.1145%2f1609967.1609972&partnerID=40&md5=28c10baefc8928bb5cc9cd32b7db993d,"Panoramic images are very effective at conveying a visual sense of presence at very low cost and great ease of authoring. They are, however, limited in the navigation options they offer, unlike 3D representations. It is therefore desirable to provide pleasing transitions from one panorama to another, or from a panorama to a 3D model. We focus on motions where the viewers move toward an area of interest, and on the problem of dealing with occluders in their path. We discuss existing transition approaches, with emphasis on the additional information they require and on the constraints they place on the authoring process. We propose a compromise approach based on faking the parallax effect with occluder mattes. We conduct a user study to determine whether additional information does in fact increase the visual appeal of transitions. We observe that the creation of occluder mattes alone is only justified if the fake parallax effect can be synchronized with the camera motion (but not necessarily consistent with it), and if viewpoint discrepancies at occlusion boundaries are small. The faster the transition, the less perceptual value there is in creating mattes. Information on view alignment is always useful, as a dissolve effect is always preferred to fading to black and back. © 2009 ACM.",Content mixing; Occlusion; Panorama; Transitioning; User study,Astrophysics; Geometrical optics; Smelting; Content mixing; Occlusion; Panorama; Transitioning; User study; Three dimensional
HMD calibration and its effects on distance judgments,2009,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-70349103388&doi=10.1145%2f1577755.1577762&partnerID=40&md5=683c48b7211ba6894529d290d6f64ccf,"Most head-mounted displays (HMDs) suffer from substantial optical distortion, and vendor-supplied specifications for field-of-view often are at variance with reality. Unless corrected, such displays do not present perspective-related visual cues in a geometrically correct manner. Distorted geometry has the potential to affect applications of HMDs, which depend on precise spatial perception. This article provides empirical evidence for the degree to which common geometric distortions affect one type of spatial judgment in virtual environments. We show that minification or magnification in the HMD that would occur from misstated HMD field of view causes significant changes in distance judgments. Incorrectly calibrated pitch and pincushion distortion, however, do not cause statistically significant changes in distance judgments for the degree of distortions examined. While the means for determining the optical distortion of display systems are well known, they are often not used in non-see-through HMDs due to problems in measuring and correcting for distortion. As a result, we also provide practical guidelines for creating geometrically calibrated systems. © 2009 ACM.",Field of view; Immersive virtual environment; Minification; Perception; Pincushion distortion; Pitch,Virtual reality; Field of view; Immersive virtual environment; Minification; Perception; Pincushion distortion; Pitch; Helmet mounted displays
FixTag: An algorithm for identifying and tagging fixations to simplify the analysis of data collected by portable eye trackers,2009,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-70349106419&doi=10.1145%2f1577755.1577759&partnerID=40&md5=ee74cdc6dc416b5cf9ae8590319e6d53,"Video-based eye trackers produce an output video showing where a subject is looking, the subject's Point-of-Regard (POR), for each frame of a video of the scene. This information can be extremely valuable, but its analysis can be overwhelming. Analysis of eye-tracked data from portable (wearable) eye trackers is especially daunting, as the scene video may be constantly changing, rendering automatic analysis more difficult. A common way to begin analysis of POR data is to group these data into fixations. In a previous article, we compared the fixations identified (i.e., start and end marked) automatically by an algorithm to those identified manually by users (i.e., manual coders). Here, we extend this automatic identification of fixations to tagging each fixation to a Region-of-Interest (ROI). Our fixation tagging algorithm, FixTag, requires the relative 3D positions of the vertices of ROIs and calibration of the scene camera. Fixation tagging is performed by first calculating the camera projection matrices for keyframes of the scene video (captured by the eye tracker) via an iterative structure and motion recovery algorithm. These matrices are then used to project 3D ROI vertices into the keyframes. A POR for each fixation is matched to a point in the closest keyframe, which is then checked against the 2D projected ROI vertices for tagging. Our fixation tags were compared to those produced by three manual coders tagging the automatically identified fixations for two different scenarios. For each scenario, eight ROIs were defined along with the 3D positions of eight calibration points. Therefore, 17 tags were available for each fixation: 8 for ROIs, 8 for calibration points, and 1 for other. For the first scenario, a subject was tracked looking through products on four store shelves, resulting in 182 automatically identified fixations. Our automatic tagging algorithm produced tags that matched those produced by at least one manual coder for 181 out of the 182 fixations (99.5% agreement). For the second scenario, a subject was tracked looking at two posters on adjoining walls of a room. Our algorithm matched at least one manual coder's tag for 169 fixations out of 172 automatically identified (98.3% agreement). © 2009 ACM.",Coding; Eye tracking; Fixations; Portable; Wearable,Automation; Calibration; Cameras; Electronic data interchange; Eye controlled devices; Three dimensional; Coding; Eye tracking; Fixations; Portable; Wearable; Algorithms
ACM Transactions on Applied Perception: Guest editorial,2009,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-70350088056&doi=10.1145%2f1609967.1609968&partnerID=40&md5=8eec0d15b8721c333142f7cffe6e093a,[No abstract available],,
Talking bodies: Sensitivity to desynchronization of conversations,2009,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-70350114084&doi=10.1145%2f1609967.1609969&partnerID=40&md5=00348a865778f944aa6b3b409c0c6d44,"In this article, we investigate human sensitivity to the coordination and timing of conversational body language for virtual characters. First, we captured the full body motions (excluding faces and hands) of three actors conversing about a range of topics, in either a polite (i.e., one person talking at a time) or debate/argument style. Stimuli were then created by applying the motion-captured conversations from the actors to virtual characters. In a 2AFC experiment, participants viewed paired sequences of synchronized and desynchronized conversations and were asked to guess which was the real one. Detection performance was above chance for both conversation styles but more so for the polite conversations, where desynchronization was more noticeable. © 2009 ACM.",Graphics; Motion capture; Perception,Acoustic fields; Body language; Desynchronization; Detection performance; Full-body motions; Graphics; Human sensitivity; Motion capture; Perception; Virtual character; Virtual reality
Perceptual influence of approximate visibility in indirect illumination,2009,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-70350114082&doi=10.1145%2f1609967.1609971&partnerID=40&md5=e94abf12acd8dd739f6a7efdb6a9ffbb,"In this article we evaluate the use of approximate visibility for efficient global illumination. Traditionally, accurate visibility is used in light transport. However, the indirect illumination we perceive on a daily basis is rarely of high-frequency nature, as the most significant aspect of light transport in real-world scenes is diffuse, and thus displays a smooth gradation. This raises the question of whether accurate visibility is perceptually necessary in this case. To answer this question, we conduct a psychophysical study on the perceptual influence of approximate visibility on indirect illumination. This study reveals that accurate visibility is not required and that certain approximations may be introduced. © 2009 ACM.",Global illumination; Perception; Visibility,Global illumination; High frequency HF; Indirect illumination; Light transport; Perception; Psychophysical studies; Real-world; Visibility
Perception of differences in natural-image stimuli: Why is peripheral viewing poorer than foveal,2009,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-70350094227&doi=10.1145%2f1609967.1609973&partnerID=40&md5=598b4d1c0194ef3bfd374aa96989d32d,"Visual Difference Predictor (VDP) models have played a key role in digital image applications such as the development of image quality metrics. However, little attention has been paid to their applicability to peripheral vision. Central (i.e., foveal) vision is extremely sensitive for the contrast detection of simple stimuli such as sinusoidal gratings, but peripheral vision is less sensitive. Furthermore, crowding is a well-documented phenomenon whereby differences in suprathreshold peripherally viewed target objects (such as individual letters or patches of sinusoidal grating) become more difficult to discriminate when surrounded by other objects (flankers). We examine three factors that might influence the degree of crowding with natural-scene stimuli (cropped from photographs of natural scenes): (1) location in the visual field, (2) distance between target and flankers, and (3) flanker-target similarity. We ask how these factors affect crowding in a suprathreshold discrimination experiment where observers rate the perceived differences between two sequentially presented target patches of natural images. The targets might differ in the shape, size, arrangement, or color of items in the scenes. Changes in uncrowded peripheral targets are perceived to be less than for the same changes viewed foveally. Consistent with previous research on simple stimuli, we find that crowding in the periphery (but not in the fovea) reduces the magnitudes of perceived changes even further, especially when the flankers are closer and more similar to the target. We have tested VDP models based on the response behavior of neurons in visual cortex and the inhibitory interactions between them. The models do not explain the lower ratings for peripherally viewed changes even when the lower peripheral contrast sensitivity was accounted for; nor could they explain the effects of crowding, which others have suggested might arise from errors in the spatial localization of features in the peripheral image. This suggests that conventional VDP models do not port well to peripheral vision. © 2009 ACM.",Crowding; Image difference metrics; Peripheral vision; Psychophysical testing; VDP models,Image quality; Photography; Targets; Vision; Crowding; Image difference metrics; Peripheral vision; Psychophysical testing; VDP models; Aberrations
Effect of scenario on perceptual sensitivity to errors in animation,2009,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-70349108823&doi=10.1145%2f1577755.1577758&partnerID=40&md5=8c0b832b02a6040141dcb9be2f00939d,"A deeper understanding of what makes animation perceptually plausible would benefit a number of applications, such as approximate collision detection and goal-directed animation. In a series of psychophysical experiments, we examine how measurements of perceptual sensitivity in realistic physical simulations compare to similar measurements done in more abstract settings. We find that participant tolerance for certain types of errors is significantly higher in a realistic snooker scenario than in the abstract test settings previously used to examine those errors. By contrast, we find tolerance for errors displayed in realistic but more neutral environments was not different from tolerance for those errors in abstract settings. Additionally, we examine the interaction of auditory and visual cues in determining participant sensitivity to spatiotemporal errors in rigid body collisions. We find that participants are predominantly affected by visual cues. Finally, we find that tolerance for spatial gaps during collision events is constant for a wide range of viewing angles if the effect of foreshortening and occlusion caused by the viewing angle is taken into account. © 2009 ACM.",Animation; Graphics; Perception; Psychophysics,Abstracting; Animation; Color photography; Physics; Psychophysiology; Collision detection; Collision events; Graphics; Neutral environment; Perception; Physical simulation; Psychophysical experiments; Psychophysics; Rigid body collision; Viewing angle; Visual cues; Errors
Screen-space perceptual rendering of human skin,2009,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-70350094229&doi=10.1145%2f1609967.1609970&partnerID=40&md5=8d6fdb50332098976900fba224a5ee54,"We propose a novel skin shader which translates the simulation of subsurface scattering from texture space to a screen-space diffusion approximation. It naturally scales well while maintaining a perceptually plausible result. This technique allows us to ensure real-time performance even when several characters may appear on screen at the same time. The visual realism of the resulting images is validated using a subjective psychophysical preference experiment. Our results show that, independent of distance and light position, the images rendered using our novel shader have as high visual realism as a previously developed physically-based shader. © 2009 ACM.",Perception; Psychophysics; Real-time skin rendering,Physics; Psychophysiology; Diffusion approximations; Human skin; Perception; Perceptual rendering; Psychophysical; Psychophysics; Real time performance; Real-time skin rendering; Subsurface scattering; Texture space; Visual realism; Skin
Auditory self-motion simulation is facilitated by haptic and vibrational cues suggesting the possibility of actual motion,2009,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-70349114266&doi=10.1145%2f1577755.1577763&partnerID=40&md5=5b7e090b7ec862ca4c5d50e00ee4ef4f,"Sound fields rotating around stationary blindfolded listeners sometimes elicit auditory circular vection, the illusion that the listener is physically rotating. Experiment 1 investigated whether auditory circular vection depends on participants' situational awareness of movability, that is, whether they sense/know that actual motion is possible or not. While previous studies often seated participants on movable chairs to suspend the disbelief of self-motion, it has never been investigated whether this does, in fact, facilitate auditory vection. To this end, 23 blindfolded participants were seated on a hammock chair with their feet either on solid ground (movement impossible) or suspended (movement possible) while listening to individualized binaural recordings of two sound sources rotating synchronously at 60°/s. Although participants never physically moved, situational awareness of movability facilitated auditory vection. Moreover, adding slight vibrations like the ones resulting from actual chair rotation increased the frequency and intensity of vection. Experiment 2 extended these findings and showed that nonindividualized binaural recordings were as effective in inducing auditory circular vection as individualized recordings. These results have important implications both for our theoretical understanding of self-motion perception and for the applied field of self-motion simulations, where vibrations, nonindividualized binaural sound, and the cognitive/perceptual framework of movability can typically be provided at minimal cost and effort. © 2009 ACM.",Auditory vection; Circular vection; Cue-integration; Higher-level/cognitive influences; HRTF; Human factors; Individualized binaural recordings; Psychophysics; Self-motion illusions; Self-motion simulation; Spatial sound; Vibrations; Virtual reality,Acoustic fields; Experiments; Human engineering; Physics; Psychophysiology; Rotation; Auditory vection; Circular vection; Cue-integration; Higher-level/cognitive influences; HRTF; Human factors; Individualized binaural recordings; Psychophysics; Self-motion illusions; Self-motion simulation; Spatial sound; Vibrations; Virtual reality
Classifying pretended and evoked facial expressions of positive and negative affective states using infrared measurement of skin temperature,2009,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-62149123619&doi=10.1145%2f1462055.1462061&partnerID=40&md5=fbe34e1b9f17d7dec9d036f5f4b66817,"Earlier researchers were able to extract the transient facial thermal features from thermal infrared images (TIRIs) to make binary distinctions between the expressions of affective states. However, effective human-computer interaction would require machines to distinguish between the subtle facial expressions of affective states. This work, for the first time, attempts to use the transient facial thermal features for recognizing a much wider range of facial expressions. A database of 324 time-sequential, visible-spectrum, and thermal facial images was developed representing different facial expressions from 23 participants in different situations. A novel facial thermal feature extraction, selection, and classification approach was developed and invoked on various Gaussian mixture models constructed using: neutral and pretended happy and sad faces, faces with multiple positive and negative facial expressions, faces with neutral and six (pretended) basic facial expressions, and faces with evoked happiness, sadness, disgust, and anger. This work demonstrates that (1) infrared imaging can be used to observe the affective-state-specific facial thermal variations, (2) pixel-grey level analysis of TIRIs can help localise significant facial thermal feature points along the major facial muscles, and (3) cluster-analytic classification of transient thermal features can help distinguish between the facial expressions of affective states in an optimized eigenspace of input thermal feature vectors. The observed classification results exhibited influence of a Gaussian mixture model's structure on classifier-performance. The work also unveiled some pertinent aspects of future research on the use of facial thermal features in automated facial expression classification and affect recognition. © 2009 ACM.",Affective computing and thermal infrared imaging; Facial expression classification; Physiology-based automated affect recognition,Automation; Blind source separation; Communication channels (information theory); Feature extraction; Human computer interaction; Imaging systems; Infrared devices; Knowledge management; Object recognition; Thermography (imaging); Trellis codes; Affective computing and thermal infrared imaging; Affective state; Classification approaches; Classification results; Eigenspace; Facial expression classification; Facial expressions; Facial images; Facial muscles; Facial thermal feature points; Gaussian mixture models; Grey-level analysis; Human-computer interactions; Infrared measurements; Skin temperatures; Thermal features; Thermal infrared images; Thermal variations; Face recognition
On spatiochromatic visual sensitivity and peripheral color LOD management,2009,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-63049113212&doi=10.1145%2f1498700.1498703&partnerID=40&md5=05334cdc9da4d10c6cae762e12e7ca7f,"Empirical findings from a gaze-contingent color degradation study report the effects of artificial reduction of the human visual system's sensitivity to peripheral chromaticity on visual search performance. To our knowledge, this is the first such investigation of peripheral color reduction. For unimpeded performance, results suggest that, unlike spatiotemporal content, peripheral chromaticity cannot be reduced within the central 20° visual angle. Somewhat analogous to dark adaptation, reduction of peripheral color tends to simulate scotopic viewing conditions. This holds significant implications for chromatic Level Of Detail management. Specifically, while peripheral spatiotemporal detail can be attenuated without affecting visual search, often dramatically (e.g., spatial detail can be so reduced up to 50% at about 5°), peripheral chromatic reduction is likely to be noticed much sooner. Therefore, color LOD reduction (e.g., via compression), should be maintained isotropically across the central 20° visual field. © 2009 ACM.",Gaze-contingent displays,Color; Visual communication; Color degradations; Color reductions; Dark adaptations; Empirical findings; Gaze-contingent displays; Human visual systems; Level of detail managements; Viewing conditions; Visual angles; Visual fields; Visual searches; Visual sensitivities; Aberrations
The spatial resolution of crossmodal attention: Implications for the design of multimodal interfaces,2009,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-62149097515&doi=10.1145%2f1462055.1462059&partnerID=40&md5=887f939fcfbeb36dbf933d272ae14d35,"Previous research on crossmodal attentional orienting has reported speeded reaction times (RT) when the stimuli from the different modalities are in the same spatial location and slowed RTs when the stimuli are presented in very different locations (e.g., opposite sides of the body). However, little is known about what occurs for spatial interactions between these two extremes. We systematically varied the separation between cues and targets to quantify the spatial distribution of crossmodal attention. The orthogonal cueing paradigm [Spence et al. 1998]was used. Visual targets presented above or below the forearm were preceded by either vibrotactile cues presented on the forearm, auditory cues presented below the forearm, or visual cues presented on the forearm. The presentation of both unimodal and crossmodal cues led to a roughly monotonic increase in RT as a function of the cue-target separation. Unimodal visual cueing resulted in an attentional focus that was significantly narrower than that produced by crossmodal cues: the distribution of visual attention for visual cues had roughly half of the lateral extent of that produced by tactile cueing and roughly one fourth of the lateral extent as that produced by auditory cueing. This occurred when both seven (Experiment 1) and three (Experiment 2) cue locations were used suggesting that the effects are not primarily due to differences in the ability to localize the cues. These findings suggest that the location of tactile and auditory warning signals does not have to be controlled as precisely as the location of visual warning signals to facilitate a response to the critical visual event. © 2009 ACM.",Attention; Warnings,Security of data; Size distribution; Targets; Attention; Auditory cues; Auditory warning signals; Cross modals; Crossmodal attentions; Multi-modal interfaces; Reaction time; Spatial distributions; Spatial interactions; Spatial locations; Spatial resolutions; Unimodal; Vibrotactile; Visual attentions; Visual cues; Visual targets; Visual warnings; Warnings; Location
Real-world vision: Selective perception and task,2009,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-63049124724&doi=10.1145%2f1498700.1498705&partnerID=40&md5=d5016908503902a27fe89197bee49fa7,"Visual perception is an inherently selective process. To understand when and why a particular region of a scene is selected, it is imperative to observe and describe the eye movements of individuals as they go about performing specific tasks. In this sense, vision is an active process that integrates scene properties with specific, goal-oriented oculomotor behavior. This study is an investigation of how task influences the visual selection of stimuli from a scene. Four eye tracking experiments were designed and conducted to determine how everyday tasks affect oculomotor behavior. A portable eyetracker was created for the specific purpose of bringing the experiments out of the laboratory and into the real world, where natural behavior is most likely to occur. The experiments provide evidence that the human visual system is not a passive collector of salient environemental stimuli, nor is vision general-purpose. Rather, vision is active and specific, tightly coupled to the requirements of a task and a plan of action. The experiments support the hypothesis that the purpose of selective attention is to maximize task efficiency by fixating relevant objects in the scene. A computational model of visual attention is presented that imposes a high-level constraint on the bottom-up salient properties of a scene for the purpose of locating regions that are likely to correspond to foreground objects rather than background or other salient nonobject stimuli. In addition to improving the correlation to human subject fixation densities over a strictly bottom-up model [Itti et al. 1998; Parkhurst et al. 2002], this model predicts a central fixation tendency when that tendency is warranted, and not as an artificially primed location bias. © 2009 ACM.",Active vision; Eye-tracking; Saliency modeling,Experiments; Feature extraction; Location; Active process; Active vision; Central fixations; Computational models; Eye-tracking; Foreground objects; Goal-oriented; Human subjects; Human visual systems; Real-world; Saliency modeling; Selective attentions; Selective perceptions; Selective process; Specific tasks; Tightly-coupled; Visual attentions; Visual perceptions; Visual selections; Eye movements
A unified information-theoretic framework for viewpoint selection and mesh saliency,2009,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-62149119524&doi=10.1145%2f1462055.1462056&partnerID=40&md5=d92993bb7342a8bd52ae99a9362c774f,"Viewpoint selection is an emerging area in computer graphics with applications in fields such as scene exploration, image-based modeling, and volume visualization. In particular, best view selection algorithms are used to obtain the minimum number of views (or images) in order to understand or model an object or scene better. In this article, we present a unified framework for viewpoint selection and mesh saliency based on the definition of an information channel between a set of viewpoints (input) and the set of polygons of an object (output). The mutual information of this channel is shown to be a powerful tool to deal with viewpoint selection, viewpoint stability, object exploration and viewpoint-based saliency. In addition, viewpoint mutual information is extended using saliency as an importance factor, showing how perceptual criteria can be incorporated to our method. Although we use a sphere of viewpoints around an object, our framework is also valid for any set of viewpoints in a closed scene. A number of experiments demonstrate the robustness of our approach and the good behavior of the proposed measures. © 2009 ACM.",Information theory; Mesh saliency; Viewpoint selection; Visual perception,Computer graphics; Data warehouses; Information theory; Vision; Image-based modeling; Importance factors; In fields; Information channels; Mesh saliency; Mutual informations; Object explorations; Unified frameworks; View selections; Viewpoint selection; Visual perception; Volume visualizations; Feature extraction
Toward a definition of visual complexity as an implicit measure of cognitive load,2009,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-63049083404&doi=10.1145%2f1498700.1498704&partnerID=40&md5=998cad8a46a3d725080c5b5ce845c19a,"The visual complexity of Web pages is much talked about; ""complex Web pages are difficult to use,"" but often regarded as a subjective decision by the user. This subjective decision is of limited use if we wish to understand the importance of visual complexity, what it means, and how it can be used. We theorize that by understanding a user's visual perception of Web page complexity, we can understand the cognitive effort required for interaction with that page. This is important because by using an easily identifiable measure, such as visual complexity, as an implicit marker of cognitive load, we can design Web pages which are easier to interact with. We have devised an initial empirical experiment, using card sorting and triadic elicitation, to test our theories and assumptions, and have built an initial baseline sequence of 20 Web pages along with a library of qualitative and anecdotal feedback. Using this library, we define visual complexity, ergo perceived interaction complexity, and by taking these pages as ""prototypes"" and ranking them into a sequence of complexity, we are able to group them into: simple, neutral, and complex. This means we can now work toward a definition of visual complexity as an implicit measure of cognitive load. © 2009 ACM.",Knowledge elicitation; Semantic Web; Visual complexity; Visual impairment; Web accessibility,Information theory; Semantic Web; Semantics; World Wide Web; Can designs; Card-sorting; Cognitive loads; Implicit measures; Knowledge elicitation; Visual complexity; Visual impairment; Visual perceptions; Web accessibility; Web pages; Image coding
Negative efficacy of fixed gain error reducing shared control for training in virtual environments,2009,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-62149093888&doi=10.1145%2f1462055.1462058&partnerID=40&md5=d563e42d05747b80feaba076c907ea19,"Virtual reality with haptic feedback provides a safe and versatile practice medium for many manual control tasks. Haptic guidance has been shown to improve performance of manual control tasks in virtual environments; however, the efficacy of haptic guidance for training in virtual environments has not been studied conclusively. This article presents experimental results that show negative efficacy of haptic guidance during training in virtual environments. The haptic guidance in this study is a fixed-gain error-reducing shared controller, with the control effort overlaid on the dynamics of the manual control task during training. Performance of the target-hitting manual control task in the absence of guidance is compared for three training protocols. One protocol contained no haptic guidance and represented virtual practice. Two protocols utilized haptic guidance, varying the duration of exposure to guidance during the training sessions. Exposure to the fixed-gain error-reducing shared controller had a detrimental effect on performance of the target-hitting task at the conclusion of a month-long training protocol, regardless of duration of exposure. While the shared controller was designed with knowledge of the task and an intuitive sense of the motions required to achieve good performance, the results indicate that the acquisition of motor skill is a complex phenomenon that is not aided with haptic guidance during training as implemented in this experiment. © 2009 ACM.",Haptic assistance; Manual control; Motor skill training; Shared control; Virtual training,Controllers; Knowledge acquisition; Motors; Virtual reality; Control efforts; Detrimental effects; Gain errors; Haptic assistance; Haptic feedbacks; Haptic guidances; Motor skill training; Shared control; Training sessions; Virtual environments; Virtual training; Manual control
A psychophysical investigation of global illumination algorithms used in augmented reality,2009,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-62149115959&doi=10.1145%2f1462055.1462057&partnerID=40&md5=00b6096a0baa9fe284c442c2bde5a095,"The overarching goal of this research was to compare different rendering solutions in order to understand why some yield better results specifically when applied to rendering synthetic objects into real photographs. A psychophysical experiment was conducted in which the composite images were judged for accuracy against the original photograph. In addition, iCAM, an image color appearance model was also used to calculate image differences for the same set of images. Conclusions obtained included the effect of global illumination on the accuracy of the final composite rendering. Also, it was discovered that the original rendering with all of its artifacts is not necessarily an indicator of the final composite image's judged accuracy. Finally, initial results show promise in using iCAM to predict a relationship similar to the psychophysics, which could eventually be used in-the-rendering-loop to achieve photorealism with minimized computation. © 2009 ACM.",Global illumination; Image difference; Perception; Psychophysics; Rendering,Augmented reality; Color photography; Physics; Virtual reality; Global illumination; Image difference; Perception; Psychophysics; Rendering; Psychophysiology
Perception of image motion during head movement,2009,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-62149102924&doi=10.1145%2f1462055.1462060&partnerID=40&md5=3fced6bd0f4ff07cdf15d65d0ec5179e,"We examined human perception of head-referenced image motion during concurrent head movement. The visual stimulus was a checkerboard image in a head mounted display that moved from side-to-side. Observers rated the magnitude of the checkerboard motion while either rotating their head about a vertical axis (yaw), about a horizontal axis (pitch), or holding it still. In Experiment 1, we tested four image oscillation frequencies (0.25, 0.5, 1, and 2 Hz) while holding the head motion frequency constant at 0.5 Hz. In Experiment 2, we tested three head motion frequencies (0.25, 0.5, and 1 Hz) while holding the image oscillation frequency constant at 1 Hz. Across all image and head motion frequencies, perceptual sensitivity to image motion was reduced by about 45% during horizontal head movement. During vertical head movement, perceptual sensitivity was reduced by about 25% when head and image motion were of the same frequency. Compared with when the head was still, horizontal and vertical head movements produced a downward shift of about 10% in overall motion magnitude estimation response. Findings from this study provide virtual environment developers with a quantitative description of the influence of concurrent head movement on the perception of frontoparallel image motion. © 2009 ACM.",Head movement; Image motion; Motion perception; Object motion; VE latency,Virtual reality; Head movement; Image motion; Motion perception; Object motion; VE latency; Vision
The effects of head-mounted display mechanical properties and field of view on distance judgments in virtual environments,2009,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-63049089489&doi=10.1145%2f1498700.1498702&partnerID=40&md5=3362dfdf382ca813677e85cbe3be4cb0,"Research has shown that people are able to judge distances accurately in full-cue, real-world environments using visually directed actions. However, in virtual environments viewed with head-mounted display (HMD) systems, there is evidence that people act as though the virtual space is smaller than intended. This is a surprising result given how well people act in real environments. The behavior in the virtual setting may be linked to distortions in the available visual cues or to a person's ability to locomote without vision. Either could result from issues related to added mass, moments of inertia, and restricted field of view in HMDs. This article describes an experiment in which distance judgments based on normal real-world and HMD viewing are compared with judgments based on real-world viewing while wearing two specialized devices. One is a mock HMD, which replicated the mass, moments of inertia, and field of view of the HMD and the other an inertial headband designed to replicate the mass and moments of inertia of the HMD, but constructed to not restrict the field of view of the observer or otherwise feel like wearing a helmet. Distance judgments using the mock HMD showed a statistically significant underestimation relative to the no restriction condition but not of a magnitude sufficient to account for all the distance compression seen in the HMD. Indicated distances with the inertial headband were not significantly smaller than those made with no restrictions. © 2009 ACM.",Distance judgments; Head-mounted displays; Perception,Mechanical properties; Virtual reality; Added mass; Distance judgments; Field of views; Head-mounted displays; Moments of inertias; Perception; Real environments; Real-world; Real-world environments; Restriction conditions; Virtual environments; Virtual spaces; Visual cues; Helmet mounted displays
Moving sounds enhance the visually-induced self-motion illusion (circular vection) in virtual reality,2009,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-63049105058&doi=10.1145%2f1498700.1498701&partnerID=40&md5=a4056d5c05d555f20bf63bb1d716512f,"While rotating visual and auditory stimuli have long been known to elicit self-motion illusions (""circular vection""), audiovisual interactions have hardly been investigated. Here, two experiments investigated whether visually induced circular vection can be enhanced by concurrently rotating auditory cues that match visual landmarks (e.g., a fountain sound). Participants sat behind a curved projection screen displaying rotating panoramic renderings of a market place. Apart from a no-sound condition, headphone-based auditory stimuli consisted of mono sound, ambient sound, or low-/high-spatial resolution auralizations using generic head-related transfer functions (HRTFs). While merely adding nonrotating (mono or ambient) sound showed no effects, moving sound stimuli facilitated both vection and presence in the virtual environment. This spatialization benefit was maximal for a medium (20° × 15°) FOV, reduced for a larger (54° × 45°) FOV and unexpectedly absent for the smallest (10° × 7.5°) FOV. Increasing auralization spatial fidelity (from low, comparable to five-channel home theatre systems, to high, 5° resolution) provided no further benefit, suggesting a ceiling effect. In conclusion, both self-motion perception and presence can benefit from adding moving auditory stimuli. This has important implications both for multimodal cue integration theories and the applied challenge of building affordable yet effective motion simulators. © 2009 ACM.",Audiovisual interactions; Presence; Psychophysics; Self-motion simulation; Spatial sound; Vection; Virtual reality,Physics; Projection screens; Psychophysiology; Rotation; Audiovisual interactions; Presence; Psychophysics; Self-motion simulation; Spatial sound; Vection; Virtual reality
Evaluating the effect of motion and body shape on the perceived sex of virtual characters,2009,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-60349087158&doi=10.1145%2f1462048.1462051&partnerID=40&md5=2bbaa757af0c2174cd454ca67057384a,"In this paper, our aim is to determine factors that influence the perceived sex of virtual characters. In Experiment 1, four different model types were used: highly realistic male and female models, an androgynous character, and a point light walker. Three different types of motion were applied to all models: motion captured male and female walks, and neutral synthetic walks. We found that both form and motion influence sex perception for these characters: for neutral synthetic motions, form determines perceived sex, whereas natural motion affects the perceived sex of both androgynous and realistic forms. These results indicate that the use of neutral walks is better than creating ambiguity by assigning an incongruent motion. In Experiment 2 we investigated further the influence of body shape and motion on realistic male and female models and found that adding stereotypical indicators of sex to the body shapes influenced sex perception. Also, that exaggerated female body shapes influences sex judgements more than exaggerated male shapes. These results have implications for variety and realism when simulating large crowds of virtual characters. © 2009 ACM.",Graphics; Motion capture; Perception,Fluid structure interaction; Body shapes; Graphics; Motion capture; Perception; Virtual Characters; Real time systems
A perceptive evaluation of volume rendering techniques,2009,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-60349120833&doi=10.1145%2f1462048.1462054&partnerID=40&md5=4fb59ce84064b5d4427bddeb9ebeaf8e,"The display of space filling data is still a challenge for the community of visualization. Direct volume rendering (DVR) is one of the most important techniques developed to achieve direct perception of such volumetric data. It is based on semitransparent representations, where the data are accumulated in a depth-dependent order. However, it produces images that may be difficult to understand, and thus several techniques have been proposed so as to improve its effectiveness, using for instance lighting models or simpler representations (e.g., maximum intensity projection). In this article, we present three perceptual studies that examine how DVR meets its goals, in either static or dynamic context. We show that a static representation is highly ambiguous, even in simple cases, but this can be counterbalanced by use of dynamic cues (i.e., motion parallax) provided that the rendering parameters are correctly tuned. In addition, perspective projections are demonstrated to provide relevant information to disambiguate depth perception in dynamic displays. © 2009 ACM.",Direct volume rendering; Perception of transparency; Perspective projection; Structure from motion,Data visualization; Depth perception; Geometrical optics; Medical imaging; Transparency; Videodisks; Voltage regulators; Volumetric analysis; Direct volume rendering; Dynamic contexts; Dynamic cues; Dynamic displays; Lighting models; Maximum intensity projections; Motion parallaxes; Perception of transparency; Perspective projection; Space fillings; Static representations; Structure from motion; Volumetric datum; Volume rendering
Hybrid image/model-based gaze-contingent rendering,2009,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-60349088689&doi=10.1145%2f1462048.1462053&partnerID=40&md5=1bfec70933b25059168bb911145b0b9a,"A nonisotropic hybrid image/model-based gaze-contingent rendering technique utilizing ray casting on a GPU is discussed. Empirical evidence derived from human subject experiments indicates an inverse relationship between a peripherally degraded scene's high-resolution inset size and mean search time, a trend consistent with existing image-based and model-based techniques. In addition, the data suggest that maintaining a target's silhouette edges decreases search times when compared to targets with degraded edges. However, analysis suggests a point of diminishing returns with an inset larger than 15° when target discrimination is a component of visual search. Benefits of the hybrid technique include simplicity of design and parallelizability, both conducive to GPU implementation. © 2009 ACM.",Eye tracking; Level of Detail,Visual communication; Empirical evidences; Eye tracking; Gaze contingents; Gpu implementations; High resolutions; Human subject experiments; Hybrid techniques; Image-based; Inverse relationships; Level of Detail; Model-based; Ray castings; Search time; Target discriminations; Visual searches; Targets
A perceptual approach to trimming and tuning unstructured lumigraphs,2009,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-60349103497&doi=10.1145%2f1462048.1462050&partnerID=40&md5=287ffb893b177f40cf4c1a1e0a5e30ab,"We present a novel perceptual method to reduce the visual redundancy of unstructured lumigraphs, an image based representation designed for interactive rendering. We combine features of the unstructured lumigraph algorithm and image fidelity metrics to efficiently rank the perceptual impact of the removal of subregions of input views (subviews). We use a greedy approach to estimate the order in which subviews should be pruned to minimize perceptual degradation at each step. Renderings using varying numbers of subviews can then be easily visualized with confidence that the retained subviews are well chosen, thus facilitating the choice of how many to retain. The regions of the input views that are left are repacked into a texture atlas. Our method takes advantage of any scene geometry information available but only requires a very coarse approximation. We perform a user study to validate its behaviour, as well as investigate the impact of the choice of image fidelity metric as well as that of user parameters. The three metrics considered fall in the physical, statistical and perceptual categories. The overall benefit of our method is the semiautomation of the view selection process, resulting in unstructured lumigraphs that are thriftier in texture memory use and faster to render. Using the same framework, we adjust the parameters of the unstructured lumigraph algorithm to optimise it on a scene by scene basis. © 2009 ACM.",Image-based rendering; Perceptual metrics,Textures; Video amplifiers; Geometry informations; Image fidelities; Image-based rendering; Image-based representations; Interactive renderings; Overall benefits; Perceptual methods; Perceptual metrics; Semi automations; Texture atlas; Texture memories; User studies; View selections; Image reconstruction
A local roughness measure for 3D meshes and its application to visual masking,2009,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-60349083707&doi=10.1145%2f1462048.1462052&partnerID=40&md5=f893158a12010c6f5512ae0658df35ee,"3D models are subject to a wide variety of processing operations such as compression, simplification or watermarking, which may introduce some geometric artifacts on the shape. The main issue is to maximize the compression/ simplification ratio or the watermark strength while minimizing these visual degradations. However few algorithms exploit the human visual system to hide these degradations, while perceptual attributes could be quite relevant for this task. Particularly, the masking effect defines the fact that one visual pattern can hide the visibility of another. In this context we introduce an algorithm for estimating the roughness of a 3D mesh, as a local measure of geometric noise on the surface. Indeed, a textured (or rough) region is able to hide geometric distortions much better than a smooth one. Our measure is based on curvature analysis on local windows of the mesh and is independent of the resolution/connectivity of the object. The accuracy and the robustness of our measure, together with its relevance regarding visual masking have been demonstrated through extensive comparisons with state-of-the-art and subjective experiment. Two applications are also presented, in which the roughness is used to lead (and improve) respectively compression and watermarking algorithms. © 2009 ACM.",3D mesh; Curvature; Masking; Roughness; Subjective evaluation,Digital watermarking; Papermaking; Watermarking; 3D mesh; Curvature; Masking; Roughness; Subjective evaluation; Three dimensional
Guest editorial: Special issue on Applied Perception in Graphics and Visualization (APGV07),2009,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-60349083442&doi=10.1145%2f1462048.1462049&partnerID=40&md5=c2f3bfc8a930d5fad88dce99794d9d85,[No abstract available],,
Auditory distance perception in an acoustic pipe,2008,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-52249098334&doi=10.1145%2f1402236.1402240&partnerID=40&md5=e9c7f5fa85f9f255c983a531bfdc89af,"In a study of auditory distance perception, we investigated the effects of exaggeration the acoustic cue of reverberation where the intensity of sound did not vary noticeably. The set of stimuli was obtained by moving a sound source inside a 10.2-m long pipe having a 0.3-m diameter. Twelve subjects were asked to listen to a speech sound while keeping their head inside the pipe and then to estimate the egocentric distance from the sound source using a magnitude production procedure. The procedure was repeated eighteen times using six different positions of the sound source. Results show that the point at which perceived distance equals physical distance is located approximately 3.5 m away from the listening point, with an average range of distance estimates of approximately 3.3 m, i.e., 1.65 to 4.9 m. The absence of intensity cues makes the acoustic pipe a potentially interesting modeling paradigm for the design of auditory interfaces in which distance is rendered independently of loudness. The proposed acoustic environment also confirms the known unreliability of certain distance cues. © 2008 ACM.",Acoustic pipe; Auditory display; Distance perception,Acoustic generators; Acoustics; Depth perception; Estimation; Models; Surface chemistry; Acoustic environment; Acoustic pipe; Auditory display; Auditory interfaces; Distance perception; Long pipe; Production procedure; Sound sources; Speech sounds; Pipe
Lead-me interface for a pulling sensation from hand-held devices,2008,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-51649117642&doi=10.1145%2f1402236.1402239&partnerID=40&md5=fcc3c4099496dd34f5a8a79a1376551b,"When a small mass in a hand-held device oscillates along a single axis with asymmetric acceleration (strongly peaked in one direction and diffuse in the other), the holder typically experiences a kinesthetic illusion characterized by the sensation of being continuously pushed or pulled by the device. This effect was investigated because of its potential application to a hand-held, nongrounded, haptic device that can convey a sense of a continuous translational force in one direction, which is a key missing piece in haptic research. A 1 degree-of-freedom (DOF) haptic device based on a crank-slider mechanism was constructed. The device converts the constant rotation of an electric motor into the constrained movement of a small mass with asymmetric acceleration. The frequency that maximizes the perceived movement offered by the haptic device was investigated. Tests using three subjects showed that for the prototype, the best frequencies were 5 and 10 cycles per second. © 2008 ACM.",Haptic perception; Interface using illusionary sensation; Mobile device,Electric machinery; Hand held computers; Asymmetric acceleration; Constrained movement; Hand-held devices; Haptic devices; Haptic perception; Interface using illusionary sensation; Mobile device; Potential applications; Single-axis; Motors
A study of the modification of the speed and size of the cursor for simulating pseudo-haptic bumps and holes,2008,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-52249113327&doi=10.1145%2f1402236.1402238&partnerID=40&md5=6618480d0dd70eb6244cfd693761a1c5,"In previous work on so-called pseudo-haptic textures, we investigated the possibility of simulating sensations of texture without haptic devices by using the sole manipulation of the speed of a mouse cursor (a technique called speed technique). In this paper, we describe another technique (called Size technique) to enhance the speed technique and simulate texture sensations by varying the size of the cursor according to the local height of the texture displayed on the computer screen. With the size technique, the user would see an increase (decrease) in cursor size corresponding to a positive (negative) slope of the texture. We have conducted a series of experiments to study and compare the use of both the size and speed technique for simulating simple shapes like bumps and holes. In Experiment 1, our results showed that participants could successfully identify bumps and holes using the size technique alone. Performances obtained with the size technique reached a similar level of accuracy as found previously with the speed technique alone. In Experiment 2, we determined a point of subjective equality between bumps simulated by each technique separately, which suggests that the two techniques provide information that can be perceptually equivalent. In Experiment 3, using paradoxical situations of conflict between the two techniques, we have found that participants' answers were more influenced by the size technique, suggesting a dominance of the size over the speed technique. Furthermore, we have found a mutual reinforcement of the techniques, i.e., when the two techniques were consistently combined, the participants were more efficient in identifying the simulated shapes. In Experiment 4, we further observed the complex interactions between the information associated with the two techniques in the perception and in the decision process related to the accurate identification of bumps and holes. Taken together, our results promote the use of both techniques for the low-cost simulation of texture sensations in applications, such as videogames, internet, and graphical user interfaces. © 2008 ACM.",Bump; Control/display ratio; Cursor; Hole; Pseudo-haptic; Size; Speed; Texture,Chlorine compounds; Computer systems; Experiments; Films; Graphical user interfaces; Reinforcement; Textures; User interfaces; Bump; Complex interactions; Computer screens; Control/display ratio; Cursor; Decision process; Haptic devices; Hole; Mouse cursors; Mutual reinforcement; Pseudo-haptic; Size; Texture; Texture sensations; Videogames; Speed
Recalibration of rotational locomotion in immersive virtual environments,2008,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-52249113611&doi=10.1145%2f1402236.1402241&partnerID=40&md5=fa610b04a97e7922a0afa59188912a6b,"This work uses an immersive virtual environment (IVE) to examine how people maintain a calibration between biomechanical and visual information for rotational self-motion. First, we show that no rotational recalibration occurs when visual and biomechanical rates of rotation are matched. Next, we demonstrate that mismatched physical and visual rotation rates cause rotational recalibration. Although previous work has shown that rotational locomotion can be recalibrated in real environments, this work extends the finding to virtual environments. We further show that people do not completely recalibrate left and right rotations independently when different visual - biomechanical discrepancies are used for left and right rotations during a recalibration phase. Finally, since the majority of participants did not notice mismatched physical and visual rotation rates, we discuss the implications of using such mismatches to enable IVE users to explore a virtual space larger than the physical space they are in. © 2008 ACM.",Perception; Recalibration; Rotation; Virtual environments,Biped locomotion; Rotation; Virtual reality; Visual communication; Immersive virtual environment; Immersive virtual environments; Perception; Physical space; Real environments; Recalibration; Rotation rates; Rotational locomotion; Self motions; Virtual environments; Virtual spaces; Visual informations; Biomechanics
Data density and trend reversals in auditory graphs: Effects on point-estimation and trend-identification tasks,2008,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-52249087549&doi=10.1145%2f1402236.1402237&partnerID=40&md5=3ef191c5f31b6bfe37ca3904fa769198,"Auditory graphs-displays that represent quantitative information with sound-have the potential to make data (and therefore science) more accessible for diverse user populations. No research to date, however, has systematically addressed the attributes of data that contribute to the complexity (the ease or difficulty of comprehension) of auditory graphs. A pair of studies examined the role of data density (i.e., the number of discrete data points presented per second) and the number of trend reversals for both point-estimation and trend-identification tasks with auditory graphs. For the point-estimation task, more trend reversals led to performance decrements. For the trend-identification task, a large main effect was again observed for trend reversals, but an interaction suggested that the effect of the number of trend reversals was different across lower data densities (i.e., as density increased from 1 to 2 data points per second). Results are discussed in terms of data sonification applications and rhythmic theories of auditory pattern perception. © 2008 ACM.",Auditory display; Auditory graphs; Sonification,Chlorine compounds; Estimation; Graph theory; Auditory display; Auditory graphs; Data densities; Data points; Data sonification; Discrete data; Main effects; Quantitative information; Sonification; Population statistics
Transactions on Applied Perception: Editorial,2008,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-50249133756&doi=10.1145%2f1279920.1361703&partnerID=40&md5=642a69f2e26321dcece8d63bd245f38c,[No abstract available],,
Visual detection of LSB-encoded natural image steganography,2008,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-76449087450&doi=10.1145%2f1328775&partnerID=40&md5=f65f2816e578c5914899601c5508c4b6,"Many steganographic systems embed hidden messages inside the least significant bit layers of colour natural images. The presence of these messages can be difficult to detect by using statistical steganalysis. However, visual steganalysis by humans may be more successful in natural image discrimination. This study examined whether humans could detect least-significant bit steganography in 15 color natural images from the VisTex database using a controlled same/different task (N = 58) and a yes/no task (N = 61). While d′ > 1 was observed for color layers 4-8, layers 1-3 had d′ < 1 in both experiments. Thus, layers 1-3 appear to be highly resistant to visual steganalysis. © 2008 ACM.",C.2 [computer communication networks]: C.2.0 general - Security and protection; Counterterrorism; J.4 [social and behavioral sciences]: - Psychology; J.7 [computers in other systems]: - Military; Steganography,Color; Discrete cosine transforms; Counter terrorism; J.7 [computers in other systems]: - Military; Least significant bits; Security and protection; Social and behavioral science; Statistical steganalysis; Steganographic system; Visual detection; Steganography
Using haptic cues to aid nonvisual structure recognition,2008,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-50249137248&doi=10.1145%2f1279920.1279922&partnerID=40&md5=c34c269e22fe0874d393324d35b2d994,"Retrieving information presented visually is difficult for visually disabled users. Current accessibility technologies, such as screen readers, fail to convey presentational layout or structure. Information presented in graphs or images is almost impossible to convey through speech alone. In this paper, we present the results of an experimental study investigating the role of touch (haptic) and auditory cues in aiding structure recognition when visual presentation is missing. We hypothesize that by guiding users toward nodes in a graph structure using force fields, users will find it easier to recognize overall structure. Nine participants were asked to explore simple 3D structures containing nodes (spheres or cubes) laid out in various spatial configurations and asked to identify the nodes and draw their overall structure. Various combinations of haptic and auditory feedback were explored. Our results demonstrate that haptic cues significantly helped participants to quickly recognize nodes and structure. Surprisingly, auditory cues alone did not speed up node recognition; however, when they were combined with haptics both node identification and structure recognition significantly improved. This result demonstrates that haptic feedback plays an important role in enabling people to recall spatial layout. © 2008 ACM.",Accessibility; Haptic perception; Multimodal cues; Visual disability,Graph theory; Three dimensional; 3-D structures; Accessibility; Auditory cues; Auditory feedback; Disabled users; Experimental studies; Force fields; Graph structures; Haptic cues; Haptic feedbacks; Haptic perception; Haptics; Multimodal cues; Screen readers; Spatial configurations; Spatial layout; Speed ups; Structure recognition; Visual disability; Visual presentations; Haptic interfaces
A tone-mapping operator for road visibility experiments,2008,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-50349100917&doi=10.1145%2f1279920.1361704&partnerID=40&md5=e42bf94862f6af1bdf64449064c45df8,"One may wish to use computer graphic images to carry out road visibility studies. Unfortunately, most display devices still have a limited luminance dynamic range, especially in driving simulators. In this paper, we propose a tone-mapping operator (TMO) to compress the luminance dynamic range while preserving the driver's performance for a visual task relevant for a driving situation. We address three display issues of some consequences for road image display: luminance dynamics, image quantization, and high minimum displayable luminance. Our TMO characterizes the effects of local adaptation with a bandpass decomposition of the image using a Laplacian pyramid, and processes the levels separately in order to mimic the human visual system. The contrast perception model uses the visibility level, a usual index in road visibility engineering applications. To assess our algorithm, a psychophysical experiment devoted to a target detection task was designed. Using a Landolt ring, the visual performances of 30 observers were measured: they stared first at a high-dynamic range image and then at the same image processed by a TMO and displayed on a low-dynamic range monitor, for comparison. The evaluation was completed with a visual appearance evaluation. Our operator gives good performances for three typical road situations (one in daylight and two at night), after comparison with four standard TMOs from the literature. The psychovisual assessment of our TMO is limited to these driving situations. © 2008 ACM.",HDR images; Psychophysics; Road visibility; Visual performance,Automobile simulators; Canning; Color photography; Computational geometry; Conformal mapping; Display devices; Experiments; Image processing; Imaging systems; Imaging techniques; Lighting; Optical data processing; Roads and streets; Standards; Technology; Visibility; Band-pass; Computer graphic images; Driving simulators; Driving situations; Dynamic range; Engineering applications; HDR images; Human Visual System; Image displays; Image quantization; Laplacian pyramids; Local adaptation; Perception modeling; Psychophysical experiments; Psychophysics; Road visibility; Target detections; Tone mappings; Visual appearances; Visual performance; Visual tasks; Optical properties
Distinctiveness of faces: A computational approach,2008,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-50249181224&doi=10.1145%2f1279920.1279925&partnerID=40&md5=805171d9cb7e7f306af9db74f5cee572,"This paper develops and demonstrates an original approach to face-image analysis based on identifying distinctive areas of each individual's face by its comparison to others in the population. The method differs from most others - that we refer as unary - where salient regions are defined by analyzing only images of the same individual. We extract a set of multiscale patches from each face image before projecting them into a common feature space. The degree of ""distinctiveness"" of any patch depends on its distance in feature space from patches mapped from other individuals. First a pairwise analysis is developed and then a simple generalization to the multiple-face case is proposed. A perceptual experiment, involving 45 observers, indicates the method to be fairly compatible with how humans mark faces as distinct. A quantitative example of face authentication is also performed in order to show the essential role played by the distinctive information. A comparative analysis shows that performance of our n-ary approach is as good as several contemporary unary, or binary, methods, while tapping a complementary source of information. Furthermore, we show it can also provide a useful degree of illumination invariance. © 2008 ACM.",Face authentication; Illumination changes; Log-polar representation,Image analysis; Imaging techniques; Comparative analysis; Computational approach; Face authentication; Face images; Feature spaces; Illumination changes; Illumination invariance; Log-polar representation; Multi scaling; Salient regions; Feature extraction
Tactile synthesis and perceptual inverse problems seen from the viewpoint of contact mechanics,2008,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-50249123197&doi=10.1145%2f1279920.1279921&partnerID=40&md5=5c9c90a946e83d216c79ef2578259c5f,"A contact-mechanics analysis was used to explain a tactile illusion engendered by straining the fingertip skin tangentially in a progressive wave pattern resulting in the perception of a moving undulating surface. We derived the strain tensor field induced by a sinusoidal surface sliding on a finger as well as the field created by a tactile transducer array deforming the fingerpad skin by lateral traction. We found that the first field could be well approximated by the second. Our results have several implications. First, tactile displays using lateral skin deformation can generate tactile sensations similar to those using normal skin deformation. Second, a synthesis approach can achieve this result if some constraints on the design of tactile stimulators are met. Third, the mechanoreceptors embedded in the skin must respond to the deviatoric part of the strain tensor field and not to its volumetric part. Finally, many tactile stimuli might represent, for the brain, an inverse problem to be solved, such specific examples of ""tactile metameres"" are given. © 2008 ACM.",Computational tactile perception; Contact mechanics; Haptics; Lateral skin deformation; Tactile sensing; Tactile synthesis; Tactile transducers arrays,Deformation; Differential equations; Mechanics; Skin; Speed; Strain; Tensors; Computational tactile perception; Contact mechanics; Deviatoric part; Fingerpad skin; Haptics; Lateral skin deformation; Mechanoreceptors; Normal skin; Sinusoidal surfaces; Skin deformation; Strain tensors; Tactile displays; Tactile sensations; Tactile sensing; Tactile stimuli; Tactile synthesis; Tactile transducer; Tactile transducers arrays; Wave patterns; Inverse problems
Content and Quality: Interpretation-Based Estimation of Image Quality,2008,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015599396&doi=10.1145%2f1278760.1278762&partnerID=40&md5=3f54b0d669af2df611751921630948e9,"Test image contents affect subjective image-quality evaluations. Psychometric methods might show that contents have an influence on image quality, but they do not tell what this influence is like, i.e., how the contents influence image quality. To obtain a holistic description of subjective image quality, we have used an interpretation-based quality (IBQ) estimation approach, which combines qualitative and quantitative methodology. The method enables simultaneous examination of psychometric results and the subjective meanings related to the perceived image-quality changes. In this way, the relationship between subjective feature detection, subjective preferences, and interpretations are revealed. We report a study that shows that different impressions are conveyed in five test image contents after similar sharpness variations. Thirty naïve observers classified and freely described the images after which magnitude estimation was used to verify that they distinguished the changes in the images. The data suggest that in the case of high image quality, the test image selection is crucial. If subjective evaluation is limited only to technical defects in test images, important subjective information of image-quality experience is lost. The approach described here can be used to examine image quality and it will help image scientists to evaluate their test images. © 2008, ACM. All rights reserved.",Experimentation; Human Factors; image contents; Image quality; Measurement; qualitative methodology; subjective measurement,
Evaluating the Perceptual Realism of Animated Facial Expressions,2008,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016138195&doi=10.1145%2f1278760.1278764&partnerID=40&md5=7abfefc3c44accc1fd9ae8d75f257f55,"The human face is capable of producing an astonishing variety of expressions—expressions for which sometimes the smallest difference changes the perceived meaning considerably. Producing realistic-looking facial animations that are able to transmit this degree of complexity continues to be a challenging research topic in computer graphics. One important question that remains to be answered is: When are facial animations good enough? Here we present an integrated framework in which psychophysical experiments are used in a first step to systematically evaluate the perceptual quality of several different computer-generated animations with respect to real-world video sequences. The first experiment provides an evaluation of several animation techniques, exposing specific animation parameters that are important to achieve perceptual fidelity. In a second experiment, we then use these benchmarked animation techniques in the context of perceptual research in order to systematically investigate the spatiotemporal characteristics of expressions. A third and final experiment uses the quality measures that were developed in the first two experiments to examine the perceptual impact of changing facial features to improve the animation techniques. Using such an integrated approach, we are able to provide important insights into facial expressions for both the perceptual and computer graphics community. © 2008, ACM. All rights reserved.",3D-scanning; avatar; Evalution of facial animations; Experimentation; perceptually adaptive graphics; psychophysics; recognition,
Identifying faces across variations in lighting: Psychophysics and computation,2008,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-50249119710&doi=10.1145%2f1279920.1279924&partnerID=40&md5=402e89da6a6dc7d710cbd53eb160b6e7,"Humans have the ability to identify objects under varying lighting conditions with extraordinary accuracy. We investigated the behavioral aspects of this ability and compared it to the performance of the illumination cones (IC) model of Belhumeur and Kriegman [1998]. In five experiments, observers learned 10 faces under a small subset of illumination directions. We then tested observers' recognition ability under different illuminations. Across all experiments, recognition performance was found to be dependent on the distance between the trained and tested illumination directions. This effect was modulated by the nature of the trained illumination directions. Generalizations from frontal illuminations were different than generalizations from extreme illuminations. Similarly, the IC model was also sensitive to whether the trained images were near-frontal or extreme. Thus, we find that the nature of the images in the training set affects the accuracy of an object's representation under variable lighting for both humans and the model. Beyond this general correspondence, the microstructure of the generalization patterns for both humans and the IC model were remarkably similar, suggesting that the two systems may employ related algorithms. © 2008 ACM.",Face recognition; Human psychophysics; Illumination invariance; Image-based models; Object recognition,Experiments; Image enhancement; Face recognition; Human psychophysics; Illumination invariance; Image-based models; Object recognition; Recognition abilities; Recognition performance; Training sets; Varying lighting; Lighting
Effectiveness of augmented-reality visualization versus cognitive mediation for learning actions in near space,2008,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-49449086036&doi=10.1145%2f1279640.1279641&partnerID=40&md5=c1259c4705931d1d1d6d035afe38cfd7,"The present study examined the impact of augmented-reality visualization, in comparison to conventional ultrasound (CUS), on the learning of ultrasound-guided needle insertion. Whereas CUS requires cognitive processes for localizing targets, our augmented-reality device, called the sonic flashlight (SF) enables direct perceptual guidance. Participants guided a needle to an ultrasound-localized target within opaque fluid. In three experiments, the SF showed higher accuracy and lower variability in aiming and endpoint placements than did CUS. The SF, but not CUS, readily transferred to new targets and starting points for action. These effects were evident in visually guided action (needle and target continuously visible) and visually directed action (target alone visible). The results have application to learning to visualize surgical targets through ultrasound. © 2008 ACM.",Augmented reality; Learning; Motor control; Perception; Spatial cognition,Acoustic waves; Bioinformatics; Education; Needles; Ultrasonics; Visualization; Augmented reality; Augmented-reality visualization; Cognitive processing; Learning; Motor control; Near space; Needle-insertion; Perception; Sonic flashlight; Spatial cognition; Targets
Perceptual Evaluation of Tone-Reproduction Operators Using the Cornsweet–Craik–O'Brien Illusion,2008,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84982121728&doi=10.1145%2f1278760.1278761&partnerID=40&md5=80c02341b2c797e6a896a72ec16ab024,"High dynamic-range images cannot be directly displayed on conventional display devices, but have to be tone-mapped first. For this purpose, a large set of tone-reproduction operators is currently available. However, it is unclear which operator is most suitable for any given task. In addition, different tasks may place different requirements upon each operator. In this paper we evaluate several tone-reproduction operators using a paradigm that does not require the construction of a real high dynamicrange scene, nor does it require the availability of a high dynamic-range display device. The user study involves a task that relates to the evaluation of contrast, which is an important attribute that needs to be preserved under tone reproduction. © 2008, ACM. All rights reserved.",dynamic-range compression; Experimentation; high dynamic-range imaging; Human Factors; Tone-mapping operators; visual psychophysics,
Perceptual Dependencies in Information Visualization Assessed by Complex Visual Search,2008,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863024480&doi=10.1145%2f1278760.1278763&partnerID=40&md5=2d439b245beb5021a4e21689eba89b62,"A common approach for visualizing data sets is to map them to images in which distinct data dimensions are mapped to distinct visual features, such as color, size and orientation. Here, we consider visualizations in which different data dimensions should receive equal weight and attention. Many of the end-user tasks performed on these images involve a form of visual search. Often, it is simply assumed that features can be judged independently of each other in such tasks. However, there is evidence for perceptual dependencies when simultaneously presenting multiple features. Such dependencies could potentially affect information visualizations that contain combinations of features for encoding information and, thereby, bias subjects into unequally weighting the relevance of different data dimensions. We experimentally assess (1) the presence of judgment dependencies in a visualization task (searching for a target node in a node-link diagram) and (2) how feature contrast relates to salience. From a visualization point of view, our most relevant findings are that (a) to equalize saliency (and thus bottom-up weighting) of size and color, color contrasts have to become very low. Moreover, orientation is less suitable for representing information that consists of a large range of data values, because it does not show a clear relationship between contrast and salience; (b) color and size are features that can be used independently to represent information, at least as far as the range of colors that were used in our study are concerned; (c) the concept of (static) feature salience hierarchies is wrong; how salient a feature is compared to another is not fixed, but a function of feature contrasts; (d) final decisions appear to be as good an indicator of perceptual performance as indicators based on measures obtained from individual fixations. Eye tracking, therefore, does not necessarily present a benefit for user studies that aim at evaluating performance in search tasks. © 2008, ACM. All rights reserved.",Color; Design; Experimentation; feature hierarchy; feature interaction; Human Factors; human vision; information visualization; Measurement; nodelink diagrams; orientation; perceptual dependencies; Performance; psychophysics; Verification; visual features; visual search,
Enhancing air traffic displays via perceptual cues,2008,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-49449108602&doi=10.1145%2f1279640.1279644&partnerID=40&md5=4f5e72143db5eea920dc3e74bc6475fa,"We examined graphical representations of aircraft altitude in simulated air traffic control (ATC) displays. In two experiments, size and contrast cues correlated with altitude improved participants' ability to detect future aircraft collisions (conflicts). Experiment 1 demonstrated that, across several set sizes, contrast and size cues to altitude improved accuracy at identifying conflicts. Experiment 2 demonstrated that graphical cues for representing altitude both improved accuracy and reduced search time for finding conflicts in large set size displays. The addition of size and contrast cues to ATC displays may offer specific benefits in aircraft conflict detection. © 2008 ACM.",Air traffic control; Applied cognitive science; Human-computer interaction; Visualization,Air traffic control; Aircraft; Aircraft accidents; Experiments; Graphic methods; Air Traffic; Air traffic control (ATC); Aircraft collisions; Aircraft conflict detection; Applied cognitive science; Graphical representations; Human-computer interaction; Search time; Set sizes; Size cues; Visualization; Accidents
Visualizing graphs in three dimensions,2008,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-49449106706&doi=10.1145%2f1279640.1279642&partnerID=40&md5=287ab35abfb2d2dba57003b5deb79911,"It has been known for some time that larger graphs can be interpreted if laid out in 3D and displayed with stereo and/or motion depth cues to support spatial perception. However, prior studies were carried out using displays that provided a level of detail far short of what the human visual system is capable of resolving. Therefore, we undertook a graph comprehension study using a very high resolution stereoscopic display. In our first experiment, we examined the effect of stereoscopic display, kinetic depth, and using 3D tubes versus lines to display the links. The results showed a much greater benefit for 3D viewing than previous studies. For example, with both motion and stereoscopic depth cues, unskilled observers could see paths between nodes in 333 node graphs with less than a 10% error rate. Skilled observers could see up to a 1000-node graph with less than a 10% error rate. This represented an order of magnitude increase over 2D display. In our second experiment, we varied both nodes and links to understand the constraints on the number of links and the size of graph that can be reliably traced. We found the difference between number of links and number of nodes to best account for error rates and suggest that this is evidence for a perceptual phase transition. These findings are discussed in terms of their implications for information display. © 2008 ACM.",Graph visualization; Network visualization; Stereoscopic displays; Visualization,Error analysis; Experiments; Learning systems; 3D viewing; Depth cueing; Error Rate; Error rates; Graph visualization; Human Visual System; Information displaying; Level of details; Network visualization; Node graph; Nodes and links; Number of nodes; Order-of magnitudes; Phase transitions; Spatial perception; Stereoscopic displays; Three dimensions; Very high resolution; Visualization; Graph theory
Visual Detection of Lsb-Encoded Natural Image Steganography,2008,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024251923&doi=10.1145%2f1279640.1328775&partnerID=40&md5=8f1df3973e57f4f67e1dbddd011a478c,"Many steganographic systems embed hidden messages inside the least significant bit layers of colour natural images. The presence of these messages can be difficult to detect by using statistical steganalysis. However, visual steganalysis by humans may be more successful in natural image discrimination. This study examined whether humans could detect least-significant bit steganography in 15 color natural images from the VisTex database using a controlled same/different task (N = 58) and a yes/no task (N = 61). While d' > 1 was observed for color layers 4–8, layers 1–3 had d' < 1 in both experiments. Thus, layers 1–3 appear to be highly resistant to visual steganalysis. © 2008, ACM. All rights reserved.",counterterrorism; Experimentation; Human Factors; Security; Steganography,
Applying computational tools to predict gaze direction in interactive visual environments,2008,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-50249088113&doi=10.1145%2f1279920.1279923&partnerID=40&md5=ffbca975a2263a20be7bcde184b1c7de,"Future interactive virtual environments will be ""attention-aware, "" capable of predicting, reacting to, and ultimately influencing the visual attention of their human operators. Before such environments can be realized, it is necessary to operationalize our understanding of the relevant aspects of visual perception, in the form of fully automated computational heuristics that can efficiently identify locations that would attract human gaze in complex dynamic environments. One promising approach to designing such heuristics draws on ideas from computational neuroscience. We compared several neurobiologically inspired heuristics with eye-movement recordings from five observers playing video games, and found that human gaze was better predicted by heuristics that detect outliers from the global distribution of visual features than by purely local heuristics. Heuristics sensitive to dynamic events performed best overall. Further, heuristic prediction power differed more between games than between different human observers. While other factors clearly also influence eye position, our findings suggest that simple neurally inspired algorithmic methods can account for a significant portion of human gaze behavior in a naturalistic, interactive setting. These algorithms may be useful in the implementation of interactive virtual environments, both to predict the cognitive state of human operators, as well as to effectively endow virtual agents in the system with humanlike visual behavior. © 2008 ACM.",Active vision; Computational modeling; Eye-movements; Immersive environments; Video games; Visual attention,Chlorine compounds; Forecasting; Game theory; Heuristic methods; Heuristic programming; Active vision; Algorithmic methods; Cognitive states; Computational modeling; Computational neuroscience; Computational tools; Dynamic environments; Dynamic events; Eye position; Eye-movements; Gaze behavior; Gaze direction; Global distributions; Human observers; Human operators; Immersive environments; Interactive virtual environments; Promising approach; Video games; Virtual agents; Visual attention; Visual behavior; Visual environments; Visual features; Visual perceptions; Virtual reality
Evaluation of Methods for Approximating Shapes Used to Synthesize 3D Solid Textures,2008,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77954149425&doi=10.1145%2f1278760.1278765&partnerID=40&md5=943f60bc7fa89d37b9574dff65d999d3,"In modern computer graphics applications, textures play an important role in conveying the appearance of real-world materials. But while surface appearance can often be effectively captured with a photograph, it is difficult to use example imagery to synthesize fully three-dimensional (3D) solid textures that are perceptually similar to their inputs. Specifically, this research focuses on human perception of 3D solid textures composed of aggregate particles in a binding matrix. Holding constant an established algorithm for approximating particle distributions, we examine the problem of estimating particle shape.We consider four methods for approximating plausible particle shapesincluding two methods of our own contribution. We compare the performance of these methods under a variety of input conditions using automated, perceptually motivated metrics, as well as a psychophysical experiment. In the course of assessing the relative performance of the four algorithms, we also evaluate the reliability of the automated metrics in predicting the results of the experiment. © 2008, ACM. All rights reserved.",Algorithms; Human Factors; Shape estimation; shape perception; solid textures; texture synthesis; volumetric textures,
A gaze-based study for investigating the perception of visual realism in simulated scenes,2008,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-49449097083&doi=10.1145%2f1279640.1279643&partnerID=40&md5=0893530f584f66766e7c51dd5e420ccc,"Visual realism has been a major objective of computer graphics since the inception of the field. However, the perception of visual realism is not a well-understood process and is usually attributed to a combination of visual cues and image features that are difficult to define or measure. For highly complex images, the problem is even more involved. The purpose of this paper is to present a study based on eye tracking for investigating the perception of visual realism of static images with different visual qualities. The eye-fixation clusters helped to define salient image features corresponding to 3D surface details and light transfer properties that attract observers' attention. This enabled the definition and categorization of image attributes affecting the perception of photorealism. The dynamics of the visual behavior of different observer groups were examined by analyzing saccadic eye movements. We also demonstrated how the different image categories used in the experiments were perceived with varying degrees of visual realism. The results presented can be used as a basis for investigating the impact of individual image features on the perception of visual realism. This study suggests that post-recall or simple abstraction of visual experience is not accurate and the use of eye tracking provides an effective way of determining relevant features that affect visual realism, thus allowing for improved rendering techniques that target these features. © 2008 ACM.",Eye tracking; Human-computer interaction; Photorealistic rendering; Simulation environment; Visual perception; Visual realism,Chlorine compounds; Computational geometry; Computer graphics; Eye movements; Image enhancement; Target tracking; 3D surfaces; Complex images; Eye tracking; Human-computer interaction; Image attributes; Image features; Light transfer; Photorealism; Photorealistic rendering; Saccadic eye movements; Simulation environment; Static imaging; Visual behavior; Visual cues; Visual experience; Visual perception; Visual qualities; Visual realism; Three dimensional
Guest Editorial,2007,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024286676&doi=10.1145%2f1278387.1278388&partnerID=40&md5=b6cb4ebb9486a7f68795ae3ab9468662,[No abstract available],,
A Feedback-Controlled Interface for Treadmill Locomotion in Virtual Environments,2007,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024278685&doi=10.1145%2f1227134.1227141&partnerID=40&md5=514c7dc13f3c1f18684a9296a0a2719b,"Virtual environments (VEs) allow safe, repeatable, and controlled evaluations of obstacle avoidance and navigation performance of people with visual impairments using visual aids. Proper simulation of mobility in a VE requires an interface, which allows subjects to set their walking pace. Using conventional treadmills, the subject can change their walking speed by pushing the tread with their feet, while leveraging handrails or ropes (self-propelled mode). We developed a feedback-controlled locomotion interface that allows the VE workstation to control the speed of the treadmill, based on the position of the user. The position and speed information is also used to implement automated safety measures, so that the treadmill can be halted in case of erratic behavior. We compared the feedback-controlled to the self-propelled mode by using speed-matching tasks (follow a moving object or match the speed of an independently moving scene) to measure the efficacy of each mode in maintaining constant subject position, subject control of the treadmill, and subject pulse rates. In addition, we measured the perception of speed in the VE on each mode. The feedback-controlled mode required less physical exertion than self-propelled. The average position of subjects on the feedback-controlled treadmill was always within a centimeter of the desired position. There was a smaller standard deviation in subject position when using the self-propelled mode than when using the feedback-controlled mode, but the difference averaged less than 6 cmacross all subjectswalking at a constant speed. Although all subjects underestimated the speed of an independently moving scene at higher speeds, their estimates were more accurate when using the feedback-controlled treadmill than the selfpropelled. © 2007, ACM. All rights reserved.",Experimentation; Human Factors; Locomotion; low-vision; Measurement; speed-matching; treadmill,
Editorial: Walking in Real and Virtual Environments,2007,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-63449116182&doi=10.1145%2f1227134.1227135&partnerID=40&md5=c54fd1ac29a6cfd2e88fb01972dfdeb0,"This special issue is devoted to understanding human movement by walking in real and virtual environments, investigating activities including obstacle avoidance, estimation of travel distance, visuomotor calibration of walking, gender differences in path integration, heading assessment in low vision and visual speed-matching on a feedback-controlled walking machine (treadmill). Whether implicitly or explicitly, every study in the issue takes a comparative approach, relating the activity in the virtual world to its counterpart in the real world, and vice versa, as without cross-validating one realm with the other it is arguably impossible to get it right. Necessarily, this is also a multidisciplinary area of research and thus, appropriately, the background of the contributors to this issue range from computer science to psychology, engineering to neuroscience, optics to ophthalmology and others. Indeed, there is as much of a goal for the issue to understand multimodal processing in human locomotion as there is to further the technology of virtual reality through its understanding, two aspects that embody the cross-disciplinary ethos of ACM TAP. The capability for a user to move around in the virtual, as in the real, world has long been a key requirement and challenge for virtual reality technologists and scientists. Full motion for navigation in virtual environments (VEs) using joy-stick, mouse or keyboard, can be effective but does not mimic important aspects of human action nor, it seems, do all graphic environments convey navigational cues equivalently. What can we learn from human locomotor movement and behavior in the real world that could yield a better interface in virtual reality (VR), and how can we use VR to improve our understanding of the former? Obstacle avoidance is no less than a requirement to our survival while moving, and thus to faithfully include it inVRrequires a comparison to be made between its characteristics inVRand the real world. In Fink et al. in this issue, small but reliable differences are found in locomotor paths, as subjects' avoided. © 2007, ACM. All rights reserved.",,
Categorization of Natural Scenes: Local Versus Global Information and the Role of Color,2007,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85010289707&doi=10.1145%2f1278387.1278393&partnerID=40&md5=b14a8da257917f2c30f7e0e8194b3193,"Categorization of scenes is a fundamental process of human vision that allows us to efficiently and rapidly analyze our surroundings. Several studies have explored the processes underlying human scene categorization, but they have focused on processing global image information. In this study, we present both psychophysical and computational experiments that investigate the role of local versus global image information in scene categorization. In a first set of human experiments, categorization performance is tested when only local or only global image information is present. Our results suggest that humans rely on local, region-based information as much as on global, configural information. In addition, humans seem to integrate both types of information for intact scene categorization. In a set of computational experiments, human performance is compared to two state-of-the-art computer vision approaches that have been shown to be psychophysically plausible and that model either local or global information. In addition to the influence of local versus global information, in a second series of experiments, we investigated the effect of color on the categorization performance of both the human observers and the computational model. Analysis of the human data suggests that color is an additional channel of perceptual information that leads to higher categorization results at the expense of increased reaction times in the intact condition. However, it does not affect reaction times when only local information is present. When color is removed, the employed computational model follows the relative performance decrease of human observers for each scene category and can thus be seen as a perceptually plausible model for human scene categorization based on local image information. © 2007, ACM. All rights reserved.",Algorithms; computational modeling; computationalgist; global configural information; Human perception; local region-based information; scene classification; Scene perception; semantic modeling,
Perceptual Rendering of Participating Media,2007,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85007250443&doi=10.1145%2f1278387.1278389&partnerID=40&md5=1adab057a5594fe995496c8c6ef5431c,"High-fidelity image synthesis is the process of computing images that are perceptually indistinguishable from the real world they are attempting to portray. Such a level of fidelity requires that the physical processes of materials and the behavior of light are accurately simulated. Most computer graphics algorithms assume that light passes freely between surfaces within an environment. However, in many applications, we also need to take into account how the light interacts with media, such as dust, smoke, fog, etc., between the surfaces. The computational requirements for calculating the interaction of light with such participating media are substantial. This process can take many hours and rendering effort is often spent on computing parts of the scene that may not be perceived by the viewer. In this paper, we present a novel perceptual strategy for physically based rendering of participating media. By using a combination of a saliency map with our new extinction map (X map), we can significantly reduce rendering times for inhomogeneous media. The visual quality of the resulting images is validated using two objective difference metrics and a subjective psychophysical experiment. Although the average pixel errors of these metric are all less than 1%, the subjective validation indicates that the degradation in quality still is noticeable for certain scenes. We thus introduce and validate a novel light map (L map) that accounts for salient features caused by multiple light scattering around light sources. © 2007, ACM. All rights reserved.",Algorithms; attention; Experimentation; extinction map; Human Factors; light map; Participating media; perception; Performance; saliency map; selective rendering,
Calibration of Locomotion Resulting from Visual Motion in a Treadmill-Based Virtual Environment,2007,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-70349542971&doi=10.1145%2f1227134.1227138&partnerID=40&md5=4d1d24b566b118aec5098eeb9eec10a9,"This paper describes the use of a treadmill-based virtual environment (VE) to investigate the influence of visual motion on locomotion. First, we establish that a computer-controlled treadmill coupled with a wide field of view computer graphics display can be used to study interactions between perception and action. Previous work has demonstrated that humans recalibrate their visually directed actions to changing circumstances in their environment. Using a treadmill VE, we show that recalibration of action is reflected in the real world as a result of manipulating the relation between the visual indication of speed, presented using computer graphics, and the biomechanical speed of walking on a treadmill.We then extend this methodology to investigate whether the recalibration is based on perception of the speed of movement through the world or on the magnitude of optic flow itself. This was done by utilizing two different visual displays, which had essentially the same magnitude of optic flow, but which differed in the information present for the speed of forward motion. These results indicate that changes in optic flow are not necessary for recalibration to occur. The recalibration effect is dependent, at least in part, on visual perception of the speed of self-movement. © 2007, ACM. All rights reserved.",Experimentation; Human Factors; locomotion; Treadmill virtual environments; visual self-motion,
Development and Evaluation of a Thermal Display for Material Identification and Discrimination,2007,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988664908&doi=10.1145%2f1265957.1265962&partnerID=40&md5=d6ef63248ec5ea3e3e214d8046f52808,"The objective of this study was to develop and evaluate a thermal display that assists in object identification in virtual environments by simulating the thermal cues associated with making contact with materials with different thermal properties. The thermal display was developed based on a semi-infinite body model. Three experiments were conducted to evaluate the performance of the display. The first experiment compared the ability of subjects' to identify various materials, which were presented physically or simulated with the thermal display. The second experiment examined the capacity of subjects to discriminate between a real and simulated material based on thermal cues. In the third experiment, the changes in skin temperature that occurred when making contact with real and simulated materials were measured to evaluate how these compare to theoretical predictions. The results indicated that there was no significant difference in material identification and discrimination when subjects were presented with real or simulated materials. The changes in skin temperature were comparable for real and simulated materials and were related to the contact coefficient of the material palpated, consistent with the semi-infinite body model. These findings suggest that a thermal display is capable of facilitating object recognition when visual cues are limited. © 2007, ACM. All rights reserved.",Design; Experimentation; hand–object interaction; Haptic interface; Human factors; material identification; Measurement; Performance; semi-infinite body model; Theory; thermal display; thermal feedback; thermal perception; virtual environment,
Heading Assessment by “Tunnel Vision” Patients and Control Subjects Standing or Walking in a Virtual Reality Environment,2007,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84984570846&doi=10.1145%2f1227134.1227142&partnerID=40&md5=73397f92402aafb67eb0f108f34f37a8,"Virtual reality locomotion simulators are a promising tool for evaluating the effectiveness of vision aids to mobility for people with low vision. This study examined two factors to gain insight into the verisimilitude requirements of the test environment: the effects of treadmill walking and the suitability of using controls as surrogate patients. Ten “tunnel vision” patients with retinitis pigmentosa (RP)were tasked with identifying which side of a clearly visible obstacle their heading through the virtual environment would lead them and were scored both on accuracy and on their distance from the obstacle when they responded. They were tested both while walking on a treadmill and while standing, as they viewed a scene representing progress through a shopping mall. Control subjects, each wearing a head-mounted field restriction to simulate the vision of a paired patient, were also tested. At wide angles of approach, controls and patients performed with a comparably high degree of accuracy, and made their choices at comparable distances from the obstacle. At narrow angles of approach, patients' accuracy increased when walking, while controls' accuracy decreased. When walking, both patients and controls delayed their decisions until closer to the obstacle. We conclude that a head-mounted field restriction is not sufficient for simulating tunnel vision, but that the improved performance observed for walking compared to standing suggests that a walking interface (such as a treadmill) may be essential for eliciting natural perceptually guided behavior in virtual reality locomotion simulators. © 2007, ACM. All rights reserved.",Experimentation; Human Factors; Locomotion; low vision; Measurement; optic flow; rehabilitation; retinitis pigmentosa (RP); surrogates; treadmill; tunnel vision; verisimilitude; walking,
Discrimination and Identification of Finger Joint-Angle Position Using Active Motion,2007,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978901588&doi=10.1145%2f1265957.1265959&partnerID=40&md5=1f3b0286c14687468925ff2646ecfc64,"The authors report six experiments on the human ability to discriminate and identify finger joint-angle positions using active motion. The PIP (proximal interphalangeal) joint of the index finger was examined in Exps. 1–3 and the MCP (metacarpophalangeal) joint in Exps. 4–6. In Exp. 1, the just noticeable difference (JND) of PIP joint-angle position was measured when the MCP joint was either fully extended or halfway bent. In Exp. 2, the JND of PIP joint-angle position as a function of PIP joint-angle reference position was measured when the PIP joint was almost fully extended, halfway bent, or almost fully flexed. In Exp. 3, the information transfer of PIP joint-angle position was estimated with the MCP joint in a fully extended position. In Exps. 4–6, the JND and the information transfer of MCP joint-angle position were studied with a similar experimental design. The results show that the JNDs of the PIP joint-angle position were roughly constant (2.5◦−2.7◦) independent of the PIP joint-angle reference position or the MCP joint-angle position used (Exps. 1 and 2). The JNDs of the MCP joint-angle position, however, increased with the flexion of both the PIP and MCP joints and ranged from 1.7◦ to 2.7◦ (Exps. 4 and 5). The information transfer of the PIP and MCP joint-angle position were similar, indicating 3–4 perfectly identifiable joint-angle positions for both joints (Exps. 3 and 6). The results provide the basic data needed for estimating, for example, the resolution of fingertip position during active free motion. They are compared to the results from previous studies on joint position, length, and thickness perception. © 2007, ACM. All rights reserved.",discrimination; Experimentation; haptic perception; Human Factors; identification; JND; Joint position; kinesthesis; Performance,
Perception-Based Contrast Enhancement of Images,2007,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011084580&doi=10.1145%2f1278387.1278391&partnerID=40&md5=4b00454e4aba16d776e2a3951965e6c6,"Study of contrast sensitivity of the human eye shows that our suprathreshold contrast sensitivity follows the Weber Law and, hence, increases proportionally with the increase in the mean local luminance. In this paper, we effectively apply this fact to design a contrast-enhancement method for images that improves the local image contrast by controlling the local image gradient with a single parameter. Unlike previous methods, we achieve this without explicit segmentation of the image, either in the spatial (multiscale) or frequency (multiresolution) domain. We pose the contrast enhancement as an optimization problem that maximizes the average local contrast of an image strictly constrained by a perceptual constraint derived directly from theWeber Law. We then propose a greedy heuristic, controlled by a single parameter, to approximate this optimization problem. © 2007, ACM. All rights reserved.",Color; Contrast; contrast enhancement; contrast sensitivity; Displays; Human perception; Perception,
Modeling Embodied Visual Behaviors,2007,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013237108&doi=10.1145%2f1265957.1265960&partnerID=40&md5=80acab0a73374dc0a2bbc31f162132d5,"To make progess in understanding human visuomotor behavior, we will need to understand its basic components at an abstract level. One way to achieve such an understanding would be to create a model of a human that has a sufficient amount of complexity so as to be capable of generating such behaviors. Recent technological advances have been made that allow progress to be made in this direction. Graphics models that simulate extensive human capabilities can be used as platforms from which to develop synthetic models of visuomotor behavior. Currently, such models can capture only a small portion of a full behavioral repertoire, but for the behaviors that they do model, they can describe complete visuomotor subsystems at a useful level of detail. The value in doing so is that the body's elaborate visuomotor structures greatly simplify the specification of the abstract behaviors that guide them. The net result is that, essentially, one is faced with proposing an embodied “operating system” model for picking the right set of abstract behaviors at each instant. This paper outlines one such model. A centerpiece of the model uses vision to aid the behavior that has the most to gain from taking environmental measurements. Preliminary tests of the model against human performance in realistic VR environments show that main features of the model show up in human behavior. © 2007, ACM. All rights reserved.",Experimentation; reinforcement learning; Theory; visual attention; Visual routines,
Gender Differences in Cue Preference During Path Integration in Virtual Environments,2007,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77950654240&doi=10.1145%2f1227134.1227140&partnerID=40&md5=5729537c99db968c1358e2f34e4f2cd7,"Three studies were conducted to examine whether men and women differ in how they recalibrate their path-integration systems when walking without vision in virtual environments. Distance cues provided by a scene and a tone, which ended each trial, were placed in conflict. Participants briefly viewed a room with a target, which was offset from their midlines and hung inside a doorframe on the far wall. After viewing, participants walked to the target's position until a tone sounded, ending the trial. In two experiments the doorframe was placed at 6 m and the tone sounded at 4 or 8 m. The rooms had minimal or photorealistic texturing applied. The third experiment used photorealistic texturing, but here the tone sounded at 6 m and the doorframe was presented at 4 or 8 m. Path angles were recorded to estimate perceived distance to the target. In all conditions tested, the women failed to scale their path angles. The men, however, scaled their path-angles with the auditory cue in the minimal-texture condition, but with the visual cue in the photorealistic-texture conditions. These results suggest that gender differences exist in the way that humans recalibrate their path-integration systems when walking without vision in virtual environments. © 2007, ACM. All rights reserved.",cue preference; egocentric reference frame; Experimentation; Gender differences; Human Factors; path integration,
Using Virtual Environments to Assess Time-to-Contact Judgments from Pedestrian Viewpoints,2007,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-70349264825&doi=10.1145%2f1278387.1278392&partnerID=40&md5=f281f7d24faa392f70aa14ccd02c0a21,"This paper describes the use of desktop and immersive virtual environments to study judgments that pedestrians make when deciding to cross a street. In particular, we assess the ability of people to discriminate and estimate time-to-contact (TTC) for approaching vehicles under a variety of conditions. Four experiments observing TTC judgments under various conditions are described. We examine the effect of type of vehicle, viewpoint, presentation mode, and TTC value on TTC judgments. We find no significant effect of type of vehicle or of viewpoint, extending prior work to cover all views typically encountered by pedestrians. Discrimination of short values for TTC judgments is generally consistent with the literature, but performance degrades significantly for long TTC values. Finally, we find no significant difference between judgments made in a desktop environment versus a head-mounted display, indicating that tracking the approaching vehicle with one's head does not aid discrimination. In general, people appear to use strategies similar to those that pedestrians use to make real-world, streetcrossing decisions. © 2007, ACM. All rights reserved.",Experimentation; Human Factors; Measurement; time-to-contact (TTC); Virtual reality (VR),
Step Frequency and Perceived Self-Motion,2007,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011346039&doi=10.1145%2f1227134.1227139&partnerID=40&md5=30e59e41b91d812771f745887545165c,"There is a discrepancy between the ability to correctly match the gains of visual and motor speed in virtual reality (VR) when walking on solid ground and the failure of this ability when walking on a treadmill. Moreover, this discrepancy has been found to interact with effects of the structure of the visual environment. The authors used a high-fidelity treadmill VR system to reproduce the high interactivity of normal walking in wide-area VR. Under these conditions, it was found that gain matches in a richly structured near environment differ by only about 10% in treadmill VR from matches in wide-area VR and that trial-to-trial variations in step frequency predicted changes in perceived locomotor speed. Gait differences resulting from treadmill walking (which are shown not to be a product of wearing a head-mounted display), apparently lead to an overestimation of motor speed on treadmills. When the near visual environment represented an empty hallway, additional errors were present that could be accounted for by known illusions in the perception of visual speed during self-motion. A study of normal gait at different speeds measured by head-tracker is reported as evidence of other possible sources of perceptual estimates of locomotor speed in normal walking. © 2007, ACM. All rights reserved.",Experimentation; gait; head-mounted displays (HMD); Human Factors; Locomotion; Perception; treadmill; virtual reality (VR); walk ratio,
Functional Similarities in Spatial Representations Between Real and Virtual Environments,2007,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979974878&doi=10.1145%2f1265957.1265961&partnerID=40&md5=cf3df12ebf9d8ac6b6a15d3aa2126c57,"This paper presents results that demonstrate functional similarities in subjects' access to spatial knowledge (or spatial representation) between real and virtual environments. Such representations are important components of the transfer of reasoning ability and knowledge between these two environments. In particular, we present two experiments aimed at investigating similarities in spatial knowledge derived from exploring on foot both physical environments and virtual environments presented through a head-mounted display. In the first experiment, subjects were asked to learn the locations of target objects in the real or virtual environment and then rotate the perspective by either physically locomoting to a new facing direction or imagining moving. The latencies and errors were generally worse after imagining locomoting and for greater degrees of rotation in perspective; they did not differ significantly across knowledge derived from exploring the physical versus virtual environments. In the second experiment, subjects were asked to imagine simple rotations versus simple translations in perspective. The errors and latencies indicated that the to-be-imagined disparity was linearly related after learning the physical and virtual environment. These results demonstrate functional similarities in access to knowledge of new perspective when it is learned by exploring physical environments and virtual renderings of the same environment. © 2007, ACM. All rights reserved.",Experimentation; Human Factors; Measurement; space perception; Virtual reality (VR),
Evaluating Hdr Rendering Algorithms,2007,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84975188047&doi=10.1145%2f1265957.1265958&partnerID=40&md5=7b57ba0671981721db355b3f2ba99dc2,"A series of three experiments has been performed to test both the preference and accuracy of high dynamic-range (HDR) rendering algorithms in digital photography application. The goal was to develop a methodology for testing a wide variety of previously published tone-mapping algorithms for overall preference and rendering accuracy. A number of algorithms were chosen and evaluated first in a paired-comparison experiment for overall image preference. A rating-scale experiment was then designed for further investigation of individual image attributes that make up overall image preference. This was designed to identify the correlations between image attributes and the overall preference results obtained from the first experiments. In a third experiment, three real-world scenes with a diversity of dynamic range and spatial configuration were designed and captured to evaluate seven HDR rendering algorithms for both of their preference and accuracy performance by comparing the appearance of the physical scenes and the corresponding tone-mapped images directly. In this series of experiments, a modified Durand and Dorsey's bilateral filter technique consistently performed well for both preference and accuracy, suggesting that it is a good candidate for a common algorithm that could be included in future HDR algorithm testing evaluations. The results of these experiments provide insight for understanding of perceptual HDR image rendering and should aid in design strategies for spatial processing and tone mapping. The results indicate ways to improve and design more robust rendering algorithms for general HDR scenes in the future. Moreover, the purpose of this research was not simply to find out the “best” algorithms, but rather to find a more general psychophysical experiment based methodology to evaluate HDR image-rendering algorithms. This paper provides an overview of the many issues involved in an experimental framework that can be used for these evaluations. © 2007, ACM. All rights reserved.",Algorithms; Experimentation; High dynamic-range imaging; Human Factors; psychophysical experiments; tone-mapping algorithms evaluation,
Obstacle Avoidance During Walking in Real and Virtual Environments,2007,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84980025512&doi=10.1145%2f1227134.1227136&partnerID=40&md5=5c76da40635f56d3f0055f7c40814ce6,"Immersive virtual environments are a promising research tool for the study of perception and action, on the assumption that visual–motor behavior in virtual and real environments is essentially similar. We investigated this issue for locomotor behavior and tested the generality of Fajen and Warren's [2003] steering dynamics model. Participants walked to a stationary goal while avoiding a stationary obstacle in matched physical and virtual environments. There were small, but reliable, differences in locomotor paths, with a larger maximum deviation (Δ = 0.16 m), larger obstacle clearance (Δ = 0.16 m), and slower walking speed (Δ = 0.13 m/s) in the virtual environment. Separate model fits closely captured the mean virtual and physical paths (R2 > 0.98). Simulations implied that the path differences are not because of walking speed or a 50% distance compression in virtual environments, but might be a result of greater uncertainty about the egocentric location of virtual obstacles. On the other hand, paths had similar shapes in the two environments with no difference in median curvature and could be modeled with a single set of parameter values (R2 > 0.95). Fajen and Warren's original parameters successfully generalized to new virtual and physical object configurations (R2 > 0.95). These results justify the use of virtual environments to study locomotor behavior. © 2007, ACM. All rights reserved.",Experimentation; Human Factors; Locomotion; modeling; Perception; virtual reality,
Estimation of Travel Distance from Visual Motion in Virtual Environments,2007,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85001592212&doi=10.1145%2f1227134.1227137&partnerID=40&md5=63aa59ddf0fad896bd36aef59691143e,"Distance estimation of visually simulated self-motion is difficult, because one has to know or make assumptions about scene layout to judge ego speed. Discrimination of the travel distances of two sequentially simulated self-motions in the same scene can be performed quite accurately (Bremmer and Lappe 1999; Frenz et al., 2003). However, the indication of the perceived distance of a single movement in terms of a spatial interval results in a depth scaling error: Intervals are correlated with the true travel distance, but underestimate travel distance by about 25% (Frenz and Lappe, 2005). Here we investigated whether the inclusion of further depth cues (disparity/motion parallax/figural cues) in the virtual environment allows more veridical interval adjustment. Experiments were conducted on a large single projection screen and in a fully immersive computer-animated virtual environment (CAVE). Forward movements in simple virtual environments were simulated with distances between 1.5 and 13 m with varying speeds. Subjects indicated the perceived distance of each movement in terms of a depth interval on the virtual ground plane. We found good correlation between simulated and indicated distances, indicative of an internal representation of the perceived distance. The slopes of the fitted regression lines revealed an underestimation of distance by about 25% under all conditions. We conclude that estimation of travel distance from optic flow is subject to scaling when compared to static intervals in the environment, irrespective of additional depth cues. © 2007, ACM. All rights reserved.",depth; distance estimation; Experimentation; Human factors; Optic flow; stereo; virtual reality,
Evaluation of Real-World and Computer-Generated Stylized Facial Expressions,2007,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011991997&doi=10.1145%2f1278387.1278390&partnerID=40&md5=13ec7f5a0bf741349c82ad0dcd173fde,"The goal of stylization is to provide an abstracted representation of an image that highlights specific types of visual information. Recent advances in computer graphics techniques have made it possible to render many varieties of stylized imagery efficiently making stylization into a useful technique, not only for artistic, but also for visualization applications. In this paper, we report results from two sets of experiments that aim at characterizing the perceptual impact and effectiveness of three different stylization techniques in the context of dynamic facial expressions. In the first set of experiments, animated facial expressions are stylized using three common techniques (brush, cartoon, and illustrative stylization) and investigated using different experimental measures. Going beyond the usual questionnaire approach, these experiments compare the techniques according to several criteria ranging from subjective preference to task-dependent measures (such as recognizability, intensity) allowing us to compare behavioral and introspective approaches. The second set of experiments use the same stylization techniques on real-world video sequences in order to compare the effect of stylization on natural and artificial stimuli. Our results shed light on how stylization of image contents affects the perception and subjective evaluation of both real and computer-generated facial expressions. © 2007, ACM. All rights reserved.",avatar; Evaluation of facial animations; Experimentation; facial expressions; perceptually adaptive graphics; psychophysics; stylization,
Evaluation of a Multiscale Color Model for Visual Difference Prediction,2006,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-38549120661&doi=10.1145%2f1166087.1166089&partnerID=40&md5=9721250b87a3383fce42542da2bfc719,"How different are two images when viewed by a human observer? There is a class of computational models which attempt to predict perceived differences between subtly different images. These are derived from theoretical considerations of human vision and are mostly validated from psychophysical experiments on stimuli, such as sinusoidal gratings. We are developing a model of visual difference prediction, based on multiscale analysis of local contrast, to be tested with psychophysical discrimination experiments on natural-scene stimuli. Here, we extend our model to account for differences in the chromatic domain by modeling differences in the luminance domain and in two opponent chromatic domains. We describe psychophysical measurements of objective (discrimination thresholds) and subjective (magnitude estimations) perceptual differences between visual stimuli derived from colored photographs of natural scenes. We use one set of psychophysical data to determine the best parameters for the model and then determine the extent to which the model generalizes to other experimental data. In particular, we show that the cues from different spatial scales and from the separate luminance and chromatic channels contribute roughly equally to discrimination and that these several cues are combined in a relatively straightforward manner. In general, the model provides good predictions of both threshold and suprathreshold image differences arising from a wide variety of geometrical and optical manipulations. This implies that models of this class can be generally useful in specifying how different two similar images will look to human observers. © 2006, ACM. All rights reserved.",color vision; Experimentation; Human Factors; image difference metrics; Psychophysical testing,
Region-Based Representations for Face Recognition,2006,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-57649211067&doi=10.1145%2f1190036.1190038&partnerID=40&md5=061cc49902d134ccf523940ad84f662f,"Face recognition is one of the most important applied aspects of visual perception. To create an automated face-recognition system, the fundamental challenge is that of finding useful features. In this paper, we suggest a new class of image features that may be a useful addition to the set of representational tools for face-recognition tasks. Our proposal is motivated by the observation that rather than relying exclusively on traditional edge-based image representations, it may be useful to also employ region-based strategies that can compare noncontiguous image regions. The spatial homogeneity within regions allows for enhanced tolerance to geometric distortions and greater freedom in the choice of sample points. We first show that under certain circumstances, comparisons between spatially disjoint image regions are, on average, more valuable for recognition than features that measure local contrast. Second, we learn “optimal” sets of region comparisons for recognizing faces across varying pose and illumination. We propose a representational primitive—the dissociated dipole—that permits an integration of edge-based and region-based representations. This primitive is then evaluated using the FERET database of face images and then compared to established local and global algorithms. © 2006, ACM. All rights reserved.",Design; Face recognition,
Haptic Discrimination of Force Direction and the Influence of Visual Information,2006,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85010564703&doi=10.1145%2f1141897.1141901&partnerID=40&md5=9686e02ff26ab3cba564f6c53fe90df3,"Despite a wealth of literature on discrimination thresholds for displacement, force magnitude, stiffness, and viscosity, there is currently a lack of data on our ability to discriminate force directions. Such data are needed in designing haptic rendering algorithms where force direction, as well as force magnitude, are used to encode information such as surface topography. Given that haptic information is typically presented in addition to visual information in a data perceptualization system, it is also important to investigate the extent to which the congruency of visual information affects force-direction discrimination. In this article, the authors report an experiment on the discrimination threshold of force directions under the three display conditions of haptics alone (H), haptics plus congruent vision (HVcong), and haptics plus incongruent vision (HVincong). Average force-direction discrimination thresholds were found to be 18.4., 25.6., and 31.9. for the HVcong, H and HVincong conditions, respectively. The results show that the congruency of visual information significantly affected haptic discrimination of force directions, and that the force-direction discrimination thresholds did not seem to depend on the reference force direction. The implications of the results for designing haptic virtual environments, especially when the numbers of sensors and actuators in a haptic display do not match, are discussed. © 2006, ACM. All rights reserved.",Discrimination threshold; Experimentation; Force direction; Haptic and visual; Haptic rendering; Human Factors; Interaction; Psychophysics,
Object Feature Validation Using Visual and Haptic Similarity Ratings,2006,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979998071&doi=10.1145%2f1166087.1166093&partnerID=40&md5=c348fd7396839490b28ba276e95fd084,"The perceived similarity between objects may well vary according to the sensory modality/modalities in which they are experienced, an important consideration for the design of multimodal interfaces. In this study, we present a similarity-based method for comparing the perceptual importance of object properties in touch and in vision and show how the method can also be used to validate computational measures of object properties. Using either vision or touch, human subjects judged the similarity between novel, 3D objects which varied parametrically in shape and texture. Similarities were also computed using a set of state-of-the art 2D and 3D computational measures. Two resolutions of 2D and 3D object data were used for these computations in order to test for scale dependencies. Multidimensional scaling (MDS) was then performed on all similarity data, yielding maps of the stimuli in both perceptual and computational spaces, as well as the relative weight of shape and texture dimensions. For this object set, we found that visual subjects accorded more importance to shape than texture, while haptic subjects weighted them roughly evenly. Fit errors between human and computational maps were then calculated to assess each feature's perceptual validity. Shape-biased features provided good overall fits to the human visual data; however, no single feature yielded a good overall fit to the haptic data, in which we observed large individual differences. This work demonstrates how MDS techniques can be used to evaluate computational object features using the criterion of perceptual similarity. It also demonstrates a way of assessing how the perceptual validity of a feature varies as a function of parameters such as the target modality and the resolution of object data. Potential applications of this method for the design of unimodal and multimodal human–machine interfaces are discussed. © 2006, ACM. All rights reserved.",Experimentation; features; haptic; Human Factors; Measurement; multidimensional scaling; perception; shape; Similarity; texture; touch; validation; vision,
Perceived Quality of Compressed Stereoscopic Images: Effects of Symmetric and Asymmetric JPEG Coding and Camera Separation,2006,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024281581&doi=10.1145%2f1141897.1141899&partnerID=40&md5=7dd36b20b069fc51ccabf542c05eb74c,"JPEG compression of the left and right components of a stereo image pair is a way to save valuable bandwidth when transmitting stereoscopic images. This paper presents results on the effects of camera-base distance (B) and JPEG coding on overall image quality, perceived depth, perceived sharpness, and perceived eye strain. In the experiment, two stereoscopic still scenes were used, varying in depth (three different camera-base distances: 0, 8, and 12 cm) and compression ratio (4 levels: original, 1:30, 1:40, and 1:60). All levels of compression were applied to both the left and right stereo image, resulting in a 4 × 4 matrix of all possible symmetric and asymmetric coding combinations. The observers were asked to assess image quality, sharpness, depth, and eye strain. Results showed that an increase in JPEG coding had a negative effect on image quality, sharpness, and eye strain, but had no effect on perceived depth. An increase in camera-base distance increased perceived depth and reported eye strain, but had no effect on perceived sharpness. Results on asymmetric and symmetric coding showed that the relationship between perceived image quality and average bit rate is not straightforward. In some cases, image quality ratings of a symmetric coded pair can be higher than for an asymmetric coded pair, even if the averaged bit rate for the symmetric pair is lower, than for the asymmetric pair. Furthermore, sharpness and eye strain correlated highly and medium, respectively, with perceived image quality. © 2006, ACM. All rights reserved.",Asymmetric/Symmetric Jpeg Coding; Depth; Eye Strain; Human Factors; Image Quality; Images; Sharpness; Stereoscopic,
A Perceptually Based Spectral Model for Isotropic Textures,2006,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-70450234162&doi=10.1145%2f1190036.1190039&partnerID=40&md5=c91855406ded058207a15a70675c1422,"Color is routinely used to visualize interval and ordinal data, while texture is not. For color, a variety of perceptually based models is available, which can be used to convey data via straightforward mapping. The dimensionality of texture is less well understood and there are almost no perceptually based and validated models available to generate textures on demand. We present a perceptually based texture synthesis model for isotropic textures. The model uses additive synthesis of band-limited noise in the spectral domain, comparable to the RGB (red, green, blue) model for color. Via user experiments, we have derived a three-dimensional model to control the amplitudes per band in a perceptually intuitive way, comparable to the HSV (hue, saturation, value) model for color. The three dimensions used are contrast, spatial frequency, and spectral purity. Besides a mapping to the amplitudes, we have derived a mapping to a perceptually equidistant space. We show how such textures can be combined with color; applications are presented. © 2006, ACM. All rights reserved.",Design; Experimentation; multivariate visualization; perception; Textures; user studies in visualization,
An Application of Eyegaze Tracking for Designing Radiologists' Workstations: Insights for Comparative Visual Search Tasks,2006,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015778214&doi=10.1145%2f1141897.1141902&partnerID=40&md5=d5320da27b21329a65453cb5d4238e11,"The goal of this research is to use eyegaze tracking data to provide insights into designing radiology workstations. We designed a look-alike radiology task with artificial stimuli. The task involved a comparative visual search of two side-by-side images, using two different interaction techniques. We tracked the eyegaze of four radiologists while they performed the task and measured the duration of the fixations on the controls, the left and right images, and on the artificial targets. Response time differences between the two interaction techniques exceeded the differences of fixations on the controls. Fixations on the left-side images are longer than the right-side images, and the search for multifeatured targets occurs in two phases: first a regular scan path search phase for a likely target and then a confirmation phase of several fixations on the target in each side-by-side image. We conclude that eyegaze tracking shows that disruption of visual search leads to cognitive disruption; subjects use the left image as a reference image and multiple saccades between left and right side images are necessary, because of the limitations of the visual working memory. © 2006, ACM. All rights reserved.",Cognition; Comparative visual search; Design; Eyegaze tracking; Human Factors; Human-computerinteraction; Measurement; Radiologist productivity; User interface evaluation,
A Perceptual Framework for Contrast Processing of High Dynamic Range Images,2006,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85001797893&doi=10.1145%2f1166087.1166095&partnerID=40&md5=84c13211b2970337a841f096649f2f95,"Image processing often involves an image transformation into a domain that is better correlated with visual perception, such as the wavelet domain, image pyramids, multiscale-contrast representations, contrast in retinex algorithms, and chroma, lightness, and colorfulness predictors in color-appearance models.Many of these transformations are not ideally suited for image processing that significantly modifies an image. For example, the modification of a single band in a multiscale model leads to an unrealistic image with severe halo artifacts. Inspired by gradient domain methods, we derive a framework that imposes constraints on the entire set of contrasts in an image for a full range of spatial frequencies. This way, even severe image modifications do not reverse the polarity of contrast. The strengths of the framework are demonstrated by aggressive contrast enhancement and a visually appealing tone mapping, which does not introduce artifacts. In addition, we perceptually linearize contrast magnitudes using a custom transducer function. The transducer function has been derived especially for the purpose of HDR images, based on the contrast-discrimination measurements for high-contrast stimuli. © 2006, ACM. All rights reserved.",Algorithms; contrast discrimination; contrast masking; contrast processing; high dynamic range; Human Factors; tone mapping; transducer; Visual perception,
Exploring Visual and Automatic Measures of Perceptual Fidelity in Real and Simulated Imagery,2006,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84977837247&doi=10.1145%2f1166087.1166092&partnerID=40&md5=5325c84746f200dea979b137cc8e1949,"This paper introduces a new psychophysical experiment developed to enable observers to judge the quality of computer graphics imagery with respect to the real scene it depicts. This new framework facilitates perceptual judgment of images against a real scene. Unlike previous work, which examined primitive objects under basic illumination, this experiment examines complex geometry illuminated using a calibrated light source. To ensure valid results, a commercial lighting booth containing rapid prototyped three-dimensional (3D) objects serves as the real scene. For comparison, a series of representative images, of varying quality, were rendered using the physically based Radiance lighting simulation software. Results from these experiments show that higher parameter settings, which lead to longer processing times, do not necessarily lead to higher quality images. To demonstrate that there is only modest benefit to setting parameters higher, images are subjected to further testing using two different visual quality discrimination operators; the Visual Differences Predictor (VDP) and the Structural SIMilarity (SSIM) for image-quality assessment. The results from the automatic operators correspond well with each other, in addition to yielding comparable outcomes as the psychophysical experiment. Although, a single scene was considered in the experiment, several scenes are tested using the image-quality metrics to lend further reliability to the assertion that higher parameter settings, which lead to extended processing times, do not necessarily lead to superior quality results. © 2006, ACM. All rights reserved.",Experimentation; image quality; Measurement; Psychophysics; Verification,
Simultaneous Measurement of Steering Performance and Perceived Heading on a Curving Path,2006,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-63549100708&doi=10.1145%2f1141897.1141898&partnerID=40&md5=463ef7158b0901694afda2d10833bd53,"The ability to judge heading (direction of travel) has been the focus of much research, but a role for perceived heading in steering has not been firmly established. Subjects steered down a road consisting of straight and curved segments and made heading judgments along the way. Heading judgments while traversing curved road segments were biased in the direction of the curve by up to 5. and position errors on the same curved roads were highly correlated with heading biases. This correlation was revealed by the simultaneous measurement of steering performance and perceived heading. © 2006, ACM. All rights reserved.",Driving; Experimentation; Heading perception; Human Factors; Optic flow; Performance; Psychophysics; Steering,
The Perceived Roughness of Resistive Virtual Textures: I. Rendering by a Force-Feedback Mouse,2006,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34250029542&doi=10.1145%2f1119766.1119767&partnerID=40&md5=caa4ec4ee8d420bb7afbe23779369730,"In previous work, we demonstrated that people reliably perceive variations in surface roughness when textured surfaces are explored with a rigid link between the surface and the skin [e.g., Klatzky and Lederman 1999; Klatzky et al. 2003]. Parallel experiments here investigated the potential of a force-feedback mouse to render surfaces varying in roughness. The stimuli were surfaces with alternating regions of high and low resistance to movement in the x (frontal) dimension (called ridges and grooves, respectively). Experiment 1 showed that magnitude ratings of roughness varied systematically with the spatial period of the resistance variation. Experiments 2 and 3 used a factorial design to disentangle the contributions of ridge and groove width. The stimuli constituted eight values of groove width at each of five levels of ridge width (Experiment 2) or the reverse (Experiment 3). Roughness magnitude increased with ridge width while remaining essentially invariant over groove width. Kinematic variations in exploration were observed across the surfaces. The data point to the promise of using inexpensive devices to create virtual textural variations under conditions of unconstrained exploration. © 2006, ACM. All rights reserved.",Artificial; Augmented; Design; Ergonomics; Evaluation/Methodology; Haptics; Haptics I/O; Human Factors; Texture Perception; User-Centered; Virtual Realities; Virtual Reality,
Personal Space in Virtual Reality,2006,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978507735&doi=10.1145%2f1190036.1190041&partnerID=40&md5=6daae90a0e320511dcabe57bc0ce1caf,"Improving the sense of “presence” is a common goal of three-dimensional (3D) display technology for film, television, and virtual reality. However, there are instances in which 3D presentations may elicit unanticipated negative responses. For example, it is well established that violations of interpersonal space cause discomfort in real-world situations. Here we ask if people respond similarly when viewing life-sized stereoscopic images. Observers rated their level of comfort in response to animate and inanimate objects in live and virtual (stereoscopic projection) viewing conditions. Electrodermal activity was also recorded to monitor their physiological response to these stimuli. Observers exhibited significant negative reactions to violations of interpersonal space in stereoscopic 3D displays, which were equivalent to those experienced in the natural environment. These data have important implications for the creation of 3D media and the use of virtual reality systems. © 2006, ACM. All rights reserved.",Experimentation; Human Factors; personal space; Stereoscopic projection; virtual reality,
Methods for the Assessment of Fused Images,2006,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015247951&doi=10.1145%2f1166087.1166096&partnerID=40&md5=c3899e710ac30db302d71f9729aadb53,"The prevalence of image fusion ‘the combination of images of different modalities, such as visible and infrared radiation’ has increased the demand for accurate methods of image-quality assessment. The current study used a signal-detection paradigm, identifying the presence or absence of a target in briefly presented images followed by an energy mask, which was compared with computational metric and subjective quality assessment results. In Study 1, 18 participants were presented with fused infrared-visible light images, with a soldier either present or not. Two independent variables, image-fusion method (averaging, contrast pyramid, dual-tree complex wavelet transform) and JPEG compression (no compression, low and high compression), were used in a repeated-measures design. Participants were presented with images and asked to state whether or not they detected the target. In addition, subjective ratings and metric results were obtained. This process was repeated in Study 2, using JPEG2000 compression. The results showed a significant effect for fusion but not compression in JPEG2000 images, while JPEG images showed significant effects for both fusion and compression. Subjective ratings differed, especially for JPEG2000 images, while metric results for both JPEG and JPEG2000 showed similar trends. These results indicate that objective and subjective ratings can differ significantly, and subjective ratings should, therefore, be used with care. © 2006, ACM. All rights reserved.",Computational metrics; Design; Human Factors; Image fusion; Performance; Psychophysical testing; Subjective quality,
A Reality Check for Tone-Mapping Operators,2006,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-45449090083&doi=10.1145%2f1190036.1190040&partnerID=40&md5=bcc0538d64dce8a36396911ea482c445,"A large number of high-quality tone-mapping operators is currently available. In addition to inherent practical value, comparing their performance is necessary to further advance the field and can provide better understanding of visual realism. In this paper, we show that it becomes very difficult to meaningfully judge relative performance of modern tone-mapping techniques with existing comparison methods and demonstrate that using real environments is crucial in such experiments. We report results of a new study comparing five recent tone-mapping techniques using this approach. © 2006, ACM. All rights reserved.",Experimentation; Human factors; Image realism; tone mapping; visual perception,
Perceptual Limits on 2D Motion-Field Visualization,2006,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85012011117&doi=10.1145%2f1166087.1166090&partnerID=40&md5=fdeeff7be4535c101a19047da7c39237,"This paper examines perceptual issues in 2D motion-field visualization. Several aspects of motion-field perception are considered, including sensitivity to spatial gradients, number of motion layers, and motion blur. Our analysis concentrates on a specific popular method for 2D flow visualization, namely, line integral convolution (LIC). Using 2D spectral analysis, we examine a tradeoff that arises in dynamic LIC between the static motion blur cue, which indicates motion direction and the dynamic cue which indicates image speed. We also present a 2D spectral synthesis method for motion-field visualization, along with several examples. The synthesis method is simple to implement and to analyze in the frequency domain, which makes it a convenient tool for studying the perception of complex motion fields. © 2006, ACM. All rights reserved.",Algorithms; Experimentation; flow visualization; Human Factors; line integral convolution; Motion layers; psychophysics; spectral synthesis,
Cognitive Factors Can Influence Self-Motion Perception (Vection) in Virtual Reality,2006,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78651231836&doi=10.1145%2f1166087.1166091&partnerID=40&md5=2e487b8e667b069f06dabc30a045ceb6,"Research on self-motion perception and simulation has traditionally focused on the contribution of physical stimulus properties (“bottom-up factors”h) using abstract stimuli. Here, we demonstrate that cognitive (“top-down”h) mechanisms like ecological relevance and presence evoked by a virtual environment can also enhance visually induced self-motion illusions (vection). In two experiments, naive observers were asked to rate presence and the onset, intensity, and convincingness of circular vection induced by different rotating visual stimuli presented on a curved projection screen (FOV: 54◦×45◦). Globally consistent stimuli depicting a natural 3D scene proved more effective in inducing vection and presence than inconsistent (scrambled) or unnatural (upside-down) stimuli with similar physical stimulus properties. Correlation analyses suggest a direct relationship between spatial presence and vection.We propose that the coherent pictorial depth cues and the spatial reference frame evoked by the naturalistic environment increased the believability of the visual stimulus, such that it was more easily accepted as a stable “scene”h with respect to which visual motion is more likely to be judged as self-motion than object motion. This work extends our understanding of mechanisms underlying self-motion perception and might thus help to improve the effectiveness and believability of virtual reality applications. © 2006, ACM. All rights reserved.",Ego–motion simulation; Experimentation; Human factors; Measurement; psychophysics; spatial orientation; spatial presence; vection; virtual reality,
Sketching Shiny Surfaces: 3D Shape Extraction and Depiction of Specular Surfaces,2006,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84992181765&doi=10.1145%2f1166087.1166094&partnerID=40&md5=a2a9333b95065b0028f01c92c053c595,"Many materials including water, plastic, and metal have specular surface characteristics. Specular reflections have commonly been considered a nuisance for the recovery of object shape. However, the way that reflections are distorted across the surface depends crucially on 3D curvature, suggesting that they could, in fact, be a useful source of information. Indeed, observers can have a vivid impression of, 3D shape when an object is perfectly mirrored (i.e., the image contains nothing but specular reflections). This leads to the question what are the underlying mechanisms of our visual system to extract this 3D shape information from a perfectly mirrored object. In this paper we propose a biologically motivated recurrent model for the extraction of visual features relevant for the perception of 3D shape information from images of mirrored objects.We qualitatively and quantitatively analyze the results of computational model simulations and show that bidirectional recurrent information processing leads to better results than pure feedforward processing. Furthermore, we utilize the model output to create a rough nonphotorealistic sketch representation of a mirrored object, which emphasizes image features that are mandatory for 3D shape perception (e.g., occluding contour and regions of high curvature). Moreover, this sketch illustrates that the model generates a representation of object features independent of the surrounding scene reflected in the mirrored object. © 2006, ACM. All rights reserved.",3D shape perception; human visual perception; Nonphotorealistic rendering; perfectly specular surfaces,
Evaluation of Spatial Displays for Navigation without Sight,2006,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016006527&doi=10.1145%2f1141897.1141900&partnerID=40&md5=50a03d5e86b1bf3101b9a6100e410413,"We report on two route guidance tasks using a highly accurate GPS receiver. Eight participants who were visually impaired or blind traveled two routes, one on a city sidewalk, and one in a city park.We tested and compared two types of spatial output devices that give route guidance information. One output display used a hand-held pointer, using a standard Talking Signs receiver that integrated the GPS signal information with the Talking Signs® signal information. This device gave travel instructions and oncourse confirmation when pointed in the proper direction. The other spatial display used auditory virtual reality that presented the audible spatial information (waypoint direction and distance) through small air-tubes inserted into the ear. Travel times, distance, and errors were recorded. In addition, we tested users ‘ability to find precise locations, such as the intersections of small paths and a bus stop pole. Various subjective ratings were collected about blind participants’ needs and perception of the various display and output options that they used. All subjects completed the tasks with both output displays, found all the waypoints and locations, and rated the two displays highly. The virtual sound display produced superior times overall and received slightly higher favorable ratings. © 2006, ACM. All rights reserved.",Artificial; Auditory I/O; Augmented; Blind Navigation; Design; Ergonomics; Evaluation/Methodology; Gps; Haptic I/O; Personal Guidance System; User-Centered; Virtual Realities; Voice I/O,
Image Retrieval and Perceptual Similarity,2006,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988573796&doi=10.1145%2f1119766.1119769&partnerID=40&md5=efe1f7cb84c85e70ff8ca239ecc8d20f,"Simple, low-level visual features are extensively used for content-based image retrieval. Our goal was to evaluate an imageindexing system based on some of the known properties of the early stages of human vision. We quantitatively measured the relationship between the similarity order induced by the indexes and perceived similarity. In contrast to previous evaluation approaches, we objectively measured similarity both for the few best-matching images and also for relatively distinct images. The results show that, to a large degree, the rank orders induced by the indexes predict the perceived similarity between images. The highest index concordance employing a single index was obtained using the chromaticity histogram. Combining different information sources substantially improved the correspondence with the observers. We conclude that image-indexing systems can provide useful measures for perceptual image similarity. The methods presented here can be used to evaluate and compare different image-retrieval systems. © 2006, ACM. All rights reserved.",Color indexing; Content-based image retrieval; Experimentation; Fourier spectrum; Image search,
Visual Calibration and Correction for Ambient Illumination,2006,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014131023&doi=10.1145%2f1190036.1190042&partnerID=40&md5=858bb2a41839977b84c41569cc0fc449,"Many applications require that an image will appear the same regardless of where or how it is displayed. However, the conditions in which an image is displayed can adversely affect its appearance. Computer monitor screens not only emit light, but can also reflect extraneous light present in the viewing environment. This can cause images displayed on a monitor to appear faded by reducing their perceived contrast. Current approaches to this problem involve measuring this ambient illumination with specialized hardware and then altering the display device or changing the viewing conditions. This is not only impractical, but also costly and time consuming. For a user who does not have the equipment, expertise, or budget to control these facets, a practical alternative is sought. This paper presents a method whereby the display device itself can be used to determine the effect of ambient light on perceived contrast, thus enabling the viewers themselves to perform visual calibration. This method is grounded in established psychophysical experimentation and we present both an extensive procedure and an equivalent rapid procedure. Our work is extended by providing a novel method of contrast correction so that the contrast of an image viewed in bright conditions can be corrected to appear the same as an image viewed in a darkened room. This is verified through formal validation. These methods are easy to apply in practical settings, while accurate enough to be useful. © 2006, ACM. All rights reserved.",ambient illumination; contrast correction; device independence; ergonomics; Experimentation; Human factors; perceptually accurate display; reflections; Standardization; Verification; Viewing conditions,
The Perceived Roughness of Resistive Virtual Textures: II. Effects of Varying Viscosity with a Force-Feedback Device,2006,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34249998483&doi=10.1145%2f1119766.1119768&partnerID=40&md5=3acd5ea73cb72fc8ae30ab0b6ce279c1,"Klatzky and Lederman [2006] have shown that tangential resistive forces may be used to convey roughness in virtual textures using the WingMan force-feedback mouse. Modeling our experiment after this study, we directly examined the effect of viscous resistance on the perceived roughness magnitude of virtual gratings using a PHANTOM. For each virtual grating, the resistance level encountered at the ridgeswas varied by altering the viscosity coefficient. Perceived roughness systematically increased as the value of the viscosity coefficient was increased. The ridge-to-groove ratio contributed a small additional effect of microgeometry. These results suggest that simple models of viscous resistance may be used to simulate varying levels of surface roughness. © 2006, ACM. All rights reserved.",Artificial; Augmented; Design; Ergonomics; Evaluation/Methodology; Experimentation; Haptics; Haptics I/O; Human Factors; Performance; Texture perception; User-Centered; Virtual Realities; Virtual reality,
A Psychophysically Plausible Model for Typicality Ranking of Natural Scenes,2006,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-63049090639&doi=10.1145%2f1190036.1190037&partnerID=40&md5=10fa8e20cdc73e52e3d334b5767068ba,"Natural scenes constitute a very heterogeneous stimulus class. Each semantic category contains exemplars of varying typicality. It is, therefore, an interesting question whether humans can categorize natural scenes consistently into a relatively small number of categories, such as, coasts, rivers/lakes, forests, plains, and mountains. This is particularly important for applications, such as, image retrieval systems. Only if typicality is consistently perceived across different individuals, a general image-retrieval system makes sense. In this study, we use psychophysics and computational modeling to gain a deeper understanding of scene typicality. In the first psychophysical experiment, we used a forced-choice categorization task in which each of 250 natural scenes had to be classified into one of the following five categories: coasts, rivers/lakes, forests, plains, and mountains. In the second experiment, the typicality of each scene had to be rated on a 50-point scale for each of the five categories. The psychophysical results show high consistency between participants not only in the categorization of natural scenes, but also in the typicality ratings. In order to model human perception, we then employ a computational approach that uses an intermediate semantic modeling step by extracting local semantic concepts, such as, rock, water, and sand. Based on the human typicality ratings, we learn a psychophysically plausible distance measure that leads to a high correlation between the computational and the human ranking of natural scenes. Interestingly, model comparisons without a semantic-modeling step correlated much less with human performance, suggesting that our model is psychophysically very plausible. © 2006, ACM. All rights reserved.",Algorithms; image-retrieval systems; Scene classification; typicality,
Detection of Electromyographic Signals from Facial Muscles with Neural Networks,2006,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79952626551&doi=10.1145%2f1119766.1119770&partnerID=40&md5=470a10d6f854ba3dd839d854ca83314f,"The goal of this research was to investigate neural network-based methods to be applied in the processing of biomedical signals. We developed a neural network-based method for the detection of voluntarily produced changes in facial muscle action potentials. Electromyographic signals were recorded from the corrugator supercilii and zygomaticus major facial muscles. The facial muscle action potentials of thirty subjects were measured while they performed a series of voluntary contractions of these muscles. Wavelet denoising or digital bandpass filtering was applied to the preprocessing of the signals. A neural network was exploited for an offline classification of various phases of these signals. The results show that the neural network-based technique developed functioned very well, producing a reliable recognition accuracy of 96 to 99%. Because of these promising results, we will proceed in the development of this method for real-time applications that benefit from the analysis of electromyographic signals. © 2006, ACM. All rights reserved.",Direct manipulation; Experimentation; Human Factors; Human—computer interaction; Learning; Neural networks; Wavelets,
Data Collection and Analysis Techniques for Evaluating the Perceptual Qualities of Auditory Stimuli,2005,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84981728101&doi=10.1145%2f1101530.1101550&partnerID=40&md5=d4adfcee17f6f43692d49062ec451c3d,"This paper describes a general methodological framework for evaluating the perceptual properties of auditory stimuli. The framework provides analysis techniques that can ensure the effective use of sound for a variety of applications, including virtual reality and data sonification systems. Specifically, we discuss data collection techniques for the perceptual qualities of single auditory stimuli including identification tasks, context-based ratings, and attribute ratings. In addition, we present methods for comparing auditory stimuli, such as discrimination tasks, similarity ratings, and sorting tasks. Finally, we discuss statistical techniques that focus on the perceptual relations among stimuli, such as Multidimensional Scaling (MDS) and Pathfinder Analysis. These methods are presented as a starting point for an organized and systematic approach for nonexperts in perceptual experimental methods, rather than as a complete manual for performing the statistical techniques and data collection methods. It is our hope that this paper will help foster further interdisciplinary collaboration among perceptual researchers, designers, engineers, and others in the development of effective auditory displays. © 2005, ACM. All rights reserved.",data collection; Experimentation; Human Factors; Measurement; Performance; Sonification; statistics,
Mappings and Metaphors in Auditory Displays: An Experimental Assessment,2005,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016909437&doi=10.1145%2f1101530.1101534&partnerID=40&md5=2959044e77582775e2e05163912da6c2,"Auditory displays are becoming more and more common, but there are still no general guidelines for mapping data dimensions (e.g., temperature) onto display dimensions (e.g., pitch). This paper presents experimental research on different mappings and metaphors, in a generic process-control task environment, with reaction time and accuracy as dependent measures. It is hoped that this area of investigation will lead to the development of mapping guidelines applicable to auditory displays in a wide range of task domains. © 2005, ACM. All rights reserved.",Auditory display; Data mapping; Experimentation; Guidelines; Human FactorsPerformance; Metaphors; Sonification,
View Changes in Augmented Reality Computer-Aided-Drawing,2005,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84995404953&doi=10.1145%2f1048687.1048688&partnerID=40&md5=e781d1eebe4b4b0d1e52fe028262595b,"A prototype augmented reality computer-aided-drawing (AR CAD) prototype aimed at supporting a design detailing and review process has been developed. Utilizing head-mounted displays, AR CAD supports users in manipulating design models with two possible view change mechanisms—observer movement around the virtual object (model) or rotation of the virtual object. Previous studies in scene recognition across views of real objects yielded performances that were better under observer movement conditions, than under object rotation conditions. Based on these studies, it is hypothesized that the perception of 3D designs in an augmented scene from the AR CAD prototype is also better when changing views by observer movement than by model rotation. This study presents an experiment to test this hypothesis, so as to address the question of the appropriate mechanisms for view change that best support the perception of 3D designs presented in an augmented reality platform. The findings from the experiment suggest that an individual's perception of 3D designs in an augmented scene from AR CAD depends, in part, on the type of view change, showing that performance was better after observer movement than after model rotation. © 2005, ACM. All rights reserved.",Augmented reality; Design; Human Factors; Spatial cognition,
"Optimizing a Virtual Speech Display: Comments on Brungart and Simpson, ICAD 2003",2005,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024256845&doi=10.1145%2f1101530.1101539&partnerID=40&md5=e6607ea844de3d85de16425b6cb9bd61,"In our 2003 ICAD Paper “Optimizing the Spatial Configuration of a Seven Talker Speech Display,” we described a hybrid nearfar spatial configuration that maximizes performance in a multitalker listening task. In this brief addendum, we describe our scientific motivation for that study, describe a supplementary experiment that extended the earlier work to two additional spatial configurations, and place the results of the study in the context of current and future research in audio displays. © 2005, ACM. All rights reserved.",Cocktail party effect; Informational masking; Spatial Audio Displays,
Eurohaptics Special Issue Editorial,2005,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024269636&doi=10.1145%2f1060581.1060582&partnerID=40&md5=6e6aab2b94fad2ac74540d0255e9b6d6,[No abstract available],,
"Visual Cues Can be Sufficient for Triggering Automatic, Reflexlike Spatial Updating",2005,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006196990&doi=10.1145%2f1077399.1077401&partnerID=40&md5=ee6887d99990ecdf9c73c1fe0d992e86,"“Spatial updating” refers to the process that automatically updates our egocentric mental representation of our immediate surround during self-motions, which is essential for quick and robust spatial orientation. To investigate the relative contribution of visual and vestibular cues to spatial updating, two experiments were performed in a high-end Virtual Reality system. Participants were seated on a motion platform and saw either the surrounding room or a photorealistic virtual model presented via headmounted display or projection screen. After upright rotations, participants had to point “as accurately and quickly as possible” to previously learned targets that were outside of the current field of view (FOV). Spatial updating performance, quantified as response time, configuration error, and pointing error, was comparable in the real and virtual reality conditions when the FOV was matched. Two further results challenge the prevailing basic assumptions about spatial updating: First, automatic, reflexlike spatial updating occurred without any physical motion, i.e., visual information from a known scene alone can, indeed, be sufficient, especially for large FOVs. Second, continuous-motion information is not, in fact, mandatory for spatial updatingerely presenting static images of new orientations proved sufficient, which motivated our distinction between continuous and instant-based spatial updating. © 2005, ACM. All rights reserved.",ego-motion simulation; Experimentation; Human Factors; human factors; multimodal; psychophysics; spatial orientation; Spatial updating; virtual reality; visuovestibular cue integration,
From Physics to Sound,2005,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024269767&doi=10.1145%2f1101530.1101555&partnerID=40&md5=5f3759654dbf5cb1326dffe93b5977e4,"Wherever we are, what we hear is mostly noise. When we ignore it, it disturbs us. When we listen to it, we find it fascinating. The sound of a truck at 50 mph. Static between the stations. Rain. We want to capture and control these sounds, to use them, not as sound effects, but as musical instruments. © 2005, ACM. All rights reserved.",Bubbles; Liquid; Sound; Synthesis; Water,
"Authors' Comments on Miner and Caudell, ICAD 1997",2005,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024269005&doi=10.1145%2f1101530.1101553&partnerID=40&md5=81d03dae12c76e0d91c766086f733809,"Sound is a critical component of multimodal representations of information in virtual environments. Our research program continues to develop and evaluate sound software system architectures, localization techniques, sound synthesis, and musical sonification as tools to enhance human comprehension of complex data sets. © 2005, ACM. All rights reserved.",auditory display; data mapping; Human Factors; Performance Experimentation; Psychoacoustics; Sonification; Sound Synthesis; Wavelets,
"Program Auralization: Author's Comments on Vickers and Alty, ICAD 2000",2005,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024260397&doi=10.1145%2f1101530.1101547&partnerID=40&md5=09876a544963f5f63daba6159943b507,"In this paper, we reflect upon the investigations into external auditory representations of programs (program auralization) reported by Vickers and Alty at ICAD 2000. First, we place the work in its historical and thematic context and explore the motivation that lay behind it. We then outline the process by which we got to the stage of being able to report empirical results in 2000 and compare the work with that done by other researchers in the field. Finally, we assess the major contribution that this work made to the field of auditory display and look to the future outlining the work still to be done since the paper was first published (we also look at work done by others in this area since 2000). © 2005, ACM. All rights reserved.",auralization; debugging; Human factors; Languages; Music; Pascal,
Guest Editorial,2005,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024273491&doi=10.1145%2f1077399.1077400&partnerID=40&md5=80b2030aa65460da40f0561888407582,[No abstract available],,
Sound Science: Marking Ten International Conferences on Auditory Display,2005,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011043827&doi=10.1145%2f1101530.1101531&partnerID=40&md5=55769d0cd5a02d00f6062f255d2d6b1e,"This special issue of ACM Transactions on Applied Perception is intended to commemorate the tenth International Conference on Auditory Display (ICAD) and to serve as an introduction and overview of the field of auditory displays. This paper discusses the goals of the issue and describes the paper selection process. The selected papers are also introduced, with their connections to each other, their place in ICAD, and their relevance to other fields briefly highlighted. © 2005, ACM. All rights reserved.",Auditory display; Experimentation; Human Factors; International Conference on Auditory Display (ICAD); Performance; Sonification,
Perceptual Auditory Design,2005,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024251509&doi=10.1145%2f1101530.1101541&partnerID=40&md5=9fc03527b8d64479660e63f8a6051a40,"Mitsopoulos and Edwards ICAD 1998 paper deals with the design of auditory widgets as exemplars of applying sound psychological theories of auditory perception and attention in the field of auditory interface design. The research, briefly herein described, is presented in detail in Mitsopoulos [2000]. Future work at all three levels of the methodology is foreseen, as well as in the integration of input and output modalities. © 2005, ACM. All rights reserved.",Auditory Display; Data Mapping; Human Factors & Performance; Sonification,
Data Sonification from the Desktop: Should Sound Be Part of Standard Data Analysis Software,2005,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016877281&doi=10.1145%2f1101530.1101544&partnerID=40&md5=1fb4e24b5a66b671a45d008c0e789b43,"The design of auditory formats for data display is presently focused on applications for blind or visually impaired users, specialized displays for use when visual attention must be devoted to other tasks, and some innovative work in revealing properties of complex data that may not be effectively rendered by traditional visual means. With the availability of high-quality and flexible sound production hardware in standard desktop computers, the potential exists for using sound to represent characteristics of typical “small and simple” samples of data in routine data inspection and analysis. Our research has shown that basic properties of simple functions, distribution properties of data samples, and patterns of covariation between two variables can be effectively displayed by simple auditory graphs involving patterns of pitch variation over time. While such developments have implications for specialized applications and populations of users, these displays are easily comprehended by normal users with minimal practice. Providing further software enhancement to encourage exploration of data representation by sound may lead to a variety of useful creative developments in data display technology. © 2005, ACM. All rights reserved.",auditory display; data mapping; Experimentation; Human Factors; Performance; Sonification,
First Evaluation of a Novel Tactile Display Exerting Shear Force Via Lateral Displacement,2005,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84986198224&doi=10.1145%2f1060581.1060586&partnerID=40&md5=bdb6310f117498b08aa9fd95d97bcca5,"Based on existing knowledge on human tactile movement perception, we constructed a prototype of a novel tactile multipin display that controls lateral pin displacement and, thus produces shear force. Two experiments focus on the question of whether the prototype display generates tactile stimulation that is appropriate for the sensitivity of human tactile perception. In particular, Experiment I studied human resolution for distinguishing between different directions of pin displacement and Experiment II explored the perceptual integration of information resulting from the displacement of multiple pins. Both experiments demonstrated that humans can discriminate between directions of the displacements, and also that the technically realized resolution of the display exceeds the perceptual resolution (>14.). Experiment II demonstrated that the human brain does not process stimulation from the different pins of the display independent of one another at least concerning direction. The acquired psychophysical knowledge based on this new technology will in return be used to improve the design of the display. © 2005, ACM. All rights reserved.",Design; Experimentation; Haptic interfaces; Human Factors; psychophysics; shear force; tactile movement perception; tangential displacement,
Enhancing Haptic Detection of Surface Undulation,2005,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017388084&doi=10.1145%2f1048687.1048691&partnerID=40&md5=11c0891ae05f53cea0ba011829cb228c,"This paper introduces a device for enhancing detection of surface undulation through active touch. This device, which we call a “tactile contact lens,” is composed of a sheet and numerous pins arranged on one side of the sheet. Experimental results show that a small bump on a surface can be detected more accurately through this device than by bare finger and than through a flat sheet. A mathematical analysis of this phenomenon suggests two possible explanations for this phenomenon. One lies in the lever-like behavior of the pins. The pins convert the local inclination of the object surface into the tangential displacement of the skin surface. The second is the spatial aliasing effect resulting from the discrete contact. Due to this effect, the temporal change in the skin surface displacement is efficiently transduced into the temporal change in the skin tissue strain. The results of this analysis are then discussed in relation to other sensitivity-enhancing materials, tactile sensing mechanisms, and tactile/haptic display devices. © 2005, ACM. All rights reserved.",Haptics; Human Factors; Sensation enhancement; Surface undulation; Tactile contact lens; Tactile sensing; Theory; Verification,
Force Constancy and its Effect on Haptic Perception of Virtual Surfaces,2005,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991799437&doi=10.1145%2f1060581.1060584&partnerID=40&md5=2a1fc01e489f8412aa7bc54fa134d9a4,"The force-constancy hypothesis states that the user of a force-feedback device maintains a constant penetration force when stroking virtual surfaces in order to perceive their topography. The hypothesis was developed to address a real-world data perceptualization problem where the perception of surface topography was distorted when the surface stiffness was nonuniform. Two experiments were conducted. In Experiment I, we recorded the penetration depths of the probe tip while the user stroked two surfaces with equal height but different stiffness values. We found that the data could be quantitatively modeled by the force-constancy hypothesis when the virtual surfaces were neither too soft nor too hard. In Experiment II, we demonstrated that given two adjacent surfaces, their perceived height difference depended on both the surface stiffness values as well as the relative heights of the surfaces. Specifically, we showed that the higher but softer surface could be perceived to be lower, at the same height, or higher than the other surface, depending on how much higher it was than the other surface. The results were consistent with the predictions of the force-constancy hypothesis. Our findings underscore the importance of understanding the interplay of haptic rendering parameters. © 2005, ACM. All rights reserved.",Experimentation; force constancy; Haptic rendering; psychophysics; surface topography; Theory; Verification,
Waypoint Navigation with a Vibrotactile Waist Belt,2005,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978278644&doi=10.1145%2f1060581.1060585&partnerID=40&md5=1e2f38af4fb3a59846d730c6ac36570a,"Presenting waypoint navigation on a visual display is not suited for all situations. The present experiments investigate if it is feasible to present the navigation information on a tactile display. Important design issue of the display is how direction and distance information must be coded. Important usability issues are the resolution of the display and its usefulness in vibrating environments. In a pilot study with 12 pedestrians, different distance-coding schemes were compared. The schemes translated distance to vibration rhythm while the direction was translated into vibration location. The display consisted of eight tactors around the user's waist. The results show that mapping waypoint direction on the location of vibration is an effective coding scheme that requires no training, but that coding for distance does not improve performance compared to a control condition with no distance information. In Experiment 2, the usefulness of the tactile display was shown in two case studies with a helicopter and a fast boat. © 2005, ACM. All rights reserved.",Design; Experimentation; Human Factors; navigation; Performance; Vehicle control; vibrotactile displays; visually handicapped,
Desktop Data Sonification,2005,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84999122347&doi=10.1145%2f1101530.1101545&partnerID=40&md5=79ec2874fcacd722124470e98bbbcc24,"Sonification tools have not yet become typical components of data analysis software, despite dramatic advances in sound-production capabilities of personal computers over the past decade. However, we continue to believe that auditory displays have the potential to be highly useful for “small scale” exploration of data for normally sighted users as well as an alternative format for users with visual impairment. Demonstration of effective examples of auditory data displays and design of flexible software tools for data sonification will be key factors in determining the impact of this method of data representation. © 2005, ACM. All rights reserved.",auditory display; data mapping; Experimentation; Human Factors; Performance; Sonification,
Optimizing the Spatial Configuration of a Seven-Talker Speech Display,2005,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77954529161&doi=10.1145%2f1101530.1101538&partnerID=40&md5=2893ab3b6bb2ed75cb59d2f0f23bd09b,"Although there is substantial evidence that performance in multitalker listening tasks can be improved by spatially separating the apparent locations of the competing talkers, very little effort has been made to determine the best locations and presentation levels for the talkers in a multichannel speech display. In this experiment, a call sign based color and number identification task was used to evaluate the effectiveness of three different spatial configurations and two different level normalization schemes in a seven-channel binaural speech display. When only two spatially adjacent channels of the seven-channel system were active, overall performance was substantially better with a geometrically spaced spatial configuration (with far-field talkers at −90°, −30°, −10°, 0°, +10°, +30°, and +90° azimuth) or a hybrid near-far configuration (with far-field talkers at +90°, +30°, 0°, +30°, and +90° azimuth and near-field talkers at ±90.) than with a more conventional linearly spaced configuration (with far-field talkers at −90°, −60°, −30°, 0°, +30°, +60°, and +90° azimuth). When all seven channels were active, performance was generally better with a “better-ear”h normalization scheme that equalized the levels of the talkers in the more intense ear than with a default normalization scheme that equalized the levels of the talkers at the center of the head. The best overall performance in the seventalker task occurred when the hybrid near-far spatial configuration was combined with the better-ear normalization scheme. This combination resulted in a 20% increase in the number of correct identifications relative to the baseline condition with linearly spaced talker locations and no level normalization. Although this is a relatively modest improvement, it should be noted that it could be achieved at little or no cost simply by reconfiguring the HRTFs used in a multitalker speech display. © 2005, ACM. All rights reserved.",Cocktail party effect; Human Factors; Informational masking; Speech Interfaces,
"Toward Perceptually Realistic Talking Heads: Models, Methods, and McGurk",2005,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-58149506686&doi=10.1145%2f1077399.1077405&partnerID=40&md5=62f6f9f322dbb04159efad1ccf75a5f2,"Motivated by the need for an informative, unbiased, and quantitative perceptual method for the evaluation of a talking head we are developing, we propose a new test based on the “McGurk Effect.” Our approach helps to identify strengths and weaknesses in visual—speech synthesis algorithms for talking heads and facial animations, in general, and uses this insight to guide further development. We also evaluate the behavioral quality of our facial animations in comparison to real-speaker footage and demonstrate our tests by applying them to our current speech-driven facial animation system. © 2005, ACM. All rights reserved.",audio analysis; Experimentation; Facial animation; Human Factors; learning; lip synching; McGurk effect; perceptual analysis; Performance; psychological analysis; Verification; video analysis; video synthesis,
Low-Level Image Cues in the Perception of Translucent Materials,2005,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33749250977&doi=10.1145%2f1077399.1077409&partnerID=40&md5=f9372043179ee36f47aed705686ff36b,"When light strikes a translucent material (such as wax, milk or fruit flesh), it enters the body of the object, scatters and reemerges from the surface. The diffusion of light through translucent materials gives them a characteristic visual softness and glow. What image properties underlie this distinctive appearance? What cues allow us to tell whether a surface is translucent or opaque? Previous work on the perception of semitransparent materials was based on a very restricted physical model of thin filters [Metelli 1970; 1974a, b]. However, recent advances in computer graphics [Jensen et al. 2001; Jensen and Buhler 2002] allow us to efficiently simulate the complex subsurface light transport effects that occur in real translucent objects. Here we use this model to study the perception of translucency, using a combination of psychophysics and image statistics. We find that many of the cues that were traditionally thought to be important for semitransparent filters (e.g., X-junctions) are not relevant for solid translucent objects. We discuss the role of highlights, color, object size, contrast, blur, and lighting direction in the perception of translucency. We argue that the physics of translucency are too complex for the visual system to estimate intrinsic physical parameters by inverse optics. Instead, we suggest that we identify translucent materials by parsing them into key regions and by gathering image statistics from these regions. © 2005, ACM. All rights reserved.",Experimentation; Human Factors; Human visual perception; illumination; image statistics; material perception; Metelli; translucency; transparency,
Physically Based Models for Liquid Sounds,2005,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745827824&doi=10.1145%2f1101530.1101554&partnerID=40&md5=8d3096ab1c22e8407df543935116ab7c,"A physically based liquid sound synthesis methodology is developed. The fundamental mechanism for the production of liquid sounds is identified as the acoustic emission of bubbles. After reviewing the physics of vibrating bubbles as it is relevant to audio synthesis, a sound model for isolated single bubbles is developed and validated with a small user study. A stochastic model for the real-time interactive synthesis of complex liquid sounds such as produced by streams, pouring water, rivers, rain, and breaking waves is based on the synthesis of single bubble sounds. It is shown how realistic complex high dimensional sound spaces can be synthesized in this manner. © 2005, ACM. All rights reserved.",Bubbles; Liquid; Sound; Synthesis; Water,
Crystallization Sonification of High-Dimensional Datasets,2005,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871193852&doi=10.1145%2f1101530.1101556&partnerID=40&md5=62fc3ef3909010eaa9c81fdfe7269ad1,"This paper introduces Crystallization Sonification, a sonification model for exploratory analysis of high-dimensional datasets. The model is designed to provide information about the intrinsic data dimensionality (which is a local feature) and the global data dimensionality, as well as the transitions between a local and global view on a dataset. Furthermore the sound allows to display the clustering in high-dimensional datasets. The model defines a crystal growth process in the high-dimensional data-space which starts at a user selected “condensation nucleus” and incrementally includes neighboring data according to some growth criterion. The sound summarizes the temporal evolution of this crystal growth process. For introducing the model, a simple growth law is used. Other growth laws which are used in the context of hierarchical clustering are also suited and their application in crystallization sonification offers new ways to inspect the results of data clustering as an alternative to dendrogram plots. In this paper, the sonification model is described and example sonifications are presented for some synthetic high-dimensional datasets. © 2005, ACM. All rights reserved.",Algorithms; Data Mining; Design; exploratory data analysis; interactive sonification; Sonification,
Sonically-Enhanced Widgets,2005,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-48049105544&doi=10.1145%2f1101530.1101543&partnerID=40&md5=429eed46a66d9aad79ca91252f2ee93c,"This paper presents a review of the research surrounding the paper “The Design and Evaluation of a Sonically Enhanced Tool Palette” by Brewster and Clarke from ICAD 1997. A historical perspective is given followed by a discussion of how this work has fed into current developments in the area. © 2005, ACM. All rights reserved.",Auditory Display; earcons; Human-Computer Interaction; Sonically enhanced widgets,
A study of Level-Of-Detail in Haptic Rendering,2005,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33746621380&doi=10.1145%2f1048687.1048689&partnerID=40&md5=7c1b165e649a48e0b9cd0653de05f8ff,"This paper presents an initial study of an approach to reduce computational overhead in haptic rendering of physically based models. Haptic rendering refers to the notion of adding physical properties and behavior, specifically a sense of touch or force feedback, to models of objects. In this way, a user through a haptic feedback device can feel interaction forces while visually observing the objects. Physically based modeling is particularly important when representing deformable objects. In this paper, an approach based on a mass-spring damper system is used in modeling deformable objects. Deformation due to interaction forces is obtained by solving a set of differential equations, a process that is in general computationally demanding. To reduce this demand, the notion of level-of-detail in haptic rendering is introduced. Here the interplay between the graphical mesh and the haptic mesh as a function of various levels of subdivision is studied. The approach we describe is to adjust model parameters such that the user feels the same reaction force for a given deformation, regardless of the level of local subdivision. A preliminary user study with simple objects suggests there can be a local subdivision threshold such that the user cannot distinguish between global subdivision and the local subdivision introduced by the level-of-detail algorithm. This conclusion is beneficial for haptic rendering of deformable objects. Similar conclusions were obtained for haptic rendering of rigid objects. These results can be used as a guideline for other approaches to modeling deformable objects, such as finite element representations. © 2005, ACM. All rights reserved.",Collision detection; Deformable objects; Experimentation; Haptic feedback; Human Factors; Level-of-detail,
"A Novel Two-Dimensional Tactile Slip Display: Design, Kinematics and Perceptual Experiments",2005,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33750949348&doi=10.1145%2f1060581.1060588&partnerID=40&md5=5cdc802816bb3656cd4120db6b911823,"A novel two-degree-of-freedom tactile display reproduces the sensations of sliding contact and incipient slip through the rotation of a ball positioned under the userfs fingertip. A pair of motor-driven wheels actuates the ball via contact friction. Mechanical performance requirements are used to define the dimensions and construction method of the device. Kinematic analysis shows that the drive wheel angles and their contact locations with the ball must be carefully selected in order to accurately control the axis of rotation and speed of the ball. However, psychophysical experiments indicate that some kinematic error is tolerable; errors of up to 20. in slip angle and 30% of a nominal velocity may be applied without detection from an average user. The lightweight, modular tactile display was attached to a multi-degree-of-freedom kinesthetic interface and used to display virtual environments with slip. Experimental results demonstrate that users complete a virtual paper manipulation task with lower applied forces using combined slip and force feedback in comparison with conventional force feedback alone. © 2005, ACM. All rights reserved.",Design; Experimentation; Haptic interface; Human factors; incipient slip; Performance; psychophysics; tactile display; virtual reality,
"Model-Based Sonification Revisited—Authors' Comments on Hermann and Ritter, ICAD 2002",2005,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-39649117044&doi=10.1145%2f1101530.1101557&partnerID=40&md5=926a92871f367d82c4a5c7320a029227,"We discuss the framework of Model-Based Sonification (MBS) and its contribution to a principled design of mediators between high-dimensional data spaces and perceptual spaces, particularly sound spaces. Data Crystallization Sonification, discussed in the reprinted paper, exemplifies the design of sonification models according to this framework. Finally, promising lines of development in this area are pointed out, concerning generalizations, applications, and open research directions. © 2005, ACM. All rights reserved.",Algorithms; data mining; Design; exploratory data analysis; interactive sonification; Sonification,
Perceptual Plasticity in Spatial Auditory Displays,2005,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-38849109551&doi=10.1145%2f1101530.1101536&partnerID=40&md5=9b8aa70e4d4ff5dde3b781215c556b34,"Often, virtual acoustic environments present cues that are inconsistent with an individual's normal experiences. Through training, however, an individual can at least partially adapt to such inconsistent cues through either short- [Kassem 1998; Shinn-Cunningham 2000; Shinn-Cunningham et al. 1998a, 1998b; Zahorik 2001] or long-[Hofman et al. 1998] term exposure. The type and degree of inconsistency as well as the length of training determine the final accuracy and consistency with which the subject can localize sounds [Shinn-Cunningham 2000]. The current experiments of short-term adaptation measure how localization bias (mean error) and resolution (precision) change when subjects are exposed to auditory cue rearrangements simpler than those previously investigated. These results, combined with those of earlier experiments, suggest that there is plasticity at many different levels of the spatial auditory processing pathway with different time scales governing the plasticity at different levels of the system. This view of spatial auditory plasticity has important implications for the design of spatial auditory displays. © 2005, ACM. All rights reserved.",Auditory display; Experimentation; Human Factors; Performance; Spatial hearing; Virtual auditory space,
"Spatial Auditory Display: Comments on Shinn-Cunningham et al., ICAD 2001",2005,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84926633727&doi=10.1145%2f1101530.1101537&partnerID=40&md5=86166d61d48d3bb5794541dc06228169,"Spatial auditory displays have received a great deal of attention in the community investigating how to present information through sound. This short commentary discusses our 2001 ICAD paper (Shinn-Cunningham, Streeter, and Gyss), which explored whether it is possible to provide enhanced spatial auditory information in an auditory display. The discussion provides some historical context and discusses how work on representing information in spatial auditory displays has progressed over the last 5 years. © 2005, ACM. All rights reserved.",Auditory display; Experimentation; Human Factors; Performance; Spatial hearing; Virtual auditory space,
Collisions and Attention,2005,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33751040045&doi=10.1145%2f1077399.1077407&partnerID=40&md5=a6aec9b6f533c192401757456054795c,"Attention is an important factor in the perception of static and dynamic scenes, which should, therefore, be taken into account when creating graphical images and animation. Recently, researchers have recognized this fact and have been investigating how the focus of attention can be measured, predicted, and exploited in graphical systems. In this article, we explore some preliminary strategies for developing an automatic means of predicting and exploiting attention in the processing of collisions and other dynamic events. Recent work on the perception of causality has shown that attention can change the way in which a dynamic scene consisting of collision events is perceived. We describe a series of experiments designed to determine the source of biases in the perception of anomalous collision dynamics and, in particular, whether attention plays a role. Using an eyetracker, eye-movements were recorded while participants viewed animations of simple causal launching events in 3D involving two colliding spheres. Results indicated that there was indeed a definite pattern to the allocation of attention based on the nature of the event, which is promising for the goal of developing a predictive metric. As a follow-up, a paper-based experiment was carried out in which participants were asked to sketch the predicted post-collision trajectories of the same two spheres printed on paper. These experiments demonstrated that attention alone was not sufficient in determining performance, but rather the nature of the dynamic event itself also played a role. © 2005, ACM. All rights reserved.",Algorithms; Animation; attention; collisions; Experimentation; eye movements; Human Factors; Measurement; perception,
"Sonification Design and Metaphors: Comments on Walker and Kramer, ICAD 1996",2005,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-62449169604&doi=10.1145%2f1101530.1101535&partnerID=40&md5=fb730608ef65b66b9f9675c0d33e7d2d,"The original Walker and Kramer paper at ICAD 1996 studied the mapping of data dimensions (e.g., temperature) onto sound dimensions (e.g., pitch). In this commentary we consider the historical and methodological context of that early work, discuss its relevance to the field of auditory display, and point out how it forms part of a body of work with ties to other researchers in the ICAD community. © 2005, ACM. All rights reserved.",Auditory display; Data mapping; Experimentation; Human Factors; Performance; Sonification,
The Design and Evaluation of a Sonically Enhanced Tool Palette,2005,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78651551579&doi=10.1145%2f1101530.1101542&partnerID=40&md5=013000c9bc62b238e670d7f965e2baf8,"This paper describes an experiment to investigate the effectiveness of adding sound to tool palettes. Palettes have usability problems because users need to see the information they present, but they are often outside the area of visual focus. We used nonspeech sounds called earcons to indicate the current tool and when tool changes occurred so that users could tell what tool they were in wherever they were looking. Results showed a significant reduction in the number of tasks performed with the wrong tool. Therefore, users knew what the current tool was and did not try to perform tasks with the wrong one. All of this was not at the expense of making the tool palettes any more annoying to use. © 2005, ACM. All rights reserved.",auditory display; Experimentation; Human Factors; human-computer interaction; Performance; Sonically enhanced widgets,
Reflections on Sonic Browsing,2005,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-70349182971&doi=10.1145%2f1101530.1101549&partnerID=40&md5=468c819605dc847c7454b918b29b8306,"In the original Fernström and McNamara paper at ICAD 1998, we described our work on a novel user interface for sonic browsing of music collections. In this commentary, we reflect upon the continued research in this area and how our interactive sonification approach has come to be used also for browsing collections of everyday sounds and for perception experiments. © 2005, ACM. All rights reserved.",auditory display; Human Factors; Performance; Sonification,
"Comparison of Auditory, Visual, and Audiovisual Navigation in a 3D Space",2005,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962670994&doi=10.1145%2f1101530.1101558&partnerID=40&md5=48339ebd9f74f5d0822996498f88199d,"A navigation test was carried out in a spatially immersive virtual environment. The test was a gamelike experience where the task of subjects was to find as many gates as possible while they navigated through a track guided by auditory and/or visual cues. The results are presented as a function of the number of found gates, searching times, and normalized path lengths. Audiovisual navigation was clearly the most efficient. Visual navigation was second and the auditory navigation the least efficient. Further analysis of travel paths indicate that auditory cues were utilized in the beginning to locate the next gate; a visual cue was the most important in the final approach to the gate. © 2005, ACM. All rights reserved.",auditory display; auditory navigation; Human Factors; Performance; spatial audio; Virtual reality,
"Author's Comments on Gröhn, Lokki, and Takala, ICAD 2003",2005,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-63049135446&doi=10.1145%2f1101530.1101559&partnerID=40&md5=06d23d8a692c98e8c51818256f6ae285,"Historical background and motivation for ICAD2003 paper is presented. In addition, the research results followed up is briefly overviewed and future work is elaborated. © 2005, ACM. All rights reserved.",auditory display; Experimentation; Human Factors; Performance; spatial audio; Virtual reality,
"Evaluation of Auditory Displays: Comments on Bonebright et al., ICAD 1998",2005,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84923427212&doi=10.1145%2f1101530.1101551&partnerID=40&md5=df686d5f903bd47f1a4d4ded6c9dd213,"This commentary was written for the article “Data collection analysis techniques for evaluating the perceptual qualities of auditory stimuli” that was presented at the International Conference on Auditory Display in 1998. The original paper was written as a primer for research design and statistical evaluation of perceptual responses to auditory displays as well as a catalyst for the formation of interdisciplinary teams for such research. The authors' commentary notes that the ICAD community has shown an increase in cross-disciplinary collaboration but that researchers need to continue to improve their research designs and their statistical techniques to ensure accurate evaluation of auditory displays. © 2005, ACM. All rights reserved.",data collection; Experimentation; Human Factors; Measurement; Performance; Sonification; statistics,
Musical Program Auralization: Empirical Studies,2005,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33749563994&doi=10.1145%2f1101530.1101546&partnerID=40&md5=ff8c953594abe9f717346e5fc1c723b4,"Program auralization aims to communicate information about program state, data, and behavior using audio. We have argued that music offers many advantages as a communication medium [Alty 1995]. The CAITLIN system [Alty and Vickers 1997; Vickers 1999; Vickers and Alty 1996, 1998] was constructed to provide auralizations within a formal structured musical framework. Pilot studies [Alty and Vickers 1997; Vickers 1999] showed that programmers could infer program structure from auralizations alone. A study was conducted using 22 novice programmers to assess (i) whether novices could understand the musical auralizations and (ii) whether the musical experience and knowledge of subjects affected their performance. The results show that novices could interpret the auralizations (with accuracy varying across different levels of abstraction) and that musical knowledge had no significant effect on performance. A second experiment was conducted with another 22 novice programmers to study the effects of musical program auralization on debugging tasks. The experiment aimed to determine whether auralizations would lead to higher bug detection rates. The results indicate that, in certain circumstances, musical auralizations can be used to help locate bugs in programs and that musical skill does not affect the ability to make use of the auralizations. In addition, the experiment showed that subjective workload increased when the musical auralizations were used. © 2005, ACM. All rights reserved.",auralization; debugging; Human Factors; Languages; Music; Pascal,
A Principled Methodology for the Specification and Design of Nonvisual Widgets,2005,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-36248981442&doi=10.1145%2f1101530.1101540&partnerID=40&md5=edd563e420cbcb8bbaa63cfceff516b9,"When the visual channel of communication is unavailable because the user is blind, nonvisual user interfaces must be developed. The proposed methodology consists of three interrelated specification levels. Information and supported tasks are specified in abstract terms at the conceptual level, taking into account requirements imposed by manipulation of interaction devices and information provided by analysis of the visual representation. The perceptual structure of the auditory scene is specified next at the structural level and then the physical dimensions of sound are defined at the implementation level. The methodology is applied to the specification of a simple listbox widget. © 2005, ACM. All rights reserved.",Auditory display; Data mapping; Human Factors; Human Factors & Performance; Performance; Sonification,
After Direct Manipulation — Direct Sonification,2005,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-56849130910&doi=10.1145%2f1101530.1101548&partnerID=40&md5=f61c9375ae584531aec572cc6683373b,"The effectiveness of providing multiple-stream audio to support browsing on a computer was investigated through the iterative development and evaluation of a series of sonic browser prototypes. The data set usedwas a database containing music. Interactive sonification was provided in conjunction with simplified human—computer interaction sequences. It was investigated to what extent interactive sonification with multiple-stream audio could enhance browsing tasks, compared to interactive sonification with single-stream audio support. It was found that with interactive multiple-stream audio, the ten users could accurately complete the browsing tasks significantly faster than those who had single-stream audio support. © 2005, ACM. All rights reserved.",auditory display; Experimentation; Human Factors; Performance; Sonification,
Throwing Versus Walking as Indicators Of Distance Perception In Similar Real And Virtual Environments,2005,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84899013186&doi=10.1145%2f1048687.1048690&partnerID=40&md5=987eee898b427895706f07a480dc64f7,"For humans to effectively interact with their environment, it is important for the visual system to determine the absolute size and distance of objects. Previous experiments performed in full-cue, real-world environments have demonstrated that blind walking to targets serves as an accurate indication of distance perception, up to about 25 m. In contrast, the same task performed in virtual environments (VEs) using head-mounted displays shows significant underestimation in walking. To date, blind walking is the only visually directed action task that has been used to evaluate distance perception in VEs beyond reaching distances. The possible influence of the response measure itself on absolute distance perception in virtual environments is currently an open question. Blind walking involves locomotion and the egocentric updating of the environment with one's own movement. We compared this measure to blind throwing, a task that involves the initiation of a movement directed by vision, but no further interaction within the environment. Both throwing and walking were compressed in the VE but accurate in the real world. We suggest that distance compression found in VEs may be a result of a general perceptual origin rather than specific to the response measure. © 2005, ACM. All rights reserved.",Blind walking; Distance perception; Experimentation; Head-mounted displays; Measurement; Performance; Virtual environments,
Haptic Walker—A Novel Haptic Foot Device,2005,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874715171&doi=10.1145%2f1060581.1060589&partnerID=40&md5=2dd497a51d02810419f4eade9a487897,"This paper presents a new haptic locomotion interface, which comprises two programmable foot platforms with permanent foot machine contact. It is designed as a scalable and modular system with unit-by-unit extensibility offering up to six plus one degrees of freedom (DOF) per foot. The basic setup comprises three DOF per foot in the sagittal plane. The machine is based on a rigid hybrid parallel-serial robot kinematics structure. It is equipped with electrical direct drive motors, enabling highly dynamic footplate motions. For contact force measurement, six DOF force/torque sensors are mounted under each foot platform. The system was developed for major application in gait rehabilitation, hence great importance was attached to the incorporation of maximum passive and active security measures for machine users and medical operating personnel. The simulator is able to perform walking trajectories with speeds of up to 5 km/h and 120 steps/min. The system is able to simulate not only slow and “smooth” trajectories like walking on an even floor, up/down staircases, but also foot motions like walking on rough ground or even stumbling or sliding, which require high system dynamics. The machine is controlled by a self-developed full-featured robot control whose soft and hardware is based on up-to-date industrial standards and interfaces. The robot control software is based on RTLinux and runs on an industrial PC. The real-time motion generator includes a newly developed Fourier-based algorithm for the interpolation of natural cyclic walking trajectories. For the implementation of asynchronous events (e.g., sliding, stumbling), the controller comprises especially developed algorithms for automatic motion override adaptation. Different modes of haptic behavior needed for gait rehabilitation, ranging from full foot support during swing phase to completely passive behavior, are currently under development. Intuitive and safe machine operation by nontechnical personnel such as clinicians and physiotherapists is achieved via a separate Windows-based graphical user interface software comprising different window areas for machine programming and operation, real-time off-line simulation and online data visualization in two and three dimensions has been developed as well. A working prototype of the system has been built and tested successfully, including all soft and hardware components. Although the machine has been designed and built for major application in gait rehabilitation, its range of applicability is not limited to this area. It could be integrated into any setup requiring a highly dynamic haptic foot interface and permanent foot machine contact if needed. © 2005, ACM. All rights reserved.",,
Predicting and Evaluating Saliency for Simplified Polygonal Models,2005,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33750594259&doi=10.1145%2f1077399.1077406&partnerID=40&md5=d9eb28a2e4847923dbc935979c323c8e,"In this paper, we consider the problem of determining feature saliency for three-dimensional (3D) objects and describe a series of experiments that examined if salient features exist and can be predicted in advance. We attempt to determine salient features by using an eye-tracking device to capture human gaze data and then investigate if the visual fidelity of simplified polygonal models can be improved by emphasizing the detail of salient features identified in this way. To try to evaluate the visual fidelity of the simplified models, a set of naming time, matching time, and forced-choice preference experiments were carried out. We found that perceptually weighted simplification led to a significant increase in visual fidelity for the lower levels of detail (LOD) of the natural objects, but that for the man-made artifacts the opposite was true. We, therefore, conclude that visually prominent features may be predicted in thisway for natural objects, but our results show that saliency prediction for synthetic objects is more difficult, perhaps because it is more strongly affected by task. As a further step we carried out some confirmation experiments to examine if the prominent features found during the saliency experiment were actually the features focused upon during the naming, matching, and forced-choice preference tasks. Results demonstrated that the heads of natural objects received a significant amount of attention, especially during the naming task. We hope that our results will lead to new insights into the nature of saliency in 3D graphics. © 2005, ACM. All rights reserved.",model simplification; salient features; Visual perception,
Display of Virtual Braille Dots by Lateral Skin Deformation: Feasibility Study,2005,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745872168&doi=10.1145%2f1060581.1060587&partnerID=40&md5=0f9d3fc5f6471e609a11cc46aea82b29,"When a progressive wave of localized deformations occurs tangentially on the fingerpad skin, one typically experiences the illusion of a small object sliding on it. This effect was investigated because of its potential application to the display of Braille. A device was constructed that could produce such deformation patterns along a line. Blind subjectsf ability to read truncated Braille characters (‘○○’, ‘○−’, ‘−○’, and ‘−−’) using the device was experimentally tested and compared to their performance with a conventional Braille medium. While subjects could identify two-character strings with a high rate of success, several factors need to be addressed before a display based on this principle can become practical. © 2005, ACM. All rights reserved.",Braille display; Design; Experimentation; Human Factors; lateral skin deformation; tactile perception,
"A Comprehensive Framework for Auditory Display: Comments on Barrass, ICAD 1994",2005,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-70349614351&doi=10.1145%2f1101530.1101533&partnerID=40&md5=8da45984671839042c1cd2d0e8b47866,"In ‘A Perceptual Framework for the Auditory Display of Scientific Data’ I described the first perceptually scaled sound space designed specifically for sonification. I modeled this sound space, and its underlying theory, on the use of perceptual colour spaces in scientific visualization. As I went on to apply the sound space in mappings of satellite data I introduced methods of data characterization and user-centered task analysis into my design framework. In trials I realized that satellite images allow you to see global information across millions of data values, whereas it was impossible to play all the data at once as sounds. This lead me to explore perceptual streaming as a means for perceiving similarity and difference in masses of sounded data. In work on sonifications for virtual reality applications I recognized the need to consider the semiotic linkage of the sound with the application domain, and the need to also link the sound with the interaction metaphor. The work described in this paper laid the foundation for the ongoing development of a comprehensive framework for auditory display that takes into consideration the perceptual organization of the sounds, the characteristics of the data, the gamut of the display device, the user's tasks, the semiotic linkage to the application domain, and the affordances for interaction. © 2005, ACM. All rights reserved.",Auditory display; Data mapping; Experimentation; Measurement; Sonification; Theory,
A Perceptual Framework for the Auditory Display of Scientific Data,2005,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868517079&doi=10.1145%2f1101530.1101532&partnerID=40&md5=520d0e6c38a6f9c659e6ece95c3e64b2,"A principal requirement of the auditory display of scientific data is an accurate portrayal of the information contained in the data. The characteristics of the display device can significantly affect the faithfulness of the data presentation. Human hearing is a complex nonlinear process and the intuitive comprehension of the display requires consideration of perceptual interactions and the natural connection between the data and the perception of the data. This chapter proposes a perceptual framework for observable and systematic specification and comprehension of sounds in a device-independent auditory display of scientific data. The framework consists of a perceptually scaled sound space and a display model that maps points from the perceptual space to a display device parameter space while preserving the interrelationships between them. The advantages of a perceptual space are described using the example of an established framework for applying perceptually uniform color models to scientific visualization. A perceptual sound space is defined and constructed by scaling a naturally ordered sound model derived from research studies in timbre perception. An auditory display is implemented on a Sun Sparc10 workstation using Csound and samples of musical instruments. The display model consists of an interpolated mapping from perceptual space to display space and a description of the display gamut that is the boundary in perceptual space between points that the display can realize and those that it cannot. The display gamut can be used to analyze the display, compare displays, and optimize data mapping sequences for the display. The concepts are illustrated by a graphic visualization of the gamut of the implemented display as a geometric shape in the perceptual sound space. © 2005, ACM. All rights reserved.",Auditory display; Data mapping; Experimentation; Human Factors; Performance; Sonification,
Distance Perception and the Visual Horizon in Head-Mounted Displays,2005,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885205112&doi=10.1145%2f1077399.1077403&partnerID=40&md5=ec4e630945984874d27589dd41c0003f,"Can distance perception be studied using virtual reality (VR) if distances are systematically underestimated in VR head-mounted displays (HMDs)? In an experiment in which a real environment was observed through an HMD, via live video, distances, as measured by visually directed walking, were underestimated even when the perceived environment was known to be real and present. However, the underestimation was linear, which means that higher-order space perception effects might be preserved in VR. This is illustrated in a second experiment, in which the visual horizon was artificially manipulated in a simulated outdoor field presented in immersive VR. As predicted by the claim that angle of declination from the horizon may serve as a strong cue to distance, lowering the horizon line produced “expansive” judgments of distance (power function exponents greater than one) both in verbal and in motor estimates. © 2005, ACM. All rights reserved.",distance perception; Experimentation; head-mounted displays (HMD); Human Factors; space perception; Virtual reality (VR),
Using Wavelets to Synthesize Stochastic-Based Sounds for Immersive Virtual Environments,2005,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-70349817573&doi=10.1145%2f1101530.1101552&partnerID=40&md5=70d0b252b02c4d5326dce619d56de590,"Stochastic, or nonpitched, sounds fill our real-world environment. Humans almost continuously hear stochastic sounds, such as wind, rain, motor sounds, and different types of impact sounds. Because of their prevalence in real-world environments, it is important to include these types of sounds for realistic virtual environment simulations. This paper describes a synthesis approach that uses wavelets for modeling stochastic-based sounds. Parameterizations of the wavelet models yield a variety of related sounds from a small set of models. The result is dynamic sound models that can change according to changes in the virtual environment. This paper contains a description of the sound synthesis process, several developed models, and the ongoing perceptual experiments for validating the sound synthesis veracity. The developed models and results demonstrate proof of the concept and illustrate the potential of this approach. © 2005, ACM. All rights reserved.",audio perception; Environmental Sound Synthesis; immersive environments; Psychoacoustics; Sound synthesis; Stochastic; virtual reality; wavelets,
Manipulating Video Sequences to Determine the Components of Conversational Facial Expressions,2005,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33749046446&doi=10.1145%2f1077399.1077404&partnerID=40&md5=fa9ccaf7b132ad9509903714ea79498d,"Communication plays a central role in everday life. During an average conversation, information is exchanged in a variety of ways, including through facial motion. Here, we employ a custom, model-based image manipulation technique to selectively “freeze” portions of a face in video recordings in order to determine the areas that are sufficient for proper recognition of nine conversational expressions. The results show that most expressions rely primarily on a single facial area to convey meaning with different expressions using different areas. The results also show that the combination of rigid head, eye, eyebrow, and mouth motions is sufficient to produce expressions that are as easy to recognize as the original, unmanipulated recordings. Finally, the results show that the manipulation technique introduced few perceptible artifacts into the altered video sequences. This fusion of psychophysics and computer graphics techniques provides not only fundamental insights into human perception and cognition, but also yields the basis for a systematic description of what needs to move in order to produce realistic, recognizable conversational facial animations. © 2005, ACM. All rights reserved.",animation; Applied perception; computer graphics; Experimentation; facial expressions; human-computer interface,
Improving Human Haptic Performance in Normal and Impaired Human Populations Through Unattended Activation-Based Learning,2005,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-27844508002&doi=10.1145%2f1060581.1060583&partnerID=40&md5=6b8611f8d66f57edc66f968384afea7d,"Humanhaptic performance is not fixed, but subject to major alterations through learning processes. We describe a new stimulation protocol that allows to improve haptic performance in humans in a highly systemic way through unattended activation-based learning. The so-called coactivation protocol is based upon temporal constraints of Hebbian learning where simultaneity plays a key role for the induction of plastic changes. We provide an overview about the potential of coactivation by summarizing recent findings showing that coactivation alters a broad range of basic as well as cognitively demanding types of haptic performance in parallel to cortical reorganization in somatosensory cortex. For example, coactivation applied to the tip of the index finger, or to all fingers of the dominant hand improves tactile acuity, but also haptic object recognition, and speeds up multiple-choice reaction times. Because such changes persist between 24 h and 1 week without further intervention, we interpret the underlying processes as a particular form of perceptual learning. We describe results where coactivation has been utilized for therapeutical purposes in impaired human populations, we outline new developments to optimize and extend unattended activation-based learning protocols, and we sketch the next steps necessary to apply the concept of unattended activation-based learning on a regular and reliable basis as a therapeutical tool in order to selectively interfere with impaired haptic performance. © 2005, ACM. All rights reserved.",aging; cognition; cortical reorganization; Experimentation; Human Factors; learn-ware; Learning; plasticity; therapy,
Distance Perception in Real and Virtual Environments,2005,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84919564575&doi=10.1145%2f1077399.1077402&partnerID=40&md5=2ceb4dbb938101cfbe9ebd9381506c41,"We conducted three experiments to compare distance perception in real and virtual environments. In Experiment 1, adults estimated how long it would take to walk to targets in real and virtual environments by starting and stopping a stopwatch while looking at a target person standing between 20 and 120 ft away. The real environment was a large grassy lawn in front of a university building. We replicated this scene in our virtual environment using a nonstereoscopic, large-screen immersive display system. We found that people underestimated time to walk in both environments for distances of 40 to 60 ft and beyond. However, time-to-walk estimates were virtually identical across the two environments, particularly when people made real environment estimates first. In Experiment 2, 10- and 12-year-old children and adults estimated time to walk in real and virtual environments both with and without vision. Adults underestimated time to walk in both environments for distances of 60 to 80 ft and beyond. Again, their estimates were virtually identical in the real and virtual environment both with and without vision. Twelve-yearolds' time-to-walk estimates were also very similar across the two environments under both viewing conditions, but 10-year-olds exhibited greater underestimation in the virtual than in the real environment. A third experiment showed that adults' time-towalk estimates were virtually identical to walking without vision. We conclude that distance perception may be better in virtual environments involving large-screen immersive displays than in those involving head-mounted displays (HMDS). © 2005, ACM. All rights reserved.",distance estimation; Experimentation; large-screen immersive displays; Measurement; perception; Performance; Virtual environments,
Example-Based Color Stylization of Images,2005,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84947263356&doi=10.1145%2f1077399.1077408&partnerID=40&md5=c6f7fef8c89816ec5d34452898c14a3f,"We describe a new computational approach to stylize the colors of an image by using a reference image. During processing, we take the characteristics of human color perception into account to generate more appealing results. Our system starts by classifying each pixel value into one of the basic color categories, derived from our psychophysical experiments. The basic color categories are perceptual categories that are universal to everyone, regardless of nationality or cultural background. These categories are used to provide restrictions on color transformations to avoid generating unnatural results. Our system then renders a new image by transferring colors from a reference image to the input image, based on these categorizations. To avoid artifacts due to the explicit clustering, our system defines fuzzy categorization when pseudocontours appear in the resulting image. We present a variety of results and show that our method performs a large, yet natural, color transformation without any sense of incongruity and that the resulting images automatically capture the characteristics of the colors used in the reference image. © 2005, ACM. All rights reserved.",Algorithms; Basic color categories; color transfer; Design; example-based; Experimentation,
A Multimodal Learning Interface for Grounding Spoken Language in Sensory Perceptions,2004,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867456688&doi=10.1145%2f1008722.1008727&partnerID=40&md5=9c576820e37f61172b8ce9dfb055a587,"We present a multimodal interface that learns words from natural interactions with users. In light of studies of human language development, the learning system is trained in an unsupervised mode in which users perform everyday tasks while providing natural language descriptions of their behaviors. The system collects acoustic signals in concert with user-centric multisensory information from nonspeech modalities, such as user's perspective video, gaze positions, head directions, and hand movements. A multimodal learning algorithm uses this data to first spot words from continuous speech and then associate action verbs and object names with their perceptually grounded meanings. The central ideas are to make use of nonspeech contextual information to facilitate word spotting, and utilize body movements as deictic references to associate temporally cooccurring data from different modalities and build hypothesized lexical items. From those items, an EM-based method is developed to select correct word—meaning pairs. Successful learning is demonstrated in the experiments of three natural tasks: “unscrewing a jar,” “stapling a letter,” and “pouring water.”. © 2004, ACM. All rights reserved.",Cognitive modeling; Experimentation; Human Factors; Multimodal interaction; Multimodal learning,
"EvoFIT: A Holistic, Evolutionary Facial Imaging Technique for Creating Composites",2004,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024289873&doi=10.1145%2f1008722.1008725&partnerID=40&md5=ee2b7f028c5b151e2a311836d26da3c2,"EvoFIT, a computerized facial composite system is being developed as an alternative to current systems. EvoFIT faces are initially presented to a witness with random characteristics, but through a process of selection and breeding, a composite is “evolved.” Comparing composites constructed with E-FIT, a current system, a naming rate of 10% was found for EvoFIT and 17% for E-FIT. Analysis revealed that target age was limiting factor for EvoFIT and a second study with age-appropriate targets visible during composite construction produced a naming rate similar to E-FIT. Two more-realistic studies were conducted that involved young target faces and two current systems (E-FIT and PROfit). Composites from both of these experiments were poorly named but a significant benefit emerged for EvoFIT. © 2004, ACM. All rights reserved.",Design; Experimentation; Human factors,
Understanding Concurrent Earcons: Applying Auditory Scene Analysis Principles to Concurrent Earcon Recognition,2004,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019477834&doi=10.1145%2f1024083.1024087&partnerID=40&md5=d908c531d77babeafc27ae5e306603c0,"Two investigations into the identification of concurrently presented, structured sounds, called earcons were carried out. One of the experiments investigated how varying the number of concurrently presented earcons affected their identification. It was found that varying the number had a significant effect on the proportion of earcons identified. Reducing the number of concurrently presented earcons lead to a general increase in the proportion of presented earcons successfully identified. The second experiment investigated how modifying the earcons and their presentation, using techniques influenced by auditory scene analysis, affected earcon identification. It was found that both modifying the earcons such that each was presented with a unique timbre, and altering their presentation such that there was a 300 ms onset-to-onset time delay between each earcon were found to significantly increase identification. Guidelines were drawn from this work to assist future interface designers when incorporating concurrently presented earcons. © 2004, ACM. All rights reserved.",Auditory display; Auditory scene analysis; Earcons; Experimentation; Human factors; Sonification,
The Effects of Subpixel Addressing on Users' Performance and Preferences During Reading-Related Tasks,2004,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84907306815&doi=10.1145%2f1024083.1024084&partnerID=40&md5=069c0bb3e5d6ef73a54fd935cd05389d,"Subpixel addressing is a font-rendering technology that triples the apparent horizontal resolution of liquid crystal displays. Four experiments measured the effects of subpixel addressing (Microsoft's ClearType) relative to standard (aliased) font-rendering techniques. Participants preferred, and gave higher readability ratings to, text that had been rendered using subpixel addressing. Subpixel addressing also significantly improved the accuracy of lexical decisions and the accuracy and speed of sentence comprehension. Subpixel addressing did not affect word-naming performance or reading speed during pleasure reading. Taken together, these findings suggest that subpixel addressing provides substantial benefits to users while adding no costs to display hardware. © 2004, ACM. All rights reserved.",Displays; Font rendering; Human Factors; Readability; Subpixel addressing,
Editorial,2004,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024287415&doi=10.1145%2f1008722.1008723&partnerID=40&md5=f266448a3756a52489809171b05d3889,[No abstract available],,
Motion to Support Rapid Interactive Queries on Node-Link Diagrams,2004,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84996246376&doi=10.1145%2f1008722.1008724&partnerID=40&md5=de68dc0e3690287854f95f5b5f701663,"Many different problems can be represented as graphs displayed in the form of node—link diagrams. However, when a graph is large it becomes visually uninterpretable because of the tangle of links. We describe a set of techniques that use motion in an interactive interface to provide effective access to larger graphs. Touching a node with the mouse cursor causes that node and the subgraph of closely connected nodes to oscillate. We argue from perceptual principles that this should be a more effective way of interactively highlighting a subgraph than more conventional static methods. The MEGraph system was developed to gain experience with different forms of motion highlighting. Based on positive feedback, three experiments were carried out to evaluate the effectiveness of motion highlighting for specific tasks. All three showed motion to be more effective than static highlighting, both in increasing the speed of response for a variety of visual queries, and in reducing errors.We argue that motion highlighting can be a valuable technique in applications that require users to understand large graphs. © 2004, ACM. All rights reserved.",Design; Design; Experimentation; Human Factors; Human—computer interaction; Information visualization; Interactive visualization; Motion highlighting; Visual queries; Visualization: data visualization,
Numerically Estimating Internal Models of Dynamic Virtual Objects,2004,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33751200407&doi=10.1145%2f1024083.1024085&partnerID=40&md5=a607b3843cd6525e0386969bd4c7b2b3,"Precise manipulation of objects is ordinarily limited by visual, kinesthetic, motor, and cognitive factors. Specially designed virtual objects and tasks minimize such limitations, making it possible to isolate and estimate the internal model that guides subjects' performance. Subjects manipulated a computer-generated virtual object (vO), attempting to align vO to a target whose position changed randomly every 10 s. To analyze the control actions subjects use while manipulating the vO, we benchmarked human performance against that of ideal performers (IPs), behavioral counterparts to ideal observers used in sensory research. These comparisons showed that subjects performed as feed-forward, predictive controllers. Simulations with degraded-IPs suggest that human asymptotic performance was not limited by imprecisions of vision or of motor timing, but resulted mainly from inaccuracies in the internal models of vO dynamics. © 2004, ACM. All rights reserved.",Dynamics; Experimentation; Human cognition; Human Factors; Human information processing; Ideal performer; Internal model; Performance; Virtual object; Virtual reality,
Auditory Perception of 3D Size:Experiments with Synthetic Resonators,2004,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33749549201&doi=10.1145%2f1024083.1024086&partnerID=40&md5=b732f67364076f7cdd630ce97d51d8a0,"For representing complex data through auditory display, the ecological approach to sound perception, if combined with the most advanced techniques for physical modeling, seems to form a good strategy to translate information into sound by composition of auditory scenes and to create an object-based auditory information space. We examine the auditory perception of one feature of simulated 3D resonators: the size. In prior publications, we assessed the ability of humans to distinguish the shapes of (spherical and cubic) resonators, and we investigated pitch perception and its relationship to the volume of the enclosures. In this paper, we focus on the latter topic, introducing our most recent experiments and results. We validate previous results in a more controlled environment and, in particular, we investigate whether the specific procedure used in the auditory display may affect the users' performance. The results of the experiment show that even the perception of a basic object property such as size can be affected by the listening procedure and can be decoupled from object shape only in controlled conditions. © 2004, ACM. All rights reserved.",AuDitory display; Auditory perception; Design; Experimentation; Performance; Pitch; Spherical and cubic resonators; Volume,
Gazing and Frowning as a New Human-Computer Interaction Technique,2004,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-29144523055&doi=10.1145%2f1008722.1008726&partnerID=40&md5=7ecb815da800b43fe0e6939f870df3d6,"The present aim was to study a new technique for human—computer interaction. It combined the use of two modalities, voluntary gaze direction and voluntary facial muscle activation for object pointing and selection. Fourteen subjects performed a series of pointing tasks with the new technique and with a mouse. At short distances the mouse was significantly faster than the new technique. However, there were no statistically significant differences at medium and long distances between the techniques. Fitts' law analyses were performed both by using only error-free trials and using also data including error trials (i.e., effective target width). In all cases both techniques seemed to follow Fitts' law, although for the new technique the effective target width correlation coefficient was smaller R = 0.776 than for the mouse R = 0.991. The regression slopes suggested that at very long distances (i.e., beyond 800 pixels) the new technique might be faster than the mouse. The new technique showed promising results already after a short practice and in the future it could be useful especially for physically challenged persons. © 2004, ACM. All rights reserved.",Electromyography; Experimentation; Facial muscle activity; Gaze direction; Human Factors; Performance,
Experience Matters: Longitudinal Changes in Sensitivity to Rotational Gains in Virtual Reality,2022,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144598124&doi=10.1145%2f3560818&partnerID=40&md5=b9141698e0fce18e34e8eded0880cfe3,"Redirected walking techniques use rotational gains to guide users away from physical obstacles as they walk in a virtual world, effectively creating the illusion of a larger virtual space than is physically present. Designers often want to keep users unaware of this manipulation, which is made possible by limitations in human perception that render rotational gains imperceptible below a certain threshold. Many aspects of these thresholds have been studied; however, no research has yet considered whether these thresholds may change over time as users gain more experience with them. To study this, we recruited 20 novice VR users (no more than 1 hour of prior experience with an HMD) and provided them with an Oculus Quest to use for 4 weeks on their own time. They were tasked to complete an activity assessing their sensitivity to rotational gain once each week, in addition to whatever other activities they wanted to perform. No feedback was provided to participants about their performance during each activity, minimizing the possibility of learning effects accounting for any observed changes over time. We observed that participants became significantly more sensitive to rotation gains over time, underscoring the importance of considering prior user experience in applications involving rotational gain, as well as how prior user experience may affect other, broader applications of VR.  © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",longitudinal; perception; Rotational gain; virtual reality,User experience; User interfaces; Change-over time; Human perception; Longitudinal; No feedbacks; Prior experience; Redirected walkings; Rotational gain; Users' experiences; Virtual spaces; Virtual worlds; Virtual reality
Sensitivity to Hand Offsets and Related Behavior in Virtual Environments over Time,2022,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147931305&doi=10.1145%2f3561055&partnerID=40&md5=5c6bf285389f58f255b1078dde8fd55f,"This work explored how users' sensitivity to offsets in their avatars' virtual hands changes as they gain exposure to virtual reality. We conducted an experiment using a two-alternative forced choice (2-AFC) design over the course of 4 weeks, split into four sessions. The trials in each session had a variety of eight offset distances paired with eight offset directions (across a two-dimensional plane). While we did not find evidence that users became more sensitive to the offsets over time, we did find evidence of behavioral changes. Specifically, participants' head-hand coordination and completion time varied significantly as the sessions went on. We discuss the implications of both results and how they could influence our understanding of long-term calibration for perception-action coordination in virtual environments.  © 2022 Association for Computing Machinery.",Body awareness; calibration; hand offsets; longitudinal,Curricula; Human engineering; Virtual reality; Alternative forced choice; Behavioral changes; Body awareness; Completion time; Hand offset; Longitudinal; Offset distances; Perception/action; Two dimensional plane; Virtual hand; Calibration
"Investigating a Combination of Input Modalities, Canvas Geometries, and Inking Triggers on On-Air Handwriting in Virtual Reality",2022,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147908831&doi=10.1145%2f3560817&partnerID=40&md5=b46d0b9a7f59b40f3a22805679e295a7,"Humans communicate by writing, often taking notes that assist thinking. With the growing popularity of collaborative Virtual Reality (VR) applications, it is imperative that we better understand aspects that affect writing in these virtual experiences. On-air writing in VR is a popular writing paradigm due to its simplicity in implementation without any explicit needs for specialized hardware. A host of factors can affect the efficacy of this writing paradigm and in this work, we delved into investigating the same. Along these lines, we investigated the effects of a combination of factors on users' on-air writing performance, aiming to understand the circumstances under which users can both effectively and efficiently write in VR. We were interested in studying the effects of the following factors: (1) input modality: brush vs. near-field raycast vs. pointing gesture, (2) inking trigger method: haptic feedback vs. button based trigger, and (3) canvas geometry: plane vs. hemisphere. To evaluate the writing performance, we conducted an empirical evaluation with thirty participants, requiring them to write the words we indicated under different combinations of these factors. Dependent measures including the writing speed, accuracy rates, perceived workloads, and so on, were analyzed. Results revealed that the brush based input modality produced the best results in writing performance, that haptic feedback was not always effective over button based triggering, and that there are trade-offs associated with the different types of canvas geometries used. This work attempts at laying a foundation for future investigations that seek to understand and further improve the on-air writing experience in immersive virtual environments.  © 2022 Association for Computing Machinery.",interaction; interfaces; text entry; Virtual reality; writing,Economic and social effects; Geometry; Collaborative virtual reality; Haptic feedbacks; Input modalities; Interaction; Near fields; Pointing gestures; Specialized hardware; Text entry; Writing; Writing performance; Virtual reality
Perceptual Guidelines for Optimizing Field of View in Stereoscopic Augmented Reality Displays,2022,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147947046&doi=10.1145%2f3554921&partnerID=40&md5=b43e96ad79ab7d7597023110be52372e,"Near-eye display systems for augmented reality (AR) aim to seamlessly merge virtual content with the user's view of the real-world. A substantial limitation of current systems is that they only present virtual content over a limited portion of the user's natural field of view (FOV). This limitation reduces the immersion and utility of these systems. Thus, it is essential to quantify FOV coverage in AR systems and understand how to maximize it. It is straightforward to determine the FOV coverage for monocular AR systems based on the system architecture. However, stereoscopic AR systems that present 3D virtual content create a more complicated scenario because the two eyes' views do not always completely overlap. The introduction of partial binocular overlap in stereoscopic systems can potentially expand the perceived horizontal FOV coverage, but it can also introduce perceptual nonuniformity artifacts. In this arrticle, we first review the principles of binocular FOV overlap for natural vision and for stereoscopic display systems. We report the results of a set of perceptual studies that examine how different amounts and types of horizontal binocular overlap in stereoscopic AR systems influence the perception of nonuniformity across the FOV. We then describe how to quantify the horizontal FOV in stereoscopic AR when taking 3D content into account. We show that all stereoscopic AR systems result in a variable horizontal FOV coverage and variable amounts of binocular overlap depending on fixation distance. Taken together, these results provide a framework for optimizing perceived FOV coverage and minimizing perceptual artifacts in stereoscopic AR systems for different use cases.  © 2022 held by the owner/author(s).",Augmented reality; binocular vision; field of view; near-eye display,Binocular vision; Binoculars; Display devices; Stereo image processing; Augmented reality systems; Current system; Eye-display systems; Field of views; Horizontal fields; Natural fields; Near-eye display; Nonuniformity; Real-world; User view; Augmented reality
Tactile Texture Display Combining Vibrotactile and Electrostatic-friction Stimuli: Substantial Effects on Realism and Moderate Effects on Behavioral Responses,2022,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147250949&doi=10.1145%2f3539733&partnerID=40&md5=698cfc9c753e1a27e8280d3b326c2b27,"There is increasing demand for tactile feedback functions for touch panels. We investigated whether virtual roughness texture quality can be improved through simultaneous use of vibrotactile and electrostatic-friction stimuli. This conjunctive use is expected to improve the perceptual quality of texture stimuli, because vibrotactile and electrostatic-friction stimuli have complementary characteristics. Our previous studies confirmed that these conjunct stimuli yield enhanced realism for simple grating roughness. In this study, we conducted experiments using simple and complex sinusoidal surface profiles consisting of one or two spatial wave components. Three different evaluation criteria were employed. The first criterion concerned the subjective realism, i.e., similarity with actual roughness textures, of virtual roughness textures. Participants compared the following three stimulus conditions: vibrotactile stimuli only, electrostatic-friction stimuli only, and their conjunct stimuli. The conjunct stimuli yielded the greatest realism. The second criterion concerned roughness texture identification under each of the three stimulus conditions for five different roughness textures. The highest identification accuracy rate was achieved under the conjunct stimulus condition; however, the performance difference was marginal. The third criterion concerned the discrimination threshold of the grating-scale spatial wavelength. There were no marked differences among the results for the three conditions. The findings of this study will improve virtual texture quality for touch-panel-type surface tactile displays.  © 2022 Copyright held by the owner/author(s).",electrostatic-friction stimuli; Tactile texture display; vibrotactile stimuli,Electrostatics; Friction; Interactive computer graphics; User interfaces; Condition; Electrostatic friction; Electrostatic-friction stimulus; Roughness texture; Tactile texture; Tactile texture display; Texture quality; Touch panels; Vibrotactile; Vibrotactile stimulus; Textures
Introduction to the Special Issue on SAP 2022,2022,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147913844&doi=10.1145%2f3563136&partnerID=40&md5=3638a1b1ce4cfc4b6aa35d87341a1b34,[No abstract available],,
Evaluating Realism in Example-based Terrain Synthesis,2022,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139220667&doi=10.1145%2f3531526&partnerID=40&md5=68b17609ee1eb5f7777781f0d74cc4ae,"We report two studies that investigate the use of subjective believability in the assessment of objective realism of terrain. The first demonstrates that there is a clear subjective feature bias that depends on the types of terrain being evaluated: Our participants found certain natural terrains to be more believable than others. This confounding factor means that any comparison experiment must not ask participants to compare terrains with different types of features. Our second experiment assesses four methods of example-based terrain synthesis, comparing them against each other and against real terrain. Our results show that, while all tested methods can produce terrain that is indistinguishable from reality, all also can produce poor terrain; that there is no one method that is consistently better than the others; and that those who have professional expertise in geology, cartography, or image analysis are better able to distinguish real terrain from synthesized terrain than the general population, but those who have professional expertise in the visual arts are not. Copyright © 2022 held by the owner/author(s). Publication rights licensed to ACM.",believability; evaluation; example-based; realism; Terrain,Maps; Believability; Evaluation; Example based; General population; Image-analysis; Professional expertise; Realism; Synthesised; Terrain; Terrain Synthesis; Landforms
Exploring Sonification Mapping Strategies for Spatial Auditory Guidance in Immersive Virtual Environments,2022,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139258305&doi=10.1145%2f3528171&partnerID=40&md5=06c0a5da5a09efd83b38af256239b0cc,"Spatial auditory cues are important for many tasks in immersive virtual environments, especially guidance tasks. However, due to the limited fidelity of spatial sounds rendered by generic Head-Related Transfer Functions (HRTFs), sound localization usually has a limited accuracy, especially in elevation, which can potentially impact the effectiveness of auditory guidance. To address this issue, we explored whether integrating sonification with spatial audio can enhance the perceptions of auditory guidance cues so user performance in auditory guidance tasks can be improved. Specifically, we investigated the effects of sonification mapping strategy using a controlled experiment that compared four elevation sonification mapping strategies: absolute elevation mapping, unsigned relative elevation mapping, signed relative elevation mapping, and binary relative elevation mapping. In addition, we examined whether azimuth sonification mapping can further benefit the perception of spatial sounds. The results demonstrate that spatial auditory cues can be effectively enhanced by integrating elevation and azimuth sonification, where the accuracy and speed of guidance tasks can be significantly improved. In particular, the overall results suggest that binary relative elevation mapping is generally the most effective strategy among four elevation sonification mapping strategies, which indicates that auditory cues with clear directional information are key to efficient auditory guidance. © 2022 Association for Computing Machinery.",3D sound; auditory feedback; Auditory user interface; interactive sonification; non-visual guidance; spatial audio,Audio acoustics; Feedback; Mapping; Virtual reality; 3D sound; Auditory feedback; Auditory user interfaces; Elevation mapping; Interactive sonification; Non visuals; Non-visual guidance; Sonifications; Spatial audio; Visual guidance; User interfaces
Machine Learning-based Modeling and Prediction of the Intrinsic Relationship between Human Emotion and Music,2022,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139227313&doi=10.1145%2f3534966&partnerID=40&md5=b85f055f69de656c9002e8af4bc7cec3,"Human emotion is one of the most complex psychophysiological phenomena and has been reported to be affected significantly by music listening. It is supposed that there is an intrinsic relationship between human emotion and music, which can be modeled and predicted quantitatively in a supervised manner. Here, a heuristic clustering analysis is carried out on large-scale free music archive to derive a genre-diverse music library, to which the emotional response of participants is measured using a standard protocol, consequently resulting in a systematic emotion-to-music profile. Eight machine learning methods are employed to statistically correlate the basic sound features of music audio tracks in the library with the measured emotional response of tested people to the music tracks in a training set and to blindly predict the emotional response from sound features in a test set. This study found that nonlinear methods are more robust and predictable but considerably more time-consuming than linear approaches. The neural networks have strong internal fittability but are associated with a significant overfitting issue. The support vector machine and Gaussian process exhibit both high internal stability and satisfactory external predictability in all used methods; they are considered as promising tools to model, predict, and explain the intrinsic relationship between human emotion and music. The psychological basis and perceptional implication underlying the built machine learning models are also discussed to find out the key music factors that affect human emotion. © 2022 Association for Computing Machinery.",genre-diverse music library; human emotion; machine learning; Music; psychophysiology; statistical modeling,Audio acoustics; Behavioral research; Computer music; Forecasting; Psychophysiology; Support vector machines; Clustering analysis; Emotional response; Genre-diverse music library; Human emotion; Large-scales; Learning Based Models; Machine-learning; Modelling and predictions; Music library; Statistic modeling; Music
"Creating Word Paintings Jointly Considering Semantics, Attention, and Aesthetics",2022,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139186809&doi=10.1145%2f3539610&partnerID=40&md5=3b3ca206d61977e869121c6b48615f66,"In this article, we present a content-aware method for generating a word painting. Word painting is a composite artwork made from the assemblage of words extracted from a given text, which carries similar semantics and visual features to a given source image. However, word painting, usually created by skilled artists, involves tedious manual processes, especially when generating streamlines and laying out text. Hence, we provide an easy method to create word paintings for users. How to design textural layout that simultaneously conveys the input image and enables easy access to the semantic theme is the key challenge to generating a visually pleasing word painting. To address this issue, given an image and its content-related text, we first decompose the input image into several regions and approximate each region with a smooth vector field. At the same time, by analyzing the input text, we extract some weighted keywords as the graphic elements. Then, to measure the likelihood of positions in the input image that attract the observers' attention, we generate a saliency map with our trained visual attention model. Finally, jointly considering visual attention and aesthetic rules, we propose an energy-based optimization framework to arrange extracted keywords into the decomposed regions and synthesize a word painting. Experimental results and user studies show that this method is able to generate a fashionable and appealing word painting. © 2022 Association for Computing Machinery.",Datasets; gaze detection; neural networks; text tagging,Behavioral research; Image processing; Neural networks; Semantics; Content-aware; Dataset; Gaze detection; Input image; Manual process; Neural-networks; Semantic features; Source images; Text tagging; Visual feature; Painting
Vibrotactile Threshold Measurements at the Wrist Using Parallel Vibration Actuators,2022,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139259708&doi=10.1145%2f3529259&partnerID=40&md5=e5da6351bf91e10c24179ebc87d9317f,"This article presents an investigation into the perceptual vibrotactile thresholds for a range of frequencies on both the inside and outside areas of the wrist when exciting the skin with parallel vibrations, realized using the L5 actuator made by Lofelt GmbH. The vibrotactile threshold of 30 participants was measured using a modified audiometry test for the frequency range of 25-1,000 Hz. The average threshold across the respective frequencies was then ultimately determined from acceleration minima. The results show that maximum sensitivity lies in the range of 100-275 Hz (peaking at 200 Hz) for the inside and 75-250 Hz (peaking at 125 Hz) for the outside of the wrist and that thresholds are overall higher for the hairy skin on the outside of the wrist than for the glabrous skin on the inside. The results also show that the vibrotactile thresholds varied highly between individuals. Hence, personalized threshold measurements at the actuator locations will be required to fine-tune a device for the user. This study is a part of an ongoing research and development project where the aim is to develop a tactile display device and a music encoding scheme with the purpose of augmenting the musical enjoyment of cochlear implant recipients. These results, along with results from planned follow-up experiments, will be used to determine the appropriate frequency range and to cast light on the dynamic range on offer for the tactile device. Copyright © 2022 held by the owner/author(s). Publication rights licensed to ACM.",Cochlear implants; music enjoyment; parallel vibration; psychophysical measurements; vibrotactile detection thresholds,Audio acoustics; Cochlear implants; Display devices; Music; Detection threshold; Frequency ranges; Maximum sensitivity; Music enjoyment; Parallel vibration; Personalized thresholds; Psychophysical measurements; Threshold measurements; Vibrotactile; Vibrotactile detection threshold; Actuators
Display-Size Dependent Effects of 3D Viewing on Subjective Impressions,2022,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134879450&doi=10.1145%2f3510461&partnerID=40&md5=8dd4f4137b8857e72138e045697ad856,"This paper describes how the screen size of 3D displays affect the subjective impressions of 3D-visualized content. The key requirement for 3D displays is the presentation of depth cues comprising binocular disparities and/or motion parallax; however, the development of displays and production of content that include these cues leads to an increase in costs. Given the variety of screen sizes, it is expected that 3D characteristics are experienced differently by viewers depending on the screen size. We asked 48 participants to evaluate the 3D experience when using three different-sized stereoscopic displays (11.5, 55, and 200 inches) with head trackers. The participants were asked to score presented stimuli on 20 opposite-term pairs based on the semantic differential method after viewing each of six stimuli. Using factor analysis, we extracted three principal factors: power, related to strong three-dimensionality, real, etc.; visibility, related to stable, natural, etc.; and space, related to agile, open, etc., which had proportions of variances of 0.317, 0.277, and 0.251, respectively; their cumulation was 0.844. We confirmed that the three different-sized displays did not produce the same subjective impressions of the 3D characteristics. In particular, on the small-sized display, we found larger effects on power and space impressions from motion parallax (η2 = 0.133 and 0.161, respectively) than for the other two sizes. We found degradation of the visibility impressions from binocular disparities, which might be caused by artifacts from stereoscopy. The effects of 3D viewing on subjective impression depends on the display size, and small-sized displays offer the largest benefits by adding 3D characteristics to 2D visualization.  © 2022 Association for Computing Machinery.",3D display; Binocular disparities; impression; motion parallax,Binoculars; Geometrical optics; Semantics; Stereo image processing; Three dimensional computer graphics; Visibility; 3-D displays; 3D viewing; 3D-displays; Binocular disparity; Display size; Impression; Motion parallax; Power; Screen sizes; Subjective impressions; Three dimensional displays
The Duration of an Auditory Icon Can Affect How the Listener Interprets Its Meaning,2022,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134876482&doi=10.1145%2f3527269&partnerID=40&md5=fb74bf3b003db84e60f38a3f14a99d49,"Initially introduced in the field of informatics, an auditory icon consists of a short sound that is present in everyday life, used to represent a specific event, object, function, or action. Auditory icons have been studied in various fields, and overall, compared to other types of auditory alarms, they can be very efficient in informing the listener about a situation or event. So far, auditory icons have been used with a wide range of durations, ranging from a few hundreds of milliseconds up to several seconds. Still little is known, however, about whether and how icon duration influences its interpretation. In the present study, we therefore asked listeners to rate 12 auditory icons, divided into four different sound categories (nonverbal human sounds, machine sounds, human activities, and animal vocalizations), in five different durations (200, 400, 800, 1,600, and 3,200 ms). They rated (1) how appropriately the icon sound itself represented the icon's referent and (2) how appropriately each duration of the icon sound represented the icon's referent. Overall, results demonstrate that the duration of the auditory icons in this stimulus set can directly affect how the icon represents the referent. Auditory icons in the test set characterized by human activities represented their referent most appropriately in a relatively shorter duration (400 or 800 ms). The majority of the auditory icons in the set consisting of machine sounds, nonverbal human sounds, and animal vocalizations, however, were considered as more appropriately representing their referent in longer durations (800 ms and 1,600 ms). Further systematic research is necessary to determine whether the duration effects shown here may generalize to other stimulus sets.  © 2022 Association for Computing Machinery.",acoustic characteristics; auditory alarm; Auditory icon; user preference,Acoustic characteristic; Auditory alarm; Auditory icon; Human activities; Human sounds; Informatics; Non-verbal human; Object actions; Object functions; User's preferences; Animals
On the Immersive Properties of High Dynamic Range Video,2022,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134875196&doi=10.1145%2f3524692&partnerID=40&md5=7ecff30634dd2431b85996f3a02cf863,"This paper presents the results from two studies which used a dual-task methodology to measure an audience's experience of immersion while watching video under typical television viewing conditions. Immersion was measured while participants watched either a high dynamic range, wide color gamut video or a standard dynamic range, standard color gamut video, in high definition or ultra-high definition. Other video parameters were carefully measured and controlled.The study found that high dynamic range, wide color gamut video is significantly more immersive than standard dynamic range, standard color gamut video in the chosen configuration. However, there was no evidence of significant differences in immersion between high-definition and ultra-high-definition resolutions.  © 2022 Copyright held by the owner/author(s).",broadcast; high dynamic range; Immersion; quality of experience,Quality of service; Broadcast; Color gamuts; Dynamic range; High definition; High dynamic range; Immersion; Immersive; Quality of experience; Ultra-high; Wide color gamut; Color
PTRM: Perceived Terrain Realism Metric,2022,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134876377&doi=10.1145%2f3514244&partnerID=40&md5=a689ca56692f0e968560fba065fbf4ab,"Terrains are visually prominent and commonly needed objects in many computer graphics applications. While there are many algorithms for synthetic terrain generation, it is rather difficult to assess the realism of a generated output. This article presents a first step toward the direction of perceptual evaluation for terrain models. We gathered and categorized several classes of real terrains, and we generated synthetic terrain models using computer graphics methods. The terrain geometries were rendered by using the same texturing, lighting, and camera position. Two studies on these image sets were conducted, ranking the terrains perceptually, and showing that the synthetic terrains are perceived as lacking realism compared to the real ones. We provide insight into the features that affect the perceived realism by a quantitative evaluation based on localized geomorphology-based landform features (geomorphons) that categorize terrain structures such as valleys, ridges, hollows, and so forth. We show that the presence or absence of certain features has a significant perceptual effect. The importance and presence of the terrain features were confirmed by using a generative deep neural network that transferred the features between the geometric models of the real terrains and the synthetic ones. The feature transfer was followed by another perceptual experiment that further showed their importance and effect on perceived realism. We then introduce Perceived Terrain Realism Metrics (PTRM), which estimates human-perceived realism of a terrain represented as a digital elevation map by relating the distribution of terrain features with their perceived realism. This metric can be used on a synthetic terrain, and it will output an estimated level of perceived realism. We validated the proposed metrics on real and synthetic data and compared them to the perceptual studies.  © 2022 Copyright held by the owner/author(s).",feature transfer; neural networks; Procedural modeling; terrains; visual perception,Deep neural networks; Interactive computer graphics; Computer graphics applications; Feature transfers; Neural-networks; Perceived realisms; Procedural models; Terrain; Terrain features; Terrain generations; Terrain Modeling; Visual perception; Landforms
A Virtual Reality Application of the Rubber Hand Illusion Induced by Ultrasonic Mid-air Haptic Stimulation,2022,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124936958&doi=10.1145%2f3487563&partnerID=40&md5=3299947e21754d17aef081ad7fedaa14,"Ultrasonic mid-air haptic technologies, which provide haptic feedback through airwaves produced using ultrasound, could be employed to investigate the sense of body ownership and immersion in virtual reality (VR) by inducing the virtual hand illusion (VHI). Ultrasonic mid-air haptic perception has solely been investigated for glabrous (hairless) skin, which has higher tactile sensitivity than hairy skin. In contrast, the VHI paradigm typically targets hairy skin without comparisons to glabrous skin. The aim of this article was to investigate illusory body ownership, the applicability of ultrasonic mid-air haptics, and perceived immersion in VR using the VHI. Fifty participants viewed a virtual hand being stroked by a feather synchronously and asynchronously with the ultrasonic stimulation applied to the glabrous skin on the palmar surface and the hairy skin on the dorsal surface of their hands. Questionnaire responses revealed that synchronous stimulation induced a stronger VHI than asynchronous stimulation. In synchronous conditions, the VHI was stronger for palmar stimulation than dorsal stimulation. The ultrasonic stimulation was also perceived as more intense on the palmar surface compared to the dorsal surface. Perceived immersion was not related to illusory body ownership per se but was enhanced by the provision of synchronous stimulation.  © 2022 Association for Computing Machinery.",body ownership; immersion; skin type; UltraLeap; ultrasonic mid-air haptics; Virtual hand illusion,Body ownership; Haptic technology; Haptics; Immersion; Skin type; Ultraleap; Ultrasonic mid-air haptic; Ultrasonic stimulations; Virtual hand; Virtual hand illusion; Virtual reality
The Effects on Driving Behavior When Using a Head-mounted Display in a Dynamic Driving Simulator,2022,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150347905&doi=10.1145%2f3483793&partnerID=40&md5=35f22783ca58719e7a7ba64bf87e9753,"Driving simulators are established tools used during automotive development and research. Most simulators use either monitors or projectors as their primary display system. However, the emergence of a new generation of head-mounted displays has triggered interest in using these as the primary display type. The general benefits and drawbacks of head-mounted displays are well researched, but their effect on driving behavior in a simulator has not been sufficiently quantified.This article presents a study of driving behavior differences between projector-based graphics and head-mounted display in a large dynamic driving simulator. This study has selected five specific driving maneuvers suspected of affecting driving behavior differently depending on the choice of display technology. Some of these maneuvers were chosen to reveal changes in lateral and longitudinal driving behavior. Others were picked for their ability to highlight the benefits and drawbacks of head-mounted displays in a driving context.The results show minor changes in lateral and longitudinal driver behavior changes when comparing projectors and a head-mounted display. The most noticeable difference in favor of projectors was seen when the display resolution is critical to the driving task. The choice of display type did not affect simulator sickness nor the realism rated by the subjects.  © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",driver behavior; driving simulator; Head-mounted display,Automobile simulators; Behavioral research; Street traffic control; Automotive development; Behaviour changes; Display resolutions; Display system; Display technologies; Driver's behavior; Driving behaviour; Driving simulator; Head-mounted-displays; Longitudinal driving; Helmet mounted displays
Motor Variability in Complex Gesture Learning: Effects of Movement Sonification and Musical Background,2022,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125348630&doi=10.1145%2f3482967&partnerID=40&md5=85c4211e59d8c07f0b6de096d57ac695,"With the increasing interest in movement sonification and expressive gesture-based interaction, it is important to understand which factors contribute to movement learning and how. We explore the effects of movement sonification and users' musical background on motor variability in complex gesture learning. We contribute an empirical study in which musicians and non-musicians learn two gesture sequences over three days, with and without movement sonification. Results show the interlaced interaction effects of these factors and how they unfold in the three-day learning process. For gesture 1, which is fast and dynamic with a direct ""action-sound""sonification, movement sonification induces higher variability for both musicians and non-musicians on day 1. While musicians reduce this variability to a similar level as no auditory feedback condition on day 2 and day 3, non-musicians remain to have significantly higher variability. Across three days, musicians also have significantly lower variability than non-musicians. For gesture 2, which is slow and smooth with an ""action-music""metaphor, there are virtually no effects. Based on these findings, we recommend future studies to take into account participants' musical background, consider longitudinal study to examine these effects on complex gestures, and use awareness when interpreting the results given a specific design of gesture and sound.  © 2022 Association for Computing Machinery.",auditory feedback; complex gesture learning; Motor variability; movement sonification; musical background,Auditory feedback; Complex gesture learning; Empirical studies; Expressive gestures; Gesture-based interaction; Learning effects; Motor variability; Movement sonification; Musical background; Sonifications; Music
The Perceptual Consistency and Association of the LMA Effort Elements,2022,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150375801&doi=10.1145%2f3473041&partnerID=40&md5=1beec07df6e6c5428d5c962fe082681f,"Laban Movement Analysis (LMA) and its Effort element provide a conceptual framework through which we can observe, describe, and interpret the intention of movement. Effort attributes provide a link between how people move and how their movement communicates to others. It is crucial to investigate the perceptual characteristics of Effort to validate whether it can serve as an effective framework to support a wide range of applications in animation and robotics that require a system for creating or perceiving expressive variation in motion. To this end, we first constructed an Effort motion database of short video clips of five different motions: walk, sit down, pass, put, wave performed in eight ways corresponding to the extremes of the Effort elements. We then performed a perceptual evaluation to examine the perceptual consistency and perceived associations among Effort elements: Space (Indirect/Direct), Time (Sustained/Sudden), Weight (Light/Strong), and Flow (Free/Bound) that appeared in the motion stimuli. The results of the perceptual consistency evaluation indicate that although the observers do not perceive the LMA Effort element 100% as intended, true response rates of seven Effort elements are higher than false response rates except for light Effort. The perceptual consistency results showed varying tendencies by motion. The perceptual association between LMA Effort elements showed that a single LMA Effort element tends to co-occur with the elements of other factors, showing significant correlation with one or two factors (e.g., indirect and free, light and free).  © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Laban Movement Analysis (LMA); LMA Effort; motion style perception; style association; style consistency,Conceptual frameworks; Laban movement analyse; Laban movement analyse effort; Motion style perception; Motion styles; Movement analysis; Response rate; Style association; Style consistency; Motion analysis
The Role of Subsurface Scattering in Glossiness Perception,2021,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112854619&doi=10.1145%2f3458438&partnerID=40&md5=70a3a88e2e9046674883b17305d3d189,"This study investigates the potential impact of subsurface light transport on gloss perception for the purposes of broadening our understanding of visual appearance in computer graphics applications. Gloss is an important attribute for characterizing material appearance. We hypothesize that subsurface scattering of light impacts the glossiness perception. However, gloss has been traditionally studied as a surface-related quality and the findings in the state-of-the-art are usually based on fully opaque materials, although the visual cues of glossiness can be impacted by light transmission as well. To address this gap and to test our hypothesis, we conducted psychophysical experiments and found that subjects are able to tell the difference in terms of gloss between stimuli that differ in subsurface light transport but have identical surface qualities and object shape. This gives us a clear indication that subsurface light transport contributes to a glossy appearance. Furthermore, we conducted additional experiments and found that the contribution of subsurface scattering to gloss varies across different shapes and levels of surface roughness. We argue that future research on gloss should include transparent and translucent media and to extend the perceptual models currently limited to surface scattering to more general ones inclusive of subsurface light transport. © 2021 Owner/Author.",gloss perception; Material appearance; MTurk; subsurface light transport; translucency perception,Computer graphics; Light transmission; Surface roughness; Visibility; Additional experiments; Computer graphics applications; Opaque materials; Perceptual model; Potential impacts; Psychophysical experiments; Subsurface scattering; Visual appearance; Surface scattering
An Extended Analysis on the Benefits of Dark Mode User Interfaces in Optical See-Through Head-Mounted Displays,2021,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113525572&doi=10.1145%2f3456874&partnerID=40&md5=d61ca7d396206bf44e1058048d38a0a9,"Light-on-dark color schemes, so-called ""Dark Mode,""are becoming more and more popular over a wide range of display technologies and application fields. Many people who have to look at computer screens for hours at a time, such as computer programmers and computer graphics artists, indicate a preference for switching colors on a computer screen from dark text on a light background to light text on a dark background due to perceived advantages related to visual comfort and acuity, specifically when working in low-light environments. In this article, we investigate the effects of dark mode color schemes in the field of optical see-through head-mounted displays (OST-HMDs), where the characteristic ""additive""light model implies that bright graphics are visible but dark graphics are transparent. We describe two human-subject studies in which we evaluated a normal and inverted color mode in front of different physical backgrounds and different lighting conditions. Our results indicate that dark mode graphics displayed on the HoloLens have significant benefits for visual acuity and usability, while user preferences depend largely on the lighting in the physical environment. We discuss the implications of these effects on user interfaces and applications. © 2021 ACM.",Augmented reality; dark mode; light mode; optical see-through display; user experience; vergence accommodation conflict; visual acuity,Color; Color computer graphics; Field emission displays; Helmet mounted displays; Lighting; Computer programmers; Computer screens; Display technologies; Extended analysis; Lighting conditions; Optical see-through head-mounted displays; Physical environments; Visual comfort; User interfaces
When Scents Help Me Remember My Password,2021,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113521488&doi=10.1145%2f3469889&partnerID=40&md5=2d1cb4fca0048e69eb6350ac78779925,"Current authentication processes overwhelmingly rely on audiovisual data, comprising images, text or audio. However, the use of olfactory data (scents) has remained unexploited in the authentication process, notwithstanding their verified potential to act as cues for information recall. Accordingly, in this paper, a new authentication process is proposed in which olfactory media are used as cues in the login phase. To this end, PassSmell , a proof of concept authentication application, is developed in which words and olfactory media act as passwords and olfactory passwords, respectively. In order to evaluate the potential of PassSmell, two different versions were developed, namely one which was olfactory-enhanced and another which did not employ olfactory media. Forty-two participants were invited to take part in the experiment, evenly split into a control and experimental group. For assessment purposes, we recorded the time taken to logon as well as the number of failed/successful login attempts; we also asked users to complete a Quality of Experience (QoE) questionnaire. In terms of time taken, a significant difference was found between the experimental and the control groups, as determined by an independent sample t-test. Similar results were found with respect to average scores and the number of successful attempts. Regarding user QoE, having olfactory media with words influenced the users positively, emphasizing the potential of using this kind of authentication application in the future. © 2021 ACM.",authentication; information recall; Olfactory media; olfactory passwords; QoE,Odors; Quality of service; Audio-visual data; Control groups; Experimental groups; Independent samples; Information recalls; Proof of concept; Quality of experience (QoE); T-tests; Authentication
Computational Model for Global Contour Precedence Based on Primary Visual Cortex Mechanisms,2021,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113553478&doi=10.1145%2f3459999&partnerID=40&md5=146f047c6ce1e32f78c0e92326f4cf43,"The edges of an image contains rich visual cognitive cues. However, the edge information of a natural scene usually is only a set of disorganized unorganized pixels for a computer. In psychology, the phenomenon of quickly perceiving global information from a complex pattern is called the global precedence effect (GPE). For example, when one observes the edge map of an image, some contours seem to automatically ""pop out""from the complex background. This is a manifestation of GPE on edge information and is called global contour precedence (GCP). The primary visual cortex (V1) is closely related to the processing of edges. In this article, a neural computational model to simulate GCP based on the mechanisms of V1 is presented. There are three layers in the proposed model: the representation of line segments, organization of edges, and perception of global contours. In experiments, the ability to group edges is tested on the public dataset BSDS500. The results show that the grouping performance, robustness, and time cost of the proposed model are superior to those of other methods. In addition, the outputs of the proposed model can also be applied to the generation of object proposals, which indicates that the proposed model can contribute significantly to high-level visual tasks. © 2021 ACM.",Global contour precedence (GCP); multi-layer neural computational model; object proposal generation; perceptual organization,Computational methods; Vision; Complex background; Complex pattern; Computational model; Edge information; Global informations; Precedence effect; Primary visual cortex; Public dataset; Computation theory
Identification of Words and Phrases through a Phonemic-Based Haptic Display,2021,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113484013&doi=10.1145%2f3458725&partnerID=40&md5=8b5637a844606d1a2da770b9dc165f41,"Stand-alone devices for tactile speech reception serve a need as communication aids for persons with profound sensory impairments as well as in applications such as human-computer interfaces and remote communication when the normal auditory and visual channels are compromised or overloaded. The current research is concerned with perceptual evaluations of a phoneme-based tactile speech communication device in which a unique tactile code was assigned to each of the 24 consonants and 15 vowels of English. The tactile phonemic display was conveyed through an array of 24 tactors that stimulated the dorsal and ventral surfaces of the forearm. Experiments examined the recognition of individual words as a function of the inter-phoneme interval (Study 1) and two-word phrases as a function of the inter-word interval (Study 2). Following an average training period of 4.3 hrs on phoneme and word recognition tasks, mean scores for the recognition of individual words in Study 1 ranged from 87.7% correct to 74.3% correct as the inter-phoneme interval decreased from 300 to 0 ms. In Study 2, following an average of 2.5 hours of training on the two-word phrase task, both words in the phrase were identified with an accuracy of 75% correct using an inter-word interval of 1 sec and an inter-phoneme interval of 150 ms. Effective transmission rates achieved on this task were estimated to be on the order of 30 to 35 words/min. © 2021 Owner/Author.",,Linguistics; Sensory aids; Communication aids; Effective transmission rate; Human computer interfaces; Perceptual evaluation; Remote communication; Sensory impairment; Stand-alone devices; Word recognition; Speech communication
MovEcho: A Gesture-Sound Interface Allowing Blind Manipulations in a Driving Context,2021,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113514012&doi=10.1145%2f3464692&partnerID=40&md5=3fa807fd8cc2d66191d04bc080b07097,"Most recent vehicles are equipped with touchscreens, which replace arrays of buttons that control secondary driving functions, such as temperature level, strength of ventilation, GPS, or choice of radio stations. While driving, manipulating such interfaces can be problematic in terms of safety, because they require the drivers' sight. In this article, we develop an innovative interface, MovEcho, which is piloted with gestures and associated with sounds that are used as informational feedback. We compare this interface to a touchscreen in a perceptual experiment that took place in a driving simulator. The results show that MovEcho allows for a better visual task completion related to traffic and is preferred by the participants. These promising results in a simulator condition have to be confirmed in future studies, in a real vehicle with a comparable expertise for both interfaces. © 2021 ACM.",cognitive load; Multisensory perception; virtual reality,Psychology computing; Sensory perception; Blind manipulation; Driving functions; Driving simulator; Informational feedback; Real vehicles; Sound interface; Temperature level; Visual tasks; Radio stations
Eye Tracking Interaction on Unmodified Mobile VR Headsets Using the Selfie Camera,2021,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112721554&doi=10.1145%2f3456875&partnerID=40&md5=02e3e8bc6a129eb73495e8afbf183c79,"Input methods for interaction in smartphone-based virtual and mixed reality (VR/MR) are currently based on uncomfortable head tracking controlling a pointer on the screen. User fixations are a fast and natural input method for VR/MR interaction. Previously, eye tracking in mobile VR suffered from low accuracy, long processing time, and the need for hardware add-ons such as anti-reflective lens coating and infrared emitters. We present an innovative mobile VR eye tracking methodology utilizing only the eye images from the front-facing (selfie) camera through the headset's lens, without any modifications. Our system first enhances the low-contrast, poorly lit eye images by applying a pipeline of customised low-level image enhancements suppressing obtrusive lens reflections. We then propose an iris region-of-interest detection algorithm that is run only once. This increases the iris tracking speed by reducing the iris search space in mobile devices. We iteratively fit a customised geometric model to the iris to refine its coordinates. We display a thin bezel of light at the top edge of the screen for constant illumination. A confidence metric calculates the probability of successful iris detection. Calibration and linear gaze mapping between the estimated iris centroid and physical pixels on the screen results in low latency, real-time iris tracking. A formal study confirmed that our system's accuracy is similar to eye trackers in commercial VR headsets in the central part of the headset's field-of-view. In a VR game, gaze-driven user completion time was as fast as with head-tracked interaction, without the need for consecutive head motions. In a VR panorama viewer, users could successfully switch between panoramas using gaze. © 2021 ACM.",eye tracking; Mobile VR,Cameras; Image enhancement; Image segmentation; Iterative methods; Mixed reality; Smartphones; Anti-reflective; Completion time; Formal studies; Geometric modeling; Infrared emitters; Iris detection; Processing time; Region of interest; Eye tracking
Does What We See Shape History? Examining Workload History as a Function of Performance and Ambient/Focal Visual Attention,2021,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108025018&doi=10.1145%2f3449066&partnerID=40&md5=2ae490ccd577cb7bb4767a5e04d6229d,"Changes in task demands can have delayed adverse impacts on performance. This phenomenon, known as the workload history effect, is especially of concern in dynamic work domains where operators manage fluctuating task demands. The existing workload history literature does not depict a consistent picture regarding how these effects manifest, prompting research to consider measures that are informative on the operator's process. One promising measure is visual attention patterns, due to its informativeness on various cognitive processes. To explore its ability to explain workload history effects, participants completed a task in an unmanned aerial vehicle command and control testbed where workload transitioned gradually and suddenly. The participants' performance and visual attention patterns were studied over time to identify workload history effects. The eye-Tracking analysis consisted of using a recently developed eye-Tracking metric called coefficient K, as it indicates whether visual attention is more focal or ambient. The performance results found workload history effects, but it depended on the workload level, time elapsed, and performance measure. The eye-Tracking analysis suggested performance suffered when focal attention was deployed during low workload, which was an unexpected finding. When synthesizing these results, they suggest unexpected visual attention patterns can impact performance immediately over time. Further research is needed; however, this work shows the value of including a real-Time visual attention measure, such as coefficient K, as a means to understand how the operator manages varying task demands in complex work environments. © 2021 ACM.",eye tracking; UAV; Workload history,Antennas; Eye tracking; Cognitive process; Command and control; Eye-tracking analysis; Impact performance; Informative ness; Performance measure; Visual Attention; Work environments; Behavioral research
Field-of-View Restriction to Reduce VR Sickness Does Not Impede Spatial Learning in Women,2021,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108028924&doi=10.1145%2f3448304&partnerID=40&md5=56f52a68398acd4dc81e61ffb02fc7b1,"Women are more likely to experience virtual reality (VR) sickness than men, which could pose a major challenge to the mass market success of VR. Because VR sickness often results from a visual-vestibular conflict, an effective strategy to mitigate conflict is to restrict the user's field-of-view (FOV) during locomotion. Sex differences in spatial cognition have been well researched, with several studies reporting that men exhibit better spatial navigation performance in desktop three-dimensional environments than women. However, additional research suggests that this sex difference can be mitigated by providing a larger FOV as this increases the availability of landmarks, which women tend to rely on more than men. Though FOV restriction is already a widely used strategy for VR headsets to minimize VR sickness, it is currently not well understood if it impedes spatial learning in women due to decreased availability of landmarks. Our study (n=28, 14 men and 14 women) found that a dynamic FOV restrictor was equally effective in reducing VR sickness in both sexes, and no sex differences in VR sickness incidence were found. Our study did find a sex difference in spatial learning ability, but an FOV restrictor did not impede spatial learning in either sex. © 2021 ACM.",field-of-view manipulation; virtual locomotion; Virtual reality; VR sickness,Diseases; Selenium compounds; Field of views; Market success; Sex difference; Sickness incidence; Spatial cognition; Spatial learning; Spatial navigation; Three-dimensional environment; Virtual reality
Effects of Tactile Textures on Preference in Visuo-Tactile Exploration,2021,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108025827&doi=10.1145%2f3449065&partnerID=40&md5=ce59a92025bcc796dad2b28fa70973ff,"The use of haptic technologies has recently become immensely essential in Human-Computer Interaction to improve user experience and performance. With the introduction of tactile feedback on a touchscreen device, commonly known as surface haptics, several applications and interaction paradigms have become a reality. However, the effects of tactile feedback on the preference of 2D images in visuo-Tactile exploration task on touchscreen devices remain largely unknown. In this study, we investigated differences of preference score (the tendency of participants to like/dislike a 2D image based on its visual and tactile properties), reach time, interaction time, and response time under four conditions of feedback: no tactile feedback, high-quality of tactile information (sharp tactile texture), low-quality of tactile information (blurred tactile texture), and incorrect tactile information (mismatch tactile texture). The tactile feedback is rendered in the form of roughness that is simulated by modulating the friction between the finger and the surface and is derived from the 2D image. Thirty-six participants completed visuo-Tactile exploration tasks for a total of 36 trials (3 2D images × 4 tactile textures × 3 repetitions). Results showed that the presence of tactile feedback enhanced users' preference (tactile feedback conditions were rated significantly higher than the no tactile feedback condition for preference regardless of the quality/correctness of tactile feedback). This finding is also supported through results from self-reporting where 88.89% of participants preferred to experience the 2D image with tactile feedback. Additionally, the presence of tactile feedback resulted in significantly larger interaction time and response time compared to the no tactile feedback condition. Furthermore, the quality and correctness of tactile information significantly impacted the preference rating (sharp tactile textures were rated statistically higher than blurred tactile and mismatched tactile textures). All of these findings demonstrate that tactile feedback plays a crucial role in users' preference and thus motivates further the development of surface haptic technologies. © 2021 ACM.",affective computing; Haptic texture; tactile perception,Human computer interaction; Textures; User experience; Haptic technology; Interaction paradigm; Interaction time; Tactile exploration; Tactile feedback; Tactile information; Tactile properties; Tactile texture; Image texture
Spot the difference: Accuracy of numerical simulations via the human visual system,2021,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108028311&doi=10.1145%2f3449064&partnerID=40&md5=71d0c5801552973fa25e1b44f5d2b5e2,"Comparative evaluation lies at the heart of science, and determining the accuracy of a computational method is crucial for evaluating its potential as well as for guiding future efforts. However, metrics that are typically used have inherent shortcomings when faced with the under-resolved solutions of real-world simulation problems. We show how to leverage the human visual system in conjunction with crowd-sourced user studies to address the fundamental problems of widely used classical evaluation metrics. We demonstrate that such user studies driven by visual perception yield a very robust metric and consistent answers for complex phenomena without any requirements for proficiency regarding the physics at hand. This holds even for cases away from convergence where traditional metrics often end up with inconclusive results. More specifically, we evaluate results of different essentially non-oscillatory (ENO) schemes in different fluid flow settings. Our methodology represents a novel and practical approach for scientific evaluations that can give answers for previously unsolved problems. © 2021 ACM.",essentially non-oscillatory schemes; Evaluation of numerical simulation; human visual system,Flow of fluids; Petroleum reservoir evaluation; Comparative evaluations; Essentially non-oscillatory; Evaluation metrics; Human Visual System; Real-world simulation; Scientific evaluations; Unsolved problems; Visual perception; Image processing
Estimating Distances in Action Space in Augmented Reality,2021,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108027060&doi=10.1145%2f3449067&partnerID=40&md5=e2ca04dfd8512ae1565224029f89cb12,"Augmented reality (AR) is important for training complex tasks, such as navigation, assembly, and medical procedures. The effectiveness of such training may depend on accurate spatial localization of AR objects in the environment. This article presents two experiments that test egocentric distance perception in augmented reality within and at the boundaries of action space (up to 35 m) in comparison with distance perception in a matched real-world (RW) environment. Using the Microsoft HoloLens, in Experiment 1, participants in two different RW settings judged egocentric distances (ranging from 10 to 35 m) to an AR avatar or a real person using a visual matching measure. Distances to augmented targets were underestimated compared to real targets in the two indoor, RW contexts. Experiment 2 aimed to generalize the results to an absolute distance measure using verbal reports in one of the indoor environments. Similar to Experiment 1, distances to augmented targets were underestimated compared to real targets. We discuss these findings with respect to the importance of methodologies that directly compare performance in real and mediated environments, as well as the inherent differences present in mediated environments that are ""matched""to the real world. © 2021 ACM.",Augmented reality; avatars; distance perception,Depth perception; Absolute distance; Action spaces; Complex task; Distance perception; Indoor environment; Medical procedures; Spatial localization; Visual matching; Augmented reality
Comparison of subjective methods for quality assessment of 3D graphics in virtual reality,2021,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100297744&doi=10.1145%2f3427931&partnerID=40&md5=9568e468aa8cd0c49209354f2d853937,"Numerous methodologies for subjective quality assessment exist in the field of image processing. In particular, the Absolute Category Rating with Hidden Reference (ACR-HR), the Double Stimulus Impairment Scale (DSIS), and the Subjective Assessment Methodology for Video Quality (SAMVIQ) are considered three of the most prominent methods for assessing the visual quality of 2D images and videos. Are these methods valid/accurate to evaluate the perceived quality of 3D graphics data? Is the presence of an explicit reference necessary, due to the lack of human prior knowledge on 3D graphics data compared to natural images/videos? To answer these questions, we compare these three subjective methods (ACR-HR, DSIS, and SAMVIQ) on a dataset of high-quality colored 3D models, impaired with various distortions. These subjective experiments were conducted in a virtual reality environment. Our results show differences in the performance of the methods depending on the 3D contents and the types of distortions. We show that DSIS and SAMVIQ outperform ACR-HR in terms of accuracy and point out a stable performance. In regard to the time-effort, DSIS achieves the highest accuracy in the shortest assessment time. Results also yield interesting conclusions on the importance of a reference for judging the quality of 3D graphics. We finally provide recommendations regarding the influence of the number of observers on the accuracy. © 2020 Association for Computing Machinery.",3D graphics; Accuracy; Double stimulus; SAMVIQ; Single stimulus; Subjective methodologies; Time-effort; Visual quality assessment,Image quality; Absolute category ratings; Perceived quality; Quality assessment; Subjective assessments; Subjective experiments; Subjective methods; Subjective quality assessments; Virtual-reality environment; Virtual reality
Evaluating automated face identity-masking methods with human perception and a deep convolutional neural network,2021,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100291124&doi=10.1145%2f3422988&partnerID=40&md5=73507d0cb144448916ae82756781b61b,"Face de-identification (or “masking”) algorithms have been developed in response to the prevalent use of video recordings in public places. We evaluated the success of face identity masking for human perceivers and a deep convolutional neural network (DCNN). Eight de-identification algorithms were applied to videos of drivers' faces, while they actively operated a motor vehicle. These masks were pre-selected to be applicable to low-quality video and to maintain coarse information about facial actions. Humans studied high-resolution images to learn driver identities and were tested on their recognition of active drivers in low-resolution videos. Faces in the videos were either unmasked or were masked by one of the eight algorithms. When participants were tested immediately after learning (Experiment 1), all masks reduced identification, with six of eight masks reducing identification to extremely poor performance. In a second experiment, two of the most effective masks were tested after a delay of 7 or 28 days. The delay did not further reduce identification of the masked faces. In all masked conditions, participants maintained stringent decision criteria, with low confidence in recognition, further indicating the effectiveness of the masks. Next, the DCNN performed an identity-matching task between high-resolution images and masked videos-a task analogous to that done by humans. The pattern of accuracy for the DCNN mirrored some, but not all, aspects of human performance, highlighting the need to test the effectiveness of identity masking for both humans and machines. The DCNN was also tested on its ability to match identity between masked and unmasked versions of the same video, based only on the face. DCNN performance for the eight masks offers insight into the nature of the information in faces that is coded in these networks. © 2020 Copyright held by the owner/author(s). Publication rights licensed to ACM.",DCNN; De-identification; Masking algorithms; Privacy,Convolution; Deep neural networks; Video recording; De-identification; Decision criterions; Driver identities; High resolution image; Human perception; Human performance; Low resolution video; Poor performance; Convolutional neural networks
Quantification of displacement for tactile sensation in a contact-type low intensity focused ultrasound haptic device,2021,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100293349&doi=10.1145%2f3422820&partnerID=40&md5=0fc360905ef7b251e90409b57f609981,"Tactile threshold of low-intensity focused ultrasound (LIFU) haptic devices has been defined as the minimum pressure required for tactile sensation. However, in contact-type LIFU haptic devices using an elastomer as a conductive medium, the tactile threshold is affected by the mechanical properties of the elastomer. Therefore, the tactile threshold needs to be redefined as a parameter that does not change with the mechanical properties of the elastomer. In this study, we used the LIFU haptic device to investigate the displacement of the elastomer surface at the tactile threshold while controlling the pulse duration, pulse repetition frequency, and pressure. We analyzed the displacement magnitude and rate to determine their relationship to the pressure. The displacement magnitude is the spatiotemporal peak of the displacement, and the displacement rate is the initial slope of the displacement at the starting point of LIFU pulse. The tactile threshold measured by the applied pressure showed the U-shaped graph, and the minimum pressure of 475 kPa at 2 ms and 407 kPa at 300 Hz was measured. The tactile threshold measured by the displacement show that the tactile sensation can be evoked at the small displacement magnitude (<3 μm) when the high displacement rate is present (>1.56 mm/s). Furthermore, the large displacement magnitude is required to induce the tactile sensation when the displacement rate is low. This result shows that the tactile threshold of a contact-type LIFU haptic device is affected by both the displacement magnitude and rate of the conductive medium. Our findings can be used as a guideline for developing a contact-type LIFU haptic device regardless of the elastomer used. © 2020 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Displacement; Low intensity focused ultrasound; Tactile threshold,Elastomers; Mechanical properties; Ultrasonics; Conductive medium; Displacement rate; Elastomer surface; Large displacements; Low intensity focused ultrasounds; Pulse repetition frequencies; Small displacement; Tactile sensation; Plastics
Crossing roads with a computer-generated agent: Persistent effects on perception-action tuning,2021,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100290709&doi=10.1145%2f3431923&partnerID=40&md5=ff09c25ab980e88a68f30fbfe8d00aa0,"This study investigated how people coordinate their decisions and actions with a risky or safe computer-generated agent in a humanoid or non-humanoid form and how this experience influences later behavior when acting alone. In Experiment 1, participants first repeatedly crossed continuous traffic in a virtual environment with a humanoid computer-generated agent (Figure 1). Participants were specifically instructed to cross with an agent that was programmed to be either safe (taking only large gaps) or risky (also taking relatively small gaps). Participants then repeatedly crossed the same roadway alone. We found that participants' experiences with crossing safe vs. risky gaps with an agent persisted in later trials when the participants crossed alone, such that participants accepted tighter gaps if they were previously paired with a risky than a safe agent. In Experiment 2 (Figure 2), we tested whether experience crossing with a risky or safe non-humanoid object (a floating box) also influenced later behavior when crossing alone. We again found that participants who crossed with the risky object partner took tighter gaps when later crossing alone than those who crossed with the safe object partner. The Discussion focuses on the impact of experiences with virtual agents on perception-action tuning and the potential of using virtual agents for training safe road-crossing behavior. © 2020 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Immersive virtual environments; Joint action; Large screen VE; Pedestrian road crossing; Stereo displays; Virtual agents,Computer generated; Perception-action; Road crossing behavior; Small gaps; Virtual agent; Behavioral research
How the presence and size of static peripheral blur affects cybersickness in virtual reality,2020,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098325072&doi=10.1145%2f3419984&partnerID=40&md5=6aa863554eae21d9eeca24053b7d1085,"Cybersickness (CS) is one of the challenges that has hindered the widespread adoption of Virtual Reality and its applications. Consequently, a number of studies have focused on extensively understanding and reducing CS. Inspired by previous work that has sought to reduce CS using foveated rendering and Field of View (FOV) restrictions, we investigated how the presence and size of a static central window in peripheral FOV blurring affects CS. To facilitate this peripheral FOV blur, we applied a Gaussian blur effect in the display peripheral region, provisioning a full-resolution central window. Thirty participants took part in a three-session, within-subjects experiment, performing search and spatial updating tasks in a first-person, slow-walking, maze-traveling scenario. Two different central window sizes (small and large) were tested against a baseline condition that didn’t feature display peripheral blurring. Results revealed that the baseline condition produced higher levels of CS than both conditions with a central window. While there were no significant differences between the small and large windows, we observed interaction effects suggesting an influence of window size on “adaptation to CS.” When the central window is small, adaptation to CS seems to take more time but is more pronounced. The interventions had no effect on spatial updating and presence, but were detectable when the blurred area was larger (small central window). Lower sickness levels observed in both window conditions supports the use of peripheral FOV blurring to reduce CS, reducing our dependence on eye tracking. This being said, researchers must strive to find the right balance between window size and detectability to ensure seamless virtual experiences. © 2020 Association for Computing Machinery.",Blurring; Cybersickness; Foveated rendering; Virtual reality,Eye tracking; Base-line conditions; Field of views; Full resolutions; Gaussian blur; Interaction effect; ITS applications; Peripheral regions; Spatial updating; Virtual reality
A fitts’ law evaluation of visuo-haptic fidelity and sensory mismatch on user performance in a near-field disc transfer task in virtual reality,2020,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098327645&doi=10.1145%2f3419986&partnerID=40&md5=405cfd1df7d08411fef108bce167fcb8,"The trade-off between speed and accuracy in precision tasks is important to evaluate during user interaction with input devices. When different sensory cues are added or altered in such interactions, those cues have an effect on this trade-off, and thus, they affect overall user performance. For instance, adding cues like haptic feedback and stereoscopic viewing will result in more realistic user interaction, thus improving performance in these tasks. Also, adding a noticeable disparity between physical and virtual movements creates a mismatch between visual and proprioceptive systems, which generally has a negative effect on performance. In this study, we investigate the effects of haptic feedback, stereoscopic viewing, and visuo-proprioceptive mismatch on how quickly and accurately users complete a virtual pick-and-place task using the PHANToM OMNI. Through this experiment, we find that in the movement phase of a ring transfer, movement time and user performance are affected by haptic feedback and visuo-proprioceptive mismatch, and the main effects of stereoscopic viewing appears to be limited to the more precise step when the ring is around the target peg. © 2020 Association for Computing Machinery.",Fitts’ law; Haptics; Near-field virtual reality; Stereo,Economic and social effects; Stereo image processing; Haptic feedbacks; Improving performance; Law evaluation; Movement time; Pick and place; Stereoscopic viewing; User interaction; User performance; Virtual reality
The effect of gender and attractiveness of motion on proximity in virtual reality,2020,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098325997&doi=10.1145%2f3419985&partnerID=40&md5=2c4837c8544a3f13362aa401a7038638,"In human interaction, people will keep different distances from each other depending on their gender. For example, males will stand further away from males and closer to females. Previous studies in virtual reality (VR), where people were interacting with virtual humans, showed a similar result. However, many other variables influence proximity, such as appearance characteristics of the virtual character (e.g., attractiveness). Our study focuses on proximity to virtual walkers, where gender could be recognised from motion only, since previous studies using point-light displays found walking motion is rich in gender cues. In our experiment, a walking wooden mannequin approached the participant embodied in a virtual avatar using the HTC Vive Pro HMD and controllers. The mannequin animation was motion captured from several male and female actors and each motion was displayed individually on the character. Participants used the controller to stop the approaching mannequin when they felt it was uncomfortably close to them. Based on previous work, we hypothesised that proximity will be affected by the gender of the character, but unlike previous research, the gender in our experiment could only be determined from character’s motion. We also expected differences in proximity according to the gender of the participant. We additionally expected some motions to be rated more attractive than others and that attractive motions would reduce the proximity measure. Our results show support for the last two assumptions, but no difference in proximity was found according to the gender of the character’s motion. Our findings have implications for the design of virtual characters in interactive virtual environments. © 2020 Association for Computing Machinery.",Gender; Perception; Proximity; Virtual reality,Human interactions; Interactive virtual environments; Light display; Proximity measure; Virtual avatar; Virtual character; Virtual humans; Walking motion; Virtual reality
Toward quantifying ambiguities in artistic images,2020,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098327228&doi=10.1145%2f3418054&partnerID=40&md5=e686a2004e88196209d4dc4214a28536,"It has long been hypothesized that perceptual ambiguities play an important role in aesthetic experience: A work with some ambiguity engages a viewer more than one that does not. However, current frameworks for testing this theory are limited by the availability of stimuli and data collection methods. This article presents an approach to measuring the perceptual ambiguity of a collection of images. Crowdworkers are asked to describe image content, after different viewing durations. Experiments are performed using images created with Generative Adversarial Networks, using the Artbreeder website. We show that text processing of viewer responses can provide a fine-grained way to measure and describe image ambiguities. © 2020 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Aesthetics; Datasets; Generative adversarial networks (GAN); Image descriptions; Text tagging,Psychology computing; Sensory perception; Adversarial networks; Data collection method; Fine grained; Image content; Perceptual ambiguities; Text processing
Introduction to the special issue on SAP 2020,2020,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098331580&doi=10.1145%2f3428144&partnerID=40&md5=f9addf9295991c3dec379d7791a5a1d8,[No abstract available],,
A systematic review of empirical measures of workload capacity,2020,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097223891&doi=10.1145%2f3422869&partnerID=40&md5=3fe4d3cea2d0edd58c5a1af7cdf39abe,"The usability of the human-machine interface is dependent on the quality of its design and testing. Defining clear criteria that the interface must meet can assist the implementation and evaluation process. These criteria may be based on performance, the quality of users' experience, error prevention, or the broad utility of the interface. In this article, we motivate the use for workload capacity as an empirical measure of usability. We first describe generic and specific uses for workload measures in terms of adaptive interfaces. We then carry out a systematic review of how workload capacity has been empirically measured, based on 172 relevant literature sources from psychology, neuroscience, engineering, and computer science. We then analyse and report on how workload capacity and related constructs, such as perceptual load, attention, and working memory have been defined and measured in these sources. We discuss similarities and differences between constructs and identify opportunities for integrating real-time workload capacity measures into dynamic interfaces. © 2020 ACM.",HCI; mental workload; multitasking; perceptual load; usability; working memory; Workload capacity,Psychology computing; Sensory perception; Adaptive interface; Capacity measures; Dynamic interface; Empirical measure; Error prevention; Human Machine Interface; Systematic Review; Users' experiences; User experience
Learning the vibrotactile morse code alphabet,2020,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097236017&doi=10.1145%2f3402935&partnerID=40&md5=12e88fcecbcf5e96aa2ec1190d5e0352,"Vibrotactile Morse code provides a way to convey words using the sense of touch with vibrations. This can be useful in applications for users with a visual and/or auditory impairment. The advantage of using vibrotactile Morse code is that it is technically easy to accomplish. The usefulness of tactile Morse code also depends on how easy it is to learn to use without providing a visual representation of the code. Here we investigated learning of the vibrotactile the Morse code alphabet without any visual representation of the code and whether the learned letters can immediately be used to recognize words. Two vibration motors were used: one was attached to the left arm (dots) and the other to the right arm (dashes). We gave the participants a learning session of 30 minutes and determined how many letters they had learned. All participants managed to learn at least 15 letters in this time. Directly afterward, they were presented with 2-, 3-, 4-, or 5-letter words consisting of only the letters they had learned. Participants were able to identify words, but correct rates decreased rapidly with word length. We can conclude that it is possible to learn vibrotactile Morse code using only a vibrotactile representation (15 to 24 letters in 30 minutes). After the learning session, it was possible to recognise words, but to increase the recognition rates extra training would be beneficial. © 2020 ACM.",Haptic communication; Morse code; vibration,Psychology computing; Sensory perception; Learning sessions; Morse codes; Sense of touch; Vibration motor; Vibrotactile; Visual representations; Word length; Signaling
On the perception analysis of user feedback for interactive face retrieval,2020,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097222826&doi=10.1145%2f3403964&partnerID=40&md5=a1c386c7191a10989a0eeb617fb23905,"In this article, we explore the coherence of face perception between human and machine in the scenario of interactive face retrieval. In the part of human perception, we collect user feedback to the stimuli of a target face and groups of displayed candidate face images in a face database with a large number of subjects. In the part of machine vision, we compare the benchmark features and general metrics to measure face similarity. We propose a series of coherence measurements to evaluate the statistic characteristic of human and machine face perception. We discover that despite the unfamiliarity of users to most faces in the database, the coherence between human and machine remains in a stable level across multiple variations in metrics, features, size of databases, and demographics. The simulation experiments with the coherence distributions demonstrate that the embedded information is valuable to speed up interactive retrieval. The comparisons over multiple parameter settings provide feasible instructions in designing the interactive face retrieval system with more consideration of human factors. © 2020 ACM.",face retrieval; human face perception; Interactive retrieval; relevance feedback,Psychology computing; Sensory perception; Coherence measurements; Embedded information; Face perceptions; Face retrieval; Human perception; Interactive retrieval; Multiple parameters; Statistic characteristics; Database systems
Providing semi-private feedback on a shared public screen by controlling presentation onset,2020,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097227977&doi=10.1145%2f3419983&partnerID=40&md5=10fd6a8c52011b824ff47e9049be4bab,"We describe a novel technique to provide semi-private feedback on a shared public screen. The technique uses a no-onset presentation that takes advantage of perceptual limitations in human vision to avoid alerting other users to feedback directed at one individual user by suppressing the sudden onset of the feedback. Three experiments evaluated the effectiveness of the technique and appropriate timing parameters and alternatives for presentation onset. Our experiments indicated that an 80 ms no-onset presentation allows participants to interpret information directed to them with over 90% accuracy, but their ability to interpret simultaneously presented information intended for others will be close to random chance. The technique initially camouflages the information being presented by overlaying additional visual elements and then removes those elements to reveal only the elements encoding the information being presented. We discuss applications for the technique, including classroom clicker usage, which was our original motivation for the study. © 2020 ACM.",Abrupt no-onset; abrupt onset; gradual no-onset; shared display; visual feedback,Psychology computing; Sensory perception; Feedback directed; Human vision; Novel techniques; Public screens; Timing parameters; Visual elements; Feedback
Establishing Vibration-Based Tactile Line Profiles for Use in Multimodal Graphics,2020,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086000233&doi=10.1145%2f3383457&partnerID=40&md5=e2c191bbed3d282212b10e83a6db1bc3,"Vibration plays a significant role in the way users interact with touchscreens. For many users, vibration affords tactile alerts and other enhancements. For eyes-free users and users with visual impairments, vibration can also serve a more primary role in the user interface, such as indicating streets on maps, conveying information about graphs, or even specifying basic graphics. However, vibration is rarely used in current user interfaces beyond basic cuing. Furthermore, designers and developers who do actually use vibration more extensively are often unable to determine the exact properties of the vibration signals they are implementing, due to out-of-the-box software and hardware limitations. We make two contributions in this work. First, we investigate the contextual properties of touchscreen vibrations and how vibrations can be used to effectively convey traditional, embossed elements, such as dashes and dots. To do so, we developed an open source, Android-based library to generate vibrations that are perceptually salient and intuitive, improving upon existing vibration libraries. Second, we conducted a user study with 26 blind or visually impaired users to evaluate and categorize the effects with respect to traditional tactile line profiles. We have established a range of vibration effects that can be reliably generated by our haptic library and are perceptible and distinguishable by users. © 2020 ACM.",Haptics; HCI; perception; touchscreen,Open source software; Contextual properties; Line profiles; Open sources; Software and hardwares; Vibration effect; Vibration signal; Visual impairment; Visually-impaired users; User interfaces
Does It Ping or Pong? Auditory and Tactile Classification of Materials by Bouncing Events,2020,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085982604&doi=10.1145%2f3393898&partnerID=40&md5=2f3cc36faafce10c722a58bf31d4245a,"Two experiments studied the role of impact sounds and vibrations in classification of materials. The task consisted of feeling on an actuated surface and listening through headphones to the recorded feedback of a ping-pong ball hitting three flat objects respectively made of wood, plastic, and metal, and then identifying their material. In Experiment 1, sounds and vibrations were recorded by keeping the objects in mechanical isolation. In Experiment 2, recordings were taken while the same objects stood on a table, causing their resonances to fade faster due to mechanical coupling with the support. A control experiment, where participants listened to and touched the real objects in mechanical isolation, showed high accuracy of classification from either sounds (90% correct) or vibrations (67% correct). Classification of reproduced bounces in Experiments 1 and 2 was less precise. In both experiments, the main effect of material was statistically significant; conversely, the main effect of modality (auditory or tactile) was significant only in the control. Identification of plastic and especially metal was less accurate in Experiment 2, suggesting that participants, when possible, classified materials by longer resonance tails. Audio-tactile summation of classification accuracy was found, suggesting that multisensory integration influences the perception of materials. Such results have prospective application to the nonvisual design of virtual buttons, which is the object of our current research. © 2020 ACM.",auditory feedback; Material classification; multisensory integration; tactile feedback; virtual buttons,Sports; Actuated surfaces; Classification accuracy; Control experiments; Mechanical coupling; Mechanical isolation; Multisensory integration; Ping pong ball; Prospective applications; Vibrations (mechanical)
Feature Weighted Linguistics Classifier for Predicting Learning Difficulty Using Eye Tracking,2020,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085965088&doi=10.1145%2f3380877&partnerID=40&md5=ef8fbb374bb57e90c96221790dd02a18,"This article presents a new approach to predict learning difficulty in applications such as e-learning using eye movement and pupil response. We have developed 12 eye response features based on psycholinguistics, contextual information processing, anticipatory behavior analysis, recurrence fixation analysis, and pupillary response. A key aspect of the proposed approach is the temporal analysis of the feature response to the same concept. Results show that variations in eye response to the same concept over time are indicative of learning difficulty. A Feature Weighted Linguistics Classifier (FWLC) was developed to predict learning difficulty in real time. The proposed approach predicts learning difficulty with an accuracy of 90%. © 2020 ACM.",e-learning; eye response analysis; eye tracking; Predicting learning difficulty; predicting levels of learning; pupillary response analysis,Classification (of information); Eye movements; Forecasting; Linguistics; Behavior analysis; Contextual information; Learning difficulties; New approaches; Pupil response; Pupillary response; Response features; Temporal analysis; Eye tracking
Three Perceptual Dimensions for Specular and Diffuse Reflection,2020,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085997091&doi=10.1145%2f3380741&partnerID=40&md5=693676cd22bc0c39c4b86c4adf62ce28,"Previous research investigated the perceptual dimensionality of achromatic reflection of opaque surfaces, by using either simple analytic models of reflection or measured reflection properties of a limited sample of materials. Here, we aim to extend this work to a broader range of simulated materials. In a first experiment, we used sparse multidimensional scaling techniques to represent a set of rendered stimuli in a perceptual space that is consistent with participants' similarity judgments. Participants were presented with one reference object and four comparisons, rendered with different material properties. They were asked to rank the comparisons according to their similarity to the reference, resulting in an efficient collection of a large number of similarity judgments. To interpret the space individuated by multidimensional scaling, we ran a second experiment in which observers were asked to rate our experimental stimuli according to a list of 30 adjectives referring to their surface reflectance properties. Our results suggest that perception of achromatic reflection is based on at least three dimensions, which we labelled ""Lightness,"" ""Gloss,"" and ""Metallicity,"" in accordance with the rating results. These dimensions are characterized by a relatively simple relationship with the parameters of the physically based rendering model used to generate our stimuli, indicating that they correspond to different physical properties of the rendered materials. Specifically, ""Lightness"" relates to diffuse reflections, ""Gloss"" to the presence of high contrast sharp specular highlights, and ""Metallicity"" to spread out specular reflections. © 2020 ACM.",BRDF; dimensionality; Perception,Multi-dimensional scaling; Multidimensional scaling techniques; Perceptual dimensions; Physically based rendering; Reflection properties; Simulated materials; Specular reflections; Surface reflectance properties; Astrophysics
The impact of olfactory and wind stimuli on 360 videos using head-mounted displays,2020,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081546176&doi=10.1145%2f3380903&partnerID=40&md5=5747cc3618d75274a90e46b7489c07ea,"Consuming 360 audiovisual content using a Head-Mounted Display (HMD) has become a standard feature for Immersive Virtual Reality (IVR). However, most applications rely only on visual and auditory feedback whereas other senses are often disregarded. The main goal of this work was to study the effect of tactile and olfactory stimuli on participants' sense of presence and cybersickness while watching a 360 video using an HMD-based IVR setup. An experiment with 48 participants and three experimental conditions (360 video, 360 video with olfactory stimulus, and 360 video with tactile stimulus) was performed. Presence and cybersickness were reported via post-test questionnaires. Statistical analysis showed a significant difference in presence between the control and the olfactory conditions. From the control to the tactile condition, mean values were higher but failed to show statistical significance. Thus, results suggest that adding an olfactory stimulus increases presence significantly while the addition of a tactile stimulus only shows a positive effect. Regarding cybersickness, no significant differences were found across conditions. We conclude that an olfactory stimulus contributes to higher presence and that a tactile stimulus, delivered in the form of cutaneous perception of wind, has no influence in presence. We further conclude that multisensory cues do not affect cybersickness. © 2020 Copernicus GmbH. All rights reserved.",Cybersickness; Immersion; Multisensory stimulation; Olfactory; Presence; Tactile; Virtual reality,Street traffic control; Surveys; Virtual reality; Cybersickness; Immersion; Multisensory stimulations; Olfactory; Presence; Tactile; Helmet mounted displays
Translational and rotational Arrow cues (TRAC) navigation method for manual alignment tasks,2020,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079740296&doi=10.1145%2f3375001&partnerID=40&md5=97e382533af4c4d57d343892cf5d2735,"Many tasks in image-guided surgery require a clinician to manually position an instrument in space, with respect to a patient, with five or six degrees of freedom (DOF). Displaying the current and desired pose of the object on a 2D display such as a computer monitor is straightforward. However, providing guidance to accurately and rapidly navigate the object in 5-DOF or 6-DOF is challenging. Guidance is typically accomplished by showing distinct orthogonal viewpoints of the workspace, requiring simultaneous alignment in all views. Although such methods are commonly used, they can be quite unintuitive, and it can take a long time to perform an accurate 5-DOF or 6-DOF alignment task. In this article, we describe a method of visually communicating navigation instructions using translational and rotational arrow cues (TRAC) defined in an object-centric frame, while displaying a single principal view that approximates the human's egocentric view of the physical object. The target pose of the object is provided but typically is used only for the initial gross alignment. During the accurate-alignment stage, the user follows the unambiguous arrow commands. In a series of human-subject studies, we show that the TRAC method outperforms two common orthogonal-view methods-the triplanar display, and a sight-alignment method that closely approximates the Acrobot Navigation System-in terms of time to complete 5-DOF and 6-DOF navigation tasks. We also find that subjects can achieve 1 mm and 1° accuracy using the TRAC method with a median completion time of less than 20 seconds. © 2020 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Image-guided surgery; Pose matching; Visual guidance,Degrees of freedom (mechanics); Navigation systems; Patient rehabilitation; Surgery; Visual communication; Alignment methods; Image guided surgery; Navigation methods; Physical objects; Pose matching; Simultaneous alignment; Six degrees of freedom; Visual guidance; Display devices
The impact of the complexity of harmony on the acceptability of music,2020,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079783256&doi=10.1145%2f3375014&partnerID=40&md5=b3cf1a2b29c1c7955a0398c50c77b78b,"In this article, we contribute to the longstanding challenge of how to explain the listener's acceptability for a particular piece of music, using harmony as one of the crucial dimensions in music, one of the least examined in this context. We propose three measures for the complexity of harmony: (i) the complexity based on usage of the basic tonal functions and parallels in the harmonic progression, (ii) the entropies of unigrams and bigrams in the sequence of chords, and (iii) the regularity of the harmonic progression. Additionally, we propose four measures for the acceptability of musical pieces (perceptual variables): difficulty, pleasantness, recognition, and repeatability. These measures have been evaluated in each musical example within our dataset, consisting of 160 carefully selected musical excerpts from different musical styles. The first and the third complexity measures and the musical style of excerpts were determined by the first author using criteria described in the article, while the entropies were computed by computer using Shannon's formula, after the harmonic progression was determined. The four perceptual variables were obtained by a group of 21 participants, taking their mean values as the final score. A statistical analysis of this dataset shows that all the measures of complexity are consistent and are together with the musical style important features in explaining the musical acceptability. These relations were further elaborated by regression tree analysis for difficulty and pleasantness after unigram entropy was eliminated due to high correlation with bigram entropy. Results offer reasonable interpretations and also illuminate the relative importance of the predictor variables. In particular, the regularity of the harmonic progression is in both cases the most important predictor. © 2020 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Dataset; Entropy of harmony; Music acceptability; Music complexity; Regression tree; Regularity,Forestry; Harmonic analysis; Dataset; Music acceptability; Music complexity; Regression trees; Regularity; Entropy
Visual information requirements for dismounted soldier target acquisition,2020,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079765461&doi=10.1145%2f3375000&partnerID=40&md5=dba6829c19c7abe0e512b16b121060eb,"We conducted an empirical investigation of the visual information requirements for target detection and threat identification decisions in the dismounted soldier context. Forty soldiers viewed digital photographs of a person standing against a forested background. The soldiers made two-alternative detection decisions requiring them to determine whether the target was present in the scene, and two-alternative threat identification decisions that required discrimination of the objects held by the target, the clothing worn by the target, and target postures. The images were presented to subjects on a computer display, and variation in the apparent target distance was simulated through digital image magnification and by varying the viewing distance to the display. Image resolution was degraded progressively by spatial frequency filtering and we estimated the resolution threshold in each task. These threshold values were compared with the historical Johnson criteria for predicting imaging device performance. Our data are broadly consistent with the previously reported values, though our threat identification decisions required subjects to perceive information with a larger spatial scale than the Johnson criterion for identification of standing human targets. In a second experiment, we employed a four-alternative identification decision and found results that were consistent with those from Experiment 1. We also confirmed that the spatial scale of visual information used for target acquisition is highly task-specific, and provided a novel demonstration of changes in visual information requirements as a function of target range. These findings pose challenges for models of target acquisition with imaging devices. © Her Majesty the Queen in Right of Canada (Department of National Defence), 2020.",Dismounted soldier; Image resolution; Johnson criteria; Target acquisition; Target detection; Target identification,Imaging techniques; Target tracking; Digital photographs; Dismounted soldiers; Empirical investigation; Johnson criteria; Spatial frequency filtering; Target acquisition; Target identification; Threat identification; Image resolution
Introduction to the special issue on SAP 2019,2019,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072540099&doi=10.1145%2f3355996&partnerID=40&md5=12bbe51040e70a4790abe2380e4cf57b,[No abstract available],,
Photoplethysmogram-based cognitive load assessment using multi-feature fusion model,2019,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072668153&doi=10.1145%2f3340962&partnerID=40&md5=c741368af31ea66c6ec83cb1c5794e40,"Cognitive load assessment is crucial for user studies and human-computer interaction designs. As a noninvasive and easy-to-use category of measures, current photoplethysmogram- (PPG) based assessment methods rely on single or small-scale predefined features to recognize responses induced by people's cognitive load, which are not stable in assessment accuracy. In this study, we propose a machine-learning method by using 46 kinds of PPG features together to improve the measurement accuracy for cognitive load. We test the method on 16 participants through the classical n-back tasks (0-back, 1-back, and 2-back). The accuracy of the machine-learning method in differentiating different levels of cognitive loads induced by task difficulties can reach 100% in 0-back vs. 2-back tasks, which outperformed the traditional HRV-based and single-PPG-feature-based methods by 12-55%. When using “leave-one-participant-out” subject-independent cross validation, 87.5% binary classification accuracy was reached, which is at the state-of-the-art level. The proposed method can also support real-time cognitive load assessment by beat-to-beat classifications with better performance than the traditional single-feature-based real-time evaluation method. © 2019 Association for Computing Machinery.",Cognitive load; Multi-feature fusion; Photoplethysmogram; Real-time assessment,Human computer interaction; Machine learning; Binary classification; Cognitive loads; Feature-based method; Human computer interaction design; Machine learning methods; Multi-feature fusion; Photo-plethysmogram; Real-time assessment; Image processing
Tactile texture display with vibrotactile and electrostatic friction stimuli mixed at appropriate ratio presents better roughness textures,2019,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072679514&doi=10.1145%2f3340961&partnerID=40&md5=0c879ff7bd6a0fee5abb62e64be0e2e9,"Vibrotactile and friction texture displays are good options for artificially presenting the roughness and frictional properties of textures, respectively. These two types of displays are compatible with touch panels and exhibit complementary characteristics. We combine vibrotactile and electrostatic friction texture displays to improve the quality of virtual textures, considering that actual textured surfaces are composed of both properties. We investigate their composition ratios when displaying roughness textures. Grating roughness scales with one of the six surface wavelengths are generated under 11 display conditions, and in 9 of which, vibrotactile and friction stimuli are combined with different composition ratios. A forced-choice experiment regarding subjective realism indicates that a vibrotactile stimulus with a slight variable-friction stimulus is effective for presenting quality textures for surface wavelengths greater than or equal to 1.0mm. © 2019 Association for Computing Machinery.",Electrostatic friction display; Surface roughness; Tactile texture display; Vibrotactile display,Electrostatics; Friction; Surface roughness; Complementary characteristics; Electrostatic friction; Forced-choice experiment; Frictional properties; Subjective realisms; Tactile texture; Variable frictions; Vibrotactile displays; Textures
Assessing neural network scene classification from degraded images,2019,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072656933&doi=10.1145%2f3342349&partnerID=40&md5=0da25fc6f22790d92e3963447a7fd090,"Scene recognition is an essential component of both machine and biological vision. Recent advances in computer vision using deep convolutional neural networks (CNNs) have demonstrated impressive sophistication in scene recognition, through training on large datasets of labeled scene images (Zhou et al. 2018, 2014). One criticism of CNN-based approaches is that performance may not generalize well beyond the training image set (Torralba and Efros 2011), and may be hampered by minor image modifications, which in some cases are barely perceptible to the human eye (Goodfellow et al. 2015; Szegedy et al. 2013). While these “adversarial examples” may be unlikely in natural contexts, during many real-world visual tasks scene information can be degraded or limited due to defocus blur, camera motion, sensor noise, or occluding objects. Here, we quantify the impact of several image degradations (some common, and some more exotic) on indoor/outdoor scene classification using CNNs. For comparison, we use human observers as a benchmark, and also evaluate performance against classifiers using limited, manually selected descriptors. While the CNNs outperformed the other classifiers and rivaled human accuracy for intact images, our results show that their classification accuracy is more affected by image degradations than human observers. On a practical level, however, accuracy of the CNNs remained well above chance for a wide range of image manipulations that disrupted both local and global image statistics. We also examine the level of image-by-image agreement with human observers, and find that the CNNs' agreement with observers varied as a function of the nature of image manipulation. In many cases, this agreement was not substantially different from the level one would expect to observe for two independent classifiers. Together, these results suggest that CNN-based scene classification techniques are relatively robust to several image degradations. However, the pattern of classifications obtained for ambiguous images does not appear to closely reflect the strategies employed by human observers. © 2019 Copyright held by the owner/author(s).",Human perception; Human scene recognition,Benchmarking; Computer vision; Deep neural networks; Large dataset; Neural networks; Vision; Classification accuracy; Convolutional neural network; Human perception; Image manipulation; Image modification; Independent classifiers; Scene classification; Scene recognition; Image classification
Predicting perceived naturalness of human animations based on generative movement primitive models,2019,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072541991&doi=10.1145%2f3355401&partnerID=40&md5=7355dd3763261b472663bd254ae77688,"We compared the perceptual validity of human avatar walking animations driven by six different representations of human movement using a graphics Turing test. All six representations are based on movement primitives (MPs), which are predictive models of full-body movement that differ in their complexity and prediction mechanism. Assuming that humans are experts at perceiving biological movement from noisy sensory signals, it follows that these percepts should be describable by a suitably constructed Bayesian ideal observer model. We build such models from MPs and investigate if the perceived naturalness of human animations are predictable from approximate Bayesian model scores of the MPs. We found that certain MP-based representations are capable of producing movements that are perceptually indistinguishable from natural movements. Furthermore, approximate Bayesian model scores of these representations can be used to predict perceived naturalness. In particular, we could show that movement dynamics are more important for perceived naturalness of human animations than single frame poses. This indicates that perception of human animations is highly sensitive to their temporal coherence. More generally, our results add evidence for a shared MP-representation of action and perception. Even though the motivation of our work is primarily drawn from neuroscience, we expect that our results will be applicable in virtual and augmented reality settings, when perceptually plausible human avatar movements are required. © 2019 Copyright held by the owner/author(s).",Dynamical movement primitives; Dynamical systems; Gaussian process dynamical model; Human animation; Movement primitives; Perception; Psychophysics,Augmented reality; Bayesian networks; Dynamical systems; Forecasting; Sensory perception; Approximate Bayesian; Dynamical model; Human animation; Ideal observer models; Movement primitives; Prediction mechanisms; Psychophysics; Virtual and augmented reality; Virtual reality
Is photorealism important for perception of expressive virtual humans in virtual reality?,2019,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072524764&doi=10.1145%2f3349609&partnerID=40&md5=20c85c793c0db4c0bb91f057a31a8419,"In recent years, the quality of real-time rendering has reached new heights—realistic reflections, physically based materials, and photometric lighting are all becoming commonplace in modern game engines and even interactive virtual environments, such as virtual reality (VR). As the strive for realism continues, there is a need to investigate the effect of photorealism on users’ perception, particularly for interactive, emotional scenarios in VR. In this article, we explored three main topics, where we predicted photorealism will make a difference: the illusion of being present with the virtual person and in an environment, altered emotional response toward the character, and a subtler response—comfort of being in close proximity to the character. We present a perceptual experiment, with an interactive expressive virtual character in VR, which was designed to induce particular social responses in people. Our participant pool was large (N = 797) and diverse in terms of demographics. We designed a between-group experiment, where each group saw either the realistic rendering or one of our stylized conditions (simple and sketch style), expressing one of three attitudes: Friendly, Unfriendly, or Sad. While the render style did not particularly effect the level of comfort with the character or increase the illusion of presence with it, our main finding shows that the photorealistic character changed the emotional responses of participants, compared to the stylized versions. We also found a preference for realism in VR, reflected in the affinity and higher place illusion in the scenario, rendered in the realistic render style. © 2019 Association for Computing Machinery.",Proximity; Render style; Virtual humans; Virtual reality,Behavioral research; Emotional response; Interactive virtual environments; Proximity; Real-time rendering; Realistic rendering; Render style; Virtual character; Virtual humans; Virtual reality
The effect of task on visual attention in interactive virtual environments,2019,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072541128&doi=10.1145%2f3352763&partnerID=40&md5=7c767ebbbb14994b567a3e48c8b4cf2f,"Virtual environments for gaming and simulation provide dynamic and adaptive experiences, but, despite advances in multi-sensory interfaces, these are still primarily visual experiences. To support real-time dynamic adaptation, interactive virtual environments could implement techniques to predict and manipulate human visual attention. One promising way of developing such techniques is to base them on psychophysical observations, an approach that requires a sound understanding of visual attention allocation. Understanding how this allocation of visual attention changes depending on a user’s task offers clear benefits in developing these techniques and improving virtual environment design. With this aim, we investigated the effect of task on visual attention in interactive virtual environments. We recorded fixation data from participants completing freeview, search, and navigation tasks in three different virtual environments. We quantified visual attention differences between conditions by identifying the predictiveness of a low-level saliency model and its corresponding color, intensity, and orientation feature-conspicuity maps, as well as measuring fixation center bias, depth, duration, and saccade amplitude. Our results show that task does affect visual attention in virtual environments. Navigation relies more than search or freeview on intensity conspicuity to allocate visual attention. Navigation also produces fixations that are more central, longer, and deeper into the scenes. Further, our results suggest that it is difficult to distinguish between freeview and search tasks. These results provide important guidance for designing virtual environments for human interaction, as well as identifying future avenues of research for developing “attention-aware” virtual worlds. © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Saliency; Virtual environments; Visual attention,Behavioral research; Navigation; Corresponding colors; Human visual attention; Interactive virtual environments; Multi-sensory interfaces; Orientation features; Real-time dynamics; Saliency; Visual Attention; Virtual reality
Keep it simple: Depth-based Dynamic Adjustment of Rendering for Head-mounted Displays Decreases Visual Comfort,2019,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072541378&doi=10.1145%2f3353902&partnerID=40&md5=6b809dc0e0afd6e6ad74401476255388,"Head-mounted displays cause discomfort. This is commonly attributed to conflicting depth cues, most prominently between vergence, which is consistent with object depth, and accommodation, which is adjusted to the near eye displays. It is possible to adjust the camera parameters, specifically interocular distance and vergence angles, for rendering the virtual environment to minimize this conflict. This requires dynamic adjustment of the parameters based on object depth. In an experiment based on a visual search task, we evaluate how dynamic adjustment affects visual comfort compared to fixed camera parameters. We collect objective as well as subjective data. Results show that dynamic adjustment decreases common objective measures of visual comfort such as pupil diameter and blink rate by a statistically significant margin. The subjective evaluation of categories such as fatigue or eye irritation shows a similar trend but was inconclusive. This suggests that rendering with fixed camera parameters is the better choice for head-mounted displays, at least in scenarios similar to the ones used here. © 2019 Association for Computing Machinery.",Fatigue; Head-mounted displays; Vergence,Cameras; Fatigue of materials; Three dimensional computer graphics; Virtual reality; Camera parameter; Dynamic adjustment; Head mounted displays; Objective measure; Pupil diameter; Subjective evaluations; Vergences; Visual comfort; Helmet mounted displays
Comparative evaluation of user perceived quality assessment of design strategies for http-based adaptive streaming,2019,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072659175&doi=10.1145%2f3345313&partnerID=40&md5=90d2e6e24af6e129d96b8e7a565b592f,"HTTP-based Adaptive Streaming (HAS) is the dominant Internet video streaming application. One specific HAS approach, Dynamic Adaptive Streaming over HTTP (DASH), is of particular interest, as it is a widely deployed, standardized implementation. Prior academic research has focused on networking and protocol issues, and has contributed an accepted understanding of the performance and possible performance issues in large deployment scenarios. Our work extends the current understanding of HAS by focusing directly on the impacts of choice of the video quality adaptation algorithm on end-user perceived quality. In congested network scenarios, the details of the adaptation algorithm determine the amount of bandwidth consumed by the application as well as the quality of the rendered video stream. HAS will lead to user-perceived changes in video quality due to intentional changes in quality video segments, or unintentional perceived quality impairments caused by video decoder artifacts such as pixelation, stutters, or short or long stalls in the rendered video when the playback buffer becomes empty. The HAS adaptation algorithm attempts to find the optimal solution to mitigate the conflict between avoiding buffer stalls and maximizing video quality. In this article, we present results from a user study that was designed to provide insights into “best practice guidelines” for a HAS adaptation algorithm. Our findings suggest that a buffer-based strategy might provide a better experience under higher network impairment conditions. For the two network scenarios considered, the buffer-based strategy is effective in avoiding stalls but does so at the cost of reduced video quality. However, the buffer-based strategy does yield a lower number of quality switches as a result of infrequent bitrate adaptations. Participants in buffer-based strategy do notice the drop in video quality causing a decrease in perceived QoE, but the perceived levels of video quality, viewer frustration, and opinions of video clarity and distortion are significantly worse due to artifacts such as stalls in capacity-based strategy. The capacity-based strategy tries to provide the highest video quality possible but produces many more artifacts during playback. The results suggest that player video quality has more of an impact on perceived quality when stalls are infrequent. The study methodology also contributes a unique method for gathering continuous quantitative subjective measure of user perceived quality using a Wii remote. © 2019 Association for Computing Machinery.",Adaptation algorithm; Dynamic adaptive streaming over HTTP; Empirical evaluation; Quality of experience; Streaming media; Video quality,Bandwidth; HTTP; Quality of service; Rendering (computer graphics); Video streaming; Adaptation algorithms; Dynamic Adaptive Streaming over HTTP; Empirical evaluations; Quality of experience (QoE); Streaming media; Video quality; Quality control
Shadow-based illusion of depth and transparency in printed images,2019,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073123611&doi=10.1145%2f3342350&partnerID=40&md5=9eba6d351d95d362d5bb2c7c0b85ac79,"A cast shadow is one of the visual features that serve as a perceptual cue to the three-dimensional (3D) layout of objects. Although it is well known that adding cast shadows to an object produces the illusion that the object has a 3D layout, investigations into this illusion have been limited to virtual objects in a display. Using a light-projection technique, we show that it is possible to create a similar 3D layout illusion for real two-dimensional objects. Specifcally, we displayed spatial patterns that look like cast shadows in the vicinity of an object depicted as a printed image. The combination of the cast shadow patterns with the printed object made it appear as if the printed object hovered over its original location even though the object was physically two-dimensional. By using this technique, we demonstrated that the shadow-induced layout illusion resulted in printed images having novel perceptual transparency. Vision researchers may fnd our technique useful if they want to extend their studies on the perception of cast shadows and transparency with real objects. © 2019 held by the owner/author(s).",Depth; Light projection; Shadow; Transparency,Psychology computing; Sensory perception; Depth; Light projection; Perceptual transparency; Shadow; Spatial patterns; Threedimensional (3-d); Two-dimensional objects; Virtual objects; Transparency
Context in photo albums: Understanding and modeling user behavior in clustering and selection,2019,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073119148&doi=10.1145%2f3333612&partnerID=40&md5=77f97895e4541910bc7018b0c3427b4f,"Recent progress in digital photography and storage availability has drastically changed our approach to photo creation. While in the era of film cameras, careful forethought would usually precede the capture of a photo; nowadays, a large number of pictures can be taken with little effort. One of the consequences is the creation of numerous photos depicting the same moment in slightly different ways, which makes the process of organizing photos laborious for the photographer. Nevertheless, photo collection organization is important both for exploring photo albums and for simplifying the ultimate task of selecting the best photos. In this work, we conduct a user study to explore how users tend to organize or cluster similar photos in albums, to what extent different users agree in their clustering decisions, and to investigate how the clusteringdefined photo context affects the subsequent photo-selection process. We also propose an automatic hierarchical clustering solution for modeling user clustering decisions. To demonstrate the usefulness of our approach, we apply it to the task of automatic photo evaluation within photo albums and propose a clustering-based context adaptation. © 2019 Association for Computing Machinery.",Image assessment; Photo albums clustering; Photo collection organization; Photo selection,Behavioral research; Clustering and selections; Context adaptation; Digital photography; Hier-archical clustering; Image assessment; Photo album; Photo collections; Photo selection; Image processing
Algorithmic perception of vertices in sketched drawings of polyhedral shapes,2019,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071903929&doi=10.1145%2f3345507&partnerID=40&md5=70f6c495f05a3d983b99c38627e33886,"In this article, visual perception principles were used to build an artificial perception model aimed at developing an algorithm for detecting junctions in line drawings of polyhedral objects that are vectorized from hand-drawn sketches. The detection is performed in two dimensions (2D), before any 3D model is available and minimal information about the shape depicted by the sketch is used. The goal of this approach is to not only detect junctions in careful sketches created by skilled engineers and designers but also detect junctions when skilled people draw casually to quickly convey rough ideas. Current approaches for extracting junctions from digital images are mostly incomplete, as they simply merge endpoints that are near each other, thus ignoring the fact that different vertices may be represented by different (but close) junctions and that the endpoints of lines that depict edges that share a common vertex may not necessarily be close to each other, particularly in quickly sketched drawings. We describe and validate a new algorithm that uses these perceptual findings to merge tips of line segments into 2D junctions that are assumed to depict 3D vertices. © 2019 Association for Computing Machinery.",Algorithmic perception; Junctions; Polyhedral shapes; Vertices,3D modeling; Waveguide junctions; Artificial perception; Hand-drawn sketches; Minimal information; Polyhedral objects; Polyhedral shapes; Two dimensions (2D); Vertices; Visual perception; Drawing (graphics)
The Supernumerary hand illusion in augmented reality,2019,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073115102&doi=10.1145%2f3341225&partnerID=40&md5=2441224e5a4d69c677798183f434efd9,"The classic rubber hand illusion (RHI) experiment studies the sense of embodiment over a fake limb. Distinguished subcomponents of embodiment are ownership (sense of self-attribution of a body), agency (sense of having motor control), and self-location (the spatial experience of being inside a body), and are typically evoked in either reality or virtual reality. In augmented reality (AR), however, visually present real limbs can be augmented with (multiple) fake virtual limbs, which results in a variation of the RHI, the augmented reality supernumerary hand illusion (ARSHI). Such conditions occur, for example, in frst-person AR games and in AR-interfaces for tele-robotics. In this article, we examined to what extent humans can experience the sense of embodiment over a supernumerary virtual arm in addition to one or two real arms. We also examine how embodiment is a?ected by the perceptual visual-tactile synchronicity of the virtual and real limbs, and by the synchronicity of active movement of the virtual and real hand. Embodiment was measured subjectively by questionnaire and objectively by skin conductance responses (SCRs). Questionnaire responses show that ownership, agency, and self-location can be evoked over the virtual arm in the presence of a real arm, and that they are signifcantly stronger for synchronous conditions than for asynchronous conditions. The perceptual and motorical synchronous condition with three visible hands led to an experience of owning the virtual hand. These responses further show that agency was also strongly experienced over the supernumerary virtual arm, and responses regarding self-location suggest a shift in sensed location when one real arm was in view and an additional location when both real arms where in view. SCRs show no signifcant e?ect of condition, but do show a signifcant habituation e?ect as a function of the number of conditions performed by participants. When analyzing the relations at the individual participant level between the questionnaire data and skin conductance, we found two clusters of participants: (1) participants with low questionnaire responses and low-medium SCRs and (2) participants with high questionnaire responses and low-high SCRs. Finally, we discuss how virtual hand appearance/realism and willingness to accept virtual limbs could play an important role in the ARSHI, and provide insights on intricacies involved with measuring and evaluating RHIs. © 2019 Association for Computing Machinery.",Agency; Augmented reality; Multisensory; Ownership; Rubber hand illusion; Self-location; Sense of embodiment; Skin conductance habituation; Supernumerary hand illusion; Virtual hand illusion,Augmented reality; Location; Rubber; Surveys; Agency; Multisensory; Ownership; Sense of embodiment; Skin conductance; Supernumerary hand illusion; Virtual hand; Virtual reality
The FechDeck: A hand tool for exploring psychophysics,2019,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065697370&doi=10.1145%2f3313186&partnerID=40&md5=fb2e32c987ffffde6ca5639aef48b927,"Learning the methods of psychophysics is an essential part of training for perceptual experimentation, and hands-on experience is vital, but gaining this experience is difficult because good tools for learning are not available. The FechDeck is an ordinary deck of playing cards that has been modified to support learning the methods of psychophysics. Card backs are printed with noise patterns that span a range of densities. Faces are augmented with line segments arranged in “L” patterns. Jokers are printed with ruled faces and with backs that serve as standards. Instructions provided with the FechDeck allow users to perform threshold experiments using Fechner's methods of adjustment, limits, and constant stimuli; scaling experiments using Thurstone's ranking, paired comparison, and successive categories methods; and Stevens's magnitude estimation method. Spreadsheets provided with the deck support easy data entry and meaningful data analysis. An online repository supporting the FechDeck has been established to facilitate dissemination and to encourage open source development of the deck. © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Education; Open source; Psychophysics; Vision,Education; Psychophysiology; Vision; Magnitude estimation methods; Online repositories; Open source development; Open sources; Paired comparison; Psychophysics; Scaling experiments; Support learning; Hand tools
Visual stabilization of balance in virtual reality using the HTC vive,2019,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065733617&doi=10.1145%2f3313902&partnerID=40&md5=c2ad8039e36bc48f9d1a16ef69fd9c9c,"Vision in real environments stabilizes balance compared to an eyes-closed condition. For virtual reality to be safe and fully effective in applications such as physical rehabilitation, vision in virtual reality should stabilize balance as much as vision in the real world. Older virtual reality technology was previously found to stabilize balance but by less than half as much as real-world vision. Recent advancements in display technology might allow for vision in virtual reality to be as stabilizing as vision in the real world. This study evaluated whether viewing a virtual environment through the HTC Vive-a new consumer-grade head-mounted display-stabilizes balance, and whether visual stabilization is similar to that provided by real-world vision. Participants viewed the real laboratory or a virtual replica of the laboratory and attempted to maintain an unstable stance with eyes open or closed while standing at one of two viewing distances. Vision was significantly stabilizing in all conditions, but the virtual environment provided less visual stabilization than did the real environment. Regardless of the environment, near viewing led to greater visual stabilization than did far viewing. The smaller stabilizing influence of viewing a virtual compared to real environment might lead to greater risk of falls in virtual reality and smaller gains in physical rehabilitation using virtual reality. © 2019 Association for Computing Machinery.",Balance; Posture; Stereoscopic displays; Virtual environments,Balancing; Helmet mounted displays; Stabilization; Stereo image processing; Closed condition; Display technologies; Head mounted displays; Physical rehabilitation; Posture; Real environments; Stereoscopic display; Virtual reality technology; Virtual reality
Human and DNN classification performance on images with quality distortions: A comparative study,2019,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063210279&doi=10.1145%2f3306241&partnerID=40&md5=aa92851a0492331f3b11f78c3eddc5ee,"Image quality is an important practical challenge that is often overlooked in the design of machine vision systems. Commonly, machine vision systems are trained and tested on high-quality image datasets, yet in practical applications the input images cannot be assumed to be of high quality. Modern deep neural networks (DNNs) have been shown to perform poorly on images affected by blur or noise distortions. In this work, we investigate whether human subjects also perform poorly on distorted stimuli and provide a direct comparison with the performance of DNNs. Specifically, we study the effect of Gaussian blur and additive Gaussian noise on human andDNNclassification performance.We perform two experiments: one crowd-sourced experiment with unlimited stimulus display time, and one lab experiment with 100ms display time. In both cases, we found that humans outperform neural networks on distorted stimuli, even when the networks are retrained with distorted data. © 2019 Association for Computing Machinery.",Deep learning; Human study; Robust visual recognition,Computer vision; Deep learning; Gaussian noise (electronic); Image classification; Machinery; Additive Gaussian noise; Classification performance; Comparative studies; High quality images; Human study; Machine vision systems; Noise distortions; Visual recognition; Deep neural networks
Quantifying visual abstraction quality for computer-generated illustrations,2019,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062344526&doi=10.1145%2f3301414&partnerID=40&md5=8f5a96afb29c87eda57a4f2fb4bd0623,"We investigate how the perceived abstraction quality of computer-generated illustrations is related to the number of primitives (points and small lines) used to create them. Since it is difficult to find objective functions that quantify the visual quality of such illustrations, we propose an approach to derive perceptual models from a user study. By gathering comparative data in a crowdsourcing user study and employing a paired comparison model, we can reconstruct absolute quality values. Based on an exemplary study for stippling, we show that it is possible to model the perceived quality of stippled representations based on the properties of an input image. The generalizability of our approach is demonstrated by comparing models for different stippling methods. By showing that our proposed approach also works for small lines, we demonstrate its applicability toward quantifying different representational drawing elements. Our results can be related to Weber-Fechner's law from psychophysics and indicate a logarithmic relationship between number of rendering primitives in an illustration and the perceived abstraction quality thereof. © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Non-photorealistic rendering; Perception; Stippling; User study; Visual abstraction,Rendering (computer graphics); Sensory perception; Computer generated; Logarithmic relationship; Non-Photorealistic Rendering; Objective functions; Paired comparison; Stippling; User study; Visual abstraction; Abstracting
Influence of visual salience on webpage product searches,2019,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062361657&doi=10.1145%2f3301413&partnerID=40&md5=856217db28f9b6ec930f06e8fcbb190f,"Visual salience can increase search efficiency in complex displays but does that influence persist when completing a specific search? In two experiments, participants were asked to search webpages for the prices of specific products. Those products were located near an area of high visual salience or low visual salience. In Experiment 1, participants were read the name of the product before searching; in Experiment 2, participants were shown an image of the exact product before searching. In both cases, participants completed their search more quickly in the high-salience condition. This was true even when there was no ambiguity about the visual characteristics of the product. Our findings suggest that salience guides users through complex displays under realistic, goal-driven task conditions. Designers can use this knowledge to create interfaces that are easier to search by aligning salience and task-critical elements. © 2019 Association for Computing Machinery.",Attention; Eye movements; Salience; Visual search; Web design,Eye movements; Web Design; Attention; Complex displays; Critical elements; Goal driven; Salience; Search efficiency; Visual salience; Visual search; Websites
Perceptual effects of inconsistency in human animations,2019,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062368440&doi=10.1145%2f3301411&partnerID=40&md5=397d4f88b99551bfe88d84a60d44ca20,"The individual shape of the human body, including the geometry of its articulated structure and the distribution of weight over that structure, influences the kinematics of a person's movements. How sensitive is the visual system to inconsistencies between shape and motion introduced by retargeting motion from one person onto the shape of another? We used optical motion capture to record five pairs of male performers with large differences in body weight, while they pushed, lifted, and threw objects. From these data, we estimated both the kinematics of the actions as well as the performer's individual body shape. To obtain consistent and inconsistent stimuli, we created animated avatars by combining the shape and motion estimates from either a single performer or from different performers. Using these stimuli we conducted three experiments in an immersive virtual reality environment. First, a group of participants detected which of two stimuli was inconsistent. Performance was very low, and results were only marginally significant. Next, a second group of participants rated perceived attractiveness, eeriness, and humanness of consistent and inconsistent stimuli, but these judgements of animation characteristics were not affected by consistency of the stimuli. Finally, a third group of participants rated properties of the objects rather than of the performers. Here, we found strong influences of shape-motion inconsistency on perceived weight and thrown distance of objects. This suggests that the visual system relies on its knowledge of shape and motion and that these components are assimilated into an altered perception of the action outcome. We propose that the visual system attempts to resist inconsistent interpretations of human animations. Actions involving object manipulations present an opportunity for the visual system to reinterpret the introduced inconsistencies as a change in the dynamics of an object rather than as an unexpected combination of body shape and body motion. © 2019 Copyright held by the owner/author(s).",Action; Animated avatars; Discrimination; Human animation; Inconsistency; Perception; Realism; Retargeting; Shape capture,Animation; Kinematics; Sensory perception; Virtual reality; Action; Animated avatars; Discrimination; Human animation; Inconsistency; Realism; Retargeting; Shape capture; Motion estimation
Non-visual Perception of Lines on a Multimodal Touchscreen Tablet,2019,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062352386&doi=10.1145%2f3301415&partnerID=40&md5=50ca4df316323289a16e001558af7a90,"While text-to-speech software has largely made textual information accessible in the digital space, analogous access to graphics still remains an unsolved problem. Because of their portability and ubiquity, several studies have alluded to touchscreens as a potential platform for such access, yet there is still a gap in our understanding of multimodal information transfer in the context of graphics. The current research demonstrates feasibility for following lines, a fundamental graphical concept, via vibrations and sounds on commercial touchscreens. Two studies were run with 21 blind and visually impaired participants (N = 12; N = 9). The first study examined the presentation of straight, linear lines using a multitude of line representations, such as vibration-only, auditory-only, vibration lines with auditory borders, and auditory lines with vibration borders. The results of this study demonstrated that both auditory and vibratory bordered lines were optimal for precise tracing, although both vibration- and auditory-only lines were also sufficient for following, with minimal deviations. The second study examined the presentation of curving, non-linear lines. Conditions differed on the number of auditory reference points presented at the inflection and deflection points. Participants showed minimal deviation from the lines during tracing, performing nearly equally in both 1- and 3-point conditions. From these studies, we demonstrate that line following via multimodal feedback is possible on touchscreens, and we present guidelines for the presentation of such non-visual graphical concepts. © 2019 Association for Computing Machinery.",Haptics; HCI; Perception; Touchscreen,Human computer interaction; Sensory perception; Touch screens; Blind and visually impaired; Deflection points; Haptics; Multi-modal information; Multimodal feedback; Reference points; Textual information; Unsolved problems; Psychology computing
Perceptual attributes analysis of real-world materials,2019,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061215223&doi=10.1145%2f3301412&partnerID=40&md5=90dcab862f5493c1d31f1e2a9a0be21b,"Material appearance is often represented by a bidirectional reffectance distribution function (BRDF). Although the concept of the BRDF is widely used in computer graphics and related applications, the number of actual captured BRDFs is limited due to a time and resources demanding measurement process. Several BRDF databases have already been provided publicly, yet subjective properties of underlying captured material samples, apart from single photographs, remain unavailable for users. In this article, we analyzed material samples, used in the creation of the UTIA BRDF database, in a psychophysical study with nine subjects and assessed its 12 visual, tactile, and subjective attributes. Further, we evaluated the relationship between the attributes and six material categories. We consider the presented perceptual analysis as valuable and complementary information to the database; that could aid users to select appropriate materials for their applications. © 2019 Association for Computing Machinery.",Attributes; BRDF; Perception; Tactile; User study; Visual,Computer graphics; Distribution functions; Sensory perception; Attributes; BRDF; Tactile; User study; Visual; Database systems
Light shapes: perception-based visualizations of the global light transport,2019,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060921968&doi=10.1145%2f3232851&partnerID=40&md5=72216d2936306cd88336a775b8bab747,"In computer graphics, illuminating a scene is a complex task, typically consisting of cycles of adjusting and rendering the scene to see the effects. We propose a technique for visualization of light as a tensor field via extracting its properties (i.e., intensity, direction, diffuseness) from (virtual) radiance measurements and showing these properties as a grid of shapes over a volume of a scene. Presented in the viewport, our visualizations give an understanding of the illumination conditions in the measured volume for both the local values and the global variations of light properties. Additionally, they allow quick inferences of the resulting visual appearance of (objects in) scenes without the need to render them. In our evaluation, observers performed at least as well using visualizations as using renderings when they were comparing illumination between parts of a scene and inferring the final appearance of objects in the measured volume. Therefore, the proposed visualizations are expected to help lighting artists by providing perceptually relevant information about the structure of the light field and flow in a scene. © 2019 Association for Computing Machinery.",Light transport; Lighting design; Perception; Visualization,Flow visualization; Light transmission; Lighting; Petroleum reservoir evaluation; Sensory perception; Visibility; Visualization; Global light transports; Global variations; Illumination conditions; Light properties; Light transport; Lighting designs; Radiance measurement; Visual appearance; Rendering (computer graphics)
Perceptual adjustment of eyeball rotation and pupil size jitter for virtual characters,2018,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061208414&doi=10.1145%2f3238302&partnerID=40&md5=528a3790ebc666893a87f7e45c401080,"Eye motions constitute an important part of our daily face-to-face interactions. Even subtle details in the eyes' motions give us clues about a person's thoughts and emotions. Believable and natural animation of the eyes is therefore crucial when creating appealing virtual characters. In this article, we investigate the perceived naturalness of detailed eye motions, more specifically of jitter of the eyeball rotation and pupil diameter on three virtual characters with differing levels of realism. Participants watched stimuli with six scaling factors from 0 to 1 in increments of 0.2, varying eye rotation and pupil size jitter individually, and they had to indicate if they would like to increase or decrease the level of jitter to make the animation look more natural. Based on participants' responses, we determine the scaling factors for noise attenuation perceived as most natural for each character when using motion-captured eye motions. We compute the corresponding average jitter amplitudes for the eyeball rotation and pupil size to serve as guidelines for other characters. We find that the amplitudes perceived as most natural depend on the character, with our character with a medium level of realism requiring the largest scaling factors. © 2018 Association for Computing Machinery.",Character animation; eye motion; jitter; perceptual study; pupil dilation,Animation; Rotation; Character animation; eye motion; Face-to-face interaction; Noise attenuation; perceptual study; Pupil dilation; Scaling factors; Virtual character; Jitter
Analysis of hair shine using rendering and subjective evaluation,2018,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061215511&doi=10.1145%2f3274478&partnerID=40&md5=09c777194f0ec0f839210be632df7cd1,"Hair shine is a highly desirable attribute to consumers within the cosmetic industry and is also an important indicator of hair health. However, perceptual evaluation of shine is a complex task as it is known that even subtle manipulation of local hair properties such as colour, thickness, or style and global properties such as lighting or environment can affect the evaluation. In this article, we are interested in the physical, optical, and chemical characteristics that affect the realism of hair along with the perception of shine. We have constructed a Computer Graphics (CG) setup, based on current physical testing systems, that reduces the number of variables that affect the perspective. Physically based shading models were used to create the images that participants assessed on realism, health, naturalness, and shine through three different evaluation experiments. Our results provide new insights on how hair is perceived, the factors that affect its realism, and the potential of using CG techniques in the cosmetic industry to replace physical testing. © 2018 Association for Computing Machinery.",hair rendering; Hair shine; perception,Color computer graphics; Cosmetics; Sensory perception; Chemical characteristic; Cosmetic industry; Evaluation experiments; Global properties; Hair rendering; Hair shine; Perceptual evaluation; Subjective evaluations; Rendering (computer graphics)
Introduction to special issue SAP 2018,2018,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061180188&doi=10.1145%2f3274477&partnerID=40&md5=cc08a66104e1695b31c3a267fa886724,[No abstract available],,
Comparison of unobtrusive visual guidance methods in an immersive dome environment,2018,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053774454&doi=10.1145%2f3238303&partnerID=40&md5=0e5af2e939005384cb8484d0e9357cbb,"In this article, we evaluate various image-space modulation techniques that aim to unobtrusively guide viewers' attention. While previous evaluations mainly target desktop settings, we examine their applicability to ultrawide field of view immersive environments, featuring technical characteristics expected for future-generation head-mounted displays. A custom-built, high-resolution immersive dome environment with high-precision eye tracking is used in our experiments. We investigate gaze guidance success rates and unobtrusiveness of five different techniques. Our results show promising guiding performance for four of the tested methods. With regard to unobtrusiveness we find that-while no method remains completely unnoticed-many participants do not report any distractions. The evaluated methods show promise to guide users' attention also in a wide field of virtual environment applications, e.g., virtually guided tours or field operation training. © 2018 Association for Computing Machinery.",Dome; Eye tracking; Perception; Postprocessing; Unobtrusive gaze guidance; Virtual Reality,Domes; Helmet mounted displays; Sensory perception; Virtual reality; Visual communication; Future generations; Gaze guidances; Head mounted displays; Image-space modulations; Immersive environment; Postprocessing; Virtual environment applications; Visual guidance; Eye tracking
Foveated depth-of-field filtering in head-mounted displays,2018,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053753698&doi=10.1145%2f3238301&partnerID=40&md5=627eb24bc8a8bc8b2d2f46848f36542d,"In recent years, a variety of methods have been introduced to exploit the decrease in visual acuity of peripheral vision, known as foveated rendering. As more and more computationally involved shading is requested and display resolutions increase, maintaining low latencies is challenging when rendering in a virtual reality context. Here, foveated rendering is a promising approach for reducing the number of shaded samples. However, besides the reduction of the visual acuity, the eye is an optical system, filtering radiance through lenses. The lenses create depth-of-field (DoF) effects when accommodated to objects at varying distances. The central idea of this article is to exploit these effects as a filtering method to conceal rendering artifacts. To showcase the potential of such filters, we present a foveated rendering system, tightly integrated with a gaze-contingent DoF filter. Besides presenting benchmarks of the DoF and rendering pipeline, we carried out a perceptual study, showing that rendering quality is rated almost on par with full rendering when using DoF in our foveated mode, while shaded samples are reduced by more than 69%. © 2018 Association for Computing Machinery.",Eye-tracking; Foveated rendering; Gaze-contingent depth-of-field; Ray tracing,Benchmarking; Eye tracking; Helmet mounted displays; Lenses; Optical systems; Ray tracing; Virtual reality; Vision; Depth of field; Display resolutions; Foveated rendering; Head mounted displays; Peripheral vision; Rendering pipelines; Rendering quality; Rendering system; Rendering (computer graphics)
Non-invasive measurement of cognitive load and stress based on the reflected stress-induced vascular response index,2018,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053412897&doi=10.1145%2f3185665&partnerID=40&md5=6a5b73798b8b49396849371e3cd8baca,"Measuring cognitive load and stress is crucial for ubiquitous human-computer interaction applications to dynamically understand and respond to the mental status of users, such as in smart healthcare, smart driving, and robotics. Various quantitative methods have been employed for this purpose, such as physiological and behavioral methods. However, the sensitivity, reliability, and usability are not satisfactory in many of the current methods, so they are not ideal for ubiquitous applications. In this study, we employed a reflected photoplethysmogram-based stress-induced vascular response index, i.e., the reflected sVRI (sVRI-r), to non-invasively measure the cognitive load and stress. This method has high usability as well as good sensitivity and reliability compared with the previously proposed transmitted sVRI (sVRI-t).We developed the basic methodology and detailed algorithm framework to validate the sVRI-r measurements, and it was implemented by employing two light sources, i.e., infrared light and green light. Compared with the simultaneously recorded blood pressure, heart rate variation, and sVRI-t, our findings demonstrated the greater potential of the sVRI-r for use as a sensitive, reliable, and usable parameter, as well as suggesting its potential integration with ubiquitous touch interactions for dynamic cognition and stress-sensing scenarios. © 2018 ACM.",Cognitive load; Mental effort; Photoplethysmogram; Reflected stress-induced vascular response index; Stress,Blood pressure; Human robot interaction; Light sources; Medical computing; Stresses; Algorithm framework; Cognitive loads; Heart rate variations; Mental effort; Non- invasive measurements; Photo-plethysmogram; Ubiquitous application; Vascular response; Human computer interaction
Toward affective handles for tuning vibrations,2018,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053377972&doi=10.1145%2f3230645&partnerID=40&md5=2d90d42ee8699dae341f0b4cfc81f10a,"When refining or personalizing a design, we count on being able to modify or move an element by changing its parameters rather than creating it anew in a different form or location-a standard utility in graphic and auditory authoring tools. Similarly, we need to tune vibrotactile sensations to fit new use cases, distinguish members of communicative icon sets, and personalize items. For tactile vibration display, however, we lack knowledge of the human perceptual mappings that must underlie such tools. Based on evidence that affective dimensions are a natural way to tune vibrations for practical purposes,we attempted to manipulate perception along three emotion dimensions (agitation, liveliness, and strangeness) using engineering parameters of hypothesized relevance. Results from two user studies show that an automatable algorithm can increase a vibration's perceived agitation and liveliness to different degrees via signal energy, while increasing its discontinuity or randomness makes it more strange. These continuous mappings apply across diverse base vibrations; the extent of achievable emotion change varies. These results illustrate the potential for developing vibrotactile emotion controls as efficient tuning for designers and end-users. © 2018 ACM.",Affective haptics; Design and personalization tools; Emotion dimensions; End-user perception,Mapping; Tuning; Affective dimensions; Continuous mappings; Emotion dimensions; End users; Engineering parameters; Haptics; Personalizations; Vibrotactile sensations; Behavioral research
A comparison of distance estimation in HMD-based virtual environments with different HMD-based conditions,2018,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053425599&doi=10.1145%2f3196885&partnerID=40&md5=01c8cd2b242219b2ab2be3dc8a188fe6,"Underestimation of egocentric distances in immersive virtual environments using various head-mounted displays (HMDs) has been a puzzling topic of research interest for several years. As more commodity-level systems become available to developers, it is important to test the variation of underestimation in each system since reasons for underestimation remain elusive. In this article, we examine several different systems in two experiments and comparatively evaluate how much users underestimate distances in each one. To observe distance estimation behavior, a standard indirect blind walking task was used. An Oculus Rift DK1, weighted Oculus Rift DK1, Oculus Rift DK1 with an artificially restricted field of view, Nvis SX60, Nvis SX111, Oculus Rift DK2, Oculus Rift consumer version (CV1), and HTC Vive were tested. The weighted and restricted field of view HMDs were evaluated to determine the effect of these factors on distance underestimation; the other systems were evaluated because they are popular systems that arewidely available.We found thatweight and field of viewrestrictions heightened underestimation in the Rift DK1. Results from these conditions were comparable to the Nvis SX60 and SX111. The Oculus Rift DK1 and CV1 possessed the least amount of distance underestimation, but in general, commodity-level HMDs provided more accurate estimates of distance than the prior generation of HMDs. © 2018 ACM.",Distance perception; Field-of-view; Head-mounted displays,Depth perception; Virtual reality; Blind walking; Distance estimation; Distance perception; Field of views; Head mounted displays; Immersive virtual environments; Research interests; Helmet mounted displays
Influence of screen size and field of view on perceived brightness,2018,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053376772&doi=10.1145%2f3190346&partnerID=40&md5=42403e5de937a5801464cbfaaeaa2ce0,"We present a study into the perception of display brightness as related to the physical size and distance of the screen from the observer. Brightness perception is a complex topic, which is influenced by a number of lower- and higher-order factors-with empirical evidence from the cinema industry suggesting that display size may play a significant role. To test this hypothesis, we conducted a series of user studies exploring brightness perception for a range of displays and distances from the observer that span representative use scenarios. Our results suggest that retinal size is not sufficient to explain the range of discovered brightness variations, but is sufficient in combination with physical distance from the observer. The resulting model can be used as a step toward perceptually correcting image brightness perception based on target display parameters. This can be leveraged for energy management and the preservation of artistic intent. A pilot study suggests that adaptation luminance is an additional factor for the magnitude of the effect. © 2018 ACM.",Brightness; Computational display; Perception; Size effects; Viewing distance,Psychology computing; Sensory perception; Brightness perception; Brightness variations; Display parameters; Field of views; Image brightness; Pilot studies; Size effects; Viewing distance; Luminance
A human-perceived softness measure of virtual 3d objects,2018,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053381444&doi=10.1145%2f3193107&partnerID=40&md5=9778586262ff233a3a9e2040008e96c8,"We introduce the problem of computing a human-perceived softness measure for virtual 3D objects. As the virtual objects do not exist in the real world, we do not directly consider their physical properties but instead compute the human-perceived softness of the geometric shapes. In an initial experiment, we find that humans are highly consistent in their responses when given a pair of vertices on a 3D model and asked to select the vertex that they perceive to be more soft. This motivates us to take a crowdsourcing and machine learning framework. We collect crowdsourced data for such pairs of vertices. We then combine a learning-to-rank approach and a multi-layer neural network to learn a non-linear softness measure mapping any vertex to a softness value. For a new3D shape,we can use the learnedmeasure to compute the relative softness of every vertex on its surface. We demonstrate the robustness of our framework with a variety of 3D shapes and compare our non-linear learning approach with a linear method from previous work. Finally, we demonstrate the accuracy of our learned measure with user studies comparing our measure with the human-perceived softness of both virtual and real objects, and we show the usefulness of our measure with some applications. © 2018 ACM.",3D modeling; Crowdsourcing; Fabrication; Learning,Crowdsourcing; Fabrication; Learning systems; Network layers; 3-d modeling; Geometric shape; Learning; Learning to rank; Linear methods; Nonlinear learning; Virtual 3D objects; Virtual objects; Virtual reality
Behavior analysis of human locomotion in the realworld and virtual reality for the manufacturing industry,2018,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053413936&doi=10.1145%2f3230648&partnerID=40&md5=6c8d8275e54ad8c15c67090c0bfc1eb4,"With the rise of immersive visualization techniques, many domains within the manufacturing industry are increasingly validating production processes in virtual reality (VR). The validity of the results gathered in such simulations, however, is widely unknown-in particular, with regard to human locomotion behavior. To bridge this gap, this article presents an experiment analyzing the behavioral disparity between human locomotion being performed without any equipment and in immersive VR while wearing a head-mounted display (HMD). The presented study (n = 30) is split up in three sections and covers linear walking, non-linear walking, and obstacle avoidance. Special care has been given to design the experiment so that findings are generally valid and can be applied to a wide range of domains beyond the manufacturing industry. The findings provide novel insights into the effect of immersive VR on specific gait parameters. In total, a comprehensive sample of 18.09km is analyzed. The results reveal that the HMD had a medium effect (up to 13%) on walking velocity, on non-linear walking toward an oriented target, and on clearance distance. The overall differences are modeled using multiple regression models, thus allowing the general usage within various domains. Summarizing, it can be concluded that VR can be used to analyze and plan human locomotion; however, specific details may have to be adjusted to transfer findings to the real world. © 2018 ACM.",Comparison; Gait analysis; Human locomotion behavior; User studies; Virtual reality,Behavioral research; Gait analysis; Helmet mounted displays; Manufacture; Regression analysis; Comparison; Head mounted displays; Human locomotions; Immersive visualization; Manufacturing industries; Multiple regression model; Production process; User study; Virtual reality
Human perception of inertial mass for joint human-robot object manipulation,2018,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053401421&doi=10.1145%2f3182176&partnerID=40&md5=cb5d7f0cc7640c960a689b9a3fa84ade,"In this article, we investigate human perception of inertial mass discrimination in active planar manipulations, as they are common in daily tasks, such as moving heavy and bulky objects. Psychophysical experiments were conducted to develop a human inertial mass perception model to improve usability and acceptance of novel haptically collaborating robotic systems. In contrast to existing literature, large-scale movements involving a broad selection of reference stimuli and larger sample sizeswere used. Linear mixed modelswere fitted to model dependent errors from the longitudinal perceptual data. Differential thresholds near the perception boundary exponentially increased and resulted in constant behavior for higher stimuli. No effect of different directions (sagittal and transversal) was found; however, a large effect of different movement types (precise and imprecise) was present in the data. Recommendations to implement the findings in novel physical assist devices are given. © 2018 ACM.",Differential thresholds; Haptic human-robot collaboration; Haptics; Human factors; Human modelling; Psychophysics,Human computer interaction; Human engineering; Differential threshold; Haptics; Human modelling; Human-robot collaboration; Psychophysics; Robots
Sensory substitution for force feedback recovery: A perception experimental study,2018,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053429138&doi=10.1145%2f3176642&partnerID=40&md5=16f5ce67c9e61f1dc561a526c8d076de,"Robotic-assisted surgeries are commonly used today as a more efficient alternative to traditional surgical options. Both surgeons and patients benefit from those systems, as they offer many advantages, including less trauma and blood loss, fewer complications, and better ergonomics. However, a remaining limitation of currently available surgical systems is the lack of force feedback due to the teleoperation setting, which prevents direct interaction with the patient. Once the force information is obtained by either a sensing device or indirectly through vision-based force estimation, a concern arises on how to transmit this information to the surgeon. An attractive alternative is sensory substitution, which allows transcoding information from one sensory modality to present it in a different sensory modality. In the current work, we used visual feedback to convey interaction forces to the surgeon. Our overarching goal was to address the following question: How should interaction forces be displayed to support efficient comprehension by the surgeon without interfering with the surgeon's perception and workflow during surgery? Until now, the use the visual modality for force feedback has not been carefully evaluated. For this reason, we conducted an experimental study with two aims: (1) to demonstrate the potential benefits of using this modality and (2) to understand the surgeons' perceptual preferences. The results derived from our study of 28 surgeons revealed a strong positive acceptance of the users (96%) using this modality. Moreover, we found that for surgeons to easily interpret the information, their mental model must be considered, meaning that the design of the visualizations should fit the perceptual and cognitive abilities of the end user. To our knowledge, this is the first time that these principles have been analyzed for exploring sensory substitution in medical robotics. Finally, we provide user-centered recommendations for the design of visual displays for robotic surgical systems. © 2018 ACM.",Flow visualization; Robotic teleoperation; Visualization,Ergonomics; Feedback; Flow visualization; Remote control; Robotics; Sensory perception; Surgery; User centered design; Visual communication; Visual servoing; Visualization; Cognitive ability; Direct interactions; Interaction forces; Potential benefits; Robotic assisted surgeries; Robotic surgical systems; Robotic teleoperation; Sensory substitution; Robotic surgery
Perceptual Evaluation of Synthesized Sound Effects,2018,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060248855&doi=10.1145%2f3165287&partnerID=40&md5=7f14b675064258cc59d1030f066fd4e1,"Sound synthesis is the process of generating artificial sounds through some form of simulation or modelling. This article aims to identify which sound synthesis methods achieve the goal of producing a believable audio sample that may replace a recorded sound sample. A perceptual evaluation experiment of five different sound synthesis techniques was undertaken. Additive synthesis, statistical modelling synthesis with two different feature sets, physically inspired synthesis, concatenative synthesis, and sinusoidal modelling synthesis were all compared. Evaluation using eight different sound class stimuli and 66 different samples was undertaken. The additive synthesizer is the only synthesis method not considered significantly different from the reference sample across all sounds classes. The results demonstrate that sound synthesis can be considered as realistic as a recorded sample and makes recommendations for use of synthesis methods, given different sound class contexts. © 2018 ACM.",evaluation; perception; procedural audio; sound effects; Sound synthesis,Additives; Additive synthesis; Audio samples; Feature sets; Perceptual evaluation; Sound synthesis; Statistical modelling; Synthesis method; Synthesized sounds; Audio acoustics
The Effects of Peripheral Vision and Light Stimulation on Distance Judgments through HMDs,2018,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063789747&doi=10.1145%2f3165286&partnerID=40&md5=0ee18558e78e1ed81341c93d6f549428,"Egocentric distances are often underestimated in virtual environments through head-mounted displays (HMDs). Previous studies suggest that peripheral vision can influence distance perception. Specifically, light in the periphery may improve distance judgments in HMDs. In this study, we conducted a series of experiments with varied peripheral treatments around the viewport. First, we found that the peripheral brightness significantly influences distance judgments when the periphery is brighter than a certain threshold, and found a possible range where the threshold was in. Second, we extended our previous research by changing the size of the peripheral treatment. A larger visual field (field of view of the HMD) resulted in significantly more accurate distance judgments compared to our original experiments with black peripheral treatment. Last, we found that applying a pixelated peripheral treatment can also improve distance judgments. The result implies that augmenting peripheral vision with secondary low-resolution displays may improve distance judgments in HMDs. © 2018 ACM.",blind walking; Distance perception; DK2; field of view; oculus rift HMDs; peripheral vision,Depth perception; Distance perception; Field of views; Head mounted displays; Light stimulation; Low resolution; Peripheral vision; Visual fields; Helmet mounted displays
Acting Together,2018,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070878761&doi=10.1145%2f3147884&partnerID=40&md5=5e2ff21a29869a8c413eaf480da60ce9,"We investigated how two people jointly coordinate their decisions and actions in a co-occupied, large-screen virtual environment. The task for participants was to physically cross a virtual road with continuous traffic without getting hit by a car. Participants performed this task either alone or with another person (see Figure 1). Two separate streams of non-stereo images were generated based on the dynamic locations of the two viewers' eye-points. Stereo shutter glasses were programmed to display a single image stream to each viewer so that they saw perspectively correct non-stereo images for their eyepoint. We found that participant pairs often crossed the same gap together and closely synchronized their movements when crossing. Pairs also chose larger gaps than individuals, presumably to accommodate the extra time needed to cross through gaps together. These results demonstrate how two people interact and coordinate their behaviors in performing whole-body, joint motions in a co-occupied virtual environment. This study also provides a foundation for future studies examining joint actions in shared VEs where participants are represented by graphic avatars. © 2018 ACM.",co-occupied virtual environments; Immersive virtual environments; joint action; joint affordance; large screen VE; pedestrian road crossing; stereo displays,Psychology computing; Sensory perception; Joint actions; Joint motion; Large screen; Single images; Stereo shutter glass; Stereo-image; Whole body; Stereo image processing
Creating Thermal Icons - A Model-Based Approach,2018,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067628522&doi=10.1145%2f3182175&partnerID=40&md5=da005af47d95b87718e448beae5e496c,"The objective of this set of experiments was to evaluate thermal pattern recognition on the hand and arm and to determine which features of thermal stimuli are encoded by cutaneous thermoreceptors and perceived by the user of a thermal display. Thermal icons were created by varying the direction, rate, and magnitude of change in temperature. It was found that thermal icons were identified more accurately when presented on the thenar eminence or the wrist, as compared to the fingertips and that thermal patterns as brief as 8s could be reliably identified. In these experiments, there was no difference in performance when identifying warm or cool stimuli. A dynamic model of the change in skin temperature as a function of the thermal input was developed based on linear system identification techniques. This model was able to predict the change in skin temperature from an unrelated experiment involving thermal icons. This opens the possibility of using a model-based approach to the development of thermal icons. © 2018 ACM.",hand-object interaction; Thermal display; thermal feedback; thermal perception,Linear systems; Model based approach; Skin temperatures; Thermal display; Thermal inputs; Thermal patterns; Thermal stimuli; Pattern recognition
Comparison of Two Methods for Improving Distance Perception in Virtual Reality,2018,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055444088&doi=10.1145%2f3165285&partnerID=40&md5=a3336a1a654615023fab31c1aad832f7,"Distance is commonly underperceived in virtual environments (VEs) compared to real environments. Past work suggests that displaying a replica VE based on the real surrounding environment leads to more accurate judgments of distance, but that work has lacked the necessary control conditions to firmly make this conclusion. Other research indicates that walking through a VE with visual feedback improves judgments of distance and size. This study evaluated and compared those two methods for improving perceived distance in VEs. All participants experienced a replica VE based on the real lab. In one condition, participants visually previewed the real lab prior to experiencing the replica VE, and in another condition they did not. Participants performed blind-walking judgments of distance and also judgments of size in the replica VE before and after walking interaction. Distance judgments were more accurate in the preview compared to no preview condition, but size judgments were unaffected by visual preview. Distance judgments and size judgments increased after walking interaction, and the improvement was larger for distance than for size judgments. After walking interaction, distance judgments did not differ based on visual preview, and walking interaction led to a larger improvement in judged distance than did visual preview. These data suggest that walking interaction may be more effective than visual preview as a method for improving perceived space in a VE. © 2018 ACM.",Depth perception; stereoscopic displays; virtual environments,Virtual reality; Visual communication; Blind walking; Distance perception; Perceived distances; Real environments; Surrounding environment; Visual feedback; Walking through; Depth perception
Perceptual constancy in the reproduction of virtual tactile textures with surface displays,2018,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042946084&doi=10.1145%2f3152764&partnerID=40&md5=a6cb04d5dfd06b4384c6e2a612569532,"For very rough surfaces, friction-induced vibrations contain frequencies that change in proportion to sliding speed. Given the poor capacity of the somatosensory system to discriminate frequencies, this fact raises the question of how accurately finger sliding speed must be known during the reproduction of virtual textures with a surface tactile display. During active touch, ten observers were asked to discriminate texture recordings corresponding to different speeds. The samples were constructed from a common texture, which was resampled at various frequencies to give a set of stimuli of different swiping speeds. In trials, they swiped their finger in rapid succession over a glass plate, which vibrated to accurately reproduce three texture recordings. Two of these recordings were identical and a third differed in that the sample represented a texture swiped at a speed different from the other two. Observers identified which of the three samples felt different. For a metal mesh texture recording, seven observers reported differences when the speed varied by 60, 80, and 100mm/s while the other three did not reach a discrimination threshold. For a finer leather chamois texture recording, thresholds were never reached in the 100mm/s range. These results show that the need for high-accuracy measurement of swiping speed during texture reproduction may actually be quite limited compared to what is commonly found in the literature. 2018 Copyright is held by the owner/author(s). Publication rights licensed to ACM.",Design requirements; Haptic texture rendering; Speed perception; Tactile stimulation,Psychology computing; Sensory perception; Discrimination thresholds; Friction induced vibration; Haptic textures; High-accuracy measurements; Somatosensory systems; Surface displays; Tactile stimulation; Texture reproductions; Speed
The contribution of stereoscopic and motion depth cues to the perception of structures in 3D point clouds,2018,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042912678&doi=10.1145%2f3147914&partnerID=40&md5=2742f82cc0629c52e263ebc0c4727ada,"Particle-based simulations are used across many science domains, and it is well known that stereoscopic viewing and kinetic depth enhance our ability to perceive the 3D structure of such data. But the relative advantages of stereo and kinetic depth have not been studied for point cloud data, although they have been studied for 3D networks. This article reports two experiments assessing human ability to perceive 3D structures in point clouds as a function of different viewing parameters. In the first study, the number of discrete views was varied to determine the extent to which smooth motion is needed. Also, half the trials had stereoscopic viewing and half had no stereo. The results showed kinetic depth to be more beneficial than stereo viewing in terms of accuracy and so long as the motion was smooth. The second experiment varied the amplitude of oscillatory motion from 0 to 16 degrees. The results showed an increase in detection rate with amplitude, with the best amplitudes being 4 degrees and greater. Overall, motion was shown to yield greater accuracy, but at the expense of longer response times in comparison with stereoscopic viewing. 2018 Copyright is held by the owner/author(s). Publication rights licensed to ACM.",3D perception; Depth cues; Point cloud data,Kinetics; 3D perception; Depth cue; Human abilities; Oscillatory motion; Particle-based simulation; Point cloud data; Stereoscopic viewing; Viewing parameters; Stereo image processing
The perceptual consequences of curved screens,2017,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032921491&doi=10.1145%2f3106012&partnerID=40&md5=a348a00b12423b3a2add1e735792c693,"Flat panels are by far the most common type of television screen. There are reasons, however, to believe that curved screens create a greater sense of immersion, reduce distracting reflections, and minimize some perceptual distortions that are commonplace with large televisions. To examine these possibilities, we calculated how curving the screen affects the field of view and the probability of seeing reflections of ambient lights. We find that screen curvature has a small beneficial effect on field of view and a large beneficial effect on the probability of seeing reflections. We also collected behavioral data to characterize perceptual distortions in various viewing configurations. We find that curved screens can in fact reduce problematic perceptual distortions on large screens, but that the benefit depends on the geometry of the projection on such screens.",Field of view; Focal length; Perceptual distortions; Reflections; Television screen,Reflection; Sensory perception; Ambient light; Behavioral data; Beneficial effects; Field of views; Focal lengths; Large screen; Perceptual consequences; Perceptual distortion; Psychology computing
Evaluating the use of sound in static program comprehension,2017,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032957830&doi=10.1145%2f3129456&partnerID=40&md5=35f8cee8bbdb41b7d4df3efe0a586432,"Florida Institute of Technology Comprehension of computer programs is daunting, due in part to clutter in the software developer's visual environment and the need for frequent visual context changes. Previous research has shown that nonspeech sound can be useful in understanding the runtime behavior of a program. We explore the viability and advantages of using nonspeech sound in an ecological framework to help understand the static structure of software. We describe a novel concept for auditory display of program elements in which sounds indicate characteristics and relationships among a Java program's classes, interfaces, and methods. An empirical study employing this concept was used to evaluate 24 sighted software professionals and students performing maintenance-oriented tasks using a 2 × 2 crossover. Viability is strong for differentiation and characterization of software entities, less so for identification. The results suggest that sonification can be advantageous under certain conditions, though they do not indicate the overall advantage of using sound in terms of task duration at a 5% level of significance. The results uncover other findings such as differences in comprehension strategy based on the available tool environment. The participants reported enthusiasm for the idea of software sonification, mitigated by lack of familiarity with the concept and the brittleness of the tool. Limitations of the present research include restriction to particular types of comprehension tasks, a single sound mapping, a single programming language, and limited training time, but the use of sound in program comprehension shows sufficient promise for continued research. © 2017 ACM.",Applied sound; Auditory display; Interactive sonification; Program comprehension; Sonification; Sound design,Computer programming; Fracture mechanics; Auditory display; Interactive sonification; Program comprehension; Sonifications; Sound designs; Computer software
Facial features of non-player creatures can influence moral decisions in video games,2017,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032958548&doi=10.1145%2f3129561&partnerID=40&md5=be2ffcc779d2d0b70659987972c4962a,"With the development of increasingly sophisticated computer graphics, there is a continuous growth of the variety and originality of virtual characters used in movies and games. So far, however, their design has mostly been led by the artist's preferences, not by perceptual studies. In this article, we explored how effective non-player character design can be used to influence gameplay. In particular, we focused on abstract virtual characters with few facial features. In experiment 1, we sought to find rules for how to use a character's facial features to elicit the perception of certain personality traits, using prior findings for human face perception as a basis. In experiment 2, we then tested how perceived personality traits of a nonplayer character could influence a player's moral decisions in a video game. We found that the appearance of the character interacting with the subject modulated aggressive behavior towards a non-present individual. Our results provide us with a better understanding of the perception of abstract virtual characters, their employment in video games, as well as giving us some insights about the factors underlying aggressive behavior in video games. © 2017 ACM.",Moral dilemmas; Personality perception; Video games; Virtual characters,Abstracting; Computer games; Computer graphics; Interactive computer graphics; Facial feature; Human faces; Moral dilemmas; Non-player character; Non-players; Personality traits; Video game; Virtual character; Human computer interaction
Improving human-machine cooperative visual search with soft highlighting,2017,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032930126&doi=10.1145%2f3129669&partnerID=40&md5=8f34a8f98598d5ea449740f47c2fd7f7,"Advances in machine learning have produced systems that attain human-level performance on certain visual tasks, e.g., object identification. Nonetheless, other tasks requiring visual expertise are unlikely to be entrusted to machines for some time, e.g., satellite and medical imagery analysis. We describe a human-machine cooperative approach to visual search, the aim of which is to outperform either human or machine acting alone. The traditional route to augmenting human performance with automatic classifiers is to draw boxes around regions of an image deemed likely to contain a target. Human experts typically reject this type of hard highlighting. We propose instead a soft highlighting technique in which the saliency of regions of the visual field is modulated in a graded fashion based on classifier confidence level. We report on experiments with both synthetic and natural images showing that soft highlighting achieves a performance synergy surpassing that attained by hard highlighting.",Soft highlighting; Target localization; Visual search,Learning systems; Satellite imagery; Automatic classifiers; Confidence levels; Human performance; Human-level performance; Object identification; Soft highlighting; Target localization; Visual search; Vision
Visual quality assessment of 3D models: On the influence of light-material interaction,2017,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032934842&doi=10.1145%2f3129505&partnerID=40&md5=6e25e20c31ad727e2cccb108e8946112,"Geometric modifications of three-dimensional (3D) digital models are commonplace for the purpose of efficient rendering or compact storage. Modifications imply visual distortions that are hard to measure numerically. They depend not only on the model itself but also on how the model is visualized. We hypothesize that the model's light environment and the way it reflects incoming light strongly influences perceived quality. Hence, we conduct a perceptual study demonstrating that the same modifications can be masked, or conversely highlighted, by different light-matter interactions. Additionally, we propose a new metric that predicts the perceived distortion of 3D modifications for a known interaction. It operates in the space of 3D meshes with the object's appearance, that is, the light emitted by its surface in any direction given a known incoming light. Despite its simplicity, this metric outperforms 3D mesh metrics and competes with sophisticated perceptual image-based metrics in terms of correlation to subjective measurements. Unlike image-based methods, it has the advantage of being computable prior to the costly rendering steps of image projection and rasterization of the scene for given camera parameters. © 2017 ACM.",3D object; Appearance; Perceptual study; Quality assessment; Surface mesh reflectance,Mesh generation; Rasterization; Rendering (computer graphics); 3D object; Appearance; Perceptual study; Quality assessment; Surface mesh; Three dimensional computer graphics
Comparative analysis of three different modalities for perception of artifacts in videos,2017,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029904367&doi=10.1145%2f3129289&partnerID=40&md5=5b4db901c6e7f5b41188938c30c0498e,"This study compares three popular modalities for analyzing perceived video quality; user ratings, eye tracking, and EEG.We contrast these three modalities for a given video sequence to determine if there is a gap between what humans consciously see and what we implicitly perceive. Participants are shown a video sequence with different artifacts appearing at specific distances in their field of vision; near foveal, middle peripheral, and far peripheral. Our results show distinct differences between what we saccade to (eye tracking), how we consciously rate video quality, and our neural responses (EEG data). Our findings indicate that the measurement of perceived quality depends on the specific modality used. 2017 Copyright is held by the owner/author(s). Publication rights licensed to ACM.",Artifacts; EEG; ERP; Eye tracking; Implicit perception; Perceptual quality; User rating; Video,Electroencephalography; Enterprise resource planning; Video recording; Artifacts; Eye-tracking; Perceptual quality; User rating; Video; Eye movements
Latency requirements for foveated rendering in virtual reality,2017,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029907071&doi=10.1145%2f3127589&partnerID=40&md5=70b88ce5fdbf3c506be2ae1245e078ba,"Foveated rendering is a performance optimization based on thewell-known degradation of peripheral visual acuity. It reduces computational costs by showing a high-quality image in the user's central (foveal) vision and a lower quality image in the periphery. Foveated rendering is a promising optimization for Virtual Reality (VR) graphics, and generally requires accurate and low-latency eye tracking to ensure correctness even when a user makes large, fast eye movements such as saccades. However, due to the phenomenon of saccadic omission, it is possible that these requirements may be relaxed. In this article, we explore the effect of latency for foveated rendering in VR applications.We evaluated the detectability of visual artifacts for three techniques capable of generating foveated images and for three different radii of the high-quality foveal region. Our results show that larger foveal regions allow for more aggressive foveation, but this effect is more pronounced for temporally stable foveation techniques. Added eye tracking latency of 80-150ms causes a significant reduction in acceptable amount of foveation, but a similar decrease in acceptable foveation was not found for shorter eye-tracking latencies of 20-40ms, suggesting that a total system latency of 50-70ms could be tolerated. © 2017 ACM.",Eye-tracking; Foveated rendering; Latency,Computer graphics; Eye movements; Virtual reality; Computational costs; Eye-tracking; Foveated rendering; High quality images; Latency; Performance optimizations; Peripheral visual acuities; Visual artifacts; Rendering (computer graphics)
Mimebot-investigating the expressibility of non-verbal communication across agent embodiments,2017,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029893975&doi=10.1145%2f3127590&partnerID=40&md5=306f260be05a49074c24254e10a3dbde,"Unlike their human counterparts, artificial agents such as robots and game characters may be deployed with a large variety of face and body configurations. Some have articulated bodies but lack facial features, and others may be talking heads ending at the neck. Generally, they have many fewer degrees of freedom than humans through which they must express themselves, and there will inevitably be a filtering effect when mapping human motion onto the agent. In this article, we investigate filtering effects on three types of embodiments: (a) an agent with a body but no facial features, (b) an agent with a head only, and (c) an agent with a body and a face. We performed a full performance capture of a mime actor enacting short interactions varying the non-verbal expression along five dimensions (e.g., level of frustration and level of certainty) for each of the three embodiments. We performed a crowd-sourced evaluation experiment comparing the video of the actor to the video of an animated robot for the different embodiments and dimensions. Our findings suggest that the face is especially important to pinpoint emotional reactions but is also most volatile to filtering effects. The body motion, on the other hand, had more diverse interpretations but tended to preserve the interpretation after mapping and thus proved to be more resilient to filtering. © 2017 ACM.",Cross-mapping; Motion capture; Perception,Degrees of freedom (mechanics); Sensory perception; Animated robots; Artificial agents; Emotional reactions; Evaluation experiments; Filtering effects; Motion capture; Non-verbal communications; Performance capture; Mapping
A feature-based quality metric for tone mapped images,2017,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029896440&doi=10.1145%2f3129675&partnerID=40&md5=4716b424e3deadad4f1b89dc46898140,"With the development of high-dynamic-range images and tone mapping operators comes a need for image quality evaluation of tone mapped images. However, because of the significant difference in dynamic range between high-dynamic-range images and tone mapped images, conventional image quality assessment algorithms that predict distortion based on the magnitude of intensity or normalized contrast are not suitable for this task. In this article, we present a feature-based quality metric for tone mapped images that predicts the perceived quality by measuring the distortion in important image features that affect quality judgment. Our metric utilizes multi-exposed virtual photographs taken from the original high-dynamic-range images to bridge the gap between dynamic ranges in image feature analysis. By combining measures for brightness distortion, visual saliency distortion, and detail distortion in light and dark areas, the metric measures the overall perceptual distortion and assigns a score to a tone mapped image. Experiments on a subject-rated database indicate that the proposed metric is more consistent with subjective evaluation results than alternative approaches. © 2017 ACM.",High dynamic range images; Image quality metrics; Tone mapping; Visual perception,Mapping; Quality control; High dynamic range images; Image feature analysis; Image quality assessment; Image quality evaluation; Image quality metrics; Subjective evaluations; Tone mapping; Visual perception; Image quality
Gaze data for the analysis of attention in feature films,2017,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029901079&doi=10.1145%2f3127588&partnerID=40&md5=b3355c24ff48828cb92a0954eae23ad7,"Film directors are masters at controlling what we look at when we watch a film. However, there have been few quantitative studies of how gaze responds to cinematographic conventions thought to influence attention. We have collected and are releasing a dataset designed to help investigate eye movements in response to higher level features such as faces, dialogue, camera movements, image composition, and edits. The dataset, which will be released to the community, includes gaze information for 21 viewers watching 15 clips from live action 2D films, which have been hand annotated for high level features. This work has implications for the media studies, display technology, immersive reality, and human cognition. © 2017 ACM.",Eye tracking; Film studies; Gaze behavior; Gaze direction; Psychophysics,Motion pictures; Eye-tracking; Film study; Gaze behavior; Gaze direction; Psychophysics; Eye movements
Perceived space in the HTC vive,2017,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027041450&doi=10.1145%2f3106155&partnerID=40&md5=5c3fcb72378b78a15184f3bd626a16e0,"Underperception of egocentric distance in virtual reality has been a persistent concern for almost 20 years. Modern headmounted displays (HMDs) appear to have begun to ameliorate underperception. The current study examined several aspects of perceived space in the HTC Vive. Blind-walking distance judgments, verbal distance judgments, and size judgments were measured in two distinct virtual environments (VEs)-a high-quality replica of a real classroom and an empty grass field-as well as the real classroom upon which the classroom VE was modeled. A brief walking interaction was also examined as an intervention for improving anticipated underperception in the VEs. Results from the Vive were compared to existing data using two older HMDs (nVisor SX111 and ST50). Blind-walking judgments were more accurate in the Vive compared to the older displays, and did not differ substantially from the real world nor across VEs. Size judgments were more accurate in the classroom VE than the grass VE and in the Vive compared to the older displays. Verbal judgments were significantly smaller in the classroom VE compared to the real classroom and did not significantly differ across VEs. Blind-walking and size judgments were more accurate after walking interaction, but verbal judgments were unaffected. The results indicate that underperception of distance in the HTC Vive is less than in older displays but has not yet been completely resolved. With more accurate space perception afforded by modern HMDs, alternative methods for improving judgments of perceived space-such as walking interaction-may no longer be necessary. © 2017 ACM.",Depth perception; Stereoscopic displays; Virtual environments,Depth perception; Stereo image processing; Blind walking; Head mounted displays; High quality; Real-world; Space perception; Stereoscopic display; Virtual reality
Perceptual lightness modeling for high-dynamic-range imaging,2017,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027004070&doi=10.1145%2f3086577&partnerID=40&md5=d6afe15dfe4be24343550737e52e2a61,"The human visual system (HVS) non-linearly processes light from the real world, allowing us to perceive detail over a wide range of illumination. Although models that describe this non-linearity are constructed based on psycho-visual experiments, they generally apply to a limited range of illumination and therefore may not fully explain the behavior of theHVS under more extreme illumination conditions. We propose a modified experimental protocol for measuring visual responses to emissive stimuli that do not require participant training, nor requiring the exclusion of non-expert participants. Furthermore, the protocol can be applied to stimuli covering an extended luminance range. Based on the outcome of our experiment, we propose a new model describing lightness response over an extended luminance range. The model can be integrated with existing color appearance models or perceptual color spaces. To demonstrate the effectiveness of our model in high dynamic range applications, we evaluate its suitability for dynamic range expansion relative to existing solutions. © 2017 ACM.",Color appearance modeling; High dynamic range imaging; ITMO; Lightness modeling; Psycho-visual experiment,Luminance; Color appearance models; Dynamic range expansions; Experimental protocols; Extreme illuminations; High dynamic range imaging; ITMO; Perceptual color space; Visual experiments; Color
Remote sighted assistants for indoor location sensing of visually impaired pedestrians,2017,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026514008&doi=10.1145%2f3047408&partnerID=40&md5=2ff26423cc3be510fd2e6e7c41b30c41,"Because indoor navigation is difficult for people with visual impairment, there is a need for the development of assistive technology. Indoor location sensing, the ability to identify a pedestrian’s location and orientation, is a key component of such technology. We tested the accuracy of a potential crowdsourcing-based indoor location sensing method. Normally sighted subjects were asked to identify the location and facing direction of photos taken by a pedestrian in a building. The subjects had available a floor plan and a small number of representative photos from key locations within the floor plan. Subjects were able to provide accurate location estimates (median location accuracy 3.87ft). This finding indicates that normally sighted subjects, with minimal training, using a simple graphical representation of a floor plan, can provide accurate location estimates based on a single, suitable photo taken by a pedestrian. We conclude that indoor localization is possible using remote, crowdsourced, human assistance. This method has the potential to be used for the location-sensing component of an indoor navigation aid for people with visual impairment.",Blind; Crowdsourcing; Indoor navigation; Location sensing; Low vision; Visual impairment; Wayfinding,Crowdsourcing; Floors; Location; Navigation; Ophthalmology; Blind; In-door navigations; Location-sensing; Low vision; Visual impairment; Way-finding; Indoor positioning systems
Computational aesthetic evaluation of logos,2017,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024500800&doi=10.1145%2f3058982&partnerID=40&md5=161a356eee883cade4eb881d91868562,"Computational aesthetics has become an active research field in recent years, but there have been few attempts in computational aesthetic evaluation of logos. In this article, we restrict our study on black-and-white logos, which are professionally designed for name-brand companies with similar properties, and apply perceptual models of standard design principles in computational aesthetic evaluation of logos. We define a group of metrics to evaluate some aspects in design principles such as balance, contrast, and harmony of logos. We also collect human ratings of balance, contrast, harmony, and aesthetics of 60 logos from 60 volunteers. Statistical linear regression models are trained on this database using a supervised machine-learning method. Experimental results show that our model-evaluated balance, contrast, and harmony have highly significant correlation of over 0.87 with human evaluations on the same dimensions. Finally, we regress human-evaluated aesthetics scores on model-evaluated balance, contrast, and harmony. The resulted regression model of aesthetics can predict human judgments on perceived aesthetics with a high correlation of 0.85. Our work provides a machine-learning-based reference framework for quantitative aesthetic evaluation of graphic design patterns and also the research of exploring the relationship between aesthetic perceptions of human and computational evaluation of design principles extracted from graphic designs. © 2017 ACM.",Computational aesthetics; Design principle; Evaluation; Human judgments; Logo designs,Design; Learning systems; Regression analysis; Computational aesthetics; Design Principles; Evaluation; Human judgments; Logo design; Education
Exploring biological motion regularities of human actions: A new perspective on video analysis,2017,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024500022&doi=10.1145%2f3086591&partnerID=40&md5=bd994be15d8fdb8a3cd41357c63db25f,"The ability to detect potentially interacting agents in the surrounding environment is acknowledged to be one of the first perceptual tasks developed by humans, supported by the ability to recognise biological motion. The precocity of this ability suggests that it might be based on rather simple motion properties, and it can be interpreted as an atomic building block of more complex perception tasks typical of interacting scenarios, as the understanding of non-verbal communication cues based on motion or the anticipation of others' action goals. In this article, we propose a novel perspective for video analysis, bridging cognitive science and machine vision, which leverages the use of computational models of the perceptual primitives that are at the basis of biological motion perception in humans. Our work offers different contributions. In a first part, we propose an empirical formulation for the Two-Thirds Power Law, a well-known invariant law of human movement, and thoroughly discuss its readability in experimental settings of increasing complexity. In particular, we consider unconstrained video analysis scenarios, where, to the best of our knowledge, the invariant law has not found application so far. The achievements of this analysis pave the way for the second part of the work, in which we propose and evaluate a general representation scheme for biological motion characterisation to discriminate biological movements with respect to non-biological dynamic events in video sequences. The method is proposed as the first layer of a more complex architecture for behaviour analysis and human-machine interaction, providing in particular a new way to approach the problem of human action understanding. © 2017 ACM.",Biological motion perception; Motion perception development; Two-Thirds Power Law,Vision; Biological motion; Complex architectures; Human machine interaction; Motion perception; Non-verbal communications; Representation schemes; Surrounding environment; Two-thirds power laws; Behavioral research
Affective Calibration of Musical Feature Sets in an Emotionally Intelligent Music Composition System,2017,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019623828&doi=10.1145%2f3059005&partnerID=40&md5=669aad35d417df8000fc9628e0e0ad9b,"Affectively driven algorithmic composition (AAC) is a rapidly growing field that exploits computer-Aided composition in order to generate new music with particular emotional qualities or affective intentions. An AAC system was devised in order to generate a stimulus set covering nine discrete sectors of a two-dimensional emotion space by means of a 16-channel feed-forward artificial neural network. This system was used to generate a stimulus set of short pieces of music, which were rendered using a sampled piano timbre and evaluated by a group of experienced listeners who ascribed a two-dimensional valence-Arousal coordinate to each stimulus. The underlying musical feature set, initially drawn from the literature, was subsequently adjusted by amplifying or attenuating the quantity of each feature in order to maximize the spread of stimuli in the valence-Arousal space before a second listener evaluation was conducted. This process was repeated a third time in order to maximize the spread of valencearousal coordinates ascribed to the generated stimulus set in comparison to a spread taken from an existing prerated database of stimuli, demonstrating that this prototype AAC system is capable of creating short sequences ofmusic with a slight improvement on the range of emotion found in a stimulus set comprised of real-world, traditionally composed musical excerpts. © 2017 ACM.",Algorithmic composition; Emotional congruence; Music perception,Neural networks; Algorithmic compositions; Emotional congruence; Emotional quality; Feed-forward artificial neural networks; Music composition; Music perception; Musical features; Short sequences; Behavioral research
How dimensional and semantic attributes of visual sign influence relative value estimation,2017,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017268001&doi=10.1145%2f3059006&partnerID=40&md5=f2d9150e78cbb5fe5dc8298ebc6b93d9,"High-quality decision making requires accurate estimation of relative values. The perceptual bias when estimating relative values displayed by a visual sign may weaken the accuracy and cause misjudgment. This research explores the heuristic estimation of relative values using visual cues, namely linear, areal, and volumetric information.We conduct experiments to empirically test the influences of dimensional information on perceptual biases. First, we investigate the conspicuity of areal information. Our experiments indicate that the responses of participants instructed to estimate rates defined by either linear or volumetric information are biased by the corresponding rates determined by areal information. Second, visual cues implying three-dimensional information (e.g., depth) can lead to overestimation. Third, we probe the influence of vividness as the boundary condition on relative value estimation. Empirical evidence on perceptual bias sheds light on the pragmatics of visual signs, helps suggest guidelines for visual persuasions, and improves decision-making quality.",Dimensional information; Information visualization; Perceptual bias; Relative value estimation; Visual cue,Information systems; Semantics; Dimensional information; Information visualization; Perceptual bias; Relative value; Visual cue; Decision making
Row-interleaved sampling for depth-enhanced 3d video coding for polarized displays,2017,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017164724&doi=10.1145%2f3047409&partnerID=40&md5=3dc4edb1399c598ad3a6aa9140fa4d32,"Passive stereoscopic displays create the illusion of three dimensions by employing orthogonal polarizing filters and projecting two images onto the same screen. In this article, a coding scheme targeting depth-enhanced stereoscopic video coding for polarized displays is introduced. We propose to use asymmetric row-interleaved sampling for texture and depth views prior to encoding. The performance of the proposed scheme is compared with several other schemes, and the objective results confirm the superior performance of the proposed method. Furthermore, subjective evaluation proves that no quality degradation is introduced by the proposed coding scheme compared to the reference method. © 2017 ACM.",3D video; Compression; Polarized display; Sub-sampling,Codes (symbols); Compaction; Image coding; Stereo image processing; 3-D videos; Interleaved samplings; Polarizing filter; Quality degradation; Stereoscopic display; Stereoscopic video coding; Sub-sampling; Subjective evaluations; Video signal processing
Automatic detection of game engine artifacts using full reference image quality metrics,2017,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016215563&doi=10.1145%2f3047407&partnerID=40&md5=0e86283cd4a91bac368e56f2e83ed0f5,"Contemporary game engines offer an outstanding graphics quality but they are not free from visual artifacts. A typical example is aliasing, which, despite advanced antialiasing techniques, is still visible to the game players. Essential deteriorations are the shadow acne and peter panning responsible for deficiency of the shadow mapping technique. Also Z-fighting, caused by the incorrect order of drawing polygons, significantly affects the quality of the graphics and makes the gameplay more difficult. In this work, we propose a technique, in which visibility of deteriorations is uncovered by the objective image quality metrics (IQMs). We test the efficiency of a simple mathematically based metric and advanced IQMs: a Spatial extension of CIELAB (S-CIELAB), the Structural SIMilarity Index (SSIM), the Multiscale Structural SIMilarity Index (MS-SSIM), and the High Dynamic Range Visual Difference Predictor-2 (HDR-VDP-2). Additionally, we evaluate the Color Image Difference (CID) metric, which is recommended to detect the differences in colors. To find out which metric is the most effective for the detection of the game engine artifacts, we build a database of manually marked images with representative set of artifacts. We conduct subjective experiments in which people manually mark the visible local artifacts in the screenshots from the games. Then the detection maps averaged over a number of observers are compared with results generated by IQMs. The obtained results show that SSIM and MS-SSIM metrics outperform other techniques. However, the results are not indisputable, because, for small and scattered aliasing artifacts, HDR-VDP-2 metrics report the results most consistent with the average human observer. As a proof of concept, we propose an application in which resolution of the shadow maps is controlled by the SSIM metric to avoid perceptually visible aliasing artifacts on the shadow edges. © 2017 ACM.",Aliasing; Game engine artifacts; Image quality metrics; Perceptual experiments; Peter panning; Shadow acne; Shadow mapping artifacts; Z-fighting,Anti-aliasing; Deterioration; Drawing (graphics); Mapping; Maps; Aliasing; Game Engine; Image quality metrics; Peter panning; Shadow acne; Shadow mappings; Z-fighting; Image quality
Fabric appearance control system for example-based interactive texture and color design,2017,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017144149&doi=10.1145%2f3054953&partnerID=40&md5=430bfe4a99b7d7c7f514cd58df6077a2,"Texture and color are important factors of fabric appearance. A system that could intuitively manipulate and design fabric texture and color would be a very powerful tool. This article presents an interactive fabric appearance design system that modulates the texture patterns of input fabric example images and transfers the color patterns from other input images onto them. For this purpose, we propose a method to synthesize a natural texture image based on our findings from subjective experiments: (1) intensity and its deviation of two input images are significantly related to the realistic appearance of synthesized textures and (2) the spatial-frequency and edge intensity of two different input images significantly influence the natural appearance of synthesized texture perception. In our procedure, first, the texture pattern of an input fabric image is modulated in terms of undulation, thickness, and roughness. Next, we transfer the color pattern of an original color image onto the modulated texture pattern in the YIQ color space. To perform this color transfer, we use the IQ component of the color image. To reduce the unnatural appearance of the output color-transfer image, we remove the high-frequency components of the original color image. In addition, the Y component of the color-transfer image is obtained by adding the deviation of the texture pattern Y component to the texture pattern of the color image. These algorithms for reducing unnaturalness and synthesizing images were developed based on our findings from several subjective experiments on natural appearance. Finally, we implemented our algorithm on a smart device. Our system allows us to interactively design the texture and color of fabric by using images. © 2017 ACM.",Appearance control; Color transfer; Interactive system; Texture extraction,Color; Color printing; Human computer interaction; Image processing; Image texture; Color transfers; High frequency components; Interactive system; Natural appearance; Spatial frequency; Subjective experiments; Synthesized texture; Texture extraction; Color image processing
Walking in virtual reality: Effects of manipulated visual self-motion on walking biomechanics,2017,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011339327&doi=10.1145%2f3022731&partnerID=40&md5=cc9c0964733400368c98d69fe039c560,"Walking constitutes the predominant form of self-propelled movement from one geographic location to another in our real world. Likewise, walking in virtual environments (VEs) is an essential part of a users experience in many application domains requiring a high degree of interactivity. However, researchers and practitioners often observe that basic implementations of virtual walking, in which head-tracked movements are mapped isometrically to a VE are not estimated as entirely natural. Instead, users estimate a virtual walking velocity as more natural when it is slightly increased compared to the users physical body movement. In this article, we investigate the effects of such nonisometric mappings between physical movements and virtual motions in the VE on walking velocity and biomechanics of the gait cycle. Therefore, we performed an experiment in which we measured and analyzed parameters of the biomechanics of walking under conditions with isometric as well as nonisometric mappings. Our results show significant differences in most gait parameters when walking in the VE in the isometric mapping condition compared to the corresponding parameters in the real world. For nonisometric mappings we found an increased divergence of gait parameters depending on the velocity of visual self-motion feedback. The results revealed a symmetrical effect of gait detriments for up-or down-scaled virtual velocities, which we discuss in the scope of the previous findings. © 2017 ACM.",Biomechanics; Gait; Real walking; Translation gains; Virtual environments,Biofeedback; Biophysics; Mapping; Velocity; Virtual reality; Gait; Geographic location; Isometric mapping; Measured and analyzed parameters; Physical movements; Real walking; Visual self-motion; Walking velocity; Biomechanics
Toward a perceptually uniform parameter space for filter transparency,2017,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011343782&doi=10.1145%2f3022732&partnerID=40&md5=3f6f6fb704c0cbd0890ae6b71d4b3e43,"Filter models of perceptual transparency relate to regularities in the retinal projections caused by light transmitting objects like clear liquids or glass and have been found to predict the color conditions for perceptual transparency more accurately than alternative models. An important but unsolved problem is how exactly the model parameters are related to the properties of the perceived transparent layer. We previously proposed a parametrization in terms of hue, saturation, overall transmittance and clarity of the filter that seems to capture important dimensions of the phenomenal impressions. However, these parameters are not independent and the corresponding scales are not perceptually uniform. Here, an invertible transformation of this parameter space is proposed that strongly mitigates these problems. This results in a more intuitively interpretable parameter set that seems well suited for the analysis of existing stimuli and the generation of transparent overlays with predefined perceptual properties. The latter property makes it suitable for graphics and visualization applications. © 2017 ACM.",Color perception; Transparency perception; Transparency picker; Visualization,Bandpass filters; Color vision; Flow visualization; Visualization; Color perception; Perceptual properties; Perceptual transparency; Transparency perception; Transparent layers; Transparent overlay; Unsolved problems; Visualization application; Transparency
Subjective and objective visual quality assessment of textured 3D meshes,2016,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994481765&doi=10.1145%2f2996296&partnerID=40&md5=e00b39a165e032658348c2ccd2f089b2,"Objective visual quality assessment of 3D models is a fundamental issue in computer graphics. Quality assessment metrics may allow a wide range of processes to be guided and evaluated, such as level of detail creation, compression, filtering, and so on. Most computer graphics assets are composed of geometric surfaces on which several texture images can be mapped to make the rendering more realistic. While some quality assessment metrics exist for geometric surfaces, almost no research has been conducted on the evaluation of texture-mapped 3D models. In this context, we present a new subjective study to evaluate the perceptual quality of textured meshes, based on a paired comparison protocol. We introduce both texture and geometry distortions on a set of 5 reference models to produce a database of 136 distorted models, evaluated using two rendering protocols. Based on analysis of the results, we propose two new metrics for visual quality assessment of textured mesh, as optimized linear combinations of accurate geometry and texture quality measurements. These proposed perceptual metrics outperform their counterparts in terms of correlation with human opinion. The database, along with the associated subjective scores, will be made publicly available online. © 2016 ACM.",Subjective study; Textured mesh; Visual quality assessment,Mesh generation; Rendering (computer graphics); Three dimensional computer graphics; Geometry distortion; Linear combinations; Perceptual metrics; Perceptual quality; Quality assessment; Subjective study; Textured mesh; Visual quality assessment; Quality control
"Seeing, listening, drawing: Interferences between sensorimotor modalities in the use of a tablet musical interface",2016,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994523261&doi=10.1145%2f2990501&partnerID=40&md5=e2f620f66e955d763395ba232a79b366,"Audio, visual, and proprioceptive actions are involved when manipulating a graphic tablet musical interface. Previous works suggested a possible dominance of the visual over the auditory modality in this situation. The main goal of the present study is to examine the interferences between these modalities in visual, audio, and audio-visual target acquisition tasks. Experiments are based on a movement replication paradigm, where a subject controls a cursor on a screen or the pitch of a synthesized sound by changing the stylus position on a covered graphic tablet. The experiments consisted of the following tasks: (1) a target acquisition task that was aimed at a visual target (reaching a cue with the cursor displayed on a screen), an audio target (reaching a reference note by changing the pitch of the sound played in headsets), or an audio-visual target, and (2) the replication of the target acquisition movement in the opposite direction. In the return phase, visual and audio feedback were suppressed. Different gain factors perturbed the relationships among the stylus movements, visual cursor movements, and audio pitch movements. The deviations between acquisition and return movements were analyzed. The results showed that hand amplitudes varied in accordance with visual, audio, and audio-visual perturbed gains, showing a larger effect for the visual modality. This indicates that visual, audio, and audio-visual actions interfered with the motor modality and confirms the spatial representation of pitch reported in previous studies. In the audio-visual situation, vision dominated over audition, as the latter had no significant influence on motor movement. Consequently, visual feedback is helpful for musical targeting of pitch on a graphic tablet, at least during the learning phase of the instrument. This result is linked to the underlying spatial organization of pitch perception. Finally, this work brings a complementary approach to previous studies showing that audition may dominate over vision for other aspects of musical sound (e.g., timing, rhythm, and timbre).",Graphic tablet; Sensorimotor modalities; Target acquisition,Audition; Feedback; Mergers and acquisitions; Visual communication; Auditory modality; Graphic tablet; Sensorimotor modalities; Spatial organization; Spatial representations; Synthesized sounds; Target acquisition; Visual modalities; Audio acoustics
Vertical field-of-view extension and walking characteristics in head-worn virtual environments,2016,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994479364&doi=10.1145%2f2983631&partnerID=40&md5=1147cb0e035c5f9baba72808fc1edb02,"In this article, we detail a series of experiments that examines the effect of vertical field-of-view extension and the addition of non-specific peripheral visual stimulation on gait characteristics and distance judgments in a head-worn virtual environment. Specifically, we examined four field-of-view configurations: a common 60° diagonal field of view (48° × 40°), a 60° diagonal field of view with the addition of a luminous white frame in the far periphery, a field of view with an extended upper edge, and a field of view with an extended lower edge. We found that extension of the field of view, either with spatially congruent or spatially non-informative visuals, resulted in improved distance judgments and changes in observed posture. However, these effects were not equal across all field-of-view configurations, suggesting that some configurations may be more appropriate than others when balancing performance, cost, and ergonomics. © 2016 ACM.",Blind walking; Distance perception; Field of view; Gait; Head-mounted displays; Head-worn displays,Depth perception; Ergonomics; Helmet mounted displays; Blind walking; Distance perception; Field of views; Gait; Head mounted displays; Head-worn displays; Virtual reality
Summary statistics and material categorization in the visual periphery,2016,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84989213618&doi=10.1145%2f2967498&partnerID=40&md5=504d3c0b1765617e0031d2a73cb1c3e1,"Material categorization from natural texture images proceeds quickly and accurately, supporting a number of visual and motor behaviors. In real-world settings, mechanisms for material categorization must function effectively based on the input from foveal vision, where image representation is high fidelity, and the input from peripheral vision, which is comparatively impoverished. What features support successful material categorization in the visual periphery, given the known reductions in acuity, contrast sensitivity, and other lossy transforms that reduce the fidelity of image representations? In general, the visual features that support material categorization remain largely unknown, but recent work suggests that observers' abilities in a number of tasks that depend on peripheral vision can be accounted for by assuming that the visual system has access to only summary statistics (texture-like descriptors) of image structure. We therefore hypothesized that a model of peripheral vision based on the Portilla-Simoncelli texture synthesis algorithm might account for material categorization abilities in the visual periphery. Using natural texture images and synthetic images made from these stimuli, we compared performance across material categories to determine whether observer performance with natural inputs could be predicted by their performance with synthetic images that reflect the constraints of a texture code. © 2016 ACM.",Material categorization; Natural images; Peripheral vision; Texture synthesis,Computer graphics; Vision; Categorization ability; Contrast sensitivity; Image representations; Natural images; Observer performance; Peripheral vision; Real world setting; Texture synthesis; Image texture
Image quality under chromatic impairments,2016,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84984693650&doi=10.1145%2f2964908&partnerID=40&md5=2f94ff87f09c721aa07abd252363d268,"The influence of chromatic impairments on the perceived image quality is studied in this article. Under the D65 standard illuminant, a set of hyperspectral images were represented into the CIELAB color space, and the corresponding chromatic coordinates were subdivided into clusters with the k-means algorithm. Each color cluster was shifted by a predefined chromatic impairment ΔEab∗ with random direction in a∗b∗ chromatic coordinates only. Applying impairments of 3, 6, 9, 12, and 15 in a∗b∗ coordinates to five hyperspectral images a set of modified images was generated. Those images were shown to subjects that were asked to rank their quality based on their naturalness. The Mean Opinion Score of the subjective evaluations was computed to quantify the sensitivity to the chromatic variations. This article is also complemented with an objective evaluation of the quality using several state-of-the-art metrics and using the CIEDE2000 color difference among others. Analyzing the correlations between subjective and objective quality evaluation helps us to conclude that the proposed quality estimators based on the CIEDE2000 provide the best representation. Moreover, it was concluded that the established quality metrics only become reliable by averaging their results on each color component. © 2016 ACM.",Image quality; Mean opinion score; Quality metrics; Quality of experience,Color; Colorimetry; Image enhancement; Image quality; Quality of service; Sensitivity analysis; Spectroscopy; Chromatic coordinate; CIEDE2000 color difference; Hyper-spectral images; Mean opinion scores; Objective evaluation; Quality metrics; Quality of experience (QoE); Subjective evaluations; Quality control
Detection of subconscious face recognition using consumer-grade brain-computer interfaces,2016,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84984807282&doi=10.1145%2f2955097&partnerID=40&md5=2910960007356d47a0a596131a68545e,"We test the possibility of tapping the subconscious mind for face recognition using consumer-grade BCIs. To this end, we performed an experiment whereby subjects were presented with photographs of famous persons with the expectation that about 20% of them would be (consciously) recognized; and since the photos are of famous persons, we expected that subjects would have seen before some of the 80% they didn't (consciously) recognize. Further, we expected that their subconscious would have recognized some of those in the 80% pool that they had seen before. An exit questionnaire and a set of criteria allowed us to label recognitions as conscious, false, no recognitions, or subconscious recognitions. We analyzed a number of event related potentials training and testing a support vector machine. We found that our method is capable of differentiating between no recognitions and subconscious recognitions with promising accuracy levels, suggesting that tapping the subconscious mind for face recognition is feasible. © 2016 ACM.",Brain-computer interfaces; Face recognition; Subconscious mind,Brain computer interface; Interfaces (computer); Accuracy level; Event related potentials; Label recognition; Subconscious mind; Training and testing; Face recognition
Effects of visual latency on vehicle driving behavior,2016,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84984918776&doi=10.1145%2f2971320&partnerID=40&md5=5f78ca44d3768e3f4733ea397b8a250f,"Using mixed reality in vehicles provides a potential alternative to using driving simulators when studying driver-vehicle interaction. However, virtual reality systems introduce latency in the visual system that may alter driving behavior, which, in turn, results in questionable validity. Previous studies have mainly focused on visual latency as a separate phenomenon. In this work, latency is studied from a task-dependent viewpoint to investigate how participants' driving behavior changed with increased latency. In this study, the investigation was performed through experiments in which regular drivers were subjected to different levels of visual latency while performing a simple slalom driving task. The drivers' performances were recorded and evaluated in both lateral and longitudinal directions along with self-assessment questionnaires regarding task performance and difficulty. All participants managed to complete the driving tasks successfully, even under high latency conditions, but were clearly affected by the increased visual latency. The results suggest that drivers compensate for longer latencies by steering more and increasing the safety margins but without reducing their speed. © 2016 ACM.",Head-mounted display; Latency; Vehicle driving; Video see-through,Behavioral research; Helmet mounted displays; Surveys; Virtual reality; Driving behavior; Driving simulator; Head mounted displays; Latency; Longitudinal direction; Vehicle interactions; Video see-through; Virtual reality system; Vehicles
An empirical evaluation of visuo-haptic feedback on physical reaching behaviors during 3D interaction in real and immersive virtual environments,2016,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041465038&doi=10.1145%2f2947617&partnerID=40&md5=24f38a48602fddee1ee86595a90c3325,"In an initial study, we characterized the properties of human reach motion in the presence or absence of visuo-haptic feedback in real and Immersive Virtual Environments (IVEs) or virtual reality within a participant’s maximum arm reach. Our goal is to understand how physical reaching actions to the perceived location of targets in the presence or absence of visuo-haptic feedback are different between real and virtual viewing conditions. Typically, participants reach to the perceived location of objects in the three-dimensional (3D) environment to perform selection and manipulation actions during 3D interaction in applications such as virtual assembly or rehabilitation. In these tasks, participants typically have distorted perceptual information in the IVE as compared to the real world, in part due to technological limitations such as minimal visual field of view, resolution, latency, and jitter. In an empirical evaluation, we asked the following questions: (i) how do the perceptual differences between virtual and real world affect our ability to accurately reach to the locations of 3D objects, and (ii) how do the motor responses of participants differ between the presence or absence of visual and haptic feedback? We examined factors such as velocity and distance of physical reaching behavior between the real world and IVE, both in the presence or absence of visuo-haptic information. The results suggest that physical reach responses vary systematically between real and virtual environments, especially in situations involving the presence or absence of visuo-haptic feedback. The implications of our study provide a methodological framework for the analysis of reaching motions for selection and manipulation with novel 3D interaction metaphors and to successfully characterize visuo-haptic versus non-visuo-haptic physical reaches in virtual and real-world situations. © 2016 ACM",And Phrases: Virtual worlds; Immersive virtual environments; Motor control; Multisensory perception; Visuo-haptic feedback,Feedback; Location; Immersive virtual environments; Motor control; Multisensory perceptions; Virtual worlds; Visuo-haptic; Virtual reality
Top-down influences in the detection of spatial displacement in a musical scene,2016,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979584047&doi=10.1145%2f2911985&partnerID=40&md5=9d395308924284395e9c91681e329139,"We investigated the detection of sound displacement in a four-voice musical piece under conditions that manipulated the attentional setting (selective or divided attention), the sound source numerosity, the spatial dispersion of the voices, and the tonal complexity of the piece. Detection was easiest when each voice was played in isolation and performance deteriorated when source numerosity increased and uncertainty with respect to the voice in which displacement would occur was introduced. Restricting the area occupied by the voices improved performance in agreement with the auditory spotlight hypothesis as did reducing the tonal complexity of the piece. Performance under increased numerosity conditions depended on the voice in which displacement occurred. The results highlight the importance of top-down processes in the context of the detection of spatial displacement in a musical scene. © 2016 ACM.",3d audio; Auditory perception; Spatial attention,Psychology computing; Sensory perception; 3D audio; Auditory perception; Divided attention; Musical pieces; Spatial attention; Spatial dispersion; Spatial displacement; Top-down process; Speech recognition
Introduction to special issue SAP 2016,2016,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979581695&doi=10.1145%2f2954927&partnerID=40&md5=b783707e6e13b927d22a25f85264e2fb,[No abstract available],,
Is the motion of a child perceivably different from the motion of an adult?,2016,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019966230&doi=10.1145%2f2947616&partnerID=40&md5=3ee041a48ea513aa31b0d0d7080e2e3a,"Artists and animators have observed that children’s movements are quite different from adults performing the same action. Previous computer graphics research on human motion has primarily focused on adult motion. There are open questions as to how different child motion actually is, and whether the differences will actually impact animation and interaction. We report the first explicit study of the perception of child motion (ages 5 to 9 years old), compared to analogous adult motion. We used markerless motion capture to collect an exploratory corpus of child and adult motion, and conducted a perceptual study with point light displays to discover whether naive viewers could identify a motion as belonging to a child or an adult. We find that people are generally successful at this task. This work has implications for creating more engaging and realistic avatars for games, online social media, and animated videos and movies. © 2016 ACM",Biological motion; Child motion; Markerless motion capture; Perception of motion; Point light displays,Computer graphics; Social networking (online); Biological motion; Child motion; Human motions; Light display; Markerless motion capture; Online social medias; Animation
Simulating visual contrast reduction during nighttime glare situations on conventional displays,2016,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979554482&doi=10.1145%2f2934684&partnerID=40&md5=b4bbe9af2b103217df74f9c117b4053b,"Bright glare in nighttime situations strongly decreases human contrast perception. Nighttime simulations therefore require a way to realistically depict contrast perception of the user. Due to the limited luminance of popular as well as specialized high-dynamic range displays, physical adaptation of the human eye cannot yet be replicated in a physically correct manner in a simulation environment. To overcome this limitation, we propose a method to emulate the adaptation in nighttime glare situations using a perception-based model. We implemented a postprocessing tone mapping algorithm that simulates the corresponding contrast reduction effect for a night-driving simulation with glares from oncoming vehicles headlights. During glare, tone mapping reduces image contrast in accordance with the incident veiling luminance. As the glare expires, the contrast starts to normalize smoothly over time. The conversion of glare parameters and elapsed time into image contrast during the readaptation phase is based on extensive user studies carried out first in a controlled laboratory setup. Additional user studies have then been conducted in field tests to ensure validity of the derived time-dependent tone-mapping function and to verify transferability onto real-world traffic scenarios. © 2016 ACM.",Driving simulation; Eye adaptation; Glare simulation; Psycho-physical experiments,Conformal mapping; Luminance; Mapping; Controlled laboratories; Driving simulation; Eye adaptation; High dynamic range; Human contrast perceptions; Physical experiments; Reduction effects; Simulation environment; Glare
FrankenFolk: Distinctiveness and attractiveness of voice and motion,2016,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979498074&doi=10.1145%2f2948066&partnerID=40&md5=e697bb8efd505549ba6e6362531c2957,"It is common practice in movies and games to use different actors for the voice and body/face motion of a virtual character. What effect does the combination of these different modalities have on the perception of the viewer? In this article, we conduct a series of experiments to evaluate the distinctiveness and attractiveness of human motions (face and body) and voices. We also create combination characters called FrankenFolks, where we mix and match the voice, body motion, face motion, and avatar of different actors and ask which modality is most dominant when determining distinctiveness and attractiveness or whether the effects are cumulative. © 2016 ACM.",Crowd variety; Multisensory perception; Virtual characters,Sensory perception; Body motions; Crowd variety; Human motions; Mix and match; Multisensory perceptions; Virtual character; Psychology computing
Reproducing reality: Multimodal contributions in natural scene discrimination,2016,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979574611&doi=10.1145%2f2915917&partnerID=40&md5=9527f9074238a97e1e6b7d0a309b8f33,"Most research on multisensory processing focuses on impoverished stimuli and simple tasks. In consequence, very little is known about the sensory contributions in the perception of real environments. Here, we presented 23 participants with paired comparison tasks, where natural scenes were discriminated in three perceptually meaningful attributes: movement, openness, and noisiness. The goal was to assess the auditory and visual modality contributions in scene discrimination with short (<500ms) natural scene exposures. The scenes were reproduced in an immersive audiovisual environment with 3D sound and surrounding visuals. Movement and openness were found to be mainly visual attributes with some input from auditory information. In some scenes, the auditory system was able to derive information about movement and openness that was comparable with audiovisual condition already after 500ms stimulation. Noisiness was mainly auditory, but visual information was found to have a facilitatory role in a few scenes. The sensory weights were highly imbalanced in favor of the stronger modality, but the weaker modality was able to affect the bimodal estimate in some scenes. © 2016 ACM.",Audiovisual; Immersive environment; Natural scenes; Sensory integration; Spatial sound,Psychology computing; Audiovisual; Immersive environment; Natural scenes; Sensory integration; Spatial sound; Sensory perception
A proposed methodology for evaluating HDR false color maps,2016,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979530358&doi=10.1145%2f2911986&partnerID=40&md5=e3b2890422937180c9951ece616fe696,"Color mapping, which involves assigning colors to the individual elements of an underlying data distribution, is a commonly used method for data visualization. Although color maps are used in many disciplines and for a variety of tasks, in this study we focus on its usage for visualizing luminance maps. Specifically, we ask ourselves the question of how to best visualize a luminance distribution encoded in a high-dynamic-range (HDR) image using false colors such that the resulting visualization is the most descriptive. To this end, we first propose a definition for descriptiveness. We then propose a methodology to evaluate it subjectively. Then, we propose an objective metric that correlates well with the subjective evaluation results. Using this metric, we evaluate several false coloring strategies using a large number of HDR images. Finally, we conduct a second psychophysical experiment using images representing a diverse set of scenes. Our results indicate that the luminance compression method has a significant effect and the commonly used logarithmic compression is inferior to histogram equalization. Furthermore, we find that the default color scale of the Radiance global illumination software consistently performs well when combined with histogram equalization. On the other hand, the commonly used rainbow color scale was found to be inferior. We believe that the proposed methodology is suitable for evaluating future color mapping strategies as well. © 2016 ACM.",False color; HDR imaging; Visualization,Data visualization; Equalizers; Flow visualization; Graphic methods; Luminance; Mapping; Visualization; False color; HDR imaging; High dynamic range images; Histogram equalizations; Logarithmic compression; Luminance distributions; Psychophysical experiments; Subjective evaluations; Color
Psychoacoustic characterization of propagation effects in virtual environments,2016,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017653435&doi=10.1145%2f2947508&partnerID=40&md5=e0c37477feb0b3cb65f2817839b1f075,"As sound propagation algorithms become faster and more accurate, the question arises as to whether the additional efforts to improve fidelity actually offer perceptual benefits over existing techniques. Could environmental sound effects go the way of music, where lower-fidelity compressed versions are actually favored by listeners? Here we address this issue with two acoustic phenomena that are known to have perceptual effects on humans and that, accordingly, might be expected to heighten their experience with simulated environments. We present two studies comparing listeners’ perceptual response to both accurate and approximate algorithms simulating two key acoustic effects: diffraction and reverberation. For each effect, we evaluate whether increased numerical accuracy of a propagation algorithm translates into increased perceptual differentiation in interactive virtual environments. Our results suggest that auditory perception does benefit from the increased accuracy, with subjects showing better perceptual differentiation when experiencing the more accurate rendering method: the diffraction experiment shows a more linearly decaying sound field (with respect to the diffraction angle) for the accurate diffraction method, whereas the reverberation experiment shows that more accurate reverberation, after modest user experience, results in near-logarithmic response to increasing room volume. © 2016 ACM",Auditory perception; Virtual environments/reality,Acoustic fields; Acoustic wave propagation; Audition; Diffraction; Reverberation; Virtual reality; Approximate algorithms; Auditory perception; Diffraction methods; Environmental sounds; Interactive virtual environments; Logarithmic response; Propagation algorithm; Simulated environment; Architectural acoustics
Using audio cues to support motion gesture interaction on mobile devices,2016,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84974623062&doi=10.1145%2f2897516&partnerID=40&md5=82a6c2bc5ef8aca4faec76e28b2e5bb5,"Motion gestures are an underutilized input modality for mobile interaction despite numerous potential advantages. Negulescu et al. found that the lack of feedback on attempted motion gestures made it difficult for participants to diagnose and correct errors, resulting in poor recognition performance and user frustration. In this article, we describe and evaluate a training and feedback technique, Glissando, which uses audio characteristics to provide feedback on the system's interpretation of user input. This technique enables feedback by verbally confirming correct gestures and notifying users of errors in addition to providing continuous feedback by manipulating the pitch of distinct musical notes mapped to each of three dimensional axes in order to provide both spatial and temporal information. © 2016 ACM.",Audio feedback; Mobile interaction; Motion gestures,Psychology computing; Sensory perception; Audio feedbacks; Feedback techniques; Input modalities; Mobile interaction; Motion gestures; Musical notes; Support motion; Temporal information; Human computer interaction
Individual differences in image-quality estimations: Estimation rules and viewing strategies,2016,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84974578350&doi=10.1145%2f2890504&partnerID=40&md5=f6501b13fb08b2ec5d51028b0c1fd00d,"Subjective image-quality estimation with high-quality images is often a preference-estimation task. Preferences are subjective, and individual differences exist. Individual differences are also seen in the eye movements of people. A task's subjectivity can result from people using different rules as a basis for their estimation. Using two studies, we investigated whether different preference-estimation rules are related to individual differences in viewing behaviour by examining the process of preference estimation of high-quality images. The estimation rules were measured from free subjective reports on important quality-related attributes (Study 1) and from estimations of the attributes' importance in preference estimation (Study 2). The free reports showed that the observers used both feature-based image-quality attributes (e.g., sharpness, illumination) and abstract attributes, which include an interpretation of the image features (e.g., atmosphere and naturalness). In addition, the observers were classified into three viewing-strategy groups differing in fixation durations in both studies. These groups also used different estimation rules. In both studies, the group with medium-length fixations differed in their estimation rules from the other groups. In Study 1, the observers in this group used more abstract attributes than those in the other groups; in Study 2, they considered atmosphere to be a more important image feature. The study shows that individual differences in a quality-estimation task are related to both estimation rules and viewing strategies, and that the difference is related to the level of abstraction of the estimations. © 2016 ACM.",Eye movements; Image-quality attributes; Individual differences; Subjective image-quality estimation,Image quality; Fixation duration; High quality images; Image quality estimation; Individual Differences; Level of abstraction; Quality attributes; Quality estimation; Subjective image quality; Eye movements
Reconstructing user's attention on the web through mouse movements and perception-based content identification,2016,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84974605256&doi=10.1145%2f2912124&partnerID=40&md5=33d8ed9d0c3dbd1225114730ce515b38,"Eye tracking is one of the most exploited techniques in literature for finding usability problems in web-based user interfaces (UIs). However, it is usually employed in a laboratory setting, considering that an eye-tracker is not commonly used in web browsing. In contrast, web application providers usually exploit remote techniques for large-scale user studies (e.g. A/B testing), tracking low-level interactions such as mouse clicks and movements. In this article, we discuss a method for predicting whether the user is looking at the content pointed by the cursor, exploiting the mouse movement data and a segmentation of the contents in a web page. We propose an automatic method for segmenting content groups inside a web page that, applying both image and code analysis techniques, identifies the user-perceived group of contents with a mean pixel-based error around the 20%. In addition, we show through a user study that such segmentation information enhances the precision and the accuracy in predicting the correlation between between the user's gaze and the mouse position at the content level, without relaying on user-specific features. © 2016 ACM.",Content segmentation; Layout analysis; Machine learning; Mouse-eye correlation; User's attention,Learning systems; Mammals; User interfaces; Websites; Automatic method; Content identifications; Content segmentation; Layout analysis; Perception-based; Segmentation informations; Usability problems; User's attention; Eye tracking
Distinguishing between abstract art by artists vs. Children and animals: Comparison between human and machine perception,2016,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84974569647&doi=10.1145%2f2912125&partnerID=40&md5=5cede99f92d00a4e7298ea3337b52cb1,"expressionism is a school of art characterized by nonrepresentational paintings where color, composition, and brush strokes are used to express emotion. These works are often misunderstood by the public who see them as requiring no skill and as images that even a child could have created. However, a recent series of studies has shown that ordinary adults untrained in art or art history, as well as young children, can differentiate paintings by abstract expressionists and superficially similar works by preschool children and even animals (monkeys, apes, elephants). Adults perform this distinction with an accuracy rate of ∼64%, significantly higher than chance. Here we ask whether machine perception can do as well. Using the same paintings, we show that in ∼68% of the cases the computer algorithm can discriminate between abstract paintings and the work of children and animals. We also applied a method that computes the correlation between the degree of artisticity deduced from human perception of the paintings and the visual content of the images, and we show significant correlation between perceived artisticity and visual content. The image content descriptor that was the strongest predictor of correct identification was the fractality of the painting. We also show that the computer algorithm predicts the perceived intentionality of the paintings by humans. These results confirm perceptible differences between works by abstract expressionists and superficially similar ones by the untrained and show that people see more than they think they see when looking at abstract expressionism. © 2016 ACM.",Abstract expressionism; Art,Algorithms; Animals; Behavioral research; Education; Abstract paintings; Express emotions; Human perception; Intentionality; Machine perception; Preschool children; Visual content; Young children; Painting
3D blur discrimination,2016,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84970016623&doi=10.1145%2f2896453&partnerID=40&md5=125a0d48ad3f9483e01b9b9beec971ff,"Blur is an important attribute in the study and modeling of the human visual system. In the blur discrimination experiments, just-noticeable additional blur required to differentiate from the reference blur level is measured. The past studies on blur discrimination have measured the sensitivity of the human visual system to blur using two-dimensional (2D) test patterns. In this study, subjective tests are performed to measure blur discrimination thresholds using stereoscopic 3D test patterns. Specifically, how the binocular disparity affects the blur sensitivity is measured on a passive stereoscopic display. A passive stereoscopic display renders the left and right eye images in a row interleaved format. The subjects have to wear circularly polarized glasses to filter the appropriate images to the left and right eyes. Positive, negative, and zero disparity values are considered in these experiments. A positive disparity value projects the objects behind the display screen, a negative disparity value projects the objects in front of the display screen, and a zero disparity value projects the objects at the display plane. The blur discrimination thresholds are measured for both symmetric and asymmetric stereo viewing cases. In the symmetric viewing case, the same level of additional blur is applied to the left and right eye stimulus. In the asymmetric viewing case, different levels of additional blur are applied to the left and right eye stimuli. The results of this study indicate that, in the symmetric stereo viewing case, binocular disparity does not affect the blur discrimination thresholds for the selected 3D test patterns. As a consequence of these findings, we conclude that the models developed for 2D blur discrimination can be used for 3D blur discrimination. We also show that the Weber model provides a good fit to the blur discrimination threshold measurements for the symmetric stereo viewing case. In the asymmetric viewing case, the blur discrimination thresholds decreased, and the decrease in threshold values is found to be dominated by eye observing the higher blur. © 2016 ACM.",3D stereo; Asymmetric stereo; Blur discrimination; Human visual system; Weber model,Binoculars; Display devices; 3d stereos; Asymmetric stereo; Binocular disparity; Blur discrimination; Circularly polarized; Human Visual System; Stereoscopic display; Two Dimensional (2 D); Stereo image processing
Color picking: The initial 20s,2016,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84969944559&doi=10.1145%2f2883613&partnerID=40&md5=7edc29b81872172445cd621d48f67396,"Color pickers are widely used in all kinds of display applications. They vary greatly in their utility, depending on user expertise. We focus on nonprofessional, occasional users. Such users may spend from a few seconds up to a few minutes to select a color. Yet, typically they reach final accuracy within the initial 20s. Additional effort leads to random walks in the neighborhood of the target. We explore the efficaciousness of five generic color pickers, analyzing the results in terms of generic user interface properties. There is a major dichotomy between three-slider interfaces, and those that offer some form of 2D selectivity. The accuracy in RGB coordinates is about one-tenth to one-twentieth of the full scale (often 0-255 in R, G, and B), whereas a little over 100 hues are resolved. The most efficient color picker, which is presently rarely used in popular applications, is much more efficient than the worst one. We speculate that this derives from a closer match to the user's internal representation of color space. The study results in explicit recommendations for the implementation of user-friendly and efficient color tools. © 2016 ACM.",Color pickers,User interfaces; Color space; Display application; Ge-neric user interfaces; Internal representation; Random Walk; Slider interface; User friendly; Color
Discerning ambient/focal attention with coefficient K,2016,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84969916933&doi=10.1145%2f2896452&partnerID=40&md5=bd78ded43249b5192afc0dadd1ad1b5d,"We introduce coefficient K, defined on a novel parametric scale, derived from processing a traditionally eye-tracked time course of eye movements. Positive and negative ordinates of K indicate focal or ambient viewing, respectively, while the abscissa serves to indicate time, so that K acts as a dynamic indicator of fluctuation between ambient/focal visual behavior. The coefficient indicates the difference between fixation duration and its subsequent saccade amplitude expressed in standard deviation units, facilitating parametric statistical testing. To validate K empirically, we test its utility by capturing ambient and focal attention during serial and parallel visual search tasks (Study 1). We then show how K quantitatively depicts the difference in scanning behaviors when attention is guided by audio description during perception of art (Study 2). © 2016 ACM.",Ambient-focal attention; Serial versus parallel search; Visual attention dynamics,Behavioral research; Ambient-focal attention; Audio description; Dynamic indicators; Fixation duration; Parallel search; Standard deviation; Statistical testing; Visual Attention; Eye movements
Assessing the impact of hand motion on virtual character personality,2016,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964545816&doi=10.1145%2f2874357&partnerID=40&md5=56ffc1b67c01ba6be5a796500c3d4ed0,"Designing virtual characters that are capable of conveying a sense of personality is important for generating realistic experiences, and thus a key goal in computer animation research. Though the influence of gesture and body motion on personality perception has been studied, little is known about which attributes of hand pose and motion convey particular personality traits. Using the ""Big Five"" model as a framework for evaluating personality traits, this work examines how variations in hand pose and motion impact the perception of a character's personality. As has been done with facial motion, we first study hand motion in isolation as a requirement for running controlled experiments that avoid the combinatorial explosion of multimodal communication (all combinations of facial expressions, arm movements, body movements, and hands) and allow us to understand the communicative content of hands. We determined a set of features likely to reflect personality, based on research in psychology and previous human motion perception work: shape, direction, amplitude, speed, and manipulation. Then we captured realistic hand motion varying these attributes and conducted three perceptual experiments to determine the contribution of these attributes to the character's personalities. Both hand poses and the amplitude of hand motion affected the perception of all five personality traits. Speed impacted all traits except openness. Direction impacted extraversion and openness. Manipulation was perceived as an indicator of introversion, disagreeableness, neuroticism, and less openness to experience. From these results, we generalize guidelines for designing detailed hand motion that can add to the expressiveness and personality of characters. We performed an evaluation study that combined hand motion with gesture and body motion. Even in the presence of body motion, hand motion still significantly impacted the perception of a character's personality and could even be the dominant factor in certain situations. © 2016 ACM.",Conversational and non-verbal behavior; Evaluation; Hand motion; Personality,Psychology computing; Sensory perception; Combinatorial explosion; Controlled experiment; Conversational and non-verbal behavior; Evaluation; Facial Expressions; Hand motion; Multimodal communications; Personality; Animation
Deformation lamps: A projection technique to make static objects perceptually dynamic,2016,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964791020&doi=10.1145%2f2874358&partnerID=40&md5=48cdb4b266db1dd4de0b9275fa2bd053,"Light projection is a powerful technique that can be used to edit the appearance of objects in the real world. Based on pixel-wise modification of light transport, previous techniques have successfully modified static surface properties such as surface color, dynamic range, gloss, and shading. Here, we propose an alternative light projection technique that adds a variety of illusory yet realistic distortions to a wide range of static 2D and 3D projection targets. The key idea of our technique, referred to as (Deformation Lamps), is to project only dynamic luminance information, which effectively activates the motion (and shape) processing in the visual system while preserving the color and texture of the original object. Although the projected dynamic luminance information is spatially inconsistent with the color and texture of the target object, the observer's brain automatically combines these sensory signals in such a way as to correct the inconsistency across visual attributes. We conducted a psychophysical experiment to investigate the characteristics of the inconsistency correction and found that the correction was critically dependent on the retinal magnitude of the inconsistency. Another experiment showed that the perceived magnitude of image deformation produced by our techniques was underestimated. The results ruled out the possibility that the effect obtained by our technique stemmed simply from the physical change in an object's appearance by light projection. Finally, we discuss how our techniques can make the observers perceive a vivid and natural movement, deformation, or oscillation of a variety of static objects, including drawn pictures, printed photographs, sculptures with 3D shading, and objects with natural textures including human bodies. © 2016 ACM.",Illusion; Light projection; Motion perception; Motion-pattern interaction,Color; Lighting; Luminance; Color and textures; Illusion; Light projection; Motion pattern; Motion perception; Projected dynamics; Projection techniques; Psychophysical experiments; Deformation
Evaluating animated characters: Facial motion magnitude influences personality perceptions,2016,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964691206&doi=10.1145%2f2851499&partnerID=40&md5=2e0040d28874d8d2a4c49f5552cf5012,"Animated characters are expected to fulfill a variety of social roles across different domains. To be successful and effective, these characters must display a wide range of personalities. Designers and animators create characters with appropriate personalities by using their intuition and artistic expertise. Our goal is to provide evidence-based principles for creating social characters. In this article, we describe the results of two experiments that show how exaggerated and damped facial motion magnitude influence impressions of cartoon and more realistic animated characters. In our first experiment, participants watched animated characters that varied in rendering style and facial motion magnitude. The participants then rated the different animated characters on extroversion, warmth, and competence, which are social traits that are relevant for characters used in entertainment, therapy, and education.We found that facial motion magnitude affected these social traits in cartoon and realistic characters differently. Facial motion magnitude affected ratings of cartoon characters' extroversion and competence more than their warmth. In contrast, facial motion magnitude affected ratings of realistic characters' extroversion but not their competence nor warmth. We ran a second experiment to extend the results of the first. In the second experiment, we added emotional valence as a variable. We also asked participants to rate the characters on more specific aspects of warmth, such as respectfulness, calmness, and attentiveness. Although the characters' emotional valence did not affect ratings, we found that facial motion magnitude influenced ratings of the characters' respectfulness and calmness but not attentiveness. These findings provide a basis for how animators can fine-tune facial motion to control perceptions of animated characters' personalities.",Animation; Facial motion; Personality perception; Rendering style,Psychology computing; Sensory perception; Animated characters; Cartoon characters; Different domains; Emotional valences; Evidence-based; Facial motions; Rendering style; Social character; Animation
Assessing and improving the identification of computer-generated portraits,2016,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84966397489&doi=10.1145%2f2871714&partnerID=40&md5=68cba019783795678e86d84e890340d0,"Modern computer graphics are capable of generating highly photorealistic images. Although this can be considered a success for the computer graphics community, it has given rise to complex forensic and legal issues. A compelling example comes from the need to distinguish between computer-generated and photographic images as it pertains to the legality and prosecution of child pornography in the United States. We performed psychophysical experiments to determine the accuracy with which observers are capable of distinguishing computer-generated from photographic images. We find that observers have considerable difficulty performing this task-more difficulty than we observed 5 years ago when computer-generated imagery was not as photorealistic. We also find that observers are more likely to report that an image is photographic rather than computer generated, and that resolution has surprisingly little effect on performance. Finally, we find that a small amount of training greatly improves accuracy. © 2016 ACM.",Computer graphics; Photo forensics; Photorealistic,Computer graphics; Imaging systems; Laws and legislation; Photography; Child pornographies; Computer generated; Computer generated imagery; Photo forensics; Photo-realistic; Photographic image; Photorealistic images; Psychophysical experiments; Computer crime
Biometric recognition via eye movements: Saccadic vigor and acceleration cues,2016,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84957075245&doi=10.1145%2f2842614&partnerID=40&md5=839a8d2915ae0a09248c72dd71909f28,"Previous research shows that human eye movements can serve as a valuable source of information about the structural elements of the oculomotor system and they also can open a window to the neural functions and cognitive mechanisms related to visual attention and perception. The research field of eye movement-driven biometrics explores the extraction of individual-specific characteristics from eye movements and their employment for recognition purposes. In this work, we present a study for the incorporation of dynamic saccadic features into a model of eye movement-driven biometrics. We show that when these features are added to our previous biometric framework and tested on a large database of 322 subjects, the biometric accuracy presents a relative improvement in the range of 31.6-33.5% for the verification scenario, and in range of 22.3-53.1% for the identification scenario. More importantly, this improvement is demonstrated for different types of visual stimulus (random dot, text, video), indicating the enhanced robustness offered by the incorporation of saccadic vigor and acceleration cues. © 2016 ACM.",Eye movement biometrics; Saccadic acceleration; Saccadic vigor,Behavioral research; Biometrics; Biometric recognition; Cognitive mechanisms; Neural functions; Oculomotor systems; Saccadic vigor; Structural elements; Visual Attention; Visual stimulus; Eye movements
Adaptation to Simulated Hypergravity in a Virtual Reality Throwing Task,2024,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189678787&doi=10.1145%2f3643849&partnerID=40&md5=0b5826732cac4c5bd00f774b977d182c,"According to previous research, humans are generally poor at adapting to earth-discrepant gravity, especially in Virtual Reality (VR), which cannot simulate the effects of gravity on the physical body. Most of the previous VR research on gravity adaptation has used perceptual or interception tasks, although adaptation to these tasks seems to be especially challenging compared to tasks with a more pronounced motor component. This article describes the results of two between-subjects studies (n = 60 and n = 42) that investigated adaptation to increased gravity simulated by an interactive VR experience. The experimental procedure was identical in both studies: In the adaptation phase, one group was trained to throw a ball at a target using Valve Index motion controllers in gravity that was simulated at five times of earth’s gravity (hypergravity group), whereas another group threw at a longer-distance target under normal gravity (normal gravity group) so both groups had to exert the same amount of force when throwing (approximated manually in Study 1 and mathematically in Study 2). Then, in the measurement phase, both groups repeatedly threw a virtual ball at targets in normal gravity. In this phase, the trajectory of the ball was hidden at the moment of release so that the participants had to rely on their internal model of gravity to hit the targets rather than on visual feedback. Target distances were placed within the same range for both groups in the measurement phase. According to our preregistered hypotheses, we predicted that the hypergravity group would display worse overall throwing accuracy and would specifically overshoot the target more often than the normal gravity group. Our experimental data supported both hypotheses in both studies. The findings indicate that training an interactive task in higher simulated gravity led participants in both studies to update their internal gravity models, and therefore, some adaptation to higher gravity did indeed occur. However, our exploratory analysis also indicates that the participants in the hypergravity group began to gradually regain their throwing accuracy throughout the course of the measurement phase. © 2024 Copyright held by the owner/author(s).",gravity models; sensory adaptation; virtual reality,Gravitation; Interactive computer graphics; Visual communication; Experimental procedure; Gravity modeling; Hypergravity; Interactive virtual reality; Internal models; Motion controller; Motor components; Normal gravities; Sensory adaptation; Virtual reality experiences; Virtual reality
Estimates of Temporal Edge Detection Filters in Human Vision,2024,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189678144&doi=10.1145%2f3639052&partnerID=40&md5=091e709c012ca63e7601a6fc8e808882,"Edge detection is an important process in human visual processing. However, as far as we know, few attempts have been made to map the temporal edge detection filters in human vision. To that end, we devised a user study and collected data from which we derived estimates of human temporal edge detection filters based on three different models, including the derivative of the infinite symmetric exponential function and temporal contrast sensitivity function. We analyze our findings using several different methods, including extending the filter to higher frequencies than were shown during the experiment. In addition, we show a proof of concept that our filter may be used in spatiotemporal image quality metrics by incorporating it into a flicker detection pipeline. © 2024 Copyright held by the owner/author(s).",Edge detection; human perception; temporal; user study,Edge detection; Edge detection filters; Filter-based; Human perception; Human vision; Human visual processing; Sensitivity functions; Symmetrics; Temporal; Temporal contrast sensitivity; User study; Exponential functions
Design and Validation of a Virtual Reality Mental Rotation Test,2024,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189707241&doi=10.1145%2f3626238&partnerID=40&md5=2f1bf7bdc2efdf76239101dd10c2127b,"Mental rotation, a common measure of spatial ability, has traditionally been assessed through paper-based instruments like the Mental Rotation Test (MRT) or the Purdue Spatial Visualization Test: Rotations (PSVT:R). The fact that these instruments present 3D shapes in a 2D format devoid of natural cues like shading and perspective likely limits their ability to accurately assess the fundamental skill of mentally rotating 3D shapes. In this paper, we describe the Virtual Reality Mental Rotation Assessment (VRMRA), a virtual reality-based mental rotation assessment derived from the Revised PSVT:R and MRT. The VRMRA reimagines traditional mental rotation assessments in a room-scale virtual environment and uses hand-tracking and elements of gamification in attempts to create an intuitive, engaging experience for test-takers. To validate the instrument, we compared response patterns in the VRMRA with patterns observed on the MRT and Revised PSVT:R. For the PSVT:R-type questions, items requiring a rotation around two axes were significantly harder than items requiring rotations around a single axis in the VRMRA, which is not the case in the Revised PSVT:R. For the MRT-type questions in the VRMRA, a moderate negative correlation was found between the degree of rotation in the X direction and item difficulty. While the problem of occlusion was reduced, features of the shapes and distractors accounted for 50.6% of the variance in item difficulty. Results suggest that the VRMRA is likely a more accurate tool to assess mental rotation ability in comparison to traditional instruments which present the stimuli through 2D media. Our findings also point to potential problems with the fundamental designs of the Revised PSVT:R and MRT question formats. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",mental rotation; MRT; PSVT:R; Spatial ability; virtual reality; visual perception,Interactive computer graphics; Rotation; 3-D shape; Item difficulties; Mental rotation; Mental rotation test; Purdue spatial visualization test: rotation; Rotations tests; Spatial abilities; Spatial visualization; Visual perception; Virtual reality
Two-finger Stiffness Discrimination with the Stochastic Resonance Effect,2024,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189322024&doi=10.1145%2f3630254&partnerID=40&md5=43fe78612c0f258a8f9e283c98ebfb72,"We investigated the ability of two fingers to discriminate stiffness with stochastic resonance. It is known that the haptic perception at the fingertip improves when vibrotactile noise propagates to the fingertip, which is a phenomenon called the stochastic resonance. The improvement in the haptic sensation of a fingertip depends on the intensity of the noise propagating to the fingertip. An improvement in the haptic sensation of multiple fingertips does not require multiple noise sources, such as vibrators, to be attached to multiple fingertips; i.e., even a single vibrator can propagate noise to multiple fingers. In this study, we focus on stiffness discrimination as a task using multiple fingers, in which the thumb and index finger are used to touch an object and perceive its stiffness. Subsequently, we demonstrate that the stiffness perception is improved by propagating sufficiently intense noise to the thumb and index finger using only a single vibrator. The findings indicate the possibility of improving the haptic sensation at multiple fingertips using one vibrator. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Haptic perception; stiffness discrimination; stochastic resonance; two-finger task,Circuit resonance; Magnetic resonance; Stiffness; Stochastic systems; Haptic perception; Haptic sensation; Index finger; Multiple noise sources; Resonance effect; Single-vibrator; Stiffness discrimination; Stochastic resonances; Two-finger task; Vibrotactile; Vibrators
The Haptic Intensity Order Illusion Is Caused by Amplitude Changes,2023,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182589142&doi=10.1145%2f3626237&partnerID=40&md5=ebae7379b3cef12dc6ffe40c57c864ec,"When two brief vibrotactile stimulations are sequentially applied to observers’ lower back, there is systematic mislocalization of the stimulation: if the second stimulation is of higher intensity than the first one, observers tend to respond that the second stimulation was above the first one, and vice versa when weak intensity stimulation follows a strong one. This haptic mislocalization effect has been called the intensity order illusion. In the original demonstration of the illusion, frequency and amplitude of the stimulation were inextricably linked so that changes in amplitude also resulted in changes in frequency. It is therefore unknown whether the illusion is caused by changes in frequency, amplitude or both. To test this, we performed a multifactorial experiment, where we used L5 actuators that allow independent manipulation of frequency and amplitude. This approach enabled us to investigate the effects of stimulus amplitude, frequency and location, and to assess any potential interactions among these factors. We report four main findings: (1) we were able to replicate the intensity order illusion with the L5 tactors; (2) the illusion mainly occurred in the upwards direction, or in other words, when strong stimulation following a weaker one occurred above or in the same location as the first stimulation; (3) the illusion did not occur when similar stimulation patterns were applied in the horizontal direction; and (4) the illusion was solely due to changes in amplitude, whereas changes in frequency (100 Hz vs 200 Hz) had no effect. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Intensity order illusion; tactile attention; tactile illusion,Amplitude changes; Haptics; High intensity; Intensity order illusion; Low back; Mislocalization; Tactile attention; Tactile illusion; Vibrotactile; Weak intensity
The Effect of Interocular Contrast Differences on the Appearance of Augmented Reality Imagery,2023,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182597987&doi=10.1145%2f3617684&partnerID=40&md5=0fefb616c61b6593422daf39a816da47,"Augmented reality (AR) devices seek to create compelling visual experiences that merge virtual imagery with the natural world. These devices often rely on wearable near-eye display systems that can optically overlay digital images to the left and right eyes of the user separately. Ideally, the two eyes should be shown images with minimal radiometric differences (e.g., the same overall luminance, contrast, and color in both eyes), but achieving this binocular equality can be challenging in wearable systems with stringent demands on weight and size. Basic vision research has shown that a spectrum of potentially detrimental perceptual effects can be elicited by imagery with radiometric differences between the eyes, but it is not clear whether and how these findings apply to the experience of modern AR devices. In this work, we first develop a testing paradigm for assessing multiple aspects of visual appearance at once, and characterize five key perceptual factors when participants viewed stimuli with interocular contrast differences. In a second experiment, we simulate optical see-through AR imagery using conventional desktop LCD monitors and use the same paradigm to evaluate the multi-faceted perceptual implications when the AR display luminance differs between the two eyes. We also include simulations of monocular AR systems (i.e., systems in which only one eye sees the displayed image). Our results suggest that interocular contrast differences can drive several potentially detrimental perceptual effects in binocular AR systems, such as binocular luster, rivalry, and spurious depth differences. In addition, monocular AR displays tend to have more artifacts than binocular displays with a large contrast difference in the two eyes. A better understanding of the range and likelihood of these perceptual phenomena can help inform design choices that support high-quality user experiences in AR. © 2023 Copyright held by the owner/author(s).","Stereoscopic displays, binocular vision","Binoculars; Digital devices; Liquid crystal displays; Luminance; Radiometry; Stereo image processing; Augmented reality systems; Digital image; Eye-display systems; Natural world; Perceptual effects; Radiometrics; Stereoscopic display; Stereoscopic display, binocular vision; Virtual imagery; Visual experiences; Augmented reality"
The Influence of the Other-Race Effect on Susceptibility to Face Morphing Attacks,2023,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182603053&doi=10.1145%2f3618113&partnerID=40&md5=af520dcda7b94c2a269499f21e13861d,"Facial morphs created between two identities resemble both of the faces used to create the morph. Consequently, humans and machines are prone to mistake morphs made from two identities for either of the faces used to create the morph. This vulnerability has been exploited in “morph attacks” in security scenarios. Here, we asked whether the “other-race effect” (ORE)—the human advantage for identifying own- vs. other-race faces—exacerbates morph attack susceptibility for humans. We also asked whether face-identification performance in a deep convolutional neural network (DCNN) is affected by the race of morphed faces. Caucasian (CA) and East-Asian (EA) participants performed a face-identity matching task on pairs of CA and EA face images in two conditions. In the morph condition, different-identity pairs consisted of an image of identity “A” and a 50/50 morph between images of identity “A” and “B”. In the baseline condition, morphs of different identities never appeared. As expected, morphs were identified mistakenly more often than original face images. Of primary interest, morph identification was substantially worse for cross-race faces than for own-race faces. Similar to humans, the DCNN performed more accurately for original face images than for morphed image pairs. Notably, the deep network proved substantially more accurate than humans in both cases. The results point to the possibility that DCNNs might be useful for improving face identification accuracy when morphed faces are presented. They also indicate the significance of the race of a face in morph attack susceptibility in applied settings. © 2023 Copyright held by the owner/author(s).",deep convolutional neural network; face identification; face matching; Face morphing; other-race effect,Convolution; Convolutional neural networks; Deep neural networks; Caucasians; Condition; Convolutional neural network; Deep convolutional neural network; Face identification; Face images; Face matching; Face Morphing; Other-race effects; Performance; Face recognition
Exploring the Relative Effects of Body Position and Locomotion Method on Presence and Cybersickness when Navigating a Virtual Environment,2023,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182601283&doi=10.1145%2f3627706&partnerID=40&md5=2e1935e8c156c263a327b706cb0709b2,"The primary goals of this research are to strengthen the understanding of the mechanisms underlying presence and cybersickness in relation to the body position and locomotion method when navigating a virtual environment (VE). In this regard, we compared two body positions (standing and sitting) and four locomotion methods (steering + embodied control [EC], steering + instrumental control [IC], teleportation + EC, and teleportation + IC) to examine the association between body position, locomotion method, presence, and cybersickness in VR. The results of a two-way ANOVA revealed a main effect of locomotion method on presence, with the sense of presence significantly lower for the steering + IC condition. However, there was no main effect of body position on presence, nor was there an interaction between body position and locomotion method. For cybersickness, nonparametric tests were used due to non-normality. The results of Mann-Whitney U tests indicated a statistically significant effect of body position on cybersickness. In particular, the level of cybersickness was significantly higher for a standing position than for a sitting position. In addition, the results of Kruskal-Wallis tests revealed that the locomotion method had a meaningful effect on cybersickness, with participants in the steering conditions feeling stronger symptoms of cybersickness than those in the teleportation conditions. Overall, this study confirmed the relationship between body position, locomotion method, presence, and cybersickness when navigating a VE. © 2023 Association for Computing Machinery. All rights reserved.",body position; cybersickness; locomotion method; presence; Virtual reality,Integrated circuits; Body positions; Condition; Cybersickness; Locomotion method; Main effect; Nonparametric tests; Presence; Relative effects; Sense of presences; Two-way ANOVA; Virtual reality
Improving the Perception of Mid-air Tactile Shapes with Spatio-temporally-modulated Tactile Pointers,2023,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179128200&doi=10.1145%2f3611388&partnerID=40&md5=cd92857d6d8ba442804c152fd9b1ff1a,"Ultrasound mid-air haptic (UMH) devices can remotely render vibrotactile shapes on the skin of unequipped users, e.g., to draw haptic icons or render virtual object shapes. Spatio-temporal modulation (STM), the state-of-the-art UMH shape-rendering method, provides large freedom in shape design and produces the strongest possible stimuli for this technology. Yet, STM shapes are often reported to be blurry, complicating shape identification. Dynamic tactile pointers (DTP) were recently introduced as a technique to overcome this issue. By tracing a contour with an amplitude-modulated focal point, they significantly improve shape identification accuracy over STM, but at the cost of much lower stimulus intensity. Building upon this, we propose spatio-temporally-modulated Tactile Pointers (STP), a novel method for rendering clearer and sharper UMH shapes while at the same time producing strong vibrotactile sensations. We ran two human participant experiments, which show that STP shapes are perceived as significantly stronger than DTP shapes, while shape identification accuracy is significantly improved over STM and on par with that obtained with DTP. Our work has implications for effective shape rendering with UMH and provides insights that could inform future psychophysical investigation into vibrotactile shape perception in UMH. © 2023 held by the owner/author(s). Publication rights licensed to ACM.",haptic perception; mid-air haptics; Ultrasound haptics,Haptic perception; Haptics; Mid-air haptic; Shape identifications; Shape rendering; Spatio-temporal; Temporal modulations; Ultrasound haptic; Vibrotactile; Ultrasonic applications
Calibrated Passability Perception in Virtual Reality Transfers to Augmented Reality,2023,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179137490&doi=10.1145%2f3613450&partnerID=40&md5=f35feb1576de68cb1d308974270aa533,"As applications for virtual reality (VR) and augmented reality (AR) technology increase, it will be important to understand how users perceive their action capabilities in virtual environments. Feedback about actions may help to calibrate perception for action opportunities (affordances) so that action judgments in VR and AR mirror actors' real abilities. Previous work indicates that walking through a virtual doorway while wielding an object can calibrate the perception of one's passability through feedback from collisions. In the current study, we aimed to replicate this calibration through feedback using a different paradigm in VR while also testing whether this calibration transfers to AR. Participants held a pole at 45°and made passability judgments in AR (pretest phase). Then, they made passability judgments in VR and received feedback on those judgments by walking through a virtual doorway while holding the pole (calibration phase). Participants then returned to AR to make posttest passability judgments. Results indicate that feedback calibrated participants' judgments in VR. Moreover, this calibration transferred to the AR environment. In other words, after experiencing feedback in VR, passability judgments in VR and in AR became closer to an actor's actual ability, which could make training applications in these technologies more effective. © 2023 held by the owner/author(s). Publication rights licensed to ACM.",Affordances; augmented reality; calibration; perception; virtual reality,Augmented reality; Doors; Poles; Virtual reality; 'current; Affordances; Augmented reality technology; Calibration transfer; Passability; Training applications; Walking through; Calibration
Changes in Navigation over Time: A Comparison of Teleportation and Joystick-Based Locomotion,2023,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179136930&doi=10.1145%2f3613902&partnerID=40&md5=22e7ca6dce95d91dc4c3c2379bf9b553,"Little research has studied how people use Virtual Reality (VR) changes as they experience VR. This article reports the results of an experiment investigating how users' behavior with two locomotion methods changed over 4 weeks: teleportation and joystick-based locomotion. Twenty novice VR users (with no more than 1 hour prior experience with any form of walking in VR) were recruited. They loaned an Oculus Quest for 4 weeks on their own time, including an activity we provided them with. Results showed that the time required to complete the navigation task decreased faster for joystick-based locomotion. Spatial memory improved with time, particularly when using teleportation (which starts disadvantaged to joystick-based locomotion). In addition, overall cybersickness decreased slightly over time; however, two dimensions of cybersickness (nausea and disorientation) increased notably over time using joystick-based navigation.  © 2023 held by the owner/author(s).",locomotion; long-term use; Virtual reality,Navigation; Cybersickness; Locomotion; Locomotion method; Long-term use; Navigation tasks; Prior experience; Spatial memory; Two-dimensions; User behaviors; Virtual reality
On Human-like Biases in Convolutional Neural Networks for the Perception of Slant from Texture,2023,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179130813&doi=10.1145%2f3613451&partnerID=40&md5=96d07d4e6436d59ab521812ac0a98607,"Depth estimation is fundamental to 3D perception, and humans are known to have biased estimates of depth. This study investigates whether convolutional neural networks (CNNs) can be biased when predicting the sign of curvature and depth of surfaces of textured surfaces under different viewing conditions (field of view) and surface parameters (slant and texture irregularity). This hypothesis is drawn from the idea that texture gradients described by local neighborhoods - a cue identified in human vision literature - are also representable within convolutional neural networks. To this end, we trained both unsupervised and supervised CNN models on the renderings of slanted surfaces with random Polka dot patterns and analyzed their internal latent representations. The results show that the unsupervised models have similar prediction biases as humans across all experiments, while supervised CNN models do not exhibit similar biases. The latent spaces of the unsupervised models can be linearly separated into axes representing field of view and optical slant. For supervised models, this ability varies substantially with model architecture and the kind of supervision (continuous slant vs. sign of slant). Even though this study says nothing of any shared mechanism, these findings suggest that unsupervised CNN models can share similar predictions to the human visual system. Code: github.com/brownvc/Slant-CNN-Biases.  © 2023 held by the owner/author(s). Publication rights licensed to ACM.",convolutional neural networks; deep learning; Perception; slant; texture,Convolution; Convolutional neural networks; Deep neural networks; Forecasting; Interactive computer graphics; Neural network models; 3D perception; Biased estimates; Convolutional neural network; Deep learning; Depth Estimation; Field of views; Human like; Neural network model; Slant; Textured surface; Textures
"Participatory Design of Virtual Humans for Mental Health Support Among North American Computer Science Students: Voice, Appearance, and the Similarity-attraction Effect",2023,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174846729&doi=10.1145%2f3613961&partnerID=40&md5=90336dfad25680447be69cd6efe36acf,"Virtual humans (VHs) have the potential to support mental wellness among college computer science (CS) students. However, designing effective VHs for counseling purposes requires a clear understanding of students' demographics, backgrounds, and expectations. To this end, we conducted two user studies with 216 CS students from a major university in North America. In the first study, we explored how students co-designed VHs to support mental wellness conversations and found that the VHs' demographics, appearance, and voice closely resembled the characteristics of their designers. In the second study, we investigated how the interplay between the VH's appearance and voice impacted the agent's effectiveness in promoting CS students' intentions toward gratitude journaling. Our findings suggest that the active participation of CS students in VH design leads to the creation of agents that closely resemble their designers. Moreover, we found that the interplay between the VH's appearance and voice impacts the agent's effectiveness in promoting CS students' intentions toward mental wellness techniques.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesEmbodied conversational agents; mental health; participatory design; similarity-attraction; virtual humans,Population statistics; Virtual reality; Additional key word and phrasesembodied conversational agent; Computer science students; Conversational agents; Key words; Mental health; North American; Participatory design; Similarity-attraction; Student's voices; Virtual humans; Students
Twin Identification over Viewpoint Change: A Deep Convolutional Neural Network Surpasses Humans,2023,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174841544&doi=10.1145%2f3609224&partnerID=40&md5=7936a510dbf929f4e0a04619336395a0,"Deep convolutional neural networks (DCNNs) have achieved human-level accuracy in face identification (Phillips et al., 2018), though it is unclear how accurately they discriminate highly-similar faces. Here, humans and a DCNN performed a challenging face-identity matching task that included identical twins. Participants (N = 87) viewed pairs of face images of three types: same-identity, general imposters (different identities from similar demographic groups), and twin imposters (identical twin siblings). The task was to determine whether the pairs showed the same person or different people. Identity comparisons were tested in three viewpoint-disparity conditions: frontal to frontal, frontal to 45° profile, and frontal to 90° profile. Accuracy for discriminating matched-identity pairs from twin-imposter pairs and general-imposter pairs was assessed in each viewpoint-disparity condition. Humans were more accurate for general-imposter pairs than twin-imposter pairs, and accuracy declined with increased viewpoint disparity between the images in a pair. A DCNN trained for face identification (Ranjan et al., 2018) was tested on the same image pairs presented to humans. Machine performance mirrored the pattern of human accuracy, but with performance at or above all humans in all but one condition. Human and machine similarity scores were compared across all image-pair types. This item-level analysis showed that human and machine similarity ratings correlated significantly in six of nine image-pair types [range r = 0.38 to r = 0.63], suggesting general accord between the perception of face similarity by humans and the DCNN. These findings also contribute to our understanding of DCNN performance for discriminating high-resemblance faces, demonstrate that the DCNN performs at a level at or above humans, and suggest a degree of parity between the features used by humans and the DCNN.  © 2023 Copyright held by the owner/author(s).",Additional Key Words and PhrasesFace recognition; deep convolutional neural network; human face recognition; human-machine comparison,Convolutional neural networks; Deep neural networks; Face recognition; Additional key word and phrasesface recognition; Condition; Convolutional neural network; Deep convolutional neural network; Face identification; Human face recognition; Human-machine; Human-machine comparison; Image pairs; Key words; Convolution
Effect of Subthreshold Electrotactile Stimulation on the Perception of Electrovibration,2023,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174826507&doi=10.1145%2f3599970&partnerID=40&md5=c71f7d08049b9c0be5e0b1fa0f745be0,"Electrovibration is used in touch enabled devices to render different textures. Tactile sub-modal stimuli can enhance texture perception when presented along with electrovibration stimuli. Perception of texture depends on the threshold of electrovibration. In the current study, we have conducted a psychophysical experiment on 13 participants to investigate the effect of introducing a subthreshold electrotactile stimulus (SES) to the perception of electrovibration. Interaction of tactile sub-modal stimuli causes masking of a stimulus in the presence of another stimulus. This study explored the occurrence of tactile masking of electrovibration by electrotactile stimulus. The results indicate the reduction of electrovibration threshold by 12.46% and 6.75% when the electrotactile stimulus was at 90% and 80% of its perception threshold, respectively. This method was tested over a wide range of frequencies from 20 Hz to 320 Hz in the tuning curve, and the variation in percentage reduction with frequency is reported. Another experiment was conducted to measure the perception of combined stimuli on the Likert scale. The results showed that the perception was more inclined towards the electrovibration at 80% of SES and was indifferent at 90% of SES. The reduction in the threshold of electrovibration reveals that the effect of tactile masking by electrotactile stimulus was not prevalent under subthreshold conditions. This study provides significant insights into developing a texture rendering algorithm based on tactile sub-modal stimuli in the future.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",electrotactile displays; electrovibration; Subthreshold; texture rendering,% reductions; 'current; Electro-tactile displays; Electrotactile; Electrotactile stimulation; Electrovibration; Psychophysical experiments; Subthreshold; Texture perception; Texture rendering; Textures
Salient-Centeredness and Saliency Size in Computational Aesthetics,2023,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162743310&doi=10.1145%2f3588317&partnerID=40&md5=2b74462637a9cab197e3d15ae2b008d4,"We investigate the optimal aesthetic location and size of a single dominant salient region in a photographic image. Existing algorithms for photographic composition do not take full account of the spatial positioning or sizes of these salient regions. We present a set of experiments to assess aesthetic preferences, inspired by theories of centeredness, principal lines, and Rule-of-Thirds. Our experimental results show a clear preference for the salient region to be centered in the image and that there is a preferred size of non-salient border around this salient region. We thus propose a novel image cropping mechanism for images containing a single salient region to achieve the best aesthetic balance. Our results show that the Rule-of-Thirds guideline is not generally valid but also allow us to hypothesize in which situations it is useful and in which it is inappropriate. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Computational aesthetics; image aesthetics; image saliency; photographic composition; Rule-of-Thirds; salient-centeredness; visual balance,Computation theory; Computational aesthetics; Image Aesthetics; Image saliencies; Photographic composition; Photographic image; Rule-of-third; Salient regions; Salient-centeredness; Spatial positioning; Visual balance; Photography
Learning GAN-Based Foveated Reconstruction to Recover Perceptually Important Image Features,2023,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162027271&doi=10.1145%2f3583072&partnerID=40&md5=e61f7eef2898649ebaba73359545c961,"A foveated image can be entirely reconstructed from a sparse set of samples distributed according to the retinal sensitivity of the human visual system, which rapidly decreases with increasing eccentricity. The use of generative adversarial networks (GANs) has recently been shown to be a promising solution for such a task, as they can successfully hallucinate missing image information. As in the case of other supervised learning approaches, the definition of the loss function and the training strategy heavily influence the quality of the output. In this work,we consider the problem of efficiently guiding the training of foveated reconstruction techniques such that they are more aware of the capabilities and limitations of the human visual system, and thus can reconstruct visually important image features. Our primary goal is to make the training procedure less sensitive to distortions that humans cannot detect and focus on penalizing perceptually important artifacts. Given the nature of GAN-based solutions, we focus on the sensitivity of human vision to hallucination in case of input samples with different densities. We propose psychophysical experiments, a dataset, and a procedure for training foveated image reconstruction. The proposed strategy renders the generator network flexible by penalizing only perceptually important deviations in the output. As a result, the method emphasized the recovery of perceptually important image features. We evaluated our strategy and compared it with alternative solutions by using a newly trained objective metric, a recent foveated video quality metric, and user experiments. Our evaluations revealed significant improvements in the perceived image reconstruction quality compared with the standard GAN-based training approach.  © 2023 Copyright held by the owner/author(s).",Additional Key Words and PhrasesFoveated rendering; generative adversarial network; image reconstruction; perception,Image enhancement; Image reconstruction; Quality control; Rendering (computer graphics); Additional key word and phrasesfoveated rendering; Foveated images; Human Visual System; Image features; Image information; Images reconstruction; Key words; Network-based; Sparse set; Supervised learning approaches; Generative adversarial networks
Identifying Lines and Interpreting Vertical Jumps in Eye Tracking Studies of Reading Text and Code,2023,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162056023&doi=10.1145%2f3579357&partnerID=40&md5=7398ebe6384777f359c894feb925194e,"Eye tracking studies have shown that reading code, in contradistinction to reading text, includes many vertical jumps. As different lines of code may have quite different functions (e.g., variable definition, flow control, or computation), it is important to accurately identify the lines being read. We design experiments that require a specific line of text to be scrutinized. Using the distribution of gazes around this line, we then calculate how the precision with which we can identify the line being read depends on the font size and spacing. The results indicate that, even after correcting for systematic bias, unnaturally large fonts and spacing may be required for reliable line identification.Interestingly, during the experiments, the participants also repeatedly re-checked their task and if they were looking at the correct line, leading to vertical jumps similar to those observed when reading code. This suggests that observed reading patterns may be ""inefficient,""in the sense that participants feel the need to repeat actions beyond the minimal number apparently required for the task. This may have implications regarding the interpretation of reading patterns. In particular, reading does not reflect only the extraction of information from the text or code. Rather, reading patterns may also reflect other types of activities, such as getting a general orientation, and searching for specific locations in the context of performing a particular task.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesEye tracking; behavior model; reading order,Codes (symbols); Eye movements; Additional key word and phraseseye tracking; Behaviour models; Design experiments; Eye-tracking studies; Flow computation; Font size; Key words; Line of codes; Reading order; Reading patterns; Eye tracking
Introduction to the SAP 2023 Special Issue,2023,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179745841&doi=10.1145%2f3629977&partnerID=40&md5=ccfd52a7c35541e63b96b2b677566ef2,[No abstract available],,
Practical Saccade Prediction for Head-Mounted Displays: Towards a Comprehensive Model,2023,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146419243&doi=10.1145%2f3568311&partnerID=40&md5=d12f185451e5d5688abae4dc290f682d,"Eye-tracking technology has started to become an integral component of new display devices such as virtual and augmented reality headsets. Applications of gaze information range from new interaction techniques that exploit eye patterns to gaze-contingent digital content creation. However, system latency is still a significant issue in many of these applications because it breaks the synchronization between the current and measured gaze positions. Consequently, it may lead to unwanted visual artifacts and degradation of the user experience. In this work, we focus on foveated rendering applications where the quality of an image is reduced towards the periphery for computational savings. In foveated rendering, the presence of system latency leads to delayed updates to the rendered frame, making the quality degradation visible to the user. To address this issue and to combat system latency, recent work proposes using saccade landing position prediction to extrapolate gaze information from delayed eye tracking samples. Although the benefits of such a strategy have already been demonstrated, the solutions range from simple and efficient ones, which make several assumptions about the saccadic eye movements, to more complex and costly ones, which use machine learning techniques. However, it is unclear to what extent the prediction can benefit from accounting for additional factors and how more complex predictions can be performed efficiently to respect the latency requirements. This paper presents a series of experiments investigating the importance of different factors for saccades prediction in common virtual and augmented reality applications. In particular, we investigate the effects of saccade orientation in 3D space and smooth pursuit eye-motion (SPEM) and how their influence compares to the variability across users. We also present a simple, yet efficient post-hoc correction method that adapts existing saccade prediction methods to handle these factors without performing extensive data collection. Furthermore, our investigation and the correction technique may also help future developments of machine-learning-based techniques by limiting the required amount of training data.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Eye tracking; gaze contingency; saccade prediction; smooth pursuit eye-motion,Augmented reality; Eye movements; Eye tracking; Helmet mounted displays; Machine learning; Rendering (computer graphics); Comprehensive modeling; Eye-tracking; Gaze contingency; Head-mounted-displays; Saccade prediction; Simple++; Smooth pursuit; Smooth pursuit eye-motion; System latency; Virtual and augmented reality; Forecasting
Gap Detection in Pairs of Ultrasound Mid-air Vibrotactile Stimuli,2023,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146425768&doi=10.1145%2f3570904&partnerID=40&md5=7066829fca580828d7ed69d3ee0fb51c,"Ultrasound mid-air haptic (UMH) devices are a novel tool for haptic feedback, capable of providing localized vibrotactile stimuli to users at a distance. UMH applications largely rely on generating tactile shape outlines on the users' skin. Here we investigate how to achieve sensations of continuity or gaps within such two-dimensional curves by studying the perception of pairs of amplitude-modulated focused ultrasound stimuli. On the one hand, we aim to investigate perceptual effects that may arise from providing simultaneous UMH stimuli. On the other hand, we wish to provide perception-based rendering guidelines for generating continuous or discontinuous sensations of tactile shapes. Finally, we hope to contribute toward a measure of the perceptually achievable resolution of UMH interfaces. We performed a user study to identify how far apart two focal points need to be to elicit a perceptual experience of two distinct stimuli separated by a gap. Mean gap detection thresholds were found at 32.3-mm spacing between focal points, but a high within- and between-subject variability was observed. Pairs spaced below 15 mm were consistently (>95%) perceived as a single stimulus, while pairs spaced 45 mm apart were consistently (84%) perceived as two separate stimuli. To investigate the observed variability, we resort to acoustic simulations of the resulting pressure fields. These show a non-linear evolution of actual peak pressure spacing as a function of nominal focal point spacing. Beyond an initial threshold in spacing (between 15 and 18 mm), which we believe to be related to the perceived size of a focal point, the probability of detecting a gap between focal points appears to linearly increase with spacing. Our work highlights physical interactions and perceptual effects to consider when designing or investigating the perception of UMH shapes.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",haptic perception; mid-air haptics; Ultrasound haptics,Acoustic fields; Focal points; Gap detection; Haptic devices; Haptic perception; Haptics; Mid-air haptic; Perceptual effects; Ultrasound haptic; Vibrotactile; Ultrasonic applications
A Content-adaptive Visibility Predictor for Perceptually Optimized Image Blending,2023,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146370462&doi=10.1145%2f3565972&partnerID=40&md5=c1ae7e8251a0794d739c25072d5b5f51,"The visibility of an image semi-transparently overlaid on another image varies significantly, depending on the content of the images. This makes it difficult to maintain the desired visibility level when the image content changes. To tackle this problem, we developed a perceptual model to predict the visibility of the blended results of arbitrarily combined images. Conventional visibility models cannot reflect the dependence of the suprathreshold visibility of the blended images on the appearance of the pre-blended image content. Therefore, we have proposed a visibility model with a content-adaptive feature aggregation mechanism, which integrates the visibility for each image feature (i.e., such as spatial frequency and colors) after applying weights that are adaptively determined according to the appearance of the input image. We conducted a large-scale psychophysical experiment to develop the visibility predictor model. Ablation studies revealed the importance of the adaptive weighting mechanism in accurately predicting the visibility of blended images. We have also proposed a technique for optimizing the image opacity such that users can set the visibility of the target image to an arbitrary level. Our evaluation revealed that the proposed perceptually optimized image blending was effective under practical conditions.  © 2023 Copyright held by the owner/author(s).",Alpha blending; contrast perception; human visual system; image blending; visibility,Interactive computer graphics; Alpha blending; Combined images; Content-adaptive; Contrast perception; Human Visual System; Image blending; Image content; Perceptual modelling; Suprathreshold; Visibility level; Visibility
Efficient Dataflow Modeling of Peripheral Encoding in the Human Visual System,2023,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146439189&doi=10.1145%2f3564605&partnerID=40&md5=a5c02c6341e3e8a27da95698ae26ad6d,"Computer graphics seeks to deliver compelling images, generated within a computing budget, targeted at a specific display device, and ultimately viewed by an individual user. The foveated nature of human vision offers an opportunity to efficiently allocate computation and compression to appropriate areas of the viewer's visual field, of particular importance with the rise of high-resolution and wide field-of-view display devices. However, while variations in acuity and contrast sensitivity across the field of view have been well-studied and modeled, a more consequential variation concerns peripheral vision's degradation in the face of clutter, known as crowding. Understanding of peripheral crowding has greatly advanced in recent years, in terms of both phenomenology and modeling. Accurately leveraging this knowledge is critical for many applications, as peripheral vision covers a majority of pixels in the image. We advance computational models for peripheral vision aimed toward their eventual use in computer graphics. In particular, researchers have recently developed high-performing models of peripheral crowding, known as ""pooling""models, which predict a wide range of phenomena but are computationally inefficient. We reformulate the problem as a dataflow computation, which enables faster processing and operating on larger images. Further, we account for the explicit encoding of ""end stopped""features in the image, which was missing from previous methods. We evaluate our model in the context of perception of textures in the periphery, including a novel texture dataset and updated textural descriptors. Our improved computational framework may simplify development and testing of more sophisticated, complete models in more robust and realistic settings relevant to computer graphics.  © 2023 Association for Computing Machinery.",foveated rendering; Human vision; image compression; perception,Display devices; Encoding (symbols); Image compression; Rendering (computer graphics); Sensitivity analysis; Signal encoding; Vision; Computing budget; Data flow modeling; Encodings; Foveated rendering; High resolution; Human vision; Human Visual System; Images compression; Peripheral vision; Visual fields; Textures
Virtual Big Heads in Extended Reality: Estimation of Ideal Head Scales and Perceptual Thresholds for Comfort and Facial Cues,2023,ACM Transactions on Applied Perception,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146421240&doi=10.1145%2f3571074&partnerID=40&md5=fb76fa2602babb94b76f1422dcb7b873,"Extended reality (XR) technologies, such as virtual reality (VR) and augmented reality (AR), provide users, their avatars, and embodied agents a shared platform to collaborate in a spatial context. Although traditional face-to-face communication is limited by users' proximity, meaning that another human's non-verbal embodied cues become more difficult to perceive the farther one is away from that person, researchers and practitioners have started to look into ways to accentuate or amplify such embodied cues and signals to counteract the effects of distance with XR technologies. In this article, we describe and evaluate the Big Head technique, in which a human's head in VR/AR is scaled up relative to their distance from the observer as a mechanism for enhancing the visibility of non-verbal facial cues, such as facial expressions or eye gaze. To better understand and explore this technique, we present two complimentary human-subject experiments in this article. In our first experiment, we conducted a VR study with a head-mounted display to understand the impact of increased or decreased head scales on participants' ability to perceive facial expressions as well as their sense of comfort and feeling of ""uncannniness""over distances of up to 10 m. We explored two different scaling methods and compared perceptual thresholds and user preferences. Our second experiment was performed in an outdoor AR environment with an optical see-through head-mounted display. Participants were asked to estimate facial expressions and eye gaze, and identify a virtual human over large distances of 30, 60, and 90 m. In both experiments, our results show significant differences in minimum, maximum, and ideal head scales for different distances and tasks related to perceiving faces, facial expressions, and eye gaze, and we also found that participants were more comfortable with slightly bigger heads at larger distances. We discuss our findings with respect to the technologies used, and we discuss implications and guidelines for practical applications that aim to leverage XR-enhanced facial cues.  © 2023 Association for Computing Machinery.",non verbal communication; outdoor augmented reality; social virtual reality; Virtual environments,Augmented reality; Helmet mounted displays; Street traffic control; User interfaces; Embodied agent; Eye-gaze; Face-to-face communications; Facial Expressions; Human head; Non-verbal communications; Outdoor augmented reality; Perceptual threshold; Social virtual reality; Spatial context; Virtual reality
