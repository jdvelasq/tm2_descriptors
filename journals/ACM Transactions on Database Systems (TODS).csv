Title,Year,Source title,Link,Abstract,Author Keywords,Index Keywords
An Execution Model for Limited Ambiguity Rules and Its Application to Derived Data Update,1995,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976736265&doi=10.1145%2f219035.219039&partnerID=40&md5=89c68383ffcf8902b7d1e67bb1a15fc6,[No abstract available],,
Space Optimization in Deductive Databases,1995,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0029492787&doi=10.1145%2f219035.219056&partnerID=40&md5=7a011a078c82120eee7dd410e5d79389,[No abstract available],,Logic programming; Optimization; Query languages; Recursive functions; Redundancy; Synchronization; Deductive databases; Space optimization; Database systems
A Structured Approach for the Definition of the Semantics of Active Databases,1995,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976810970&doi=10.1145%2f219035.219042&partnerID=40&md5=83f45e3ba5430325572376a7227a2b09,[No abstract available],,
Containment of Conjunctive Queries: Beyond Relations as Sets,1995,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976718399&doi=10.1145%2f211414.211419&partnerID=40&md5=b3fbe5fcbf55e218e34b0f6959b8ac10,"We have generalized the notion of a relational database to cover fuzzy sets, multisets, and other refinements to the concept of a relation as a set. We have examined the problem of conjunctive query containment for two important types of label systems. Specifically, we have shown that earlier theorems that deal with relations as sets are generalized for all label systems of type A, which include sets and fuzzy sets as special cases. We have also provided sufficient conditions, and in special cases necessary and sufficient conditions, for label systems of type B, which include relations as multisets, an important special case due to its significance in SQL. We have also addressed the problem of containment of sets of conjunctive queries, generalized again earlier set-based results for all label systems of type A and proving undecidability for label systems of type B. An interesting open problem is that of containment of individual conjunctive queries for type B systems. We have presented a necessary and sufficient condition for queries with no repeated predicates and a sufficient condition for the general case. Is there a general necessary and sufficient condition? Is the problem decidable? (Note that Chaludhuri and Vardi [1993] established a lower bound for this problem, but not an upper bound.) Given that the problem is decidable for both individual queries and sets of queries over relations as sets and our undecidability result for sets of queries over multi sets, this raises the possibility that the problem is undecidable even for individual queries. There are several additional areas that require further exploration. These include dealing with recursive queries in the context of deductive databases, formalizing a label system that captures the traditional notion of probabilities and addressing the query containment problem for it, and incorporating the techniques develope © 1995, ACM. All rights reserved.",Algorithms; Conjunctive queries; equivalence; Languages; multisets; query containment; query optimization; Theory,
Editorial Directions,1995,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976744204&doi=10.1145%2f211414.569785&partnerID=40&md5=a08a3261f782340e5cf8c8ce4ddcd257,[No abstract available],,
Efficient Checking of Temporal Integrity Constraints Using Bounded History Encoding,1995,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976843391&doi=10.1145%2f210197.210200&partnerID=40&md5=a8c41b8309a15f3bef812242ef6606c4,"We present an efficient implementation method for temporal integrity constraints formulated in Past Temporal Logic. Although the constraints can refer to past states of the database, their checking does not require that the entire database history be stored. Instead, every database state is extended with auxdiary relations that contain the historical information necessary for checking constraints. Auxiliary relations can be implemented as materialized relational views. © 1995, ACM. All rights reserved.",Active databases; database integrity; integrity constraints; real-time databases; temporal databases; temporal logic; triggers,
Charter And Scope,1995,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976848563&doi=10.1145%2f202106.569784&partnerID=40&md5=952570a4b12e76304fd7c5660d897978,[No abstract available],,
Fast Algorithms for Universal Quantification in Large Databases,1995,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976837031&doi=10.1145%2f210197.210202&partnerID=40&md5=837a44328f54359980b71a98f34ed85d,"Universal quantification is not supported directly in most database systems despite the fact that it adds significant power to a system's query processing and inference capabilities, in particular for the analysis of many-to-many relationships and of set-valued attributes. One of the main reasons for this omission has been that universal quantification algorithms and their performance have not been explored for large databases. In this article, we describe and compare three known algorithms and one recently proposed algorithm for relational division, the algebra operator that embodies universal quantification. For each algorithm, we investigate the performance effects of explicit duplicate removal and referential integrity enforcement, variants for inputs larger than memory, and parallel execution strategies. Analytical and experimental performance comparisons illustrate the substantial differences among the algorithms. Moreover, comparisons demonstrate that the recently proposed division algorithm evaluates a universal quantification predicate over two relations as fast as hash (semi-) join evaluates an existential quantification predicate over the same relations. Thus, existential and universal quantification can be supported with equal efficiency by adding the recently proposed algorithm to a query evaluation system. A second result of our study is that universal quantification should be expressed directly in a database query language because most query optimizers do not recognize the rather indirect formulations available in SQL as relational division, and therefore produce very poor evaluation plans for many universal quantification queries. © 1995, ACM. All rights reserved.",Algorithms; Experimentation,
Incremental Computation of Nested Relational Query Expressions,1995,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976804217&doi=10.1145%2f210197.210198&partnerID=40&md5=02b96530a0877620740808f284a41cf3,"Efficient algorithms for incrementally computmg nested query expressions do not exist. Nested query expressions are query expressions in which selection/join predicates contain subqueries. In order to respond to this problem, we propose a two-step strategy for incrementally computing nested query expressions. In step (1), the query expression is transformed into an equivalent unnested flat query expression, In step (2), the flat query expression is incrementally computed. To support step (1), we have developed a very concise algebra-to-algebra transformation algorithm, and we have formally proved its correctness The flat query expressions resulting from the transformation make intensive use of the relational set-difference operator. To support step (2), we present and analyze an efficient algorithm for incrementally computing set differences based on view pointer caches. When combined with existing incremental algorithms for SPJ queries, our incremental set-difference algorithm can be used to compute the unnested flat query expressions efficiently. It is important to notice that without our incremental set-difference algorithm the existing incremental algorithms for SPJ queries are useless for any query involving the set-difference operator, including queries that are not the result of unnesting nested queries. © 1995, ACM. All rights reserved.",Incremental computation; nested query expressions; set differences; unnesting; view pointer caches,
Declarative Updates of Relational Databases,1995,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976689401&doi=10.1145%2f202106.202110&partnerID=40&md5=2173d2b2fd07f2eb41bbd0cb54488006,"This article presents a declarative language, called update calculus, of relational database updates. A formula in update calculus involves conditions for the current database, as well as assertions about a new database. Logical connectives and quantifiers become constructors of complex updates, offering flexible specifications of database transformations. Update calculus can express all nondeterministic database transformations that are polynomial time. For set-at-a-time evaluation of updates, we present a corresponding update algebra. Existing techniques of query processing can be incorporated into update evaluation. We show that updates in update calculus can be translated into expressions in update algebra and vice versa. © 1995, ACM. All rights reserved.",database languages; database updates; expressive power; update algebra; update calculus,
The Incinerate Data Model,1995,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976821069&doi=10.1145%2f202106.202113&partnerID=40&md5=72c85af5c4c26fb59b1299f36e2247a6,"In this article, we present an extended relational algebra with universally or existentially quantified classes as attribute values. The proposed extension can greatly enhance the expressive power of relational systems, and significantly reduce the size of a database, at small additional computational cost. We also show how the proposed extensions can be built on top of a standard relational database system. © 1995, ACM. All rights reserved.",,
Transaction Chopping: Algorithms and Performance Studies,1995,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976746133&doi=10.1145%2f211414.211427&partnerID=40&md5=3881cc9d3b0aa10155a69c3ef4d1e31c,[No abstract available],,
Static Analysis Techniques for Predicting the Behavior of Active Database Rules,1995,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0029277484&doi=10.1145%2f202106.202107&partnerID=40&md5=c4ba9700c4f2367e7af23d242648cfba,"This article gives methods for statically analyzing sets of active database rules to determine if the rules are 1995 guaranteed to terminate, (2) guaranteed to produce a unique final database state, and (3) guaranteed to produce a unique stream of observable actions. If the analysis determines that one of these properties is not guaranteed, it isolates the rules responsible for the problem and determines criteria that, if satisfied, guarantee the property. The analysis methods are presented in the context of the Starburst Rule System. © 1995, ACM. All rights reserved.",active database systems; confluence; database rule processing; static analysis; termination,Algorithms; Artificial intelligence; Computer hardware description languages; Computer software; Data handling; Software engineering; Systems analysis; Active database rules; Active databases; Program verification; Static analysis; Validation; Database systems
Query Evaluation in Deductive Databases with Alternating Fixpoint Semantics,1995,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0029369683&doi=10.1145%2f211414.211416&partnerID=40&md5=3e57dacf267b9879e8f5a0b79b910079,[No abstract available],,Artificial intelligence; Computational complexity; Computational linguistics; Formal logic; Logic programming; Query languages; Theorem proving; Alternating fixpoint; Deductive databases; Negation; Predicate logic; Query evaluation; Semantics; SLG resolution; Database systems
Evaluation of Remote Backup Algorithms for Transaction-Processing Systems,1994,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0028514207&doi=10.1145%2f185827.185836&partnerID=40&md5=f67303f4aa3a4fca7e7ea21529cc1b44,"A remote backup is a copy of a primary database maintained at a geographically separate location and is used to increase data availability. Remote backup systems are typically log-based and can be classified into 2-safe and 1-safe, depending on whether transactions commit at both sites simultaneously or first commit at the primary and are later propagated to the backup. We have built an experimental database system on which we evaluated the performance of the epoch and the dependency reconstruction algorithms, two 1-safe algorithms we have developed. We compared the 1-safe with the 2-safe approach under various conditions. © 1994, ACM. All rights reserved.",disaster recovery; hot spare; hot standby; remote backup,Algorithms; Computer system recovery; Data processing; Distributed database systems; Fault tolerant computer systems; Performance; Reliability; Disaster recovery; Distributed applications; Hot spare; Hot standby; Remote backup algorithms; Transaction processing systems; Database systems
Formal Aspects of Concurrency Control in Long-Duration Transaction Systems using the NT/PV Model,1994,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0028514325&doi=10.1145%2f185827.185854&partnerID=40&md5=ffb3987a7dc7873864dd6b6dbf9af5f3,"In the typical database system, an execution is correct if it is equivalent to some serial execution. This criterion, called serializability, is unacceptable for new database applications which require long-duration transactions. We present a new transaction model which allows correctness criteria more suitable for these applications. This model combines three enhancements to the standard model: nested transactions, explicit predicates, and multiple versions. These features yield the name of the new model, nested transactions with predicates and versions, or NT/PV. The modular nature of the NT/PV model allows a straightforward representation of simple systems. It also provides a formal framework for describing complex interactions. The most complex interactions the model allows can be captured by a protocol which exploits all of the semantics available to the NT/PV model. An example of these interactions is shown in a CASE application. The example shows how a system based on the NT/PV model is superior to both standard database techniques and unrestricted systems in both correctness and performance. © 1994, ACM. All rights reserved.",Concurrency control protocol; semantic information; transaction processing,Algorithms; Computational linguistics; Computer aided software engineering; Computer simulation; Concurrency control; Data processing; Network protocols; Long duration transaction systems; Semantic information; Serializability; Transaction processing; Database systems
Tools and Transformations—Rigorous and Otherwise—for Practical Database Design,1994,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0028445456&doi=10.1145%2f176567.176568&partnerID=40&md5=5c8c0773702950f0c130fd23cd840fac,"We describe the tools and theory of a comprehensive system for database design, and show how they work together to support multiple conceptual and logical design processes. The Database Design and Evaluation Workbench 1994 system uses a rigorous, information-content-preserving approach to schema transformation, but combines it with heuristics, guess work, and user interactions. The main contribution lies in illustrating how theory was adapted to a practical system, and how the consistency and power of a design system can be increased by use of theory. First, we explain why a design system needs multiple data models, and how implementation over a unified underlying model reduces redundancy and inconsistency. Second, we present a core set of small but fundamental algorithms that reaarange a schema without changing its information content. From these reusable components, we easily built larger tools and transformations that were still formally justified. Third, we describe heuristic tools that attempt to improve a schema, often by adding missing information. In these tools, unreliable techniques such as normalization and relationship inference are bolstered by system-guided user interactions to remove errors. We present a rigorous criterion for identifying unnecessary relationships, and discuss an interactive view integrator. Last, we examine the relevance of database theory to building these practically motivated tools and contrast the paradigms of system builders with those of theoreticians. © 1994, ACM. All rights reserved.",applications of database theory; computer-aided software engineering; data model translation; database design; database equivalence; design heuristics; entity-relationship model; heuristics; normalization; view integration,Algorithms; Computer aided logic design; Data structures; Errors; Heuristic programming; Logic design; Programming theory; Redundancy; Software engineering; Data model translation; Database design; Database design and evaluation; Database theory; Design heuristics; Entity relationship model; Normalization; Schema transformation; View integration; Workbench system; Database systems
Formal query languages for secure relational databases,1994,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0028720693&doi=10.1145%2f195664.195675&partnerID=40&md5=84c1cad3c8075250fed392834cd6d519,"The addition of stringent security specifications to the list of requirements for an application poses many new problems in DBMS design and implementation, as well as database design, use, and maintenance. Tight security requirements, such as those that result in silent masking of witholding of true information from a user or the introduction of false information into query answers, also raise fundamental questions about the meaning of the database and the semantics of accompanying query languages. In this paper, we propose a belief-based semantics for secure databases, which provides a semantics for databases that can “lie” about the state of the world, or about their knowledge about the state of the world, in order to preserve security. This kind of semantics can be used as a helpful retrofit for the proposals for a “multilevel secure” database model 1994, and may be useful for less restrictive security policies as well. We also propose a family of query languages for multilevel secure relational database applications, and base the semantics of those languages on our semantics for secure databases. Our query languages are free of the semantic problems associated with use of ordinary SQL in a multilevel secure context, and should be easy for users to understand and employ. © 1994, ACM. All rights reserved.",formal security models; information security; multilevel secure databases,Computational linguistics; Data structures; Query languages; Security of data; Systems analysis; Belief based semantics; Database design; Formal query languages; Formal security models; Multilevel secure databases; Stringent security specifications; Relational database systems
Bounded Ignorance: A Technique for Increasing Concurrency in a Replicated System,1994,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0028727903&doi=10.1145%2f195664.195670&partnerID=40&md5=297cc794c8679d2c43254c2e42521f59,"Databases are replicated to improve performance and availability. The notion of correctness that has commonly been adopted for concurrent access by transactions to shared, possibly replicated, data is serializability. However, serializability may be impractical in high-performance applications since it imposes too stringent a restriction on concurrency. When serializability is relaxed, the integrity constraints describing the data may be violated. By allowing bounded violations of the integrity constraints, however, we are able to increase the concurrency of transactions that execute in a replicated environment. In this article, we introduce the notion of an N-ignorant transaction, which is a transaction that may be ignorant of the results of at most N prior transactions, which is a transaction that may be ignorant of the results of at most N prior transactions. A system in which all transactions are N-ignorant can have an N + 1-fold increase in concurrency over serializable systems, at the expense of bounded violations of its integrity constraints. We present algorithms for implementing replicated databases in N-ignorant systems. We then provide constructive methods for calculating the reachable states in such systems, given the value of N, so that one may assess the maximum liability that is incurred in allowing constraint violation. Finally, we generalize the notion of N-ignorance to a matrix of ignorance for the purpose of higher concurrency. © 1994, ACM. All rights reserved.",concurrency control; integrity constraints; reachability analysis; replication; serializability,Algorithms; Availability; Concurrency control; Constraint theory; Data processing; Performance; Integrity constraints; Maximum liability; N-ignorant transaction; Reachability analysis; Replication; Serializability; Distributed database systems
Synthesis of Extended Transaction Models Using ACTA,1994,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0028513399&doi=10.1145%2f185827.185843&partnerID=40&md5=effa8f33fb579ca52c9066a257843412,"ACTA is a comprehensive transaction framework that facilitates the formal description of properties of extended transaction models. Specifically, using ACTA, one can specify and reason about 1994 the effects of transactions on objects and (2) the interactions between transactions. This article presents ACTA as a tool for the synthesis of extended transaction models, one which supports the development and analysis of new extended transaction models in a systematic manner. Here, this is demonstrated by deriving new transaction definitions (1) by modifying the specifications of existing transaction models, (2) by combining the specifications of existing models, and (3) by starting from first principles. To exemplify the first, new models are synthesized from atomic transactions and join transactions. To illustrate the second, we synthesize a model that combines aspect of the nested- and split-transaction models. We demonstrate the latter by deriving the specification of an open-nested-transaction model from high-level requirements. © 1994, ACM. All rights reserved.",concurrency control; correctness criteria; semantics; serializability theory; transaction models,Computational linguistics; Computer hardware description languages; Computer operating systems; Computer simulation; Computer software; Concurrency control; Data processing; Distributed database systems; Formal logic; Reliability; Systems analysis; Correctness criteria; Extended transaction models; Serializability theory; Computer aided software engineering
Automated Resolution of Semantic Heterogeneity in Multidatabases,1994,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0028445789&doi=10.1145%2f176567.176569&partnerID=40&md5=67c5ec745e4b0c5f79972768cf9928b9,"A multidatabase system provides integrated access to heterogeneous, autonomous local databases in a distributed system. An important problem in current multidatabase systems is identification of semantically similar data in different local databases. The Summary Schemas Model 1994 is proposed as an extension to multidatabase systems to aid in semantic identification. The SSM uses a global data structure to abstract the information available in a multidatabase system. This abstracted form allows users to use their own terms (imprecise queries) when accessing data rather than being forced to use system-specified terms. The system uses the global data structure to match the user's terms to the semantically closest available system terms. A simulation of the SSM is presented to compare imprecise-query processing with corresponding query-processing costs in a standard multidatabase system. The costs and benefits of the SSM are discussed, and future research directions are presented. © 1994, ACM. All rights reserved.",federated database; imprecise queries; multidatabase; schemas; semantic heterogeneity,Abstracting; Computational linguistics; Computer simulation; Costs; Data structures; Query languages; Federated database; Global data structures; Imprecise queries; Multidatabases; Schemas; Semantic heterogeneity; Summary schemes model; Distributed database systems
Quorum Consensus in Nested Transaction Systems,1994,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0028694977&doi=10.1145%2f195664.195666&partnerID=40&md5=137c24610899e21014d9d1039799dffa,"Gifford's Quorum Consensus algorithm for data replication is studied in the context of nested transactions and transaction failures 1994, and a fully developed reconfiguration strategy is presented. A formal description of the algorithm is presented using the Input/Output automaton model for nested-transaction systems due to Lynch and Merritt. In this description, the algorithm itself is described in terms of nested transactions. The formal description is used to construct a complete proof of correctness that uses standard assertional techniques, is based on a natural correctness condition, and takes advantage of modularity that arises from describing the algorithm as nested transactions. The proof is accomplished hierarchically, showing that a fully replicated reconfigurable system “simulates” an intermediate replicated system, and that the intermediate system simulates an unreplicated system. The presentation and proof treat issues of data replication entirely separately from issues of concurrency control and recovery. © 1994, ACM. All rights reserved.",Concurrency control; data replication; hierarchical proofs; I/O automata; nested transactions; quorum consensus,Algorithms; Automata theory; Availability; Concurrency control; Data processing; Performance; Reliability; Concurrency recovery; Data replication; Formal description; Input output automaton model; Intermediate replicated system; Modularity; Nested transaction systems; Quorum consensus; Standard assertional techniques; Transaction failures; Distributed database systems
Using Semantic Values to Facilitate Interoperability Among Heterogeneous Information Systems,1994,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0028446067&doi=10.1145%2f176567.176570&partnerID=40&md5=f91374c3deb6f0bdf2c1d99da316f91c,"Large organizations need to exchange information among many separately developed systems. In order for this exchange to be useful, the individual systems must agree on the meaning of their exchanged data. That is, the organization must ensure semantic interoperability. This paper provides a theory of semantic values as a unit of exchange that facilitates semantic interoperability betweeen heterogeneous information systems. We show how semantic values can either be stored explicitly or be defined by environments. A system architecture is presented that allows autonomous components to share semantic values. The key component in this architecture is called the context mediator, whose job is to identify and construct the semantic values being sent, to determine when the exchange is meaningful, and to convert the semantic values to the form required by the receiver. Our theory is then applied to the relational model. We provide an interpretation of standard SQL queries in which context conversions and manipulations are transparent to the user. We also introduce an extension of SQL, called Context-SQL 1994, in which the context of a semantic value can be explicitly accessed and updated. Finally, we describe the implementation of a prototype context mediator for a relational C-SQL system. © 1994, ACM. All rights reserved.",,Computational linguistics; Computer architecture; Data processing; Information services; Query languages; Societies and institutions; Context mediator; Data translation; Information system; Relational models; Semantic interoperability; SQL queries; Distributed database systems
Amalgamating Knowledge Bases,1994,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0028449454&doi=10.1145%2f176567.176571&partnerID=40&md5=7cb1964be2df0e352316750335312258,"The integration of knowledge for multiple sources is an importantaspect of automated reasoning systems. When different knowledge basesare used to store knowledge provided by multiple sources, we are facedwith the problem of integrating multiple knowledge bases: Under thesecircumstances, we are also confronted with the prospect ofinconsistency. In this paper we present a uniform theoretical framework,based on annotated logics, foramalgamating multiple knowledge bases when these knowledge bases1994 contain inconsistencies, uncertainties, and nonmonotonicmodes of negation. We show that annotated logics may be used, with somemodifications, to mediate betweendifferent knowledge bases. The multiple knowledge bases are amalgamatedby a transformation of the individual knowledge bases into new annotatedlogic programs, together with the addition of a new axiom scheme. Wecharacterize the declarative semantics of such amalgamated knowledgebases and study how the semantics of the amalgam is related to thesemantics of the individual knowledge bases being combined. © 1994, ACM. All rights reserved.",amalgamated knowledge bases; annotated logics,Artificial intelligence; Computational linguistics; Data processing; Database systems; Formal logic; Logic programming; Amalgamated knowledge bases; Annotated logics; Automated reasoning systems; Data translation; Logic programs; Representation languages; Semantics; Knowledge based systems
Index Structures for Selective Dissemination of Information Under the Boolean Model,1994,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0028448529&doi=10.1145%2f176567.176573&partnerID=40&md5=21ac6de77e8ce58da77ba02b29983214,"The number, size, and user population of bibliographic and full-text document databases are rapidly growing. With a high document arrival rate, it becomes essential for users of such databases to have access to the very latest documents; yet the high document arrival rate also makes it difficult for users to keep themselves updated. It is desirable to allow users to submit profiles, i.e., queries that are constantly evaluated, so that they will be automatically informed of new additions that may be of interest. Such service is traditionally called Selective Dissemination of Information 1994. The high document arrival rate, the huge number of users, and the timeliness requirement of the service pose a challenge in achieving efficient SDL. In this article, we propose several index structures for indexing profiles and algorithms that efficiently match documents against large number of profiles. We also present analysis and simulation results to compare their performance under different scenarios. © 1994, ACM. All rights reserved.",,Algorithms; Boolean algebra; Computer simulation; Data communication systems; Indexing (of information); Information analysis; Information dissemination; Information services; Performance; Boolean model; Index structures; Information filtering; Selective dissemination of information; Database systems
Automatic Generation of Production Rules for Integrity Maintenance,1994,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0028515678&doi=10.1145%2f185827.185828&partnerID=40&md5=c55917875024d3a8bfad7f7d8e14fd2a,"In this article we present an approach to integrity maintenance, consisting of automatically generating production rules for integrity enforcement. Constraints are expressed as particular formulas of Domain Relational Calculus; they are automatically translated into a set of repair actions, encoded as production rules of an active database system. Production rules may be redundant 1994 and conflicting (because repairing one constraint may cause the violation of another constraint). Thus, it is necessary to develop techniques for analyzing the properties of the set of active rules and for ensuring that any computation of production rules after any incorrect transaction terminates and produces a consistent database state. Along these guidelines, we describe a specific architecture for constraint definition and enforcement. The components of the architecture include a Rule Generator, for producing all possible repair actions, and a Rule Analyzer and Selector, for producing a collection of production rules such that their execution after an incorrect transaction always terminates in a consistent state (possibly by rolling back the transaction); moreover, the needs of applications are modeled, so that integrity-enforcing rules reach the final state that better represents the original intentions of the transaction's supplier. Specific input from the designer can also drive the process and integrate or modify the rules generated automatically by the method. Experimental results of a prototype implementation of the proposed architecture are also described. © 1994, ACM. All rights reserved.",Automatic generation of production rules for integrity enforcement,Artificial intelligence; Computer architecture; Constraint theory; Encoding (symbols); Logic programming; Maintenance; Security of data; Standards; Systems analysis; Automatic generation; Automatic programming; Integrity maintenance; Production rules; Database systems
Altruistic Locking,1994,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0028396765&doi=10.1145%2f174638.174639&partnerID=40&md5=f52be9ca0ce75248a1d266b1fa1bbe23,"Long-lived transactions 1994 hold on to database resources for relatively long periods of time, significantly delaying the completion of shorter and more common transactions. To alleviate this problem we propose an extension to two-phase locking, called altruistic locking, whereby LLTs can release their locks early. Transactions that access this released data are said to run in the wake of the LLT and must follow special locking rules. Like two-phase locking, altruistic locking is easy to implement and guarantees serializability. © 1994, ACM. All rights reserved.",atomicity; locking; scheduling; serializability,Management; Security of data; Altruistic locking; Long-lived transactions; Two phase locking; Database systems
A Taxonomy for Secure Object-Oriented Databases,1994,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0028398878&doi=10.1145%2f174638.174640&partnerID=40&md5=5fad1c40dce61a9ae9a27f0207761ac2,"This paper proposes a taxonomy for secure object-oriented databases in order to clarify the issues in modeling and implementing such databases. It also indicates some implications of the various choices one may make when designing such a database. Most secure database models have been designed for relational databases. The object-oriented database model is more complex than the relational model. For these reasons, models for secure object-oriented databases are more complex than their relational counterparts. Furthermore, since views of the object-oriented model differ, each security model has to make some assumptions about the object-oriented model used for its particular database. A number of models for secure object-oriented databases have been proposed. These models differ in many respects, because they focus on different aspects of the security problem, or because they make different assumptions about what constitutes a secure database or because they make different assumptions about the object-oriented model. The taxonomy proposed in this paper may be used to compare the various models: Models that focus on specific issues may be positioned in the broader context with the aid of the taxonomy. The taxonomy also identifies the major aspects where security models may differ and indicates some alternatives available to the system designer for each such design choice. We show some implications of using specific alternatives. Since differences between models for secure object-oriented databases are often subtle, a formal notation is necessary for a proper comparison. Such a formal notation also facilitates the formal derivation of restrictions that apply under specific conditions. The formal approach also gives a clear indication about the assumptions made by us—given as axioms—and the consequences of those assumptions 1994—given as theorems. © 1994, ACM. All rights reserved.",formal security models; information security; multilevel secure databases; object-orientation,Information retrieval systems; Management; Object oriented programming; Information security; Taxonomy; Database systems
On Completeness of Historical Relational Query Languages,1994,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0028396334&doi=10.1145%2f174638.174642&partnerID=40&md5=01361ead0147b6070b78d76552977a43,"Numerous proposals for extending the relational data model to incorporate the temporal dimension of data have appeared in the past several years. These proposals have differed considerably in the way that the temporal dimension has been incorporated both into the structure of the extended relations of these temporal models and into the extended relational algebra or calculus that they define. Because of these differences, it has been difficult to compare the proposed models and to make judgments as to which of them might in some sense be equivalent or even better. In this paper we define temporally grouped and temporally ungrouped historical data models and propose two notions of historical relational completeness, analogous to Codd's notion of relational completeness, one for each type of model. We show that the temporally ungrouped models are less expressive than the grouped models, but demonstrate a technique for extending the ungrouped models with a grouping mechanism to capture the additional semantic power of temporal grouping. For the ungrouped models, we define three different languages, a logic with explicit reference to time, a temporal logic, and a temporal algebra, and motivate our choice for the first of these as the basis for completeness for these models. For the grouped models, we define a many-sorted logic with variables over ordinary values, historical values, and times. Finally, we demonstrate the equivalence of this grouped calculus and the ungrouped calculus extended with a grouping mechanism. We believe the classification of historical data models into grouped and ungrouped models provides a useful framework for the comparison of models in the literature, and furthermore, the exposition of equivalent languages for each type provides reasonable standards for common, and minimal, notions of historical relational completeness. © 1994, ACM. All rights reserved.",completeness; historical databases; query languages; relational model; temporal databases; temporal grouping; temporal logic,Logic design; Management; Query languages; Completeness; Relational database systems
A Modified Random Perturbation Method for Database Security,1994,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0028393221&doi=10.1145%2f174638.174641&partnerID=40&md5=a951c62cacf1b860d848e6ef812647a1,"The random data perturbation 1994 method of preserving the privacy of individual records in a statistical database is discussed. In particular, it is shown that if confidential attributes are allowed as query-defining variables, severe biases may result in responses to queries. It is also shown that even if query definition through confidential variables is not allowed, biases can still occur in responses to queries such as those involving proportions or counts. In either case, serious distortions may occur in user statistical analyses. A modified version of RDP is presented, in the form of a query adjustment procedure and specialized perturbation structure which will produce unbiased results. © 1994, ACM. All rights reserved.",bias; correlation; noise addition; random perturbation method,Design; Management; Perturbation techniques; Random processes; Security systems; Database security; Noise addition; Database systems
The Deductive Synthesis of Database Transactions,1993,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0027797782&doi=10.1145%2f169725.169716&partnerID=40&md5=aee3fa33a86620cfe23893a77f1f30a3,"Database programming requ]res knowledge of� database semantics both to mamtam database mtegrlty and to explore more optimization opportumties. Automated programming of database transactions is desirable and feasible. In general, transactions use simple constructs and algorithms; specifications of database semantics are available; and transactions perform small Incremental updates to database contents. Automated programming m such a restricted but well understood and important domam 1s promising. We approach the synthesis of database transactions that preserve the validity of integrity constraints using deductive techmques, A transaction logic for a fairly expressive class of transactions is developed as the formahsm within which the synthesis 1s conducted Transactions are generated as the by-product of proving specifications in the logic, The Manna-Waldinger deductwe-tableau system is extended w]th reference rules for tbe extraction of transactions from proofs, which reqmre the cooperation of multiple tableaux Control strateges are developed that utdlze the database semantics to reduce search effort m the symthes]s process and to enhance runtlme efficiency of the generated transactions. © 1993, ACM. All rights reserved.",database programming; deductive tableau; integrity constraints; search control; transaction logic; transaction synthesis,Computational linguistics; Computer programming; Constraint theory; Control system synthesis; Control systems; Formal logic; Online searching; Optimization; Database programming; Deductive tableau; Integrity constraint; Search control; Transaction logic; Transaction synthesis; Database systems
Optimal Histograms for Limiting Worst-Case Error Propagation in the Size of Join Results,1993,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0027872183&doi=10.1145%2f169725.169708&partnerID=40&md5=48fc66a0257b89ce199d1de54a1fd576,[No abstract available],histograms; join size estimation; query optimization; vector majorization,Approximation theory; Data processing; Error analysis; Estimation; Graphic methods; Information theory; Optimization; Query languages; Vectors; Error propagation; Frequency distribution; Join size estimation; Optimal histogram; Query optimization; Query processing; Vector majorization; Database systems
Two-Phase Locking Performance and its Thrashing Behavior,1993,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0027886694&doi=10.1145%2f169725.169720&partnerID=40&md5=5128482c162b0356fff1b3b334ba86c5,[No abstract available],concurrency control; data contention; load control; thrashing; two-phase locking,Computer systems; Control systems; Numerical analysis; Parameter estimation; Performance; Statistical methods; Concurrency control method; Mean value analysis method; Processing time distribution; Trashing behavior; Two phase locking; Phase locked loops
A Universal-Scheme Approach to Statistical Databases Containing Homogeneous Summary Tables,1993,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0027874192&doi=10.1145%2f169725.169712&partnerID=40&md5=0d070fbdda923656867d4aceb4612ebf,"In many situations a statistical database contains multiple summary tables, which report summary statistics on the same summary variable for the same population of individuals or objects using different classification criteria (�homogeneous� summary tables) Existing query languages consider only those queries which may aggregate data stored in a single summary table When a statistical database contains homogeneous summary tables, such query languages do not allow an integrated view of data, whereas statistic] ans are inclined to view and query a collection of homogeneous summary tables as if they were actually a single higher-dimensional summary table This le@imizes the search for a universal-scheme solution to the problem of data integration m such statistical databases. It is shown that such a solution can be found if the database tables contain additive summary data. Accordingly, queries are grouped into three classes: queries that can be evaluated to single values (evatuabte queries): queries that can be evaluated to value ranges (ansu,erable querkes); and queries whose values remain unknown (unansu,erable querzes). The membership of a given query to one of these three classes us not an intrinsic property of the query, but depends on both the type of the summary variable and tbe dependencies that are assumed in the universal scheme by the database designer On the basis of such information, linear-time procedures for recogmzing and answering answerable and evaluable queries are developed. © 1993, ACM. All rights reserved.",Bipartite graph; category relation; query-answering system; statistical database; summary table; universal classification scheme,Algorithms; Data processing; Data structures; Linear algebra; Optimization; Query languages; Statistical methods; Bipartite graph; Category relation; Query answering system; Query processing; Statistical database; Summary table; Universal classification scheme; Database systems
"The Model, Language, and Implementation of an Object-Oriented Multimedia Knowledge Base Management System",1993,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0027558867&doi=10.1145%2f151284.151285&partnerID=40&md5=485dae07cdf673e5114f05e3172658b6,"New applications such as CAD, AI, and hypermedia require directrepresentation and flexible use of complex objects, behavioralknowledge, and multimedia data. To this end, we have devised a knowledgebase management system called Jasmine. An object-oriented approach in aprogramming language also seems promising for use in Jasmine. Jasmineextends the current object-oriented approach and provides the followingfeatures. Our object model is based on functional data models andwell-established set theory. Attributes or functions composing objectscan represent both structural and behavioral knowledge. The object modelcan represent incomplete and generic knowledge. The model can supportthe basic storage and operations of multimedia data. The facets ofattributes can flexibly represent constraints and triggers. The objectmanipulation language can support associative access of objects. Thestructural and behavioral knowledge can be uniformly treated to allowthe user to specify complex object operations in a compact manner. Theuser-defined and system-defined attributes can be uniformly specified toease user customization of the language. The classes and instances canbe uniformly accessed. Incomplete knowledge can be flexibly accessed.The system has a layered architecture. Objects are stored in nestedrelations provided by extensive DBMS as a sublayer. User query ofobjects is compiled into relational operations such as select and join,which can be efficiently processed using hashing. The behavioralknowledge is compiled into predicate and manipulation function interfaces that can directly access tuples in a buffer. © 1993, ACM. All rights reserved.",,Computer programming languages; Data processing; Data structures; Interfaces (computer); Knowledge based systems; Object oriented programming; Query languages; Set theory; Functional data model; Knowledge base management system Jasmine; Multimedia; Object manipulation language; Database systems
Solving Queries by Tree Projections,1993,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0027656255&doi=10.1145%2f155271.155277&partnerID=40&md5=d52142ccceefc6faedbe8ba42bc58606,"Suppose a database schema D is extended to D¯ by adding new relation schemas, and states for D are extended to states for D¯ by applying joins and projections to existing relations. It is shown that certain desirable properties that D¯ has with respect to D. These properties amount to the ability to compute efficiently the join of all relations in a state for D from an extension of this state over D¯. The equivalence is proved for unrestricted 1993 databases. If D¯ is obtained from D by adding a set of new relation schemas that form a tree schema, then the equivalence also holds for finite databases. In this case there is also a polynomial time algorithm for testing the existence of a tree projection of D¯ with respect to D. © 1993, ACM. All rights reserved.",Acyclicity; chase; database schema; hypergraph; inclusion dependency; join; monotone join expression; projection; qual graph; relational database; semijoin; semijoin reduction; tableau; tree projection; tree schema,Algorithms; Data processing; Data reduction; Graph theory; Programming theory; Relational database systems; Trees (mathematics); Acyclicity; Database schema; Hypergraph; Inclusion dependency; Monotone join expression; Qual graph; Semijoin reduction; Tree projections; Query languages
Processing Time-Constrained Aggregate Queries in CASE-DB,1993,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0027610331&doi=10.1145%2f151634.151636&partnerID=40&md5=8b45dc88449c45fc4d88ea9a9100e46f,"In this paper, we present an algorithm to strictly control the time to process an estimator for an aggregate relational query. The algorithm implemented in a prototype database management system, called CASE-DB, iteratively samples from input relations, and evaluates the associated estimator until the time quota expires. In order to estimate the time cost of a query, CASE-DB uses adaptive time cost formulas. The formulas are adaptive in that the parameters of the formulas can be adjusted at runtime to better fit the characteristics of a query. To control the use of time quota, CASE-DB adopts the one-at-a-time-interval time control strategy to make a tradeoff between the risks of overspending and the overhead, finally, experimental evaluation of the methodology is presented. © 1993, ACM. All rights reserved.",estimation; relational algebra; risk of overspending; sampling; selectivity; time constraints,Algorithms; Control systems; Database management systems; Management information systems; Query languages; Statistical methods; Relational algebra; Time constrained aggregate queries; Time control strategy; Time cost formula; Database systems
The Performance of Current B-Tree Algorithms,1993,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0027557582&doi=10.1145%2f151284.151286&partnerID=40&md5=bf338685f9f0c559fbb53cfbb328aded,[No abstract available],B-trees; concurrent B-trees; concurrent data structures; performance of concurrent algorithms,Computer system recovery; Data structures; Database systems; Information retrieval systems; Mathematical models; Performance; Trees (mathematics); Concurrent algorithms; Concurrent B trees; Lehman Yao algorithm; Locking algorithms; Two phase locking; Algorithms
Finite Representation of Infinite Query Answers,1993,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0027609181&doi=10.1145%2f151634.151635&partnerID=40&md5=8b9a64735542a7a7b8e3f09b4e310deb,"We define here a formal notion of finite representation of infinite query answers in logic programs. We apply this notion to DatalognS programs may be infinite and consequently queries may have infinite answers. We present a method to finitely represent infinite least Herbrand models of DatalognS program 1993 can be forgotten. Given a query to be evaluated, it is easy to obtain from the relational specification finitely many answer substitutions that represent infinitely many answer substitutions to the query. The method involved is a combination of a simple, unificationless, computational mechanism (graph traversal, congruence closure, or term rewriting) and standard relational query evaluation methods. Second, a relational specification is effectively computable and its computation is no harder, in the sense of the complexity class, than answering yes-no queries. Our method is applicable to every range-restricted DatalognS program. We also show that for some very simple non-DatalognS logic programs, finite representations of query answers do not exist. © 1993, ACM. All rights reserved.",,Algorithms; Artificial intelligence; Logic programming; Answer substitutions; Finite representation; Infinite query answers; Relational query evaluation method; Relational specification; Relational database systems
Cost and Availability Tradeoffs in Replicated Data Concurrency Control,1993,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0027560644&doi=10.1145%2f151284.151287&partnerID=40&md5=5951b4f1f76bdb1dc1390e3387118e9f,[No abstract available],availability; replicated database,Algorithms; Computational methods; Constraint theory; Costs; Data communication systems; Data processing; Heuristic methods; Mathematical models; Optimization; Availability; Replicated database; Transaction processing; Distributed database systems
Optimal Disk Allocation for Partial Match Queries,1993,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0027559934&doi=10.1145%2f151284.151288&partnerID=40&md5=c76efcb289b95b8d83bdd67a2fae8a61,"The problem of disk allocation addresses the issue of how to distribute a file on several disks in order to maximize concurrent disk accesses in response to a partial match query. In this paper a coding-theoretic analysis of this problem is presented, and both necessary and sufficient conditions for the existence of strictly optimal allocation methods are provided. Based on a class of optimal codes, known as maximum distance separable codes, strictly optimal allocation methods are constructed. Using the necessary conditions proved, we argue that the standard definition of strict optimality is too strong and cannot be attained, in general. Hence, we reconsider the definition of optimality. Instead of basing it on an abstract definition that may not be attainable, we propose a new definition based on the best possible allocation method. Using coding theory, allocation methods that are optimal according to our proposed criterion are developed. © 1993, ACM. All rights reserved.",Cartesian product files; coding theory; multiple disk systems; partial match queries,Codes (symbols); Data structures; File organization; Information retrieval systems; Information theory; Optimization; Storage allocation (computer); Cartesian product files; Maximum distance separable codes; Multiple disk systems; Optimal disk allocation; Partial match queries; Database systems
Consistency and Orderability: Semantics-Based Correctness Criteria for Databases,1993,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0027657561&doi=10.1145%2f155271.155276&partnerID=40&md5=dce04dd10245a22f663ba5c2c55b42f6,"The semantics of objects and transactions in database systems are investigated. User-defined predicates called consistency assertions are used to specify user programs. Three new correctness criteria are proposed. The first correctness criterion consistency is based solely on the users' specifications and admit nonserializable executions that are acceptable to the users. Integrity constraints of the database are maintained through consistency assertions. The second correctness criterion orderability is a generalization of view serializability and represents a weak notion of equivalence to a serial schedule. Finally, the third correctness criterion strong order-ability is introduced as a generalization of conflict serializability. Unlike consistency, the notions of orderability allow users to operate an isolation as maintenance of the integrity constrainst now becomes the responsibility of the database system. © 1993, ACM. All rights reserved.",Concurrency control; object-oriented databases; semantics; serializability theory,Computational linguistics; Constraint theory; Data acquisition; Data handling; Data structures; Formal logic; Object oriented programming; Programming theory; Query languages; Software engineering; Theorem proving; Concurrency control; Consistency; Integrity constraints; Orderability; Semantics based correctness criteria; Serializability theory; Database systems
Semantics for Null Extended Nested Relations,1993,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0027657562&doi=10.1145%2f155271.155275&partnerID=40&md5=cb784fab1cbcdc7eeb944a2039013a7c,"The nested relational model extends the flat relational model by relaxing the first normal form assumption in order to allow the modeling of complex objects. Much of the previous work on the nested relational model has concentrated on defining the data structures and query language for the model. The work done on integrity constraints in nested relations has mainly focused on characterizing subclasses of nested relations and defining normal forms for nested relations with certain desirable properties. In this paper we define the semantics of nested relations, which may contain null values, in terms of integrity constraints, called null extended data dependencies, which extend functional dependencies and join dependencies encountered in flat relational database theory. We formalize incomplete information in nested relations by allowing only one unmarked generic null value, whose semantics we do not further specify. The motivation for the choice of a generic null is our desire to investigate only fundamental semantics which are common to all unmarked null types. This lead us to define a preorder on nested relations, which allows us to measure the relative information content of nested relations. We also define a procedure, called the extended chase procedure, for testing satisfaction of null extended data dependencies and for making inferences by using these null extended data dependencies. The extended chase procedure is shown to generalize the classical chase procedure, which is of major importance in flat relational database theory. As a consequence of our approach we are able to capture the novel notion of losslessness in nested relations, called herein null extended lossless decomposition. Finally, we show that the semantics of nested relations are a natural extension of the semantics of flat relations. © 1993, ACM. All rights reserved.",Extended chase; nested relations; null extended algebra; null extended data dependencies; nulls,Algorithms; Computational linguistics; Constraint theory; Data structures; Mathematical models; Programming theory; Query languages; Extended chase procedure; Null extended data dependencies; Null extended nested relations; Relational database systems
Transitive Closure Algorithms Based on Graph Traversal,1993,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0027657166&doi=10.1145%2f155271.155273&partnerID=40&md5=6e82c72f3fa16a68316de93cb830681f,"Several graph-based algorithms have been proposed in the literature to compute the transitive closure of a directed graph. We develop two new algorithms 1993 and compare the performance of their implementations in a disk-based environment with a well-known graph-based algorithm proposed by Schmitz. Our algorithms use depth-first search to traverse a graph and a technique called marking to avoid processing some of the arcs in the graph. They compute the closure by processing nodes in reverse topological order, building descendent sets by adding the descendent sets of children. While the details of these algorithms differ considerably, one important difference among them is the time at which descendent set additions are performed. Basic_TC, results in superior performance. The first reason is that early additions result in larger descendent set sizes on the average over the duration of the execution, thereby causing more I/O; very often this turns out to more than offset the gains of not having to fetch certain sets again to add them. The second reason is that information collected in the first pass can be used to apply several optimizations in the second pass. To the extent possible, we also adapt these algorithms to perform path computations. Again, our performance comparison confirms the trends seen in reachability queries. Taken in conjunction with another performance study our results indicate that all graph-based algorithms significantly outperform other types of algorithms such as Seminaive and Warren. © 1993, ACM. All rights reserved.",Depth-first search; node reachability; path computations; transitive closure,Algorithms; Computational methods; Computer programming languages; Critical path analysis; Data structures; Graph theory; Query languages; Recursive functions; Root loci; Trees (mathematics); Graph traversal; Schmitz algorithms; Transitive closure algorithms; Database systems
Open Commit Protocols Tolerating Commission Failures,1993,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0027607347&doi=10.1145%2f151634.151638&partnerID=40&md5=1568f8cb99068ef0ff7ff83ee320321a,"To ensure atomicity of transactions in distributed systems so-called 2-phase commit 1993 protocols have been proposed. The basic assumption of these protocols is that the processing nodes involved in transactions are “sane,” i.e., they only fail with omission failures, and nodes eventually recover from failures. Unfortunately, this assumption is not realistic for so-called Open Distributed Systems (ODSs), in which nodes may have totally different reliability characteristics. In ODSs, nodes can be classified into trusted nodes (e.g., a banking server) and nontrusted nodes (e.g., a home PC requesting a remote banking service). While trusted nodes are assumed to be sane, nontrusted nodes may fail permanently and even cause commission failures to occur. In this paper, we propose a family of 2PC protocols that tolerate any number of omission failures at trusted nodes and any number of commission and omission failures at nontrusted nodes. The proposed protocols ensure that (at least) the trusted nodes participating in a transaction eventually terminate the transaction in a consistent manner. Unlike Byzantine commit protocols, our protocols do not incorporate mechanisms for achieving Byzantine agreement, which has advantages in terms of complexity: Our protocols have the same or only a slightly higher message complexity than traditional 2PC protocols. © 1993, ACM. All rights reserved.",commit protocols; open systems,Algorithms; Data communication systems; Database systems; Reliability; Byzantine agreement; Commission failures; Nontrusted nodes; Open commit protocols; Open systems; Trusted nodes; Network protocols
Empirical Performance Evaluation of Concurrency and Coherency Control Protocols for Database Sharing Systems,1993,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0027608338&doi=10.1145%2f151634.151639&partnerID=40&md5=f575fbe77cd442e833fc51bbeaac1a4a,"Database Sharing 1993 refers to a general approach for building a distributed high performance transaction system. The nodes of a DB-sharing system are locally coupled via a high-speed interconnect and share a common database at the disk level. This is also known as a “shared disk” approach. We compare database sharing with the database partitioning (shared nothing) approach and discuss the functional DBMS components that require new and coordinated solutions for DB-sharing. The performance of DB-sharing systems critically depends on the protocols used for concurrency and coherency control. The frequency of communication required for these functions has to be kept as low as possible in order to achieve high transation rates and short response times. A trace-driven simulation system for DB-sharing complexes has been developed that allows a realistic performance comparison of four different concurrency and coherency control protocols. We consider two locking and two optimistic schemes which operate either under central or distributed control. For coherency control, we investigate so-called on-request and broadcast invalidation schemes, and employ buffer-to-buffer communication to exchange modified pages directly between different nodes. The performance impact of random routing versus affinity-based load distribution and different communication costs is also examined. In addition, we analyze potential performance bottlenecks created by hot spot pages. © 1993, ACM. All rights reserved.",coherency control; concurrency control; database partitioning; database sharing; performance analysis; shared disk; shared nothing; trace-driven simulation,Control systems; Data communication systems; Network protocols; Performance; Coherency control protocols; Concurrency control protocols; Database sharing systems; Distributed control; Empirical performance evaluation; Database systems
Stochastic Query Optimization in Distributed Databases,1993,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0027608339&doi=10.1145%2f151634.151637&partnerID=40&md5=b726842277a3414c69c88821eb07b1f2,"Many algorithms have been devised for minimizing the costs associated with obtaining the answer to a single, isolated query in a distributed database system. However, if more than one query may be processed by the system at the same time and if the arrival times of the queries are unknown, the determination of optimal query-processing strategies becomes a stochastic optimization problem. In order to cope with such problems, a theoretical state-transition model is presented that treats the system as one operating under a stochastic load. Query-processing strategies may then be distributed over the processors of a network as probability distributions, in a manner which accommodates many queries over time. It is then shown that the model leads to the determination of optimal query-processing strategies as the solution of mathematical programming problems, and analytical results for several examples are presented. Furthermore, a divide-and-conquer approach is introduced for decomposing stochastic query optimization problems into distinct subproblems for processing queries sequentially and in parallel. © 1993, ACM. All rights reserved.",distributed query processing; state-transition model; stochastic query optimization,Algorithms; Computer networks; Decomposition; Mathematical programming; Optimization; Probability; Problem solving; Query processing strategies; State transition models; Stochastic query optimization; Subproblems; Distributed database systems
Performance Analysis of File Organizations that Use Multibucket Data Leaves with Partial Expansions,1993,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0027557144&doi=10.1145%2f151284.151289&partnerID=40&md5=b47fd896350f025841eca3cca9ef00ae,"We present an exact performance analysis, under random insertions, of file organizations that use multibucket data leaves and perform partial expansions before splitting. We evaluate the expected disk space utilization of the file and show how the expected search and insert costs can be estimated. The analytical results are confirmed by simulations. The analysis can be used to investigate both the dynamic and the asymptotic behaviors. © 1993, ACM. All rights reserved.",bounded disorder files; multibucket data leaves; partial expansion; performance analysis; search structures,Computer simulation; Data storage equipment; Data structures; Performance; Storage allocation (computer); Bounded disorder files; Data space utilization; Multibucket data leaves; Partial expansions; Search structures; File organization
The logical data model,1993,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0027657377&doi=10.1145%2f155271.155274&partnerID=40&md5=8b010609accc3625458d24f3c2723a37,"We propose an object-oriented data model that generalizes the relational, hierarchical, and network models. A database scheme in this model is a directed graph, whose leaves represent data and whose internal nodes represent connections among the data. Instances are constructed from objects, which have separate names and values. We define a logic for the model, and describe a nonprocedural query language that is based on the logic. We also describe an algebraic query language and show that it is equivalent to the logical language. © 1993, ACM. All rights reserved.",Algebra; database schema; logic; relational database; tuple calculus,Algebra; Computational methods; Data structures; Formal languages; Formal logic; Graph theory; Hierarchical systems; Logic programming; Object oriented programming; Programming theory; Query languages; Database schema; Logical data model; Object oriented data model; Tuple calculus; Relational database systems
A Method for Automatic Rule Derivation to Support Semantic Query Optimization,1992,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0026989423&doi=10.1145%2f146931.146932&partnerID=40&md5=d4a93e90d39fbb7659837e60f6d63216,"The use of inference rules to support intelligent data processing is an increasingly important tool in many areas of computer science. In database systems, rules are used in semantic query optimization as a method for reducing query processing costs. The savings is dependent on the ability of experts to supply a set of useful rules and the ability of the optimizer to quickly find the appropriate transformations generated by these rules. Unfortunately, the most useful rules are not always those that would or could be specified by an expert. This paper describes the architecture of a system having two interrelated components: a combined conventional/semantic query optimizer, and an automatic rule deriver. Our automatic rule derivation method uses intermediate results from the optimization process to direct the search for learning new rules. Unlike a system employing only user-specified rules, a system with an automatic capability can derive rules that may be true only in the current state of the database and can modify the rule set to reflect changes in the database and its usage pattern. This system has been implemented as an extension of the EXODUS conventional query optimizer generator. We describe the implementation, and show how semantic query optimization is an extension of conventional optimization in this context. © 1992, ACM. All rights reserved.",integrity constraint; learning; transformation heuristic,Computational linguistics; Constraint theory; Data processing; Database systems; Formal languages; Heuristic methods; Inference engines; Learning systems; Optimization; Program processors; Automatic rule derivation; EXODUS conventional query optimizer generator; Integrity constraint; Semantic query optimization; Transformation heuristic; Query languages
Updating Relational Databases Through Weak Instance Interfaces,1992,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0026962259&doi=10.1145%2f146931.146936&partnerID=40&md5=64b2820801df2c6c10fa39627ba0c803,"The problem of updating databases through interfaces based on the weak instance model is studied, thus extending previous proposals that considered them only from the query point of view. Insertions and deletions of tuples are considered. As a preliminary tool, a lattice on states is defined, based on the information content of the various states. Potential results of an insertion are states that contain at least the information in the original state and that in the new tuple. Sometimes there is no potential result, and in the other cases there may be many of them. We argue that the insertion is deterministic if the state that contains the information common to all the potential results 1992 is a potential result itself. Effective characterizations for the various cases exist. A symmetric approach is followed for deletions, with fewer cases, since there are always potential results; determinism is characterized as a consequence. © 1992, ACM. All rights reserved.",,Algorithms; Data processing; User interfaces; Database updating; Lattice of states; Tuples; Weak instance model; Relational database systems
Automatic Deduction of Temporal Information,1992,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0027004846&doi=10.1145%2f146931.146934&partnerID=40&md5=fa83cf8597cb40b5b62e2843697c1dc6,"In many computer-based applications, temporal information has to be stored, retrieved, and related to other temporal information. Several time models have been proposed to manage temporal knowledge in the fields of conceptual modeling, database systems, and artificial intelligence. In this paper we present TSOS, a system for reasoning about time that can be integrated as a time expert in environments designed for broader problem-solving domains. The main intended goal of TSOS is to allow a user to infer further information on the temporal data stored in the database through a set of deduction rules handling various aspects of time. For this purpose, TSOS provides the capability of answering queries about the temporal specifications it has in its temporal database. Distinctive time-modeling features of TSOS are the introduction of temporal modalitites, i.e., the possibility of specifying if a piece of information is always true within a time interval, or if it is only sometimes true, and the capability of answering about the possibility and the necessity of the validity of some information at a given time, the association of temporal knowledge both to instances of data and to types of data, and the development of a time calculus for reasoning on temporal data. Another relevant feature of TSOS is the capability to reason about temporal data specified at different time granularities. © 1992, ACM. All rights reserved.",events; meta-level temporal assertions; propositions; temporal database; temporal modalities; time calculus,Artificial intelligence; Data structures; Database systems; Expert systems; Information management; Query languages; Events; Metalevel temporal assertions; Propositions; Temporal database; Temporal modalities; Time calculus; TSOS time modeling system; Data processing
The Generalized Tree Quorum Protocol: An Efficient Approach for Managing Replicated Data,1992,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0027002360&doi=10.1145%2f146931.146935&partnerID=40&md5=2ac80db48918e865402d143afafdfd6f,"In this paper, we present a low-cost fault-tolerant protocol for managing replicated data. We impose a logical tree structure on the set of copies of an object and develop a protocol that uses the information available in the logical structure to reduce the communication requirements for read and write operations. The tree quorum protocol is a generalization of the static voting protocol with two degrees of freedom for choosing quorums. In general, this results in significantly lower communication costs for comparable data availability. The protocol exhibits the property of graceful degradation, i.e., communication costs for executing operations are minimal in a failure-free environment but may increase as failures occur. This approach in designing distributed systems is desirable since it provides fault-tolerance without imposing unnecessary costs on the failure-free mode of operations. © 1992, ACM. All rights reserved.",,Algorithms; Computer system recovery; Data communication systems; Data processing; Data structures; Fault tolerant computer systems; Network protocols; Trees (mathematics); Data availability; Generalized tree quorum protocol; Graceful degradation; Replicated data management; Distributed database systems
Intelligent Database Caching Through the Use of Page-Answers and Page-Traces,1992,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0026981183&doi=10.1145%2f146931.146933&partnerID=40&md5=e3860dd631781af8c5dde3c89f396ef5,"In this paper a new method to improve the utilization of main memory systems is presented. The new method is based on prestoring in main memory a number of query answers, each evaluated out of a single memory page. To this end, the ideas of page-answers and page-traces are formally described and their properties analyzed. The query model used here allows for selection, projection, join, recursive queries as well as arbitrary combinations. We also show how to apply the approach under update traffic. This concept is especially useful in managing the main memories of an important class of applications. This class includes the evaluation of triggers and alerters, performance improvement of rule-based systems, integrity constraint checking, and materialized views. These applications are characterized by the existence at compile time of a predetermined set of queries, by a slow but persistent update traffic, and by their need to repetitively reevaluate the query set. The new approach represents a new type of intelligent database caching, which contrasts with traditional caching primarily in that the cache elements are derived data and as a consequence, they overlap arbitrarily and do not have a fixed length. The contents of the main memory cache are selected based on the data distribution within the database, the set of fixed queries to preprocess, and the paging characteristics. Page-answers and page-traces are used as the smallest indivisible units in the cache. An efficient heuristic to select a near optimal set of page-answers and page-traces to populate the main memory has been developed, implemented, and tested. Finally, quantitative measurements of performance benefits are reported. © 1992, ACM. All rights reserved.",artificial intelligence; databases; page access,Algorithms; Artificial intelligence; Data processing; Heuristic methods; Optimization; Performance; Program compilers; Query languages; Random access storage; Recursive functions; Intelligent database caching; Page access; Page answers; Page traces; Database systems
Performance Evaluation of Cautious Waiting,1992,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0026916581&doi=10.1145%2f132271.132275&partnerID=40&md5=640ba3e8def73c559dd770b212af46b5,"We study a deadlock-free locking-based concurrency control algorithm, called cautious waiting, which allows for a limited form of waiting. The algorithm is very simple to implement. We present an analytical solution to its performance evaluation based on the mean-value approach proposed by Tay et al. [18]. From the modeling point of view, we are able to do away with a major assumption used in Tay's previous work, and therefore capture more accurately both the restart and the blocking rates in the system. We show that to solve for this model we only need to solve for the root of a polynomial. The analytical tools developed enable us to see that the cautious waiting algorithm manages to achieve a delicate balance between restart and blocking, and therefore is superior 1992 and the general waiting algorithms under a wide range of system parameters. The study substantiates the argument that balancing restart and blocking is important in locking systems. © 1992, ACM. All rights reserved.",cautious waiting; concurrency control,Algorithms; Computational methods; Data processing; Performance; Polynomials; Cautious waiting; Concurrency control; Distributed database systems
Concurrency Control for High Contention Environments,1992,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0026870277&doi=10.1145%2f128903.128906&partnerID=40&md5=f12a28653fe85c27a448f12ad09ba23a,"Future transaction processing systems may have substantially higher levels of concurrency due to reasons which include: 1992 increasing disparity between processor speeds and data access latencies, (2) large numbers of processors, and (3) distributed databases. Another influence is the trend towards longer or more complex transactions. A possible consequence is substantially more data contention, which could limit total achievable throughput. In particular, it is known that the usual locking method of concurrency control is not well suited to environments where data contention is a significant factor. Here we consider a number of concurrency control concepts and transaction scheduling techniques that are applicable to high contention environments, and that do not rely on database semantics to reduce contention. These include access invariance and its application to prefetching of data, approximations to essential blocking such as wait depth limited scheduling, and phase dependent control. The performance of various concurrency control methods based on these concepts are studied using detailed simulation models. The results indicate that the new techniques can offer substantial benefits for systems with high levels of data contention. © 1992, ACM. All rights reserved.",concurrency control; transaction processing,Algorithms; Computer operating systems; Data processing; Invariance; Scheduling; Access invariance; Concurrency control; Data contention; Essential blocking; Phase dependent control; Transaction processing; Wait depth limited scheduling; Distributed database systems
Optimal Weight Assignment for Signature Generation,1992,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0026870276&doi=10.1145%2f128903.128907&partnerID=40&md5=2e7b19215b9f69a94a312b653edd9cee,"Previous work on superimposed coding has been characterized by two aspects. First, it is generally assumed that signatures are generated from logical text blocks of the same size; that is, each block contains the same number of unique terms after stopword and duplicate removal. We call this approach the fixed-size block 1992 method, since each text block has the same size, as measured by the number of unique terms contained in it. Second, with only a few exceptions [6,7,8,9,17], most previous work has assumed that each term in the text contributes the same number of ones to the signature (i.e., the weight of the term signatures is fixed). The main objective of this paper is to derive an optimal weight assignment that assigns weights to document terms according to their occurrence and query frequencies in order to minimize the false-drop probability. The optimal scheme can account for both uniform and nonuniform occurence and query frequencies, and the signature generation method is still based on hashing rather than on table lookup. Furthermore, a new way of generating signatures, the fixed-weight block (FWB) method, is introduced. FWB controls the weight of every signature to a constant, whereas in FSB, only the expected signature weight is constant. We have shown that FWB has a lower false-drop probability than that of the FSB method, but its storage overhead is slightly higher. Other advantages of FWB are that the optimal weight assignment can be obtained analytically without making unrealistic assumptions and that the formula for computing the term signature weights is simple and efficient. © 1992, ACM. All rights reserved.",access method; coding methods; document retrieval; information retrieval; optimization; signature file; superimposed coding; text retrieval,Algorithms; Codes (symbols); Computational methods; Data processing; Information retrieval; Optimization; Access method; Fixed size block (FSB) method; Fixed weight block (FWB) method; Optimal weight assignment; Signature file; Superimposed coding; Text processing; Encoding (symbols)
Rule-based optimization and query processing in an extensible geometric database system,1992,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0026870396&doi=10.1145%2f128903.128905&partnerID=40&md5=45fdb95624226a3a3113d76b71eefd90,"Gral is an extensible database system, based on the formal concept of a many-sorted relational algebra. Many-sorted algebra is used to define any application's query language, its query execution language, and its optimiztion rules. In this paper we describe Gral's optimization component. It provides 1992 a sophisticated rule language—rules are transformations of abstract algebra expressions, (2) a general optimization framework under which more specific optimization algorithms can be implemented, and (3) several control mechanisms for the application of rules. An optimization algorithm can be specified as a series of steps. Each step is defined by its own collection of rules together with a selected control strategy. The general facilities are illustrated by the complete design of an example optimizer—in the form of a rule file—for a small nonstandard query language and an associated execution language. The query language includes selection, join, ordering, embedding derived values, aggregate functions, and several geometric operations. The example shows in particular how the special processing techniques of a geometric database systems, such as spatial join methods and geometric index structures, can be integrated into query processing and optimization of a relational database system. A similar, though larger, optimizer is fully functional within the geometric database system implemented as a Gral prototype. © 1992, ACM. All rights reserved.",extensibility; geometric query processing; many-sorted algebra; optimization; relational algebra; rule-based optimization,Algebra; Data processing; Knowledge based systems; Optimization; Query languages; Extensibility; Geometric query processing; Gral database system; Many-sorted algebra; Relational algebra; Rule based optimization; Relational database systems
Constant-Time Maintainability: A Generalization of Independence,1992,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0026875739&doi=10.1145%2f128903.128904&partnerID=40&md5=83a9dafcdf40ab99c0735ec9f69b4c08,"The maintenance problem of a database scheme is the following decision problem: Given a consistent database state ρ and a new tuple u over some relation scheme of ρ, is the modified state ρ ∪ {u} still consistent? A database scheme is said to be constant-time-maintainable1992 if there exists an algorithm that solves its maintenance problem by making a fixed number of tuple retrievals. We present a practically useful algorithm, called the canonical maintenance algorithm, that solves the maintenance problem of all ctm database schemes within a “not too large” bound. A number of interesting properties are shown for ctm database schemes, among them that non-ctm database schemes are not maintainable in less than a linear time in the state size. A test method is given when only cover embedded functional dependencies (fds) appear. When the given dependencies consist of fds and the join dependency (jd) ⋈ R of the database scheme, testing whether a database scheme is ctm is reduced to the case of cover embedded fds. When dependency-preserving database schemes with only equality-generating dependencies (egds) are considered, it is shown that every ctm database scheme has a set of dependencies that is equivalent to a set of embedded fds, and thus, our test method for the case of embedded fds can be applied. In particular, this includes the important case of lossless database schemes with only egds. © 1992, ACM. All rights reserved.",chase; constraint enforcement; functional dependency; independent database schemes; join dependency; lossless join; relational database; representative instance; tableau,Algorithms; Data processing; Maintainability; Chase; Constant time maintainability; Constraint enforcement; Functional dependence; Independent database schemes; Join dependency; Lossless join; Representative instance; Tableau; Database systems
Scheduling Real-Time Transactions: A Performance Evaluation,1992,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0026913684&doi=10.1145%2f132271.132276&partnerID=40&md5=6e035ac6c663c456b7d141522448d3f0,"Managing transactions with real-time requirements presents many new problems. In this paper we address several: How can we schedule transactions with deadlines? How do the real-time constraints affect concurrency control? How should overloads be handled? How does the scheduling of I/O requests affect the timeliness of transactions? How should exclusive and shared locking be handled? We describe a new group of algorithms for scheduling real-time transactions that produce serializable schedules. We present a model for scheduling transactions with deadlines on a single processor disk resident database system, and evaluate the scheduling algorithms through detailed simulation experiments. © 1992, ACM. All rights reserved.",deadlines; locking protocols; real-time systems,Algorithms; Computer simulation; Data processing; Information management; Network protocols; Optimization; Performance; Real time systems; Scheduling; Deadlines; Locking protocols; Transaction processing; Distributed database systems
On Taxonomic Reasoning in Conceptual Design,1992,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0026915553&doi=10.1145%2f132271.132272&partnerID=40&md5=650ab50d1f30b78c673d2adce5593255,"Taxonomic reasoning is a typical task performed by many AI knowledge representation systems. In this paper, the effectiveness of taxonomic reasoning techniques as an active support to knowledge acquisition and conceptual schema design is shown. The idea developed is that by extending conceptual models with defined concepts and giving them rigorous logic semantics, it is possible to infer isa relationships between concepts on the basis of their descriptions. From a theoretical point of view, this approach makes it possible to give a formal definition for consistency and minimality of a conceptual schema. From a pragmatic point of view it is possible to develop an active environment that allows automatic classification of a new concept in the right position of a given taxonomy, ensuring the consistency and minimality of a conceptual schema. A formalism that includes the data semantics of models giving prominence to type constructors 1992 and algorithms for taxonomic inferences are presented: their soundness, completeness, and tractability properties are proved. Finally, an extended formalism and taxonomic inference algorithms for models giving prominence to attributes (FDM, IFO) are given. © 1992, ACM. All rights reserved.",schema consistency; schema minimality; semantic models; taxonomic reasoning,Algorithms; Artificial intelligence; Classification (of information); Computational linguistics; Formal languages; Formal logic; Conceptual design; Schema consistency; Schema minimality; Semantic models; Taxonomic reasoning; Database systems
Representing Extended Entity-Relationship Structures in Relational Databases: A Modular Approach,1992,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0026913249&doi=10.1145%2f132271.132273&partnerID=40&md5=97948faa36501036eef6b9f2f4250a0b,"A common approach to database design is to describe the structures and constraints of the database application in terms of a semantic data model, and then represent the resulting schema using the data model of a commercial database management system. Often, in practice, Extended Entity-Relationship 1992 schemas are translated into equivalent relational schemas. This translation involves different aspects: representing the EER schema using relational constructs, assigning names to relational attributes, normalization, and merging relations. Considering these aspects together, as is usually done in the design methodologies proposed in the literature, is confusing and leads to inaccurate results. We propose to treat separately these aspects and split the translation into four stages (modules) corresponding to the four aspects mentioned above. We define criteria for both evaluating the correctness of and characterizing the relationship between alternative relational representations of EER schemas. © 1992, ACM. All rights reserved.",database design; extended entity-relationship model; relational data model; schema translation; semantic data model,Algorithms; Computational linguistics; Constraint theory; Data description; Data structures; Formal languages; Program translators; Extended entity relationship model; Relational data model; Schema translation; Semantic data model; Relational database systems
"On Roth, Korth, and Silberschatz's extended algebra and calculus for nested relational databases",1992,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0026875023&doi=10.1145%2f128903.128908&partnerID=40&md5=a613392c689c8eab4b56122d6745d1cc,"We discuss the issues encountered in the extended algebra and calculus languages for nested relations defined by Roth, Korth, and Silberschatz.[4]. Their equivalence proof between algebra and calculus fails because of the keying problems and the use of extended set operations. Extended set operations also have unintended side effects. Furthermore, their calculus seems to allow the generation of power sets, thus making it more powerful than their algebra. © 1992, ACM. All rights reserved.",equivalence of algebra and calculus; nested relations; relational algebra; relational calculus,Algebra; Computer programming languages; Set theory; Theorem proving; Equivalence of algebra and calculus; Nested relations; Relational algebra; Relational calculus; Relational database systems
Simple Conditions for Guaranteeing Higher Normal Forms in Relational Databases,1992,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0026912035&doi=10.1145%2f132271.132274&partnerID=40&md5=34da3ce2f8e62bf6027ba5bc0c27449c,"A key is simple if it consists of a single attribute. It is shown that if a relation schema is in third normal form and every key is simple, then it is in projection-join normal form 1992, the ultimate normal form with respect to projections and joins. Furthermore, it is shown that if a relation schema is in Boyce-Codd normal form and some key is simple, then it is in fourth normal form (but not necessarily projection-join normal form). These results give the database designer simple sufficient conditions, defined in terms of functional dependencies alone, that guarantee that the schema being designed is automatically in higher normal forms. © 1992, ACM. All rights reserved.",5NF; BCNF; Boyce-Codd normal form; database design; fifth normal form; fourth normal form 4NF; functional dependency; join dependency; multivalued dependency; normalization; PJ/NF; projection-join normal form; relational database; simple key,Computational linguistics; Data processing; Design; Boyce Codd normal form; Fifth normal form; Fourth normal form; Functional dependency; Join dependency; Multivalued dependency; Normalization; Projection join normal form; Simple key; Relational database systems
Distributive join: A New Algorithm for Joining Relations,1991,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0026303078&doi=10.1145%2f115302.115299&partnerID=40&md5=ae53fcc41086ef20ef08806bc447279e,[No abstract available],buffer; hashing; join; merging scan; nested scan; sort,Algorithms; Data handling; Merging; Sorting; Buffer; Distributive join algorithm; Hashing; Joins; Merging scan algorithm; Nested scan; Relational database systems
Semantics-Based Concurrency Control: Beyond Commutativity,1992,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0026828686&doi=10.1145%2f128765.128771&partnerID=40&md5=e388ae0d5418266b62709affdf56b128,"The concurrency of transactions executing on atomic data types can be enhanced through the use of semantic information about operations defined on these types. Hitherto, commutativity of operations has been exploited to provide enchanced concurrency while avoiding cascading aborts. We have identified a property known as recoverability which can be used to decrease the delay involved in processing noncommuting operations while still avoiding cascading aborts. When an invoked operation is recoverable with respect to an uncommitted operation, the invoked operation can be executed by forcing a commit dependency between the invoked operation and the uncommitted operation; the transaction invoking the operation will not have to wait for the uncommitted operation to abort or commit. Further, this commit dependency only affects the order in which the operations should commit, if both commit; if either operation aborts, the other can still commit thus avoiding cascading aborts. To ensure the serializability of transactions, we force the recoverability relationship between transactions to be acyclic. Simulation studies, based on the model presented by Agrawal et al. [1], indicate that using recoverability, the turnaround time of transactions can be reduced. Further, our studies show enchancement in concurrency even when resource constraints are taken into consideration. The magnitude of enchancement is dependent on the resource contention; the lower the resource contention, the higher the improvement. © 1992, ACM. All rights reserved.",concurrency control; semantic information,Algorithms; Computational linguistics; Computational methods; Computer simulation; Computer system recovery; Constraint theory; Data processing; Optimization; Commutativity; Concurrency control; Recoverability; Resource constraints; Semantic information; Transaction processing; Distributed database systems
Converting Nested Algebra Expressions into Flat Algebra Expressions,1992,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0026822475&doi=10.1145%2f128765.128768&partnerID=40&md5=65b9b90e26a08f79926305eb8c56f8af,"Nested relations generalize ordinary flat relations by allowing tuple values to be either atomic or set valued. The nested algebra is a generalization of the flat relational algebra to manipulate nested relations. In this paper we study the expressive power of the nested algebra relative to its operation on flat relational databases. We show that the flat relational algebra is rich enough to extract the same “flat information” from a flat database as the nested algebra does. Theoretically, this result implies that recursive queries such as the transitive closure of a binary relation cannot be expressed in the nested algebra. Practically, this result is relevant to 1992 relational query optimization. © 1992, ACM. All rights reserved.",algebraic query transformation; nested algebra; nested calculus; nested relations; relational databases,Algebra; Algorithms; Data processing; Information retrieval; Mathematical models; Optimization; Query languages; Recursive functions; Algebraic query transformation; Flat relational databases; Nested algebra; Nested calculus; Nested relations; Tuples; Relational database systems
Constant-Time-Maintainable BCNF Database Schemes,1991,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0026274557&doi=10.1145%2f115302.115301&partnerID=40&md5=dfae325131f854cb93d1db82bd573e1b,"The maintenance problem (for database states) ofa database scheme Rwithrespect toa set of functional dependencies Fisthefollowing decision problem. Letrbea consistent state of Rwith respect to F and assume we insert a tuple t into rP∈r. Is r ∪ {t}a consistent state of R with respect to F? R is said to be constant-time-maintainable with respect to F if there is an algorithm that solves the maintenance problem of R with respect to F in time independent of the state size. A characterization of constant-time-maintainability for the class of BCNF database schemes is given. Inefficient algorithm that tests this characterization is shown, as well as an algorithm for solving the maintenance problem in time independent of the state size. It is also shown that total projections of the representative instance can be computed via unions of projections of sequential extension joins. Throughout we assume that database schemes are dependency preserving and BCNF, and that functional dependencies are given intheform of key dependencies. © 1991, ACM. All rights reserved.",boundedness; constraint enforcement; data dependencies; query processing,Algorithms; Computability and decidability; Computational complexity; Constraint theory; Data handling; Data processing; Design; Logic programming; Security of data; BCNF database schemes; Boundedness; Constant time maintainability; Constraint enforcement; Data dependencies; Query processing; Database systems
On Robust Transaction Routing and Load Sharing,1991,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0026217767&doi=10.1145%2f111197.111210&partnerID=40&md5=bd160bcb5f161c88d17490b16112585c,"In this paper we examine the issue of robust transaction routing in a locally distributed database environment where transaction characteristics such as reference locality imply that certain processing systems can be identified as being more suitable than others for a given transaction class. A response time based routing strategy can strike a balance between indiscriminate sharing of the load and routing based only on transaction affinity. Since response time estimates depend on workload and system parameters that may not be readily available, it is important to examine the robustness of routing decisions to information accuracy. We find that a strategy which strictly tries to minimize the response time of incoming transactions is sensitive to the accuracy of certain parameter values. On the other hand, naive strategies, that simply ignore the parameters in making routing decisions, have even worse performance. Three alternative strategies are therefore examined: threshold, discriminatory, and adaptive. Instead of just optimizing an incoming transaction's response time, the first two strategies pursue a strategy that is somewhat more oriented towards global optimization. This is achieved by being more restrictive on either the condition or the candidate for balancing the load. The third strategy, while trying to minimize the response time of individual incoming transactions, employs a feedback process to adaptively adjust future response time estimates. It monitors the discrepancy between the actual and estimated response times and introduces a correction factor based on regression analysis. All three strategies are shown to be robust with respect to the accuracy of workload and system parameters used in the response time estimation. © 1991, ACM. All rights reserved.",distributed database; load balancing; performance analysis; transaction routing,Computer operating systems; Digital signal processing; Distributed computer systems; Information theory; Optimization; Performance; Queueing theory; Response time (computer systems); Adaptive strategy; Discriminatory strategy; Load sharing; Threshold strategy; Transaction routing; Distributed database systems
Towards a Semantic View of an Extended Entity-Relationship Model,1991,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0026217768&doi=10.1145%2f111197.111200&partnerID=40&md5=66e06bf5e8889dd276de15c093809637,"Nearly all query languages discussed recently for the Entity-Relationship 1991 model do not possess a formal semantics. Languages are often defined by means of examples only. The reason for this phenomenon is the essential gap between features of query languages and theoretical foundations like algebras and calculi. Known languages offer arithmetic capabilities and allow for aggregates, but algebras and calculi defined for ER models do not. This paper introduces an extended ER model concentrating nearly all concepts of known so-called semantic data models in a few syntactical constructs. Moreover, we provide our extended ER model with a formal mathematical semantics. On this basis a well-founded calculus is developed taking into account data operations on arbitrary user-defined data types and aggregate functions. We pay special attention to arithmetic operations, as well as multivalued terms allowing nested queries, in a uniform and consistent manner. We prove our calculus only allows the formulation of safe terms and queries yielding a finite result, and to be (at least) as expressive as the relational calculi. © 1991, ACM. All rights reserved.",abstract data type; aggregate function; calculus; entity-relationship model; formal semantics; relational completeness; safeness; semantic data model,Data description; Data structures; Formal logic; Mathematical techniques; Models; Programming theory; Query languages; Abstract data type; Aggregate function; Calculus; Entity relationship (ER) model; Formal semantics; Relational completeness; Safeness; Semantic data model; Relational database systems
Formal Semantics of SQL Queries,1991,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0026213846&doi=10.1145%2f111197.111212&partnerID=40&md5=000841129c0e59deb814e61839661b91,"The semantics of SQL queries is formally defined by stating a set of rules that determine a syntax-driven translation of an SQL query to a formal model. The target model, called Extended Three Valued Predicate Calculus 1991, is largely based on a set of well-known mathematical concepts. The rules which allow the transformation of a general E3VPC expression to a Canonical Form, which can be manipulated using traditional, two-valued predicate calculus are also given; in this way, problems like equivalence analysis of SQL queries are completely solved. Finally, the fact that reasoning about the equivalence of SQL queries using two-valued predicate calculus, without taking care of the real SQL semantics can lead to errors is shown, and the reasons for this are analyzed. © 1991, ACM. All rights reserved.",query equivalence; query semantics; SQL,Computational linguistics; Data processing; Equivalence classes; Formal languages; Logic programming; Many valued logics; Mathematical transformations; Relational database systems; Canonical form; Extended three valued predicate calculus (E3VPC); Query equivalence; Query semantics; Structured query language (SQL); Query languages
ARIES: A Transaction Recovery Method Supporting Fine-Granularity Locking and Partial Rollbacks Using Write-Ahead Logging,1992,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0026822597&doi=10.1145%2f128765.128770&partnerID=40&md5=3f0664bfedfbba7153474e179cebcf1c,"DB2TM, IMS, and TandemTM systems. ARIES is applicable not only to database management systems but also to persistent object-oriented languages, recoverable file systems and transaction-based operating systems. ARIES has been implemented, to varying degrees, in IBM's OS/2TM Extended Edition Database Manager, DB2, Workstation Data Save Facility/VM, Starburst and QuickSilver, and in the University of Wisconsin's EXODUS and Gamma database machine. © 1992, ACM. All rights reserved.",buffer management; latching; locking; space management; write-ahead logging,Algorithms; Computer system recovery; Data processing; Fuzzy sets; Information management; Object oriented programming; Parallel processing systems; Performance; Reliability; Storage allocation (computer); Algorithm for recovery and isolation exploiting semantics (ARIES); Buffer management; Fine granularity locking; Latching; Partial rollbacks; Space management; Transaction recovery method; Write ahead logging; Distributed database systems
Conflict Detection Tradeoffs for Replicated Data,1991,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0026279530&doi=10.1145%2f115302.115289&partnerID=40&md5=8bf5be6014b4f4fd487ea56fbe0a3f5c,[No abstract available],concurrency control; replicated data,Algorithms; Computer networks; Control; Data communication systems; Data structures; Performance; Concurrency control; Replicated data; Update transactions; Distributed database systems
"An Incremental Access Method for ViewCache: Concept, Algorithms, and Cost Analysis",1991,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0026213736&doi=10.1145%2f111197.111215&partnerID=40&md5=fd886f79ff3921bcebe3d32489a0a1d3,"A ViewCache is a stored collection of pointers pointing to records of underlying relations needed to materialize a view. This paper presents an Incremental Access Method 1991 that amortizes the maintenance cost of ViewCaches over a long time period or indefinitely. Amortization is based on deferred and other update propagation strategies. A deferred update strategy allows a ViewCache to remain outdated until a query needs to selectively or exhaustively materialize the view. At that point, an incremental update of the ViewCache is performed. This paper defines a set of conditions under which incremental access to the ViewCache is cost effective. The decision criteria are based on some dynamically maintained cost parameters, which provide accurate information but require inexpensive bookkeeping. The IAM capitalizes on the ViewCache storage organization for performing the update and the materialization of the ViewCaches in an interleaved mode using one-pass algorithms. Compared to the standard technique for supporting views that requires reexecution of the definition of the view, the IAM offers significant performance advantages. We will show that under favorable conditions, most of which depend on the size of the incremental update logs between consecutive accesses of the views, the incremental access method outperforms query modification. Performance gains are higher for multilevel ViewCaches because all the I/O and CPU for handling intermediate results are avoided. © 1991, ACM. All rights reserved.",terms,Algorithms; Data transfer; Performance; Query languages; Relational database systems; Self organizing storage; Incremental access method (IAM); Joint index; Pointer cache; Query optimization; Subsumption; View index; ViewCache; Information retrieval
Reactive Consistency Control in Deductive Databases,1991,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0026263192&doi=10.1145%2f115302.115298&partnerID=40&md5=ab26ebae8dfc398ec594feda651d5f14,[No abstract available],,Artificial intelligence; Constraint theory; Data handling; Formal logic; Heuristic methods; Reliability; Security of data; Theorem proving; Deductive databases; Integrity constraints; Database systems
Object Operations Benchmark,1992,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0026824633&doi=10.1145%2f128765.128766&partnerID=40&md5=843a1537ec7b95bb3d54e3f4e0e2da3e,"Performance is a major issue in the acceptance of object-oriented and relational database systems aimed at engineering applications such as computer-aided software engineering 1992 and computer-aided design (CAD). Because traditional database systems benchmarks are inapproriate to measure performance for operations on engineering objects, we designed a new benchmark Object Operations version 1 (OO1) to focus on important characteristics of these applications. OO1 is descended from an earlier benchmark for simple database operations and is based on several years experience with that benchmark. In this paper we describe the OO1 benchmark and results we obtained running it on a variety of database systems. We provide a careful specification of the benchmark, show how it can be implemented on database systems, and present evidence that more than an order of magnitude difference in performance can result from a DBMS implementation quite different from current products; minimizing overhead per database call, offloading database server functionality to workstations, taking advantage of large main memories, and using link-based methods. © 1992, ACM. All rights reserved.",CAD; CASE; client-server architecture; engineering database benchmark; hypermodel; object operations benchmark; object-oriented DBMS's; relation of DBMS's; workstations,Algorithms; Computer aided design; Computer aided software engineering; Computer architecture; Data structures; Object oriented programming; Performance; Query languages; Client server architecture; Hypermodel; Object operations benchmark; Object operations version 1 (OO1); Object oriented databases; Relational database systems
Simplification Rules and Complete Axiomatization for Relational Update Transactions,1991,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0026212381&doi=10.1145%2f111197.111208&partnerID=40&md5=6ba9625a16f1066414bd1b6004dd184f,"Relational update transactions consisting of line programs of inserts, deletes, and modifications are studied with respect to equivalence and simplification. A sound and complete set of axioms for proving transaction equivalence is exhibited. The axioms yield a set of simplification rules that can be used to optimize efficiently a large class of transactions of practical interest. The simplification rules are particularly well suited to a dynamic environment where transactions are presented in an on-line fashion, and where the time available for optimization may consist of arbitrarily short and sparse intervals. © 1991, ACM. All rights reserved.",performance,Algorithms; Data processing; Equivalence classes; Formal languages; Formal logic; Information retrieval; Information retrieval systems; Logic programming; Theorem proving; Axiomatization; Equivalence; Relational update transactions; Simplification; Relational database systems
A Note on Estimating the Cardinality of the Projection of a Database Relation,1991,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0026216118&doi=10.1145%2f111197.111218&partnerID=40&md5=9d37c85ed5cb6e3568c3c3551999971f,"The paper by Ahad et al. [1] derives an analytical expression to estimate the cardinality of the projection of a database relation. In this note, we propose to show that this expression is in error even when all the parameters are assumed to be constant. We derive the correct formula for this expression. © 1991, ACM. All rights reserved.",block access estimation; query cost-estimation; relational databases,Algorithms; Analysis; Data processing; Error correction; Estimation; Logic programming; Modification; Query languages; Block access estimation; Cardinality; Database relation projection; Query cost estimation; Relational database systems
Using Annotations to Support Multiple Kinds of Versioning in an Object-Oriented Database System,1991,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0026216119&doi=10.1145%2f111197.111205&partnerID=40&md5=f363bb19d408aff007693d452abc2a2f,"The concept of annotation from object-oriented languages is adapted to object-oriented databases. It is shown how annotations can be used to model activities such as constraint checking, default values, and triggers. Annotations also are an appropriate way to model different versioning concepts. This paper discusses three kinds of versioning—histories, revisions, and alternatives—and demonstrates how each one can be modeled effectively using annotations. The use of annotations also allows other kinds of versioning to be defined extensibly, and arbitrary combinations of versions can be handled easily. © 1991, ACM. All rights reserved.",configuration management; object-oriented databases; versions,Computer programming; Data structures; Models; Object oriented programming; Query languages; Alternatives; Annotations; Configuration management; Histories; Object-oriented databases; Revisions; Versioning; Database systems
Statistical Estimators for Aggregate Relational Algebra Queries,1991,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0026274506&doi=10.1145%2f115302.115300&partnerID=40&md5=695cc0e361bb10f025937e2352201151,[No abstract available],relational algebra; sampling; selectivity; simple random sampling; statistical estimators,Algebra; Algorithms; Computation theory; Estimation; Evaluation; Performance; Random processes; Sampling; Statistical methods; Aggregate relational algebra queries; Cluster sampling; Relational algebra; Simple random sampling; Statistical estimators; Relational database systems
Reasoning about Functional Dependencies Generalized for Semantic Data Models,1992,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0026822489&doi=10.1145%2f128765.128767&partnerID=40&md5=c5926ef059457e667f3be654555e4baf,"We propose a more general form of functional dependency for semantic data models that derives from their common feature in which the separate notions of domain and relation in the relational model are combined into a single notion of class. This usually results in a richer terminological component for their query languages, whereby terms may navigate through any number of properties, including none. We prove the richer expressiveness of this more general functional dependency, and exhibit a sound and complete set of inference axioms. Although the general problem of decidability of their logical implication remains open at this time, we present decision procedures for cases in which the dependencies included in a schema correspond to keys, or in which the schema itself is acyclic. The theory is then extended to include a form of conjunctive query. Of particular significance is that the query becomes an additional source of functional dependency. Finally, we outline several applications of the theory to various problems in physical design and in query optimization. The applications derive from an ability to predict when a query can have at most one solution. © 1992, ACM. All rights reserved.",constraint theory; functional dependencies; query optimization; semantic data models,Algorithms; Computability and decidability; Computational linguistics; Computational methods; Constraint theory; Decision theory; Mathematical models; Optimization; Relational database systems; Functional dependencies; Inference axioms; Query optimization; Semantic data models; Tuples; Query languages
Optimizing Equijoin Queries in Distributed Databases Where Relations are Hash Partitioned,1991,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0026168378&doi=10.1145%2f114325.103713&partnerID=40&md5=e4fc49aee755a579314cffc358f620c1,"Consider the class of distributed database systems consisting of a set of nodes connected by a high bandwidth network. Each node consists of a processor, a random access memory, and a slower but much larger memory such as a disk. There is no shared memory among the nodes. The data are horizontally partitioned often using a hash function. Such a description characterizes many parallel or distributed database systems that have recently been proposed, both commercial and academic. We study the optimization problem that arises when the query processor must repartition the relations and intermediate results participating in a multijoin query. Using estimates of the sizes of intermediate relations, we show 1991 optimum solutions for closed chain queries; (2) the NP-completeness of the optimization problem for star, tree, and general graph queries; and (3) effective heuristics for these hard cases. Our general approach and many of our results extend to other attribute partitioning schemes, for example, sort-partitioning on attributes, and to partitioned object databases. © 1991, ACM. All rights reserved.",equijoin; hashing; NP-complete problems; relational data models; spanning trees; systems,Computational complexity; Data processing; Estimation; Optimization; Query languages; Relational database systems; Storage allocation (computer); Trees (mathematics); Equijoin; Hashing; Spanning trees; Distributed database systems
A Parallel Algorithm for Record Clustering,1990,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0025595564&doi=10.1145%2f99935.99947&partnerID=40&md5=848ca20e38e92014995628c7680e9f20,"We present an efficient heuristic algorithm for record clustering that can run on a SIMD machine. We introduce the P-tree, and its associated numbering scheme, which in the split phase allows each processor independently to compute the unique cluster number of a record satisfying an arbitrary query. We show that by restricting ourselves in the merge phase to combining only sibling clusters, we obtain a parallel algorithm whose speedup ratio is optimal in the number of processors used. Finally, we report on experiments showing that our method produces substantial savings in an enviornment with relatively little overlap among the queries. © 1990, ACM. All rights reserved.",,Computer Programming--Algorithms; Computer Systems Programming--Multiprocessing Programs; Parallel Algorithms; Record Clustering; Information Science
Management of a Remote Backup Copy for Disaster Recovery,1991,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0026167985&doi=10.1145%2f114325.103715&partnerID=40&md5=6ad8397b8e420039512d0b54e793b96f,"A remote backup database system tracks the state of a primary system, taking over transaction processing when disaster hits the primary site. The primary and backup sites are physically isolated so that failures at one site are unlikely to propogate to the other. For correctness, the execution schedule at the backup must be equivalent to that at the primary. When the primary and backup sites contain a single processor, it is easy to achieve this property. However, this is harder to do when each site contains multiple processors and sites are connected via multiple communication lines. We present an efficient transaction processing mechanism for multiprocessor systems that guarantees this and other important properties. We also present a database initialization algorithm that copies the database to a backup site while transactions are being processed. © 1991, ACM. All rights reserved.",database initialization; hot spare; hot standby; remote backup,Algorithms; Computer system recovery; Copying; Data handling; Data processing; Distributed computer systems; Management; Security of data; Database initialization; Hot spare; Hot standby; Remote backup database system; Distributed database systems
A Model of Authorization for Next-Generation Database Systems,1991,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0026119329&doi=10.1145%2f103140.103144&partnerID=40&md5=67de5fc0de5124392b9bf6e9d678e199,"The conventional models of authorization have been designed for database systems supporting the hierarchical, network, and relational models of data. However, these models are not adequate for next-generation database systems that support richer data models that include object-oriented concepts and semantic data modeling concepts. Rabitti, Woelk, and Kim [14] presented a preliminary model of authorization for use as the basis of an authorization mechanism in such database systems. In this paper we present a fuller model of authorization that fills a few major gaps that the conventional models of authorization cannot fill for next-generation database systems. We also further formalize the notion of implicit authorization and refine the application of the notion of implicit authorization to object-oriented and semantic modeling concepts. We also describe a user interface for using the model of authorization and consider key issues in implementing the authorization model. © 1991, ACM. All rights reserved.",object-oriented database; semantic database,Computer Interfaces; Authorization Models; Data Models; Implicit Authorization; Object Oriented Modeling; Semantic Modeling; User Interfaces; Database Systems
Functional Dependencies in Horn Clause Queries,1991,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0026117702&doi=10.1145%2f103140.103142&partnerID=40&md5=3f2f12b5feedfaff8ce4080b3e46e7c9,"When a database query is expressed as a set of Horn clauses whose execution is by top-down resolution of goals, there is a need to improve the backtracking behavior of the interpreter. Rather than putting on the programmer the onus of using extra-logical operators such as cut to improve performance, we show that some uses of the cut can be automated by inferring them from functional dependencies. This requires some knowledge of which variables are guaranteed to be bound at query execution time; we give a method for deriving such information using data flow analysis. © 1991, ACM. All rights reserved.",data flow analysis; functional dependency; logic programming; relational database,Computer Programming--Logic Programming; Database Queries; Dataflow Analysis; Functional Dependencies; Horn Clauses; Database Systems
The Multicast Policy and Its Relationship to Replicated Data Placement,1991,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0026121916&doi=10.1145%2f103140.103146&partnerID=40&md5=e265f6a65bc1df3e032a6f53444408eb,"In this paper we consider the communication complexity of maintaining the replicas of a logical data-item, in a database distributed over a computer network. We propose a new method, called the minimum spanning tree write, by which a processor in the network should multicast a write of a logical data-item, to all the processors that store replicas of the items. Then we show that the minimum spanning tree write is optimal from the communication cost point of view. We also demonstate that the method by which a write is multicast to all the replicas of a data-item affects the optimal replication scheme of the item, i.e., at which processors in the network the replicas should be located. Therefore, next we consider the problem of determining an optimal replicaiton scheme for a data item, assuming that each processor employs the minimum spanning tree write at run-time. The problem for general networks is shown NP-Complete, but we provide efficient algorithms to obtain an optimal allocation scheme for three common types of network topologies. They are completely-connected, tree, and ring networks. For these topologies, efficient algorithms are also provided for the case in which reliability considerations dictate a minimum number of replicas. © 1991, ACM. All rights reserved.",complexity; computer network; file allocation; message passing; NP-Complete,Computer Networks; Computer Programming--Algorithms; Communication Complexity; Minimum Spanning Tree Write; Multicasting; Replicated Data Placement; Database Systems
Concepts and Effectiveness of the Cover-Coefficient-Based Clustering Methodology for Text Databases,1990,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0025597381&doi=10.1145%2f99935.99938&partnerID=40&md5=ec96452b726a1cbfddc4d8e382c11899,"A new algorithm for document clustering is introduced. The base concept of the algorithm, the cover coefficient 1990 concept, provides a means of estimating the number of clusters within a document database and related indexing and clustering analytically. The CC concept is used also to identify the cluster seeds and to form clusters with these seeds. It is shown that the complexity of the clustering process is very low. The retrieval experiments show that the information-retrieval effectiveness of the algorithm is compatible with a very demanding complete linkage clustering method that is known to have good retrieval performance. The experiments also show that the algorithm is 15.1 to 63.5 (with an average of 47.5) percent better than four other clustering algorithms in cluster-based information retrieval. The experiments have validated the indexing-clustering relationships and the complexity of the algorithm and have shown improvements in retrieval effectiveness. In the experiments two document databases are used: TODS214 and INSPEC. The latter is a common database with 12,684 documents. © 1990, ACM. All rights reserved.",cluster validity; clustering-indexing relationships; cover coefficient; decoupling coefficient; document retrieval; retrieval effectiveness,Computer Programming--Algorithms; Information Science--Information Retrieval; Document Clustering; Text Databases; Database Systems
A Rule-Based Language with Functions and Sets,1991,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0026119979&doi=10.1145%2f103140.103141&partnerID=40&md5=54e136d12c75da7efbbf191b4ec5e3f4,"A logic based language for manipulating complex objects constructed using set and tuple constructors is introduced. A key feature of the COL language is the use of base and derived data functions. Under some stratification restrictions, the semantics of programs is given by a minimal and justified model that can be computed using a finite sequence of fixpoints. The language is extended using external functions and predicates. An implementation of COL in a functional language is briefly discussed. © 1991, ACM. All rights reserved.",complex objects; deductive databases; fixpoint semantics; knowledge bases; object-oriented databases; rule based,Computer Programming--Logic Programming; Database Systems; COL Language; Functional Languages; Logic Based Languages; Rule Based Languages; Computer Programming Languages
A Compression Technique to Materialize Transitive Closure,1990,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0025631041&doi=10.1145%2f99935.99944&partnerID=40&md5=df9ffe72ee824c3934a1ddc836b63c2c,"An important feature of database support for expert systems is the ability of the database to answer queries regarding the existence of a path from one node to another in the directed graph underlying some database relation. Given just the database relation, answering such a query is time-consuming, but given the transitive closure of the database relation a table look-up suffices. We present an indexing scheme that permits the storage of the pre-computed transitive closure of a database relation in a compressed form. The existence of a specified tuple in the closure can be determined from this compressed store by a single look-up followed by an index comparision. We show how to add nodes and arcs to the compressed closure incrementally. We also suggest how this compression technique can be used to reduce the effort required to compute the transitive closure. © 1990, ACM. All rights reserved.",,Computer Programming--Algorithms; Expert Systems; Compression Techniques; Transitive Closure; Database Systems
Principles and Realization Strategies of Multilevel Transaction Management,1991,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0026120441&doi=10.1145%2f103140.103145&partnerID=40&md5=3067b051382678f2be5c17179aa97a33,"One of the demands of database system transaction management is to achieve a high degree of concurrency by taking into consideration the semantics of high-level operations. On the other hand, the implementation of such operations must pay attention to conflicts on the storage representation levels below. To meet these requirements in a layered architecture, we propose a multilevel transaction management utilizing layer-specific semantics. Based on the theoretical notion of multilevel serializability, a family of concurrency control strategies is developed. Suitable recovery protocols are investigated for aborting single transactions and for restarting the system after a crash. The choice of levels involved in a multilevel transaction strategy reveals an inherent trade-off between increased concurrency and growing recovery costs. A series of measurements has been performed in order to compare several strategies. Preliminary results indicate considerable performance gains of the multilevel transaction approach. © 1991, ACM. All rights reserved.",atomicity persistence concurrency control; multilevel transactions; persistence; serializability,Computer Operating Systems; Computer Simulation; Scheduling; Concurrency Control; Layered Database Architecture; Multilevel Transaction Management; Recovery Protocols; Serializability; Transaction Management; Database Systems
Safety and Translation of Relational Calculus,1991,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0026166840&doi=10.1145%2f114325.103712&partnerID=40&md5=cfbd878a8bb5a3c15a4ae7f4563595b5,"Not all queries in relational calculus can be answered sensibly when disjunction, negation, and universal quantification are allowed. The class of relation calculus queries or formulas that have sensible answers is called the domain independent class which is known to be undecidable. Subsequent research has focused on identifying large decidable subclasses of domain independent formulas. In this paper we investigate the properties of two such classes: the evaluable formulas and the allowed formulas. Although both classes have been defined before, we give simplified definitions, present short proofs of their main properties, and describe a method to incorporate equality. Although evaluable queries have sensible answers, it is not straightforward to compute them efficiently or correctly. We introduce relational algebra normal form for formulas from which form the correct translation into relational algebra is trivial. We give algorithms to transform an evaluable formula into an equivalent allowed formula and from there into relational algebra normal form. Our algorithms avoid use of the so-called Dom relation, consisting of all constants appearing in the database or the query. Finally, we describe a restriction under which every domain independent formula is evaluable and argue that the class of evaluable formulas is the largest decidable subclass of the domain independent formulas that can be efficiently recognized. © 1991, ACM. All rights reserved.",allowed formulas; domain independence; evaluable formulas; existential normal; query translation; relational algebra; relational calculus,Algebra; Algorithms; Mathematical models; Relational database systems; Domain independent class; Evaluable formulas; Query translation; Relational algebra normal form; Relational calculus; Query languages
A Dynamic Hash Method with Signature,1991,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0026166838&doi=10.1145%2f114325.103714&partnerID=40&md5=b34678903e28445c274a79489b5cd4ee,"We present a dynamic external hash method that allows retrieval of a record by only one access to mass storage while maintaining a high load factor. The hash function is based on generalized spiral storage. Both primary and overflow records are allocated to the same file, and file expansion depends on being able to allocate every overflow chain to one bucket. An in-core index, built by means of a signature function, discriminates between primary and overflow records and assures one access to storage in the case of either successful or unsuccessful searching. Simulation results confirm the good expected performance. © 1991, ACM. All rights reserved.",dynamic hashing; external hashing; generalized spiral storage; signature functions,Data handling; File organization; Information retrieval; Security of data; Simulation; Storage allocation (computer); Dynamic hashing; External hashing; Generalized spiral storage; Signature functions; Database systems
The Complexity of Operations on a Fragmented Relation,1991,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0026119787&doi=10.1145%2f103140.103143&partnerID=40&md5=567d01419b83a09501b0b2b495efb96b,"Data fragmentation is an important aspect of distributed database design, in which portions of relations, tailored to the specific needs of local applications, are defined to be further allocated to the sites of the computer network supporting the database system. In this paper we present a theory of fragmentation with overlapping fragments to study the complexity of the problems involved in checking the completeness of a fragmentation schema and in querying and updating a fragmented relation. We analyze these problems from the complexity viewpoint and present sound and complete algorithms for their solution. © 1991, ACM. All rights reserved.",completeness of fragmentation schemas; NP-hardness; query optimization; relation fragmentation; updates,Computer Programming--Algorithms; Data Fragmentation; Distributed Database Design; Fragmented Relations; Query Optimization; Database Systems
The hB-Tree: A Multiattribute Indexing Method with Good Guaranteed Performance,1990,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0025625409&doi=10.1145%2f99935.99949&partnerID=40&md5=16b460d4e2b2cc4c61a472d3fac9a177,"A new multiattribute index structure called the hB-tree is introduced. It is derived from the K-D-B-tree of Robinson [15] but has additional desirable properties. The hB-tree internode search and growth processes are precisely analogous to the corresponding processes in B-trees [1]. The intranode processes are unique. A k-d tree is used as the structure within nodes for very efficient searching. Node splitting requires that this k-d tree be split. This produces nodes which no longer represent brick-like regions in k-space, but that can be characterized as holey bricks, bricks in which subregions have been extracted. We present results that guarantee hB-tree users decent storage utilization, reasonable size index terms, and good search and insert performance. These results guarantee that the hB-tree copes well with arbitrary distributions of keys. © 1990, ACM. All rights reserved.",,Information Science--Information Retrieval; HB Trees; Index Structures; Multiattribute Indexing; Data Processing
A Software Tool for Modular Database Design,1991,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0026167270&doi=10.1145%2f114325.103711&partnerID=40&md5=efd2296e6e7f511bcce36d822439abeb,"A modularization discipline for database schemas is first described. The dicipline incorporates both a strategy for enforcing integrity constraints and a tactic for organizing large sets of database structures, integrity constraints, and operations. A software tool that helps the development and maintenance of database schemas modularized according to the discipline is then presented. It offers a user-friendly interface that guides the designer through the various stages of the creation of a new module or through the process of changing objects of existing modules. The tool incorporates, in a declarative style, a description of the design and redesign rules behind the modularization discipline, hence facilitating the incremental addition of new expertise about database design. © 1991, ACM. All rights reserved.",abstract data types; consistency preservation; encapsulation; integrity constraints; logical database design; modular design; module constructors,Artificial intelligence; Computer software; Constraint theory; Data structures; Encapsulation; Modula (programming language); Abstract data types; Consistency preservation; Integrity constraints; Module constructors; Software tools; Database systems
Direct Transitive Closure Algorithms: Design and Performance Evaluation,1990,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0025494195&doi=10.1145%2f88636.88888&partnerID=40&md5=54b524d7bc0e8648d70fd22ad6d6af0b,"We present new algorithms for computing transitive closure of large database relations. Unlike iterative algorithms, such as the seminaive and logarithmic algorithms, the termination of our algorithms does not depend on the length of paths in the underlying graph 1990. Besides reachability computations, the proposed algorithms can also be used for solving path problems. We discuss issues related to the efficient implementation of these algorithms, and present experimental results that show the direct algorithms perform uniformly better than the iterative algorithms. A side benefit of this work is that we have proposed a new methodology for evaluating the performance of recursive queries. © 1990, ACM. All rights reserved.",deductive databases; query processing; transitive closure,Computer Programming - Algorithms; Deductive Databases; Transitive Closure Algorithms; Database Systems
Indefinite and Maybe Information in Relational Databases,1990,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0025404596&doi=10.1145%2f77643.77644&partnerID=40&md5=c053e68b6d676e8e1d3c6f50c3a2e9fb,"This paper extends the relational model to represent indefinite and maybe kinds of incomplete information. A data structure, called an I-table, which is capable of representing indefinite and maybe facts, is introduced. The information content of I-tables is precisely defined, and an operator to remove redundant facts is presented. The relational algebra is then extended in a semantically correct way to operate on I-tables. Queries are posed in the same way as in conventional relational algebra; however, the user may now expect indefinite as well as maybe answers. © 1990, ACM. All rights reserved.",,Data Processing--Data Structures; Query Languages; Relational Algebra; Database Systems
Necessary and Sufficient Conditions to Linearize Doubly Recursive Programs in Logic Databases,1990,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0025492206&doi=10.1145%2f88636.89237&partnerID=40&md5=b551069f8e9e7bc02db8c6ce285324b9,"Linearization of nonlinear recursive programs is an important issue in logic databases for both practical and theoretical reasons. If a nonlinear recursive program can be transformed into an equivalent linear recursive program, then it may be computed more efficiently than when the tranformation is not possible. We provide a set of necessary and sufficient conditions for a simple doubly recursive program to be equivalent to a simple linear recursive program. The necessary and sufficient conditions can be verified effectively. © 1990, ACM. All rights reserved.",database; logic,Computer Metatheory - Programming Theory; Doubly Recursive Programs; Logic Databases; Recursive Programs; Recursive Queries; Database Systems
Dynamic Voting Algorithms for Maintaining the Consistency of a Replicated Database,1990,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0025446697&doi=10.1145%2f78922.78926&partnerID=40&md5=01af5f7954ef0a8645557aadcdccaf13,"There are several replica control algorithms for managing replicated files in the face of network partitioning due to site or communication link failures. Pessimistic algorithms ensure consistency at the price of reduced availability; they permit at most one 1990 partition to process updates at any given time. The best known pessimistic algorithm, voting, is a “static” algorithm, meaning that all potential distinguished partitions can be listed in advance. We present a dynamic extension of voting called dynamic voting. This algorithm permits updates in a partition provided it contains more than half of the up-to-date copies of the replicated file. We also present an extension of dynamic voting called dynamic voting with linearly ordered copies (abbreviated as dynamic-linear). These algorithms are dynamic because the order in which past distinguished partitions were created plays a role in the selection of the next distinguished partition. Our algorithms have all the virtues of ordinary voting, including its simplicity, and provide improved availability as well. We provide two stochastic models to support the latter claim. In the first (site) model, sites may fail but communication links are infallible; in the second (link) model the reverse is true. We prove that under the site model, dynamic-linear has greater availability than any static algorithm, including weighted voting, if there are four or more sites in the network. In the link model, we consider all biconnected five-site networks and a wide variety of failure and repair rates. In all cases considered, dynamic-linear had greater availability than any static algorithm. © 1990, ACM. All rights reserved.",,Computer Programming--Algorithms; Dynamic Voting; Network Partitioning; Pessimistic Algorithms; Replicated Databases; Serializability; Voting Algorithms; Database Systems
Logic-Based Approach to Semantic Query Optimization,1990,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0025444719&doi=10.1145%2f78922.78924&partnerID=40&md5=e6a93095dd796409ee46fa858d799a8f,"The purpose of semantic query optimization is to use semantic knowledge (e.g., integrity constraints) for transforming a query into a form that may be answered more efficiently than the original version. In several previous papers we described and proved the correctness of a method for semantic query optimization in deductive databases couched in first-order logic. This paper consolidates the major results of these papers emphasizing the techniques and their applicability for optimizing relational queries. Additionally, we show how this method subsumes and generalizes earlier work on semantic query optimization. We also indicate how semantic query optimization techniques can be extended to databases that support recursion and integrity constraints that contain disjunction, negation, and recursion. © 1990, ACM. All rights reserved.",,Computer Metatheory--Formal Logic; Optimization; Deductive Databases; Integrity Constraints; Query Optimization; Recursion; Semantic Query Optimization; Database Systems
On Compile-Time Query Optimization in Deductive Databases by Means of Static Filtering,1990,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0025494277&doi=10.1145%2f88636.87121&partnerID=40&md5=4c8fedaebaca0c987d3621d03a7dc3bd,"We extend the query optimization techniques known as algebric manipulations with relational expressions [48] to work with deductive databases. In particular, we propose a method for moving data-independent selections and projections into recursive axioms, which extends all other known techniques for performing that task [2, 3, 9, 18, 20]. We also show that, in a well-defined sense, our algorithm is optimal among the algorithms that propagate data-independent selections through recursion. © 1990, ACM. All rights reserved.",dataflow; deductive databases; filtering; fixpoint; graph representation; inference; projection; recursive rules; selection,Computer Programming - Algorithms; Mathematical Techniques - Algebra; Optimization; Deductive Databases; Query Optimization; Database Systems
Scaling up Output Capacity and Performance Results From Information Systems Prototypes,1990,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0025493980&doi=10.1145%2f88636.87943&partnerID=40&md5=da03dfdebcd060abc13e84be5a090a23,"The advantage of information system prototyping arises from its predict problems and end-user satisfaction with a system early in the development process, before significant commitments of time and effort have been made. Predictions of problems and end-user satisfaction have risen in importance with the increasing complexity of business information systems and the exponential growth of database size. This research investigates the reporting of information to an end user, and the process of inferring from a prototype to a full-scale information system. This inference is called scaling up, and is an important part of the systems development planning process. The research investigates information systems reporting from a linguistic perspective, where a database is used as a central receptacle for information storage. It then investigates the manner in which reporting statistics from the prototype information system may be used to infer the behavior and performance of the full-scale system. An example is presented for the application of the algorithm, and the final section discusses the usefulness, application, and implications of the algorithm developed in this research. © 1990, ACM. All rights reserved.",inclusion-exclusion; principle,Computer Programming - Algorithms; Information Systems; Prototyping; Database Systems
Apologizing versus asking permission: Optimistic concurrency control for abstract data types,1990,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0025399444&doi=10.1145%2f77643.77647&partnerID=40&md5=365ec56e706d1780d86d05cb80b0fff6,"An optimistic concurrency control technique is one that allows transactions to execute without synchronization, relying on commit-time validation to ensure serializability. Several new optimistic concurrency control techniques for objects in decentralized distributed systems are described here, their correctness and optimality properties are proved, and the circumstances under which each is likely to be useful are characterized. Unlike many methods that classify operations only as Reads or Writes, these techniques systematically exploit type-specific properties of objects to validate more interleavings. Necessary and sufficient validation conditions can be derived directly from an object's data type specification. These techniques are also modular: they can be applied selectively on a per-object 1990 basis in conjunction with standard pessimistic techniques such as two-phase locking, permitting optimistic methods to be introduced exactly where they will be most effective. These techniques can be used to reduce the algorithmic complexity of achieving high levels of concurrency, since certain scheduling decisions that are NP-complete for pessimistic schedulers can be validated after the fact in time, independent of the level of concurrency. These techniques can also enhance the availability of replicated data, circumventing certain tradeoffs between concurrency and availability imposed by comparable pessimistic techniques. © 1990, ACM. All rights reserved.",,Abstract Data Types; Concurrency Control; Serializability; Database Systems
Explaining Ambiguity in a Formal Query Language,1990,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0025445021&doi=10.1145%2f78922.78923&partnerID=40&md5=9c22b2700f48e51e0b86d02e722250c0,"The problem of generating reasonable natural language-like responses to queries formulated in nonnavigational query languages with logical data independence is addressed. An extended ER model, the Entity-Relationship-Involvement model, is defined which assists in providing a greater degree of logical data independence and the generation of natural language explanations of a query processor's interpretation of a query. These are accomplished with the addition of the concept of an involvement to the model. Based on involvement definitions in a formally defined data definition language, DDL, an innovative strategy for generating explanations is outlined and exemplified. In the conclusion, possible extensions to the approach are given. © 1990, ACM. All rights reserved.",,Entity Relationship Models; Natural Language Systems; Database Systems
Query optimization in a memory-resident domain relational calculus database system,1990,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0025400102&doi=10.1145%2f77643.77646&partnerID=40&md5=4a67440ef91db43a5aec5bb99878ec05,"We present techniques for optimizing queries in memory-resident database systems. Optimization techniques in memory-resident database systems differ significantly from those in conventional disk-resident database systems. In this paper we address the following aspects of query optimization in such systems and present specific solutions for them: 1990 a new approach to developing a CPU-intensive cost model; (2) new optimization strategies for main-memory query processing; (3) new insight into join algorithms and access structures that take advantage of memory residency of data; and (4) the effect of the operating system's scheduling algorithm on the memory-residency assumption. We present an interesting result that a major cost of processing queries in memory-resident database systems is incurred by evaluation of predicates. We discuss optimization techniques using the Office-by-Example (OBE) that has been under development at IBM Research. We also present the results of performance measurements, which prove to be excellent in the current state of the art. Despite recent work on memory-resident database systems, query optimization aspects in these systems have not been well studied. We believe this paper opens the issues of query optimization in memory-resident database systems and presents practical solutions to them. © 1990, ACM. All rights reserved.",,Optimization; Domain Relational Calculus; Memory Resident Database; Query Optimization; Database Systems
A Decision-Theoretic Approach to Information Retrieval,1990,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0025491711&doi=10.1145%2f88636.88597&partnerID=40&md5=71679ea21e3f439a962c15d085f0a8c7,"We present the file search problem in a decision-theoretic framework, and discuss a variation of it that we call the common index problem. The goal of the common index problem is to return the best available record in the file, where best is in terms of a class of user preferences. We use dynamic programming to construct an optimal algorithm using two different optimality criteria, and we develop sufficient conditions for obtaining complete information. © 1990, ACM. All rights reserved.",approximate algorithms; economics of information; information retrieval,"Computer Programming - Algorithms; Decision Theory and Analysis; Mathematical Programming, Dynamic; Optimization; Common Index Problem; File Search; Information Science"
Data Caching Issues in an Information Retrieval System,1990,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0025494701&doi=10.1145%2f88636.87848&partnerID=40&md5=0f45703077629c95ae01b0e89c2199a6,"Currently, a variety of information retrieval systems are availableto potential users.… While in many cases these systems areaccessed from personal computers, typically no advantage is taken of thecomputing resources of those machines 1990. In this paper we explore the possibility of using the user'slocal storage capabilities to cache data at the user's site. This wouldimprove the response time of user queries albeit at the cost ofincurring the overhead required in maintaining multiple copies. In orderto reduce this overhead it may be appropriate to allow copies to divergein a controlled fashion.… Thus, we introduce the notion of quasi-copies, which embodies theideas sketched above. We also define the types of deviations that seemuseful, and discuss the available implementation strategies. © 1990, ACM. All rights reserved.",cache coherency; data sharing; information retrieval systems,Database Systems - Distributed; Cache Memories; Data Caching; Information Retrieval Systems
Translation with Optimization from Relational Calculus to Relational Algebra Having Aggregate Functions,1990,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0025593744&doi=10.1145%2f99935.99943&partnerID=40&md5=98a057566385a1f264b59f52715a377a,"Most of the previous translations of relational calculus to relational algebra aimed at proving that the two languages have the equivalent expressive power, thereby generating very complicated relational algebra expressions, especially when aggregate functions are introduced. This paper presents a rule-based translation method from relational calculus expressions having both aggregate functions and null values to optimized relational algebra expressions. Thus, logical optimization is carried out through translation. The translation method comprises two parts: the translational of the relational calculus kernel and the translation of aggregate functions. The former uses the familiar step-wise rewriting strategy, while the latter adopts a two-phase rewriting strategy via standard aggregate expressions. Each translation proceeds by applying a heuristic rewriting rule in preference to a basic rewriting rule. After introducing SQL-type null values, their impact on the translation is thoroughly investigated, resulting in several extensions of the translation. A translation experiment with many queries shows that the proposed translation method generates optimized relational algebra expressions. It is shown that heuristic rewriting rules play an essential role in the optimization. The correctness of the present translation is also shown. Each translation proceeds by applying a heuristic rewriting rule in preference to a basic rewriting rule. After introducing SQL-type null values, their impact on the translation is thoroughly investigated, resulting in several extensions of the translation. A translation experiment with many queries shows that the proposed translation method generates optimized relational. © 1990, ACM. All rights reserved.",,Mathematical Techniques; Optimization; Relational Algebra; Relational Calculus; Rewriting Rules; Database Systems
A Linear-Time Probabilistic Counting Algorithm for Database Applications,1990,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0025449292&doi=10.1145%2f78922.78925&partnerID=40&md5=e323296b84007fb50e8d7bbdbef45690,"We present a probabilistic algorithm for counting the number of unique values in the presence of duplicates. This algorithm has O1990 time complexity, where q is the number of values including duplicates, and produces an estimation with an arbitrary accuracy prespecified by the user using only a small amount of space. Traditionally, accurate counts of unique values were obtained by sorting, which has O(q log q) time complexity. Our technique, called linear counting, is based on hashing. We present a comprehensive theoretical and experimental analysis of linear counting. The analysis reveals an interesting result: A load factor (number of unique values/hash table size) much larger than 1.0 (e.g., 12) can be used for accurate estimation (e.g., 1% of error). We present this technique with two important applications to database problems: namely, (1) obtaining the column cardinality (the number of unique values in a column of a relation) and (2) obtaining the join selectivity (the number of unique values in the join column resulting from an unconditional join divided by the number of unique join column values in the relation to he joined). These two parameters are important statistics that are used in relational query optimization and physical database design. © 1990, ACM. All rights reserved.",,Computer Programming--Algorithms; Probability; Counting Algorithms; Probabilistic Algorithms; Query Optimization; Statistical Databases; Database Systems
View updates in relational databases with an independent scheme,1990,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0025401306&doi=10.1145%2f77643.77645&partnerID=40&md5=8000eae9936c51deb3f11eb08ce9bb2a,"A view on a database is a mapping that provides a user or application with a suitable way of looking at the data. Updates specified on a view have to be translated into updates on the underlying database. We study the view update translation problem for a relational data model in which the base relations may contain 1990 nulls. The representative instance is considered to be the correct representation of all data in the database; the class of views that is studied consists of total projections of the representative instance. Only independent database schemes are considered, that is, schemes for which global consistency is implied by local consistency. A view update can be an insertion, a deletion, or a modification of a single view tuple. It is proven that the constant complement method of Bancilhon and Spyratos is too restrictive to be useful in this context. Structural properties of extension joins are derived that are important for understanding views. On the basis of these properties, minimal algorithms for translating a single view-tuple update are given. © 1990, ACM. All rights reserved.",,Computer Programming--Algorithms; Extension Joins; View Updates; Database Systems
The Five Color Concurrency Control Protocol: Non-Two-Phase Locking in General Databases,1990,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0025446491&doi=10.1145%2f78922.78927&partnerID=40&md5=844c3d149a46c5822b750ccef5021bea,"Concurrency control protocols based on two-phase locking are a popular family of locking protocols that preserve serializability in general 1990 database systems. A concurrency control algorithm (for databases with no inherent structure) is presented that is practical, non two-phase, and allows varieties of serializable logs not possible with any commonly known locking schemes. All transactions are required to predeclare the data they intend to read or write. Using this information, the protocol anticipates the existence (or absence) of possible conflicts and hence can allow non-two-phase locking. It is well known that serializability is characterized by acyclicity of the conflict graph representation of interleaved executions. The two-phase locking protocols allow only forward growth of the paths in the graph. The Five Color protocol allows the conflict graph to grow in any direction (avoiding two-phase constraints) and prevents cycles in the graph by maintaining transaction access information in the form of data-item markers. The read and write set information can also be used to provide relative immunity from deadlocks. © 1990, ACM. All rights reserved.",,Computer Programming--Algorithms; Concurrency Control; Deadlocks; Locking Protocols; Serializability; Database Systems
Using Semantic Knowledge of Transactions to Increase Concurrency,1989,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0024885855&doi=10.1145%2f76902.76905&partnerID=40&md5=a3be4f2e8173c390935d51593cbf081d,"When the only information available about transactions is syntactic information, serializability is the main correctness criterion for concurrency control. Serializability requires that the execution of each transaction must appear to every other transaction as a single atomic step (i.e., the execution of the transaction cannot be interrupted by other transactions). Many researchers, however, have realized that this requirement is unnecessarily strong for many applications and can significantly increase transaction response time. To overcome this problem, a new approach for controlling concurrency that exploits the semantic information available about transactions to allow controlled nonserializable interleavings has recently been proposed. This approach is useful when the cost of producing only serializable interleavings is unacceptably high. The main drawback of the approach is the extra overhead incurred by utilizing the semantic information. We examine this new approach in this paper and discuss its strengths and weaknesses. We introduce a new formalization for the concurrency control problem when semantic information is available about the transactions. This semantic information takes the form of transaction types, transaction steps, and transaction break-points. We define a new class of “safe” schedules called relatively consistent (RC) schedules. This class contains serializable as well as nonserializable schedules. We prove that the execution of an RC schedule cannot violate consistency and propose a new concurrency control mechanism that produces only RC schedules. Our mechanism assumes fewer restrictions on the interleavings among transactions than previously introduced semantic-based mechanisms. © 1989, ACM. All rights reserved.",breakpoint; concurrency control; Database; locking; semantic knowledge; transaction,Concurrency Control; Semantic Knowledge; Serializability; Database Systems
Integrity = Validity + Completeness,1989,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0024941096&doi=10.1145%2f76902.76904&partnerID=40&md5=aa3d7807e75c19346cdaae36ec76ce1b,"Database integrity has two complementary components: validity, which guarantees that all false information is excluded from the database, and completeness, which guarantees that all true information is included in the database. This article describes a uniform model of integrity for relational databases, that considers both validity and completeness. To a large degree, this model subsumes the prevailing model of integrity (i.e., integrity constraints). One of the features of the new model is the determination of the integrity of answers issued by the database system in response to user queries. To users, answers that are accompanied with such detailed certifications of their integrity are more meaningful. First, the model is defined and discussed. Then, a specific mechanism is described that implements this model. With this mechanism, the determination of the integrity of an answer is a process analogous to the determination of the answer itself. © 1989, ACM. All rights reserved.",Closed world assumption; completeness; integrity; integrity constraints; metarelation; relational algebra; relational database; relational view; validity,Database Completeness; Database Integrity; Database Validity; Database Systems
A framework for effective retrieval,1989,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0024673712&doi=10.1145%2f63500.63519&partnerID=40&md5=f173222b9a41d473199fc61c7059b6d0,"The aim of an effective retrieval system is to yield high recall and precision (retrieval effectiveness). The nonbinary independence model, which takes into consideration the number of occurrences of terms in documents, is introduced. It is shown to be optimal under the assumption that terms are independent. It is verified by experiments to yield significant improvement over the binary independence model. The nonbinary model is extended to normalized vectors and is applicable to more general queries. Various ways to alleviate the consequences of the term independence assumption are discussed. Estimation of parameters required for the nonbinary independence model is provided, taking into consideration that a term may have different meanings. © 1989, ACM. All rights reserved.",Accumulation of statistics; clustering; effective information retrieval; nonbinary independence model; optimal retrieval; parameter estimation; probabilistic model; transformation of terms,Database Systems; Mathematical Techniques--Estimation; Nonbinary Independence Model; Parameter Estimation; Retrieval Effectiveness; Information Science
NFQL: The natural forms query language,1989,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0024673795&doi=10.1145%2f63500.64125&partnerID=40&md5=55f25e0d3152900e262154563377f397,"A means by which ordinary forms can be exploited to provide a basis for nonprocedural specification of information processing is discussed. The Natural Forms Query Language (NFQL) is defined. In NFQL data retrieval requests and computation specifications are formulated by sketching ordinary forms to show what data are desired and update operations are specified by altering data on filled-in forms. The meaning of a form depends on a store of knowledge that includes extended abstract data types for defining elementary data items, a database scheme defined by an entity-relationship model, and a conceptual model of an ordinary form. Based on this store of knowledge, several issues are addressed and resolved in the context of NFQL. These issues include automatic generation of query expressions from weak specifications, the view update problem, power and completeness, and a heuristic approach to resolving computational relationships. A brief status report of an implementation of NFQL is also given. © 1989, ACM. All rights reserved.",Application generators; forms systems; forms-oriented interfaces; informal software specification,Information Science--Information Retrieval; Entity-Relationship Model; Forms Oriented Interfaces; Informal Software Specification; Natural Forms Query Language; Nonprocedural Specification; Relational Database; Database Systems
File organization using composite perfect hashing,1989,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0024673671&doi=10.1145%2f63500.63521&partnerID=40&md5=5b33fbb7b7c410196225c02f87147742,"Perfect hashing refers to hashing with no overflows. We propose and analyze a composite perfect hashing scheme for large external files. The scheme guarantees retrieval of any record in a single disk access. Insertions and deletions are simple, and the file size may vary considerably without adversely affecting the performance. A simple variant of the scheme supports efficient range searches in addition to being a completely dynamic file organization scheme. These advantages are achieved at the cost of a small amount of additional internal storage and increased cost of insertions. © 1989, ACM. All rights reserved.",Composite perfect hashing; direct perfect hashing; external perfect hashing; file organization; file structures; hashing; hashing functions; perfect hashing; single access retrieval; universal hashing,Database Systems; Information Science--Information Retrieval; Composite Perfect Hashing; Dynamic File Organization; Large External Files; Range Searches; Data Processing
Automatic Verification of Database Transaction Safety,1989,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0024739089&doi=10.1145%2f68012.68014&partnerID=40&md5=56806d73a36f52a4de07ad902ae867a8,"Maintaining the integrity of databases is one of the promises of database management systems. This includes assuring that integrity constraints are invariants of database transactions. This is very difficult to accomplish efficiently in the presence of complex constraints and large amounts of data. One way to minimize the amount of processing required to maintain database integrity over transaction processing is to prove at compile-time that transactions cannot, if run atomically, disobey integrity constraints. We report on a system that performs such verification for a robust set of constraint and transaction classes. The system accepts database schemas written in a more or less traditional style and accepts programs in a high-level programming language. Automatic verification fast enough to be effective on current workstation hardware is performed. © 1989, ACM. All rights reserved.",Constraint maintenance; database integrity,Constraint Maintenance; Database Integrity; Database Transaction Safety; Database Systems
Performance of a Two-Headed Disk System when Serving Database Queries Under the Scan Policy,1989,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0024738564&doi=10.1145%2f68012.68017&partnerID=40&md5=64dd4c6fefb9cb1f3ff5a122536a4d89,"Disk drives with movable two-headed arms are now commercially available. The two heads are separated by a fixed number of cylinders. A major problem for optimizing disk head movement, when answering database requests, is the specification of the optimum number of cylinders separating the two heads. An earlier analytical study assumed a FCFS model and concluded that the optimum separation distance should be equal to 0.44657 of the number of cylinders N of the disk. This paper considers that the SCAN scheduling policy is used in file access, and it applies combinatorial analysis to derive exact formulas for the expected head movement. Furthermore, it is proven that the optimum separation distance is N/2 - 1 (⌈N/2 - 1⌉ and ⌊N/2 - 1⌋) if N is even (odd). In addition, a comparison with a single-headed disk system operating under the same scheduling policy shows that if the two heads are optimally spaced, then the mean seek distance is less than one-half of the value obtained with one head. In fact that the SCAN policy is used for many database applications (for example,batching and secondary key retrieval) demonstrates the potential of two-headed disk systems for improving the performance of database systems. © 1989, ACM. All rights reserved.",Batching; disk design; performance evaluation; secondary key retrieval; seek distance; seek time; two-headed disk,Database Systems; Query Processing; SCAN Scheduling Policy; Secondary Storage Devices; Two-Headed Disk Systems; Data Storage Units
Query Processing Techniques in the Summary-Table-by-Example Database Query Language,1989,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0024905570&doi=10.1145%2f76902.76906&partnerID=40&md5=e8c4ce585f3fa7e55843a6670453cb9b,"Summary-Table-by-Example (STBE) is a graphical language suitable for statistical database applications. STBE queries have a hierarchical subquery structure and manipulate summary tables and relations with set-valued attributes. The hierarchical arrangement of STBE queries naturally implies a tuple-by-tuple subquery evaluation strategy (similar to the nested loops join implementation technique) which may not be the best query processing strategy. In this paper we discuss the query processing techniques used in STBE. We first convert an STBE query into an “extended” relational algebra (ERA) expression. Two transformations are introduced to remove the hierarchical arrangement of subqueries so that query optimization is possible. To solve the “empty partition” problem of aggregate function evaluation, directional join (one-sided outer-join) is utilized. We give the algebraic properties of the ERA operators to obtain an “improved” ERA expression. Finally we briefly discuss the generation of alternative implementations of a given ERA expression. STBE is implemented in a prototype statistical database management system. We discuss the STBE-related features of the implemented system. © 1989, ACM. All rights reserved.",Extended relational algebra; statistical database management systems; STBE,Mathematical Techniques--Set Theory; Extended Relational Algebra; Query Processing Techniques; Statistical Databases; Summary-Table-by-Example Query Language; Database Systems
On the Effect of Join Operations on Relation Sizes,1989,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0024902180&doi=10.1145%2f76902.76907&partnerID=40&md5=d5cfc470ffd29356d4693e8d89f2f005,"We propose a generating function approach to the problem of evaluating the sizes of derived relations in a relational database framework. We present a model of relations and show how to use it to deduce probabilistic estimations of derived relation sizes. These are found to asymptotically follow normal distributions under a variety of assumptions. © 1989, ACM. All rights reserved.",Asymptotic normality; join; projection; relation size; relational database,Mathematical Techniques--Estimation; Probability; Asymptotic Normality; Join Operations; Normal Distributions; Relation Sizes; Database Systems
Index Scans Using a Finite LRU Buffer: A Validated I/O Model,1989,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0024739585&doi=10.1145%2f68012.68016&partnerID=40&md5=0e40b9760271e28703088e4bca739bdf,"Indexes are commonly employed to retrieve a portion of a file or to retrieve its records in a particular order. An accurate performance model of indexes is essential to the design, analysis, and tuning of file management and database systems, and particularly to database query optimization. Many previous studies have addressed the problem of estimating the number of disk page fetches when randomly accessing k records out of N given records stored on T disk pages. This paper generalizes these results, relaxing two assumptions that usually do not hold in practice: unlimited buffer and unique records for each key value. Experiments show that the performance of an index scan is very sensitive to buffer size limitations and multiple records per key value. A model for these more practical situations is presented and a formula derived for estimating the performance of an index scan. We also give a closed-form approximation that is easy to compute. The theoretical results are validated using the R* distributed relational database system. Although we use database terminology throughout the paper, the model is more generally applicable whenever random accesses are made using keys. © 1989, ACM. All rights reserved.",B-tree; buffer management; database performance; file access; index; information retrieval; inverted file organization; query optimization,Data Processing--File Organization; Database Systems; Buffer Management; File Access; Index Scans; Inverted File Organization; Query Optimization; Information Science
Efficient optimization of simple chase join expressions,1989,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0024673809&doi=10.1145%2f63500.63520&partnerID=40&md5=130d344bf5e2050aa1cd8b603691e49c,"Simple chase join expressions are relational algebra expressions, involving only projection and join operators, defined on the basis of the functional dependencies associated with the database scheme. They are meaningful in the weak instance model, because for certain classes of schemes, including independent schemes, the total projections of the representative instance can be computed by means of unions of simple chase join expressions. We show how unions of simple chase join expressions can be optimized efficiently, without constructing and chasing the corresponding tableaux. We also present efficient algorithms for testing containment and equivalence, and for optimizing individual simple chase join expressions. © 1989, ACM. All rights reserved.",Chase; functional dependency; query equivalence; query optimization; relational database; tableau,Computer Programming--Algorithms; Optimization; Containment Testing; Equivalence Testing; Functional Dependencies; Relational Algebra Expressions; Simple Chase Join Expressions; Database Systems
Maintaining availability in partitioned replicated databases,1989,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0024673638&doi=10.1145%2f63500.63501&partnerID=40&md5=fec5a55052599e42dbfde58c551819dc,"In a replicated database, a data item may have copies residing on several sites. A replica control protocol is necessary to ensure that data items with several copies behave as if they consist of a single copy, as far as users can tell. We describe a new replica control protocol that allows the accessing of data in spite of site failures and network partitioning. This protocol provides the database designer with a large degree of flexibility in deciding the degree of data availability, as well as the cost of accessing data. © 1989, ACM. All rights reserved.",Concurrency control; partitioning failures; replica control; replicated databases; serializability theory,Computer Operating Systems; Concurrency Control; Database Availability; Partitioned Replicated Databases; Partitioning Failures; Replica Control; Serializability; Database Systems
Imprecise Schema: A Rationale for Relations with Embedded Subrelations,1989,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0024930376&doi=10.1145%2f76902.76903&partnerID=40&md5=3b60b3b9fa2e6c6b3370e5bafcb177de,"Exceptional conditions are anomalous data which meet the intent of a schema but not the schema definition, represent a small proportion of the database extension, and may become known only after the schema is in use. Admission of exceptional conditions is argued to suggest a representation that locally stretches the schema definition by use of relations with embedded subrelations. Attempted normalization of these relations to 1NF does not yield the static schema typically associated with such transformations. A class of relations, termed Exceptional Condition Nested Form (ECNF), is defined which allows the necessary representation of exceptional conditions while containing sufficient restrictions to prevent arbitrary and chaotic inclusion of embedded subrelations. Queries on a subset of exceptional conditions, the exceptional constraints, are provided an interpretation via an algorithm that transforms ECNF relations into 1NF relations containing two types of null values. Extensions of relational algebraic operators, suitable for interactive query navigation, are defined for use with ECNF relations containing all forms of exceptional conditions. © 1989, ACM. All rights reserved.",Embedded subrelations; exceptional conditions; imprecise schema; non-lNF; null values,Computer Programming--Algorithms; Anomalous Data; Embedded Subrelations; Imprecise Schemas; Interactive Query Navigation; Database Systems
Updating Derived Relations: Detecting Irrelevant and Autonomously Computable Updates,1989,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0024737704&doi=10.1145%2f68012.68015&partnerID=40&md5=ea087e10a9977cbcf481d1ecc62ae5c9,"Consider a database containing not only base relations but also stored derived relations (also called materialized or concrete views). When a base relation is updated, it may also be necessary to update some of the derived relations. This paper gives sufficient and necessary conditions for detecting when an update of a base relation cannot affect a derived relation (an irrelevant update), and for detecting when a derived relation can be correctly updated using no data other than the derived relation itself and the given update operation (an autonomously computable update). The class of derived relations considered is restricted to those defined by PSJ-expressions, that is, any relational algebra expressions constructed from an arbitrary number of project, select and join operations (but containing no self-joins). The class of update operations consists of insertions, deletions, and modifications, where the set of tuples to be deleted or modified is specified by a selection condition on attributes of the relation being updated. © 1989, ACM. All rights reserved.",Conceptual relations; database design; derived relations; materialized views; prejoined relations; relational databases; stored relations,Database Updates; Derived Relations; Database Systems
A Unified Analysis of Batched Searching of Sequential and Tree-Structured Files,1989,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0024922381&doi=10.1145%2f76902.76908&partnerID=40&md5=a10b9d31d7bbbc401f7ef2f24591a2d6,"A direct and unified approach is used to analyze the efficiency of batched searching of sequential and tree-structured files. The analysis is applicable to arbitrary search distributions, and closed-form expressions are obtained for the expected batched searching cost and savings. In particular, we consider a search distribution satisfying Zipf's law for sequential files and four types of uniform (random) search distribution for sequential and tree-structured files. These results unify and extend earlier research on batched searching and estimating block accesses for database systems. © 1989, ACM. All rights reserved.",Batched searching; block access; database design; sequential search,Data Processing--File Organization; Database Systems; Batched Searching; Search Methods; Sequential Files; Tree Structured Files; Information Science
"Cactis: A Self-Adaptive, Concurrent Implementation of an Object-Oriented Database Management System",1989,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0024735098&doi=10.1145%2f68012.68013&partnerID=40&md5=7c5fd89c4105f076f9d673167bb55c32,"Cactis is an object-oriented, multiuser DBMS developed at the University of Colorado. The system supports functionally-defined data and uses techniques based on attributed graphs to optimize the maintenance of functionally-defined data. The implementation is self-adaptive in that the physical organization and the update algorithms dynamically change in order to reduce disk access. The system is also concurrent. At any given time there are some number of computations that must be performed to bring the database up to date; these computations are scheduled independently and are performed when the expected cost to do so is minimal. The DBMS runs in the Unix/C Sun workstation environment. Cactis is designed to support applications that require rich data modeling capabilities and the ability to specify functionally-defined data, but that also demand good performance. Specifically, Cactis is intended for use in the support of such applications as VLSI and PCB design, and software environments. © 1989, ACM. All rights reserved.",Buffer management; derived data; object-oriented database systems; self-adapted optimization,Computer Programming--Algorithms; Mathematical Techniques--Graph Theory; Attributed Graphs; Cactis DBMS; Object Oriented Database Management System; Update Algorithms; Database Systems
On the Translation of Relational Queries into Iterative Programs,1989,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0024621006&doi=10.1145%2f62032.62033&partnerID=40&md5=b1fa0f479c4831c4c1a8248355c2f146,"This paper investigates the problem of translating set-oriented query specifications into iterative programs. The translation uses techniques of functional programming and program transformation. We present two algorithms that generate iterative programs from algebra-based query specifications. The first algorithm translates query specifications into recursive programs. Those are simplified by sets of transformation rules before the algorithm generates the final iterative form. The second algorithm uses a two-level translation that generates iterative programs faster than the first algorithm. On the first level a small set of transformation rules performs structural simplification before the functional combination on the second level yields the final iterative form. © 1989, ACM. All rights reserved.",Algebraic specification; database; iterative programs,Computer Programming--Algorithms; Mathematical Techniques--Algebra; Algebraic Specification; Functional Programming; Iterative Programs; Program Transformation; Relational Queries; Database Systems
On Estimating the Cardinality of the Projection of a Database Relation,1989,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0024621489&doi=10.1145%2f62032.62034&partnerID=40&md5=d50d7e6d7e7764c6b2006b5824994813,"We present an analytical formula for estimating the cardinality of the projection on certain attributes of a subset of a relation in a relational database. This formula takes into account a priori knowledge of the semantics of the real-world objects and relationships that the database is intended to represent. Experimental testing of the formula shows that it has an acceptably low percentage error, and that its worst-case error is smaller than the best-known formula. Furthermore, the formula presented here has the advantage that it does not require a scan of the relation. © 1989, ACM. All rights reserved.",Application semantics; block access estimation; data semantics; query cost estimation; query optimization; relational databases,Computer Simulation; Application Semantics; Cardinality Estimation; Data Semantics; Database Systems
AGM: A Dataflow Database Machine,1989,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0024620684&doi=10.1145%2f62032.62037&partnerID=40&md5=34b993726166aab7657c05573cd8bb6c,"In recent years, a number of database machines consisting of large numbers of parallel processing elements have been proposed. Unfortunately, there are two main limitations in database processing that prevent a high degree of parallelism; these are the available I/O bandwidth of the underlying storage devices and the concurrency control mechanisms necessary to guarantee data integrity. The main problem with conventional approaches is the lack of a computational model capable of utilizing the potential of any significant number of processing elements and storage devices and, at the same time, preserving the integrity of the database. This paper presents a database model and its associated architecture, which is based on the principles of data-driven computation. According to this model, the database is represented as a network in which each node is conceptually an independent, asynchronous processing element, capable of communicating with other nodes by exchanging messages along the network arcs. To answer a query, one or more such messages, called tokens, are created and injected into the network. These then propagate asynchronously through the network in search of results satisfying the given query. The asynchronous nature of processing permits the model to be mapped onto a computer architecture consisting of large numbers of independent disk units and processing elements. This increases both the available I/O bandwidth as well as the processing potential of the machine. At the same time, new concurrency control and error recovery mechanisms are necessary to cope with the resulting parallelism. © 1989, ACM. All rights reserved.",,Computer Architecture; Active Graph Machine; AGM Model; Database Machines; Dataflow Computing; Database Systems
Variable-Depth Trie Index Optimization: Theory and Experimental Results,1989,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0024620579&doi=10.1145%2f62032.77249&partnerID=40&md5=8a7a4fcd690da8dc2b39c1c6156e9dec,"We develop an efficient approach to Trie index optimization. A Trie is a data structure used to index a file having a set of attributes as record identifiers. In the proposed methodology, a file is horizontally partitioned into subsets of records using a Trie index whose depth of indexing is allowed to vary. The retrieval of a record from the file proceeds by “stepping through” the index to identify a subset of records in the file in which a binary search is performed. This paper develops a taxonomy of optimization problems underlying variable-depth Trie index construction. All these problems are solvable in polynomial time, and their characteristics are studied. Exact algorithms and heuristics for their solution are presented. The algorithms are employed in CRES-an expert system for editing written narrative material, developed for the Department of the Navy. CRES uses several large-to-very-large dictionary files for which Trie indexes are constructed using these algorithms. Computational experience with CRES shows that search and retrieval using variable-depth Trie indexes can be as much as six times faster than pure binary search. The space requirements of the Tries are reasonable. The results show that the variable-depth Tries constructed according to the proposed algorithms are viable and efficient for indexing large-to-very-large files by attributes in practical applications. © 1989, ACM. All rights reserved.",Combinatorial Optimization; data structures; optimal data structures; recursion; retrieval; search algorithms; searching; Trie; Trie index optimization,Artificial Intelligence--Expert Systems; Computer Programming--Algorithms; Information Science--Information Retrieval; Optimization; Computer Editing; CRES Expert System; Trie Index Optimization; Data Processing
Further Results on the Security of Partitioned Dynamic Statistical Databases,1989,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0024620697&doi=10.1145%2f62032.62036&partnerID=40&md5=50f9160fb15e20627a4a5e70da6c056a,"Partitioning is a highly secure approach to protecting statistical databases. When updates are introduced, security depends on putting restrictions on the sizes of partition sets which may be queried. To overcome this problem, attempts have been made to add “dummy” records. Recent work has shown that this leads to high information loss. This paper reconsiders the restrictions on the size of partitioning sets required to achieve a high level of security. Updates of two records at a time were studied earlier, and security was found to hold if the sizes of the partition sets were kept even. In this paper an extended model is presented, allowing very general updates to be performed. The security problem is thoroughly studied, giving if and only if conditions. The earlier result is shown to be part of a corollary to the main theorem of this paper. Alternatives to adding dummy records are presented and the practical implications of the theory for the database manager are discussed. © 1989, ACM. All rights reserved.",Algebraic methods; dynamic partitioned systems; mean and median queries,Data Processing--Security of Data; Statistical Methods; Database Security; Partitioned Databases; Statistical Databases; Database Systems
Data Replicas in Distributed Information Services,1989,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0024622934&doi=10.1145%2f62032.62035&partnerID=40&md5=32d6a6567eb5e46a8f6e1bab8c848242,"In an information distribution network in which records are repeatedly read, it is cost-effective to keep read-only copies in work locations. This paper presents a method of updating replicas that need not be immediately synchronized with the source data or with each other. The method allows an arbitrary mapping from source records to replica records. It is fail-safe, maximizes workstation autonomy, and is well suited to a network with slow, unreliable, and/or expensive communications links. The algorithm is a manipulation of queries, which are represented as short encodings. When a response is generated, we record which portion of the source database was used. Later, when the source data are updated, this information is used to identify obsolete replicas. For each workstation, the identity of obsolete replicas is saved until a workstation process asks for this information. This workstation process deletes each obsolete replica, and replaces it by an up-to-date version either promptly or the next time the application asks for this particular item. Throughout, queries are grouped so that the impact of each source update transaction takes effect atomically at each workstation. Optimizations of the basic algorithm are outlined. These overlap change dissemination with user service, allow the mechanism to be hidden within the data delivery subsystem, and permit very large networks. © 1989, ACM. All rights reserved.",Cache; consistency; copies; currency; query; read-only; replicas; transaction; workstation,Computer Programming--Algorithms; Information Services; Optimization; Database Systems
Linear Hashing with Separators—A Dynamic Hashing Scheme Achieving One-Access Retrieval,1988,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0024085004&doi=10.1145%2f44498.44500&partnerID=40&md5=8617c09234dd8833a9b3d8833bfacfbf,"A new dynamic hashing scheme is presented. Its most outstanding feature is that any record can be retrieved in exactly one disk access. This is achieved by using a small amount of supplemental internal storage that stores enough information to uniquely determine the current location of any record. The amount of internal storage required is small: typically one byte for each page of the file. The necessary address computation, insertion, and expansion algorithms are presented and the performance is studied by means of simulation. The new method is the first practical method offering one-access retrieval for large dynamic files. © 1988, ACM. All rights reserved.",Dynamic hashing schemes; extendible hashing; linear probing; one-access retrieval; open addressing,DATA PROCESSING - File Organization; ADDRESS COMPUTATION; DYNAMIC HASHING SCHEMES; EXTENDIBLE HASHING; LINEAR HASHING; ONE-ACCESS RETRIEVAL; OPEN ADDRESSING; DATABASE SYSTEMS
Extended algebra and calculus for nested relational databases,1988,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0024135267&doi=10.1145%2f49346.49347&partnerID=40&md5=10520d6c658953598ac6f1dfc26717e3,"Relaxing the assumption that relations are always in First-Normal-Form (1NF) necessitates a reexamination of the fundamentals of relational database theory. In this paper we take a first step towards unifying the various theories of ¬1NF databases. We start by determining an appropriate model to couch our formalisms in. We then define an extended relational calculus as the theoretical basis for our ¬1NF database query language. We define a minimal extended relational algebra and prove its equivalence to the ¬1NF relational calculus. We define a class of ¬1NF relations with certain “good” properties and extend our algebra operators to work within this domain. We prove certain desirable equivalences that hold only if we restrict our language to this domain. © 1988, ACM. All rights reserved.",Extended algebra and calculus; nested relations; non-first normal form; partitioned normal form,Mathematical Techniques--Algebra; Nested Relational Databases; Non-First Normal Form Database; Partitioned Normal Forms; Query Languages; Relational Calculus; Database Systems
A Dynamic Framework for Object Projection Views,1988,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0023981219&doi=10.1145%2f42201.42202&partnerID=40&md5=8b97daa22d064d593a4d1dcbc5f16739,"User views in a relational database obtained through a single projection (“projection views”) are considered in a new framework. Specifically, such views, where each tuple in the view represents an object (“object-projection views”), are studied using the dynamic relational model, which captures the evolution of the database through consecutive updates. Attribute sets that yield object-projection views are characterized using the static and dynamic functional dependencies satisfied by the database. Object-projection views are then described using the static and dynamic functional dependencies “inherited” from the original database. Finally, the impact of dynamic constraints on the view update problem is studied in a limited context. This paper demonstrates that new, useful information about views can be obtained by looking at the evolution of the database as captured by the dynamic relational model. © 1988, ACM. All rights reserved.",Database schema; dynamic constraints; functional dependency; object; relational database; update; view,DATABASE SCHEMA; DYNAMIC CONSTRAINTS; FUNCTIONAL DEPENDENCY; OBJECT PROJECTION VIEWS; DATABASE SYSTEMS
Fuzzy Functional Dependencies and Lossless Join Decomposition of Fuzzy Relational Database Systems,1988,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0024034059&doi=10.1145%2f42338.42344&partnerID=40&md5=4368a33fc7aaeede689048b9a195a608,"This paper deals with the application of fuzzy logic in a relational database environment with the objective of capturing more meaning of the data. It is shown that with suitable interpretations for the fuzzy membership functions, a fuzzy relational data model can be used to represent ambiguities in data values as well as impreciseness in the association among them. Relational operators for fuzzy relations have been studied, and applicability of fuzzy logic in capturing integrity constraints has been investigated. By introducing a fuzzy resemblance measure EQUAL for comparing domain values, the definition of classical functional dependency has been generalized to fuzzy functional dependency (ffd). The implication problem of ffds has been examined and a set of sound and complete inference axioms has been proposed. Next, the problem of lossless join decomposition of fuzzy relations for a given set of fuzzy functional dependencies is investigated. It is proved that with a suitable restriction on EQUAL, the design theory of a classical relational database with functional dependencies can be extended to fuzzy relations satisfying fuzzy functional dependencies. © 1988, ACM. All rights reserved.",Functional dependency; fuzzy functional dependency; fuzzy logic; fuzzy measure; fuzzy relation; inference axiom; lossless join; membership function; relational database,MATHEMATICAL TECHNIQUES - Fuzzy Sets; FUZZY FUNCTIONAL DEPENDENCIES; FUZZY RELATIONAL DATABASE SYSTEMS; LOSSLESS JOIN DECOMPOSITION; DATABASE SYSTEMS
Concurrent Search Structure Algorithms,1988,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0023984157&doi=10.1145%2f42201.42204&partnerID=40&md5=2aaed9d4cbda9eec8b011466e36a8628,"A dictionary is an abstract data type supporting the actions member, insert, and delete. A search structure is a data structure used to implement a dictionary. Examples include B trees, hash structures, and unordered lists. Concurrent algorithms on search structures can achieve more parallelism than standard concurrency control methods would suggest, by exploiting the fact that many different search structure states represent one dictionary state. We present a framework for verifying such algorithms and for inventing new ones. We give several examples, one of which exploits the structure of Banyan family interconnection networks. We also discuss the interaction between concurrency control and recovery as applied to search structures. © 1988, ACM. All rights reserved.",Algorithms; B-tree; concurrency; data structure; hash structure; parallel,COMPUTER PROGRAMMING - Algorithms; ABSTRACT DATA TYPE; BANYAN INTERCONNECTION NETWORKS; CONCURRENT ALGORITHMS; DICTIONARY; SEARCH STRUCTURE; DATA PROCESSING
New Methods and Fast Algorithms for Database Normalization,1988,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0024087601&doi=10.1145%2f44498.44499&partnerID=40&md5=691ca371db9ebec8f2b6c36b8f56084a,"A new method for computing minimal covers is presented using a new type of closure that allows significant reductions in the number of closures computed for normalizing relations. Benchmarks are reported comparing the new and the standard techniques. © 1988, ACM. All rights reserved.",Functional dependency; normalization; relations; third normal form,COMPUTER PROGRAMMING - Algorithms; DATABASE NORMALIZATION; FAST ALGORITHMS; FUNCTIONAL DEPENDENCY; REDUNDANT DEPENDENCIES; RELATIONAL DATABASE; DATABASE SYSTEMS
Extending a Database System with Procedures,1987,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0023415885&doi=10.1145%2f27629.27631&partnerID=40&md5=853afa26e57cec85e3b706ac3ca7fad7,"This paper suggests that more powerful database systems (DBMS) can be built by supporting database procedures as full-fledged database objects. In particular, allowing fields of a database to be a collection of queries in the query language of the system is shown to allow the natural expression of complex data relationships. Moreover, many of the features present in object-oriented systems and semantic data models can be supported by this facility. In order to implement this construct, extensions to a typical relational query language must be made, and considerable work on the execution engine of the underlying DBMS must be accomplished. This paper reports on the extensions for one particular query language and data manager and then gives performance figures for a prototype implementation. Even though the performance of the prototype is competitive with that of a conventional system, suggestions for improvement are presented. © 1987, ACM. All rights reserved.",Extended DBMS; object-oriented DBMS; procedures; relational database,DATABASE PROCEDURES; OBJECT-ORIENTED SYSTEMS; RELATIONAL QUERY LANGUAGE; SEMANTIC DATA MODELS; DATABASE SYSTEMS
Physical Database Design for Relational Databases,1988,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0023980306&doi=10.1145%2f42201.42205&partnerID=40&md5=4dcd34b19a21a11a690a22975796c921,"This paper describes the concepts used in the implementation of DBDSGN, an experimental physical design tool for relational databases developed at the IBM San Jose Research Laboratory. Given a workload for System R (consisting of a set of SQL statements and their execution frequencies), DBDSGN suggests physical configurations for efficient performance. Each configuration consists of a set of indices and an ordering for each table. Workload statements are evaluated only for atomic configurations of indices, which have only one index per table. Costs for any configuration can be obtained from those of the atomic configurations. DBDSGN uses information supplied by the System R optimizer both to determine which columns might be worth indexing and to obtain estimates of the cost of executing statements in different configurations. The tool finds efficient solutions to the index-selection problem; if we assume the cost estimates supplied by the optimizer are the actual execution costs, it finds the optimal solution. Optionally, heuristics can be used to reduce execution time. The approach taken by DBDSGN in solving the index-selection problem for multiple-table statements significantly reduces the complexity of the problem. DBDSGN's principles were used in the Relational Design Tool (RDT), an IBM product based on DBDSGN, which performs design for SQL/DS, a relational system based on System R. System R actually uses DBDSGN's suggested solutions as the tool expects because cost estimates and other necessary information can be obtained from System R using a new SQL statement, the EXPLAIN statement. This illustrates how a system can export a model of its internal assumptions and behavior so that other systems (such as tools) can share this model. © 1988, ACM. All rights reserved.",Index selection; physical database design; query optimization; relational database,OPTIMIZATION; DBDSGN; PHYSICAL DATABASE DESIGN; SYSTEM R; DATABASE SYSTEMS
Multiple-Query Optimization,1988,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0023977778&doi=10.1145%2f42201.42203&partnerID=40&md5=04589a258b11b6a500518157142c4741,"Some recently proposed extensions to relational database systems, as well as to deductive database systems, require support for multiple-query processing. For example, in a database system enhanced with inference capabilities, a simple query involving a rule with multiple definitions may expand to more than one actual query that has to be run over the database. It is an interesting problem then to come up with algorithms that process these queries together instead of one query at a time. The main motivation for performing such an interquery optimization lies in the fact that queries may share common data. We examine the problem of multiple-query optimization in this paper. The first major contribution of the paper is a systematic look at the problem, along with the presentation and analysis of algorithms that can be used for multiple-query optimization. The second contribution lies in the presentation of experimental results. Our results show that using multiple-query processing algorithms may reduce execution cost considerably. © 1988, ACM. All rights reserved.",Common access paths; deductive databases; query optimization; relational databases; sharing of data,COMPUTER PROGRAMMING - Algorithms; OPTIMIZATION; COMMON ACCESS PATHS; DEDUCTIVE DATABASES; HEURISTIC METHODS; MULTIPLE-QUERY OPTIMIZATION; DATABASE SYSTEMS
Serializability with Constraints,1987,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0023416326&doi=10.1145%2f27629.214284&partnerID=40&md5=f43211a50ba427e91988099ed67e8bce,"This paper deals with the serializability theory for single-version and multiversion database systems. We first introduce the concept of disjoint-interval topological sort (DITS, for short) of an arc-labeled directed acyclic graph. It is shown that a history is serializable if and only if its transaction IO graph has a DITS. We then define several subclasses of serializable histories, based on the constraints imposed by write-write, write-read, read-write, or read-read conflicts, and investigate inclusion relationships among them. In terms of DITS, we give a sufficient condition for a class of serializable histories to be polynomially recognizable, which is then used to show that a new class of histories, named WRW, can be recognized in polynomial time. We also present NP-completeness results for the problem of testing membership in some other classes. In the second half of this paper, we extend these results to multiversion database systems. The inclusion relationships among multiversion classes defined by constraints, such as write-write and write-read, are investigated. One such class coincides with class DMVSR, introduced by Papadimitriou and Kanellakis, and gives a simple characterization of this class. It is shown that for most constraints, multiversion classes properly contain the corresponding single-version classes. Complexity results for the membership testing are also discussed. © 1987, ACM. All rights reserved.",Concurrency control; serializability; transactions,COMPUTER SYSTEMS PROGRAMMING - Sorting; CONCURRENCY CONTROL; DISJOINT-INTERVAL TOPOLOGICAL SORT; SERIALIZABILITY; DATABASE SYSTEMS
On First-Order-Logic Databases,1987,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0023416040&doi=10.1145%2f27629.27630&partnerID=40&md5=9438975bb89fe0e7b1551a52b486b046,"The use of first-order logic as database logic is shown to be powerful enough for formalizing and implementing not only relational but also hierarchical and network-type databases. It enables one to treat all the types of databases in a uniform manner. This paper focuses on the database language for heterogeneous databases. The language is shown to be general enough to specify constraints for a particular type of database, so that a specification of database type can be “translated” to the specification given in the database language, creating a “logical environment” for different views that can be defined by users. Owing to the fact that any database schema is seen as a first-order theory expressed by a finite set of sentences, the problems concerned with completeness and compactness of the database logic discussed by Jacobs (“On Database Logic,” J. ACM 29,2 (Apr. 1982), 310-332) are avoided. © 1987, ACM. All rights reserved.",Completeness; database language; database logic; semantic model; soundness,COMPUTER METATHEORY - Formal Logic; DATABASE LOGIC; FIRST-ORDER LOGIC; HIERARCHICAL DATABASES; NETWORK DATABASES; RELATIONAL DATABASES; DATABASE SYSTEMS
Optimal Signature Extraction and Information Loss,1987,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0023417725&doi=10.1145%2f27629.214285&partnerID=40&md5=7383ee1640bab9e65054a5102e346179,"Signature files seem to be a promising access method for text and attributes. According to this method, the documents (or records) are stored sequentially in one file (“text file”), while abstractions of the documents (“signatures”) are stored sequentially in another file (“signature file”). In order to resolve a query, the signature file is scanned first, and many nonqualifying documents are immediately rejected. We develop a framework that includes primary key hashing, multiattribute hashing, and signature files. Our effort is to find the optimal signature extraction method. The main contribution of this paper is that we present optimal and efficient suboptimal algorithms for assigning words to signatures in several environments. Another contribution is that we use information theory, and study the relationship of the false drop probability Fd and the information that is lost during signature extraction. We give tight lower bounds on the achievable Fd and show that a simple relationship holds between the two quantities in the case of optimal signature extraction with uniform occurrence and query frequencies. We examine hashing as a method to map words to signatures (instead of the optimal way), and show that the same relationship holds between Fd and loss, indicating that an invariant may exist between these two quantities for every signature extraction method. © 1987, ACM. All rights reserved.",Hashing; information theory; signature files; superimposed coding,DATABASE SYSTEMS; INFORMATION LOSS; OPTIMAL SIGNATURE EXTRACTION; SIGNATURE FILES; DATA PROCESSING
Reduced MVDs and Minimal Covers,1987,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0023419903&doi=10.1145%2f27629.214286&partnerID=40&md5=d3c5e8ee6c5c29be5cfb62eab4c99cb2,"Multivalued dependencies (MVDs) are data dependencies that appear frequently in the “real world” and play an important role in designing relational database schemes. Given a set of MVDs to constrain a database scheme, it is desirable to obtain an equivalent set of MVDs that do not have any redundancies. In this paper we define such a set of MVDs, called reduced MVDs, and present an algorithm to obtain reduced MVDs. We also define a minimal cover of a set of MVDs, which is a set of reduced MVDs, and give an efficient method to find such a minimal cover. The significance and properties of reduced MVDs are also discussed in the context of database design (e.g., 4NF decomposition) and conflict-free MVDs. © 1987, ACM. All rights reserved.",Database design; decomposition; fourth minimal form; minimal cover; multivalued dependency,COMPUTER PROGRAMMING - Algorithms; DATABASE SCHEMA DESIGN; MINIMAL COVERS; MULTIVALUED DEPENDENCIES; DATABASE SYSTEMS
A simple bounded disorder file organization with good performance,1988,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0024138189&doi=10.1145%2f49346.50067&partnerID=40&md5=fea6fc141d9c5cde5751e4fad4f64df4,"A bounded-disorder (BD) file is one in which data are organized into nodes that are indexed, e.g., by means of a B-tree. The data nodes are multibucket nodes that are accessed by hashing. In this paper we present two important improvements to the BD organization as originally described. First, records in a data node that overflow their designated primary bucket are stored in a single overflow bucket which is itself a bucket of the data node. Second, when file space needs to be increased, partial expansions are used that employ elastic buckets. Analysis and simulation results demonstrate that this variant of the BD organization has utilization, random access performance, and file growth performance that can be competitive with good extendible hashing methods, while supporting high-performance sequential access. The simplicity of the organization results in simple algorithms for realizing the organization. © 1988, ACM. All rights reserved.",Access performance; B-trees; dynamic files; extendible hashing; index sequential access; indexed files; storage management,Computer Programming--Algorithms; Computer Simulation; Bounded Disorder File Organization; Dynamic Files; Index Sequential Access; Indexed Files; Storage Management; Data Processing
Implementation Concepts for an Extensible Data Model and Data Language,1988,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0024082732&doi=10.1145%2f44498.45062&partnerID=40&md5=57fefb19db4da42c9e933f5e4a1f0216,"Future database systems must feature extensible data models and data languages in order to accommodate the novel data types and special-purpose operations that are required by nontraditional database applications. In this paper, we outline a functional data model and data language that are targeted for the semantic interface of GENESIS, an extensible DBMS. The model and language are generalizations of FQL [11] and DAPLEX [40], and have an implementation that fits ideally with the modularity required by extensible database technologies. We explore different implementations of functional operators and present experimental evidence that they have efficient implementations. We also explain the advantages of a functional front-end to ¬1NF databases, and show how our language and implementation are being used to process queries on both 1NF and ¬1NF relations. © 1988, ACM. All rights reserved.",Coroutines; functional data model; lazy evaluation,COMPUTER PROGRAMMING LANGUAGES; DATA LANGUAGE; FUNCTIONAL DATA MODEL; LAZY EVALUATION; STREAM TRANSLATORS; DATABASE SYSTEMS
Update and Retrieval in a Relational Database Through a Universal Schema Interface,1988,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0024130348&doi=10.1145%2f49346.49884&partnerID=40&md5=b2ea9c559c7c551c04c95763b9e84cbf,"A database system that is based on the universal relation (UR) model aims at freeing its users from specifying access paths on both the physical and on the logical levels. All information about the logical structure of the database (i.e., its conceptual scheme) is hidden from users; they need only to know the attribute names, which now carry all the semantics of the database. Previous work on UR interfaces has concentrated on the design and implementation of query languages that serve to facilitate retrieval of data from a relational database. On the other hand, updates are always handled as before, which means that users still have to know the logical structure of the database in case they want to insert, delete, or modify tuples. In this paper the concepts underlying a UR interface, which is really “universal,” are presented; it is based on the UR model, and it permits not only queries but also updates: Combinations of attributes that may participate in an update-operation (“objects”) have to be specified during the design phase of the database, and are then embodied into the database scheme by an extended synthesis algorithm. They form the basis for any insertion or deletion operation. A precise definition of “insertable” tuples, and of the insert- and delete-operation in this new context, is given. It is then shown that these operations modify a database state in such a way that a representative instance always exists. This is accomplished by providing a more detailed version of Sagiv’s uniqueness condition and by exploring the structure of nonunique objects. Since the underlying database always has a representative instance, this instance can be used to define the window function for retrieval. It is shown that it is still possible to compute windows by a union of minimal extension joins. © 1988, ACM. All rights reserved.",Chase process; deletion; functional dependency; insertion; key; minimal extension join; null value; relational algebra; relational database; representative instance; scheme; synthesis algorithm; total projection; uniqueness condition; universal relation interface,Computer Interfaces; Information Science--Information Retrieval; Database Updates; Minimal Extension Joins; Universal Relation Interface; Universal Scheme Interfaces; Database Systems
A homogeneous relational model and query languages for temporal databases,1988,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0024130068&doi=10.1145%2f49346.50065&partnerID=40&md5=fb86c797b8f28bfb250421ca5d8b6fa0,"In a temporal database, time values are associated with data item to indicate their periods of validity. We propose a model for temporal databases within the framework of the classical database theory. Our model is realized as a temporal parameterization of static relations. We do not impose any restrictions upon the schemes of temporal relations. The classical concepts of normal forms and dependencies are easily extended to our model, allowing a suitable design for a database scheme. We present a relational algebra and a tuple calculus for our model and prove their equivalence. Our data model is homogeneous in the sense that the periods of validity of all the attributes in a given tuple of a temporal relation are identical. We discuss how to relax the homogeneity requirement to extend the application domain of our approach. © 1988, ACM. All rights reserved.",Historical databases; relational calculus; relational model; temporal data; temporal databases; time; tuple calculus,Mathematical Techniques--Algebra; Query Languages; Relational Algebra; Relational Calculus; Temporal Databases; Tuple Calculus; Database Systems
A Model-Based Approach to Updating Databases with Incomplete Information,1988,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0024034483&doi=10.1145%2f42338.42386&partnerID=40&md5=0520e8d1a4268a99b7f7fe665c8afb72,"Suppose one wishes to construct, use, and maintain a database of facts about the real world, even though the state of that world is only partially known. In the artificial intelligence domain, this problem arises when an agent has a base set of beliefs that reflect partial knowledge about the world, and then tries to incorporate new, possibly contradictory knowledge into this set of beliefs. In the database domain, one facet of this situation is the well-known null values problem. We choose to represent such a database as a logical theory, and view the models of the theory as representing possible states of the world that are consistent with all known information. How can new information be incorporated into the database? For example, given the new information that “b or c is true,” how can one get rid of all outdated information about b and c, add the new information, and yet in the process not disturb any other information in the database? In current-day database management systems, the difficult and tedious burden of determining exactly what to add and remove from the database is placed on the user. The goal of our research was to relieve users of that burden, by equipping the database management system with update algorithms that can automatically determine what to add and remove from the database. Under our approach, new information about the state of the world is input to the database management system as a well-formed formula that the state of the world is now known to satisfy. We have constructed database update algorithms to interpret this update formula and incorporate the new information represented by the formula into the database without further assistance from the user. In this paper we show how to embed the incomplete database and the incoming information in the language of mathematical logic, explain the semantics of our update operators, and discuss the algorithms that implement these operators. © 1988, ACM. All rights reserved.",Fuzzy and probabilistic reasoning; incomplete information; uncertainty; updates,COMPUTER PROGRAMMING - Algorithms; DATABASE UPDATES; INCOMPLETE INFORMATION; UNCERTAINTY; UPDATE ALGORITHMS; DATABASE SYSTEMS
Data Allocation in Distributed Database Systems,1988,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0024082509&doi=10.1145%2f44498.45063&partnerID=40&md5=17ced119414fa545af70c992ab242fa7,"The problem of allocating the data of a database to the sites of a communication network is investigated. This problem deviates from the well-known file allocation problem in several aspects. First, the objects to be allocated are not known a priori; second, these objects are accessed by schedules that contain transmissions between objects to produce the result. A model that makes it possible to compare the cost of allocations is presented; the cost can be computed for different cost functions and for processing schedules produced by arbitrary query processing algorithms. For minimizing the total transmission cost, a method is proposed to determine the fragments to be allocated from the relations in the conceptual schema and the queries and updates executed by the users. For the same cost function, the complexity of the data allocation problem is investigated. Methods for obtaining optimal and heuristic solutions under various ways of computing the cost of an allocation are presented and compared. Two different approaches to the allocation management problem are presented and their merits are discussed. © 1988, ACM. All rights reserved.",Allocation; complexity analysis; database; greedy method; partitioning,COMPUTER NETWORKS; COMPUTER SOFTWARE - Software Engineering; DATA ALLOCATION; DYNAMIC SCHEDULES; GREEDY METHOD; HEURISTIC ALLOCATIONS; OPTIMAL ALLOCATIONS; STATIC SCHEDULES; DATABASE SYSTEMS
The Overhead of Locking (and Commit) Protocols in Distributed Databases,1987,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0023419805&doi=10.1145%2f27629.28053&partnerID=40&md5=20d54c8bc76f928c78dca351cda610ee,"The main purpose of a locking protocol is to ensure correct interleaving of actions executed by concurrent transactions. The locking protocol consists of a set of rules dictating how accessed entities should be locked and unlocked. As a result of obeying the rules, transactions in a distributed database incur an overhead. We propose three measures of evaluating this overhead, each most suitable to a different type of underlying communication network. Then, using a graph theoretic model, we analyze and compare three protocols according to each measure: two-phase locking, two-phase locking with a fixed order imposed on the database entities (ensuring deadlock freedom), and the tree protocol. In practice, a transaction also executes the two-phase commit protocol in order to guarantee atomicity. Therefore, the combined overhead of each locking protocol and the two-phase commit protocol is also determined. © 1987, ACM. All rights reserved.",Commit protocols; computer network; concurrency control; database locking; deadlock; locking protocols; message passing; transaction,COMPUTER NETWORKS - Protocols; COMMIT PROTOCOLS; CONCURRENCY CONTROL; LOCKING PROTOCOLS; MESSAGE PASSING; DATABASE SYSTEMS
A Methodology for Creating User Views in Database Design,1988,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0024082510&doi=10.1145%2f44498.45064&partnerID=40&md5=b559932ff1da09d70eb5bb9b5dd4b6b8,"The View Creation System (VCS) is an expert system that engages a user in a dialogue about the information requirements for some application, develops an Entity-Relationship model for the user's database view, and then converts the E-R model to a set of Fourth Normal Form relations. This paper describes the knowledge base of VCS. That is, it presents a formal methodology, capable of mechanization as a computer program, for accepting requirements from a user, identifying and resolving inconsistencies, redundancies, and ambiguities, and ultimately producing a normalized relational representation. Key aspects of the methodology are illustrated by applying VCS's knowledge base to an actual database design task. © 1988, ACM. All rights reserved.",View Creation System,COMPUTER PROGRAMMING; EXPERT SYSTEM; KNOWLEDGE BASE; USER VIEWS; VIEW CREATION SYSTEM (VCS); DATABASE SYSTEMS
Properties and update semantics of consistent views,1988,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0024140322&doi=10.1145%2f49346.50068&partnerID=40&md5=36c0804c4ea84d4d5ecf6270c6822b0f,"The problem of translating view updates to database updates is considered. Both databases and views are modeled as data abstractions. A data abstraction consists of a set of states and of a set of primitive update operators representing state transition functions. It is shown how complex update programs can be built from primitive update operators and how view update programs are translated into database update programs. Special attention is paid to a class of views that we call “consistent.” Loosely speaking, a consistent view is a view with the following property: If the effect of a view update program on a view state is determined, then the effect of the corresponding database update is unambiguously determined. Thus, in order to know how to translate a given view update into a database update, it is sufficient to be aware of a functional specification of such a program. We show that consistent views have a number of interesting properties with respect to the concurrency of (high-level) update transactions. Moreover we show that the class of consistent views includes as a subset the class of views that translate updates under maintenance of a constant complement. However, we show that there exist consistent views that do not translate under constant complement. The results of Bancilhon and Spyratos [6] are generalized in order to capture the update semantics of the entire class of consistent views. In particular we show that the class of consistent views is obtained if we relax the requirement of a constant complement by allowing the complement to decrease according to a suitable partial order. © 1988, ACM. All rights reserved.",Concurrency; consistency; data abstractions; transactions; update semantics; view definition,Concurrency; Consistent Views; Data Abstractions; Update Semantics; View Updates; Database Systems
Database Concurrency Control Using Data Flow Graphs,1988,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0024034060&doi=10.1145%2f42338.42345&partnerID=40&md5=29a13d1e56114ecca840ee6754d3b486,"A specialized data flow graph, Database Flow Graph (DBFG) is introduced. DBFGs may be used for scheduling database operations, particularly in an MIMD database machine environment. A DBFG explicitly maintains intertransaction and intratransaction dependencies, and is constructed from the Transaction Flow Graphs (TFG) of active transactions. A TFG, in turn, is the generalization of a query tree used, for example, in DIRECT [15]. All DBFG schedules are serializable and deadlock free. Operations needed to create and maintain the DBFG structure as transactions are added or removed from the system are discussed. Simulation results show that DBFG scheduling performs as well as two-phase locking. © 1988, ACM. All rights reserved.",Concurrency control,MATHEMATICAL TECHNIQUES - Graph Theory; DATA FLOW GRAPHS; DATABASE CONCURRENCY CONTROL; DEADLOCK; SERIALIZABILITY; DATABASE SYSTEMS
Comment on Bancilhon and Spyratos’ “Update Semantics and Relational Views”,1987,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976762728&doi=10.1145%2f27629.214296&partnerID=40&md5=df65321d6e12a747c10a9850d6496df9,[No abstract available],,
Buffer Management in Relational Database Systems,1986,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0022887571&doi=10.1145%2f7239.7336&partnerID=40&md5=79bb2641cc22dd17cf5ce5344b8f4e06,"The hot-set model, characterizing the buffer requirements of relational queries, is presented. This model allows the system to determine the optimal buffer space to be allocated to a query; it can also be used by the query optimizer to derive efficient execution plans accounting for the available buffer space, and by a query scheduler to prevent thrashing. The hot-set model is compared with the working-set model. A simulation study is presented. © 1986, ACM. All rights reserved.",Buffer management; hot points; hot-set model; merging scans; nested scans; scheduling; sequential scans; thrashing; unstable intervals; working set,SCHEDULING; BUFFER MANAGEMENT; QUERY OPTIMIZER; QUERY PROCESSING; DATABASE SYSTEMS
The Temporal Query Language TQuel,1987,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0023366818&doi=10.1145%2f22952.22956&partnerID=40&md5=36885f746dbe7843fc27e25bfc10a53e,"Recently, attention has been focused on temporal databases, representing an enterprise over time. We have developed a new language, Tquel, to query a temporal database. TQuel was designed to be a minimal extension, both syntactically and semantically, of Quel, the query language in the Ingres relational database management system. This paper discusses the language informally, then provides a tuple relational calculus semantics for the TQuel statements that differ from their Quel counterparts, including the modification statements. The three additional temporal constructs defined in Tquel are shown to be direct semantic analogues of Quel's where clause and target list. We also discuss reducibility of the semantics to Quel's semantics when applied to a static database. TQuel is compared with ten other query languages supporting time. © 1987, ACM. All rights reserved.",Historical database; Quel; relational calculus; relational model; rollback database; temporal database; tuple calculus,COMPUTER PROGRAMMING LANGUAGES; RELATIONAL CALCULUS; TEMPORAL DATABASES; TEMPORAL QUERY LANGUAGE; TQUEL; TUPLE CALCULUS; DATABASE SYSTEMS
Partial expansions for file organizations with an index,1987,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0023310972&doi=10.1145%2f12047.12049&partnerID=40&md5=76ad5739c3d1c16ff3b9a48ed545de39,"A new way to increase file space in dynamically growing files is introduced in which substantial improvement in file utilization can be achieved. It makes use of partial expansions in which, instead of doubling the space associated with some part of the file, the space grows at a slower rate. Unlike previous versions of partial expansion in which the number of buckets involved in file growth is increased by less than a factor of two, the new method expands file space by increasing bucket size via “elastic buckets.” This permits partial expansions to be used with a wide range of indexed files, including B-trees. The results of using partial expansions are analyzed, and the analysis confirmed by a simulation study. The analysis and simulation demonstrate that the file utilization gains are substantial and that fears of excessive insertion cost resulting from more frequent file growth are unfounded. © 1987, ACM. All rights reserved.",B-trees; dynamic files; indexed files; partial expansions; storage management; storage utilization,DATABASE SYSTEMS; FILE UTILIZATION; INSERTION COST; PARTIAL EXPANSIONS; DATA PROCESSING
The Use of Regression Methodology for the Compromise of Confidential Information in Statistical Databases,1987,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976672368&doi=10.1145%2f32204.42174&partnerID=40&md5=0fdf203ac011a4c302e2d5457561a0fc,"A regression methodology based technique can be used to compromise confidentiality in a statistical database. This holds true even when the DBMS prevents application of regression methodology to the database. Existing inference controls, including cell restriction, perturbation, and table restriction approaches, are shown to be generally ineffective against this compromise technique. The effect of incomplete supplemental knowledge on the regression methodology based compromise technique is examined. Finally, some potential complicators of this disclosure scheme are introduced. © 1987, ACM. All rights reserved.",Compromise; confidentiality; disclosure; inference controls; privacy; synthetic database; U.S. Census database,
Multikey Access Methods Based on Superimposed Coding Techniques,1987,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976700871&doi=10.1145%2f32204.32222&partnerID=40&md5=5be232b911b4a973876fa3ba2a1656f2,"Both single-level and two-level indexed descriptor schemes for multikey retrieval are presented and compared. The descriptors are formed using superimposed coding techniques and stored using a bit-inversion technique. A fast-batch insertion algorithm for which the cost of forming the bit-inverted file is less than one disk access per record is presented. For large data files, it is shown that the two-level implementation is generally more efficient for queries with a small number of matching records. For queries that specify two or more values, there is a potential problem with the two-level implementation in that costs may accrue when blocks of records match the query but individual records within these blocks do not. One approach to overcoming this problem is to set bits in the descriptors based on pairs of indexed terms. This approach is presented and analyzed. © 1987, ACM. All rights reserved.",Descriptors; hashing; partial match retrieval; record signatures; superimposed coding,
A new normal form for nested relations,1987,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0023312522&doi=10.1145%2f12047.13676&partnerID=40&md5=6df78134b0d6174312fd08313da09de3,"We consider nested relations whose schemes are structured as trees, called scheme trees, and introduce a normal form for such relations, called the nested normal form. Given a set of attributes U, and a set of multivalued dependencies (MVDs) M over these attributes, we present an algorithm to obtain a nested normal form decomposition of U with respect to M. Such a decomposition has several desirable properties, such as explicitly representing a set of full and embedded MVDs implied by M, and being a faithful and nonredundant representation of U. Moreover, if the given set of MVDs is conflict-free, then the nested normal form decomposition is also dependency-preserving. Finally, we show that if M is conflict-free, then the set of root-to-leaf paths of scheme trees in nested normal form decomposition is precisely the unique 4NF decomposition [9, 16] of U with respect to M. © 1987, ACM. All rights reserved.",Database design; decomposition; dependency preservation; lossless; multivalued dependency; nested relation; scheme tree,DATABASE SYSTEMS - Design; DECOMPOSITION; MULTIVALUED DEPENDENCY; NESTED RELATIONS; NORMAL FORM; DATA PROCESSING
The Partition Model: A Deductive Database Model,1987,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0023310620&doi=10.1145%2f12047.22718&partnerID=40&md5=154797a101074c4e24af632d2f148c8b,"We present a new database model in which each attribute is modeled by a family of disjoint subsets of an underlying population of objects. Such a family is called a partitioning, and the set of all partitionings is turned into a lattice by appropriately defining product and sum. A database is seen as a function from a sublattice into the lattice of partitionings. The model combines the following features: (1) syntactic simplicity (essentially that of the relational model), (2) powerful means for the specification of semantic information (in the form of lattice equations), and (3) deductive capability (essentially that of set theory). The relational model of data and the basic constructs of semantic modeling can be embedded into our model in a simple and straightforward manner. © 1987, ACM. All rights reserved.",Database semantics; database state transitions,DATABASE SEMANTICS; DEDUCTIVE DATABASE MODEL; PARTITION MODEL; DATABASE SYSTEMS
IFO: A Formal Semantic Database Model,1987,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976802358&doi=10.1145%2f32204.32205&partnerID=40&md5=8af6c9d0e507131970d1a4e32a3092c0,"A new, formally defined database model is introduced that combines fundamental principles of “semantic” database modeling in a coherent fashion. Using a graph-based formalism, the IFO model provides mechanisms for representing structured objects, and functional and ISA relationships between them. A number of fundamental results concerning semantic data modeling are obtained in the context of the IFO model. Notably, the types of object structure that can arise as a result of multiple uses of ISA relationships and object construction are described. Also, a natural, formal definition of update propagation is given, and it is shown that (under certain conditions) a correct update always exists. © 1987, ACM. All rights reserved.",Complex database objects; database theory; semantic database models; update propagation,
Report on the International Workshop on High-Performance Transaction Systems,1986,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976816236&doi=10.1145%2f7239.17346&partnerID=40&md5=9bb870499fb1e132085d91d70c074d9f,[No abstract available],,
Associative Searching in Multiple Storage Units,1987,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0023315010&doi=10.1145%2f12047.12048&partnerID=40&md5=f360a2b0ac5f3f60dc88b7c71d9307aa,"A file maintenance model, called the multiple random access storage units model, is introduced. Storage units can be accessed simultaneously, and the parallel processing of an associative query is achieved by distributing data evenly among the storage units. Maximum parallelism is obtained when data satisfying an associative query are evenly distributed for every possible query. An allocation scheme called M-cycle allocation is proposed to maintain large files of data on multiple random access storage units. The allocation scheme provides an efficient and straightforward indexing over multidimensional key spaces and supports the parallel processing of orthogonal range queries. Our analysis shows that M-cycle allocation achieves the near-optimum parallelism for processing the orthogonal range queries. Moreover, there is no duplication of records and no increase in insertion/deletion cost. © 1987, ACM. All rights reserved.",Complexity; hashing; order preservation; range queries,DATA PROCESSING - File Organization; INFORMATION SCIENCE - Information Retrieval; ASSOCIATIVE SEARCHING; FILE MAINTENANCE MODEL; MULTIPLE STORAGE UNITS; RANDOM ACCESS; DATABASE SYSTEMS
Extending Relational Algebra and Relational Calculus with Set-Valued Attributes and Aggregate Functions,1987,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976714128&doi=10.1145%2f32204.32219&partnerID=40&md5=f776f43ed65a4b20b8bfa2ad37a02e5b,"In commercial network database management systems, set-valued fields and aggregate functions are commonly supported. However, the relational database model, as defined by Codd, does not include set-valued attributes or aggregate functions. Recently, Klug extended the relational model by incorporating aggregate functions and by defining relational algebra and calculus languages. In this paper, relational algebra and relational calculus database query languages (as defined by Klug) are extended to manipulate set-valued attributes and to utilize aggregate functions. The expressive power of the extended languages is shown to be equivalent. We extend the relational algebra with three new operators, namely, pack, unpack, and aggregation-by-template. The extended languages form a theoretical framework for statistical database query languages. © 1987, ACM. All rights reserved.",Aggregation; extended relational algebra and calculus; set-valued relations,
Join Indices,1987,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0023365834&doi=10.1145%2f22952.22955&partnerID=40&md5=67ecdd8939f85699b8ac3ef8f93ef19a,"In new application areas of relational database systems, such as artificial intelligence, the join operator is used more extensively than in conventional applications. In this paper, we propose a simple data structure, called a join index, for improving the performance of joins in the context of complex queries. For most of the joins, updates to join indices incur very little overhead. Some properties of a join index are (i) its efficient use of memory and adaptiveness to parallel execution, (ii) its compatibility with other operations (including select and union), (iii) its support for abstract data type join predicates, (iv) its support for multirelation clustering, and (v) its use in representing directed graphs and in evaluating recursive queries. Finally, the analysis of the join algorithm using join indices shows its excellent performance. © 1987, ACM. All rights reserved.",Abstract data types; join; multirelation clustering; recursive query; relational query; semijoin; updates,COMPUTER PROGRAMMING - Algorithms; DATA PROCESSING - Data Structures; ABSTRACT DATA TYPES; JOIN ALGORITHM; JOIN INDEX; MULTIRELATION CLUSTERING; DATABASE SYSTEMS
Concurrency Control Performance Modeling: Alternatives and Implications,1987,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976728043&doi=10.1145%2f32204.32220&partnerID=40&md5=2280bc01df7d11be72b635c21ebef2e8,"A number of recent studies have examined the performance of concurrency control algorithms for database management systems. The results reported to date, rather than being definitive, have tended to be contradictory. In this paper, rather than presenting “yet another algorithm performance study,” we critically investigate the assumptions made in the models used in past studies and their implications. We employ a fairly complete model of a database environment for studying the relative performance of three different approaches to the concurrency control problem under a variety of modeling assumptions. The three approaches studied represent different extremes in how transaction conflicts are dealt with, and the assumptions addressed pertain to the nature of the database system’s resources, how transaction restarts are modeled, and the amount of information available to the concurrency control algorithm about transactions’ reference strings. We show that differences in the underlying assumptions explain the seemingly contradictory performance results. We also address the question of how realistic the various assumptions are for actual database systems. © 1987, ACM. All rights reserved.",Concurrency control,
On Modeling of Information Retrieval Concepts in Vector Spaces,1987,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0023366859&doi=10.1145%2f22952.22957&partnerID=40&md5=80c95a6c76389233f41c4942530640b2,"The Vector Space Model (VSM) has been adopted in information retrieval as a means of coping with inexact representation of documents and queries, and the resulting difficulties in determining the relevance of a document relative to a given query. The major problem in employing this approach is that the explicit representation of term vectors is not known a priori. Consequently, earlier researchers made the assumption that the vectors corresponding to terms are pairwise orthogonal. Such an assumption is clearly unrealistic. Although attempts have been made to compensate for this assumption by some separate, corrective steps, such methods are ad hoc and, in most cases, formally inconsistent. In this paper, a generalization of the VSM, called the GVSM, is advanced. The developments provide a solution not only for the computation of a measure of similarity (correlation) between terms, but also for the incorporation of these similarities into the retrieval process. The major strength of the GVSM derives from the fact that it is theoretically sound and elegant. Furthermore, experimental evaluation of the model on several test collections indicates that the performance is better than that of the VSM. Experiments have been performed on some variations of the GVSM, and all these results have also been compared to those of the VSM, based on inverse document frequency weighting. These results and some ideas for the efficient implementation of the GVSM are discussed. © 1987, ACM. All rights reserved.",Boolean algebra; document representation; generalized vector space; retrieval strategy; term cooccurrence; vector space theory,INFORMATION RETRIEVAL SYSTEMS - Mathematical Models; DOCUMENT REPRESENTATION; GENERALIZED VECTOR SPACE; QUERY REPRESENTATION; INFORMATION SCIENCE
Dynamic Quorum Adjustment for Partitioned Data,1987,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0023365499&doi=10.1145%2f22952.22953&partnerID=40&md5=139d2728e7087ad9be23349ef047eecf,"A partition occurs when functioning sites in a distributed system are unable to communicate. This paper introduces a new method for managing replicated data objects in the presence of partitions. Each operation provided by a replicated object has a set. of quorums, which are sets of sites whose cooperation suffices to execute the operation. The method permits an object's quorums to be adjusted dynamically in response to failures and recoveries. A transaction that is unable to progress using one set of quorums may switch to another, more favorable set, and transactions in different. Partitions may progress using different sets. This method has three novel aspects: (1) it supports a wider range of quorums than earlier proposals, (2) it, scales up effectively to large systems because quorum adjustments do not require global reconfiguration, and (3) it, systematically exploits the semantics of typed objects to support more flexible quorum adjustment. © 1987, ACM. All rights reserved.",Abstract data types; reconfiguration; replication,"COMPUTER SYSTEMS, DIGITAL - Distributed; DYNAMIC QUORUM ADJUSTMENT; PARTITIONED DATA; DATABASE SYSTEMS"
Concurrency in Linear Hashing,1987,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0023366343&doi=10.1145%2f22952.22954&partnerID=40&md5=efcc159616fa09fa3729afeaca0e5588,"Concurrent access to complex shared data structures, particularly structures useful as database indices, has long been of interest in the database community. In dynamic databases, tree structures such as B-trees have been used as indices because of their ability to handle growth; whereas hashing has been used for fast access in relatively static databases. Recently, a number of techniques for dynamic hashing have appeared. They address the major deficiency of traditional hashing when applied to databases that experience significant change in the amount of data being stored. This paper presents a solution that allows concurrency in one of these dynamic hashing data structures, namely linear hash files. The solution is based on locking protocols and minor modifications in the data structures. © 1987, ACM. All rights reserved.",concurrency control; concurrent algorithms; data structures; dynamic hashing; locking protocols,DATABASE SYSTEMS; COMPLEX SHARED DATA STRUCTURES; CONCURRENT ACCESS; LINEAR HASHING; DATA PROCESSING
Analysis of Retrieval Performance for Records and Objects Using Optical Disk Technology,1987,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0023364656&doi=10.1145%2f22952.23015&partnerID=40&md5=38d3ff01c6147df1ca08498bab1bbb27,"In this paper we examine the problem of object and record retrieval from optical disks. General objects (such as images, documents, etc.) may be long and their length may have high variance. We assume that all the components of an object are stored consecutively in storage to speed-up retrieval performance. We first present an optical disk model and an optimal schedule for retrieval of records and objects which qualify in a single query on a file stored on an optical disk device. We then provide exact and approximate analytic results for evaluating the retrieval performance for objects from an optical disk. The analysis provides some basic analytic tools for studying the performance of various file and database organizations for optical disks. The results involve probability distribution of block accesses, probability distributions of span accesses, and probability distribution of seek times. Record retrieval is an important special case. This analysis differs from similar ones in database environments in the following respects: (1) the large size and large variance of the size of objects; (2) crossing of track boundaries by objects; (3) the capability for span access that optical disks provide (e.g., when the optical assembly is located in a given position, information can be read from a number of consecutive tracks (span) with a small additional cost). © 1987, ACM. All rights reserved.",Image storage and retrieval; optical disks; SCAN algorithm,"DATA STORAGE, OPTICAL - Storage Devices; OPTICAL DISK TECHNOLOGY; RETRIEVAL PERFORMANCE; INFORMATION SCIENCE"
Database Performance Evaluation in an Indexed File Environment,1987,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0023313710&doi=10.1145%2f12047.13675&partnerID=40&md5=44ac9e46126cb5003c81e2ac73b97e8c,"The use of database systems for managerial decision making often incorporates information-retrieval capabilities with numeric report generation. Of great concern to the user of such a system is the response time associated with issuing a query to the database. This study presents a procedure for estimating response time for one of the most frequently encountered physical storage mechanisms, the indexed file. The model provides a fairly high degree of accuracy, but is simple enough so that the cost of applying the model is not exorbitant. The model incorporates the knowledge that the distribution of access key occurrences is known to follow Zipf's law. It first estimates the access time required to complete the query, which includes the time needed for all input and output transactions, and CPU time used in performing the search. The effects of multiple users on an individual's response time are then assessed using a simple regression estimation technique. The two-step procedure allows for the separation of access time from multiuser influences. © 1987, ACM. All rights reserved.",Indexed file; information retrieval; performance evaluation; retrieval models,DATA PROCESSING - File Organization; MANAGEMENT - Information Systems; STATISTICAL METHODS - Regression Analysis; DATABASE PERFORMANCE; INDEXED FILE ENVIRONMENT; MULTIPLE USERS; RESPONSE TIME; ZIPF'S LAW; DATABASE SYSTEMS
Transaction Management in the R* Distributed Database Management System,1986,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0022876835&doi=10.1145%2f7239.7266&partnerID=40&md5=ddd166bbc71ede5b93b1ddcfa7d8f69b,"This paper deals with the transaction management aspects of the R* distributed database system. It concentrates primarily on the description of the R* commit protocols, Presumed Abort (PA) and Presumed Commit (PC). PA and PC are extensions of the well-known, two-phase (2P) commit protocol. PA is optimized for read-only transactions and a class of multisite update transactions, and PC is optimized for other classes of multisite update transactions. The optimizations result in reduced intersite message traffic and log writes, and, consequently, a better response time. The paper also discusses R*‘s approach toward distributed deadlock detection and resolution. © 1986, ACM. All rights reserved.",Commit protocols; deadlock victim selection,OPTIMIZATION; COMMIT PROTOCOLS; CONCURRENCY CONTROL; DEADLOCK VICTIM SELECTION; TRANSACTION MANAGEMENT; DATABASE SYSTEMS
ELFS: English Language From SQL,1986,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0022876837&doi=10.1145%2f7239.384276&partnerID=40&md5=fc82ea7ca946b975f4c83b8b7cbc36d6,"In this paper we describe a system which, given a query in SQL-like relational database language, will display its meaning in clear, unambiguous natural language. The syntax-driven translation mechanism is independent of the application domain. It has direct applications in designing computer-based SQL tutorial systems and program debugging systems. The research results obtained in the paper will also be useful in query optimization and design of a more user-friendly language front-endfor casual users. © 1986, ACM. All rights reserved.",Natural language feedback; SQL; user interface,COMPUTER PROGRAMMING - Program Debugging; ELFS; RELATIONAL DATABASE LANGUAGE; SQL; DATABASE SYSTEMS
The Escrow Transactional Method,1986,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0022880421&doi=10.1145%2f7239.7265&partnerID=40&md5=426dc7a82d5e88f9978a702411d1eca0,"A method is presented for permitting record updates by long-lived transactions without forbidding simultaneous access by other users to records modified. Earlier methods presented separately by Gawlick and Reuter are comparable but concentrate on “hot-spot” situations, where even short transactions cannot lock frequently accessed fields without causing bottlenecks. The Escrow Method offered here is designed to support nonblocking record updates by transactions that are “long lived” and thus require long periods to complete. Recoverability of intermediate results prior to commit thus becomes a design goal, so that updates as of a given time can be guaranteed against memory or media failure while still retaining the prerogative to abort. This guarantee basically completes phase one of a two-phase commit, and several advantages result: (1) As with Gawlick's and Reuter's methods, high-concurrency items in the database will not act as a bottleneck; (2) transaction commit of different updates can be performed asynchronously, allowing natural distributed transactions; indeed, distributed transactions in the presence of delayed messages or occasional line disconnection become feasible in a way that we argue will tie up minimal resources for the purpose intended; and (3) it becomes natural to allow for human interaction in the middle of a transaction without loss of concurrent access or any special difficulty for the application programmer. The Escrow Method, like Gawlick's Fast Path and Reuter's Method, requires the database system to be an “expert” about the type of transactional updates performed, most commonly updates involving incremental changes to aggregate quantities. However, the Escrow Method is extendable to other types of updates. © 1986, ACM. All rights reserved.",Escrow transactions; hot spots; long-lived transactions; nested transactions; two-phase commit,ESCROW TRANSACTIONAL METHOD; LONG-LIVED TRANSACTIONS; MULTIUSER ENVIRONMENT; NESTED TRANSACTIONS; TWO-PHASE COMMIT; DATABASE SYSTEMS
Partitioned Two-Phase Locking,1986,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0022866757&doi=10.1145%2f7239.7477&partnerID=40&md5=e6b5ca87ef23e7e7f8f3b8d77a4970d7,"In a large integrated database, there often exists an “information hierarchy,” where both raw data and derived data are stored and used together. Therefore, among update transactions, there will often be some that perform only read accesses from a certain (i.e., the “raw” data) portion of the database and write into another (i.e., the “derived” data) portion. A conventional concurrency control algorithm would have treated such transactions as regular update transactions and subjected them to the usual protocols for synchronizing update transactions. In this paper such transactions are examined more closely. The purpose is to devise concurrency control methods that allow the computation of derived information to proceed without interfering with the updating of raw data. The first part of the paper presents a proof method for correctness of concurrency control algorithms in a hierarchically decomposed database. The proof method provides a framework for understanding the intricacies in dealing with hierarchically decomposed databases. The second part of the paper is an application of the proof method to show the correctness of a two-phase-locking- based algorithm, called partitioned two-phase locking, for hierarchically decomposed databases. This algorithm is a natural extension to the Version Pool method proposed previously in the literature. © 1986, ACM. All rights reserved.",Concurrency control; transaction processing,COMPUTER PROGRAMMING - Algorithms; CONCURRENCY CONTROL; TRANSACTION PROCESSING; TWO-PHASE LOCKING; DATABASE SYSTEMS
The Design of a Relational Database System with Abstract Data Types for Domains,1986,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976854302&doi=10.1145%2f6314.6461&partnerID=40&md5=dc33ebf101156b119319b43fe342731a,"An extension to the relational model is described in which domains can he arbitrarily defined as abstract data types. Operations on these data types include primitive operations, aggregates, and transformations. It is shown that these operations make the query language complete in the sense of Chandra and Harel. The system has been designed in such a way that new data types and their operations can be defined with a minimal amount of interaction with the database management system. © 1986, ACM. All rights reserved.",Relational completeness; relational databases,
A Temporally Oriented Data Model,1986,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0022873144&doi=10.1145%2f7239.7350&partnerID=40&md5=053ab7466e3a5be2912f33a05a2761a6,"The research into time and data models has so far focused on the identification of extensions to the classical relational model that would provide it with “adequate” semantic capacity to deal with time. The temporally oriented data model (TODM) presented in this paper is a result of a different approach, namely, it directly operationalizes the pervasive three-dimensional metaphor for time. One of the main results is thus the development of the notion of the data cube: a three-dimensional and inherently temporal data construct where time, objects, and attributes are the primary dimensions of stored data. TODM's cube adds historical depth to the tabular notions of data and provides a framework for storing and retrieving data within their temporal context. The basic operations in the model allow the formation of new cubic views from existing ones, or viewing data as one moves up and down in time within cubes. This paper introduces TODM, a consistent set of temporally oriented data constructs, operations, and constraints, and then presents TOSQL, a corresponding end-user's SQL-like query syntax. The model is a restricted but consistent superset of the relational model, and the query syntax incorporates temporal notions in a manner that likewise avoids penalizing users who are interested solely in the current view of data (rather than in a temporal perspective). The naturalness of the spatial reference to time and the added semantic capacity of TODM come with a price—the definitions of the cubic constructs and basic operations are relatively cumbersome. As rudimentary as it is, TODM nonetheless provides a comprehensive basis for formulating an external data model for a temporally oriented database. © 1986, ACM. All rights reserved.",Information modeling; information systems; relational model; time dimension,DATA CUBE; INFORMATION MODELING; TEMPORALLY ORIENTED DATA MODEL; TODM; TOSQL; DATABASE SYSTEMS
Set Query Optimization in Distributed Database Systems,1986,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976758947&doi=10.1145%2f6314.6488&partnerID=40&md5=8760b87ef2cc7ab787082951434e47be,"This paper addresses the problem of optimizing queries that involve set operations (set queries) in a distributed relational database system. A particular emphasis is put on the optimization of such queries in horizontally partitioned database systems. A mathematical programming model of the set query problem is developed and its NP-completeness is proved. Solution procedures are proposed and computational results presented. One of the main results of the computational experiments is that, for many queries, the solution procedures are not sensitive to errors in estimating the size of results of set operations. © 1986, ACM. All rights reserved.",Database; horizontal partitioning; lower bound; query,
An Integrated Approach to Logical Design of Relational Database Schemes,1986,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0022733977&doi=10.1145%2f5922.214291&partnerID=40&md5=ceff8554e0681ea4cde92bac6fc55ea4,"We propose a new approach to the design of relational database schemes. The main features of the approach are the following: A combination of the traditional decomposition and synthesis approaches, thus allowing the use of both functional and multivalued dependencies. Separation of structural dependencies relevant for the design process from integrity constraints, that is, constraints that do not bear any structural information about the data and which should therefore be discarded at the design stage. This separation is supported by a simple syntactic test filtering out nonstructural dependencies. Automatic correction of schemes which lack certain desirable properties. © 1986, ACM. All rights reserved.",Acyclic schemes; conflict-free sets of dependencies; decomposition; functional dependencies; logical design; multivalued dependencies; schema extension; synthesis,ACYCLIC SCHEMES; DECOMPOSITION; FUNCTIONAL DEPENDENCIES; MULTIVALUED DEPENDENCIES; SYNTHESIS; DATABASE SYSTEMS
Consistency of Transactions and Random Batch,1986,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0022863562&doi=10.1145%2f7239.214287&partnerID=40&md5=d53edd90b3b7c782f077d1a1bbb37207,"A synchronization technique and scheduling strategy is described, which allows us to run a batch process simultaneously with on-line transactions. The batch process and the transactions are serialized in such a way that consistency level 3 is achieved. © 1986, ACM. All rights reserved.",Batch processing; concurrency control; consistency levels; database systems; random batch; transaction processing,SCHEDULING; CONCURRENCY CONTROL; CONSISTENCY OF TRANSACTIONS; RANDOM BATCH; SYNCHRONIZATION; DATABASE SYSTEMS
A State Transition Model for Distributed Query Processing,1986,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976755938&doi=10.1145%2f6314.6460&partnerID=40&md5=09cfb96a3d45e93eee95d51a9b5fffe2,"A state transition model for the optimization of query processing in a distributed database system is presented. The problem is parameterized by means of a state describing the amount of processing that has been performed at each site where the database is located. A state transition occurs each time a new join or semijoin is executed. Dynamic programming is used to compute recursively the costs of the states and the globally optimal solution, taking into account communication and local processing costs. The state transition model is general enough to account for the possibility of parallel processing among the various sites, as well as for redundancy in the database. The model also permits significant reductions of the necessary computations by taking advantage of simple additivity and site-uniformity properties of a cost model, and of clever strategies that improve on the basic dynamic programming algorithm. © 1986, ACM. All rights reserved.",Distributed query processing; query optimization; semijoin; state transition model; tree queries,
Order-preserving key transformations,1986,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0022736864&doi=10.1145%2f5922.5923&partnerID=40&md5=95b96804a30b0e4d60c91a6d9219ef8e,"File organizations based on conventional hash functions provide faster access to the stored records in comparison with tree-like file structures. Tree structures such as B+-trees and ISAM do provide for sequential processing, but require considerable storage for the indices. When sequential processing is needed a table that performs an order-preserving transformation on keys can be used. H is an order-preserving key transform if H(K1)  H(K2), for all keys K1> K2. We present methodologies for constructing such key transforms, and illustrate them for some real-life key sets. Storage requirements for the table needed to carry out the transformation are less than those needed for the indices. © 1986, ACM. All rights reserved.",B-tree; dynamic tiles; hashing; order-preserving hashing; sequential and random access,DATABASE SYSTEMS; ACCESS METHODS; DYNAMIC FILES; KEY TRANSFORMATIONS; ORDER-PRESERVING HASHING; DATA PROCESSING
Fragmentation: A Technique for Efficient Query Processing,1986,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0022737290&doi=10.1145%2f5922.5638&partnerID=40&md5=4346b4c9cf86eb11f8caf1b03cdfdded,"A “divide and conquer” strategy to compute natural joins by sequential scans on unordered relations is described. This strategy is shown to always he better than merging SCBIIS when both relations must he sorted before joining, and generally better in practical cases when only the largest relation mutt be sorted. © 1986, ACM. All rights reserved.",Buffers; hashing; join; merging scana; nested scans,COMPUTER PROGRAMMING - Algorithms; DIVIDE-AND-CONQUER ALGORITHMS; FRAGMENTATION; NATURAL JOINS; QUERY PROCESSING; DATABASE SYSTEMS
Computation-Tuple Sequences and Object Histories,1986,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0022737616&doi=10.1145%2f5922.5924&partnerID=40&md5=81ad7b3ab1049398954bba1e0fbc534e,"A record-based, algebraically-oriented model is introduced for describing data for “object histories” (with computation), such as checking accounts, credit card accounts, taxes, schedules, and so on. The model consists of sequences of computation tuples defined by a computation-tuple sequence scheme (CSS). The CSS has three major features (in addition to input data): computation (involving previous computation tuples), “uniform” constraints (whose satisfaction by a computation-tuple sequence u implies satisfaction by every interval of u), and specific sequences with which to start the valid computation-tuple sequences. A special type of CSS, called “local,” is singled out for its relative simplicity in maintaining the validity of a computation-tuple sequence. A necessary and sufficient condition for a CSS to be equivalent to at least one local CSS is given. Finally, the notion of “local bisimulatability” is introduced for regarding two CSS as conveying the same information, and two results on local bisimulatability in connection with local CSS are established. © 1986, ACM. All rights reserved.",Computation history; database state transitions; information presentation,COMPUTATION HISTORY; DATA DESCRIPTION; DATABASE STATE TRANSITIONS; TRANSACTION PROCESSING; DATABASE SYSTEMS
Incomplete Information Costs and Database Design,1986,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0022739113&doi=10.1145%2f5922.5678&partnerID=40&md5=0871425c4ee32db03e572904cfb03ea6,"This paper presents a methodology for trading-off the cost of incomplete information against the data-related costs in the design of database systems. It investigates how the usage patterns of the database, defined by the characteristics of information requests presented to it, affect its conceptual design. The construction of minimum-cost answers to information requests for a variety of query types and cost structures is also studied. The resulting costs of incomplete database information are balanced against the data-related costs in the derivation of the optimal design. © 1986, ACM. All rights reserved.",Information economics; query processing; retrieval models; value of information,OPTIMIZATION; DATA-RELATED COSTS; INCOMPLETE INFORMATION COSTS; DATABASE SYSTEMS
Performance Analysis of Several Back-End Database Architectures,1986,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976667803&doi=10.1145%2f5236.5242&partnerID=40&md5=2335019e2496d5fc22bb2c6d1a699277,"The growing acceptance of database systems makes their performance increasingly more important. One way to gain performance is to off-load some of the functions of the database system to aback-end computer. The problem is what functions should be off-loaded to maximize the benefits of distributed processing. Our approach to this problem consisted of constructing several variants of an existing relational database system. INGRES, that partition the database system software into two parts, and assigning these two parts to two computers connected by a local area network. For the purposes of this experiment, six different variants of the database software were constructed to test the sir most interesting functional subdivisions. Each variant was then benchmarked using two different databases and query streams. The communication medium and the communication software were also benchmarked to measure their contribution to the performance of each configuration. Combining the database and network measurement results, various conclusions were reached about the viability of the configurations, the desirable properties of the communications mechanisms to he used, the operating system interface and overhead, and the performance of the database system. The variants to be preferred depend on the hardware technology, operating system features, database system internal structure, and network software overhead. © 1986, ACM. All rights reserved.",,
A Problem-Oriented Inferential Database System,1986,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976738361&doi=10.1145%2f6314.6419&partnerID=40&md5=c4689a4a03120fc115797749607e7ebe,"Recently developed inferential database systems face some common problems: a very fast growth of search space and difficulties in recognizing inference termination (especially for recursive axioms). These shortcomings stem mainly from the fact that the inference process is usually separated from database operations. A problem-oriented inferential system i8 described which refers to the database prior to query (or subquery) processing, so that the inference from the very beginning is directed by data relevant to the query. A multiprocessor implementation of the system is presented based on a computer network conforming to database relations and axioms. The system provides an efficient indication of query termination, and is complete in the sense that it produces all correct answers to a query in a finite time. © 1986, ACM. All rights reserved.",Inference; parallel processing; problem-oriented deduction; relational database; search strategy,
Join Processing in Database Systems with Large Main Memories,1986,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976660052&doi=10.1145%2f6314.6315&partnerID=40&md5=fa726355b1ab775c3bc4f3c5c53a5b33,"We study algorithms for computing the equijoin of two relations in a system with a standard architecture hut with large amounts of main memory. Our algorithms are especially efficient when the main memory available is a significant fraction of the size of one of the relations to he joined; but they can be applied whenever there is memory equal to approximately the square root of the size of one relation. We present a new algorithm which is a hybrid of two hash-based algorithms and which dominates the other algorithms we present, including sort-merge. Even in a virtual memory environment, the hybrid algorithm dominates all the others we study. Finally, we describe how three popular tools to increase the efficiency of joins, namely filters, Babb arrays, and semijoins, can he grafted onto any of our algorithms. © 1986, ACM. All rights reserved.",Hash join; join processing; large main memory; sort-merge join,
"Height-Balanced Trees of Order (β, γ, δ)",1985,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0022076474&doi=10.1145%2f3857.3858&partnerID=40&md5=b829385ba9bf6d79b4fcbf1711c0d4d1,"We study restricted classes of B-trees, called H(β, γ, δ) trees. A class is defined by three parameters: β, the size of a node; γ, the minimal number of grandsons a node must have; and δ, the minimal number of leaves bottom nodes must have. This generalizes the brother condition of 2-3 brother trees in a uniform way to B-trees of higher order. The class of B-trees of order m is obtained by choosing β = m, γ = (m/2)2 and δ = m/2. An algorithm to construct H-trees for any given number of keys is given in Section 1. Insertion and deletion algorithms are given in Section 2. The costs of these algorithms increase smoothly as the parameters are increased. Furthermore, it is proved that the insertion can be done in time O(β + log N), where N is the number of nodes in the tree. Deletion can also be accomplished without reconstructing the entire tree. Properties of H-trees are given in Section 3. It is shown that the height of H-trees decreases as γ increases, and the storage utilization increases significantly as δ increases. Finally, comparisons with other restricted classes of B-trees are given in Section 4 to show the attractiveness of H-trees. © 1985, ACM. All rights reserved.",,COMPUTER PROGRAMMING - Algorithms; B-TREES; COMPACT B-TREES; DENSE MULTIWAY TREES; HEIGHT-BALANCED TREES; DATA PROCESSING
Estimating the Cost of Updates in a Relational Database,1985,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0022075546&doi=10.1145%2f3857.3863&partnerID=40&md5=2d65c565dd0b648d5bb075a539ab963f,"In this paper, cost formulas are derived for the updates of data and indexes in a relational database. The costs depend on the data scan type and the predicates involved in the update statements. We show that update costs have a considerable influence, both in the context of the physical database design problem and in access path selection in query optimization for relational DBMSs. © 1985, ACM. All rights reserved.",,QUERY OPTIMIZATION; RELATIONAL DATABASES; UPDATE COSTS; DATABASE SYSTEMS
A Data Distortion by Probability Distribution,1985,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0022130080&doi=10.1145%2f3979.4017&partnerID=40&md5=aa245a150e5eaa1a2bcdfa8760a46223,"This paper introduces data distortion by probability distribution, a probability distortion that involves three steps. The first step is to identify the underlying density function of the original series and to estimate the parameters of this density function. The second step is to generate a series of data from the estimated density function. And the final step is to map and replace the generated series for the original one. Because it is replaced by the distorted data set, probability distortion guards the privacy of an individual belonging to the original data set. At the same time, the probability distorted series provides asymptotically the same statistical properties as those of the original series, since both are under the same distribution. Unlike conventional point distortion, probability distortion is difficult to compromise by repeated queries, and provides a maximum exposure for statistical analysis. © 1985, ACM. All rights reserved.",Compromisability; individual privacy; microdata file; point distortion; probability distortion; statistical database,PROBABILITY; DATA DISTORTION; PROBABILITY DISTORTION; STATISTICAL DATABASES; DATABASE SYSTEMS
Modeling concepts for VLSI CAD objects,1985,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0022129182&doi=10.1145%2f3979.4018&partnerID=40&md5=ddf6111921974c2413a96f33baa1a3e0,"VLSI CAD applications deal with design objects that have an interface description and an implementation description. Versions of design objects have a common interface but differ in their implementations. A molecular object is a modeling construct which enables a database entity to be represented by two sets of heterogeneous records, one set describes the object's interface and the other describes its implementation. Thus a reasonable starting point for modeling design objects is to begin with the concept of molecular objects. In this paper, we identify modeling concepts that are fundamental to capturing the semantics of VLSI CAD design objects and versions in terms of molecular objects. A provisional set of user operations on design objects, consistent with these modeling concepts, is also defined. The modeling framework that we present has been found useful for investigating physical storage techniques and change notification problems in version control. © 1985, ACM. All rights reserved.",CAD; CAM; design automation; relational databases; versions; VLSI,"DATABASE SYSTEMS; CAD; DESIGN AUTOMATION; RELATIONAL DATABASES; STORAGE TECHNIQUES; VERSION CONTROL; INTEGRATED CIRCUITS, VLSI"
Language features for flexible handling of exceptions in information systems,1985,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0022240587&doi=10.1145%2f4879.4995&partnerID=40&md5=19e64efa4c85538308a090757224d852,"An exception-handling facility suitable for languages used to implement database-intensive information systems is presented. Such a mechanism facilitates the development and maintenance of more flexible software systems by supporting the abstraction of details concerning special or abnormal occurrences. The type constraints imposed by the schema as well as various semantic integrity assertions are considered to be normalcy conditions, and the key contribution of this work is to allow exceptions to these constraints to persist. To achieve this, solutions are proposed to a range of problems, including sharing and computing with exceptional information, exception handling by users, the logic of constraints with exceptions, and implementation issues. The use of exception handling in dealing with null values, estimates, and measurement is also illustrated. © 1985, ACM. All rights reserved.",accommodating exceptions; conceptual models; exception handling; Semantic integrity; violations of type constraints,COMPUTER PROGRAMMING LANGUAGES; EXCEPTION HANDLING; SEMANTIC INTEGRITY; TYPE CONSTRAINTS; DATABASE SYSTEMS
Modeling the storage architectures of commercial database systems,1985,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0022245356&doi=10.1145%2f4879.5392&partnerID=40&md5=5cf69722ef9a7670fcc8a4cab5b29f93,"Modeling the storage structures of a DBMS is a prerequisite to understanding and optimizing database performance. Previously, such modeling was very difficult because the fundamental role of conceptual-to-internal mappings in DBMS implementations went unrecognized. In this paper we present a model of physical databases, called the transformation model, that makes conceptual-to-internal mappings explicit. By exposing such mappings, we show that it is possible to model the storage architectures (i.e., the storage structures and mappings) of many commercial DBMSs in a precise, systematic, and comprehendible way. Models of the INQUIRE, ADABAS, and SYSTEM 2000 storage architectures are presented as examples of the model's utility. We believe the transformation model helps bridge the gap between physical database theory and practice. It also reveals the possibility of a technology to automate the development of physical database software. © 1985, ACM. All rights reserved.",,DATA PROCESSING - Data Structures; STORAGE ARCHITECTURES; DATABASE SYSTEMS
Optimization of Join Operations in Horizontally Partitioned Database Systems,1986,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0022680543&doi=10.1145%2f5236.5241&partnerID=40&md5=5c981a1ecbe258b37def0a30366c4b48,"This paper analyzes the problem of joining two horizontally partitioned relations in a distributed database system. Two types of semijoin strategies are introduced, local and remote. Local semijoins are performed at the site of the restricted relation (or fragment), and remote semijoins can be performed at an arbitrary site. A mathematical model of a semijoin strategy for the case of remote semijoins is developed, and lower bounding and heuristic procedures are proposed. The results of computational experiments are reported. The experiments include an analysis of the heuristics’ performance relative to the lower bounds, sensitivity analysis, and error analysis. These results reveal a good performance of the heuristic procedures, and demonstrate the benefit of using semijoin operations to reduce the size of fragments prior to their transmission. The algorithms for the case of remote semijoins were found to be superior to the algorithms for the case of local semijoins. In addition, we found that the estimation accuracy of the selectivity factors has a significant effect on the incurred communication cost. © 1986, ACM. All rights reserved.",,COMPUTER PROGRAMMING - Algorithms; MATHEMATICAL MODELS; OPTIMIZATION; HORIZONTALLY PARTITIONED DATABASE SYSTEMS; JOIN OPERATIONS; DATABASE SYSTEMS
An Efficient I/O Interface for Optical Disks,1985,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0022076726&doi=10.1145%2f3857.3862&partnerID=40&md5=21ecbe56fa0c65eea1e1663f7f8734a0,"We introduce the notion of an I/O interface for optical digital (write-once) disks, which is quite different from earlier research. The purpose of an I/O interface is to allow existing operating systems and application programs that use magnetic disks to use optical disks instead, with minimal change. We define what it means for an I/O interface to be disk-efficient. We demonstrate a practical disk- efficient I/O interface and show that its I/O performance in many cases is optimum, up to a constant factor, among all disk-efficient interfaces. The interface is most effective for applications that are not update-intensive. An additional capability is a built-in history mechanism that provides software support for accessing previous versions of records. Even if not implemented, the I/O interface can be used as a programming tool to develop efficient special purpose applications for use with optical disks. © 1985, ACM. All rights reserved.",,"COMPUTER INTERFACES; I/O INTERFACE; OPTICAL DISKS; DATA STORAGE, OPTICAL"
Implementation of Logical Query Languages for Databases,1985,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0022128534&doi=10.1145%2f3979.3980&partnerID=40&md5=208af43375baa7f888ed2448c7e08c71,"We examine methods of implementing queries about relational databases in the case where these queries are expressed in first-order logic as a collection of Horn clauses. Because queries may be defined recursively, straightforward methods of query evaluation do not always work, and a variety of strategies have been proposed to handle subsets of recursive queries. We express such query evaluation techniques as “capture rules” on a graph representing clauses and predicates. One essential property of capture rules is that they can be applied independently, thus providing a clean interface for query-evaluation systems that use several different strategies in different situations. Another is that there be an efficient test for the applicability of a given rule. We define basic capture rules corresponding to application of operators from relational algebra, a top-down capture rule corresponding to “backward chaining,” that is, repeated resolution of goals, a bottom-up rule, corresponding to “forward chaining,” where we attempt to deduce all true facts in a given class, and a “sideways” rule that allows us to pass results from one goal to another. © 1985, ACM. All rights reserved.",Capture rule; Horn clause; Prolog; recursion; relational database,COMPUTER PROGRAMMING LANGUAGES; HORN CLAUSES; LOGICAL QUERY LANGUAGES; RELATIONAL DATABASES; DATABASE SYSTEMS
Applications of Byzantine Agreement in Database Systems,1986,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0022676219&doi=10.1145%2f5236.5243&partnerID=40&md5=73e944bf80690136e781432e037eb076,"In this paper we study when and how B Byzantine agreement protocol can he used in general-purpose database management systems. We present an overview of the failure model used for Byzantine agreement, and of the protocol itself. We then present correctness criteria for database processing in this failure environment and discuss strategies for satisfying them. In doing this, we present new failure models for input/output nodes and study ways to distribute input transactions to processing nodes under these models. Finally, we investigate applications of Byzantine agreement protocols in the more common failure environment where processors are assumed to halt after a failure. © 1986, ACM. All rights reserved.",,DATA PROCESSING; BYZANTING AGREEMENT PROTOCOL; FAILURE MODELS; FAULT TOLERANCE; DATABASE SYSTEMS
Semantics of Query Languages for Network Databases,1985,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0022131241&doi=10.1145%2f3979.214293&partnerID=40&md5=7f1a7b6061c7a7bfdd646a56f23efb36,"Semantics determines the meaning of language constructs; hence it says much more than syntax does about implementing the language. The main purpose of this paper is a formal presentation of the meaning of basic language constructs employed in many database languages (sublanguages). Therefore, stylized query languages SSL (Sample Selection Language) and J (Joins) are introduced, wherein most of the typical entries present in other query languages are collected. The semantics of SSL and J are defined by means of the denotational method and explained informally. In SSL and J, four types of expressions are introduced: a selector (denotes a set of addresses), a term (denotes a set of values), a formula (denotes a truth value), and a join (denotes a set of n-tuples of addresses or values). In many cases alternative semantics are given and discussed. In order to obtain more general properties of the proposed languages, a new database access model is introduced, intended to be a tool for the description of the logical access paths to data. In particular, the access paths of the network and relational models can be described. SSL and J expressions may be addressed to both data structures. In the case of the relational model, expressions of J are similar to SQL or QUEL statements. Thus J may be considered a generalization of relational query languages for the network model. Finally, a programming language, based on SSL and J, is outlined, and the issues of SSL and J implementation are considered. © 1985, ACM. All rights reserved.",data manipulation language; denotational semantics; network database; Query language; query optimization,COMPUTER PROGRAMMING LANGUAGES; DATA MANIPULATION LANGUAGES; DENOTATIONAL SEMANTICS; QUERY LANGUAGES; QUERY OPTIMIZATION; DATABASE SYSTEMS
On the Complexity of Join Dependencies,1986,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0022679733&doi=10.1145%2f5236.5237&partnerID=40&md5=0fb86c5ba2af99dca25f8a9fa0c81c70,"In [10] a method is proposed for decomposing join dependencies (jds) in a relational database using the notion of a hinge. This method was subsequently studied in [11] and [12]. We show how the technique of decomposition can be used to make integrity checking more efficient. It turns out that it is important to find a decomposition that minimizes the number of edges of its largest element. We show that the decompositions obtained with the method described in [10] are optimal in this respect. This minimality criterion leads to the definition of the degree of cyclicity, which allows us to classify jds and leads to the notion of n-cyclicity, of which acyclicity is a special case for n = 2. We then show that, for a fixed value of n (which may be greater than 2). integrity checking can be performed in polynomial time provided we restrict ourselves to n-cyclic jds. Finally, we generalize a well-known characterization for acyclic jds by proving that n-cyclicity is equivalent to “n-wise consistency implies global consistency.” As a consequence, consistency checking can be performed in polynomial time if we restrict ourselves to n-cyclic jds, for a tired value of n, not necessarily equal to 2. © 1986, ACM. All rights reserved.",,MATHEMATICAL TECHNIQUES - Graph Theory; CYCLICITY; DECOMPOSITION; JOIN DEPENDENCIES; DATABASE SYSTEMS
Locking performance in centralized databases,1985,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0022200912&doi=10.1145%2f4879.4880&partnerID=40&md5=d3b8ec8caef682ce7b4593292de3043e,"An analytic model is used to study the performance of dynamic locking. The analysis uses only the steady-state average values of the variables. The solution to the model is given by a cubic, which has exactly one valid root for the range of parametric values that is of interest. The model's predictions agree well with simulation results for transactions that require up to twenty locks. The model separates data contention from resource contention, thus facilitating an analysis of their separate effects and their interaction. It shows that systems with a particular form of nonuniform access, or with shared locks, are equivalent to systems with uniform access and only exclusive locks. Blocking due to conflicts is found to impose an upper bound on transaction throughput; this fact leads to a rule of thumb on how much data contention should be permitted in a system. Throughput can exceed this bound if a transaction is restarted whenever it encounters a conflict, provided restart costs and resource contention are low. It can also be exceeded by making transactions predeclare their locks. Raising the multiprogramming level to increase throughput also raises the number of restarts per completion. Transactions should minimize their lock requests, because data contention is proportional to the square of the number of requests. The choice of how much data to lock at a time depends on which part of a general granularity curve the system sees. © 1985, ACM. All rights reserved.",blocking; Concurrency control; data contention; database locking; granularity; resource contention; restarts; shared data,CONCURRENCY CONTROL; DATA CONTENTION; DATABASE LOCKING; RESOURCE CONTENTION; DATABASE SYSTEMS
Adaptive Record Clustering,1985,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0022076368&doi=10.1145%2f3857.3861&partnerID=40&md5=66f3fccc8385dd9f2020e53ba377fe11,"An algorithm for record clustering is presented. It is capable of detecting sudden changes in users’ access patterns and then suggesting an appropriate assignment of records to blocks. It is conceptually simple, highly intuitive, does not need to classify queries into types, and avoids collecting individual query statistics. Experimental results indicate that it converges rapidly; its performance is about 50 percent better than that of the total sort method, and about 100 percent better than that of randomly assigning records to blocks. © 1985, ACM. All rights reserved.",,COMPUTER PROGRAMMING - Algorithms; DATABASE SYSTEMS; ADAPTIVE RECORD CLUSTERING; PHYSICAL DATABASE DESIGN; PROBABILISTIC RETRIEVAL; DATA PROCESSING
Integrated concurrency control and recovery mechanisms: Design and performance evaluation,1985,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0022182726&doi=10.1145%2f4879.4958&partnerID=40&md5=b874f9d01d1547800b25ba5f5a9695d2,"In spite of the wide variety of concurrency control and recovery mechanisms proposed during the past decade, the behavior and the performance of various concurrency control and recovery mechanisms remain largely not well understood. In addition, although concurrency control and recovery mechanisms are intimately related, the interaction between them has not been adequately explored. In this paper, we take a unified view of the problems associated with concurrency control and recovery for transaction-oriented multiuser centralized database management systems, and we present several integrated mechanisms. We then develop analytical models to study the behavior and compare the performance of these integrated mechanisms, and we present the results of our performance evaluation. © 1985, ACM. All rights reserved.",Concurrency control; recovery; transaction processing,CONCURRENCY CONTROL; RECOVERY MECHANISMS; TRANSACTION PROCESSING; DATABASE SYSTEMS
Comments on Batched Searching of Sequential and Tree-Structured Files,1985,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976818613&doi=10.1145%2f3857.214294&partnerID=40&md5=7f271c2a25eda9f96d5bdc644407c219,"Exact formulas for the expected cost savings from batching requests against two types of j-ary trees are given. Approximate expressions are also presented. © 1985, ACM. All rights reserved.",,
Cautious Transaction Schedulers with Admission Control,1985,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0022076934&doi=10.1145%2f3857.3860&partnerID=40&md5=a7071e55ba2b90080bb1b5fa8fb413f0,"We propose a new class of schedulers, called cautious schedulers, that grant an input request if it will not necessitate any rollback in the future. In particular, we investigate cautious WRW-schedulers that output schedules in class WRW only. Class WRW consists of all schedules that are serializable, while preserving the write-read and read-write conflict, and is the largest polynomially recognizable subclass of serializable schedules currently known. It is shown, in this paper however, that cautious WRW-scheduling is, in general, NP-complete. Therefore, we introduce a special type (type 1R) of transaction, which consists of no more than one read step (an indivisible set of read operations) followed by multiple write steps. It is shown that cautious WRW-scheduling can be performed efficiently if all transactions are of type 1R and if admission control can be exercised. Admission control rejects a transaction unless its first request is immediately grantable. © 1985, ACM. All rights reserved.",,CONCURRENCY CONTROL; SERIALIZABILITY; TRANSACTION SCHEDULER; DATABASE SYSTEMS
Analysis of New variants of Coalesced Hashing,1984,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0021582735&doi=10.1145%2f1994.2205&partnerID=40&md5=a21c7ce2f4545f829e0a3ff7af649e63,"The coalesced hashing method has been shown to be very fast for dynamic information storage and retrieval. This paper analyzes in a uniform way the performance of coalesced hashing and its variants, thus settling some open questions in the literature. In all the variants, the range of the hash function is called the address region, and extra space reserved for storing colliders is called the cellar. We refer to the unmodified method, which was analyzed previously, as late-insertion coalesced hashing. In this paper we analyze late insertion and two new variations called early insertion and varied insertion. When there is no cellar, the early-insertion method is better than late insertion; however, past experience has indicated that it might be worse when there is a cellar. Our analysis confirms that it is worse. The varied-insertion method was introduced as a means of combining the advantages of late insertion and early insertion. This paper shows that varied insertion requires fewer probes per search, on the average, than do the other variants. Each of these three coalesced hashing methods has a parameter that relates the sizes of the address region and the cellar. Techniques in this paper are designed for tuning the parameter in order to achieve optimum search times. We conclude with a list of open problems. © 1984, ACM. All rights reserved.",Analysis of algorithms; asymptotic analysis; average-case; coalesced hashing; data structures; hashing; information retrieval; searching,COMPUTER PROGRAMMING - Algorithms; DATA PROCESSING; ADDRESS REGION; CELLAR; COALESCED HASHING; EARLY INSERTION; LATE INSERTION; INFORMATION RETRIEVAL SYSTEMS
Database Hosting in Strongly-Typed Programming Languages,1985,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0022024980&doi=10.1145%2f3148.3327&partnerID=40&md5=dd75cefc1215d77a6e2bd3adfc0c8727,"Database system support has become an essential part of many computer applications, which have extended beyond the more traditional commercial applications to, among others, engineering applications. Correspondingly, application programming with the need to access databases has progressively shifted to scientifically oriented languages. Modern developments in these languages are characterized by advanced mechanisms for the liberal declaration of data types, for type checking, and facilities for modularization of large programs. The present paper examines how a DBMS can be accessed from such a language in a way that conforms to its syntax and utilizes its type-checking facilities, without modifying the language specification itself, and hence its compilers. The basic idea is to rely on facilities for defining modules as separately compilable units, and to use these to declare user-defined abstract data types. The idea is demonstrated by an experiment in which a specific DBMS (ADABAS) is hosted in the programming language (LIS). The paper outlines a number of approaches and their problems, shows how to embed the DML into LIS, and how a more user-oriented DML can be provided in LIS. © 1985, ACM. All rights reserved.",,COMPUTER PROGRAMMING LANGUAGES; DATABASE HOSTING; PARAMETERIZED DATA TYPES; SCHEMA MAPPING; STRONGLY-TYPED PROGRAMMING LANGUAGES; DATABASE SYSTEMS
Database Partitioning in a Cluster of Processors,1985,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0022024916&doi=10.1145%2f3148.3161&partnerID=40&md5=4c4994ea84d143b169bc3d2795447191,"In a distributed database system the partitioning and allocation of the database over the processor nodes of the network can be a critical aspect of the database design effort. In this paper we develop and evaluate algorithms that perform this task in a computationally feasible manner. The network we consider is characterized by a relatively high communication bandwidth, considering the processing and input output capacities in its processors. Such a balance is typical if the processors are connected via busses or local networks. The common constraint that transactions have a specific root node no longer exists, so that there are more distribution choices. However, a poor distribution leads to less efficient computation, higher costs, and higher loads in the nodes or in the communication network so that the system may not be able to handle the required set of transactions. Our approach is to first split the database into fragments which constitute appropriate units for allocation. The fragments to be allocated are selected based on maximal benefit criteria using a greedy heuristic. The assignment to processor nodes uses a first-fit algorithm. The complete algorithm, called GFF, is stated in a procedural form. The complexity of the problem and of its candidate solutions are analyzed and several interesting relationships are proven. Alternate benefit metrics are considered, since the execution cost of the allocation procedure varies by orders of magnitude with the alternatives of benefit evaluation. A mixed benefit evaluation strategy is eventually proposed. A model for evaluation is presented. Two of the strategies are experimentally evaluated, and the reported results support the discussion. The approach should be suitable for other cases where resources have to be allocated subject to resource constraints. © 1985, ACM. All rights reserved.",,"COMPUTER SYSTEMS, DIGITAL - Multiprocessing; DATABASE PARTITIONING; DATABASE SYSTEMS"
The Statistical Security of a Statistical Database,1984,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0021622802&doi=10.1145%2f1994.383392&partnerID=40&md5=106eee4330a1c17307387ebc7f8df2b7,"This note proposes a statistical perturbation scheme to protect a statistical database against compromise. The proposed scheme can handle the security of numerical as well as nonnumerical sensitive fields. Furthermore, knowledge of some records in a database does not help to compromise unknown records. We use Chebyshev's inequality to analyze the trade-offs among the magnitude of the perturbations, the error incurred by statistical queries, and the size of the query set to which they apply. We show that if the statistician is given absolute error guarantees, then a compromise is possible, but the cost is made exponential in the size of the database. © 1984, ACM. All rights reserved.",Chebyshev's inequality; complexity of compromise; Statistical database,DATA PROCESSING - Security of Data; CHEBYSHEV'S INEQUALITY; COMPLEXITY OF COMPROMISE; SECURITY; STATISTICAL DATABASE; DATABASE SYSTEMS
Recursive linear hashing,1984,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0021500782&doi=10.1145%2f1270.1285&partnerID=40&md5=fbb5c13d6e019fd3f4f4da1cd5d52449,"A modification of linear hashing is proposed for which the conventional use of overflow records is avoided. Furthermore, an implementation of linear hashing is presented for which the amount of physical storage claimed is only fractionally more than the minimum required. This implementation uses a fixed amount of in-core space. Simulation results are given which indicate that even for storage utilizations approaching 95 percent, the average successful search cost for this method is close to one disk access. © 1984, ACM. All rights reserved.",,COMPUTER SIMULATION; DYNAMIC FILES; LINEAR HASHING; DATA PROCESSING
Vertical Partitioning Algorithms for Database Design,1984,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0021625744&doi=10.1145%2f1994.2209&partnerID=40&md5=090b78d54e6b7025355a490a699a9e58,"This paper addresses the vertical partitioning of a set of logical records or a relation into fragments. The rationale behind vertical partitioning is to produce fragments, groups of attribute columns, that “closely match” the requirements of transactions. Vertical partitioning is applied in three contexts: a database stored on devices of a single type, a database stored in different memory levels, and a distributed database. In a two-level memory hierarchy, most transactions should be processed using the fragments in primary memory. In distributed databases, fragment allocation should maximize the amount of local transaction processing. Fragments may be nonoverlapping or overlapping. A two-phase approach for the determination of fragments is proposed; in the first phase, the design is driven by empirical objective functions which do not require specific cost information. The second phase performs cost optimization by incorporating the knowledge of a specific application environment. The algorithms presented in this paper have been implemented, and examples of their actual use are shown. © 1984, ACM. All rights reserved.",,DATA PROCESSING; DATABASE SYSTEMS - Design; CLUSTERS; FRAGMENT ALLOCATION; MEMORY LEVELS; VERTICAL PARTITIONING ALGORITHMS; COMPUTER PROGRAMMING
An Algorithm for Concurrency Control and Recovery in Replicated Distributed Databases,1984,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0021583622&doi=10.1145%2f1994.2207&partnerID=40&md5=ccc3b9e845a7518a394822b87c99f0e2,"In a one-copy distributed database, each data item is stored at exactly one site. In a replicated database, some data items may be stored at multiple sites. The main motivation is improved reliability: by storing important data at multiple sites, the DBS can operate even though some sites have failed. This paper describes an algorithm for handling replicated data, which allows users to operate on data so long as one copy is “available.” A copy is “available” when (i) its site is up, and (ii) the copy is not out-of-date because of an earlier crash. The algorithm handles clean, detectable site failures, but not Byzantine failures or network partitions. © 1984, ACM. All rights reserved.",continuous operation; distributed databases; replicated databases; Serializability; transaction processing,DATABASE SYSTEMS; CONCURRENCY CONTROL AND RECOVERY; CONTINOUS OPERATION; REPLICATED DISTRIBUTED DATABASES; SERIALIZABILITY; TRANSACTION PROCESSING; COMPUTER PROGRAMMING
Principles of Database Buffer Management,1984,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0021625746&doi=10.1145%2f1994.2022&partnerID=40&md5=17197e47600d90fa024b145f26f3e9ee,"This paper discusses the implementation of a database buffer manager as a component of a DBMS. The interface between calling components of higher system layers and the buffer manager is described; the principal differences between virtual memory paging and database buffer management are outlined; the notion of referencing versus addressing of database pages is introduced; and the concept of fixing pages in the buffer to prevent uncontrolled replacement is explained. Three basic tasks have to be performed by the buffer manager: buffer search, allocation of frames to concurrent transactions, and page replacement. For each of these tasks, implementation alternatives are discussed and illustrated by examples from a performance evaluation project of a CODASYL DBMS. © 1984, ACM. All rights reserved.",buffer management; Database; locality; replacement algorithms,COMPUTER PROGRAMMING - Algorithms; DATA PROCESSING; BUFFER MANAGEMENT; MEMORY PAGING; REFERENCING DATABASE PAGES; REPLACEMENT ALGORITHMS; DATABASE SYSTEMS
Performance Analysis of Recovery Techniques,1984,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0021615097&doi=10.1145%2f1994.1996&partnerID=40&md5=1fdde6b6143a932335d2f208ed854c12,"Various logging and recovery techniques for centralized transaction-oriented database systems under performance aspects are described and discussed. The classification of functional principles that has been developed in a companion paper is used as a terminological basis. In the main sections, a set of analytic models is introduced and evaluated in order to compare the performance characteristics of nine different recovery techniques with respect to four key parameters and a set of other parameters with less influence. Finally, the results of model evaluation as well as the limitations of the models themselves are discussed. © 1984, ACM. All rights reserved.",,COMPUTER PROGRAMMING - Algorithms; DATA PROCESSING; LOGGING AND RECOVERY; RECOVERY AND RESTART; RECOVERY TECHNIQUES; TRANSACTION PROCESSING; DATABASE SYSTEMS
Analysis of Dynamic Hashing with Deferred Splitting,1985,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0022025121&doi=10.1145%2f3148.318987&partnerID=40&md5=a22ad9b5a2100636fd706c5408b0b97e,"Dynamic hashing with deferred splitting is a file organization scheme which increases storage utilization, as compared to standard dynamic hashing. In this scheme, splitting of a bucket is deferred if the bucket is full but its brother can accommodate new records. The performance of the scheme is analyzed. In a typical case the expected storage utilization increases from 69 to 76 percent. © 1985, ACM. All rights reserved.",,DATABASE SYSTEMS; DEFERRED SPLITTING; DYNAMIC HASHING; STORAGE UTILIZATION; DATA PROCESSING
Expressions for Batched Searching of Sequential and Hierarchical Files,1985,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0022028915&doi=10.1145%2f3148.3326&partnerID=40&md5=bcf0416544fb02a4a638b58954edafe9,"Batching yields significant savings in access costs in sequential, tree structured, and random files. A direct and simple expression is developed for computing the average number of records/pages accessed to satisfy a batched query of a sequential tile. The advantages of batching for sequential and random files are discussed. A direct equation is provided for the number of nodes accessed in unhatched queries of hierarchical files. An exact recursive expression is developed for node accesses in batched queries of hierarchical files. In addition to the recursive relationship, good, closed-form upper- and lower-bound approximations are provided for the case of batched queries of hierarchical files. © 1985, ACM. All rights reserved.",,INFORMATION SCIENCE - Information Retrieval; BATCHED SEARCHING; HIERARCHICAL FILES; SEQUENTIAL FILES; DATABASE SYSTEMS
Organization of Clustered Files for Consecutive Retrieval,1984,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0021609025&doi=10.1145%2f1994.2208&partnerID=40&md5=39079343fbff5cebea2d6236ed671996,"This paper studies the problem of storing single-level and multilevel clustered files. Necessary and sufficient conditions for a single-level clustered file to have the consecutive retrieval property (CRP) are developed. A linear time algorithm to test the CRP for a given clustered file and to identify the proper arrangement of objects, if CRP exists, is presented. For the single-level clustered files that do not have CRP, it is shown that the problem of identifying a storage organization with minimum redundancy is NP-complete. Consequently, an efficient heuristic algorithm to generate a good storage organization for such files is developed. Furthermore, it is shown that, for certain types of multilevel clustered files, there exists a storage organization such that the objects in each cluster, for all clusters in each level of the clustering, appear in consecutive locations. © 1984, ACM. All rights reserved.",consecutive retrieval property; heuristic algorithm; minimum redundancy organization; multilevel clustered files; NP-complete; overlapping and nonoverlapping clustering; Single-level clustered files,COMPUTER PROGRAMMING - Algorithms; DATA PROCESSING - File Organization; CLUSTERED FILES; CONSECUTIVE RETRIEVAL; FILE ORGANIZATION; NP-COMPLETE; INFORMATION RETRIEVAL SYSTEMS
Use of Graph-Theoretic Models for Optimal Relational Database Accesses to Perform Join,1985,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0022029215&doi=10.1145%2f3148.3325&partnerID=40&md5=8bda76e084f423f1321c64962a5465fd,"A graph model is presented to analyze the performance of a relational join. The amount of page reaccesses, the page access sequence, and the amount of buffer needed are represented in terms of graph parameters. By using the graph model formed from the index on the join attributes, we determine the relationships between these parameters. Two types of buffer allocation strategies are studied, and the upper bound on the buffer size with no page reaccess is given. This bound is shown to be the maximum cut value of a graph. Hence, the problem of computing this upper bound is NP-hard. We also give algorithms to determine a page access sequence requiring a near optimal buffer size with no page reaccess. The optimal page access sequence for a fixed buffer size has also been considered. © 1985, ACM. All rights reserved.",,COMPUTER PROGRAMMING - Algorithms; MATHEMATICAL TECHNIQUES - Graph Theory; GRAPH-THEORETIC MODELS; RELATIONAL DATABASE ACCESSES; RELATIONAL JOIN; DATABASE SYSTEMS
Linear Hashing with Overflow-Handling by Linear Probing,1985,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0022026346&doi=10.1145%2f3148.3324&partnerID=40&md5=0657692cde5667dd3eb01700236efc3b,"Linear hashing is a file structure for dynamic files. In this paper, a new, simple method for handling overflow records in connection with linear hashing is proposed. The method is based on linear probing and does not rely on chaining. No dedicated overflow area is required. The expansion sequence of liner hashing is modified to improve the performance, which requires changes in the address computation. A new address computation algorithm and an expansion algorithm are given. The performance of the method is studied by simulation. The algorithms for the basic file operations are very simple, and the overall performance is competitive with that of other variants of linear hashing. © 1985, ACM. All rights reserved.",,COMPUTER PROGRAMMING - Algorithms; DATABASE SYSTEMS; DYNAMIC HASHING; LINEAR HASHING; OPEN ADDRESSING; DATA PROCESSING
Limitations of Concurrency in Transaction Processing,1985,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976842700&doi=10.1145%2f3148.3160&partnerID=40&md5=b8100d956c19e146a3c20b6390b55c80,"Given the pairwise probability of conflict p among transactions in a transaction processing system, together with the total number of concurrent transactions n, the effective level of concurrency E(n,p) is defined as the expected number of the n transactions that can run concurrently and actually do useful work. Using a random graph model of concurrency, we show for three general classes of concurrency control methods, examples of which are (1) standard locking, (2) strict priority scheduling, and (3) optimistic methods, that (1) E(n, p) ⩽ n(1 - p/2)n-1, (2) E(n, p) ⩽ (1 - (1 - p)n)/p, and (3) 1 + ((1 - p)/p)ln(p(n - 1) + 1) ⩽ E(n, p) ⩽ 1 + (1/p)ln(p(n - 1) + 1). Thus, for fixed p, as n ↣ ∞), (1) E ↣ 0 for standard locking methods, (2) E ⩽ 1/p for strict priority scheduling methods, and (3) E ↣ ∞ for optimistic methods. Also found are bounds on E in the case where conflicts are analyzed so as to maximize E. The predictions of the random graph model are confirmed by simulations of an abstract transaction processing system. In practice, though, there is a price to pay for the increased effective level of concurrency of methods (2) and (3): using these methods there is more wasted work (i.e., more steps executed by transactions that are later aborted). In response to this problem, three new concurrency control methods suggested by the random graph model analysis are developed. Two of these, called (a) running priority and (b) older or running priority, are shown by the simulation results to perform better than the previously known methods (l)-(3) for relatively large n or large p, in terms of achieving a high effective level of concurrency at a comparatively small cost in wasted work. © 1985, ACM. All rights reserved.",,
"Galileo: A Strongly-Typed, Interactive Conceptual Language",1985,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0022075207&doi=10.1145%2f3857.3859&partnerID=40&md5=e964f606491e1c7ba4ed94bb0d7402d6,"Galileo, a programming language for database applications, is presented. Galileo is a strongly-typed, interactive programming language designed specifically to support semantic data model features (classification, aggregation, and specialization), as well as the abstraction mechanisms of modern programming languages (types, abstract types, and modularization). The main contributions of Galileo are (a) a flexible type system to model database structure and semantic integrity constraints; (b) the inclusion of type hierarchies to support the specialization abstraction mechanisms of semantic data models; (c) a modularization mechanism to structure data and operations into interrelated units (d) the integration of abstraction mechanisms into an expression-based language that allows interactive use of the database without resorting to a new stand-alone query language. Galileo will be used in the immediate future as a tool for database design and, in the long term, as a high-level interface for DBMSs. © 1985, ACM. All rights reserved.",,DATABASE SYSTEMS; DATA DESCRIPTION LANGUAGES; DATA MANIPULATION; GALILEO; QUERY LANGUAGES; COMPUTER PROGRAMMING LANGUAGES
A Database Cache for High Performance and Fast Restart in Database Systems,1984,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0021627852&doi=10.1145%2f1994.1995&partnerID=40&md5=7ca62f67ddc2be40ab02dd9a25431618,"Performance in database systems is strongly influenced by buffer management and transaction recovery methods. This paper presents the principles of the database cache, which replaces the traditional buffer. In comparison to buffer management, cache management is more carefully coordinated with transaction management, and integrates transaction recovery. High throughput ofsmall- and medium-sized transactions is achieved by fast commit processing and low database traffic. Very fast handling of transaction failures and short restart time after system failure are guaranteed in such an environment. Very long retrieval and update transactions are also supported. © 1984, ACM. All rights reserved.",Buffer management; crash recovery; media failure; very long transactions,DATA PROCESSING; BUFFER MANAGEMENT; CRASH RECOVERY; DATABASE CACHE; FAST RESTART; MEDIA FAILURE; DATABASE SYSTEMS
"Logical, internal, and physical reference behavior in CODASYL database systems",1984,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0021444721&doi=10.1145%2f329.331&partnerID=40&md5=5fd5e3952e40f84eead872a40b5ba7e5,"This work investigates one aspect of the performance of CODASYL database systems: the data reference behavior. We introduce a model of database traversals at three levels: the logical, internal, and physical levels. The mapping between the logical and internal levels is defined by the internal schema, whereas the mapping between the internal and the physical levels depends on cluster properties of the database. Our model explains the physical reference behavior for a given sequence of DML statements at the logical level. Software has been implemented to monitor references in two selected CODASYL DBMS applications. In a series of experiments the physical reference behavior was observed for varying internal schemas and cluster properties of the database. The measurements were limited to retrieval transactions, so that a variety of queries could be analyzed for the same well-known state of the database. Also, all databases were relatively small in order to allow fast reloading with varying internal schema parameters. In all cases, the database transactions showed less locality of reference than do programs under virtual memory operating systems; some databases showed no locality at all. No evidence of physical sequentiality was found. This suggests that standard page replacement strategies are not optimal for CODASYL database buffer management; instead, replacement decisions in a database buffer should be based on specific knowledge available from higher system layers. © 1984, ACM. All rights reserved.",buffer management; CODASYL database system; Database reference behavior,DATABASE SYSTEMS
On the optimal nesting order for computing N-relational joins,1984,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0021497828&doi=10.1145%2f1270.1498&partnerID=40&md5=bcf22bbc1ee6a98f50f0821d8256d3c8,"Using the nested loops method, this paper addresses the problem of minimizing the number of page fetches necessary to evaluate a given query to a relational database. We first propose a data structure whereby the number of page fetches required for query evaluation is substantially reduced and then derive a formula for the expected number of page fetches. An optimal solution to our problem is the nesting order of relations in the evaluation program, which minimizes the number of page fetches. Since the minimization of the formula is NP-hard, as shown in the Appendix, we propose a heuristic algorithm which produces a good suboptimal solution in polynomial time. For the special case where the input query is a “tree query,” we present an efficient algorithm for finding an optimal nesting order. © 1984, ACM. All rights reserved.",,N-RELATIONAL JOINS; OPTIMAL NESTING ORDER; DATABASE SYSTEMS
Resolving the query inference problem using Steiner trees,1984,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0021500078&doi=10.1145%2f1270.1275&partnerID=40&md5=83d5662ce1a61e6ae2f98b9553924b09,"The query inference problem is to translate a sentence of a query language into an unambiguous representation of a query. A query is represented as an expression over a set of query trees. A metric is introduced for measuring the complexity of a query and also a proposal that a sentence be translated into the least complex query which “satisfies” the sentence. This method of query inference can be used to resolve ambiguous sentences and leads to easier formulation of sentences. © 1984, ACM. All rights reserved.",,QUERY INFERENCE PROBLEM; STEINER TREES; DATABASE SYSTEMS
"The Grid File: An Adaptable, Symmetric Multikey File Structure",1984,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0021392590&doi=10.1145%2f348.318586&partnerID=40&md5=ce5d8c8e99c0d7f7af79669fb0463963,"Traditional file structures that provide multikey access to records, for example, inverted files, are extensions of file structures originally designed for single-key access. They manifest various deficiencies in particular for multikey access to highly dynamic files. We study the dynamic aspects of file structures that treat all keys symmetrically, that is, file structures which avoid the distinction between primary and secondary keys. We start from a bitmap approach and treat the problem of file design as one of data compression of a large sparse matrix. This leads to the notions of a grid partition of the search space and of a grid directory, which are the keys to a dynamic file structure called the grid file. This file system adapts gracefully to its contents under insertions and deletions, and thus achieves an upper bound of two disk accesses for single record retrieval; it also handles range queries and partially specified queries efficiently. We discuss in detail the design decisions that led to the grid file, present simulation results of its behavior, and compare it to other multikey access file structures. © 1984, ACM. All rights reserved.",database; dynamic storage allocation; File structures; multidimensional data; multikey searching,DATA PROCESSING
Database states and their tableaux,1984,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0021439314&doi=10.1145%2f329.318579&partnerID=40&md5=0a6eecec2b3f05598ddaa9586b3b7e87,"Recent work considers a database state to satisfy a set of dependencies if there exists a satisfying universal relation whose projections contain each of the relations in the state. Such relations are called weak instances for the state. We propose the set of all weak instances for a state as an embodiment of the information represented by the state. We characterize states that have the same set of weak instances by the equivalence of their associated tableaux. We apply this notion to the comparison of database schemes and characterize all pairs of schemes such that for every legal state of one of them there exists an equivalent legal state of the other one. We use this approach to provide a new characterization of Boyce-Codd Normal Form relation schemes. © 1984, ACM. All rights reserved.",Boyce-Codd Normal Form; Chase; database scheme equivalence; functional dependency; join dependency; relational database; tableau; universal relation scheme,DATABASE SYSTEMS
On the selection of efficient record segmentations and backup strategies for large shared databases,1984,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0021494128&doi=10.1145%2f1270.1481&partnerID=40&md5=8e5bcbc6df0f03b026093f813311a01b,"In recent years the information processing requirements of business organizations have expanded tremendously. With this expansion, the design of databases to efficiently manage and protect business information has become critical. We analyze the impacts of record segmentation (the assignment of data items to segments defining subfiles), an efficiency-oriented design technique, and of backup and recovery strategies, a data protection technique, on the overall process of database design. A combined record segmentation/backup and recovery procedure is presented and an application of the procedure is discussed. Results in which problem characteristics are varied along three dimensions: update frequencies, available types of access paths, and the predominant type of data retrieval that must be supported by the database, are presented. © 1984, ACM. All rights reserved.",,BACKUP STRATEGIES; LARGE SHARED DATABASES; RECORD SEGMENTATIONS; DATABASE SYSTEMS
A parallel pipelined relational query processor,1984,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0021445039&doi=10.1145%2f329.332&partnerID=40&md5=40b9b825009d18c9f681f5ad53a0b947,"This paper presents the design of a relational query processor. The query processor consists of only four processing PIPEs and a number of random-access memory modules. Each PIPE processes tuples of relations in a bit-serial, tuple-parallel manner for each of the primitive database operations which comprise a complex relational query. The design of the query processor meets three major objectives: the query processor must be manufacturable using existing and near-term LSI (VLSI) technology; it must support in a uniform manner both the numeric and nonnumeric processing requirements a high-level user interface like SQL presents; and it must support the query-processing strategy derived in the query optimizer to satisfy certain system-wide performance optimality criteria. © 1984, ACM. All rights reserved.",Relational model; SQL,COMPUTER ARCHITECTURE; PIPELINE PROCESSING; RELATIONAL QUERY PROCESSOR; DATABASE SYSTEMS
A Database Management System for the Federal Courts,1984,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0021390368&doi=10.1145%2f348.318587&partnerID=40&md5=ae0f2f7a0969a417ee74240e58d4fd1b,"A judicial systems laboratory has been established and several large-scale information management systems projects have been undertaken within the Federal Judicial Center in Washington, D.C. The newness of the court application area, together with the experimental nature of the initial prototypes, required that the system building tools be as flexible and efficient as possible for effective software design and development. The size of the databases, the expected transaction volumes, and the long-term value of the court records required a data manipulation system capable of providing high performance and integrity. The resulting design criteria, the programming capabilities developed, and their use in system construction are described herein. This database programming facility has been especially designed as a technical management tool for the database administrator, while providing the applications programmer with a flexible database software interface for high productivity. Specifically, a network-type database management system using SAIL as the data manipulation host language is described. Generic data manipulation verb formats using SAIL’s macro facilities and dynamic data structuring facilities allowing in-core database representations have been developed to achieve a level of flexibility not usually attained in conventional database systems. © 1984, ACM. All rights reserved.",network model; SAIL,DATABASE SYSTEMS
An extension of conflict-free multivalued dependency sets,1984,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0021443094&doi=10.1145%2f329.354&partnerID=40&md5=5c25f845f98294727c4a216aeeafdaca,"Several researchers (Beeri, Bernstein, Chiu, Fagin, Goodman, Maier, Mendelzon, Ullman, and Yannakakis) have introduced a special class of database schemes, called acyclic or tree schemes. Beeri et al. have shown that an acyclic join dependency, naturally defined by an acyclic database scheme, has several desirable properties, and that an acyclic join dependency is equivalent to a conflict-free set of multivalued dependencies. However, since their results are confined to multivalued and join dependencies, it is not clear whether we can handle functional dependencies independently of other dependencies. In the present paper we define an extension of a conflict-free set, called an extended conflict-free set, including multivalued dependencies and functional dependencies, and show the following two properties of an extended conflict-free set: There are three equivalent definitions of an extended conflict-free set. One of them is defined as a set including an acyclic joint dependency and a set of functional dependencies such that the left and right sides of each functional dependency are included in one of the attribute sets that construct the acyclic join dependency. For a relation scheme with an extended conflict-free set, there is a decomposition into third normal form with a lossless join and preservation of dependencies. © 1984, ACM. All rights reserved.",Acyclic; conflict-free; database scheme; hypergraph; join dependency; multivalued dependency; relational database,DATABASE SYSTEMS
The average time until bucket overflow,1984,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0021493684&doi=10.1145%2f1270.318575&partnerID=40&md5=bd55bd6cb71a84fc1979235760f83829,"It is common for file structures to be divided into equal-length partitions, called buckets, into which records arrive for insertion and from which records are physically deleted. We give a simple algorithm which permits calculation of the average time until overflow for a bucket of capacity n records, assuming that record insertions and deletions can be modeled as a stochastic process in the usual manner of queueing theory. We present some numerical examples, from which we make some general observations about the relationships among insertion and deletion rates, bucket capacity, initial fill, and average time until overflow. In particular, we observe that it makes sense to define the stable point as the product of the arrival rate and the average residence time of the records; then a bucket tends to fill up to its stable point quickly, in an amount of time almost independent of the stable point, but the average time until overflow increases rapidly with the difference between the bucket capacity and the stable point. © 1984, ACM. All rights reserved.",,BUCKET OVERFLOW; DATA PROCESSING
SYSTEM/U: A database system based on the universal relation assumption,1984,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0021497238&doi=10.1145%2f1270.1209&partnerID=40&md5=71aa6e83058f814d240759c56681c7a9,"System/U is a universal relation database system under development at Standford University which uses the language C on UNIX. The system is intended to test the use of the universal view, in which the entire database is seen as one relation. This paper describes the theory behind System/U, in particular the theory of maximal objects and the connection between a set of attributes. We also describe the implementation of the DDL (Data Description Language) and the DML (Data Manipulation Language), and discuss in detail how the DDL finds maximal objects and how the DML determines the connection between the attributes that appear in a query. © 1984, ACM. All rights reserved.",,COMPUTER PROGRAMMING LANGUAGES; RELATIONAL DATABASE; SYSTEM/U; UNIVERSAL RELATION; DATABASE SYSTEMS
Algorithms for trie compaction,1984,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0021441657&doi=10.1145%2f329.295&partnerID=40&md5=7b031c84ed308814be0085991202f9df,"The trie data structure has many properties which make it especially attractive for representing large files of data. These properties include fast retrieval time, quick unsuccessful search determination, and finding the longest match to a given identifier. The main drawback is the space requirement. In this paper the concept of trie compaction is formalized. An exact algorithm for optimal trie compaction and three algorithms for approximate trie compaction are given, and an analysis of the three algorithms is done. The analysis indicate that for actual tries, reductions of around 70 percent in the space required by the uncompacted trie can be expected. The quality of the compaction is shown to be insensitive to the number of nodes, while a more relevant parameter is the alphabet size of the key. © 1984, ACM. All rights reserved.",approximation algorithm; data structure; node compaction; storage retrieval; Trie,DATA PROCESSING
Concurrency control in a dynamic search structure,1984,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0021500156&doi=10.1145%2f1270.318576&partnerID=40&md5=53326371bb007b19caa347944d24d41b,"A design of a data structure and efficient algorithms for concurrent manipulations of a dynamic search structure by independent user processes is presented in this paper. The algorithms include updating data, inserting new elements, and deleting elements. The algorithms support a high level of concurrency. Each of the operations listed above requires only constant amount of locking. In order to make the system even more efficient for the user processes, maintenance processes are introduced. The maintenance processes operate independently in the background to reorganize the data structure and “clean up” after the (more urgent) user processes. A proof of correctness of the algorithms is given and some experimental results and extensions are examined. © 1984, ACM. All rights reserved.",,COMPUTER PROGRAMMING - Algorithms; CONCURRENCY CONTROL; DYNAMIC SEARCH STRUCTURE; DATA PROCESSING
On the foundations of the universal relation model,1984,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0021439622&doi=10.1145%2f329.318580&partnerID=40&md5=97f45aa2e9ce04c800ac28b37ee660d2,"The universal relation model aims at achieving complete access-path independence in relational databases by relieving the user of the need for logical navigation among relations. We clarify the assumptions underlying it and explore the approaches suggested for implementing it. The essential idea of the universal relation model is that access paths are embedded in attribute names. Thus attribute names must play unique “roles.” Furthermore, it assumes that for every set of attributes there is a basic relationship that the user has in mind. The user’s queries refer to these basic relationships rather than to the underlying database. Two fundamentally different approaches to the universal relation model have been taken. According to the first approach, the user’s view of the database is a universal relation or many universal relations, about which the user poses queries. The second approach sees the model as having query-processing capabilities that relieve the user of the need to specify the logical access path. Thus, while the first approach gives a denotational semantics to query answering, the second approach gives it an operational semantics. We investigate the relationship between these two approaches. © 1984, ACM. All rights reserved.",Access path; database; dependency; losslessness; query; relational algebra; relational model; representative instance; tableau; universal relation; user view; weak universal relations,DATABASE SYSTEMS
Practical data-swapping: The first steps,1984,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0021389555&doi=10.1145%2f348.349&partnerID=40&md5=98b994de08aec926c48af290fb4a4e0e,"The problem of statistical database confidentiality in releasing microdata is addressed through the use of approximate data-swapping. Here, a portion of the microdata is replaced with a database that has been selected with approximately the same statistics. The result guarantees the confidentiality of the original data, while providing microdata with accurate statistics. Methods for achieving such transformations are considered and analyzed through simulation. © 1984, ACM. All rights reserved.",Confidentiality; data-swapping; statistical databases,DATABASE SYSTEMS
Join and Semijoin Algorithms for a Multiprocessor Database Machine,1984,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0021386165&doi=10.1145%2f348.318590&partnerID=40&md5=32e68eb4243369c0190975a3e3957d13,"This paper presents and analyzes algorithms for computing joins and semijoins of relations in a multiprocessor database machine. First, a model of the multiprocessor architecture is described, incorporating parameters defining I/O, CPU, and message transmission times that permit calculation of the execution times of these algorithms. Then, three join algorithms are presented and compared. It is shown that, for a given configuration, each algorithm has an application domain defined by the characteristics of the operand and result relations. Since a semijoin operator is useful for decreasing I/O and transmission times in a multiprocessor system, we present and compare two equi-semijoin algorithms and one non-equi-semijoin algorithm. The execution times of these algorithms are generally linearly proportional to the size of the operand and result relations, and inversely proportional to the number of processors. We then compare a method which consists of joining two relations to a method whereby one joins their semijoins. Finally, it is shown that the latter method, using semijoins, is generally better. The various algorithms presented are implemented in the SABRE database system; an evaluation model selects the best algorithm for performing a join according to the results presented here. A first version of the SABRE system is currently operational at INRIA. © 1984, ACM. All rights reserved.",join; Relational algebra; semijoin,"COMPUTER SYSTEMS, DIGITAL - Multiprocessing; DATABASE SYSTEMS"
On Concurrency Control by Multiple Versions,1984,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0021393120&doi=10.1145%2f348.318588&partnerID=40&md5=37bde9cbf1c38badb9f0e909b16075ae,"We examine the problem of concurrency control when the database management system supports multiple versions of the data. We characterize the limit of the parallelism achievable by the multiversion approach and demonstrate the resulting space-parallelism trade-off. © 1984, ACM. All rights reserved.",Concurrency control; multiple versions; NP-complete; scheduler; serializability,DATABASE SYSTEMS
Implications of certain assumptions in database performance evauation,1984,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0021443979&doi=10.1145%2f329.318578&partnerID=40&md5=285b8f0cc8e991fd3e8a9f6cc27a2a32,"The assumptions of uniformity and independence of attribute values in a file, uniformity of queries, constant number of records per block, and random placement of qualifying records among the blocks of a file are frequently used in database performance evaluation studies. In this paper we show that these assumptions often result in predicting only an upper bound of the expected system cost. We then discuss the implications of nonrandom placement, nonuniformity, and dependencies of attribute values on database design and database performance evaluation. © 1984, ACM. All rights reserved.",Performance; Theory,DATABASE SYSTEMS
Optimism and consistency in partitioned distributed database systems,1984,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0021496873&doi=10.1145%2f1270.1499&partnerID=40&md5=b6989de9c84ea9377d38ae0f497c2726,"A protocol for transaction processing during partition failures is presented which guarantees mutual consistency between copies of data-items after repair is completed. The protocol is “optimistic” in that transactions are processed without restrictions during failure; conflicts are then detected at repair time using a precedence graph, and are resolved by backing out transactions according to some backout strategy. The resulting database state then corresponds to a serial execution of some subset of transactions run during the failure. Results from simulation and probabilistic modeling show that the optimistic protocol is a reasonable alternative in many cases. Conditions under which the protocol performs well are noted, and suggestions are made as to how performance can be improved. In particular, a backout strategy is presented which takes into account individual transaction costs and attempts to minimize total backout cost. Although the problem of choosing transactions to minimize total backout cost is, in general, NP-complete, the backout strategy is efficient and produces very good results. © 1984, ACM. All rights reserved.",,DISTRIBUTED DATABASE SYSTEMS; DATABASE SYSTEMS
Response Time Analysis of Multiprocessor Computers for Database Support,1984,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0021393269&doi=10.1145%2f348.318589&partnerID=40&md5=c6dcf818c00bf242cd56f79e5843a928,"Comparison of three multiprocessor computer architectures for database support is made possible through evaluation of response time expressions. These expressions are derived by parameterizing algorithms performed by each machine to execute a relational algebra query. Parameters represent properties of the database and components of the machines. Studies of particular parameter values exhibit response times for conventional machine technology, for low selectivity, high duplicate occurrence, and parallel disk access, increasing the number of processors, and improving communication and processing technology. © 1984, ACM. All rights reserved.",DIRECT; HYPERTREE; relational model; Relational queries; REPT; response analysis,"COMPUTER SYSTEMS, DIGITAL - Multiprocessing; DATABASE SYSTEMS"
Designing a Portable Natural Language Database Query System,1984,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0021393026&doi=10.1145%2f348.318584&partnerID=40&md5=e04a4e4c117659e025d85359eaf1e423,"One barrier to the acceptance of natural language database query systems is the substantial installation effort required for each new database. Much of this effort involves the encoding of semantic knowledge for the domain of discourse, necessary to correctly interpret and respond to natural language questions. For such systems to be practical, techniques must be developed to increase their portability to new domains. This paper discusses several issues involving the portability of natural language interfaces to database systems, and presents the approach taken in CO-OP — a natural language database query system that provides cooperative responses to English questions and operates with a typical CODA-SYL database system. CO-OP derives its domain-specific knowledge from a lexicon (the list of words known to the system) and the information already present in the structure and content of the underlying database. Experience with the implementation suggests that strategies that are not directly derivative of cognitive or linguistic models may nonetheless play an important role in the development of practical natural language systems. © 1984, ACM. All rights reserved.",Cooperative responses; indirect answers to database queries; portable natural language interfaces,DATABASE SYSTEMS
Performance of Recovery Architectures in Parallel Associative Database Processors,1983,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0020827237&doi=10.1145%2f319989.319990&partnerID=40&md5=dc62f32f886e61fa4b9a035bd657d8f4,"The need for robust recovery facilities in modern database management systems is quite well known. Various authors have addressed recovery facilities and specific techniques, but none have delved into the problem of recovery in database machines. In this paper, the types of undesirable events that occur in a database environment are classified and the necessary recovery information, with subsequent actions to recover the correct state of the database, is summarized. A model of the “processor-per-track” class of parallel associative database processor is presented. Three different types of recovery mechanisms that may be considered for parallel associative database processors are identified. For each architecture, both the workload imposed by the recovery mechanisms on the execution of database operations (i.e., retrieve, modify, delete, and insert) and the workload involved in the recovery actions (i.e., rollback, restart, restore, and reconstruct) are analyzed. The performance of the three architectures is quantitatively compared. This comparison is made in terms of the number of extra revolutions of the database area required to process a transaction versus the number of records affected by a transaction. A variety of different design parameters of the database processor, of the database, and of a mix of transaction types (modify, insert, and delete) are considered. A large number of combinations is selected and the effects of the parameters on the extra processing time are identified. © 1983, ACM. All rights reserved.",Associative database processors,DATABASE SYSTEMS
The Universal Relation Revisited,1983,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976839096&doi=10.1145%2f319996.320019&partnerID=40&md5=265ad0c9b567d1b7335ea0bb4bb39fb5,[No abstract available],,
On Kent's “Consequences of Assuming a Universal Relation”,1983,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976825809&doi=10.1145%2f319996.320017&partnerID=40&md5=6a83078b06e2b8cf348a91bbd9fc2713,[No abstract available],,
EAS-E: An Integrated Approach to Application Development,1983,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0020877298&doi=10.1145%2f319996.320003&partnerID=40&md5=b999e8e4c2d44a3dd60a32aca5531cbf,"EAS-E (pronounced EASY) is an experimental programming language integrated with a database management system now running on VM/370 at the IBM Thomas J. Watson Research Center. The EAS-E programming language is built around the entity, attribute, and set (EAS) view of application development. It provides a means for translating operations on EAS structures directly into executable code. EAS-E commands have an English-like syntax, and thus EAS-E programs are easy to read and understand. EAS-E programs are also more compact than equivalent programs in other database languages.The EAS-E database management system allows many users simultaneous access to the database. It supports locking and deadlock detection and is capable of efficiently supporting network databases of various sizes including very large databases, consisting of several millions of entities stored on multiple DASD extends. Also available is a nonprocedural facility that allows a user to browse and update the database without writing programs. © 1983, ACM. All rights reserved.",entity relationship model,DATABASE SYSTEMS; COMPUTER PROGRAMMING LANGUAGES
Partial-Match Retrieval Using Hashing and Descriptors,1983,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0020908479&doi=10.1145%2f319996.320006&partnerID=40&md5=3aadd182f6ff27bf73f59b7fb9499c60,"This paper studies a partial-match retrieval scheme based on hash functions and descriptors. The emphasis is placed on showing how the use of a descriptor file can improve the performance of the scheme. Records in the file are given addresses according to hash functions for each field in the record. Furthermore, each page of the file has associated with it a descriptor, which is a fixed-length bit string, determined by the records actually present in the page. Before a page is accessed to see if it contains records in the answer to a query, the descriptor for the page is checked. This check may show that no relevant records are on the page and, hence, that the page does not have to be accessed. The method is shown to have a very substantial performance advantage over pure hashing schemes, when some fields in the records have large key spaces. A mathematical model of the scheme, plus an algorithm for optimizing performance, is given. © 1983, ACM. All rights reserved.",descriptors; dynamic file; hashing; optimization; partial-match retrieval,COMPUTER PROGRAMMING - Algorithms; DATA PROCESSING - File Organization; MATHEMATICAL MODELS; OPTIMIZATION; INFORMATION SCIENCE
Multiversion Concurrency Control—Theory and Algorithms,1983,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0020907710&doi=10.1145%2f319996.319998&partnerID=40&md5=2219304c209f542215387c37a91481bd,"Concurrency control is the activity of synchronizing operations issued by concurrently executing programs on a shared database. The goal is to produce an execution that has the same effect as a serial (noninterleaved) one. In a multiversion database system, each write on a data item produces a new copy (or version) of that data item. This paper presents a theory for analyzing the correctness of concurrency control algorithms for multiversion database systems. We use the theory to analyze some new algorithms and some previously published ones. © 1983, ACM. All rights reserved.",transaction processing,COMPUTER PROGRAMMING - Algorithms; DATABASE SYSTEMS
Correctness of Query Execution Strategies in Distributed Databases,1983,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0020881450&doi=10.1145%2f319996.320009&partnerID=40&md5=b5a78a2c1c30844f6e3dbf769e3d0fe3,"A major requirement of a Distributed DataBase Management System (DDBMS) is to enable users to write queries as though the database were not distributed (distribution transparency). The DDBMS transforms the user's queries into execution strategies, that is, sequences of operations on the various nodes of the network and of transmissions between them. An execution strategy on a distributed database is correct if it returns the same result as if the query were applied to a nondistributed database.This paper analyzes the correctness problem for query execution strategies. A formal model, called Multirelational Algebra, is used as a unifying framework for this purpose. The problem of proving the correctness of execution strategies is reduced to the problem of proving the equivalence of two expressions of Multirelational Algebra. A set of theorems on equivalence is given in order to facilitate this task.The proposed approach can be used also for the generation of correct execution strategies, because it defines the rules which allow the transformation of a correct strategy into an equivalent one. This paper does not deal with the problem of evaluating equivalent strategies, and therefore is not in itself a proposal for a query optimizer for distributed databases. However, it constitutes a theoretical foundation for the design of such optimizers. © 1983, ACM. All rights reserved.",correctness of database access; distributed database access; read-only transactions; relational algebra,DATABASE SYSTEMS
A foundation of codd's relational maybe-operations,1983,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0020948474&doi=10.1145%2f319996.320014&partnerID=40&md5=c84e39b7caa46b5788687e1e9a99f5e2,"Database relations which possibly contain maybe-tuples and null values of type “value at present unknown” are studied. Maybe-tuples and null values are formally interpreted by our notion of representation, which uses classical notions of predicate logic, elaborates Codd's proposal of maybe-tuples, and adopts Reiter's concept of a closed world. Precise notions of information content and redundancy, associated with our notion of representation, are investigated. Extensions of the relational algebra to relations with maybe-tuples and null values are proposed. Our extensions are essentially Codd's, with some modifications. It is proved that these extensions have natural properties which are formally stated as being adequate and restricted.By the treatment of difference and division, our formal framework can be used even for operations that require “negative information.” Finally, extensions of update operations are discussed. © 1983, ACM. All rights reserved.",closed world assumption; information content; maybe-tuple; negative information; null value; open word assumption; redundancy; relational algebra; relational database; representation,DATABASE SYSTEMS
Shadowed Management of Free Disk Pages with a Linked List,1983,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0020944191&doi=10.1145%2f319996.320002&partnerID=40&md5=90727569bb43a5c1d6796a7a31af89e3,"We describe and prove correct a programming technique using a linked list of pages for managing the free disk pages of a file system where shadowing is the recovery technique. Our technique requires a window of only two pages of main memory for accessing and maintaining the free list, and avoids wholesale copying of free-list pages during a checkpoint or recover operation. © 1983, ACM. All rights reserved.",checkpoint; dynamic storage allocation; file system; recovery; shadowing; storage management,COMPUTER PROGRAMMING; DATABASE SYSTEMS; COMPUTER OPERATING SYSTEMS
Hierarchical file organization and its application to similar-string matching,1983,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0020822266&doi=10.1145%2f319989.319994&partnerID=40&md5=a8fae20825a27b82aa1a31ff5e6cc846,"The automatic correction of misspelled inputs is discussed from a viewpoint of similar-string matching. First a hierarchical file organization based on a linear ordering of records is presented for retrieving records highly similar to any input query. Then the spelling problem is attacked by constructing a hierarchical file for a set of strings in a dictionary of English words. The spelling correction steps proceed as follows: (1) find one of the best-match strings which are most similar to a query, (2) expand the search area for obtaining the good-match strings, and (3) interrupt the file search as soon as the required string is displayed. Computational experiments verify the performance of the proposed methods for similar-string matching under the UNIX#8482; time-sharing system. © 1983, ACM. All rights reserved.",best match; file organization; good match; hierarchical clustering; linear ordering; office automation; similar-string; similarity; spelling correction; text editor,DATA PROCESSING
Multisafe—a modular multiprocessing approach to secure database management,1983,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0020821853&doi=10.1145%2f319989.319993&partnerID=40&md5=029f20a1a9ee8ee88f60d851e4d5d7df,"This paper describes the configuration and intermodule communication of a MULTImodule system for supporting Secure Authorization with Full Enforcement (MULTISAFE) for database management. A modular architecture is described which provides secure, controlled access to shared data in a multiuser environment, with low performance penalties, even for complex protection policies. The primary mechanisms are structured and verifiable. The entire approach is immediately extendible to distributed protection of distributed data. The system includes a user and applications module (UAM), a data storage and retrieval module (SRM), and a protection and security module (PSM). The control of intermodule communication is based on a data abstraction approach, initially described in terms of function invocations. An implementation within a formal message system is then described. The discussion of function invocations begins with the single terminal case and extends to the multiterminal case. Some physical implementation aspects are also discussed, and some examples of message sequences are given. © 1983, ACM. All rights reserved.",abstract data types; access control; back-end database; intermodule communication; secure database,DATABASE SYSTEMS
Achieving robustness in distributed database systems,1983,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0020824684&doi=10.1145%2f319989.319992&partnerID=40&md5=5b0986a53db736efc85ea314e923e72d,"The problem of concurrency control in distributed database systems in which site and communication link failures may occur is considered. The possible range of failures is not restricted; in particular, failures may induce an arbitrary network partitioning. It is desirable to attain a high “level of robustness” in such a system; that is, these failures should have only a small impact on system operation.A level of robustness termed maximal partial operability is identified. Under our models of concurrency control and robustness, this robustness level is the highest level attainable without significantly degrading performance.A basis for the implementation of maximal partial operability is presented. To illustrate its use, it is applied to a distributed locking concurrency control method and to a method that utilizes timestamps. When no failures are present, the robustness modifications for these methods induce no significant additional overhead. © 1983, ACM. All rights reserved.",concurrency control; network partitioning; robustness; searializability,DATABASE SYSTEMS
On the Complexity of Designing Optimal Partial-Match Retrieval Systems,1983,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0020937808&doi=10.1145%2f319996.320004&partnerID=40&md5=f1a866a7db086083c604ad1da27297de,"We consider the problem of designing an information retrieval system on which partial match queries have to be answered. Each record in the system consists of a list of attributes, and a partial match query specifies the values of some of the attributes. The records are stored in buckets in a secondary memory, and in order to answer a partial match query all the buckets that may contain a record satisfying the specifications of that query must be retrieved. The bucket in which a given record is stored is found by a multiple key hashing function, which maps each attribute to a string of a fixed number of bits. The address of that bucket is then represented by the string obtained by concatenating the strings on which the various attributes were mapped. A partial match query may specify only part of the bits in the string representing the address, and the larger the number of bits specified, the smaller the number of buckets that have to be retrieved in order to answer the query.The optimization problem considered in this paper is that of deciding to how many bits each attribute should be mapped by the bashing function above, so that the expected number of buckets retrieved per query is minimized. Efficient solutions for special cases of this problem have been obtained in [1], [12], and [14]. It is shown that in general the problem is NP-hard, and that if P ≠ NP, it is also not fully approximable. Two heuristic algorithms for the problem are also given and compared. © 1983, ACM. All rights reserved.",approximation algorithms; file organization; hashing; NP-hard problems; partial match retrieval; searching,COMPUTER PROGRAMMING - Algorithms; OPTIMIZATION; INFORMATION SCIENCE
Parallel algorithms for the execution of relational database operations,1983,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0020826601&doi=10.1145%2f319989.319991&partnerID=40&md5=8ed84234b7d147ecf6a90247386334d9,"This paper presents and analyzes algorithms for parallel processing of relational database operations in a general multiprocessor framework. To analyze alternative algorithms, we introduce an analysis methodology which incorporates I/O, CPU, and message costs and which can be adjusted to fit different multiprocessor architectures. Algorithms are presented and analyzed for sorting, projection, and join operations. While some of these algorithms have been presented and analyzed previously, we have generalized each in order to handle the case where the number of pages is significantly larger than the number of processors. In addition, we present and analyze algorithms for the parallel execution of update and aggregate operations. © 1983, ACM. All rights reserved.",aggregate operations; database machines; join operation; parallel processing; projection operator; sorting,DATABASE SYSTEMS
Multilevel Atomicity—A New Correctness Criterion for Database Concurrency Control,1983,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0020900969&doi=10.1145%2f319996.319999&partnerID=40&md5=024f21c21e25aec330587d6531ae5f1b,"Multilevel atomicity, a new correctness criteria for database concurrency control, is defined. It weakens the usual notion of serializability by permitting controlled interleaving among transactions. It appears to be especially suitable for applications in which the set of transactions has a natural hierarchical structure based on the hierarchical structure of an organization. A characterization for multilevel atomicity, in terms of the absence of cycles in a dependency relation among transaction steps, is given. Some remarks are made concerning implementation. © 1983, ACM. All rights reserved.",atomicity; breakpoint; transaction,DATABASE SYSTEMS
Indexing and retrieval strategies for natural language fact retrieval,1983,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0020824705&doi=10.1145%2f319989.319995&partnerID=40&md5=2a106219f3a3331e44e6ba927ca978d0,"Researchers in artificial intelligence have recently become interested in natural language fact retrieval; currently, their research is at a point where it can begin contributing to the field of Information Retrieval. In this paper, strategies for a natural language fact retrieval system are mapped out, and approaches to many of the organization and retrieval problems are presented. The CYRUS system, which keeps track of important people and is queried in English, is presented and used to illustrate those solutions. © 1983, ACM. All rights reserved.",artificial intelligence; conceptual memory; database retrieval; fact retrieval; natural language processing; question answering,INFORMATION SCIENCE
Maximal Objects and the Semantics of Universal Relation Databases,1983,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0020720446&doi=10.1145%2f319830.319831&partnerID=40&md5=da0fdbd98384e747e3cd17dbf2e79f30,"The universal relation concept is intended to provide the database user with a simplified model in which he can compose queries without regard to the underlying structure of the relations in the database. Frequently, the lossless join criterion provides the query interpreter with the clue needed to interpret the query as the user intended. However, some examples exist where interpretation by the lossless-join rule runs contrary to our intuition. To handle some of these cases, we propose a concept called maximal objects, which modifies the universal relation concept in exactly those situations where it appears to go awry—when the underlying relational structure has “cycles.” We offer examples of how the maximal object concept provides intuitively correct interpretations. We also consider how one might construct maximal objects mechanically from purely syntactic structural information—the relation schemes and functional dependencies—about the database. © 1983, ACM. All rights reserved.",acyclic hypergraph; relational database; universal relation,DATABASE SYSTEMS
Answering Queries Without Revealing Secrets,1983,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0020718929&doi=10.1145%2f319830.319833&partnerID=40&md5=3d45f7427fe41017b69c697f30fdf2bb,"Question-answering systems must often keep certain information secret. This can be accomplished, for example, by sometimes refusing to answer a query. Here the danger of revealing a secret by refusing to answer a query is investigated. First several criteria that can be used to decide whether or not to answer a query are developed. Then it is shown which of these criteria are safe if the questioner knows nothing at all about what is kept secret. Furthermore, it is proved that one of these criteria is safe even if the user of the system knows which information is to be kept secret. © 1983, ACM. All rights reserved.",keeping secrets; refusal to answer; strategy,DATABASE SYSTEMS
A Characterization of Globally Consistent Databases and Their Correct Access Paths,1983,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0020764827&doi=10.1145%2f319983.319988&partnerID=40&md5=450d230e63d2da60e702d9ee689dffe5,"The representative instance is proposed as a representation of the data stored in a database whose relations are not the projections of a universal instance. Database schemes are characterized for which local consistency implies global consistency. (Local consistency means that each relation satisfies its own functional dependencies; global consistency means that the representative instance satisfies all the functional dependencies.) A method of efficiently computing projections of the representative instance is given, provided that local consistency implies global consistency. Throughout, it is assumed that a cover of the functional dependencies is embodied in the database scheme in the form of keys. © 1983, ACM. All rights reserved.",Chase; extension join; functional dependency; null value; relational algebra; relational database; representative instance; universal relation scheme,DATABASE SYSTEMS
Rosolving Conflicts in Global Storage Design through Replication,1983,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0020717024&doi=10.1145%2f319830.319836&partnerID=40&md5=606f9e80127922f4adbb88f6270c1f10,"We present a conceptual framework in which a database's intra- and interrecord set access requirements are specified as a constrained assignment of abstract characteristics (“evaluated,” “indexed,” “clustered,” “well-placed”) to logical access paths. We derive a physical schema by choosing an available storage structure that most closely provides the desired access characteristics. We use explicit replication of schema objects to reduce the access cost along certain paths, and analyze the trade-offs between increased update overhead and improved retrieval access. Finally, we given an algorithm to select storage structures for a CODASYL 78 DBTG schema, given its access requirements specification. © 1983, ACM. All rights reserved.",access path selection; functional data model; storage structure choice,DATABASE SYSTEMS
Duplicate Record Elimination in Large Data Files,1983,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0020763652&doi=10.1145%2f319983.319987&partnerID=40&md5=e48382ba2fdaf8bd2b27b5bd2d3a135b,"The issue of duplicate elimination for large data files in which many occurrences of the same record may appear is addressed. A comprehensive cost analysis of the duplicate elimination operation is presented. This analysis is based on a combinatorial model developed for estimating the size of intermediate runs produced by a modified merge-sort procedure. The performance of this modified merge-sort procedure is demonstrated to be significantly superior to the standard duplicate elimination technique of sorting followed by a sequential pass to locate duplicate records. The results can also be used to provide critical input to a query optimizer in a relational database system. © 1983, ACM. All rights reserved.",duplicate elimination; projection operator; sorting,DATABASE SYSTEMS
Functions in Databases,1983,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0020717076&doi=10.1145%2f319830.319835&partnerID=40&md5=27f9546cb56b47d1d962bee96ead5842,"We discuss the objectives of including functional dependencies in the definition of a relational database. We find two distinct objectives. The appearance of a dependency in the definition of a database indicates that the states of the database are to encode a function. A method based on the chase of calculating the function encoded by a particular state is given and compared to methods utilizing derivations of the dependency. A test for deciding whether the states of a schema may encode a nonempty function is presented as is a characterization of the class of schemas which are capable of encoding nonempty functions for all the dependencies in the definition. This class is the class of dependency preserving schemas as defined by Beeri et al. and is strictly larger than the class presented by Bernstein.The second objective of including a functional dependency in the definition of a database is that the dependency be capable of constraining the states of the database; that is, capable of uncovering input errors made by the users. We show that this capability is weaker than the first objective; thus, even dependencies whose functions are everywhere empty may still act as constraints. Bounds on the requirements for a dependency to act as a constraint are derived.These results are founded on the notion of a weak instance for a database state, which replaces the universal relation instance assumption and is both intuitively and computationally more nearly acceptable. © 1983, ACM. All rights reserved.",functional dependencies; tableaux,DATABASE SYSTEMS
Bounded Index Exponential Hashing,1983,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0020720352&doi=10.1145%2f319830.319837&partnerID=40&md5=64bfa5043cb5084392ed12a696b5517b,"Bounded index exponential hashing, a new form of extendible hashing, is described. It has the important advantages over most of the other extendible hashing variants of both (i) providing random access to any record of a file in close to one disk access and (ii) having performance which does not vary with file size. It is straightforward to implement and demands only a fixed and specifiable amount of main storage to achieve this performance. Its underlying physical disk storage is readily managed and record overflow is handled so as to insure that unsuccessful searches never take more than two accesses. The method's ability to access data in close to a single disk access makes it possible to organize a database, in which files have a primary key and multiple secondary keys, such that the result is a significant performance advantage over existing organizations. © 1983, ACM. All rights reserved.",extendible hasing; tree index methods,DATA PROCESSING
A practical guide to the design of differential files for recovery of on-line databases,1982,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0020296797&doi=10.1145%2f319758.319762&partnerID=40&md5=23c6f3b5679f20da283b3e5e7df7218a,"The concept of a differential file has previously been proposed as an efficient means of collecting database updates for on-line systems. This paper studies the problem of database backup and recovery for such systems, and presents an analytic model of their operation. Five key design decisions are identified and an optimization procedure for each is developed. A design algorithm that quickly provides parameters for a near-optimal differential file architecture is provided. © 1982, ACM. All rights reserved.",backup and recovery; database maintenance; differential files; hashing functions; numerical methods; optimization; reorganization,DATABASE SYSTEMS; DATA PROCESSING
Tree queries: A simple class of relational queries,1982,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0020296796&doi=10.1145%2f319758.319775&partnerID=40&md5=123f11081ccc2c7ec2277e3dbc5885f4,"One can partition the class of relational database schemas into tree schemas and cyclic schemas. (These are called acyclic hypergraphs and cyclic hypergraphs elsewhere in the literature.) This partition has interesting implications in query processing, dependency theory, and graph theory.The tree/cyclic partitioning of database schemas originated with a similar partition of equijoin queries. Given an arbitrary equijoin query one can obtain an equivalent query that calculates the natural join of all relations in (an efficiently) derived database; such a query is called a natural join (NJ) query. If the derived database is a tree schema the original query is said to be a tree query, and otherwise a cyclic query.In this paper we analyze query processing consequences of the tree/cyclic partitioning. We are able to argue, qualitatively, that queries which imply a tree schema are easier to process than those implying a cyclic schema. Our results also extend the study of the semijoin operator. © 1982, ACM. All rights reserved.",acyclic schemes; cyclic schemas; join; semijoin; tree schemas,TREE QUERIES; DATABASE SYSTEMS
Operational Characteristics of a Harware-Based Pattern Matcher,1983,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0020722018&doi=10.1145%2f319830.319832&partnerID=40&md5=224a05598a8c7053ba470eda8cdb2d94,"The design and operation of a new class of hardware-based pattern matchers, such as would be used in a backended database processor in a full-text or other retrieval system, is presented. This recognizer is based on a unique implementation technique for finite state automata consisting of partitioning the state table among a number of simple digital machines. It avoids the problems generally associated with implementing finite state machines, such as large state table memories, complex control mechanisms, and state encodings. Because it consists primarily of memory, with its high regularity and density, needs only limited static interconnections, and operates at a relatively low speed, it can be easily constructed using integrated circuit techniques.After a brief discussion of other pattern-matching hardware, the structure and operation of the partitioned finite state automaton is given, along with a simplified discussion of how the state tables are partitioned. The expected performance of the resulting system and the state table partitioning programs is then discussed. © 1983, ACM. All rights reserved.",backend processors; computer system architecture; finite state automata; full text retrieval systems; text searching,DATABASE SYSTEMS; INFORMATION SCIENCE
Query optimization in star computer networks,1982,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0020296795&doi=10.1145%2f319758.319778&partnerID=40&md5=60195a978737315a8e3b0e704ce94ec5,"Query processing is investigated for relational databases distributed over several computers organized in a star network. Minimal response-time processing strategies are presented for queries involving the select, project, and join commands. These strategies depend on system parameters such as communication costs and different machine processing speeds; database parameters such as relation cardinality and file size; and query parameters such as estimates of the size and number of tuples in the result relation. The optimal strategies specify relation preparation processes, the shipping strategy, serial or parallel processing, and, where applicable, the site of join filtering and merging. Strategies for optimizing select and join queries have been implemented and tested. © 1982, ACM. All rights reserved.",query optimization; relational database system; star computer network,COMPUTER NETWORKS; QUERY OPTIMIZATION; DATABASE SYSTEMS
Using Semantic Knowledge for Transaction Processing in a Distributed Database,1983,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0020766588&doi=10.1145%2f319983.319985&partnerID=40&md5=e455ed7db4d06ff8bf595d1044eb75df,"This paper investigates how the semantic knowledge of an application can be used in a distributed database to process transactions efficiently and to avoid some of the delays associated with failures. The main idea is to allow nonserializable schedules which preserve consistency and which are acceptable to the system users. To produce such schedules, the transaction processing mechanism receives semantic information from the users in the form of transaction semantic types, a division of transactions into steps, compatibility sets, and countersteps. Using these notions, we propose a mechanism which allows users to exploit their semantic knowledge in an organized fashion. The strengths and weaknesses of this approach are discussed. © 1983, ACM. All rights reserved.",concurrency control; consistency; locking; schedule; semantic knowledge; serializability,DATABASE SYSTEMS
Formal Semantics for Time in Databases,1983,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0020766744&doi=10.1145%2f319983.319986&partnerID=40&md5=40fb0a873f3bce40c0bbc27f218cf897,"The concept of a historical database is introduced as a tool for modeling the dynamic nature of some part of the real world. Just as first-order logic has been shown to be a useful formalism for expressing and understanding the underlying semantics of the relational database model, intensional logic is presented as an analogous formalism for expressing and understanding the temporal semantics involved in a historical database. The various components of the relational model, as extended to include historical relations, are discussed in terms of the model theory for the logic ILs, a variation of the logic IL formulated by Richard Montague. The modal concepts of intensional and extensional data constraints and queries are introduced and contrasted. Finally, the potential application of these ideas to the problem of natural language database querying is discussed. © 1983, ACM. All rights reserved.",entity-relationship model; historical databases; intensional logic; relational database; temporal semantics,DATABASE SYSTEMS
Performance Enhancements to a Relational Database System,1983,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0020766187&doi=10.1145%2f319983.319984&partnerID=40&md5=6e03e30f1342d98070da6156a7e2f3aa,"In this paper we examine four performance enhancements to a database management system: dynamic compilation, microcoded routines, a special-purpose file system, and a special-purpose operating system. All were examined in the context of the INGRES database management system. Benchmark timings that are included suggest the attractiveness of dynamic compilation and a special-purpose file system. Microcode and a special-purpose operating system are analyzed and appear to be of more limited utility in the INGRES context. © 1983, ACM. All rights reserved.",compiled query languages; database performance; file systems for databases; microcode,DATABASE SYSTEMS
Compromising Statistical Databases Responding to Queries About Means,1983,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0020718732&doi=10.1145%2f319830.319834&partnerID=40&md5=36e6c90dd39358286013242c06d8c7ca,"This paper describes how to compromise a statistical database which only answers queries about arithmetic means for query sets whose cardinality falls in the range [k, N - k], for some k > 0, where N ≥ 2k is the number of records in the database. The compromise is shown to be easy and to require only a little preknowledge; knowing the cardinality of just one nonempty query set is usually sufficient.This means that not only count and sum queries, but also queries for arithmetic means can be extremely dangerous for the security of a statistical database, and that this threat must be taken into account explicitly by protective measures. This seems quite important from a practical standpoint: while arithmetic means were known for some time to be not altogether harmless, the (perhaps surprising) extent of the threat is now shown. © 1983, ACM. All rights reserved.",compromise; database security; statistical databases,DATABASE SYSTEMS
Three Principles of Representation for Semantic Networks,1982,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0020189481&doi=10.1145%2f319732.319743&partnerID=40&md5=ba69a1967ac3cd3fbd9f35f26894ac7f,"Semantic networks are so intuitive and easy to use that they are often employed without much thought as to the phenomenon of semantic nets themselves. Since they are becoming more and more a tool of artificial intelligence and now database technology, it is appropriate to focus on the principles of semantic nets. Such focus finds a harmonious and consistent base which can increase the semantic quality and usefulness of such nets. Three rules of representation are presented which achieve greater conceptual simplicity for users, simplifications in semantic net implementations and maintenance, and greater consistency across semantic net applications. These rules, applied to elements of the net itself, reveal how fundamental structures should be organized, and show that the common labeled-edge semantic net can be derived from a more primitive structure involving only nodes and membership relationships (and special nodes which represent names). Also, the correlation between binary and n-ary relations is presented. © 1982, ACM. All rights reserved.",,SEMANTIC NETWORKS; DATA BASE SYSTEMS
On Interpretations of Relational Languages and Solutions to the Implied Constraint Problem,1982,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0020142950&doi=10.1145%2f319702.319730&partnerID=40&md5=8535368bdb1090b2662f4fe81e5fe107,"The interconnection between conceptual and external levels of a relational database is made precise in terms of the notion of “interpretation” between first-order languages. This is then used to obtain a methodology for discovering constraints at the external level that are “implied” by constraints at the conceptual level and by conceptual-to-external mappings. It is also seen that these concepts are important in other database issues, namely, automatic program conversion, database design, and compile-time error checking of embedded database languages. Although this the deals exclusively with the relational approach, it also discusses how these ideas can be extended to hierarchical and network databases. © 1982, ACM. All rights reserved.",constraints; program conversion; relational database,DATA BASE SYSTEMS
Read-Only Transactions in a Distributed Database,1982,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0020145706&doi=10.1145%2f319702.319704&partnerID=40&md5=9685068a3cc91b454908b5abd393d491,"A read-only transaction or query is a transaction which does not modify any data. Read-only transactions could be processed with general transaction processing algorithms, but in many cases it is more efficient to process read-only transactions with special algorithms which take advantage of the knowledge that the transaction only reads. This paper defines the various consistency and currency requirements that read-only transactions may have. The processing of the different classes of read-only transactions in a distributed database is discussed. The concept of R insularity is introduced to characterize both the read-only and update algorithms. Several simple update and read-only transaction processing algorithms are presented to illustrate how the query requirements and the update algorithms affect the read-only transaction processing algorithms. © 1982, ACM. All rights reserved.",concurrency control; consistency; currency; query; R insularity; read-only transaction; schedule; serializability; transaction; transaction processing algorithm,DATA BASE SYSTEMS
A Relation-Based Language Interpreter for a Content Addressable File Store,1982,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0020148131&doi=10.1145%2f319702.319705&partnerID=40&md5=24002abd9387b7806681ae4a350ae281,"The combination of the Content Addressable File Store (CAFS®; CAFS is a registered trademark of International Computers Limited) and an extension of relational analysis is described. This combination allows a simple and compact implementation of a database query and update language (FIDL). The language has one of the important properties of a “natural” language interface by using a “world model” derived from the relational analysis. The interpreter (FLIN) takes full advantage of the CAFS by employing a unique database storage technique which results in a fast response to both queries and updates. © 1982, ACM. All rights reserved.",content addressing,COMPUTER OPERATING SYSTEMS - Program Interpreters; DATA BASE SYSTEMS
Decompiling CODASYL DML into Retional Queries,1982,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0020100802&doi=10.1145%2f319682.319688&partnerID=40&md5=1e81ecfa093b5687fca44fb85bb00cf5,"A “decompilation” algorithm is developed to transform a program written with the procedural operations of CODASYL DML into one which interacts with a relational system via a nonprocedural query specification. An Access Path Model is introduced to interpret the semantic accesses performed by the program. Data flow analysis is used to determine how FIND operations implement semantic accesses. A sequence of these is mapped into a relational query and embedded into the original program. The class of programs for which the algorithm succeeds is characterized. © 1982, ACM. All rights reserved.",decompilation; semantic data models,DATA BASE SYSTEMS
A unifying model of physical databases,1982,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0020265397&doi=10.1145%2f319758.319760&partnerID=40&md5=ea29b3e549eb13c006a14250ebabd159,"A unifying model for the study of database performance is proposed. Applications of the model are shown to relate and extend important work concerning batched searching, transposed files, index selection, dynamic hash-based files, generalized access path structures, differential files, network databases, and multifile query processing. © 1982, ACM. All rights reserved.",decomposition; linksets; simple files; unifying model,DATABASE SYSTEMS
A Simplied Universal Relation Assumption and Its Properties,1982,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0020186707&doi=10.1145%2f319732.319735&partnerID=40&md5=90447aa5caa02263da0a68be5da2443e,"One problem concerning the universal relation assumption is the inability of known methods to obtain a database scheme design in the general case, where the real-world constraints are given by a set of dependencies that includes embedded multivalued dependencies. We propose a simpler method of describing the real world, where constraints are given by functional dependencies and a single join dependency. The relationship between this method of defining the real world and the classical methods is exposed. We characterize in terms of hypergraphs those multivalued dependencies that are the consequence of a given join dependency. Also characterized in terms of hypergraphs are those join dependencies that are equivalent to a set of multivalued dependencies. © 1982, ACM. All rights reserved.",acyclic; database scheme; hypergraph; join dependency; multivalued dependency; relational database,RELATIONAL DATABASES; DATA BASE SYSTEMS
A Statistical Approach to Incomplete Information in Database Systems,1982,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0020186708&doi=10.1145%2f319732.319747&partnerID=40&md5=baec9398211f84baec1382ce11a52688,"There are numerous situations in which a database cannot provide a precise answer to some of the questions that are posed. Sources of imprecision vary and include examples such as recording errors, incompatible scaling, and obsolete data. In many such situations, considerable prior information concerning the imprecision exists and can be exploited to provide valuable information for queries to which no exact answer can be given. The objective of this paper is to provide a framework for doing so. © 1982, ACM. All rights reserved.",incomplete information; missing values; null values,DATA BASE SYSTEMS
Deadlock freedom using edge locks,1982,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0020250561&doi=10.1145%2f319758.319772&partnerID=40&md5=333af817ca7cdcd187691402c72634cc,"We define a series of locking protocols for database systems that all have three main features: freedom from deadlock, multiple granularity, and support for general collections of locking primitives. A rooted directed acyclic graph is used to represent multiple granularities, as in System R. Deadlock freedom is guaranteed by extending the System R protocol to require locks on edges of the graph in addition to the locks required on nodes. © 1982, ACM. All rights reserved.",concurrency control; locking; serializability,DATABASE SYSTEMS
Joined normal form: A storage encoding for relational databasess,1982,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0020273568&doi=10.1145%2f319758.319768&partnerID=40&md5=dd20caaeef05deb99def2f1dd8c111aa,"A new on-line query language and storage structure for a database machine is presented. By including a mathematical model in the interpreter the query language has been substantially simplified so that no reference to relation names is necessary. By storing the model as a single joined normal form (JNF) file, it has been possible to exploit the powerful search capability of the Content Addressable File Store (CAFS® CAFS is a registered trademark of International Computers Limited) database machine. © 1982, ACM. All rights reserved.",CAFS; content addressing hardware; functional dependencies; implication network; joins; mathematical model; network; queries; relational database; storage encoding tags; third normal form; updates,JOINED NORMAL FORM; STORAGE ENCODING; DATABASE SYSTEMS
Determining View Dependencies Using Tableaux,1982,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0020182799&doi=10.1145%2f319732.319738&partnerID=40&md5=0ca0ec99d3e476c2b766755310cb9211,"A relational database models some part of the real world by a set of relations and a set of constraints. The constraints model properties of the stored information and must be maintained true at all times. For views defined over physically stored (base) relations, this is done by determining whether the view constraints are logical consequences of base relation constraints. A technique for determining such valid view constraints is presented in this paper. A generalization of the tableau chase is used. The idea of the method is to generate a tableau for the expression whose summary violates the test constraints in a “canonical” way. The chase then tries to remove this violation.It is also shown how this method has applications to schema design. Relations not in normal form or having other deficiencies can be replaced by normal form projections without losing the ability to represent all constraint information. © 1982, ACM. All rights reserved.",chase; dependencies; rational algebra; relational model; tableaux,RELATIONAL DATABASES; DATA BASE SYSTEMS
Disk Allocation for Cartesian Product Files on Multiple-Disk Systems,1982,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0020098779&doi=10.1145%2f319682.319698&partnerID=40&md5=62347135dd48e903be11824393360238,"Cartesian product files have recently been shown to exhibit attractive properties for partial match queries. This paper considers the file allocation problem for Cartesian product files, which can be stated as follows: Given a k-attribute Cartesian product file and an m-disk system, allocate buckets among the m disks in such a way that, for all possible partial match queries, the concurrency of disk accesses is maximized. The Disk Modulo (DM) allocation method is described first, and it is shown to be strict optimal under many conditions commonly occurring in practice, including all possible partial match queries when the number of disks is 2 or 3. It is also shown that although it has good performance, the DM allocation method is not strict optimal for all possible partial match queries when the number of disks is greater than 3. The General Disk Modulo (GDM) allocation method is then described, and a sufficient but not necessary condition for strict optimality of the GDM method for all partial match queries and any number of disks is then derived. Simulation studies comparing the DM and random allocation methods in terms of the average number of disk accesses, in response to various classes of partial match queries, show the former to be significantly more effective even when the number of disks is greater than 3, that is, even in cases where the DM method is not strict optimal. The results that have been derived formally and shown by simulation can be used for more effective design of optimal file systems for partial match queries. When considering multiple-disk systems with independent access paths, it is important to ensure that similar records are clustered into the same or similar buckets, while similar buckets should be dispersed uniformly among the disks. © 1982, ACM. All rights reserved.",Cartesian product files,DATA PROCESSING
An Implementation Technique for Database Query Languages,1982,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0020148286&doi=10.1145%2f319702.319711&partnerID=40&md5=1bbdaa0286285336f6c3c0121fcfce2f,"Structured query languages, such as those available for relational databases, are becoming increasingly desirable for all database management systems. Such languages are applicative: there is no need for an assignment or update statement. A new technique is described that allows for the implementation of applicative query languages against most commonly used database systems. The technique involves “lazy” evaluation and has a number of advantages over existing methods: it allows queries and functions of arbitrary complexity to be constructed; it reduces the use of secondary storage; it provides a simple control structure through which interfaces to other programs may be constructed; and the implementation, including the database interface, is quite compact. Although the technique is presented for a specific functional programming system and for a CODASYL DBMS, it is general and may be used for other query languages and database systems. © 1982, ACM. All rights reserved.",applicative programing; coroutines; database interfaces; lazy evaluation,QUERY LANGUAGES; DATA BASE SYSTEMS
Transactions and Consistency in Distributed Database Systems,1982,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0020186296&doi=10.1145%2f319732.319734&partnerID=40&md5=5196f1a735d0a1e73ba97521e38cb33a,"The concepts of transaction and of data consistency are defined for a distributed system. The cases of partitioned data, where fragments of a file are stored at multiple nodes, and replicated data, where a file is replicated at several nodes, are discussed. It is argued that the distribution and replication of data should be transparent to the programs which use the data. That is, the programming interface should provide location transparency, replica transparency, concurrency transparency, and failure transparency. Techniques for providing such transparencies are abstracted and discussed.By extending the notions of system schedule and system clock to handle multiple nodes, it is shown that a distributed system can be modeled as a single sequential execution sequence. This model is then used to discuss simple techniques for implementing the various forms of transparency. © 1982, ACM. All rights reserved.",concurrency control; data partitioning; data replication; recovery,DISTRIBUTED DATABASES; DATA BASE SYSTEMS
View Indexing in Relational Databases,1982,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0020143328&doi=10.1145%2f319702.319729&partnerID=40&md5=efed531a074d69a1d979a20fe983e8e9,"The design and maintenance of a useful database system require efficient optimization of the logical access paths which demonstrate repetitive usage patterns. Views (classes of queries given by a query model) are an appropriate intermediate logical representation for database. Frequently accessed views of databases need to be supported by indexing to enhance retrieval. This paper investigates the problem of selecting an optimal index set of views and describes an efficient algorithm for this selection. © 1982, ACM. All rights reserved.",index selection,DATA BASE SYSTEMS
A Formal Approach to the Definition and the Design of Conceptual Schemata for Databased Systems,1982,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0020101802&doi=10.1145%2f319682.319695&partnerID=40&md5=0bf6ff431bf166cd0a7e19486b88c13d,"A formal approach is proposed to the definition and the design of conceptual database diagrams to be used as conceptual schemata in a system featuring a multilevel schema architecture, and as an aid for the design of other forms of schemata. We consider E-R (entity-relationship) diagrams, and we introduce a new representation called CAZ-graphs. A rigorous connection is established between these diagrams and some formal constraints used to describe relationships in the framework of the relational data model. These include functional and multivalued dependencies of database relations. The basis for our schemata is a combined representation for two fundamental structures underlying every relation: the first defined by its minimal atomic decompositions, the second by its elementary functional dependencies.The interaction between these two structures is explored, and we show that, jointly, they can represent a wide spectrum of database relationships, of which the well-known one-to-one, one-to-many, and many-to-many associations constitute only a small subset. It is suggested that a main objective in conceptual schema design is to ensure a complete representation of these two structures. A procedure is presented to design schemata which obtain this objective while eliminating redundancy. A simple correspondence between the topological properties of these schemata and the structure of multivalued dependencies of the original relation is established. Various applications are discussed and a number of illustrative examples are given. © 1982, ACM. All rights reserved.",,DATA BASE SYSTEMS
Optimal File Designs and Reorganization Points,1982,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0020100713&doi=10.1145%2f319682.319696&partnerID=40&md5=b296d5e4667df4599659e15152ca1b0a,"A model for studying the combined problems of file design and file reorganization is presented. New modeling techniques for predicting the performance evolution of files and for finding optimal reorganization points for files are introduced. Applications of the model to hash-based and indexed-sequential files reveal important relationships between initial loading factors and reorganization frequency. A practical file design strategy, based on these relationships, is proposed. © 1982, ACM. All rights reserved.",file design; file reorganization,DATA PROCESSING
On Database Systems Development Through Logic,1982,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0020100609&doi=10.1145%2f319682.319700&partnerID=40&md5=b66b237feb3efac346abd3461fc6363a,"The use of logic as a single tool for formalizing and implementing different aspects of database systems in a uniform manner is discussed. The discussion focuses on relational databases with deductive capabilities and very high-level querying and defining features. The computational interpretation of logic is briefly reviewed, and then several pros and cons concerning the description of data, programs, queries, and language parser in terms of logic programs are examined. The inadequacies are discussed, and it is shown that they can be overcome by the introduction of convenient extensions into logic programming. Finally, an experimental database query system with a natural language front end, implemented in PROLOG, is presented as an illustration of these concepts. A description of the latter from the user's point of view and a sample consultation session in Spanish are included. © 1982, ACM. All rights reserved.",rational database,DATA BASE SYSTEMS
Distributed Deadlock Detection Algorithm,1982,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0020147771&doi=10.1145%2f319702.319717&partnerID=40&md5=2a043c6a23c121f59b908794fb091251,"We propose an algorithm for detecting deadlocks among transactions running concurrently in a distributed processing network (i.e., a distributed database system). The proposed algorithm is a distributed deadlock detection algorithm. A proof of the correctness of the distributed portion of the algorithm is given, followed by an example of the algorithm in operation. The performance characteristics of the algorithm are also presented. © 1982, ACM. All rights reserved.",,DATA BASE SYSTEMS; DEADLOCK DETECTION; COMPUTER PROGRAMMING
An Architecture for Automatic Relational Database Sytem Conversion,1982,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0020148116&doi=10.1145%2f319702.319724&partnerID=40&md5=2581cf0ee69be67f56c30c16b0e90a28,"Changes in requirements for database systems necessitate schema restructuring, database translation, and application or query program conversion. An alternative to the lengthy manual revision process is proposed by offering a set of 15 transformations keyed to the relational model of data and the relational algebra. Motivations, examples, and detailed descriptions are provided. © 1982, ACM. All rights reserved.",automatic conversion; database systems; relational model; transformations,DATA BASE SYSTEMS
Performance analysis of linear hashing with partial expansions,1982,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0020272149&doi=10.1145%2f319758.319763&partnerID=40&md5=e1f42c0764a493562446b7f3c8f10ac3,"Linear hashing with partial expansions is a new file organization primarily intended for files which grow and shrink dynamically. This paper presents a mathematical analysis of the expected performance of the new scheme. The following measures are considered: length of successful and unsuccessful searches, accesses required to insert or delete a record, and the size of the overflow area. The performance is cyclical. For all performance measures, the necessary formulas are derived for computing the expected performance at any point of a cycle and the average over a cycle. Furthermore, the expected worst case in connection with searching is analyzed. The overall performance depends on several file parameters. The numerical results show that for many realistic parameter combinations the performance is expected to be extremely good. Even the longest search is expected to be of quite reasonable length. © 1982, ACM. All rights reserved.",dynamic hashing schemes; extendible hashing; hashing; linear hashing,LINEAR HASHING; DATA PROCESSING
Mathematical Models of Database Degradation,1982,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0020250018&doi=10.1145%2f319758.319771&partnerID=40&md5=175ec52f8730ccb0a66cec57232d7539,"As data are updated, the initial physical structure of a database is changed and retrieval of specific pieces of data becomes more time consuming. This phenomenon is called database degradation. In this paper two models of database degradation are described. Each model refers to a different aspect of the problem.It is assumed that transactions are statistically independent and either add, delete, or update data. The first model examines the time during which a block of data is filling up. The second model examines the overflows from a block of data, which essentially describes the buildup of disorganization. Analytical results are obtained for both models. In addition, several numerical examples are presented which show that the mean number of overflows grows approximately linearly with time. This approximation is used to devise a simple formula for the optimal time to reorganize a stochastically growing database. © 1982, ACM. All rights reserved.",data overflows; file organization,MATHEMATICAL MODELS; DATABASE DEGRADATION; DATABASE SYSTEMS
A Clustered Search Algorithm Incorporating Arbitrary Term Dependencies,1982,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0020185998&doi=10.1145%2f319732.319756&partnerID=40&md5=0af1a9a2a66cfccccceb4a6488049202,"The documents in a database are organized into clusters, where each cluster contains similar documents and a representative of these documents. A user query is compared with all the representatives of the clusters, and on the basis of such comparisons, those clusters having many close neighbors with respect to the query are selected for searching. This paper presents an estimation of the number of close neighbors in a cluster in relation to the given query. The estimation takes into consideration the dependencies between terms. It is demonstrated by experiments that the estimate is accurate and the time to generate the estimate is small. © 1982, ACM. All rights reserved.",Bahadur-Lazarsfeld expansion; clustered search; generating polynomial; term dependencies,COMPUTER PROGRAMMING - SUBROUTINES; CLUSTERED SEARCH ALGORITHM; DATA BASE SYSTEMS
On optimizing an SQL-like Nested Query,1982,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0020182801&doi=10.1145%2f319732.319745&partnerID=40&md5=39f50974e17144bc6e8fd88b865d3847,"SQL is a high-level nonprocedural data language which has received wide recognition in relational databases. One of the most interesting features of SQL is the nesting of query blocks to an arbitrary depth. An SQL-like query nested to an arbitrary depth is shown to be composed of five basic types of nesting. Four of them have not been well understood and more work needs to be done to improve their execution efficiency. Algorithms are developed that transform queries involving these basic types of nesting into semantically equivalent queries that are amenable to efficient processing by existing query-processing subsystems. These algorithms are then combined into a coherent strategy for processing a general nested query of arbitrary complexity. © 1982, ACM. All rights reserved.",aggregate function; divide; join; nested query; predicate; relational database,COMPUTER PROGRAMMING - SUBROUTINES; QUERY OPTIMIZATION; RELATIONAL DATABASES; DATA BASE SYSTEMS
A New Normal Form for the Design of Relational Database Schemata,1982,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0020185874&doi=10.1145%2f319732.319749&partnerID=40&md5=014bbaa1455993918520b3d0d29dabf9,"This paper addresses the problem of database schema design in the framework of the relational data model and functional dependencies. It suggests that both Third Normal Form (3NF) and Boyce-Codd Normal Form (BCNF) supply an inadequate basis for relational schema design. The main problem with 3NF is that it is too forgiving and does not enforce the separation principle as strictly as it should. On the other hand, BCNF is incompatible with the principle of representation and prone to computational complexity. Thus a new normal form, which lies between these two and captures the salient qualities of both is proposed. The new normal form is stricter than 3NF, but it is still compatible with the representation principle. First a simpler definition of 3NF is derived, and the analogy of this new definition to the definition of BCNF is noted. This analogy is used to derive the new normal form. Finally, it is proved that Bernstein's algorithm for schema design synthesizes schemata that are already in the new normal form. © 1982, ACM. All rights reserved.",database schema; functional dependencies; relational model,DATABASE SCHEMA; RELATIONAL DATABASES; DATA BASE SYSTEMS
On the Correct Translation of Update Operations on Relational Views,1982,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0020183057&doi=10.1145%2f319732.319740&partnerID=40&md5=30026d020f18e06da891e2bae94aaa07,"Most relational database systems provide a facility for supporting user views. Permitting this level of abstraction has the danger, however, that update requests issued by a user within the context of his view may not translate correctly into equivalent updates on the underlying database. The purpose of this paper is to formalize the notion of update translation and derive conditions under which translation procedures will produce correct translations of view updates. © 1982, ACM. All rights reserved.",external schemata; relational databases; schema mapping; update translation; user views,RELATIONAL DATABASES; DATA BASE SYSTEMS
New File Organization Based on Dynamic Hashing,1981,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0019543410&doi=10.1145%2f319540.319564&partnerID=40&md5=75339dd7e694ba6fbff4065187b49fc2,"New file organizations based on hashing and suitable for data whose volume may vary rapidly recently appeared in the literature. In the three schemes which have been independently proposed, rehashing is avoided, storage space is dynamically adjusted to the number of records actually stored, and there are no overflow records. Two of these techniques employ an index to the data file. Retrieval is fast and storage utilization is low.In order to increase storage utilization, we introduce two schemes based on a similar idea and analyze the performance of the second scheme. Both techniques use an index of much smaller size. In both schemes, overflow records are accepted. The price which has to be paid for the improvement in storage utilization is a slight access cost degradation. © 1981, ACM. All rights reserved.",data structure; dynamic hashing; file organization; hashing; linear splitting,DATA PROCESSING
Security of Statistical Databases: Multidimensional Transformation,1981,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0019546487&doi=10.1145%2f319540.319555&partnerID=40&md5=de03d501f0d123bf4e2a456cc937b2c9,"The concept of multidimensional transformation of statistical databases is described. A given set of statistical output may be compatible with more than one statistical database. A transformed database D’ is a database which (1) differs from the original database D in its record content, but (2) produces, within certain limits, the same statistical output as the original database. For a transformable database D there are two options: One may physically transform D into a suitable database D’, or one may release only that output which will not permit the users to decide whether it comes from D or D’. The second way is, of course, the easier one. Basic structural requirements for transformable statistical databases are investigated. The closing section discusses advantages, drawbacks, and open questions. © 1981, ACM. All rights reserved.",confidentiality; database; database security; matrices; security; statistical database,DATA PROCESSING - Security of Data; DATA BASE SYSTEMS
The functional data model and the data languages DAPLEX,1981,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0019541764&doi=10.1145%2f319540.319561&partnerID=40&md5=4d5d80e2aaf785f823c4779825cad434,"DAPLEX is a database language which incorporates: a formulation of data in terms of entities; a functional representation for both actual and virtual data relationships; a rich collection of language constructs for expressing entity selection criteria; a notion of subtype/supertype relationships among entity types. This paper presents and motivates the DAPLEX language and the underlying data model on which it is based. © 1981, ACM. All rights reserved.",database; functional data model; language,DATA BASE SYSTEMS; COMPUTER PROGRAMMING LANGUAGES
Support for Repetitive Transactions and Ad Hoc Queries in System R,1981,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0019541763&doi=10.1145%2f319540.319550&partnerID=40&md5=71e0c9b24ec1bac6b1aee43f0ac04c3c,"System R supports a high-level relational user language called SQL which may be used by ad hoc users at terminals or as an embedded data sublanguage in PL/I or COBOL. Host-language programs with embedded SQL statements are processed by the System R precompiler which replaces the SQL statements by calls to a machine-language access module. The precompilation approach removes much of the work of parsing, name binding, and access path selection from the path of a running program, enabling highly efficient support for repetitive transactions. Ad hoc queries are processed by a similar approach of name binding and access path selection which takes place on-line when the query is specified. By providing a flexible spectrum of binding times, System R permits transaction-oriented programs and ad hoc query users to share a database without loss of efficiency.System R is an experimental database management system designed and built by members of the IBM San Jose Research Laboratory as part of a research program on the relational model of data. This paper describes the architecture of System R, and gives some preliminary measurements of system performance in both the ad hoc query and the “canned program” environments. © 1981, ACM. All rights reserved.",compilation; performance measurements; query languages; relational database systems; transaction processing,DATA BASE SYSTEMS
Update Semantics of Relational Views,1981,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0019666494&doi=10.1145%2f319628.319634&partnerID=40&md5=204e9c33347912c38a8826f5c919487e,"A database view is a portion of the data structured in a way suitable to a specific application. Updates on views must be translated into updates on the underlying database. This paper studies the translation process in the relational model.The procedure is as follows: first, a “complete” set of updates is defined such that together with every update the set contains a “return” update, that is, one that brings the view back to the original state; given two updates in the set, their composition is also in the set. To translate a complete set, we define a mapping called a “translator,” that associates with each view update a unique database update called a “translation.” The constraint on a translation is to take the database to a state mapping onto the updated view. The constraint on the translator is to be a morphism.We propose a method for defining translators. Together with the user-defined view, we define a “complementary” view such that the database could be computed from the view and its complement. We show that a view can have many different complements and that the choice of a complement determines an update policy. Thus, we fix a view complement and we define the translation of a given view update in such a way that the complement remains invariant (“translation under constant complement”). The main result of the paper states that, given a complete set U of view updates, U has a translator if and only if U is translatable under constant complement. © 1981, ACM. All rights reserved.",conceptual model; data model; data semantics; database view; relation; relational model database; update translation; view updating,DATA BASE SYSTEMS
Efficient Locking for Concurrent Operations on B-trees,1981,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0019666493&doi=10.1145%2f319628.319663&partnerID=40&md5=3b100ed561201ea656c33ea342914759,"The B-tree and its variants have been found to be highly useful (both theoretically and in practice) for storing large amounts of information, especially on secondary storage devices. We examine the problem of overcoming the inherent difficulty of concurrent operations on such structures, using a practical storage model. A single additional “link” pointer in each node allows a process to easily recover from tree modifications performed by other concurrent processes. Our solution compares favorably with earlier solutions in that the locking scheme is simpler (no read-locks are used) and only a (small) constant number of nodes are locked by any update process at any given time. An informal correctness proof for our system is given. © 1981, ACM. All rights reserved.",B-tree; concurrenct algorithms; concurrency controls; consistencey; correctness; data structures; database; index organizations; locking protocols; multiway search trees,DATA PROCESSING
Hierarchical Schemata for Relational Databases,1981,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0019544003&doi=10.1145%2f319540.319546&partnerID=40&md5=16ef9bd8384dc8d1a342648321de4d55,"Most database design methods for the relational model produce a flat database, that is, a family of relations with no explicit interrelational connections. The user of a flat database is likely to be unaware of certain interrelational semantics. In contrast, the entity-relationship model provides schema graphs as a description of the database, as well as for navigating the database. Nevertheless, the user of an entity-relationship database may still commit semantic errors, such as performing a lossy join. This paper proposes a nonflat, or hierarchical, view of relational databases. Relations are grouped together to form relation hierarchies in which lossless joins are explicitly shown whereas lossy joins are excluded. Relation hierarchies resemble the schema graphs in the entity-relationship model.An approach to the design of relation hierarchies is outlined in the context of data dependencies and relational decomposition. The approach consists of two steps; each is described as an algorithm. Algorithm DEC decomposes a given universal relation according to a given set of data dependencies and produces a set of nondecomposable relation schemes. This algorithm differs from its predecessors in that it produces no redundant relation schemes. Algorithm RH further structures the relation schemes produced by Algorithm DEC into a hierarchical schema. These algorithms can be useful software tools for database designers. © 1981, ACM. All rights reserved.",database design; lossless join; multivalued dependency; relation normalization,DATA BASE SYSTEMS
Analysis of a Heuristic for Full Trie Minimization,1981,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0019611334&doi=10.1145%2f319587.319618&partnerID=40&md5=449a8b3a063a62cea68fc6dec13e3fd6,"A trie is a distributed-key search tree in which records from a file correspond to leaves in the tree. Retrieval consists of following a path from one root to a leaf, where the choice of edge at each node is determined by attribute values of the key. For full tries, those in which all leaves lie at the same depth, the problem of finding an ordering of attributes which yields a minimum size trie is NP-complete.This paper considers a “greedy” heuristic for constructing low-cost tries. It presents simulation experiments which show that the greedy method tends to produce tries with small size, and analysis leading to a worst case bound on approximations produced by the heuristic. It also shows a class of files for which the greedy method may perform badly, producing tries of high cost. © 1981, ACM. All rights reserved.",heuristic; trie index; trie size,DATA PROCESSING
Query processing in a system for distributed databases (SDD-1),1981,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0019703516&doi=10.1145%2f319628.319650&partnerID=40&md5=1b9d6762f3ff69bb0583e8add7846b37,"This paper describes the techniques used to optimize relational queries in the SDD-1 distributed database system. Queries are submitted to SDD-1 in a high-level procedural language called Datalanguage. Optimization begins by translating each Datalanguage query into a relational calculus form called an envelope, which is essentially an aggregate-free QUEL query. This paper is primarily concerned with the optimization of envelopes.Envelopes are processed in two phases. The first phase executes relational operations at various sites of the distributed database in order to delimit a subset of the database that contains all data relevant to the envelope. This subset is called a reduction of the database. The second phase transmits the reduction to one designated site, and the query is executed locally at that site.The critical optimization problem is to perform the reduction phase efficiently. Success depends on designing a good repertoire of operators to use during this phase, and an effective algorithm for deciding which of these operators to use in processing a given envelope against a given database. The principal reduction operator that we employ is called a semijoin. In this paper we define the semijoin operator, explain why semijoin is an effective reduction operator, and present an algorithm that constructs a cost-effective program of semijoins, given an envelope and a database. © 1981, ACM. All rights reserved.",distributed databases; query optimization; query processing; relational databases; semijoins,COMPUTER PROGRAMMING - Subroutines; QUERY PROCESSING; DATA BASE SYSTEMS
Processor Allocation Strategies for Multiprocessor Database Machines,1981,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0019574780&doi=10.1145%2f319566.319570&partnerID=40&md5=7762b2944c6897c24bde73f51a77bc0f,"In this paper four alternative strategies for assigning processors to queries in multiprocessor database machines are described and evaluated. The results demonstrate that SIMD database machines are indeed a poor design when their performance is compared with that of the three MIMD strategies presented.Also introduced is the application of data-flow machine techniques to the processing of relational algebra queries. A strategy that employs data-flow techniques is shown to be superior to the other strategies described by several experiments. Furthermore, if the data-flow query processing strategy is employed, the results indicate that a two-level storage hierarchy (in which relations are paged between a shared data cache and mass storage) does not have a significant impact on performance. © 1981, ACM. All rights reserved.",associative processors; back-end computers; computer architective; data-flow computers; database machines; database management; parallel processors; processor scheduling,DATA BASE SYSTEMS
An Improved Third Normal Form for Relational Databases,1981,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0019576575&doi=10.1145%2f319566.319583&partnerID=40&md5=2e868b8f3f049552a4a1e4491f61981b,"In this paper, we show that some Codd third normal form relations may contain “superfluous” attributes because the definitions of transitive dependency and prime attribute are inadequate when applied to sets of relations. To correct this, an improved third normal form is defined and an algorithm is given to construct a set of relations from a given set of functional dependencies in such a way that the superfluous attributes are guaranteed to be removed. This new normal form is compared with other existing definitions of third normal form, and the deletion normalization method proposed is shown to subsume the decomposition method of normalization. © 1981, ACM. All rights reserved.",covering; database design; functional dependency; normalization; prime attribute; reconstructibility; relational schema; third normal form; transitive dependency,DATA BASE SYSTEMS
On Optimistic Methods for Concurrency Control,1981,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0019574432&doi=10.1145%2f319566.319567&partnerID=40&md5=1d3d91bf5fb3fca9be70384cd5829e1f,"Most current approaches to concurrency control in database systems rely on locking of data objects as a control mechanism. In this paper, two families of nonlocking concurrency controls are presented. The methods used are “optimistic” in the sense that they rely mainly on transaction backup as a control mechanism, “hoping” that conflicts between transactions will not occur. Applications for which these methods should be more efficient than locking are discussed. © 1981, ACM. All rights reserved.",concurrency controls; databases; transaction processing,DATA BASE SYSTEMS
Time- and Space-Optimality in B-Trees,1981,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0019541551&doi=10.1145%2f319540.319565&partnerID=40&md5=6f6edec67a0d55fbbecb1e8b71eba0f1,"A B-tree is compact if it is minimal in number of nodes, hence has optimal space utilization, among equally capacious B-trees of the same order. The space utilization of compact B-trees is analyzed and compared with that of noncompact B-trees and with (node)-visit-optimal B-trees, which minimize the expected number of nodes visited per key access. Compact B-trees can be as much as a factor of 2.5 more space efficient than visit-optimal B-trees; and the node-visit cost of a compact tree is never more than 1 + the node-visit cost of an optimal tree. The utility of initializing a B-tree to be compact (which initialization can be done in time linear in the number of keys if the keys are presorted) is demonstrated by comparing the space utilization of a compact tree that has been augmented by random insertions with that of a tree that has been grown entirely by random insertions. Even after increasing the number of keys by a modest amount, the effects of compact initialization are still felt. Once the tree has grown so large that these effects are no longer discernible, the tree can be expeditiously compacted in place using an algorithm presented here; and the benefits of compactness resume. © 1981, ACM. All rights reserved.","2,3-tree; B-tree; bushy B-tree; compact B-tree; node-visit cost; space utilization",DATA PROCESSING
Statistical Database Design,1981,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0019548231&doi=10.1145%2f319540.319558&partnerID=40&md5=748f4e96324f36996f8a3ba1240fb492,"The security problem of a statistical database is to limit the use of the database so that no sequence of statistical queries is sufficient to deduce confidential or private information. In this paper it is suggested that the problem be investigated at the conceptual data model level. The design of a statistical database should utilize a statistical security management facility to enforce the security constraints at the conceptual model level. Information revealed to users is well defined in the sense that it can at most be reduced to nondecomposable information involving a group of individuals. In addition, the design also takes into consideration means of storing the query information for auditing purposes, changes in the database, users’ knowledge, and some security measures. © 1981, ACM. All rights reserved.",compromisability; conceptual databases model; database design; protection; security; statistical database,DATA PROCESSING - Security of Data; DATA BASE SYSTEMS
Database Description with SDM: A Semantic Database Model,1981,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0019612794&doi=10.1145%2f319587.319588&partnerID=40&md5=d4e0f4e472f178fff5294f6a1df641c3,"SDM is a high-level semantics-based database description and structuring formalism (database model) for databases. This database model is designed to capture more of the meaning of an application environment than is possible with contemporary database models. An SDM specification describes a database in terms of the kinds of entities that exist in the application environment, the classifications and groupings of those entities, and the structural interconnections among them. SDM provides a collection of high-level modeling primitives to capture the semantics of an application environment. By accommodating derived information in a database structural specification, SDM allows the same information to be viewed in several ways; this makes it possible to directly accommodate the variety of needs and processing requirements typically present in database applications. The design of the present SDM is based on our experience in using a preliminary version of it.SDM is designed to enhance the effectiveness and usability of database systems. An SDM database description can serve as a formal specification and documentation tool for a database; it can provide a basis for supporting a variety of powerful user interface facilities, it can serve as a conceptual database model in the database design process; and, it can be used as the database model for a new kind of database management system. © 1981, ACM. All rights reserved.",database definition; database management; database modeling; database models; database semantics; logical database design,DATA BASE SYSTEMS
Frame Memory: A Storage Architecture to Support Rapid Design and Implementation of Efficient Databases,1981,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0019614335&doi=10.1145%2f319587.319598&partnerID=40&md5=4e7fed023f0fc8d0a73b55e7154303c2,"Frame memory is a virtual view of secondary storage that can be implemented with reasonable overhead to support database record storage and accessing requirements. Frame memory is designed so that its operating characteristics can be easily manipulated by either designers or design algorithms, while performance effects of such changes can be accurately predicted. Automated design procedures exist to generate and evaluate alternative database designs built upon frame memory, and the existence of these procedures establishes frames as an attractive memory management architecture for future database management systems. © 1981, ACM. All rights reserved.",analytic modeling; database design system; virtual secondary storge,DATA BASE SYSTEMS
Human Factors Comparison of a Procedural and a Nonprocedural Query Language,1981,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0019669154&doi=10.1145%2f319628.319656&partnerID=40&md5=b7e230b0e5324b9f7d90e20a28ca09b3,"Two experiments testing the ability of subjects to write queries in two different query languages were run. The two languages, SQL and TABLET, differ primarily in their procedurality; both languages use the relational data model, and their Halstead levels are similar. Constructs in the languages which do not affect their procedurality are identical. The two languages were learned by the experimental subjects almost exclusively from manuals presenting the same examples and problems ordered identically for both languages. The results of the experiments show that subjects using the more procedural language wrote difficult queries better than subjects using the less procedural language. The results of the experiments are also used to compare corresponding constructs in the two languages and to recommend improvements for these constructs. © 1981, ACM. All rights reserved.",database systems; human factors; procedural and nonprocedural languages; query languages,QUERY LANGUAGES; DATA BASE SYSTEMS
Design of an External Schema Facility to Define and Process Recursive Structures,1981,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0019576917&doi=10.1145%2f319566.319576&partnerID=40&md5=40a5fdec361136eee42051841a14d5b5,"The role of the external schema is to support user views of data and thus to provide programmers with easier data access. This author believes that an external schema facility is best based on hierarchies, both simple and recursive. After a brief introduction to an external schema facility to support simple hierarchical user views, the requirements for a facility for recursive hierarchies are listed and the necessary extensions to the external schema definition language are offered.Functions that must be provided for generality in definition are node specification and node control. Tree traversal functions must be provided for processing. Definitions of each and examples of use are presented. © 1981, ACM. All rights reserved.",ANSI SPARC architectures; external schemata; recursive data structures; user views,DATA BASE SYSTEMS
Transformation of Data Traversals and Operations in Application Programs to Account for Semantic Changes of Databases,1981,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0019574619&doi=10.1145%2f319566.319573&partnerID=40&md5=8c20851fa26b72372501b409f84f6db0,"This paper addresses the problem of application program conversion to account for changes in database semantics that result in changes in the schema and database contents. With the observation that the existing data models can be viewed as alternative ways of modeling the same database semantics, a methodology of application program analysis and conversion based on an existing-DBMS-model-and schema-independent representation of both the database and programs is presented. In this methodology, the source and target databases are described in terms of the association types of a semantic association model. The structural properties, the integrity constraints, and the operational characteristics (storage operation behaviors) of the association types are more explicitly defined to reveal the semantics that is generally hidden in application programs. The explicit descriptions of the source and target databases are used as the basis for program analysis and conversion. Application programs are described in terms of a small number of “access patterns” which define the data traversals and operations of the programs. In addition to the methodology, this paper (1) describes a model of a generalized application program conversion system that serves as a framework for research, (2) presents an analysis of access patterns that serve as the primitives for program description, (3) delineates some meaningful semantic changes to databases and their corresponding transformation rules for program conversion, (4) illustrates the application of these rules to two different approaches to program conversion problems, and (5) reports on the development effort undertaken at the University of Florida. © 1981, ACM. All rights reserved.",access pattern; application program conversion; database changes; semantic data model; transformation rules,DATA BASE SYSTEMS
Associative Hardware and Software Techniques for Integrity Control,1981,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0019608598&doi=10.1145%2f319587.319594&partnerID=40&md5=ca97b4d0841d23b602b128e06b99e9dd,"This paper presents the integrity control mechanism of the associative processing system, CASSM. The mechanism takes advantage of the associative techniques, such as content and context addressing, tagging and marking data, parallel processing, automatic triggering of integrity control procedures, etc., for integrity control and as a result offers three significant advantages: (1) The problem of staging data in a main memory for integrity checking can be eliminated because database storage operations are verified at the place where the data are stored. (2) The backout or merging procedures are relatively easy and inexpensive in the associative system because modified copies can be substituted for the originals or may be discarded by merely changing their associated tags. (3) The database management system software is simplified because database integrity functions are handled by the associative processing system to which a mainframe computer is a front-end computer. © 1981, ACM. All rights reserved.",assertion and trigger; associative techniques; cellular-logic devices; database integrity; database management; integrity control,INTEGRITY CONTROL; DATA BASE SYSTEMS
Comments on SDD-1 concurrency control mechanisms,1981,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976720452&doi=10.1145%2f319566.319585&partnerID=40&md5=58ccf2895865834e400aa571d0820b2e,[No abstract available],,
A Database Encryption System with Subkeys,1981,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0019573099&doi=10.1145%2f319566.319580&partnerID=40&md5=54f04d3706605b548170ce7c3ed3e4eb,"A new cryptosystem that is suitable for database encryption is presented. The system has the important property of having subkeys that allow the encryption and decryption of fields within a record. The system is based on the Chinese Remainder Theorem. © 1981, ACM. All rights reserved.",data security; databases; decryption; encryption; subkeys,"CODES, SYMBOLIC; DATA BASE SYSTEMS"
High-Level Programming Features for Improving the Efficiency of a Relational Database System,1981,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0019613789&doi=10.1145%2f319587.319604&partnerID=40&md5=3195c0154239a73920b471a29687cb21,"This paper discusses some high-level language programming constructs that can be used to manipulate the relations of a relational database system efficiently. Three different constructs are described: (1) tuple identifiers that directly reference tuples of a relation; (2) cursors that may iterate over the tuples of a relation; and (3) markings, a form of temporary relation consisting of a set of tuple identifiers. In each case, attention is given to syntactic, semantic, and implementation considerations.The use of these features is first presented within the context of the programming language PLAIN, and it is then shown how these features could be used more generally to provide database manipulation capabilities in a high-level programming language. Consideration is also given to issues of programming methodology, with an important goal being the achievement of a balance between the enforcement of good programming practices and the ability to write efficient programs. © 1981, ACM. All rights reserved.",markings; PLAIN; programming languages; programming methodology; relational algebra; relational database management,COMPUTER PROGRAMMING; DATA BASE SYSTEMS
Dense Multiway Trees,1981,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0019608763&doi=10.1145%2f319587.319612&partnerID=40&md5=a9f09566447ba32cb5c46c5461967ec7,"B-trees of order m are a “balanced” class of m-ary trees, which have applications in the areas of file organization. In fact, they have been the only choice when balanced multiway trees are required. Although they have very simple insertion and deletion algorithms, their storage utilization, that is, the number of keys per page or node, is at worst 50 percent. In the present paper we investigate a new class of balanced m-ary trees, the dense multiway trees, and compare their storage utilization with that of B-trees of order m.Surprisingly, we are able to demonstrate that weakly dense multiway trees have an Ο(log2 N) insertion algorithm. We also show that inserting mh - 1 keys in ascending order into an initially empty dense multiway tree yields the complete m-ary tree of height h, and that at intermediate steps in the insertion sequence the intermediate trees can also be considered to be as dense as possible. Furthermore, an analysis of the limiting dynamic behavior of the dense m-ary trees under insertion shows that the average storage utilization tends to 1; that is, the trees become as dense as possible. This motivates the use of the term “dense.”. © 1981, ACM. All rights reserved.",B-trees; balanced trees; dense trees; multiway trees; search trees; storage utilization,DATA PROCESSING
Consequences of Assuming a Universal Relation,1981,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0019708177&doi=10.1145%2f319628.319630&partnerID=40&md5=0ca06863b7c8c98b1f741c50c268e2ce,"Although central to the current direction of dependency theory, the assumption of a universal relation is incompatible with some aspects of relational database theory and practice. Furthermore, the universal relation is itself ill defined in some important ways. And, under the universal relation assumption, the decomposition approach to database design becomes virtually indistinguishable from the synthetic approach. © 1981, ACM. All rights reserved.",database design; dependency theory; relational database; relational theory; universal relation,DATA BASE SYSTEMS
A Normal Form for Relational Databases That Is Based on Domains and keys,1981,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0019611007&doi=10.1145%2f319587.319592&partnerID=40&md5=2515b960829f649adecba0f25b41e212,"A new normal form for relational databases, called domain-key normal form (DK/NF), is defined. Also, formal definitions of insertion anomaly and deletion anomaly are presented. It is shown that a schema is in DK/NF if and only if it has no insertion or deletion anomalies. Unlike previously defined normal forms, DK/NF is not defined in terms of traditional dependencies (functional, multivalued, or join). Instead, it is defined in terms of the more primitive concepts of domain and key, along with the general concept of a “constraint.” We also consider how the definitions of traditional normal forms might be modified by taking into consideration, for the first time, the combinatorial consequences of bounded domain sizes. It is shown that after this modification, these traditional normal forms are all implied by DK/NF. In particular, if all domains are infinite, then these traditional normal forms are all implied by DK/NF. © 1981, ACM. All rights reserved.",anomaly; complexity; database design; DK/NF; domain-key normal form; functional dependency; join dependency; multivalued dependency; normalization; relational database,DATA BASE SYSTEMS
TODS—The First Three Years (1976–1978),1980,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976771028&doi=10.1145%2f320610.320611&partnerID=40&md5=c3b750f12d1b6d6e20882917d9980edc,[No abstract available],,
A locking protocol for resource coordination in distributed databases,1980,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0019027128&doi=10.1145%2f320141.320143&partnerID=40&md5=d634669302856aaa3648debadc00d433,"A locking protocol to coordinate access to a distributed database and to maintain system consistency throughout normal and abnormal conditions is presented. The proposed protocol is robust in the face of crashes of any participating site, as well as communication failures. Recovery from any number of failures during normal operation or any of the recovery stages is supported. Recovery is done in such a way that maximum forward progress is achieved by the recovery procedures. Integration of virtually any locking discipline including predicate lock methods is permitted by this protocol. The locking algorithm operates, and operates correctly, when the network is partitioned, either intentionally or by failure of communication lines. Each partition is able to continue with work local to it, and operation merges gracefully when the partitions are reconnected. A subroutine of the protocol, that assures reliable communication among sites, is shown to have better performance than two-phase commit methods. For many topologies of interest, the delay introduced by the overall protocol is not a direct function of the size of the network. The communications cost is shown to grow in a relatively slow, linear fashion with the number of sites participating in the transaction. An informal proof of the correctness of the algorithm is also presented in this paper. The algorithm has as its core a centralized locking protocol with distributed recovery procedures. A centralized controller with local appendages at each site coordinates all resource control, with requests initiated by application programs at any site. However, no site experiences undue load. Recovery is broken down into three disjoint mechanisms: for single node recovery, merge of partitions, and reconstruction of the centralized controller and tables. The disjointness of the mechanisms contributes to comprehensibility and ease of proof. The paper concludes with a proposal for an extension aimed at optimizing operation of the algorithm to adapt to highly skewed distributions of activity. The extension applies nicely to interconnected computer networks. © 1980, ACM. All rights reserved.",concurrency; consistency; crash recovery; distributed databases; locking protocol,COMPUTER NETWORKS; COMPUTER PROGRAMMING - Subroutines; DATA BASE SYSTEMS
Retrospection on a database system,1980,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0019026674&doi=10.1145%2f320141.320158&partnerID=40&md5=693d9cf31bbe0bb21ed94ecfaf3ffc6d,"This paper describes the implementation history of the INGRES database system. It focuses on mistakes that were made in progress rather than on eventual corrections. Some attention is also given to the role of structured design in a database system implementation and to the problem of supporting nontrivial users. Lastly, miscellaneous impressions of UNIX, the PDP-11, and data models are given. © 1980, ACM. All rights reserved.",concurrency; integrity; nonprocedural languages; protection; recovery; relational databases,DATA BASE SYSTEMS
A security machanism for statistical database,1980,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0019056889&doi=10.1145%2f320613.320617&partnerID=40&md5=1ffc54d6fff74807ab950cf3f53b13bc,"The problem of user inference in statistical databases is discussed and illustrated with several examples. It is assumed that the database allows “total,” “average,” “count,” and “percentile” queries; a query may refer to any arbitrary subset of the database. Methods for protecting the security of such a database are considered; it is shown that any scheme which gives “statistically correct” answers is vulnerable to penetration. A precise definition of compromisability (in a statistical sense) is given. A general model of user inference is proposed; two special cases of this model appear to contain all previously published strategies for compromising a statistical database. A method for protecting the security of such a statistical database against these types of user inference is presented and discussed. It is shown that the number of queries required to compromise the database can be made arbitrarily large by accepting moderate increases in the variance of responses to queries. A numerical example is presented to illustrate the application of the techniques discussed. © 1980, ACM. All rights reserved.",compromisability; data security; database inference; privacy protection; statistical databases; statistical queries,DATA BASE SYSTEMS
Schema analysis for database restructuring,1980,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0019026673&doi=10.1145%2f320141.320147&partnerID=40&md5=429064c2333d2e2a2d822125bf91fa43,"The problem of generalized restructuring of databases has been addressed with two limitations: first, it is assumed that the restructuring user is able to describe the source and target databases in terms of the implicit data model of a particular methodology; second, the restructuring user is faced with the task of judging the scope and applicability of the defined types of restructuring to his database implementation and then of actually specifying his restructuring needs by translating them into the restructuring operations on a foreign data model. A certain amount of analysis of the logical and physical structure of databases must be performed, and the basic ingredients for such an analysis are developed here. The distinction between hierarchical and nonhierarchical data relationships is discussed, and a classification for database schemata is proposed. Examples are given to illustrate how these schemata arise in the conventional hierarchical and network systems. Application of the schema analysis methodology to restructuring specification is also discussed. An example is presented to illustrate the different implications of restructuring three seemingly identical database structures. © 1980, ACM. All rights reserved.",data model; data relationships; data semantics; data structure; database; database design; database management systems; database restructuring; graphical representation of data; schema; stored data,DATA BASE SYSTEMS
Secure statistical databases with random sample queries,1980,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0019054751&doi=10.1145%2f320613.320616&partnerID=40&md5=f9a0562e302d4ae67c86f6f4c3608569,"A new inference control, called random sample queries, is proposed for safeguarding confidential data in on-line statistical databases. The random sample queries control deals directly with the basic principle of compromise by making it impossible for a questioner to control precisely the formation of query sets. Queries for relative frequencies and averages are computed using random samples drawn from the query sets. The sampling strategy permits the release of accurate and timely statistics and can be implemented at very low cost. Analysis shows the relative error in the statistics decreases as the query set size increases; in contrast, the effort required to compromise increases with the query set size due to large absolute errors. Experiments performed on a simulated database support the analysis. © 1980, ACM. All rights reserved.",confidentiality; database security; disclosure controls; sampling; statistical database,DATA BASE SYSTEMS
Disclosure from Statistical Databases: Quantitative Aspects of Trackers,1980,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0019149034&doi=10.1145%2f320610.320645&partnerID=40&md5=93dd1eb90765f00ace2f6a2331549300,"Statistical evaluation of databases which contain personal records may entail risks for the confidentiality of the individual records. The risk has increased with the availability of flexible interactive evaluation programs which permit the use of trackers, the most dangerous class of snooping tools known. A class of trackers, called union trackers, is described. They permit reconstruction of the entire database without supplementary knowledge and include the general tracker recently described as a special case. For many real statistical databases the overwhelming majority of definable sets of records will form trackers. For such databases a random search for a tracker is likely to succeed rapidly. Individual trackers are redefined and counted and their cardinalities are investigated. If there are n records in the database, then most individual trackers employ innocent cardinalities near n/3, making them difficult to detect. Disclosure with trackers usually requires little effort per retrieved data element. © 1980, ACM. All rights reserved.",confidentiality; database security; security; statistical database; tracker,DATA BASE SYSTEMS
On the menbership problem for functional and multivalued dependencies in relational databases,1980,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0019058623&doi=10.1145%2f320613.320614&partnerID=40&md5=5cc4db6c4fe00e3aaa1399dbfe848834,"The problem of whether a given dependency in a database relation can be derived from a given set of dependencies is investigated. We show that the problem can be decided in polynomial time when the given set consists of either multivalued dependencies only or of both functional and multivalued dependencies and the given dependency is also either a functional or a multivalued dependency. These results hold when the derivations are restricted not to use the complementation rule. © 1980, ACM. All rights reserved.",functional dependency; inference rule; membership; multivalued dependency; relations,DATA BASE SYSTEMS
Quintary trees: A file structure for multidimensional datbase sytems,1980,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0019054807&doi=10.1145%2f320613.320618&partnerID=40&md5=7f2d2d919813fe9fa831554260c544ae,"In this paper we present a fide structure designed for a database system in which four types of retrieval requests (queries) are allowed: exact match, partial match, range, and partial range queries. For a file of N records, each of k keys, the worst-case running times of our search algorithms are bounded above, respectively, by O(k + log N), O(t + 3k-(s + log N)), O(t + (log N)k), and O(t + 3k-s(log N)) where s is the number of keys specified in a partial match (or range) query and t is the number of records retrieved by the query. The fde structure can be built in O(N(log N)k/(k - l)!) time and requires similar storage; by a slight modification, both of these requirements can be reduced to O(N(log N)k-/(k - l)!). We also sketch outlines for inserting and deleting records that require O(k + (log N)k) time, on the average. This structure achieves faster response time than previously known structures (for many of the queries) at the cost of extra storage. © 1980, ACM. All rights reserved.",database system; exact match queries; file maintenance; information retrieval; key; multidimensional space; queries; range search; search,DATA PROCESSING - Data Structures; DATA BASE SYSTEMS
Concurrent manipulation of binary search trees,1980,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0019054806&doi=10.1145%2f320613.320619&partnerID=40&md5=9711758f2bb83ed2dd4a4d485199791d,"The concurrent manipulation of a binary search tree is considered in this paper. The systems presented can support any number of concurrent processes which perform searching, insertion, deletion, and rotation (reorganization) on the tree, but allow any process to lock only a constant number of nodes at any time. Also, in the systems, searches are essentially never blocked. The concurrency control techniques introduced in the paper include the use of special nodes and pointers to redirect searches, and the use of copies of sections of the tree to introduce many changes simultaneously and therefore avoid unpredictable interleaving. Methods developed in this paper may provide new insights into other problems in the area of concurrent database manipulation. © 1980, ACM. All rights reserved.",binary search trees; concurrency controls; concurrent algorithm; consistency; correctness; data structures; databases; locking protocols,DATA PROCESSING
On the Design of Relational Database Schemata,1981,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0019541550&doi=10.1145%2f319540.319542&partnerID=40&md5=07c748dab73628624c75f6dc030dae16,"The purpose of this paper is to present a new approach to the conceptual design of relational databases based on the complete relatability conditions (CRCs).It is shown that current database design methodology based upon the elimination of anomalies is not adequate. In contradistinction, the CRCs are shown to provide a powerful criticism for decomposition. A decomposition algorithm is presented which (1) permits decomposition of complex relations into simple, well-defined primitives, (2) preserves all the original information, and (3) minimizes redundancy.The paper gives a complete derivation of the CRCs, beginning with a unified treatment of functional and multivalued dependencies, and introduces the concept of elementary functional dependencies and multiple elementary multivalued dependencies. Admissibility of covers and validation of results are also discussed, and it is shown how these concepts may be used to improve the design of 3NF schemata. Finally, a convenient graphical representation is proposed, and several examples are described in detail to illustrate the method. © 1981, ACM. All rights reserved.",decompositon; functional dependencies; minimal covers; multivalued dependencies; relational databases; schema design,DATA BASE SYSTEMS
Reliability Mechanisms for SDD-1: A System for Distributed Databases,1980,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0019149414&doi=10.1145%2f320610.320621&partnerID=40&md5=31c2d865a3c33fd3ebc5a755ca6ff311,"This paper presents the reliability mechanisms of SDD-1, a prototype distributed database system being developed by the Computer Corporation of America. Reliability algorithms in SDD-1 center around the concept of the Reliable Network (RelNet). The RelNet is a communications medium incorporating facilities for site status monitoring, event timestamping, multiply buffered message delivery, and the atomic control of distributed transactions. This paper is one of a series of companion papers on SDD-1 [3, 4, 6, 13]. © 1980, ACM. All rights reserved.",atomicity; distributed databases; recovery; reliability,DATA BASE SYSTEMS
A language facility for designing database-intensive applications,1980,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0019022430&doi=10.1145%2f320141.320150&partnerID=40&md5=b03190abb229395fd291c6e8c4737406,"TAXIS, a language for the design of interactive information systems (e.g., credit card verification, student-course registration, and airline reservations) is described. TAXIS offers (relational) database management facilities, a means of specifying semantic integrity constraints, and an exception-handling mechanism, integrated into a single language through the concepts of class, property, and the IS-A (generalization) relationship. A description of the main constructs of TAXIS is included and their usefulness illustrated with examples. © 1980, ACM. All rights reserved.",abstract data type; applications programming; exception handling; information system; relational data model; sesmantic network,DATA BASE SYSTEMS; COMPUTER PROGRAMMING LANGUAGES
Parallelism and recovery in database systems,1980,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0019027683&doi=10.1145%2f320141.320146&partnerID=40&md5=215fd6a882ae0c938b807bd00d71aebf,"In this paper a new method to increase parallelism in database systems is described. Use is made of the fact that for recovery reasons, we often have two values for one object in the database—the new one and the old one. Introduced and discussed in detail is a certain scheme by which readers and writers may work simultaneously on the same object. It is proved that transactions executed according to this scheme have the correct effect; i.e., consistency is preserved. Several variations of the basic scheme which are suitable depending on the degree of parallelism required, are described. © 1980, ACM. All rights reserved.",concurrency; consistency; deadlock; integrity; recovery; synchronization; transaction; two phase locking,DATA BASE SYSTEMS
Spatial Management of Data,1980,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0019147922&doi=10.1145%2f320610.320648&partnerID=40&md5=896f56626335f7cf4003a55a96337dfb,"Spatial data management is a technique for organizing and retrieving information by positioning it in a graphical data space (GDS). This graphical data space is viewed through a color raster-scan display which enables users to traverse the GDS surface or zoom into the image to obtain greater detail. In contrast to conventional database management systems, in which users access data by asking questions in a formal query language, a spatial data management system (SDMS) presents the information graphically in a form that seems to encourage browsing and to require less prior knowledge of the contents and organization of the database. This paper presents an overview of the SDMS concept and describes its implementation in a prototype system for retrieving information from both a symbolic database management system and an optical videodisk. © 1980, ACM. All rights reserved.",computer graphics; database query languages; graphics languages; man-machine interaction,DATA BASE SYSTEMS
Construction of relations in relational databases,1980,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0019027314&doi=10.1145%2f320141.320155&partnerID=40&md5=adf63b00688c400832b43702d0bd43dd,"Using a nonprocedural language for query formulation requires certain automatization of a query answering process. Given a query for creation of a new relation, the problem is to find an efficient procedure which produces this relation from a given relational database. We concentrate upon sequences of join operations which losslessly produce a relation required by a query. A new property of such sequences is analyzed which provides a basis for the presented algorithms that construct an efficient join procedure. The algorithms have polynomial complexity. A modified AND/OR graph is used for the display of a given set of dependencies and a collection of relations representing a database. © 1980, ACM. All rights reserved.",algorithms; AND/OR graphs; lossless joins; query answering; relational databases,DATA BASE SYSTEMS
Calculating constraints on relational expression,1980,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0019054853&doi=10.1145%2f320613.320615&partnerID=40&md5=78db36c7dbcda4ec60740a4521afdd29,"This paper deals with the problem of determining which of a certain class of constraints hold on a given relational algebra expression where the base relations come from a given schema. The class of constraints includes functional dependencies, equality of domains, and constancy of domains. The relational algebra consists of projection, selection, restriction, cross product, union, and difference. The problem as given is undecidable, but if set difference is removed from the algebra, there is a solution. Operators specifying a closure function (similar to functional dependency closure on one relation) are defined; these will generate exactly the set of constraints valid on the given relational algebra expression. We prove that the operators are sound and complete. © 1980, ACM. All rights reserved.",constraints; derivation rules; eompleteness; functional dependencies; Views,DATA BASE SYSTEMS
Decompositions and Functional Dependencies in Relations,1980,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0019183913&doi=10.1145%2f320610.320620&partnerID=40&md5=4b56314960bd1fa2dde920be94d7c3dd,"A general study is made of two basic integrity constraints on relations: functional and multivalued dependencies. The latter are studied via an equivalent concept: decompositions. A model is constructed for any possible combination of functional dependencies and decompositions. The model embodies some decompositions as unions of relations having different schemata of functional dependencies. This suggests a new, stronger integrity constraint, the degenerate decomposition. More generally, the theory demonstrates the importance of using the union operation in database design and of allowing different schemata on the operands of a union. Techniques based on the union lead to a method for solving the problem of membership of a decomposition in the closure of a given set of functional dependencies and decompositions. The concept of antiroot is introduced as a tool for describing families of decompositions, and its fundamental importance for database design is indicated. © 1980, ACM. All rights reserved.",decomposition; functional dependency; integrity constraint; multivalued dependency; relational database,DATA BASE SYSTEMS
Theseus—a programming language for relational databeses,1979,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0018699135&doi=10.1145%2f320107.320121&partnerID=40&md5=3b518f807be73e49e01d1cd04d53b021,"Theseus, a very high-level programming language extending EUCLID, is described. Data objects in Theseus include relations and a-sets, a generalization of records. The primary design goals of Theseus are to facilitate the writing of well-structured programs for database applications and to serve as a vehicle for research in automatic program optimization. © 1979, ACM. All rights reserved.",compiler organization; relational database languages; very high-level languages,DATA BASE SYSTEMS; COMPUTER PROGRAMMING LANGUAGES
Partial-Match Hash Coding: Benefits of Redundancy,1979,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0018480580&doi=10.1145%2f320071.320079&partnerID=40&md5=78382b73c2b673a0b754f12499e4d010,"File designs suitable for retrieval from a file of k-field records when queries may be partially specified are examined. Storage redundancy is introduced to obtain improved worst-case and average-case performances. The resulting storage schemes are appropriate for replicated distributed database environments; it is possible to improve the overall average and worst-case behavior for query response as well as provide an environment with very high reliability. Within practical systems it will be possible to improve the query response time performance as well as reliability over comparable systems without replication. © 1979, ACM. All rights reserved.",access methods; algorithms; analysis; data structures; database systems; replication; searching,DATA PROCESSING - File Organization; DATA BASE SYSTEMS
Testing implications of data dependencies,1979,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0018699134&doi=10.1145%2f320107.320115&partnerID=40&md5=2201f3e9401f890837fabf3b07001bf3,"Presented is a computation method—the chase—for testing implication of data dependencies by a set of data dependencies. The chase operates on tableaux similar to those of Aho, Sagiv, and Ullman. The chase includes previous tableau computation methods as special cases. By interpreting tableaux alternately as mappings or as templates for relations, it is possible to test implication of join dependencies (including multivalued dependencies) and functional dependencies by a set of dependencies. © 1979, ACM. All rights reserved.",chase; data dependencies; functional dependencies; join dependencies; multivalued dependencies; relational databases; tableaux,DATA BASE SYSTEMS
Linear Queries in Statistical Databases,1979,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0018479599&doi=10.1145%2f320071.320073&partnerID=40&md5=24098cd21cfe727915aef2656e471760,"A database is compromised if a user can determine the data elements associated with keys which he did not know previously. If it is possible, compromise can be achieved by posing a finite set of queries over sets of data elements and employing initial information to solve the resulting system of equations. Assuming the allowable queries are linear, that is, weighted sums of data elements, we show how compromise can be achieved and we characterize the maximal initial information permitted of a user in a secure system. When compromise is possible, the initial information and the number of queries required to achieve it is surprisingly small. © 1979, ACM. All rights reserved.",confidentiality; data security; database security; inference; linear query; secure query functions; statical database,DATA PROCESSING - Security of Data; DATA BASE SYSTEMS
Optimal Partial-Match Retrieval When Fields are Independently Specified,1979,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0018480450&doi=10.1145%2f320071.320074&partnerID=40&md5=221e7550b4f221d735c323f62b443a4b,"This paper considers the design of a system to answer partial-match queries from a file containing a collection of records, each record consisting of a sequence of fields. A partial-match query is a specification of values for zero or more fields of a record, and the answer to a query is a listing of all records in the file whose fields match the specified values. A design is considered in which the file is stored in a set of bins. A formula is derived for the optimal number of bits in a bin address to assign to each field, assuming the probability that a given field is specified in a query is independent of what other fields are specified. Implications of the optimality criterion on the size of bins are also discussed. © 1979, ACM. All rights reserved.",associative searching; file organization; hashing; information retrieval; partial-match retrieval; searching,DATA PROCESSING - File Organization; INFORMATION SCIENCE
A Fast Procedure for Finding a Tracker in a Statistical Database,1980,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0018998235&doi=10.1145%2f320128.320138&partnerID=40&md5=0e48ef3d429b6e4e35c81a3933ecd832,"To avoid trivial compromises, most on-line statistical databases refuse to answer queries for statistics about small subgroups. Previous research discovered a powerful snooping tool, the tracker, with which the answers to these unanswerable queries are easily calculated. However, the extent of this threat was not clear, for no one had shown that finding a tracker is guaranteed to be easy. This paper gives a simple algorithm for finding a tracker when the maximum number of identical records is not too large. The number of queries required to find a tracker is at most Ο(log2S) queries, where S is the number of distinct records possible. Experimental results show that the procedure often finds a tracker with just a few queries. The threat posed by trackers is therefore considerable. © 1980, ACM. All rights reserved.",confidentiality; data security; database security; statistical database; tracker,DATA BASE SYSTEMS
Comments on “Process synchronizaiton in databases systems”,1979,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976677814&doi=10.1145%2f320107.320126&partnerID=40&md5=c6a60391049a3568aa5c737787873089,[No abstract available],,
A Majority Consensus Approach to Concurrency Control For Multiple Copy Databases,1979,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0018480001&doi=10.1145%2f320071.320076&partnerID=40&md5=a131160d7068ceb91f7846488d3c10e6,"A “majority consensus” algorithm which represents a new solution to the update synchronization problem for multiple copy databases is presented. The algorithm embodies distributed control and can function effectively in the presence of communication and database site outages. The correctness of the algorithm is demonstrated and the cost of using it is analyzed. Several examples that illustrate aspects of the algorithm operation are included in the Appendix. © 1979, ACM. All rights reserved.",clock synchronization; computer networks; concurrency control; distributed computation; distributed control; distributed databases; multiprocess systems; update synchronization,DATA BASE SYSTEMS
Experiments on the Determination of the Relationships Between Terms,1979,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0018480123&doi=10.1145%2f320071.320081&partnerID=40&md5=da4dbc575f2ae189376da94ff66f5fc5,"The retrieval effectiveness of an automatic method that uses relevance judgments for the determination of positive as well as negative relationships between terms is evaluated. The term relationships are incorporated into the retrieval process by using a generalized similarity function that has a term match component, a positive term relationship component, and a negative term relationship component. Two strategies, query partitioning and query clustering, for the evaluation of the effectiveness of the term relationships are investigated. The latter appears to be more attractive from linguistic as well as economic points of view. The positive and the negative relationships are verified to be effective both when used individually, and in combination. The importance attached to the term relationship components relative to that of term match component is found to have a substantial effect on the retrieval performance. The usefulness of discriminant analysis as a technique for determining the relative importance of these components is investigated. © 1979, ACM. All rights reserved.",antonym; document retrieval; feedback; pseudoclassification; semantics; statistical discrimination; synonym; term associations; thesaurus,INFORMATION SCIENCE
On Semantic Issues Connected with Incomplete Information Databases,1979,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0018518835&doi=10.1145%2f320083.320088&partnerID=40&md5=5f43ddef647e85ec4d1f4c10a5059dad,"Various approaches to interpreting queries in a database with incomplete information are discussed. A simple model of a database is described, based on attributes which can take values in specified attribute domains. Information incompleteness means that instead of having a single value of an attribute, we have a subset of the attribute domain, which represents our knowledge that the actual value, though unknown, is one of the values in this subset. This extends the idea of Codd's null value, corresponding to the case when this subset is the whole attribute domain. A simple query language to communicate with such a system is described and its various semantics are precisely defined. We emphasize the distinction between two different interpretations of the query language—the external one, which refers the queries directly to the real world modeled in an incomplete way by the system, and the internal one, under which the queries refer to the system's information about this world, rather than to the world itself. Both external and internal interpretations are provided with the corresponding sets of axioms which serve as a basis for equivalent transformations of queries. The technique of equivalent transformations of queries is then extensively exploited for evaluating the interpretation of (i.e. the response to) a query. © 1979, ACM. All rights reserved.",database; incomplete information; model logic; null values; query language semantics; relational model,DATA BASE SYSTEMS
On searching transposed files,1979,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0018683646&doi=10.1145%2f320107.320125&partnerID=40&md5=ee4539535d2d2aec9d54dc2080c726c4,"A transposed file is a collection of nonsequential files called subfiles. Each subfile contains selected attribute data for all records. It is shown that transposed file performance can be enhanced by using a proper strategy to process queries. Analytic cost expressions for processing conjunctive, disjunctive, and batched queries are developed and an effective heuristic for minimizing query processing costs is presented. Formulations of the problem of optimally processing queries for a particular family or transposed files are shown to be NP-complete. Query processing performance comparisons of multilist, inverted, and nonsequential files with transposed files are also considered. © 1979, ACM. All rights reserved.",file searching; inveited file; multilist; NP-complete; query processing; transposed file,DATA BASE SYSTEMS
The Correctness of Concurrency Control Mechanisms in a System for Distributed Databases (SDD-1),1980,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0018998388&doi=10.1145%2f320128.320133&partnerID=40&md5=34b83b96290224507ddc84d7df6650ac,"This paper presents a formal analysis of the concurrency control strategy of SDD-1. SDD-1, a System for Distributed Databases, is a prototype distributed database system being developed by Computer Corporation of America. In SDD-1, portions of data distributed throughout a network may be replicated at multiple sites. The SDD-1 concurrency control guarantees database consistency in the face of such distribution and replication. This paper is one of a series of companion papers on SDD-1 [2, 8]. © 1980, ACM. All rights reserved.",conflict graph; correctness of concurrency control; distributed database system; serializability theory,DATA BASE SYSTEMS
Propeties of Storage Hierarchy Systems with Multiple Page Sizes and Redundant Data,1979,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0018519774&doi=10.1145%2f320083.320095&partnerID=40&md5=35131063b15e1e5b1609f098a0e6cd2c,"The need for high performance, highly reliable storage for very large on-line databases, coupled with rapid advances in storage device technology, has made the study of generalized storage hierarchies an important area of research. This paper analyzes properties of a data storage hierarchy system specifically designed for handling very large on-line databases. To attain high performance and high reliability, the data storage hierarchy makes use of multiple page sizes in different storage levels and maintains multiple copies of the same information across the storage levels. Such a storage hierarchy system is currently being designed as part of the INFOPLEX database computer project. Previous studies of storage hierarchies have primarily focused on virtual memories for program storage and hierarchies with a single page size across all storage levels and/or a single copy of information in the hierarchy. In the INFOPLEX design, extensions to the least recently used (LRU) algorithm are used to manage the storage levels. The read-through technique is used to initially load a referenced page of the appropriate size into all storage levels above the one in which the page is found. Since each storage level is viewed as an extension of the immediate higher level, an overflow page from level i is always placed in level i + 1. Important properties of these algorithms are derived. It is shown that depending on the types of algorithms used and the relative sizes of the storage levels, it is not always possible to guarantee that the contents of a given storage level i is always a superset of the contents of its immediate higher storage level i - 1. The necessary and sufficient conditions for this property to hold are identified and proved. Furthermore, it is possible that increasing the size of intermediate storage levels may actually increase the number of references to lower storage levels, resulting in reduced performance. Conditions necessary to avoid such an anomaly are also identified and proved. © 1979, ACM. All rights reserved.",data storage hierarchy; database computer; inclusion properties; modeling; perform and reliability analysis; storage management algorithms; very large databases,DATA BASE SYSTEMS
Extendible Hashing—a Fast Access Method for Dynamic Files,1979,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0018517078&doi=10.1145%2f320083.320092&partnerID=40&md5=d862f5872d5efd1122abc455cb0a5780,"Extendible hashing is a new access technique, in which the user is guaranteed no more than two page faults to locate the data associated with a given unique identifier, or key. Unlike conventional hashing, extendible hashing has a dynamic structure that grows and shrinks gracefully as the database grows and shrinks. This approach simultaneously solves the problem of making hash tables that are extendible and of making radix search trees that are balanced. We study, by analysis and simulation, the performance of extendible hashing. The results indicate that extendible hashing provides an attractive alternative to other access methods, such as balanced trees. © 1979, ACM. All rights reserved.",access method; B-tree; directory; extendible hashing; external hashing; file organization; hashing; index; radix search; searching; trie,DATA PROCESSING
The Theory of Joins in Relational Databases,1979,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0018517077&doi=10.1145%2f320083.320091&partnerID=40&md5=884019f71ebdbaa613d910215b4ef3ca,"Answering queries in a relational database often requires that the natural join of two or more relations be computed. However, the result of a join may not be what one expects. In this paper we give efficient algorithms to determine whether the join of several relations has the intuitively expected value (is lossless) and to determine whether a set of relations has a subset with a lossy join. These algorithms assume that all data dependencies are functional. We then discuss the extension of our techniques to the case where data dependencies are multivalued. © 1979, ACM. All rights reserved.",decompositon of database schemes; functional dependenies; lossless join; multivalued dependencies; natural join; projection of dependencies; relational databases,DATA BASE SYSTEMS
Heuristics for Trie Index Minimization,1979,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976862124&doi=10.1145%2f320083.320102&partnerID=40&md5=3fd57727a8da2b1265e559f6c91f5158,"A trie is a digital search tree in which leaves correspond to records in a file. Searching proceeds from the root to a leaf, where the edge taken at each node depends on the value of an attribute in the query. Trie implementations have the advantage of being fast, but the disadvantage of achieving that speed at great expense in storage space. Of primary concern in making a trie practical, therefore, is the problem of minimizing storage requirements. One method for reducing the space required is to reorder attribute testing. Unfortunately, the problem of finding an ordering which guarantees a minimum-size trie is NP-complete. In this paper we investigate several heuristics for reordering attributes, and derive bounds on the sizes of the worst tries produced by them in terms of the underlying file. Although the analysis is presented for a binary file, extensions to files of higher degree are shown. Another alternative for reducing the space required by a trie is an implementation, called an Ο-trie, in which the order of attribute testing is contained in the trie itself. We show that for most applications, Ο-tries are smaller than other implementations of tries, even when heuristics for improving storage requirements are employed. © 1979, ACM. All rights reserved.",doubly chained tree; index; trie; trie minimization,
Pipelining: A technique for implementing data restructurers,1979,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0018684082&doi=10.1145%2f320107.320118&partnerID=40&md5=ddc8a5efcbb8960212312306a5f7e043,"In the past several years much attention has been given to the problem of data translation. The focus has been mainly on methodologies and specification languages for accomplishing this task. Recently, several prototype systems have emerged, and now the issues of implementation and performance must be addressed. In general, a data restructuring specification may contain multiple source and target files. This specification can be viewed as a “process graph” which is a network of restructuring operations subject to precedence constraints. One technique used to achieve good performance is that of pipelining data in the process graph. In this paper we address a number of issues pertinent to a pipelining architecture. Specifically, we give algorithms for resolving deadlock situations which can arise, and partitioning the process graph to achieve an optimal schedule for executing the restructuring steps. In addition, we discuss how pipelining has influenced the design of the restructuring operations and the file structures used in an actual system. © 1979, ACM. All rights reserved.",data translation; database conversion; deadlock; pipelining; process scheduling,DATA PROCESSING
Introduction to a System for Distributed Databases (SDD-1),1980,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0018995726&doi=10.1145%2f320128.320129&partnerID=40&md5=6b85093ef6037664468552bd66870185,"The declining cost of computer hardware and the increasing data processing needs of geographically dispersed organizations have led to substantial interest in distributed data management. SDD-1 is a distributed database management system currently being developed by Computer Corporation of America. Users interact with SDD-1 precisely as if it were a nondistributed database system because SDD-1 handles all issues arising from the distribution of data. These issues include distributed concurrency control, distributed query processing, resiliency to component failure, and distributed directory management. This paper presents an overview of the SDD-1 design and its solutions to the above problems. This paper is the first of a series of companion papers on SDD-1 (Bernstein and Shipman [2], Bernstein et al. [4], and Hammer and Shipman [14]). © 1980, ACM. All rights reserved.",concurrency control; database reliability; distributed database system; query processing; relational data model,DATA BASE SYSTEMS
Concurrency Control in a System for Distributed Databases (SDD-1),1980,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0018995727&doi=10.1145%2f320128.320131&partnerID=40&md5=028a99f73aebf990b772704cd37d9117,"This paper presents the concurrency control strategy of SDD-1. SDD-1, a System for Distributed Databases, is a prototype distributed database system being developed by Computer Corporation of America. In SDD-1, portions of data distributed throughout a network may be replicated at multiple sites. The SDD-1 concurrency control guarantees database consistency in the face of such distribution and replication. This paper is one of a series of companion papers on SDD-1 [4, 10, 12, 21]. © 1980, ACM. All rights reserved.",concurrency control; conflict graph; distributed database system; serializability; synchronization; timestamps,DATA BASE SYSTEMS
Efficient Optimization of a Class of Relational Expressions,1979,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0018724359&doi=10.1145%2f320107.320112&partnerID=40&md5=6644e87fb368048d2848fbde01b7f40b,"The design of several database query languages has been influenced by Codd's relational algebra. This paper discusses the difficulty of optimizing queries based on the relational algebra operations select, project, and join. A matrix, called a tableau, is proposed as a useful device for representing the value of a query, and optimization of queries is couched in terms of finding a minimal tableau equivalent to a given one. Functional dependencies can be used to imply additional equivalences among tableaux. Although the optimization problem is NP-complete, a polynomial time algorithm exists to optimize tableaux that correspond to an important subclass of queries. © 1979, ACM. All rights reserved.",equivalence of queries; NP-completeness; query optimization; relational algebra; relational database; tableaux,DATA BASE SYSTEMS
Performance Evaluation of Attribute-Based Tree Organization,1980,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0018996627&doi=10.1145%2f320128.320135&partnerID=40&md5=f92442e42fe5317574b61379dc3bdb6e,"A modified version of the multiple attribute tree (MAT) database organization, which uses a compact directory, is discussed. An efficient algorithm to process the directory for carrying out the node searches is presented. Statistical procedures are developed to estimate the number of nodes searched and the number of data blocks retrieved for most general and complex queries. The performance of inverted file and modified MAT organizations are compared using six real-life databases and four types of query complexities. Careful tradeoffs are established in terms of storage and access times for directory and data, query complexities, and database characteristics. © 1980, ACM. All rights reserved.",access time; average retrieval time per query; database organization; database performance; directory search time; modified multiple attribute tree; query complexity; secondary index organization,DATA BASE SYSTEMS
Design of a balanced multiple-valued file-organization scheme with the least redundancy,1979,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0018706578&doi=10.1145%2f320107.320123&partnerID=40&md5=d573086150809be41fdcefea6800ae95,"A new balanced file-organization scheme of order two for multiple-valued records is presented. This scheme is called HUBMFS2 (Hiroshima University Balanced Multiple-valued File-organization Scheme of order two). It is assumed that records are characterized by m attributes having n possible values each, and the query set consists of queries which specify values of two attributes. It is shown that the redundancy of the bucket (the probability of storing a record in the bucket) is minimized if and only if the structure of the bucket is a partite-claw. A necessary and sufficient condition for the existence of an HUBMFS2, which is composed exclusively of partite-claw buckets, is given. A construction algorithm is also given. The proposed HUBMFS2 is superior to existing BMFS2 (Balanced Multiple-valued File-organization Schemes of order two) in that it has the least redundancy among all possible BMFS2‘s having the same parameters and that it can be constructed for a less restrictive set of parameters. © 1979, ACM. All rights reserved.",balanced filing scheme; bucket; claw; file organization; graph decomposition; information retrieval; information storage; inverted file; multipartite graph; multiple-valued attributes; redundancy; secondary index,DATA PROCESSING
Extending the database relational model to capture more meaning,1979,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0018705521&doi=10.1145%2f320107.320109&partnerID=40&md5=96dd15830ec758a0bc5c9dfe326e9275,"During the last three or four years several investigators have been exploring “semantic models” for formatted databases. The intent is to capture (in a more or less formal way) more of the meaning of the data so that database design can become more systematic and the database system itself can behave more intelligently. Two major thrusts are clear.(1) the search for meaningful units that are as small as possible—atomic semantics;(2) the search for meaningful units that are larger than the usual n-ary relation—molecular semantics. In this paper we propose extensions to the relational model to support certain atomic and molecular semantics. These extensions represent a synthesis of many ideas from the published work in semantic modeling plus the introduction of new rules for insertion, update, and deletion, as well as new algebraic operators. © 1979, ACM. All rights reserved.",conceptual model; conceptual schema; data model; data semantics; database; database schema; entity model; knowledge base; knowledge representation; relatinal database; relation; relational model; relational schema; semantic model,DATA BASE SYSTEMS
Locking Granularity Revisited,1979,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0018479691&doi=10.1145%2f320071.320078&partnerID=40&md5=4a4d80a5cea86ef6f7637f2442bacca1,"Locking granularity refers to the size and hence the number of locks used to ensure the consistency of a database during multiple concurrent updates. In an earlier simulation study we concluded that coarse granularity, such as area or file locking, is to be preferred to fine granularity such as individual page or record locking. However, alternate assumptions than those used in the original paper can change that conclusion. First, we modified the assumptions concerning the placement of the locks on the database with respect to the accessing transactions. In the original model the locks were assumed to be well placed. Under worse case and random placement assumptions when only very small transactions access the database, fine granularity is preferable. Second, we extended the simulation to model a lock hierarchy where large transactions use large locks and small transactions use small locks. In this scenario, again under the random and worse case lock placement assumptions, fine granularity is preferable if all transactions accessing more than 1 percent of the database use large locks. Finally, the simulation was extended to model a “claim as needed” locking strategy together with the resultant possibility of deadlock. In the original study all locks were claimed in one atomic operation at the beginning of a transaction. The claim as needed strategy does not change the conclusions concerning the desired granularity. © 1979, ACM. All rights reserved.",concurrency; database management; locking granulaity; locking hierarchies; multiple updates,DATA BASE SYSTEMS
Efficiently Monitoring Relational Databases,1979,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0018515483&doi=10.1145%2f320083.320099&partnerID=40&md5=fcc1c4faaf0a96479bdfe2490e56f906,"An alerter is a program which monitors a database and reports to some user or program when a specified condition occurs. It may be that the condition is a complicated expression involving several entities in the database; in this case the evaluation of the expression may be computationally expensive. A scheme is presented in which alerters may be placed on a complex query involving a relational database, and a method is demonstrated for reducing the amount of computation involved in checking whether an alerter should be triggered. © 1979, ACM. All rights reserved.",alerters; exception reporting; integrity constraints; programming techniques; relational databases,DATA BASE SYSTEMS
Improving the human factors aspect of database interactions,1978,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0018052487&doi=10.1145%2f320289.320295&partnerID=40&md5=55f6f07b573d9d603a41e48b1a0b66ab,"The widespread dissemination of computer and information systems to nontechnically trained individuals requires a new approach to the design and development of database interfaces. This paper provides the motivational background for controlled psychological experimentation in exploring the person/machine interface. Frameworks for the reductionist approach are given, research methods discussed, research issues presented, and a small experiment is offered as an example of what can be accomplished. This experiment is a comparison of natural and artificial language query facilities. Although subjects posed approximately equal numbers of valid queries with either facility, natural language users made significantly more invalid queries which could not be answered from the database that was described. © 1978, ACM. All rights reserved.",data models; database systems; experimentation; human factors; natural language interfaces; psychology; query languages,DATA BASE SYSTEMS; SYSTEMS SCIENCE AND CYBERNETICS - Man Machine Systems; HUMAN ENGINEERING
System Level Concurrency Control for Distributed Database Systems,1978,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0017983393&doi=10.1145%2f320251.320260&partnerID=40&md5=f9e29d6090631ad52be21f11058c159d,"A distributed database system is one in which the database is spread among several sites and application programs “move” from site to site to access and update the data they need. The concurrency control is that portion of the system that responds to the read and write requests of the application programs. Its job is to maintain the global consistency of the distributed database while ensuring that the termination of the application programs is not prevented by phenomena such as deadlock. We assume each individual site has its own local concurrency control which responds to requests at that site and can only communicate with concurrency controls at other sites when an application program moves from site to site, terminates, or aborts. This paper presents designs for several distributed concurrency controls and demonstrates that they work correctly. It also investigates some of the implications of global consistency of a distributed database and discusses phenomena that can prevent termination of application programs. © 1978, ACM. All rights reserved.",concurrency; consistency; database; deadlock; deadly embrace; distributed; integrity; lock; readers and writers; restart; rollback; transaction,DATA BASE SYSTEMS
Generation and search of clustered files,1978,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0018057808&doi=10.1145%2f320289.320291&partnerID=40&md5=f7130b5267aa44c82fa4dc2224c714d6,"A classified, or clustered file is one where related, or similar records are grouped into classes, or clusters of items in such a way that all items within a cluster are jointly retrievable. Clustered files are easily adapted to broad and narrow search strategies, and simple file updating methods are available. An inexpensive file clustering method applicable to large files is given together with appropriate file search methods. An abstract model is then introduced to predict the retrieval effectiveness of various search methods in a clustered file environment. Experimental evidence is included to test the versatility of the model and to demonstrate the role of various parameters in the cluster search process. © 1978, ACM. All rights reserved.",automatic classification; cluster searching; clustered files; fast classification; file organization; probabilistic models,DATA PROCESSING
Secure Databases: Protection Against User Influence,1979,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0018444418&doi=10.1145%2f320064.320068&partnerID=40&md5=f9a0799bfbcaaa52f3ecfd65a3b8a159,"Users may be able to compromise databases by asking a series of questions and then inferring new information from the answers. The complexity of protecting a database against this technique is discussed here. © 1979, ACM. All rights reserved.",compromise; database; inference; information flow; protection; security; statistical query,DATA BASE SYSTEMS; DATA PROCESSING
Computational problems related to the design of normal form relational schemas,1979,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0018442877&doi=10.1145%2f320064.320066&partnerID=40&md5=70266af13df8e52cdc489180a5e189ab,"Problems related to functional dependencies and the algorithmic design of relational schemas are examined. Specifically, the following results are presented: (1) a tree model of derivations of functional dependencies from other functional dependencies; (2) a linear-time algorithm to test if a functional dependency is in the closure of a set of functional dependencies; (3) a quadratic-time implementation of Bernstein's third normal form schema synthesis algorithm. Furthermore, it is shown that most interesting algorithmic questions about Boyce-Codd normal form and keys are NP-complete and are therefore probably not amenable to fast algorithmic solutions. © 1979, ACM. All rights reserved.",database design; derivation tree; function dependency; membership algorithm; NP-complete; relational database; third normal form,DATA BASE SYSTEMS
Limitations of record-based information models,1979,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976841118&doi=10.1145%2f320064.320070&partnerID=40&md5=a151284761827d891fd7b844ba013dda,"Record structures are generally efficient, familiar, and easy to use for most current data processing applications. But they are not complete in their ability to represent information, nor are they fully self-describing. © 1979, ACM. All rights reserved.",conceptual model; data model; entities; first normal form; information model; normalization; records; relationships; semantic model,
Data abstractions for database systems,1979,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0018445596&doi=10.1145%2f320064.320067&partnerID=40&md5=9f64bbf8c9102d0bca927785c66472cf,"Data abstractions were originally conceived as a specification tool in programming. They also appear to be useful for exploring and explaining the capabilities and shortcomings of the data definition and manipulation facilities of present-day database systems. Moreover they may lead to new approaches to the design of these facilities. In the first section the paper introduces an axiomatic method for specifying data abstractions and, on that basis, gives precise meaning to familiar notions such as data model, data type, and database schema. In a second step the various possibilities for specifying data types within a given data model are examined and illustrated. It is shown that data types prescribe the individual operations that are allowed within a database. Finally, some additions to the method are discussed which permit the formulation of interrelationships between arbitrary operations. © 1979, ACM. All rights reserved.",abstract data type; data abstraction; data definition language; data manipulation language; data model; data structure; data type; database consistency; database design; database schema; integrity constraints; specification,DATA BASE SYSTEMS
Sequentiality and Prefetching in Database Systems,1978,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0018008987&doi=10.1145%2f320263.320276&partnerID=40&md5=e014c2ff318b2d2376dcc97ccf12e78f,"Sequentiality of access is an inherent characteristic of many database systems. We use this observation to develop an algorithm which selectively prefetches data blocks ahead of the point of reference. The number of blocks prefetched is chosen by using the empirical run length distribution and conditioning on the observed number of sequential block references immediately preceding reference to the current block. The optimal number of blocks to prefetch is estimated as a function of a number of “costs,” including the cost of accessing a block not resident in the buffer (a miss), the cost of fetching additional data blocks at fault times, and the cost of fetching blocks that are never referenced. We estimate this latter cost, described as memory pollution, in two ways. We consider the treatment (in the replacement algorithm) of prefetched blocks, whether they are treated as referenced or not, and find that it makes very little difference. Trace data taken from an operational IMS database system is analyzed and the results are presented. We show how to determine optimal block sizes. We find that anticipatory fetching of data can lead to significant improvements in system operation. © 1978, ACM. All rights reserved.",buffer management; database systems; dynamic programming; IMS; paging; prefetching; sequentiality,DATA BASE SYSTEMS
On a Partitioning Problem,1978,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0018008988&doi=10.1145%2f320263.320287&partnerID=40&md5=440b6297babeb0bb6c700ca69a6121cd,"This paper investigates the problem of locating a set of “boundary points” of a large number of records. Conceptually, the boundary points partition the records into subsets of roughly the same number of elements, such that the key values of the records in one subset are all smaller or all larger than those of the records in another subset. We guess the locations of the boundary points by linear interpolation and check their accuracy by reading the key values of the records on one pass. This process is repeated until all boundary points are determined. Clearly, this problem can also be solved by performing an external tape sort. Both analytical and empirical results indicate that the number of passes required is small in comparison with that in an external tape sort. This kind of record partitioning may be of interest in setting up a statistical database system. © 1978, ACM. All rights reserved.",external sort; key value; partition; passes; tape probability,DATA BASE SYSTEMS
Implementing a Generalized Access Path Structure for a Relational Database System,1978,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0018015498&doi=10.1145%2f320263.320284&partnerID=40&md5=a70824046c0c7cddd8666ed554511af4,"A new kind of implementation technique for access paths connecting sets of tuples qualified by attribute values is described. It combines the advantages of pointer chain and multilevel index implementation techniques. Compared to these structures the generalized access path structure is at least competitive in performing retrieval and update operations, while a considerable storage space saving is gained. Some additional features of this structure support m-way joins and the evaluation of multirelation queries, and allow efficient checks of integrity assertions and simple reorganization schemes. © 1978, ACM. All rights reserved.",access path structures; B-trees; database; index structures; relational model,DATA BASE SYSTEMS
Normalization and Hierarchical Dependencies in the Relational Data Model,1978,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0018015497&doi=10.1145%2f320263.320271&partnerID=40&md5=6822186ec31022fa4f316ae0b02c1722,"The purpose of this paper is to present a new approach to the conceptual design of logical schemata for relational databases. One-to-one, one-to-many, and many-to-many relationships between the attributes of database relations are modeled by means of functional dependencies and multivalued dependencies. A new type of dependency is introduced: first-order hierarchical decomposition. The properties of this new type of dependency are studied and related to the normalization process of relations. The relationship between the concept of first-order hierarchical decomposition and the notion of hierarchical organization of data is discussed through the normalization process. © 1978, ACM. All rights reserved.",data model; first-order hierarchical dependency; functional dependency; hierarchical schema; multivalued dependency; normalization process; relational database; relational model,DATA BASE SYSTEMS
Optimization of Query Evaluation Algorithms,1979,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0018478434&doi=10.1145%2f320071.320072&partnerID=40&md5=43d301102d1975e08c3abb16f81a10a5,"A model of database storage and access is presented. The model represents many evaluation algorithms as special cases, and helps to break a complex algorithm into simple access operations. Generalized access cost equations associated with the model are developed and analyzed. Optimization of these cost equations yields an optimal access algorithm which can be synthesized by a query subsystem whose design is based on the modular access operations. © 1979, ACM. All rights reserved.",data manipulation language; database optimization; inverted file; query language; query optimization; relational data model,COMPUTER PROGRAMMING LANGUAGES; QUERY LANGUAGES; DATA BASE SYSTEMS
Developing a Natural Language Interface to Complex Data,1978,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0017981971&doi=10.1145%2f320251.320253&partnerID=40&md5=334fdb1602a1788ec33ff88518f02316,"Aspects of an intelligent interface that provides natural language access to a large body of data distributed over a computer network are described. The overall system architecture is presented, showing how a user is buffered from the actual database management systems (DBMSs) by three layers of insulating components. These layers operate in series to convert natural language queries into calls to DBMSs at remote sites. Attention is then focused on the first of the insulating components, the natural language system. A pragmatic approach to language access that has proved useful for building interfaces to databases is described and illustrated by examples. Special language features that increase system usability, such as spelling correction, processing of incomplete inputs, and run-time system personalization, are also discussed. The language system is contrasted with other work in applied natural language processing, and the system's limitations are analyzed. © 1978, ACM. All rights reserved.",database access; human engineering; intelligent interface; natural language; run-time personalization; semantic grammar,DATA BASE SYSTEMS; COMPUTER INTERFACES
Search Strategy and Selection Function for an Inferential Relational System,1978,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0017947787&doi=10.1145%2f320241.320242&partnerID=40&md5=e3b87bc5b7daeb000f9328aaf00b6841,"An inferential relational system is one in which data in the system consists of both explicit facts and general axioms (or “views”). The general axioms are used together with the explicit facts to derive the facts that are implicit (virtual relations) within the system. A top-down algorithm, as used in artificial intelligence work, is described to develop inferences within the system. The top-down approach starts with the query, a conjunction of relations, to be answered. Either a relational fact solves a given relation in a conjunct, or the relation is replaced by a conjunct of relations which must be solved to solve the given relation. The approach requires that one and only one relation in a conjunction be replaced (or expanded) by the given facts and general axioms. The decision to expand only a single relation is termed a selection function. It is shown for relational systems that such a restriction still guarantees that a solution to the problem will be found if one exists. The algorithm provides for heuristic direction in the search process. Experimental results are presented which illustrate the techniques. A bookkeeping mechanism is described which permits one to know when subproblems are solved. It further facilitates the outputting of reasons for the deductively found answer in a coherent fashion. © 1978, ACM. All rights reserved.",answer and reason extraction; heuristics; inference mechanism; logic; predicate calculus; relational databases; search strategy; selection function; top-down search; virtual relations,DATA BASE SYSTEMS
Process Synchronization in Database Systems,1978,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0018016760&doi=10.1145%2f320263.320279&partnerID=40&md5=eb91c2f0b7b62ec1d22c0b672fd08397,"The problem of process synchronization in database systems is analyzed in a strictly systematic way, on a rather abstract level; the abstraction is chosen such that the essential characteristics of the problem can be distinctly modeled and investigated. Using a small set of concepts, a consistent description of the whole problem is developed; many widely used, but only vaguely defined, notions are defined exactly within this framework. The abstract treatment of the problem immediately leads to practically useful insights with respect to possible solutions, although implementational aspects are not discussed in detail. © 1978, ACM. All rights reserved.",database consistency; database systems; integrity; locking; parallel process systems; process synchronization,DATA BASE SYSTEMS
On the Estimation of the Number of Desired Records with Respect to a Given Query,1978,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0017949980&doi=10.1145%2f320241.320245&partnerID=40&md5=3627e055665f8a6773ff1193498a976b,"The importance of the estimation of the number of desired records for a given query is outlined. Two algorithms for the estimation in the “closest neighbors problem” are presented. The numbers of operations of the algorithms are Ο(ml2) and Ο(ml), where m is the number of clusters and l is the “length” of the query. © 1978, ACM. All rights reserved.",closest neighbors; database; estimate; query,INFORMATION SCIENCE
Data File Management in Shift-Register Memories,1978,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0017982086&doi=10.1145%2f320251.320256&partnerID=40&md5=fd9ad2854907f03da632d6bbc5174636,"The paper proposes a shift-register memory, structured as a two-dimensional array of uniform shift-register loops which are linked by flow-steering switches, whose switch control scheme is tailored to perform with great efficiency data management operations on sequentially organized files. The memory operates in a linear input/output mode to perform record insertion, deletion, and relocation on an existing file, and in a sublinear mode for rapid internal file movement to expedite file positioning and record retrieval and update operations. The memory, implemented as a large capacity charge-coupled device or magnetic domain memory, permits efficient data management on very large databases at the level of secondary storage and lends itself to applications as a universal disk replacement, particularly in database computers. © 1978, ACM. All rights reserved.",data transformations; deletion; insertion; LIFO/FIFO operation modes; management of sequentially organized files; record retrieval; relocation; shift-register memories; updating,COMPUTER OPERATING SYSTEMS
Security in Statistical Databases for Queries with Small Counts,1978,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0017943805&doi=10.1145%2f320241.320250&partnerID=40&md5=d9e04c5653696b2f34384b49b3a416fc,"The security problem of statistical databases containing anonymous but individual records which may be evaluated by queries about sums and averages is considered. A model, more realistic than the previous ones, is proposed, in which nonexisting records for some keys can be allowed. Under the assumption that the system protects the individual's information by the well-known technique which avoids publishing summaries with small counts, several properties about the system and a necessary and sufficient condition for compromising the database have been derived. The minimum number of queries needed to compromise the database is also discussed. © 1978, ACM. All rights reserved.",compromisability; data security; protection; statistical databases,DATA PROCESSING - Security of Data; DATA BASE SYSTEMS
The difficulty of optimum index selection,1978,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976836562&doi=10.1145%2f320289.320296&partnerID=40&md5=86947b75353b1c95c4e11bf3f8c25d74,"Given a file on a secondary store in which each record has several attributes, it is usually advantageous to build an index mechanism to decrease the cost of conducting transactions to the file. The problem of selecting attributes over which to index has been studied in the context of various storage structures and access assumptions. One algorithm to make an optimum index selection requires 2k steps in the worst case, where k is the number of attributes in the file. We examine the question of whether a more efficient algorithm might exist and show that even under a simple cost criterion the problem is computationally difficult in a precise sense. Our results extend directly to other related problems where the cost of the index depends on fixed values which are assigned to each attribute. Some practical implications are discussed. © 1978, ACM. All rights reserved.",attribute selection; complexity; index selection; secondary index,
Optimum Reorganization Points for Linearly Growing Files,1978,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0017942954&doi=10.1145%2f320241.320244&partnerID=40&md5=22ae4f91f4b10e473c5c7054256888dd,"The problem of finding optimal reorganization intervals for linearly growing files is solved. An approximate reorganization policy, independent of file lifetime, is obtained. Both the optimum and approximate policies are compared to previously published results using a numerical example. © 1978, ACM. All rights reserved.",database; file organization; optimization; reorganization,DATA PROCESSING - File Organization; DATA BASE SYSTEMS
Implementing a relational database by means of specialzed hardware,1979,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0018443029&doi=10.1145%2f320064.320065&partnerID=40&md5=f422c1f59f99a132b65addc31436f853,"New hardware is described which allows the rapid execution of queries demanding the joining of physically stored relations. The main feature of the hardware is a special store which can rapidly remember or recall data. This data might be pointers from one file to another, in which case the memory helps with queries on joins of files. Alternatively, the memory can help remove redundant data during projection, giving a considerable speed advantage over conventional hardware. © 1979, ACM. All rights reserved.",bit array; CAFS; content addressing; database; hashing; information retrieval; join; projection; relational model; selection; special hardware,DATA BASE SYSTEMS
CASDAL: CASSM's DAta Language,1978,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0017943804&doi=10.1145%2f320241.320246&partnerID=40&md5=33ad905cde5c01d9b5c3d297e9a90345,"CASDAL is a high level data language designed and implemented for the database machine CASSM. The language is used for the manipulation and maintenance of a database using an unnormalized (hierarchically structured) relational data model. It also has facilities to define, modify, and maintain the data model definition. The uniqueness of CASDAL lies in its power to specify complex operations in terms of several new language constructs and its concepts of tagging or marking tuples and of matching values when walking from relation to relation. The language is a result of a top-down design and development effort for a database machine in which high level language constructs are directly supported by the hardware. This paper (1) gives justifications for the use of an unnormalized relational model on which the language is based, (2) presents the CASDAL language constructs with examples, and (3) describes CASSM's architecture and hardware primitives which match closely with the high level language constructs and facilitate the translation process. This paper also attempts to show how the efficiency of the language and the translation task can be achieved and simplified in a system in which the language is the result of a top-down system design and development. © 1978, ACM. All rights reserved.",associative memory; database; nonprocedural language; query language; relational model,COMPUTER PROGRAMMING LANGUAGES; DATA LANGUAGE; DATA BASE SYSTEMS
An extended owner-coupled set data model and predicate calculus for database management,1978,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0018057103&doi=10.1145%2f320289.320294&partnerID=40&md5=100d5d6d1a88e59fc4ca13ddb5bc9c8e,"A data model is presented, based on the extension of the concept of a DBTG owner-coupled set to permit static and dynamic sets and a new kind of set referred to as a virtual set. The notion of connection fields is introduced, and it is shown how connection fields may be used to construct derived information bearing set names, and hence permit the specification of (dynamic) sets which are not predeclared in a schema. Virtual sets are shown to reflect the functional dependencies which can exist within a file. A technique which permits the data model to be fully described diagrammatically by extended Bachman diagrams is described. A predicate calculus for manipulation of this data model is presented. Expressions written in this calculus are compared with corresponding expressions in a relational predicate calculus, DSL ALPHA. An argument for the relational completeness of the language is given. © 1978, ACM. All rights reserved.",Codasyl DBTG; connection field; DSL AlPHA; dynamic set; extended Bachman diagram; extended owner-coupled set data model; extended owner-coupled set predicate calculus; functional dependency; information bearing set name; owner-coupled set; static set; virtual set,DATA BASE SYSTEMS
Concepts and Capabilities of a Database Computer,1978,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0018056771&doi=10.1145%2f320289.320292&partnerID=40&md5=d300c1e883f9f0a9da70583383527eb8,"The concepts and capabilities of a database computer (DBC) are given in this paper. The proposed design overcomes many of the traditional problems of database system software and is one of the first to describe a complete data-secure computer capable of handling large databases. This paper begins by characterizing the major problems facing today's database system designers. These problems are intrinsically related to the nature of conventional hardware and can only be solved by introducing new architectural concepts. Several such concepts are brought to bear in the later sections of this paper. These architectural principles have a major impact upon the design of the system and so they are discussed in some detail. A key aspect of these principles is that they can be implemented with near-term technology. The rest of the paper is devoted to the functional characteristics and the theory of operation of the DBC. The theory of operation is based on a series of abstract models of the components and data structures employed by the DBC. These models are used to illustrate how the DBC performs access operations, manages data structures and security specifications, and enforces security requirements. Short Algol-like algorithms are used to show how these operations are carried out. This part of the paper concludes with a high-level description of the DBC organization. The actual details of the DBC hardware are quite involved and so their presentation is not the subject of this paper. A sample database is included in the Appendix to illustrate the working of the security and clustering mechanisms of the DBC. © 1978, ACM. All rights reserved.",author-keywords; clustering; content-addressable memory; database computers; mass memory; performance; security; structure memory,DATA BASE SYSTEMS; COMPUTER ARCHITECTURE
On an Authorization Mechanism,1978,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0018008817&doi=10.1145%2f320263.320288&partnerID=40&md5=3cc9736b5836524f43c4638be23c75cf,"Griffiths and Wade (ACM Trans. Database Syst. 1,3, (Sept. 1976), 242-255) have defined a dynamic authorization mechanism that goes beyond the traditional password approach. A database user can grant or revoke privileges (such as to read, insert, or delete) on a file that he has created. Furthermore, he can authorize others to grant these same privileges. The database management system keeps track of a directed graph, emanating from the creator, of granted privileges. The nodes of the graph correspond to users, and the edges (each of which is labeled with a timestamp) correspond to grants. The edges are of two types, corresponding to whether or not the recipient of the grant has been given the option to make further grants of this privilege. Furthermore, for each pair A, B of nodes, there can be no more than one edge of each type from A to B. We modify this approach by allowing graphs in which there can be multiple edges of each type from one node to another. We prove correctness (in a certain strong sense) for our modified authorization mechanism. Further, we show by example that under the original mechanism, the system might forbid some user from exercising or granting a privilege that he “should” be allowed to exercise or grant. © 1978, ACM. All rights reserved.",access control; authorization; database; privacy; proof of correctness; protection; revocation; security,DATA BASE SYSTEMS
Specialized Merge Processor Networks for Combining Sorted Lists,1978,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0018016661&doi=10.1145%2f320263.320281&partnerID=40&md5=f7012e998ab6eac0ff89bd4989a59df2,"In inverted file database systems, index lists consisting of pointers to items within the database are combined to form a list of items which potentially satisfy a user's query. This list merging is similar to the common data processing operation of combining two or more sorted input files to form a sorted output file, and generally represents a large percentage of the computer time used by the retrieval system. Unfortunately, a general purpose digital computer is better suited for complicated numeric processing rather than the simple combining of data. The overhead of adjusting and checking pointers, aligning data, and testing for completion of the operation overwhelm the processing of the data. A specialized processor can perform most of these overhead operations in parallel with the processing of the data, thereby offering speed increases by a factor from 10 to 100 over conventional computers, depending on whether a higher speed memory is used for storing the lists. These processors can also be combined into networks capable of directly forming the result of a complex expression, with another order of magnitude speed increase possible. The programming and operation of these processors and networks is discussed, and comparisons are made with the speed and efficiency of conventional general purpose computers. © 1978, ACM. All rights reserved.",backend processors; binary tree networks; computer system architecture; full text retrieval systems; inverted file databases; nonnumeric processing; pipelined networks; sorted list merging,COMPUTER ARCHITECTURE - Program Processors; DATA BASE SYSTEMS
The tracker: A threat to statistical database security,1979,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0018442066&doi=10.1145%2f320064.320069&partnerID=40&md5=035514be65a054bef1a8554602191759,"The query programs of certain databases report raw statistics for query sets, which are groups of records specified implicitly by a characteristic formula. The raw statistics include query set size and sums of powers of values in the query set. Many users and designers believe that the individual records will remain confidential as long as query programs refuse to report the statistics of query sets which are too small. It is shown that the compromise of small query sets can in fact almost always be accomplished with the help of characteristic formulas called trackers. Schlörer's individual tracker is reviewed; it is derived from known characteristics of a given individual and permits deducing additional characteristics he may have. The general tracker is introduced: It permits calculating statistics for arbitrary query sets, without requiring preknowledge of anything in the database. General trackers always exist if there are enough distinguishable classes of individuals in the database, in which case the trackers have a simple form. Almost all databases have a general tracker, and general trackers are almost always easy to find. Security is not guaranteed by the lack of a general tracker. © 1979, ACM. All rights reserved.",confidentiality; data security; database security; secure query functions; statistical database; tracker,DATA BASE SYSTEMS; DATA PROCESSING
A Note on Associative Processors for Data Management,1978,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0017982808&doi=10.1145%2f320251.320254&partnerID=40&md5=412bfd3d66016ea85d6d4c9f202bad6c,"Associative “logic-per-track” processors for data management are examined from a technological and engineering point of view. Architectural and design decisions are discussed. Some alternatives to the design of comparators, garbage collection, and domain extraction for architectures like the Relational Associative Processor (RAP) are offered. © 1978, ACM. All rights reserved.",associative processors; database machines,DATA BASE SYSTEMS; COMPUTER OPERATING SYSTEMS
Validation Algorithms for Pointer Values in DBTG Databases,1977,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0017723517&doi=10.1145%2f320576.320589&partnerID=40&md5=3dc500c8182eaa87a892483b20dff406,"This paper develops algorithms for verifying pointer values in DBTG (Data Base Task Group) type databases. To validate pointer implemented access paths and set structures, two algorithms are developed. The first procedure exploits the “typed pointer” concept employed in modern programming languages to diagnose abnormalities in directories and set instances. The second algorithm completes pointer validation by examining set instances to ensure that each DBTG set has a unique owner. Sequential processing is used by both algorithms, allowing a straightforward implementation which is efficient in both time and space. As presented, the algorithms are independent of implementation schema and physical structure. © 1977, ACM. All rights reserved. © 1977, ACM. All rights reserved.",database integrity; database utilities; type checking; validation,DATA BASE SYSTEMS
A Transformational Grammar-Based Query Processor for Access Control in a Planning System,1977,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0017635457&doi=10.1145%2f320576.320582&partnerID=40&md5=de23f2a161ae1dcde0232f06924ecbf4,"Providing computer facilities and data availability to larger numbers of users generates increased system vulnerability which is partially offset by software security systems. Much too often these systems are presented as ad hoc additions to the basic data management system. One very important constituent of software security systems is the access control mechanism which may be the last resource available to prohibit unauthorized data retrieval. This paper presents a specification for an access control mechanism. The mechanism is specified in a context for use with the GPLAN decision support system by a theoretical description consistent with the formal definition of GPLAN's query language. Incorporation of the mechanism into the language guarantees it will not be an ad hoc addition. Furthermore, it provides a facile introduction of data security dictates into the language processor. © 1977, ACM. All rights reserved. © 1977, ACM. All rights reserved.",access control; data security; database; decision support system; planning system,DATA PROCESSING
Minimum Cost Selection of Secondary Indexes for Formatted Files,1977,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976803190&doi=10.1145%2f320521.320537&partnerID=40&md5=d3d469c4d728135af9553591985697a8,"Secondary indexes are often used in database management systems for secondary key retrieval. Although their use can improve retrieval time significantly, the cost of index maintenance and storage increases the overhead of the file processing application. The optimal set of indexed secondary keys for a particular application depends on a number of application dependent factors. In this paper a cost function is developed for the evaluation of candidate indexing choices and applied to the optimization of index selection. Factors accounted for include file size, the relative rates of retrieval and maintenance and the distribution of retrieval and maintenance over the candidate keys, index structure, and system charging rates. Among the results demonstrated are the increased effectiveness of secondary indexes for large files, the effect of the relative rates of retrieval and maintenance, the greater cost of allowing for arbitrarily formulated queries, and the impact on cost of the use of different index structures. © 1977, ACM. All rights reserved.",access methods; access path; Boolean query; cost function; data management; database; file design; file organization; inverted file; inverted index; maintenance; optimization; retrieval; secondary index; secondary key; secondary key access,
Performance Evaluation of a Relational Associative Processor,1977,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976786412&doi=10.1145%2f320544.320553&partnerID=40&md5=7e13f3a4e8daab10f2293e34d421e45d,"An associative processor called RAP has been designed to provide hardware support for the use and manipulation of databases. RAP is particularly suited for supporting relational databases. In this paper, the relational operations provided by the RAP hardware are described, and a representative approach to providing the same relational operations with conventional software and hardware is devised. Analytic models are constructed for RAP and the conventional system. The execution times of several of the operations are shown to be vastly improved with RAP for large relations. © 1977, ACM. All rights reserved. © 1977, ACM. All rights reserved.",associative processors; database machines; performance evaluation; relational databases,
An Extension of the Performance of a Database Manager in a Virtual Memory System Using Partially Locked Virtual Buffers,1977,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976713137&doi=10.1145%2f320544.320556&partnerID=40&md5=94e726fe3cbf2e04cfaccff9aea49128,"Buffer pools are created and managed in database systems in order to reduce the total number of accesses to the I/O devices. In systems using virtual memory, any reduction in I/O accesses may be accompanied by an increase in paging. The effects of these factors on system performance are quantified, where system performance is a function of page faults and database accesses to the I/O devices. A previous study of this phenomenon is extended through the analysis of empirical data gathered in a multifactor experiment. In this study memory is partitioned between the program and the buffer so that the impact of the controlled factors can be more effectively evaluated. It is possible to improve system performance through the use of different paging algorithms in the program partition and the buffer partition. Also, the effects on system performance as the virtual buffer size is increased beyond the real memory allocated to the buffer partition are investigated. © 1977, ACM. All rights reserved. © 1977, ACM. All rights reserved.",buffer manager; database management; double paging; locked buffer; page faults; page replacement algorithm; performance; virtual buffer; virtual memory,
The Determination of Efficient Record Segmentations and Blocking Factors for Shared Data Files,1977,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0017537522&doi=10.1145%2f320557.320574&partnerID=40&md5=836c03ed6aee6556a5e560f9b964a660,"It is generally believed that 80 percent of all retrieval from a commercial database is directed at only 20 percent of the stored data items. By partitioning data items into primary and secondary record segments, storing them in physically separate files, and judiciously allocating available buffer space to the two files, it is possible to significantly reduce the average cost of information retrieval from a shared database. An analytic model, based upon knowledge of data item lengths, data access costs, and user retrieval patterns, is developed to assist an analyst with this assignment problem. A computationally tractable design algorithm is presented and results of its application are described. © 1977, ACM. All rights reserved. © 1977, ACM. All rights reserved.",bicriterion mathematical programs; branch and bound; buffer allocation; data management; network flows; record design; record segmentation,INFORMATION SCIENCE - Information Retrieval; DATA BASE SYSTEMS
A Facility for Defining and Manipulating Generalized Data Structures,1977,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0017631234&doi=10.1145%2f320576.320591&partnerID=40&md5=7963a26d648eb73e9526c31f3abd8fd5,"A data structure definition facility (DSDF) is described that provides definitions for several primitive data types, homogeneous and heterogeneous arrays, cells, stacks, queues, trees, and general lists. Each nonprimitive data structure consists of two separate entities—a head and a body. The head contains the entry point(s) to the body of the structure; by treating the head like a cell, the DSDF operations are capable of creating and manipulating very general data structures. A template structure is described that permits data structures to share templates. The primary objectives of the DSDF are: (1) to develop a definition facility that permits the programmer to explicitly define and manipulate generalized data structures in a consistent manner, (2) to detect mistakes and prevent the programmer from creating (either inadvertently or intentionally) undesirable (or illegal) data structures, (3) to provide a syntactic construction mechanism that separates the implementation of a data structure from its use in the program in which it is defined, and (4) to facilitate the development of reliable software. © 1977, ACM. All rights reserved. © 1977, ACM. All rights reserved.",data definition languages; data structure definition facility; data structures; database management,DATA PROCESSING
An Authorization Mechanism for a Relational Database System,1976,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976657250&doi=10.1145%2f320473.320482&partnerID=40&md5=318ff66387d732ce6cf48ebd3a7c5568,"A multiuser database system must selectively permit users to share data, while retaining the ability to restrict data access. There must be a mechanism to provide protection and security, permitting information to be accessed only by properly authorized users. Further, when tables or restricted views of tables are created and destroyed dynamically, the granting, authentication, and revocation of authorization to use them must also be dynamic. Each of these issues and their solutions in the context of the relational database management system System R are discussed. When a database user creates a table, he is fully and solely authorized to perform upon it actions such as read, insert, update, and delete. He may explicitly grant to any other user any or all of his privileges on the table. In addition he may specify that that user is authorized to further grant these privileges to still other users. The result is a directed graph of granted privileges originating from the table creator. At some later time a user A may revoke some or all of the privileges which he previously granted to another user B. This action usually revokes the entire subgraph of the grants originating from A's grant to B. It may be, however, that B will still possess the revoked privileges by means of a grant from another user C, and therefore some or all of B's grants should not be revoked. This problem is discussed in detail, and an algorithm for detecting exactly which of B's grants should be revoked is presented. © 1976, ACM. All rights reserved.",access control; authorization; data dependent authorization; database systems; privacy; protection in databases; revocation of authorization; security,
Databsse System Approach the Management Decision Support,1976,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976863559&doi=10.1145%2f320493.320500&partnerID=40&md5=5b888b4bb9144b3e508c5de200daf5ac,"Traditional intuitive methods of decision-making are no longer adequate to deal with the complex problems faced by the modern policymaker. Thus systems must be developed to provide the information and analysis necessary for the decisions which must be made. These systems are called decision support systems. Although database systems provide a key ingredient to decision support systems, the problems now facing the policymaker are different from those problems to which database systems have been applied in the past. The problems are usually not known in advance, they are constantly changing, and answers are needed quickly. Hence additional technologies, methodologies, and approaches must expand the traditional areas of database and operating systems research (as well as other software and hardware research) in order for them to become truly effective in supporting policymakers. This paper describes recent work in this area and indicates where future work is needed. Specifically the paper discusses: (1) why there exists a vital need for decision support systems; (2) examples from work in the field of energy which make explicit the characteristics which distinguish these decision support systems from traditional operational and managerial systems; (3) how an awareness of decision support systems has evolved, including a brief review of work done by others and a statement of the computational needs of decision support systems which are consistent with contemporary technology; (4) an approach which has been made to meet many of these computational needs through the development and implementation of a computational facility, the Generalized Management Information System (GMIS); and (5) the application of this computational facility to a complex and important energy problem facing New England in a typical study within the New England Energy Management Information System (NEEMIS) Project. © 1976, ACM. All rights reserved.",database systems; decision support systems; management applications; modeling; networking; relational; virtual machines,
Differential Files: Their Application to the Maintenance of Large Databases,1976,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976829162&doi=10.1145%2f320473.320484&partnerID=40&md5=48e442b00035deb327682c1fb26c7731,"The representation of a collection of data in terms of its differences from some preestablished point of reference is a basic storage compaction technique which finds wide applicability. This paper describes a differential database representation which is shown to be an efficient method for storing large and volatile databases. The technique confines database modifications to a relatively small area of physical storage and as a result offers two significant operational advantages. First, because the “reference point” for the database is inherently static, it can be simply and efficiently stored. Second, since all modifications to the database are physically localized, the process of backup and the process of recovery are relatively fast and inexpensive. © 1976, ACM. All rights reserved.",backup and recovery; data sharing; database maaintenance; differential files,
Batched searching of sequential and tree structured files,1976,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976669473&doi=10.1145%2f320473.320487&partnerID=40&md5=5cd0169648ea860bb677d39ab4c4502f,"The technique of batching searches has been ignored in the context of disk based online data retrieval systems. This paper suggests that batching be reconsidered for such systems since the potential reduction in processor demand may actually reduce response time. An analysis with sample numerical results and algorithms is presented. © 1976, ACM. All rights reserved.",B-trees; batched searching; data structures; database systems; file management; indexes; informational retrieval; sequential files; tree structures,
"Optimal Policy for Batch Operations: Backup, Checkpointing, Reorganization, and Updating",1977,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0017532566&doi=10.1145%2f320557.320558&partnerID=40&md5=1e066f1d8752614164023c2e43d92fa3,"Many database maintenance operations are performed periodically in batches, even in realtime systems. The purpose of this paper is to present a general model for determining the optimal frequency of these batch operations. Specifically, optimal backup, checkpointing, batch updating, and reorganization policies are derived. The approach used exploits inventory parallels by seeking the optimal number of items—rather than a time interval—to trigger a batch. The Renewal Reward Theorem is used to find the average long run costs for backup, recovery, and item storage, per unit time, which is then minimized to find the optimal backup policy. This approach permits far less restrictive assumptions about the update arrival process than did previous models, as well as inclusion of storage costs for the updates. The optimal checkpointing, batch updating, and reorganization policies are shown to be special cases of this optimal backup policy. The derivation of previous results as special cases of this model, and an example, demonstrate the generality of the methodology developed. © 1977, ACM. All rights reserved. © 1977, ACM. All rights reserved.",backup frequency; batch operations; batch update; checkpoint interval; database maintenance; file reorganization; inventory theory; real-time systems; renewel theory,DATA BASE SYSTEMS
"EXPRESS: A Data EXtraction, Processing, and REStructuring System",1977,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976700402&doi=10.1145%2f320544.320549&partnerID=40&md5=99ade7cb28f000adf140cfca978ac9bc,"EXPRESS is an experimental prototype data translation system which can access a wide variety of data and restructure it for new uses. The system is driven by two very high level nonprocedural languages: DEFINE for data description and CONVERT for data restructuring. Program generation and cooperating process techniques are used to achieve efficient operation. This paper describes the design and implementation of EXPRESS. DEFINE and CONVERT are summarized and the implementation architecture presented. The DEFINE description is compiled into a customized PL/1 program for accessing source data. The restructuring specified in CONVERT is compiled into a set of customized PL/1 procedures to derive multiple target files from multiple input files. Job steps and job control statements are generated automatically. During execution, the generated procedures run under control of a process supervisor, which coordinates buffer management and handles file allocation, deallocation, and all input/output requests. The architecture of EXPRESS allows efficiency in execution by avoiding unnecessary secondary storage references while at the same time allowing the individual procedures to be independent of each other. Its modular structure permits the system to be extended or transferred to another environment easily. © 1977, ACM. All rights reserved. © 1977, ACM. All rights reserved.",data conversion; data description languages; data manipulation languages; data restructuring; data translation; file conversion; program generation; very high level languages,
Synthesizing Third Normal Form Relations from Functional Dependencies,1976,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976728813&doi=10.1145%2f320493.320489&partnerID=40&md5=89312db4ac16d6efea639e4e7a9e02b5,"It has been proposed that the description of a relational database can be formulated as a set of functional relationships among database attributes. These functional relationships can then be used to synthesize algorithmically a relational scheme. It is the purpose of this paper to present an effective procedure for performing such a synthesis. The schema that results from this procedure is proved to be in Codd's third normal form and to contain the fewest possible number of relations. Problems with earlier attempts to construct such a procedure are also discussed. © 1976, ACM. All rights reserved.",database schema; functional dependency; relational model; semantics of data; third normal form,
On User Criteria for Data Model Evaluation,1976,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976767537&doi=10.1145%2f320493.320504&partnerID=40&md5=ad11691da06fc7e8cb1d5e5fe8eab502,"The emergence of a database technology in recent years has focused interest on the subject of data models. A data model is the class of logical data structures which a computer system or language makes available to the user for the purpose of formulating data processing applications. The diversity of computer systems and languages has resulted in a corresponding diversity of data models, and has created a problem for the user in selecting a data model which is in some sense appropriate to a given application. An evaluation procedure is needed which will allow the user to evaluate alternative models in the context of a specific set of applications. This paper takes a first step toward such a procedure by identifying the attributes of a data model which can be used as criteria for evaluating the model. Two kinds of criteria are presented: use criteria, which measure the usability of the model; and implementation criteria, which measure the implementability of the model and the efficiency of the resulting implementation. The use of the criteria is illustrated by applying them to three specific models: an n-ary relational model, a hierarchic model, and a network model. © 1976, ACM. All rights reserved.",data model; data model evaluation; data model selection; hierarchic model; network model; relational model,
Analysis of aarchitectural features for enhancing the performance of a database machine,1977,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0017677820&doi=10.1145%2f320576.320577&partnerID=40&md5=606bdca5ae7232c7e83c9ec4ce724b04,"RAP (Relational Associative Processor) is a “back-end” database processor that is intended to take over much of the effort of database management in a computer system. In order to enhance RAP's performance its design includes mechanisms for permitting features analogous to multiprogramming and virtual memory as in general purpose computer systems. It is the purpose of this paper to present the detailed design of these mechanisms, along with some analysis that supports their value. Specifically, (1) the response time provided by RAP under several scheduling disciplines involving priority by class is analyzed, (2) the cost effectiveness of the additional hardware in RAP necessary to support multiprogramming is assessed, and (3) a detailed design of the RAP virtual memory system and its monitor is presented. © 1977, ACM. All rights reserved. © 1977, ACM. All rights reserved.",associative processors; computer architecture; database machines; database management,COMPUTER ARCHITECTURE; DATA BASE SYSTEMS
Database Abstractions: Aggregation and Generalization,1977,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976682945&doi=10.1145%2f320544.320546&partnerID=40&md5=e79b224fc692320ce6aeafa338de20f2,"Two kinds of abstraction that are fundamentally important in database design and usage are defined. Aggregation is an abstraction which turns a relationship between objects into an aggregate object. Generalization is an abstraction which turns a class of objects into a generic object. It is suggested that all objects (individual, aggregate, generic) should be given uniform treatment in models of the real world. A new data type, called generic, is developed as a primitive for defining such models. Models defined with this primitive are structured as a set of aggregation hierarchies intersecting with a set of generalization hierarchies. Abstract objects occur at the points of intersection. This high level structure provides a discipline for the organization of relational databases. In particular this discipline allows: (i) an important class of views to be integrated and maintained; (ii) stability of data and programs under certain evolutionary changes; (iii) easier understanding of complex models and more natural query formulation; (iv) a more systematic approach to database design; (v) more optimization to be performed at lower implementation levels. The generic type is formalized by a set of invariant properties. These properties should be satisfied by all relations in a database if abstractions are to be preserved. A triggering mechanism for automatically maintaining these invariants during update operations is proposed. A simple mapping of aggregation/generalization hierarchies onto owner-coupled set structures is given. © 1977, ACM. All rights reserved. © 1977, ACM. All rights reserved.",aggregation; data abstraction; data model; data type; database design; generalization; integrity constraints; knowledge representation; relational database,
Database Buffer Paging in Virtual Storage Systems,1977,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0017721084&doi=10.1145%2f320576.320585&partnerID=40&md5=e6086ad5fba7e686ddddc4af465ef5f9,"Three models, corresponding to different sets of assumptions, are analyzed to study the behavior of a database buffer in a paging environment. The models correspond to practical situations and vary in their search strategies and replacement algorithms. The variation of I/O cost with respect to buffer size is determined for the three models. The analysis is valid for arbitrary database and buffer sizes, and the I/O cost is obtained in terms of the miss ratio, the buffer size, the number of main memory pages available for the buffer, and the relative buffer and database access costs. © 1977, ACM. All rights reserved. © 1977, ACM. All rights reserved.",buffer management; computer systems performance; database performance; page replacement algorithm; virtual memory,DATA BASE SYSTEMS
Hashing and trie algorithms for partial match retrieval,1976,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976780855&doi=10.1145%2f320455.320469&partnerID=40&md5=f47cd549218efa9debf225d3fe5d613a,"File designs suitable for retrieval from a file of k-letter words when queries may be only partially specified are examined. A new class of partial match file designs (called PMF designs) based upon hash coding and trie search algorithms which provide good worst-case performance is introduced. Upper bounds on the worst-case performance of these designs are given along with examples of files achieving the bound. Other instances of PMF designs are known to have better worst-case performances. The implementation of the file designs with associated retrieval algorithms is considered. The amount of storage required is essentially that required of the records themselves. © 1976, ACM. All rights reserved.",algorithms; analysis; associative retrieval; hash coding; partial match; retrieval; searching; trie search,
Decomposition—a strategy for query processing,1976,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976812728&doi=10.1145%2f320473.320479&partnerID=40&md5=6da51b95631947c967c9e64071529ac1,"Strategy for processing multivariable queries in the database management system INGRES is considered. The general procedure is to decompose the query into a sequence of one-variable queries by alternating between (a) reduction: breaking off components of the query which are joined to it by a single variable, and (b) tuple substitution: substituting for one of the variables a tuple at a time. Algorithms for reduction and for choosing the variable to be substituted are given. In most cases the latter decision depends on estimation of costs; heuristic procedures for making such estimates are outlined. © 1976, ACM. All rights reserved.",connected query; decomposition; detachment; irreducible query; joining (overlapping) variable; query processing; relational database; tuple substitution; variable selection,
Some High Level Language Constructs for Data of Type Relation,1977,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0017532210&doi=10.1145%2f320557.320568&partnerID=40&md5=fbf518f31812d388f3c1849544c7b449,"For the extension of high level languages by data types of mode relation, three language constructs are proposed and discussed: a repetition statement controlled by relations, predicates as a generalization of Boolean expressions, and a constructor for relations using predicates. The language constructs are developed step by step starting with a set of elementary operations on relations. They are designed to fit into PASCAL without introducing too many additional concepts. © 1977, ACM. All rights reserved. © 1977, ACM. All rights reserved.",data type; database; high level language; language extension; nonprocedural language; relational calculus; relational model,COMPUTER PROGRAMMING LANGUAGES
Multivalued Dependencies and a New Normal Form for Relational Databases,1977,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0017532380&doi=10.1145%2f320557.320571&partnerID=40&md5=df2fa1546b4d4bcd1bd6fef502837faf,"A new type of dependency, which includes the well-known functional dependencies as a special case, is defined for relational databases. By using this concept, a new (“fourth”) normal form for relation schemata is defined. This fourth normal form is strictly stronger than Codd's “improved third normal form” (or “Boyce-Codd normal form”). It is shown that every relation schema can be decomposed into a family of relation schemata in fourth normal form without loss of information (that is, the original relation can be obtained from the new relations by taking joins). © 1977, ACM. All rights reserved. © 1977, ACM. All rights reserved.",3NF; 4NF; Boyce-Codd normal form; database design; decomposition; fourth normal form; functional dependency; multivalued dependency; normalization; relatioanl database; third normal form,DATA BASE SYSTEMS
Interval Hierarchies and Their Application to Predicate Files,1977,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0017538369&doi=10.1145%2f320557.320562&partnerID=40&md5=0dacfc86e9e149d038ec048035d0603b,"Predicates are used extensively in modern database systems for purposes ranging from user specification of associative accesses to data, to user-invisible system control functions such as concurrency control and data distribution. Collections of predicates, or predicate files, must be maintained and accessed efficiently. This paper describes a dynamic index, called an interval hierarchy, which supports several important retrieval operations on files of simple conjunctive predicates. Search and maintenance algorithms for interval hierarchies are given. For a file of n predicates, typical of the kind expected in practice, these algorithms require time equal to Ο(log n). © 1977, ACM. All rights reserved. © 1977, ACM. All rights reserved.",concurrency control; database system; distributed data; index; interval; predicate file,DATA BASE SYSTEMS
Algorithms for Parsing Search Queries in Systems with Inverted file Organization,1976,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976702849&doi=10.1145%2f320493.320490&partnerID=40&md5=9c8d54787aff4b224669bdbe5021bcd4,"In an inverted file system a query is in the form of a Boolean expression of index terms. In response to a query the system accesses the inverted lists corresponding to the index terms, merges them, and selects from the merged list those records that satisfy the search logic. Considered in this paper is the problem of determining a Boolean expression which leads to the minimum total merge time among all Boolean expressions that are equivalent to the expression given in the query. This problem is the same as finding an optimal merge tree among all trees that realize the truth function determined by the Boolean expression in the query. Several algorithms are described which generate optimal merge trees when the sizes of overlaps between different lists are small compared with the length of the lists. © 1976, ACM. All rights reserved.",inverted file systems; merge algorithms; parsing Boolean queries,
Independent Components of Relations,1977,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0017675271&doi=10.1145%2f320576.320580&partnerID=40&md5=10c91065e629e3c82d8dbaf03a2f4a29,"In a multiattribute relation or, equivalently, a multicolumn table a certain collection of the projections can be shown to be independent in much the same way as the factors in a Cartesian product or orthogonal components of a vector. A precise notion of independence for relations is defined and studied. The main result states that the operator which reconstructs the original relation from its independent components is the natural join, and that independent components split the full family of functional dependencies into corresponding component families. These give an easy-to-check criterion for independence. © 1977, ACM. All rights reserved. © 1977, ACM. All rights reserved.",database; functional dependencies; relations,DATA BASE SYSTEMS
An Attribute Based Model for Database Access Cost Analysis,1977,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-30244496829&doi=10.1145%2f320521.320535&partnerID=40&md5=d7f6af1523c020def640a760111dc7af,"A generalized model for physical database organizations is presented. Existing database organizations are shown to fit easily into the model as special cases. Generalized access algorithms and cost equations associated with the model are developed and analyzed. The model provides a general design framework in which the distinguishing properties of database organizations are made explicit and their performances can be compared. © 1977, ACM. All rights reserved.",B-tree; database model; database organization; database performance; evaluation; index organization; index sequential; inverted file; multilist,
Physical Integrity in a Large Segmented Database,1977,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0343624391&doi=10.1145%2f320521.320540&partnerID=40&md5=5e8b8119b8462c801d914faadce31a89,"A database system can generally be divided into three major components. One component supports the logical database as seen by the user. Another component maps the information into physical records. The third component, called the storage component, is responsible for mapping these records onto auxiliary storage (generally disks) and controlling their transfer to and from main storage. This paper is primarily concerned with the implementation of a storage component. It considers a simple and classical interface to the storage component: Seen at this level the database is a collection of segments. Each segment is a linear address space. A recovery scheme is first proposed for system failure (hardware or software error which causes the contents of main storage to be lost). It is based on maintaining a dual mapping between pages and their location on disk. One mapping represents the current state of a segment being modified; the other represents a previous backup state. At any time the backup state can be replaced by the current state without any data merging. Procedures for segment modification, save, and restore are analyzed. Another section proposes a facility for protection against damage to the auxiliary storage itself. It is shown how such protection can be obtained by copying on a tape (checkpoint) only those pages that have been modified since the last checkpoint. © 1977, ACM. All rights reserved.",checkpoint-restart; database; recovery; storage management,
A Clustering Algorithm for Hierarchical Structures,1977,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976691066&doi=10.1145%2f320521.320531&partnerID=40&md5=bd816693a131ff2f1aa5bef440a2f8e5,[No abstract available],access models; clustering techniques; database design; hierarchical model; hierarchical structures; page faults; storage allocation,
Restructuring for large databases: Three levels of abstraction,1976,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976750410&doi=10.1145%2f320455.320461&partnerID=40&md5=fa6009febbfde4a4071fbcba964aff32,"The development of a powerful restructuring function involves two important components—the unambiguous specification of the restructuring operations and the realization of these operations in a software system. This paper is directed to the first component in the belief that a precise specification will provide a firm foundation for the development of restructuring algorithms and, subsequently, their implementation. The paper completely defines the semantics of the restructuring of tree structured databases. The delineation of the restructuring function is accomplished by formulating three different levels of abstraction, with each level of abstraction representing successively more detailed semantics of the function. At the first level of abstraction, the schema modification, three types are identified—naming, combining, and relating; these three types are further divided into eight schema operations. The second level of abstraction, the instance operations, constitutes the transformations on the data instances; they are divided into group operations such as replication, factoring, union, etc., and group relation operations such as collapsing, refinement, fusion, etc. The final level, the item value operations, includes the actual item operations, such as copy value, delete value, or create a null value. © 1976, ACM. All rights reserved.",data definition; data translation; database; database management systems; logical restructuring,
Prefix B-Trees,1977,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976838475&doi=10.1145%2f320521.320530&partnerID=40&md5=5057f623c9c9924534d9960bb6b00d3e,"Two modifications of B-trees are described, simple prefix B-trees and prefix B-trees. Both store only parts of keys, namely prefixes, in the index part of a B*-tree. In simple prefix B-trees those prefixes are selected carefully to minimize their length. In prefix B-trees the prefixes need not be fully stored, but are reconstructed as the tree is searched. Prefix B-trees are designed to combine some of the advantages of B-trees, digital search trees, and key compression techniques while reducing the processing overhead of compression techniques. © 1977, ACM. All rights reserved.",B-trees; key compression; multiway search trees,
System R: Relational approach to database management,1976,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976770519&doi=10.1145%2f320455.320457&partnerID=40&md5=8f6097cb430b59c1ef2193e834120120,"System R is a database management system which provides a high level relational data interface. The systems provides a high level of data independence by isolating the end user as much as possible from underlying storage structures. The system permits definition of a variety of relational views on common underlying data. Data control features are provided, including authorization, integrity assertions, triggered transactions, a logging and recovery subsystem, and facilities for maintaining data consistency in a shared-update environment. This paper contains a description of the overall architecture and design of the system. At the present time the system is being implemented and the design evaluated. We emphasize that System R is a vehicle for research in database architecture, and is not planned as a product. © 1976, ACM. All rights reserved.",authorization; data structures; database; index structures; locking; nonprocedural language; recovery; relational model,
A Model of Statistical Database and Their Security,1977,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-1842510810&doi=10.1145%2f320521.320525&partnerID=40&md5=948f8cb80a36e45be8ad014a8a4922b6,"Considered here, for a particular model of databases in which only information about relatively large sets of records can be obtained, is the question of whether one can from statistical information obtain information about individuals. Under the assumption that the data in the database is taken from arbitrary integers, it is shown that essentially nothing can be inferred. It is also shown that when the values are known to be imprecise in some fixed range, one can often deduce the values of individual records. © 1977, ACM. All rights reserved.",compromisability; data security; linear independence; statistical database; vector spece,
Performance of a Database Manager in a Virtual Memory System,1976,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976751552&doi=10.1145%2f320493.320494&partnerID=40&md5=39df3fba83965874c05598bedf10a2aa,"Buffer space is created and managed in database systems in order to reduce accesses to the I/O devices for database information. In systems using virtual memory any increase in the buffer space may be accompanied by an increase in paging. The effects of these factors on system performance are quantified where system performance is a function of page faults and database accesses to I/O devices. This phenomenon is examined through the analysis of empirical data gathered in a multifactor experiment. The factors considered are memory size, size of buffer space, memory replacement algorithm, and buffer management algorithm. The improvement of system performance through an increase in the size of the buffer space is demonstrated. It is also shown that for certain values of the other factors an increase in the size of the buffer space can cause performance to deteriorate. © 1976, ACM. All rights reserved.",buffer manager; database management; double paging; page faults; page replacement algorithm; performance; virtual buffer; virtual memory,
Effects of Locking Granularity in a Database Management System,1977,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0017537936&doi=10.1145%2f320557.320566&partnerID=40&md5=cdb3fad81487d4f92e2a1de3f5fe4b1d,"Many database systems guarantee some form of integrity control upon multiple concurrent updates by some form of locking. Some “granule” of the database is chosen as the unit which is individually locked, and a lock management algorithm is used to ensure integrity. Using a simulation model, this paper explores the desired size of a granule. Under a wide variety of seemingly realistic conditions, surprisingly coarse granularity is called for. The paper concludes with some implications of these results concerning the viability of so-called “predicate locking”. © 1977, ACM. All rights reserved. © 1977, ACM. All rights reserved.",concurrency; consistency; database management; locking granularity; multiple updates; predicate locks,DATA BASE SYSTEMS
The design and implementation of INGRES,1976,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976702115&doi=10.1145%2f320473.320476&partnerID=40&md5=289433d0b1c1fa966ca971c30ad4a22e,"The currently operational (March 1976) version of the INGRES database management system is described. This multiuser system gives a relational view of data, supports two high level nonprocedural data sublanguages, and runs as a collection of user processes on top of the UNIX operating system for Digital Equipment Corporation PDP 11/40, 11/45, and 11/70 computers. Emphasis is on the design decisions and tradeoffs related to (1) structuring the system into processes, (2) embedding one command language in a general purpose programming language, (3) the algorithms implemented to process interactions, (4) the access methods implemented, (5) the concurrency and recovery control currently provided, and (6) the data structures used for system catalogs and the role of the database administrator. Also discussed are (1) support for integrity constraints (which is only partly operational), (2) the not yet supported features concerning views and protection, and (3) future plans concerning the system. © 1976, ACM. All rights reserved.",concurrency; data integrity; data organization; data sublanguage; database optimization; nonprocedural language; protection; query decompositon; query language; relational database,
A Dynamic Database Reorganization Algorithm,1976,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976699787&doi=10.1145%2f320455.320467&partnerID=40&md5=5d13686526d1f242d2c37a808f8943a7,"Reorganization is necessary in some databases for overcoming the performance deterioration caused by updates. The paper presents a dynamic reorganization algorithm which makes the reorganization decision by measuring the database search costs. Previously, the reorganization intervals could only be determined for linear deterioration and known database lifetime. It is shown that the dynamic reorganization algorithm is near optimum for constant reorganization cost and is superior for increasing reorganization cost. In addition, it can be applied to cases of unknown database lifetime and nonlinear performance deterioration. The simplicity, generality, and efficiency appear to make this good heuristic for database reorganization. © 1976, ACM. All rights reserved.",database; file organization; information retrieval; reorganization,
The Entity-Relationship Model—toward a Unified View of Data,1976,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976766949&doi=10.1145%2f320434.320440&partnerID=40&md5=b1794d3e7a6479d9593e63d21ee32f6b,"A data model, called the entity-relationship model, is proposed. This model incorporates some of the important semantic information about the real world. A special diagrammatic technique is introduced as a tool for database design. An example of database design and description using the model and the diagrammatic technique is given. Some implications for data integrity, information retrieval, and data manipulation are discussed. The entity-relationship model can be used as a basis for unification of different views of data: the network model, the relational model, and the entity set model. Semantic ambiguities in these models are analyzed. Possible ways to derive their views of data from the entity-relationship model are presented. © 1976, ACM. All rights reserved.",Data Base Task Group; data definition and manipulation; data integrity and consistency; data models; database design; entity set model; entity-relationship model; logical view of data; network model; relational model; semantics of data,
The Design of a Rotating Associative Memory for Relational Database Applications,1976,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976846895&doi=10.1145%2f320434.320447&partnerID=40&md5=6f15e685c92402102ffb9d1657c71a22,"The design and motivation for a rotating associative relational store (RARES) is described. RARES is designed to enhance the performance of an optimizing relational query interface by supporting important high level optimization techniques. In particular, it can perform tuple selection operations at the storage device and also can provide a mechanism for efficient sorting. Like other designs for rotating associative stores, RARES contains search logic which is attached to the heads of a rotating head-per-track storage device. RARES is distinct from other designs in that it utilizes a novel “orthogonal” storage layout. This layout allows a high output rate of selected tuples even when a sort order in the stored relation must be preserved. As in certain other designs, RARES can usually output a tuple as soon as it is found to satisfy the selection criteria. However, relative to these designs, the orthogonal layout allows an order of magnitude reduction in the capacity of storage local to the search logic. © 1976, ACM. All rights reserved.",associative memory; content addressability; data organization; head-per-track disks; memory systems; relational database; rotating devices; search logic; sorting technique,
A Database Management Facility for Automatic Generation of Database Managers,1976,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-84909499476&doi=10.1145%2f320434.320452&partnerID=40&md5=c2d593f3ac2761438d26e9f88865e933,"A facility is described for the implementation of database management systems having high degrees of horizontal data independence, i.e. independence from chosen logical properties of a database as opposed to vertical independence from storage structures. The facility consists of a high level language for the specification of virtual database managers, a compiler from this language to a pseudomachine language, and an interpreter for the pseudomachine language. It is shown how this facility can be used to produce efficient database management systems with any degree of both horizontal and vertical data independence. Two key features of this tool are the compilation of tailored database managers from individual schemas and multiple levels of optional binding. © 1976, ACM. All rights reserved.",data independence; database mangement systems,
ACM transactions on database systems: Aim and Scope,1976,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976852496&doi=10.1145%2f320434.320436&partnerID=40&md5=993e8a3854a32d4b2da413bacd3a335e,[No abstract available],,
On the Encipherment of Search Trees and Random Access Files,1976,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976748588&doi=10.1145%2f320434.320445&partnerID=40&md5=6c624a60622e6262d4f879316aae6ace,"The securing of information in indexed, random access files by means of privacy transformations must be considered as a problem distinct from that for sequential files. Not only must processing overhead due to encrypting be considered, but also threats to encipherment arising from updating and the file structure itself must be countered. A general encipherment scheme is proposed for files maintained in a paged structure in secondary storage. This is applied to the encipherment of indexes organized as B-trees; a B-tree is a particular type of multiway search tree. Threats to the encipherment of B-trees, especially relating to updating, are examined, and countermeasures are proposed for each. In addition, the effect of encipherment on file access and update, on paging mechanisms, and on files related to the enciphered index are discussed. Many of the concepts presented may be readily transferred to other forms of multiway index trees and to binary search trees. © 1976, ACM. All rights reserved.",B-trees; cryptography; encipherment; indexed sequential files; indexes; paging; privacy; privacy transformation; protection; random access files; search trees; security,
Optimal Allocation of Resources in Distributed Information Networks,1976,ACM Transactions on Database Systems (TODS),https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976735840&doi=10.1145%2f320434.320449&partnerID=40&md5=6a39995090f756e6efaeb63471fd2615,"The problems of file allocation and capacity assignment in a fixed topology distributed computer network are examined. These two aspects of the design are tightly coupled by means of an average message delay constraint. The objective is to allocate copies of information files to network nodes and capacities to network links so that a minimum cost is achieved subject to network delay and file availability constraints. A model for solving the problem is formulated and the resulting optimization problem is shown to fall into a class of nonlinear integer programming problems. Deterministic techniques for solving this class of problems are computationally cumbersome, even for small size problems. A new heuristic algorithm is developed, which is based on a decomposition technique that greatly reduces the computational complexity of the problem. Numerical results for a variety of network configurations indicate that the heuristic algorithm, while not theoretically convergent, yields practicable low cost solutions with substantial savings in computer processing time and storage requirements. Moreover, it is shown that this algorithm is capable of solving realistic network problems whose solutions using deterministic techniques are computationally intractable. © 1976, ACM. All rights reserved.",data files; distributed computing; information networks; link capacities; resource sharing,
