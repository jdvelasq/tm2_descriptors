Title,Year,Source title,Link,Abstract,Author Keywords,Index Keywords
Toward Personalized Affect-Aware Socially Assistive Robot Tutors for Long-Term Interventions with Children with Autism,2022,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141078889&doi=10.1145%2f3526111&partnerID=40&md5=812f9fdfcc8d4a85c6a6340bd5a364fe,"Affect-aware socially assistive robotics (SAR) has shown great potential for augmenting interventions for children with autism spectrum disorders (ASD). However, current SAR cannot yet perceive the unique and diverse set of atypical cognitive-affective behaviors from children with ASD in an automatic and personalized fashion in long-term (multi-session) real-world interactions. To bridge this gap, this work designed and validated personalized models of arousal and valence for children with ASD using a multi-session in-home dataset of SAR interventions. By training machine learning (ML) algorithms with supervised domain adaptation (s-DA), the personalized models were able to tradeoff between the limited individual data and the more abundant less personal data pooled from other study participants. We evaluated the effects of personalization on a long-term multimodal dataset consisting of four children with ASD with a total of 19 sessions, and derived inter-rater reliability (IR) scores for binary arousal (IR = 83%) and valence (IR = 81%) labels between human annotators. Our results show that personalized Gradient Boosted Decision Trees (XGBoost) models with s-DA outperformed two non-personalized individualized and generic model baselines not only on the weighted average of all sessions, but also statistically (p < .05) across individual sessions. This work paves the way for the development of personalized autonomous SAR systems tailored toward individuals with atypical cognitive-affective and socio-emotional needs. © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",affective computing; autism spectrum disorders; Human-robot interaction; personalized machine learning; socially assistive robotics,Bridges; Cognitive systems; Decision trees; Diseases; Economic and social effects; Machine learning; Affective Computing; Assistive robotics; Atypicals; Autism spectrum disorders; Children with autisms; Humans-robot interactions; Interrater reliability; Machine-learning; Personalized machine learning; Socially assistive robotic; Human robot interaction
"Examining Attachment to Robots: Benefits, Challenges, and Alternatives",2022,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141038111&doi=10.1145%2f3526105&partnerID=40&md5=431de19961449a0aaf97f86814c7aeaf,"Potential applications of robots in private and public human spaces have prompted the design of so-called ""social robots""that can interact with humans in social settings and potentially cause humans to attach to the robots. The focus of this article is an analysis of possible benefits and challenges arising from such human-robot attachment as reported in the HRI literature, followed by guidelines for the use and the design of robots that might elicit attachment bonds. We start by analyzing the potential benefits for humans becoming attached to robots, which might include increased natural interaction, effectiveness and acceptance of the robot, social companionship, and well-being for the human. Turning to the potential risks associated with human-robot attachment, we discuss the possibly suboptimal use of the robot in the most benign cases, but also the potential formation of unidirectional emotional bonds, and the potential for deception and subconscious influence of the robot on the person in more severe cases. The upshot of the analysis then is a recommendation to reconceptualize relationships with social robots in an attempt to retain potential benefits of human-robot attachment, while mitigating (to the extent possible) its downsides. © 2022 Copyright held by the owner/author(s).",attachment; emotional bond; Human-robot interaction; social bond,Economic and social effects; Man machine systems; Social robots; Attachment; Benefit and challenges; Emotional bond; Human robots; Humans-robot interactions; Natural interactions; Potential benefits; Social bond; Social robots; Social settings; Machine design
"Participatory Design, Development, and Testing of Assistive Health Robots with Older Adults: An International Four-year Project",2022,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141085733&doi=10.1145%2f3533726&partnerID=40&md5=bd2d090ad7b645b2479ff486178a5b71,"Participatory design includes stakeholders in the development of products intended to solve real-life challenges. Involving end users in the design of robots is vital for developing effective, useful, acceptable and user-friendly products that meet expectations, needs, and preferences. This four-year international project developed and evaluated a home-based robot for mood stabilization and cognitive improvement in older adults with mild cognitive impairment and age-related health needs. The daily-care robot was developed in collaboration with experts, carers, relatives, and older adults, through six phases. Two phases were dedicated to cognitive stimulation games. This paper provides a summary of the participatory design and mixed-methods evaluation processes undertaken to develop, refine, and test the robot. The final robot and games were acceptable to older adults, and useful for delivering stimulating activities and providing reminders for medication, health and wellbeing checks. Personalization is required to optimize human-robot interaction, and imagery and speech should be consistent with local users. Functions should be personalizable to accommodate individual health needs and preferences. This project highlights the importance of participatory design and testing robotics in end-user environments, as technical issues associated with long-term use were uncovered. Recommendations for future development and the design of assistive health robots are made. © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",assistive robots; cognitive robots; gerontechnology; HCI design and evaluation methods; human computer interaction (HCI); Human-centered computing; human-robot interaction; Participatory design; usability testing; user perspective,Health; Human computer interaction; Machine design; Man machine systems; Product design; Assistive robots; Cognitive robots; Design and evaluation methods; Gerontechnology; Human computer interaction; Human computer interaction design and evaluation method; Human-centered computing; Human-computer-interaction designs; Humans-robot interactions; Interaction design methods; Interaction evaluations; Participatory design; Usability testing; Users perspective; Human robot interaction
Ergodic Shared Control: Closing the Loop on pHRI Based on Information Encoded in Motion,2022,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141086958&doi=10.1145%2f3526106&partnerID=40&md5=9bd40d9adfbfee0399177fb16c963423,"Advances in exoskeletons and robot arms have given us increasing opportunities for providing physical support and meaningful feedback in training and rehabilitation settings. However, the chosen control strategies must support motor learning and provide mathematical task definitions that are actionable for the actuation. Typical robot control architectures rely on measuring error from a reference trajectory. In physical human-robot interaction, this leads to low engagement, invariant practice, and few errors, which are not conducive to motor learning. A reliance on reference trajectories means that the task definition is both over-specified-requiring specific timings not critical to task success-and lacking information about normal variability. In this article, we examine a way to define tasks and close the loop using an ergodic measure that quantifies how much information about a task is encoded in the human-robot motion. This measure can capture the natural variability that exists in typical human motion, enabling therapy based on scientific principles of motor learning. We implement an ergodic hybrid shared controller (HSC) on a robotic arm as well as an error-based controller-virtual fixtures-in a timed drawing task. In a study of 24 participants, we compare ergodic HSC with virtual fixtures and find that ergodic HSC leads to improved training outcomes. © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Human-robot interaction; rehabilitation robotics; shared autonomy,Errors; Exoskeleton (Robotics); Fixtures (tooling); Human robot interaction; Man machine systems; Robotic arms; Ergodics; Exoskeleton arms; Humans-robot interactions; Motor learning; Reference trajectories; Rehabilitation robotics; Robot arms; Shared autonomy; Shared control; Virtual fixture; Controllers
The Impact of Route Descriptions on Human Expectations for Robot Navigation,2022,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141038557&doi=10.1145%2f3526104&partnerID=40&md5=42be4c923b6e556edb34b39095d91953,"As robots are deployed to work in our environments, we must build appropriate expectations of their behavior so that we can trust them to perform their jobs autonomously as we attend to other tasks. Many types of explanations for robot behavior have been proposed, but they have not been fully analyzed for their impact on aligning expectations of robot paths for navigation. In this work, we evaluate several types of robot navigation explanations to understand their impact on the ability of humans to anticipate a robot's paths. We performed an experiment in which we gave participants an explanation of a robot path and then measured (i) their ability to predict that path, (ii) their allocation of attention on the robot navigating the path versus their own dot-tracking task, and (iii) their subjective ratings of the robot's predictability and trustworthiness. Our results show that explanations do significantly affect people's ability to predict robot paths and that explanations that are concise and do not require readers to perform mental transformations are most effective at reducing attention to the robot. © 2022 Association for Computing Machinery.",dual-task experiment; Explanations; predictability; robot behavior; robot navigation; user expectations,Behavioral research; Robots; Allocation of attentions; Dual-task experiment; Dual-tasks; Explanation; Predictability; Robot behavior; Robot navigation; Robot path; Route descriptions; User expectations; Navigation
Neurophysiological and Behavioral Differences in Human-Multiagent Tasks: An EEG Network Perspective,2022,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141069608&doi=10.1145%2f3527928&partnerID=40&md5=fe5405f3cc9ffb004b1c61fcc853ebb5,"Effective human-multiagent teams will incorporate the cognitive skills of the human with the autonomous capabilities of the multiagent group to maximize task performance. However, producing a seamless fusion requires a greater understanding of the human's cognitive state as it reacts to uncertainties in both the task environment and agent dynamics. This study examines external behaviors in concert with neurophysiological measures acquired via electroencephalography (EEG) to probe the interactions between cognitive processes, behaviors, and performance in a human-multiagent team task. We show that changes in the α (8-12 Hz) and θ (4-8 Hz) bands of EEG indicate a higher burden on the cognitive resources associated with visual-spatial reasoning required to estimate a more complex kinematic state of robotic agents. These results are reinforced by complementary behavioral shifts in gaze and pilot inputs. Additionally, higher-performing participants tend to engage more actively in the task by utilizing greater amounts of visual-spatial reasoning. Finally, we show that features based on EEG dynamic-network-metrics provide discriminative information that distinguishes gaze behaviors associated with the attention process. © 2022 Copyright held by the owner/author(s).",electroencephalogram; functional connectivity network; gaze detection; human behavior; Human-multiagent performance,Behavioral research; Electrophysiology; Multi agent systems; Neurophysiology; Cognitive skill; Functional connectivity networks; Gaze detection; Human behaviors; Human-multiagent performance; Multi agent; Multiagent teams; Performance; Spatial reasoning; Visual-spatial; Electroencephalography
Personal Space in Human-Robot Interaction at Work: Effect of Room Size and Working Memory Load,2022,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141048834&doi=10.1145%2f3536167&partnerID=40&md5=ecf152ab98458e5b8500eb8d491676de,"A recent literature review on personal space in human-robot interaction identified a research gap for the influence of contextual factors. At the same time, psychological research on interpersonal distancing and theoretical considerations based on compensatory control models suggest the importance of considering these factors in robot path planning. To address this gap, we tested the effect of room size and working memory load on participants' comfort distance toward an approaching robot. In a preregistered 3 × 2 within-subject design, N = 72 participants were approached by a mobile manufacturing robot in a corridor with varying room size and with and without a cognitive secondary task. As dependent variables, comfort distance, arousal, and perceived control were measured. While room size and working memory load had no significant direct effect on comfort distance, participants felt higher arousal and lower control in smaller rooms and in conditions with high working memory load, which in turn caused larger comfort distances (indirect effect). With experience, comfort distances decreased. Based on the indirect effects, future studies should test the effect of more extreme manipulations on comfort distances. Robots should adapt their path planning by keeping larger distances toward human workers in stressful environments to avoid discomfort. © 2022 Copyright held by the owner/author(s).",arousal; equilibrium models; interpersonal distance; perceived control; Proxemics,Machine design; Man machine systems; Motion planning; Robot programming; Arousal; Equilibrium modelling; Humans-robot interactions; Interpersonal distance; Memory load; Perceived control; Personal spaces; Proxemic; Room size; Working memory; Human robot interaction
Can Artificial Intelligence Make Art?: Folk Intuitions as to whether AI-driven Robots Can Be Viewed as Artists and Produce Art,2022,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141087693&doi=10.1145%2f3530875&partnerID=40&md5=0e242de7d873ec959934909ab606a8d6,"In two experiments (total N = 693), we explored whether people are willing to consider paintings made by AI-driven robots as art, and robots as artists. Across the two experiments, we manipulated three factors: (i) agent type (AI-driven robot vs. human agent), (ii) behavior type (intentional creation of a painting vs. accidental creation), and (iii) object type (abstract vs. representational painting). We found that people judge robot paintings and human paintings as art to roughly the same extent. However, people are much less willing to consider robots as artists than humans, which is partially explained by the fact that they are less disposed to attribute artistic intentions to robots. © 2022 Association for Computing Machinery.",aesthetics; AI art; Artificial intelligence; creativity; mental states,Arts computing; Intelligent robots; AI art; Creativity; Esthetic; Human agent; Mental state; Robot painting; Painting
Here's What I've Learned: Asking Questions that Reveal Reward Learning,2022,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130333816&doi=10.1145%2f3526107&partnerID=40&md5=b298413927bb6a3955bb54c498beaedd,"Robots can learn from humans by asking questions. In these questions, the robot demonstrates a few different behaviors and asks the human for their favorite. But how should robots choose which questions to ask? Today's robots optimize for informative questions that actively probe the human's preferences as efficiently as possible. But while informative questions make sense from the robot's perspective, human onlookers may find them arbitrary and misleading. For example, consider an assistive robot learning to put away the dishes. Based on your answers to previous questions this robot knows where it should stack each dish; however, the robot is unsure about right height to carry these dishes. A robot optimizing only for informative questions focuses purely on this height: it shows trajectories that carry the plates near or far from the table, regardless of whether or not they stack the dishes correctly. As a result, when we see this question, we mistakenly think that the robot is still confused about where to stack the dishes! In this article, we formalize active preference-based learning from the human's perspective. We hypothesize that-from the human's point-of-view-the robot's questions reveal what the robot has and has not learned. Our insight enables robots to use questions to make their learning process transparent to the human operator. We develop and test a model that robots can leverage to relate the questions they ask to the information these questions reveal. We then introduce a tradeoff between informative and revealing questions that considers both human and robot perspectives: a robot that optimizes for this tradeoff actively gathers information from the human while simultaneously keeping the human up to date with what it has learned. We evaluate our approach across simulations, online surveys, and in-person user studies. We find that robots, which consider the human's point of view learn just as quickly as state-of-the-art baselines while also communicating what they have learned to the human operator. Videos of our user studies and results are available here: https://youtu.be/tC6y-jHN7Vw. © 2022 Copyright held by the owner/author(s).",active learning; Human-robot interaction; reward learning; trust and interpretability,Behavioral research; Learning systems; Active Learning; Assistive robots; Human operator; Humans-robot interactions; Interpretability; Learn+; Question focus; Reward learning; Trust and interpretability; User study; Human robot interaction
A Methodology to Design and Evaluate HRI Teaming Tasks in Robotic Competitions,2022,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139334733&doi=10.1145%2f3528415&partnerID=40&md5=8d92ef8cb3170bdd2e4407984cac2f47,"As social robots become more prominent in our lives, their interaction with humans takes an increasing role, and new collaborative scenarios emerge. This development brings the need to realize robust test methods enabling the design and evaluation of Human-Robot Interaction (HRI) teaming tasks to prove functionality and promote adoption. In this article, we present a general-purpose and repeatable methodology for conducting studies in collaborative HRI in the range of robotic competitions. The methodology includes a step-by-step approach to design HRI teaming tasks tailored to be enacted in a robotic competition and to evaluate the performance of social robots to execute the designed tasks, exploring the relationship between robots' performance and user perceptions based on the feedback of the users participating to such tasks. We assess the feasibility of the methodology to design and evaluate an HRI teaming task in the context of ""Smart CIties RObotics Challenges""(SciRoc) competition, which targets at investigating the impact of social of robots in smart cities.  © 2022 Association for Computing Machinery.",design of HRI teaming task; HRI evaluation; Methodology; robotic competition; SciRoc,"Machine design; Smart city; Testing; Design of human-robot interaction teaming task; Human-robot interaction evaluation; Humans-robot interactions; Interaction evaluations; Methodology; Robotics competitions; Robust tests; Smart city robotic challenge""; Social robots; Human robot interaction"
Introduction to the Special Issue on Test Methods for Human-Robot Teaming Performance Evaluations,2022,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139592925&doi=10.1145%2f3544303&partnerID=40&md5=95f2e949cd285fcba13f2e90e304112b,"This special issue of the Transactions on Human-Robot Interaction highlights, documents, and explores the metrics, test methods, and artifacts used in human-robot interaction (HRI) research. This collection of articles brings to attention the commonalities between the application of measurement science for the assessment and assurance of human-centric robotics in a variety of application domains, including industry, education, and defense. This special issue draws specific attention to the use and impact of metrology toward the advancement of HRI technologies and algorithms, and it promotes the application of measurement science toward the benchmarking and replication of HRI research. Special attention is given to the use cases, data sets, test methodologies, measurement techniques, metrics, and statistical analyses used to evaluate system performance.  © 2022 Copyright held by the owner/author(s).",benchmarking; data sets; performance measures; repeatability studies; Test methods and metrics; use cases,Human robot interaction; Man machine systems; Testing; Data set; Human robots; Humans-robot interactions; Measurement science; Performance measure; Performances evaluation; Repeatability study; Test method; Test metrics; Use case; Benchmarking
Metrics for Human-Robot Team Design: A Teamwork Perspective on Evaluation of Human-Robot Teams,2022,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139335520&doi=10.1145%2f3522581&partnerID=40&md5=aefce58599f876460f85aa1ef54586d8,"Metrics for human-robot teaming should extend to teams consisting of multiple human and robotic agents, and to teams working in complex, dynamic work domains. This work proposes that to comprehensively analyze and evaluate multi-human, multi-robot teams, traditional HRI metrics of performance, and efficiency must be expanded upon to incorporate metrics of teamwork. We develop five distinct metrics to capture both ecological and cognitive aspects of teamwork found to be important in human-automation interaction, inspired by research in the cognitive systems engineering (CSE) community. We demonstrate the application of these metrics in a spacecraft maintenance case study comparing multiple human-robot team architectures. The case study demonstrates that the teamwork metrics capture aspects of human-robot interaction (HRI) not apparent when using only traditional performance and efficiency metrics. The article concludes that the proposed teamwork metrics are complementary to existing metrics in HRI and should be included in the evaluation of human-robot teams.  © 2022 Copyright held by the owner/author(s).",Human-robot interaction; metrics; teamwork,Cognitive systems; Efficiency; Machine design; Man machine systems; Systems engineering; Case-studies; Human agent; Human robots; Human-robot-team; Humans-robot interactions; Metric; Robotic agents; Team designs; Team working; Teamwork; Human robot interaction
Socially Assistive Robots as Storytellers that Elicit Empathy,2022,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136151336&doi=10.1145%2f3538409&partnerID=40&md5=56544a63935530518bdd0bbcc618a8b9,"Empathy is the ability to share someone else's feelings or experiences; it influences how people interact and relate. Socially assistive robots (SAR) are a promising means of conveying and eliciting empathy toward facilitating human-robot interaction. This work examines factors that influence the amount of empathy elicited by a SAR storyteller and users' perceptions of that robot. We conducted an empirical mixed-design study (N=46) using an autonomous SAR storyteller that told three stories, each with a different human or robot target of empathy. The robot storyteller used the first-person narrative voice (1PNV) with half of the participants and the third-person narrative voice (3PNV) with the other half. We found that the SAR storyteller elicited significantly more empathy when the story target of empathy matched the SAR narrator, i.e., was also a robot. Additionally, the 1PNV robot elicited significantly more empathy and was perceived as more human-like, easy to interact with, and trustworthy than the 3PNV robot. Finally, participants who empathized more with the robot displayed facial expressions consistent with the emotional story content. These insights inform the design of SAR storytellers capable of eliciting empathy toward creating compelling and effective human-robot interactions. © 2022 Association for Computing Machinery.",empathy; engagement; expression; narrative voice; robot storyteller; Socially assistive robot,Economic and social effects; Machine design; Man machine systems; Design studies; Empathy; Engagement; Expression; First person; Humans-robot interactions; Narrative voice; Robot storyteller; Socially assistive robots; User perceptions; Human robot interaction
A Human Factors Approach to Validating Driver Models for Interaction-aware Automated Vehicles,2022,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133497973&doi=10.1145%2f3538705&partnerID=40&md5=264c24876fef09d03ebcb44cb7978094,"A major challenge for autonomous vehicles is interacting with other traffic participants safely and smoothly. A promising approach to handle such traffic interactions is equipping autonomous vehicles with interaction-aware controllers (IACs). These controllers predict how surrounding human drivers will respond to the autonomous vehicle's actions, based on a driver model. However, the predictive validity of driver models used in IACs is rarely validated, which can limit the interactive capabilities of IACs outside the simple simulated environments in which they are demonstrated. In this article, we argue that besides evaluating the interactive capabilities of IACs, their underlying driver models should be validated on natural human driving behavior. We propose a workflow for this validation that includes scenario-based data extraction and a two-stage (tactical/operational) evaluation procedure based on human factors literature. We demonstrate this workflow in a case study on an inverse-reinforcement-learning-based driver model replicated from an existing IAC. This model only showed the correct tactical behavior in 40% of the predictions. The model's operational behavior was inconsistent with observed human behavior. The case study illustrates that a principled evaluation workflow is useful and needed. We believe that our workflow will support the development of appropriate driver models for future automated vehicles. © 2022 Copyright held by the owner/author(s).",automated driving; Driver model validation; interaction-aware controllers; inverse reinforcement learning driver model,Autonomous vehicles; Behavioral research; Human engineering; Learning systems; Reinforcement learning; Automated driving; Autonomous Vehicles; Driver model validation; Driver modelling; Interaction-aware controller; Inverse reinforcement learning; Inverse reinforcement learning driver model; Model validation; Work-flows; Controllers
"Mental State Attribution to Robots: A Systematic Review of Conceptions, Methods, and Findings",2022,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140006494&doi=10.1145%2f3526112&partnerID=40&md5=338426185465ecc4327d7ceadffecddc,"The topic of mental state attribution to robots has been approached by researchers from a variety of disciplines, including psychology, neuroscience, computer science, and philosophy. As a consequence, the empirical studies that have been conducted so far exhibit considerable diversity in terms of how the phenomenon is described and how it is approached from a theoretical and methodological standpoint. This literature review addresses the need for a shared scientific understanding of mental state attribution to robots by systematically and comprehensively collating conceptions, methods, and findings from 155 empirical studies across multiple disciplines. The findings of the review include that: (1) the terminology used to describe mental state attribution to robots is diverse but largely homogenous in usage; (2) the tendency to attribute mental states to robots is determined by factors such as the age and motivation of the human as well as the behavior, appearance, and identity of the robot; (3) there is a computer < robot < human pattern in the tendency to attribute mental states that appears to be moderated by the presence of socially interactive behavior; (4) there are conflicting findings in the empirical literature that stem from different sources of evidence, including self-report and non-verbal behavioral or neurological data. The review contributes toward more cumulative research on the topic and opens up for a transdisciplinary discussion about the nature of the phenomenon and what types of research methods are appropriate for investigation. © 2022 Association for Computing Machinery.",anthropomorphism; folk psychology; Human-robot interaction; intentional stance; mentalizing; mind perception; theory of mind,Neurology; Anthropomorphism; Empirical studies; Folk psychology; Humans-robot interactions; Intentional stance; Mental state; Mentalizing; Mind perception; Systematic Review; Theory of minds; Human robot interaction
Trust Measurement in Human-Autonomy Teams: Development of a Conceptual Toolkit,2022,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139346110&doi=10.1145%2f3530874&partnerID=40&md5=53ce55b879b11e1559801f38d0d98b6c,"The rise in artificial intelligence capabilities in autonomy-enabled systems and robotics has pushed research to address the unique nature of human-autonomy team collaboration. The goal of these advanced technologies is to enable rapid decision-making, enhance situation awareness, promote shared understanding, and improve team dynamics. Simultaneously, use of these technologies is expected to reduce risk to those who collaborate with these systems. Yet, for appropriate human-autonomy teaming to take place, especially as we move beyond dyadic partnerships, proper calibration of team trust is needed to effectively coordinate interactions during high-risk operations. But to meet this end, critical measures of team trust for this new dynamic of human-autonomy teams are needed. This article seeks to expand on trust measurement principles and the foundation of human-autonomy teaming to propose a ""toolkit"" of novel methods that support the development, maintenance, and calibration of trust in human-autonomy teams operating within uncertain, risky, and dynamic environments. © 2022 Copyright held by the owner/author(s).",Human-autonomy teaming; robot; team trust; trust measurement,Decision making; Advanced technology; Decisions makings; Human-autonomy teaming; Shared understanding; Situation awareness; Team collaboration; Team development; Team dynamics; Team trust; Trust measurement; Calibration
Unified Meaning Representation Format (UMRF)-A Task Description and Execution Formalism for HRI,2022,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131280312&doi=10.1145%2f3522580&partnerID=40&md5=b8eb33e04242d9c2cfdd9afab9e80c21,"To facilitate continuous development of novel HRI systems, it is beneficial to have tools that enable quick adjustments, flexibility, or re-invention of the human interfaces when system requirements change due to updates in the state-of-the-art, application domain, etc. Thus, modularity is a key design principle which promotes software reuse and scalability, and reduces development time and cost. Hence, a robot's autonomous capabilities should not depend on the command interface and should be decoupled via a common format that possesses the descriptive capabilities for outlining tasks and has a sensible syntax for HRI. In this paper, we propose the Unified Meaning Representation Format (UMRF), which provides the syntax and semantics for passing both simple and complex commands modelled as control flow graphs. UMRF is a standalone meaning representation container that supports embedding other meaning representation formalisms, such as predicate-argument semantics and graphical meaning representation formats, making it adoptable as a standard task description format for semi-autonomous systems in HRI domains. In this article, we define the UMRF syntax and semantics, summarize its unique aspects relative to related task description formats, and demonstrate its descriptiveness by navigating a robot via concurrent (e.g., gestures and speech) and interchangeable input systems (e.g., Google Assistant, Amazon Alexa). © 2022 Association for Computing Machinery.",human-robot collaboration; software modularity; Task description languages,Abstracting; Application programs; Computer software reusability; Flow graphs; Interface states; Robots; Syntactics; Continuous development; Human Interface; Human-robot collaboration; Quick adjustment; Requirements change; Software modularity; System requirements; Task description; Task description languages; Task executions; Semantics
Evaluating Human-Robot Interaction Algorithms in Shared Autonomy via Quality Diversity Scenario Generation,2022,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130800914&doi=10.1145%2f3476412&partnerID=40&md5=268b4c781705c12f21042929bc1aac72,"The growth of scale and complexity of interactions between humans and robots highlights the need for new computational methods to automatically evaluate novel algorithms and applications. Exploring diverse scenarios of humans and robots interacting in simulation can improve understanding of the robotic system and avoid potentially costly failures in real-world settings. We formulate this problem as a quality diversity (QD) problem, of which the goal is to discover diverse failure scenarios by simultaneously exploring both environments and human actions. We focus on the shared autonomy domain, in which the robot attempts to infer the goal of a human operator, and adopt the QD algorithms CMA-ME and MAP-Elites to generate scenarios for two published algorithms in this domain: shared autonomy via hindsight optimization and linear policy blending. Some of the generated scenarios confirm previous theoretical findings, while others are surprising and bring about a new understanding of state-of-the-art implementations. Our experiments show that the QD algorithms CMA-ME and MAP-Elites outperform Monte-Carlo simulation and optimization-based methods in effectively searching the scenario space, highlighting their promise for automatic evaluation of algorithms in human-robot interaction.  © 2022 Copyright held by the owner/author(s).",automatic scenario generation; human-robot interaction; Quality diversity optimization,Human robot interaction; Intelligent systems; Man machine systems; Monte Carlo methods; Automatic scenario generation; Humans-robot interactions; Novel algorithm; Novel applications; Optimisations; Quality diversity optimization; Real world setting; Robotic systems; Scenarios generation; Shared autonomy; Blending
SocNavBench: A Grounded Simulation Testing Framework for Evaluating Social Navigation,2022,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137956668&doi=10.1145%2f3476413&partnerID=40&md5=701c2b87b10736a982444ae2c158fc8a,"The human-robot interaction community has developed many methods for robots to navigate safely and socially alongside humans. However, experimental procedures to evaluate these works are usually constructed on a per-method basis. Such disparate evaluations make it difficult to compare the performance of such methods across the literature. To bridge this gap, we introduce SocNavBench, a simulation framework for evaluating social navigation algorithms. SocNavBench comprises a simulator with photo-realistic capabilities and curated social navigation scenarios grounded in real-world pedestrian data. We also provide an implementation of a suite of metrics to quantify the performance of navigation algorithms on these scenarios. Altogether, SocNavBench provides a test framework for evaluating disparate social navigation methods in a consistent and interpretable manner. To illustrate its use, we demonstrate testing three existing social navigation methods and a baseline method on SocNavBench, showing how the suite of metrics helps infer their performance trade-offs. Our code is open-source, allowing the addition of new scenarios and metrics by the community to help evolve SocNavBench to reflect advancements in our understanding of social navigation.  © 2022 Association for Computing Machinery.",Benchmark; human robot interaction; pedestrian; social navigation,Economic and social effects; Man machine systems; Navigation; Open source software; Open systems; Benchmark; Experimental procedure; Humans-robot interactions; Navigation algorithms; Navigation methods; Pedestrian; Performance; Simulation testing; Social navigation; Testing framework; Human robot interaction
Metrics for Robot Proficiency Self-assessment and Communication of Proficiency in Human-robot Teams,2022,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139320060&doi=10.1145%2f3522579&partnerID=40&md5=3ec944b1bcad414f9ff19a129d1dda8e,"As development of robots with the ability to self-assess their proficiency for accomplishing tasks continues to grow, metrics are needed to evaluate the characteristics and performance of these robot systems and their interactions with humans. This proficiency-based human-robot interaction (HRI) use case can occur before, during, or after the performance of a task. This article presents a set of metrics for this use case, driven by a four-stage cyclical interaction flow: (1) robot self-assessment of proficiency (RSA), (2) robot communication of proficiency to the human (RCP), (3) human understanding of proficiency (HUP), and (4) robot perception of the human's intentions, values, and assessments (RPH). This effort leverages work from related fields including explainability, transparency, and introspection, by repurposing metrics under the context of proficiency self-assessment. Considerations for temporal level (a priori, in situ, and post hoc) on the metrics are reviewed, as are the connections between metrics within or across stages in the proficiency-based interaction flow. This article provides a common framework and language for metrics to enhance the development and measurement of HRI in the field of proficiency self-assessment.  © 2022 Association for Computing Machinery.",Human-robot interaction; metrics; performance evaluation; proficiency self-assessment,Human robot interaction; Human understanding; Human-robot-team; Humans-robot interactions; Metric; Performance; Performances evaluation; Proficiency self-assessment; Robot communication; Robots system; Self-assessment; Man machine systems
Design and Evaluation of an Augmented Reality Head-mounted Display Interface for Human Robot Teams Collaborating in Physically Shared Manufacturing Tasks,2022,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137033259&doi=10.1145%2f3524082&partnerID=40&md5=49b19335046dc3f750166f3608304401,"We provide an experimental evaluation of a wearable augmented reality (AR) system we have developed for human-robot teams working on tasks requiring collaboration in shared physical workspace. Recent advances in AR technology have facilitated the development of more intuitive user interfaces for many human-robot interaction applications. While it has been anticipated that AR can provide a more intuitive interface to robot assistants helping human workers in various manufacturing scenarios, existing studies in robotics have been largely limited to teleoperation and programming. Industry 5.0 envisions cooperation between human and robot working in teams. Indeed, there exist many industrial tasks that can benefit from human-robot collaboration. A prime example is high-value composite manufacturing. Working with our industry partner towards this example application, we evaluated our AR interface design for shared physical workspace collaboration in human-robot teams. We conducted a multi-dimensional analysis of our interface using established metrics. Results from our user study (n = 26) show that, subjectively, the AR interface feels more novel and a standard joystick interface feels more dependable to users. However, the AR interface was found to reduce physical demand and task completion time, while increasing robot utilization. Furthermore, user's freedom of choice to collaborate with the robot may also affect the perceived usability of the system.  © 2022 Association for Computing Machinery.",assistive robotic; augmented reality; collaborative manufacturing; Human-robot interaction; wearable interface,Collaborative robots; Helmet mounted displays; Machine design; Man machine systems; Manufacture; Robot programming; User interfaces; Virtual reality; Assistive robotics; Collaborative manufacturing; Design and evaluations; Display interfaces; Head-mounted-displays; Human-robot-team; Humans-robot interactions; Manufacturing tasks; Reality interface; Wearable interfaces; Augmented reality
Human-Robot Scaffolding: An Architecture to Foster Problem-solving Skills,2022,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139338801&doi=10.1145%2f3526109&partnerID=40&md5=9395ce60d6f1f52d7b40d3aa2189f389,"In order to give assertive support, robots need to understand the cognitive and emotional characteristics of learners while in the learning process. Also, if the task involves handling capacity, robots must have similar skills. Three concepts were explored to control the cognitive and emotional robot's behavior: the psychological flow theory, the scaffolding pedagogic strategy, and the multiagents-software paradigm. Based on these concepts, the Human-Robot Scaffolding architecture was designed. It is divided into five blocks. First, the sensory block recognizes body gestures, speech, and task state. Second, the beliefs block estimates the skills and emotional state of learners. Third, the desires block validates the goals the robot can reach; the goals are grouped in skills development, emotional control, cognitive control, challenge control, life signals, and immediate support. Fourth, the intentions block, based on the goals competition strategy, selects the goal that the robot will perform. Finally, the action-planner block regulates the robot's movements according to the robot's emotions. The validation procedure was done with 53 learners ranging between 10 and 13 years old who study in public and private schools. Based on the research achievements, the robot fosters learning the Mean-Ends Analysis strategy and the solution of a problem. A video fragment that summarizes the research process is available in https://youtu.be/qbohCjBIwYc.  © 2022 Association for Computing Machinery.",Anthropomorphic robots; human-robot scaffolding; multi-agent systems; robots for education,Anthropomorphic robots; Behavioral research; Learning systems; Scaffolds; Cognitive robots; Emotional robots; Flow theories; Handling capacity; Human robots; Human-robot scaffolding; Learning process; Problem solving skills; Robot behavior; Robot for education; Multi agent systems
Measuring the Quality of Learning in a Human-Robot Collaboration: A Study of Laparoscopic Surgery,2022,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139274391&doi=10.1145%2f3476414&partnerID=40&md5=7347458fb539b5cb4cf04780f081262e,"Robot-Assisted Laparoscopic Surgery (RALS) is now prevalent in operating rooms. This situation requires future surgeons to learn Classic Laparoscopic Surgery (CLS) and RALS simultaneously. Therefore, along with the investigation of the differences in performance between the two techniques, it is essential to study the impact of training in RALS on the skills mastered in CLS. In this article, we study comanipulated RALS (Co-RALS), one of the two designs for RALS, where the human and the robot share the execution of the task. We use a rarely used in Human-Robot Interaction measuring tool: gaze tracking and time recording to measure for the acquisition of skills in CLS when training in Co-RALS or in CLS and time recording to compare the learning curves between Co-RALS and CLS. These metrics allow us to observe differences in Co-RALS and CLS. Training in Co-RALS develops slightly better but not significantly better hand-eye coordination skills and significantly better timewise performance compared with training in CLS alone. Co-RALS enhances timewise performance in laparoscopic surgery on specific types of tasks that require precision rather than depth perception skills compared with CLS. The results obtained enable us to further define the Human-Robot Interaction quality in Co-RALS.  © 2022 Association for Computing Machinery.",Datasets; gaze detection; neural networks; text tagging,Depth perception; Eye tracking; Laparoscopy; Machine design; Man machine systems; Robotic surgery; Surgical equipment; Dataset; Gaze detection; Human-robot collaboration; Humans-robot interactions; Laparoscopic surgery; Learn+; Neural-networks; Performance; Quality of learning; Text tagging; Human robot interaction
A Framework to Explore Proximate Human-Robot Coordination,2022,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139318625&doi=10.1145%2f3526101&partnerID=40&md5=00cd4dc7cca9b41da8aa42db51f28e1c,"Proximate human-robot teaming (pxHRT) is a complex subspace within human-robot interaction. Studies in this space involve a range of equipment and methods, including the ability to sense people and robots precisely. Research in this area draws from a wide variety of other fields, from human-human interaction to control theory, making the study design complex, particularly for those outside the field of HRI. In this paper, we introduce a framework that helps researchers consider tradeoffs across various task contexts, platforms, sensors, and analysis methods; metrics frequently used in the field; and common challenges researchers may face. We demonstrate the use of the framework via a case study which employs an autonomous mobile manipulator continuously engaging in shared workspace, handover, and co-manipulation tasks with people, and explores the effect of cognitive workload on pxHRT dynamics. We also demonstrate the utility of the framework in a case study with two groups of researchers new to pxHRT. With this framework, we hope to enable researchers, especially those outside HRI, to more thoroughly consider these complex components within their studies, more easily design experiments, and more fully explore research questions within the space of pxHRT.  © 2022 Association for Computing Machinery.",handovers; Human robot teaming; proximate human robot interaction; research methods,Human resource management; Man machine systems; Manipulators; Case-studies; Hand over; Human robot teaming; Human robots; Human-human interactions; Humans-robot interactions; Proximate human robot interaction; Research method; Robot coordination; Study design; Human robot interaction
"Intuitive, Efficient and Ergonomic Tele-Nursing Robot Interfaces: Design Evaluation and Evolution",2022,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136151880&doi=10.1145%2f3526108&partnerID=40&md5=21af3c45bbc178bbe8f1f63023dbed30,"Tele-nursing robots provide a safe approach for patient-caring in quarantine areas. For effective nurse-robot collaboration, ergonomic teleoperation and intuitive interfaces with low physical and cognitive workload must be developed. We propose a framework to evaluate the control interfaces to iteratively develop an intuitive, efficient, and ergonomic teleoperation interface. The framework is a hierarchical procedure that incorporates general to specific assessment and its role in design evolution. We first present pre-defined objective and subjective metrics used to evaluate three representative contemporary teleoperation interfaces. The results indicate that teleoperation via human motion mapping outperforms the gamepad and stylus interfaces. The tradeoff with using motion mapping as a teleoperation interface is the non-trivial physical fatigue. To understand the impact of heavy physical demand during motion mapping teleoperation, we propose an objective assessment of physical workload in teleoperation using electromyography. We find that physical fatigue happens in the actions that involve precise manipulation and steady posture maintenance. We further implemented teleoperation assistance in the form of shared autonomy to eliminate the fatigue-causing component in robot teleoperation via motion mapping. The experimental results show that the autonomous feature effectively reduces the physical effort while improving the efficiency and accuracy of the teleoperation interface.  © 2022 Copyright held by the owner/author(s).",human workload; shared autonomy; Teleoperation interface,Machine design; Mapping; Nursing; Remote control; Robots; Design evaluation; Design evolution; Human workload; Interface designs; Intuitive interfaces; Motion mapping; Physical fatigues; Robot interface; Shared autonomy; Teleoperation interface; Ergonomics
Accepting Human-like Avatars in Social and Professional Roles,2022,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139593674&doi=10.1145%2f3526026&partnerID=40&md5=14b9e5b4ec0e3c62ec3982746ec8d0ac,"Humans report perceptions of unease or eeriness as humanoid/android robots and digital avatars approach human-like physical resemblance, a phenomenon alluded by the Uncanny Valley theory. This study extends the discussions on interactions and acceptance of digital avatars with findings from three experiments. In the first, perceptive evaluation of actors in clips from computer-generated animation and a live-action version of the same movie was examined. In the second experiment, we considered short clips with highly realistic digital avatars to measure recognition ability, the extent of eeriness, and specific physical features identified as unreal. The fixation area and pupil size variation recorded using an eye tracker were analyzed to infer attention to the body, face, and emotional response, respectively. Building on these findings, the third experiment looked at acceptance in roles requiring human skill, empathy, and cognitive ability. The results show that based on perceptions from physical attributes, the eeriness scores diverge from the uncanny valley theory as human-likeness increases. The realistic CGI and mocap technology could have helped cross the valley. Visual attention inferred from gaze behavior was similar for live-action and CGI. At the same time, we observe pupil size changes reflecting emotions like eeriness when the avatars either talked or smiled. Proficiency and acceptance scores were lower for roles requiring complex social cognition processes, such as friends and judicial decision-making. Interestingly, real-life stereotypes of gender roles were transferred to digital avatars too. The findings suggest an ambiguity in accepting human-like avatars in social and professional interactions, emphasizing the need for a multi-dimensional approach when applying the uncanny valley theory. A detailed and contextual examination is imperative as technological advancements have placed humans closer to co-existing with digital or physical android/humanoid robots.  © 2022 Association for Computing Machinery.",acceptance; animation; digital human; eeriness; human-computer interaction; motions capture techniques; Uncanny valley,Behavioral research; Computation theory; Decision making; Emotion Recognition; Eye movements; Eye tracking; Human computer interaction; Landforms; Acceptance; Digital humans; Eeriness; Human like; Live actions; Motion capture; Motion capture technique; Pupil size; Social roles; Uncanny valley; Animation
Non-Dyadic Interaction: A Literature Review of 15 Years of Human-Robot Interaction Conference Publications,2022,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127514681&doi=10.1145%2f3488242&partnerID=40&md5=49121d90f11620ce794c9c7c37fc7281,"Going beyond dyadic (one-to-one) interaction has been increasingly explored in HRI. Yet we lack a comprehensive view on non-dyadic interaction research in HRI. To map out 15 years of works investigating non-dyadic interaction, and thereby identifying the trend of the field and future research areas, we performed a literature review containing all 164 publications (2006-2020) from the HRI conference investigating non-dyadic interaction. Our approach is inspired by the 4C framework, an interaction framework focusing on understanding and categorising different types of interaction between humans and digital artefacts. The 4C framework consists of eight interaction principles for multi-user/multi-artefact interaction categorised into four broader themes. We modified the 4C framework to increase applicability and relevance in the context of non-dyadic human-robot interaction. We identify an increasing tendency towards non-dyadic research (36% in 2020), as well as a focus on simultaneous studies (85% from 2006-2020) over sequential. We also articulate seven interaction principles utilised in non-dyadic HRI and provide specific examples. Last, based on our findings, we discuss several salient points of non-dyadic HRI, the applicability of the modified 4C framework to HRI and potential future topics of interest as well as open-questions for non-dyadic research. © 2022 Association for Computing Machinery.",literature review; Non-Dyadic HRI; simultaneous human-robot interaction,Man machine systems; Conference publications; Digital artifacts; Dyadic interaction; Dyadic researches; Humans-robot interactions; Interaction framework; Literature reviews; Non-dyadic HRI; Research areas; Simultaneous human-robot interaction; Human robot interaction
Social Momentum: Design and Evaluation of a Framework for Socially Competent Robot Navigation,2022,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127503539&doi=10.1145%2f3495244&partnerID=40&md5=2faf7c5eac188666481103614619fcee,"Mobile robots struggle to integrate seamlessly in crowded environments such as pedestrian scenes, often disrupting human activity. One obstacle preventing their smooth integration is our limited understanding of how humans may perceive and react to robot motion. Motivated by recent studies highlighting the benefits of intent-expressive motion for robots operating close to humans, we describe Social Momentum (SM), a planning framework for legible robot motion generation in multiagent domains. We investigate the properties of motion generated by SM via two large-scale user studies: an online, video-based study (N = 180) focusing on the legibility of motion produced by SM and a lab study (N = 105) focusing on the perceptions of users navigating next to a robot running SM in a crowded space. Through statistical and thematic analyses of collected data, we present evidence suggesting that (a) motion generated by SM enables quick inference of the robot's navigation strategy; (b) humans navigating close to a robot running SM follow comfortable, low-acceleration paths; and (c) robot motion generated by SM is positively perceived and indistinguishable from a teleoperated baseline. Through the discussion of experimental insights and lessons learned, this article aspires to inform future algorithmic and experimental design for social robot navigation. © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",benchmarking; multiagent systems; Social navigation; social robotics,Machine design; Mobile robots; Navigation; Robot programming; Robotics; Design and evaluations; Human activities; Multi agent; Planning framework; Property; Robot motion; Robot motion generations; Robot navigation; Social navigation; Social robotics; Multi agent systems
Study of Kinesthetic Negotiation Ability in Lightweight Comanipulative Decision-making Tasks: Design and Study of a Virtual Partner based on Human-human Interaction Observation,2022,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127475676&doi=10.1145%2f3485753&partnerID=40&md5=c9a6cfbbd17adb38a2420069931eae3d,"This article presents the results of an experiment on physical Human-Human Interaction (pHHI), where human dyads cooperate on a one-dimensional comanipulative task in a novel lightweight teleoperation setup. The results of this experiment show that humans are able to handle asymmetrical information about the task and solve conflicts using only the kinesthetic channel. Data from the pHHI experiment is used to design a virtual partner that can perform the task alongside a human. The virtual partner behavior is based on the observation that initiative is highly correlated to decision-making in our pHHI negotiation scenario. The virtual agent is then evaluated in a physical Human-Robot Interaction (pHRI) experiment. The results of the second experiment show that the virtual partner is able to perform the task without compromising the performances of the dyad and that a similar role distribution is observed in human-human and human-robot dyads. Moreover, the knowledge of the partner's nature does not seem to influence the performances. The results obtained with the virtual partner are encouraging and could be used to design kinesthetic negotiation algorithms in pHRI settings.The main contributions of this article are: (1) supporting evidence of the possibility for humans to use the kinesthetic channel as mean of negotiation during physical Human-Human Interaction in the lightest known impedance negotiation tasks (with no virtual mass involved), (2) highlighting of the correlation between initiative and dyadic decision making, (3) design of a simple yet efficient virtual partner algorithm capable of realistic physical Human-Robot Interaction in the one-dimension tracking task used in the experimental setup. This design combines minimum jerk trajectory, switching role ability, decision criteria, and statistical parameters that are detailed in this article. © 2022 Association for Computing Machinery.",comanipulation; haptic negotiation; Physical human-human interaction; physical human-robot interaction,Human robot interaction; Machine design; Man machine systems; Virtual reality; Comanipulation; Decisions makings; Haptic negotiations; Human-human interactions; Kinesthetics; One-dimensional; Performance; Physical human-human interaction; Physical humanrobot interaction (phri); Task design; Decision making
Robot Communication Via Motion: A Study on Modalities for Robot-to-Human Communication in the Field,2022,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127473304&doi=10.1145%2f3495245&partnerID=40&md5=a2433ef0ffd1f805bded4715250e3584,"In this article, we propose, implement, and evaluate a motion-based communication system for field robots: robots that operate in dynamic, unstructured, outdoor environments. We perform two pilot studies to guide our development of the system, then evaluate it alongside an audio communication system, an LCD display, and a system of blinking LEDs. We compare the usage of these four systems with three different robots from up to five different viewpoints of interaction in a large study administered via Amazon Mechanical Turk. We contribute in two ways to the development of a more robust form of field human-robot interaction, wherein robots can select the most appropriate communication vector for a given situation and context. First, we contribute a motion-based communication system for field robots along with three baseline systems against which to test it. Second, we present results from our development of this motion system, showing that it is easier to learn than a baseline blinking LED system, viable for use underwater, aerial, and terrestrial field robots, and less negatively affected by adverse viewpoints than other communication methods. © 2022 Association for Computing Machinery.",field robotics; human-robot-interaction; non-humanoid robots; Robotics,Antennas; Human robot interaction; Light emitting diodes; Liquid crystal displays; Man machine systems; Robotics; Communications systems; Field robot; Field robotics; Human communications; Humanoid robot; Humans-robot interactions; Non-humanoid robot; Outdoor environment; Pilot studies; Robot communication; Anthropomorphic robots
Have i Got the Power? Analysing and Reporting Statistical Power in HRI,2022,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127056055&doi=10.1145%2f3495246&partnerID=40&md5=556d02f46fdba65c7d250b57b213e70f,"This article presents a discussion of the importance of power analyses, providing an overview of when power analyses should be run in the context of the field of Human-Robot Interaction, as well as some examples of how to perform a power analysis. This work was motivated by the observation that the majority of papers published in the proceedings of recent HRI conferences did not report conducting a power analysis; an observation that has concerning implications for many conclusions drawn by these studies. This work is intended to raise awareness and encourage researchers to conduct power analyses when designing research studies using human participants. © 2022 Copyright held by the owner/author(s).",best practice; methodology; power; Reporting practices,Best practices; Humans-robot interactions; Methodology; Power; Power analysis; Reporting practice; Research studies; Statistical power; Human robot interaction
Transparency's Influence on Human-collective Interactions,2022,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127474000&doi=10.1145%2f3507470&partnerID=40&md5=b137a16fb67774e85979ba2130b18607,"Collective robotic systems are biologically inspired and advantageous due to their apparent global intelligence and emergent behaviors. Many applications can benefit from the incorporation of collectives, including environmental monitoring, disaster response missions, and infrastructure support. Transparency research has primarily focused on how the design of the models, visualizations, and control mechanisms influence human-collective interactions. Traditionally most transparency research has evaluated one system design element. This article analyzed two models and visualizations to understand how the system design elements impacted human-collective interactions, to quantify which model and visualization combination provided the best transparency, and provide design guidance, based on remote supervision of collectives. The consensus decision-making and baseline models, as well as an individual collective entity and abstract visualizations, were analyzed for sequential best-of-n decision-making tasks involving four collectives, composed of 200 entities each. Both models and visualizations provided transparency and influenced human-collective interactions differently. No single combination provided the best transparency. © 2022 Association for Computing Machinery.",collective systems; human-collective interactions; Transparency,Abstracting; Biomimetics; Decision making; Design; Systems analysis; Visualization; Biologically-inspired; Collective robotic systems; Collective systems; Design elements; Disaster-response; Emergent behaviours; Environmental Monitoring; Global intelligence; Human-collective interaction; Model mechanisms; Transparency
Identification of Low-engaged Learners in Robot-led Second Language Conversations with Adults,2022,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127492914&doi=10.1145%2f3503799&partnerID=40&md5=e1c37163339c4f90c4f54cd74213f117,"The main aim of this study is to investigate if verbal, vocal, and facial information can be used to identify low-engaged second language learners in robot-led conversation practice. The experiments were performed on voice recordings and video data from 50 conversations, in which a robotic head talks with pairs of adult language learners using four different interaction strategies with varying robot-learner focus and initiative. It was found that these robot interaction strategies influenced learner activity and engagement. The verbal analysis indicated that learners with low activity rated the robot significantly lower on two out of four scales related to social competence. The acoustic vocal and video-based facial analysis, based on manual annotations or machine learning classification, both showed that learners with low engagement rated the robot's social competencies consistently, and in several cases significantly, lower, and in addition rated the learning effectiveness lower. The agreement between manual and automatic identification of low-engaged learners based on voice recordings or face videos was further found to be adequate for future use. These experiments constitute a first step towards enabling adaption to learners' activity and engagement through within-and between-strategy changes of the robot's interaction with learners. © 2022 Copyright held by the owner/author(s).",facial emotion expressions; Robot-assisted language learning; speech emotion recognition; user engagement,Automation; Face recognition; Learning systems; Robots; Emotion expression; Facial emotion expression; Facial emotions; Interaction strategy; Robot interactions; Robot-assisted language learning; Second language; Second language learners; Speech emotion recognition; User engagement; Speech recognition
Enabling Morally Sensitive Robotic Clarification Requests,2022,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127494259&doi=10.1145%2f3503795&partnerID=40&md5=474c07bbeec8796df8f7db091f1e804e,"The design of current natural language-oriented robot architectures enables certain architectural components to circumvent moral reasoning capabilities. One example of this is reflexive generation of clarification requests as soon as referential ambiguity is detected in a human utterance. As shown in previous research, this can lead robots to (1) miscommunicate their moral dispositions and (2) weaken human perception or application of moral norms within their current context. We present a solution to these problems by performing moral reasoning on each potential disambiguation of an ambiguous human utterance and responding accordingly, rather than immediately and naively requesting clarification. We implement our solution in the Distributed Integrated Cognition Affect and Reflection robot architecture, which, to our knowledge, is the only current robot architecture with both moral reasoning and clarification request generation capabilities. We then evaluate our method with a human subjects experiment, the results of which indicate that our approach successfully ameliorates the two identified concerns. © 2022 Copyright held by the owner/author(s).",Clarification; dialogue systems; natural language generation,Architecture; Clarifiers; Natural language processing systems; Robots; Speech processing; 'current; Architectural components; Dialogue systems; Human perception; Lead robots; Moral reasoning; Natural language generation; Natural languages; Reasoning capabilities; Robot architecture; Machine design
Levels of Automation for a Mobile Robot Teleoperated by a Caregiver,2022,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127498697&doi=10.1145%2f3507471&partnerID=40&md5=6a6c8a3fe845a2892ae0dc631bf35246,"Caregivers in eldercare can benefit from telepresence robots that allow them to perform a variety of tasks remotely. In order for such robots to be operated effectively and efficiently by non-technical users, it is important to examine if and how the robotic system's level of automation (LOA) impacts their performance. The objective of this work was to develop suitable LOA modes for a mobile robotic telepresence (MRP) system for eldercare and assess their influence on users' performance, workload, awareness of the environment, and usability at two different levels of task complexity. For this purpose, two LOA modes were implemented on the MRP platform: assisted teleoperation (low LOA mode) and autonomous navigation (high LOA mode). The system was evaluated in a user study with 20 participants, who, in the role of the caregiver, navigated the robot through a home-like environment to perform control and perception tasks. Results revealed that performance improved in the high LOA when task complexity was low. However, when task complexity increased, lower LOA improved performance. This opposite trend was also observed in the results for workload and situation awareness. We discuss the results in terms of the LOAs' impact on users' attitude towards automation and implications on usability. © 2022 Copyright held by the owner/author(s).",eldercare; levels of automation; Mobile robotic telepresence,Human engineering; Robotics; Robots; Eldercare; Levels of automation; Mobile robotic; Mobile robotic telepresence; Non-technical users; Performance; Task complexity; Teleoperated; Telepresence; Telepresence robots; Visual communication
Stop Ignoring Me! on Fighting the Trivialization of Social Robots in Public Spaces,2022,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127504921&doi=10.1145%2f3488241&partnerID=40&md5=50d537912f717ad9f839075f6910f35e,"Service and social robot in public scenarios will face various tasks in future applications, such as guiding people or admonishing them to provide assistance or convey social norms. Robots in public spaces might also incorporate roles of authority figures who might admonish people (e.g., security or guard robots). However, recent investigations showed that people ignore the admonishment of robots. Thus, in this work, we are looking at the reasons why people might ignore robots based on the Cognitive Dissonance Theory (CDT). We present the results of two consecutive field observations where a robot admonishes participants (i.e., pedestrians in a shopping mall) and requests them to stop using a smartphone while walking, which is considered an unmoral behavior. In the first field observation, we approached 160 participants over four days and conducted semi-structured interviews with 19 of them. Approximately half of the people ignored the robot, and half of them followed the instructions. Our interview results show that people who ignore the robot indeed use trivialization as a cognitive dissonance reduction strategy to justify ignoring the robot. Based on our analysis of the results, we developed a counter-trivialization strategy that anticipates this dissonance reduction strategy. We admonished 167 participants in our second field observation over four days, and our results show that significantly fewer people ignore the instructions of the robot when the robot uses a counter-trivialization strategy. © 2022 Association for Computing Machinery.",admonishment; cognitive dissonance theory; Social robots; trivialization,Admonishment; Cognitive dissonance theories; Field observations; Future applications; Public space; Reduction strategy; Service robots; Social norm; Social robots; Trivialization; Robots
The Influence of Robot Designs on Human Compliance and Emotion: A Virtual Reality Study in the Context of Future Public Transport,2022,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127513360&doi=10.1145%2f3507472&partnerID=40&md5=2b0c10c81b9a569762c9b811a57d039c,"As robots enter everyday environments, they start performing tasks originally performed by humans. One field of application is the public transport sector. The deployment of autonomous transport systems comes with a lack of human contact persons for help, guidance, and crowd management. This elicits challenges regarding redirecting and managing passengers. Current solutions on platforms can be replaced or enriched with service robots whose task includes crowd management as well as social interaction. This study investigates how the human-likeness of a robot influences the compliance and emotions of public transport users. A Virtual Reality experiment was conducted (N=33) to evaluate two different robot designs in a bus stop boarding scenario. The two robot designs differ in terms of humanoid appearance. In different experimental trials, participants had to perform a given task that was nullified by instructions from one of the two robots. Additionally, the dissonance of the situation was altered so that the environment either justified the robot's interference or not. Compliant behavior, pleasure, and arousal ratings, as well as task processing times were recorded. The experiment included an individual interview and a post-study questionnaire. The results suggest that future deployment of service robots has the potential to redirect passengers. In dissonant situations, clear reasoning must be given to make the robot effective. However, the robot's visual appearance has a more substantial impact on arousal and subjective preferences than on evoked behavior. The study implies that the presence of a service robot can influence peoples' choices and gives hints about the importance of giving a reason. However, objectively, the level of the robot's humanoid appearance did not make a difference. © 2022 Association for Computing Machinery.",affective design; crowd management; Human-robot interaction; public transport; service robots; virtual reality,Human robot interaction; Machine design; Mobile robots; Surveys; 'current; Affective design; Crowd managements; Humans-robot interactions; Public transport; Robot designs; Service robots; Social interactions; Transport sectors; Transport systems; Virtual reality
Coordinating Human-Robot Teams with Dynamic and Stochastic Task Proficiencies,2022,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124795217&doi=10.1145%2f3477391&partnerID=40&md5=85a629c03d6804a6a8d85d6405ad6b69,"As robots become ubiquitous in the workforce, it is essential that human-robot collaboration be both intuitive and adaptive. A robot's ability to coordinate team activities improves based on its ability to infer and reason about the dynamic (i.e., the ""learning curve"") and stochastic task performance of its human counterparts. We introduce a novel resource coordination algorithm that enables robots to schedule team activities by (1) actively characterizing the task performance of their human teammates and (2) ensuring the schedule is robust to temporal constraints given this characterization. We first validate our modeling assumptions via user study. From this user study, we create a data-driven prior distribution over human task performance for our virtual and physical evaluations of human-robot teaming. Second, we show that our methods are scalable and produce high-quality schedules. Third, we conduct a between-subjects experiment (n = 90) to assess the effects on a human-robot team of a robot scheduler actively exploring the humans' task proficiency. Our results indicate that human-robot working alliance () and human performance () are maximized when the robot dedicates more time to exploring the capabilities of human teammates. © 2021 Association for Computing Machinery.",human-robot teaming; optimization; Scheduling,Human resource management; Robots; Stochastic systems; Virtual reality; Dynamic tasks; Human robots; Human tasks; Human-robot collaboration; Human-robot teaming; Human-robot-team; Optimisations; Stochastic task; Task performance; User study; Scheduling
Tracking Anthropomorphizing Behavior in Human-Robot Interaction,2022,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124799659&doi=10.1145%2f3442677&partnerID=40&md5=8b54ad241fc736a8dd56661ec593ae58,"Existing methodologies to describe anthropomorphism in human-robot interaction often rely either on specific one-time responses to robot behavior, such as keeping the robot's secret, or on post hoc measures, such as questionnaires. Currently, there is no method to describe the dynamics of people's behavior over the course of an interaction and in response to robot behavior. In this paper, I propose a method that allows the researcher to trace anthropomorphizing and non-anthropomorphizing responses to robots dynamically moment-by-moment over the course of human-robot interactions. I illustrate this methodology in a case study and find considerable variation between participants, but also considerable intrapersonal variation in the ways the robot is anthropomorphized. That is, people may respond to the robot as if it was another human in one moment and to its machine-like properties in the next. These findings may influence explanatory models of anthropomorphism. © 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Anthropomorphism; computers-are-social-actors; interaction analysis; methodology; mindless transfer,Behavioral research; Man machine systems; Surveys; Anthropomorphism; Case-studies; Computers are social actors; Humans-robot interactions; Interaction analysis; Intra-personal variations; Methodology; Mindless transfer; People behavior; Robot behavior; Human robot interaction
A Humanoid Robot s Effortful Adaptation Boosts Partners' Commitment to an Interactive Teaching Task,2022,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124796956&doi=10.1145%2f3481586&partnerID=40&md5=c46867919a5c2a5b25c0e867ae27e19c,"We tested the hypothesis that, if a robot apparently invests effort in teaching a new skill to a human participant, the human participant will reciprocate by investing more effort in teaching the robot a new skill, too. To this end, we devised a scenario in which the iCub and a human participant alternated in teaching each other new skills. In the Adaptive condition of the robot teaching phase, the iCub slowed down its movements when repeating a demonstration for the human learner, whereas in the Unadaptive condition it sped the movements up when repeating the demonstration. In a subsequent participant teaching phase, human participants were asked to give the iCub a demonstration, and then to repeat it if the iCub had not understood. We predicted that in the Adaptive condition, participants would reciprocate the iCub's adaptivity by investing more effort to slow down their movements and to increase segmentation when repeating their demonstration. The results showed that this was true when participants experienced the Adaptive condition after the Unadaptive condition and not when the order was inverted, indicating that participants were particularly sensitive to the changes in the iCub's level of commitment over the course of the experiment. © 2021 Copyright held by the owner/author(s).",Commitment; human-robot interaction; kinematics; movement understanding; non-verbal communication,Anthropomorphic robots; Demonstrations; Teaching; Adaptivity; Commitment; Condition; Humanoid robot; Humans-robot interactions; Movement understanding; Non-verbal communications; Robot teaching; Human robot interaction
Impact of Anthropomorphic Robot Design on Trust and Attention in Industrial Human-Robot Interaction,2022,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124335516&doi=10.1145%2f3472224&partnerID=40&md5=90e0bf780df3d5ea2293bab3d82d10e0,"The application of anthropomorphic features to robots is generally considered beneficial for human-robot interaction (HRI). Although previous research has mainly focused on social robots, the phenomenon gains increasing attention in industrial human-Robot interaction as well. In this study, the impact of anthropomorphic design of a collaborative industrial robot on the dynamics of trust and visual attention allocation was examined. Participants interacted with a robot, which was either anthropomorphically or non-anthropomorphically designed. Unexpectedly, attribute-based trust measures revealed no beneficial effect of anthropomorphism but even a negative impact on the perceived reliability of the robot. Trust behavior was not significantly affected by an anthropomorphic robot design during faultless interactions, but showed a relatively steeper decrease after participants experienced a failure of the robot. With regard to attention allocation, the study clearly reveals a distracting effect of anthropomorphic robot design. The results emphasize that anthropomorphism might not be an appropriate feature in industrial HRI as it not only failed to reveal positive effects on trust, but distracted participants from relevant task areas which might be a significant drawback with regard to occupational safety in HRI. © 2021 Copyright held by the owner/author(s).",anthropomorphism; attention allocation; Human-robot interaction; robotic design; trust,Accident prevention; Anthropomorphic robots; Behavioral research; Industrial research; Machine design; Man machine systems; Occupational risks; Anthropomorphic design; Anthropomorphism; Attention allocation; Dynamics of trusts; Humans-robot interactions; Robot designs; Robotic design; Social robots; Trust; Visual Attention; Human robot interaction
Multi-modal Open World User Identification,2022,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116887507&doi=10.1145%2f3477963&partnerID=40&md5=adde19c14b7e2b1557d02208db8be6e9,"User identification is an essential step in creating a personalised long-term interaction with robots. This requires learning the users continuously and incrementally, possibly starting from a state without any known user. In this article, we describe a multi-modal incremental Bayesian network with online learning, which is the first method that can be applied in such scenarios. Face recognition is used as the primary biometric, and it is combined with ancillary information, such as gender, age, height, and time of interaction to improve the recognition. The Multi-modal Long-term User Recognition Dataset is generated to simulate various human-robot interaction (HRI) scenarios and evaluate our approach in comparison to face recognition, soft biometrics, and a state-of-the-art open world recognition method (Extreme Value Machine). The results show that the proposed methods significantly outperform the baselines, with an increase in the identification rate up to 47.9% in open-set and closed-set scenarios, and a significant decrease in long-term recognition performance loss. The proposed models generalise well to new users, provide stability, improve over time, and decrease the bias of face recognition. The models were applied in HRI studies for user recognition, personalised rehabilitation, and customer-oriented service, which showed that they are suitable for long-term HRI in the real world. © 2021 Copyright held by the owner/author(s).",Bayesian network; Human-Robot Interaction; incremental learning; long-term user recognition; multi-modal dataset; online learning; Open world recognition; soft biometrics,Bayesian networks; Biometrics; E-learning; Face recognition; Learning systems; Man machine systems; Bayesia n networks; Humans-robot interactions; Incremental learning; Long-term user recognition; Multi-modal; Multi-modal dataset; Online learning; Open world; Open world recognition; Soft biometrics; Human robot interaction
ACM Transactions on Human-Robot Interaction: The State of the Journal,2022,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124810404&doi=10.1145%2f3488567&partnerID=40&md5=282b92192366db3d22bdda249210f883,[No abstract available],,
The Effectiveness of Dynamically Processed Incremental Descriptions in Human Robot Interaction,2022,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124806151&doi=10.1145%2f3481628&partnerID=40&md5=78f2925328a99e2fb0ec8d79dd189d1d,"We explore the effectiveness of a dynamically processed incremental referring description system using under-specified ambiguous descriptions that are then built upon using linguistic repair statements, which we refer to as a dynamic system. We build a dynamically processed incremental referring description generation system that is able to provide contextual navigational statements to describe an object in a potential real-world situation of nuclear waste sorting and maintenance. In a study of 31 participants, we test the dynamic system in a case where a user is remote operating a robot to sort nuclear waste, with the robot assisting them in identifying the correct barrels to be removed. We compare these against a static non-ambiguous description given in the same scenario. As well as looking at efficiency with time and distance measurements, we also look at user preference. Results show that our dynamic system was a much more efficient method - taking only 62% of the time on average - for finding the correct barrel. Participants also favoured our dynamic system. © 2021 Association for Computing Machinery.",ambiguous; dynamic description; Human robot interaction; machine learning; natural language; robots for nuclear environments; spatial referring expressions; user study,Machine learning; Man machine systems; Ambiguous; Ambiguous description; Dynamic description; Humans-robot interactions; Natural languages; Nuclear environments; Referring expressions; Robot for nuclear environment; Spatial referring expression; User study; Human robot interaction
Social Robot Co-Design Canvases: A Participatory Design Framework,2022,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124808701&doi=10.1145%2f3472225&partnerID=40&md5=ff0868a26c8733af4d39b7d283db4580,"Design teams of social robots are often multidisciplinary, due to the broad knowledge from different scientific domains needed to develop such complex technology. However, tools to facilitate multidisciplinary collaboration are scarce. We introduce a framework for the participatory design of social robots and corresponding canvas tool for participatory design. The canvases can be applied in different parts of the design process to facilitate collaboration between experts of different fields, as well as to incorporate prospective users of the robot into the design process. We investigate the usability of the proposed canvases with two social robot design case studies: a robot that played games online with teenage users and a librarian robot that guided users at a public library. We observe through participants' feedback that the canvases have the advantages of (1) providing structure, clarity, and a clear process to the design; (2) encouraging designers and users to share their viewpoints to progress toward a shared one; and (3) providing an educational and enjoyable design experience for the teams. © 2021 Copyright held by the owner/author(s).",design tool; Human-robot interaction; participatory design; robot design; social robots,Libraries; Machine design; Co-designs; Design frameworks; Design team; Design tool; Design-process; Humans-robot interactions; Multi-disciplinary collaborations; Participatory design; Robot designs; Social robots; Human robot interaction
A Meta-analysis of the Uncanny Valley's Independent and Dependent Variables,2022,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114704437&doi=10.1145%2f3470742&partnerID=40&md5=9cec6364949b3df6a636427a095efd3f,"The uncanny valley (UV) effect is a negative affective reaction to human-looking artificial entities. It hinders comfortable, trust-based interactions with android robots and virtual characters. Despite extensive research, a consensus has not formed on its theoretical basis or methodologies. We conducted a meta-analysis to assess operationalizations of human likeness (independent variable) and the UV effect (dependent variable). Of 468 studies, 72 met the inclusion criteria. These studies employed 10 different stimulus creation techniques, 39 affect measures, and 14 indirect measures. Based on 247 effect sizes, a three-level meta-analysis model revealed the UV effect had a large effect size, Hedges' g = 1.01 [0.80, 1.22]. A mixed-effects meta-regression model with creation technique as the moderator variable revealed face distortion produced the largest effect size, g = 1.46 [0.69, 2.24], followed by distinct entities, g = 1.20 [1.02, 1.38], realism render, g = 0.99 [0.62, 1.36], and morphing, g = 0.94 [0.64, 1.24]. Affective indices producing the largest effects were threatening, likable, aesthetics, familiarity, and eeriness, and indirect measures were dislike frequency, categorization reaction time, like frequency, avoidance, and viewing duration. This meta-analysis - the first on the UV effect - provides a methodological foundation and design principles for future research. © 2021 Copyright held by the owner/author(s).",Anthropomorphism; computer animation; face perception; robotics; uncanny valley,Human robot interaction; Landforms; Regression analysis; Affective reactions; Anthropomorphism; Computer animation; Dependent variables; Effect size; Face perceptions; Independent variables; Indirect measure; Meta-analysis; Uncanny valley; Animation
Planning to Minimize the Human Muscular Effort during Forceful Human-Robot Collaboration,2022,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124806988&doi=10.1145%2f3481587&partnerID=40&md5=00e9242a563c3bc0f53cbd6e42ecfa14,"This work addresses the problem of planning a robot configuration and grasp to position a shared object during forceful human-robot collaboration, such as a puncturing or a cutting task. Particularly, our goal is to find a robot configuration that positions the jointly manipulated object such that the muscular effort of the human, operating on the same object, is minimized while also ensuring the stability of the interaction for the robot. This raises three challenges. First, we predict the human muscular effort given a human-robot combined kinematic configuration and the interaction forces of a task. To do this, we perform task-space to muscle-space mapping for two different musculoskeletal models of the human arm. Second, we predict the human body kinematic configuration given a robot configuration and the resulting object pose in the workspace. To do this, we assume that the human prefers the body configuration that minimizes the muscular effort. And third, we ensure that, under the forces applied by the human, the robot grasp on the object is stable and the robot joint torques are within limits. Addressing these three challenges, we build a planner that, given a forceful task description, can output the robot grasp on an object and the robot configuration to position the shared object in space. We quantitatively analyze the performance of the planner and the validity of our assumptions. We conduct experiments with human subjects to measure their kinematic configurations, muscular activity, and force output during collaborative puncturing and cutting tasks. The results illustrate the effectiveness of our planner in reducing the human muscular load. For instance, for the puncturing task, our planner is able to reduce muscular load by compared to a user-based selection of object poses. © 2021 Copyright held by the owner/author(s).",biomechanical modeling and limits; forceful human-robot collaboration; muscle estimation; Physical human-robot collaboration,Human robot interaction; Kinematics; Robot programming; Biomechanical model; Biomechanical modeling and limit; Forceful human-robot collaboration; Human-robot collaboration; Kinematic configuration; Muscle estimation; Muscular effort; Physical human-robot collaboration; Robot configurations; Muscle
"A Cloud-based Robot System for Long-term Interaction: Principles, Implementation, Lessons Learned",2022,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124797103&doi=10.1145%2f3481585&partnerID=40&md5=874f7f4ee5a4e8dec0028e15a377a564,"Making the transition to long-term interaction with social-robot systems has been identified as one of the main challenges in human-robot interaction. This article identifies four design principles to address this challenge and applies them in a real-world implementation: cloud-based robot control, a modular design, one common knowledge base for all applications, and hybrid artificial intelligence for decision making and reasoning. The control architecture for this robot includes a common Knowledge-base (ontologies), Data-base, ""Hybrid Artificial Brain""(dialogue manager, action selection and explainable AI), Activities Centre (Timeline, Quiz, Break and Sort, Memory, Tip of the Day, ), Embodied Conversational Agent (ECA, i.e., robot and avatar), and Dashboards (for authoring and monitoring the interaction). Further, the ECA is integrated with an expandable set of (mobile) health applications. The resulting system is a Personal Assistant for a healthy Lifestyle (PAL), which supports diabetic children with self-management and educates them on health-related issues (48 children, aged 6-14, recruited via hospitals in the Netherlands and in Italy). It is capable of autonomous interaction ""in the wild""for prolonged periods of time without the need for a ""Wizard-of-Oz""(up until 6 months online). PAL is an exemplary system that provides personalised, stable and diverse, long-term human-robot interaction. © 2021 Copyright held by the owner/author(s).",Cloud-based robots; conversational agents; long-term human-robot interaction; pervasive lifestyle support,Brain; Decision making; Intelligent robots; Knowledge based systems; Machine design; Man machine systems; Cloud-based; Cloud-based robot; Common knowledge; Conversational agents; Humans-robot interactions; Long-term human-robot interaction; Long-term interaction; Pervasive lifestyle support; Robots system; Social robots; Human robot interaction
Tactile Perception for Teleoperated Robotic Exploration within Granular Media,2021,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115151528&doi=10.1145%2f3459996&partnerID=40&md5=bbb54eb6b6b7ee37bed22cb7ae3482a8,"The sense of touch is essential for locating buried objects when vision-based approaches are limited. We present an approach for tactile perception when sensorized robot fingertips are used to directly interact with granular media particles in teleoperated systems. We evaluate the effects of linear and nonlinear classifier model architectures and three tactile sensor modalities (vibration, internal fluid pressure, fingerpad deformation) on the accuracy of estimates of fingertip contact state. We propose an architecture called the Sparse-Fusion Recurrent Neural Network (SF-RNN) in which sparse features are autonomously extracted prior to fusing multimodal tactile data in a fully connected RNN input layer. The multimodal SF-RNN model achieved 98.7% test accuracy and was robust to modest variations in granular media type and particle size, fingertip orientation, fingertip speed, and object location. Fingerpad deformation was the most informative modality for haptic exploration within granular media while vibration and internal fluid pressure provided additional information with appropriate signal processing. We introduce a real-time visualization of tactile percepts for remote exploration by constructing a belief map that combines probabilistic contact state estimates and fingertip location. The belief map visualizes the probability of an object being buried in the search region and could be used for planning.  © 2021 ACM.",Belief map; granular media; recurrent neural networks; sensor fusion; tactile perception; tactile sensors,Agricultural robots; Deformation; Granular materials; Location; Multilayer neural networks; Network architecture; Particle size; Robotics; Signal processing; Haptic explorations; Internal fluid pressure; Nonlinear classifiers; Real time visualization; Remote explorations; Robotic explorations; Tele-operated systems; Vision-based approaches; Recurrent neural networks
Methods for Expressing Robot Intent for Human-Robot Collaboration in Shared Workspaces,2021,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115157015&doi=10.1145%2f3472223&partnerID=40&md5=2bbe2b247fc02c5b939f107e2773aba1,"Human-robot collaboration is becoming increasingly common in factories around the world; accordingly, we need to improve the interaction experiences between humans and robots working in these spaces. In this article, we report on a user study that investigated methods for providing information to a person about a robot's intent to move when working together in a shared workspace through signals provided by the robot. In this case, the workspace was the surface of a tabletop. Our study tested the effectiveness of three motion-based and three light-based intent signals as well as the overall level of comfort participants felt while working with the robot to sort colored blocks on the tabletop. Although not significant, our findings suggest that the light signal located closest to the workspace-an LED bracelet located closest to the robot's end effector-was the most noticeable and least confusing to participants. These findings can be leveraged to support human-robot collaborations in shared spaces.  © 2021 ACM.",collaborative robots; feedback methods; Signaling robot intent,Agricultural robots; End effectors; Human-robot collaboration; Interaction experiences; Light signal; Robot's end effectors; Robots working; Shared spaces; Shared-workspace; User study; Social robots
Data-Driven Imitation Learning for a Shopkeeper Robot with Periodically Changing Product Information,2021,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115191628&doi=10.1145%2f3451883&partnerID=40&md5=941ed94c2f36da3977e3d1a9e033e9a1,"Data-driven imitation learning enables service robots to learn social interaction behaviors, but these systems cannot adapt after training to changes in the environment, such as changing products in a store. To solve this, a novel learning system that uses neural attention and approximate string matching to copy information from a product information database to its output is proposed. A camera shop interaction dataset was simulated for training/testing. The proposed system was found to outperform a baseline and a previous state of the art in an offline, human-judged evaluation.  © 2021 Owner/Author.",database question answering; Human-robot interaction; imitation learning; knowledge base question answering; retail robot; service robot; social robot,Agricultural robots; Educational robots; Robots; Approximate string matching; Data driven; Imitation learning; Offline; Product information; Service robots; Social interactions; State of the art; Learning systems
Robo Ludens: A Game Design Taxonomy for Multiplayer Games Using Socially Interactive Robots,2021,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115152132&doi=10.1145%2f3451343&partnerID=40&md5=ec5afbb384e800d3da314bd2b156b239,"The use of games as vehicles to study human-robot interaction (HRI) has been established as a suitable solution to create more realistic and naturalistic opportunities to investigate human behavior. In particular, multiplayer games that involve at least two human players and one or more robots have raised the attention of the research community. This article proposes a scoping review to qualitatively examine the literature on the use of multiplayer games in HRI scenarios employing embodied robots aiming to find experimental patterns and common game design elements. We find that researchers have been using multiplayer games in a wide variety of applications in HRI, including training, entertainment and education, allowing robots to take different roles. Moreover, robots have included different capabilities and sensing technologies, and elements such as external screens or motion controllers were used to foster gameplay. Based on our findings, we propose a design taxonomy called Robo Ludens, which identifies HRI elements and game design fundamentals and classifies important components used in multiplayer HRI scenarios. The Robo Ludens taxonomy covers considerations from a robot-oriented perspective as well as game design aspects to provide a comprehensive list of elements that can foster gameplay and bring enjoyable experiences in HRI scenarios.  © 2021 Owner/Author.",cognitive stimulation; education; embodiment; Game design; multiplayer; social interaction; social robotics; therapy,Agricultural robots; Behavioral research; Machine design; Social robots; Taxonomies; Human behaviors; Human robot Interaction (HRI); Interactive robot; Motion controller; Multiplayer games; Research communities; Sensing technology; Suitable solutions; Educational robots
Robot-Delivered Cognitive Stimulation Games for Older Adults: Usability and Acceptability Evaluation,2021,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115172379&doi=10.1145%2f3451882&partnerID=40&md5=a3bcf29795d3ab5385a49824a41b261c,"Cognitive stimulation games delivered on robots may be able to improve cognitive functioning and delay decline in older adults. However, little is known about older adults' in-depth opinions of robot-delivered games, as current research primarily focuses on technical development and one-off use. This article explores the usability, acceptability, and perceptions of community-dwelling older adults towards cognitive games delivered on a robot that incorporated movable interactive blocks. Semi-structured interviews were conducted with participants at the end of a 12-week cognitive stimulation games intervention delivered entirely on robots. Participants were 10 older adults purposively sampled from two retirement villages. A framework analysis approach was used to code data to predefined themes related to technology acceptance (perceived benefits, satisfaction, and preference), and usability (effectiveness, efficiency, and satisfaction). Results indicated that cognitive games delivered on a robot may be a valuable addition to existing cognitive stimulation activities. The robot was considered easy to use and useful in improving cognitive functioning. Future developments should incorporate interactive gaming tools, the use of social anthropomorphic robots, contrasting colour schemes to accommodate macular degeneration, and cultural-specific imagery and language. This will help cater to the preferences and age-related health needs of older adults, to ultimately enhance usability and acceptability.  © 2021 ACM.",cognitive robots; cognitive stimulation; gerontechnology; Human-robot interaction; interactive games; technology acceptance; user perspective,Agricultural robots; End effectors; Usability engineering; Analysis approach; Cognitive stimulations; Interactive gaming; Macular degeneration; Perceived benefits; Semi structured interviews; Technical development; Technology acceptance; Anthropomorphic robots
The Need for Verbal Robot Explanations and How People Would like a Robot to Explain Itself,2021,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115178637&doi=10.1145%2f3469652&partnerID=40&md5=244710134260e8a1697567f0d38d4c2f,"Although non-verbal cues such as arm movement and eye gaze can convey robot intention, they alone may not provide enough information for a human to fully understand a robot's behavior. To better understand how to convey robot intention, we conducted an experiment (N = 366) investigating the need for robots to explain, and the content and properties of a desired explanation such as timing, engagement importance, similarity to human explanations, and summarization. Participants watched a video where the robot was commanded to hand an almost-reachable cup and one of six reactions intended to show the unreachability: doing nothing (No Cue), turning its head to the cup (Look), or turning its head to the cup with the addition of repeated arm movement pointed towards the cup (Look & Point), and each of these with or without a Headshake. The results indicated that participants agreed robot behavior should be explained across all conditions, in situ, in a similar manner as what human explain, and provide concise summaries and respond to only a few follow-up questions by participants. Additionally, we replicated the study again with N = 366 participants after a 15-month span and all major conclusions still held.  © 2021 ACM.",behavior explanation; Robot explanation; system transparency,Agricultural robots; Eye movements; Arm movements; Eye-gaze; Follow up; Robot behavior; Robots
Are Preferences Useful for Better Assistance?: A Physically Assistive Robotics User Study,2021,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115156061&doi=10.1145%2f3472208&partnerID=40&md5=9af402abfa326ad77c760bc2c33672c6,"Assistive Robots have an inherent need of adapting to the user they are assisting. This is crucial for the correct development of the task, user safety, and comfort. However, adaptation can be performed in several manners. We believe user preferences are key to this adaptation. In this article, we evaluate the use of preferences for Physically Assistive Robotics tasks in a Human-Robot Interaction user evaluation. Three assistive tasks have been implemented consisting of assisted feeding, shoe-fitting, and jacket dressing, where the robot performs each task in a different manner based on user preferences. We assess the ability of the users to determine which execution of the task used their chosen preferences (if any). The obtained results show that most of the users were able to successfully guess the cases where their preferences were used even when they had not seen the task before. We also observe that their satisfaction with the task increases when the chosen preferences are employed. Finally, we also analyze the user's opinions regarding assistive tasks and preferences, showing promising expectations as to the benefits of adapting the robot behavior to the user through preferences.  © 2021 Owner/Author.",assistive dressing; assistive feeding; HRI; physically assistive robotics; shoe fitting; user preferences,Agricultural robots; Human robot interaction; Robotics; Assistive; Assistive robotics; Assistive robots; Robot behavior; User evaluations; User study; Behavioral research
What Happens When Robots Punish? Evaluating Human Task Performance during Robot-Initiated Punishment,2021,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115148616&doi=10.1145%2f3472207&partnerID=40&md5=7c541e0537cc70b911a60e4377f057ff,"This article examines how people respond to robot-administered verbal and physical punishments. Human participants were tasked with sorting colored chips under time pressure and were punished by a robot when they made mistakes, such as inaccurate sorting or sorting too slowly. Participants were either punished verbally by being told to stop sorting for a fixed time, or physically, by restraining their ability to sort with an in-house crafted robotic exoskeleton. Either a human experimenter or the robot exoskeleton administered punishments, with participant task performance and subjective perceptions of their interaction with the robot recorded. The results indicate that participants made more mistakes on the task when under the threat of robot-administered punishment. Participants also tended to comply with robot-administered punishments at a lesser rate than human-administered punishments, which suggests that humans may not afford a robot the social authority to administer punishments. This study also contributes to our understanding of compliance with a robot and whether people accept a robot's authority to punish. The results may influence the design of robots placed in authoritative roles and promote discussion of the ethical ramifications of robot-administered punishment.  © 2021 ACM.",authority; ethics; exoskeleton; Human-robot interaction (HRI); punishment; roboethics,Agricultural robots; Exoskeleton (Robotics); Machine design; Fixed time; Human tasks; Robotic exoskeletons; Subjective perceptions; Task performance; Time pressures; Social robots
Cognitive Impact of Anthropomorphized Robot Gaze: Anthropomorphic Gaze as Social Cues,2021,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115189883&doi=10.1145%2f3459994&partnerID=40&md5=0893e93bb12d3b3385817e21277a5dbd,"Attentional control does not have fix functioning and can be strongly impacted by the presence of other human beings or humanoid robots. In two studies, this phenomenon was investigated while focusing exclusively on robot gaze as a potential determinant of attentional control along with the role of participants' anthropomorphic inferences toward the robot. In study 1, we expected and found higher interference in trials including a direct robot gaze compared to an averted gaze on a task measuring attentional control (Eriksen flanker task). Participants' anthropomorphic inferences about the social robot mediated this interference. In study 2, we found that averted gazes congruent with the correct answer (same task as study 1) facilitated performance. Again, this effect was mediated by anthropomorphic inferences. These two studies show the importance of anthropomorphic robotic gaze on human cognitive processing, especially attentional control, and also open new avenues of research in social robotics.  © 2021 Owner/Author.",Anthropomorphism; Eriksen Flanker task; human-robot interaction; robot gaze; selective attention; Social robotics,Agricultural robots; Anthropomorphic robots; Robotics; Cognitive processing; Human being; Humanoid robot; Social cues; Social robotics; Social robots
Social Robots for the Care of Persons with Dementia: A Systematic Review,2021,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115155319&doi=10.1145%2f3469653&partnerID=40&md5=655bab8904e4a48c54fa3d47c781a75d,"Intelligent assistive robots can enhance the quality of life of people with dementia and their caregivers. They can increase the independence of older adults, reduce tensions between a person with dementia and their caregiver, and increase social engagement. This article provides a review of assistive robots designed for and evaluated by persons with dementia. Assistive robots that only increased mobility or brain-computer interfaces were excluded. Google Scholar, IEEE Digital Library, PubMed, and ACM Digital Library were searched. A final set of 53 articles covering research in 16 different countries are reviewed. Assistive robots are categorized into five different applications and evaluated for their effectiveness, as well as the robots' social and emotional capabilities. Our findings show that robots used in the context of therapy or for increasing engagement received the most attention in the literature, whereas the robots that assist by providing health guidance or help with an activity of daily living received relatively limited attention. PARO was the most commonly used robot in dementia care studies. The effectiveness of each assistive robot and the outcome of the studies are discussed, and particularly, the social/emotional capabilities of each assistive robot are summarized. Gaps in the research literature are identified and we provide directions for future work.  © 2021 ACM.",assistive robot; dementia; emotions; quality of life; Social robots,Agricultural robots; Brain computer interface; Digital libraries; Neurodegenerative diseases; Social robots; Activity of daily livings; Assistive robots; Limited attentions; Person with dementias; Persons with dementia; Quality of life; Social engagement; Systematic Review; Economic and social effects
Emerging Roles for Social Robots in Rehabilitation: Current Directions,2021,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111396699&doi=10.1145%2f3462256&partnerID=40&md5=f07f6253150df363708ece1c5b3da895,Insights from social and cognitive neuroscience should inform the design of socially assistive robots for neurorehabilitation as novel roles emerge for them in human-human interactions.  © 2021 ACM.,Human-robot interaction; neurorehabilitation; socially assistive robotics,Agricultural robots; Machine design; Cognitive neurosciences; Current direction; Human-human interactions; Neurorehabilitation; Socially assistive robots; Social robots
Mixed-initiative Variable Autonomy for Remotely Operated Mobile Robots,2021,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111177499&doi=10.1145%2f3472206&partnerID=40&md5=683088fcd5bdea50ed3458d279c6220a,"This article presents an Expert-guided Mixed-initiative Control Switcher (EMICS) for remotely operated mobile robots. The EMICS enables switching between different levels of autonomy during task execution initiated by either the human operator and/or the EMICS. The EMICS is evaluated in two disaster-response-inspired experiments, one with a simulated robot and test arena, and one with a real robot in a realistic environment. Analyses from the two experiments provide evidence that: (a) Human-Initiative (HI) systems outperform systems with single modes of operation, such as pure teleoperation, in navigation tasks; (b) in the context of the simulated robot experiment, Mixed-initiative (MI) systems provide improved performance in navigation tasks, improved operator performance in cognitive demanding secondary tasks, and improved operator workload compared to HI. Last, our experiment on a physical robot provides empirical evidence that identify two major challenges for MI control: (a) the design of context-aware MI control systems; and (b) the conflict for control between the robot's MI control system and the operator. Insights regarding these challenges are discussed and ways to tackle them are proposed.  © 2021 ACM.",conflict for control; human-robot interaction; Mixed-initiative control; shared control; variable autonomy,Agricultural robots; Cognitive systems; Control theory; Machine design; Man machine systems; Mobile robots; Remote control; Disaster response; Mixed initiative; Navigation tasks; Operator performance; Operator workload; Realistic environments; Simulated robot; Task executions; Social robots
Back-off: Evaluation of Robot Motion Strategies to Facilitate Human-Robot Spatial Interaction,2021,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111562649&doi=10.1145%2f3418303&partnerID=40&md5=bc2c0eb50c366cbc4417d8faa65d876e,"Standstill behavior by a robot is deemed to be ineffective and inefficient to convey a robot's intention to yield priority to another party in spatial interaction. Instead, robots could convey their intention and thus their next action via motion. We developed a back-off (BO) movement to communicate the intention of yielding priority to pedestrians at bottlenecks. To evaluate human sensory perception and subjective legibility, the BO is compared to three other motion strategies in a video study with 167 interviewees at the university and public spaces, where it excels regarding legibility. Implemented in a real encounter, objective motion behavior of 78 participants as a reaction to a stop-and-wait strategy, and two versions of BO (short and long), shows an improvement of the pedestrians' efficiency in the second encounter with the robot's short BO version compared to the stop strategy. Eventually, in the third encounter with all motion strategies, interaction causes only a small time consumption still required by the cognitive process of perceiving an object in the visual field. Hence, the design of kinematic parameters, BO path and time, exhibits the potential to increase the fluency of an interaction with robots at bottlenecks.  © 2021 Owner/Author.",Human-robot spatial interaction; legibility of motion; metrics for human-robot interaction,Agricultural robots; Machine design; Sensory perception; Cognitive process; Kinematic parameters; Motion behavior; Motion strategy; Spatial interaction; Stop-and-wait; Time consumption; Visual fields; Human robot interaction
Plant Robot for At-Home Behavioral Activation Therapy Reminders to Young Adults with Depression,2021,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111577320&doi=10.1145%2f3442680&partnerID=40&md5=99ebaa871afab79ec245d1b6133ea927,"Adolescents with depression who participate in behavioral activation therapy may find it hard to be motivated to perform tasks at home that their therapists recommend. We describe the initial design and usability evaluation of a home device (""PlantBot"") that could be used to remind young adults with depression at home of their behavioral activation therapy-related tasks. The prototype features electronics in a two-layer base, with a fake plant on top and supported using the Amazon Echo voice agent. We use an online panel study to evaluate the usability of our system with youth with past depression (N = 30). Initial findings highlight the device's usability, potential benefit, and attractiveness of the plant component, as well as multiple improvements to be made.  © 2021 ACM.",adolescent; Amazon Echo; Behavioral activation therapy; depression; Internet of Things; plant robot; voice agent; young adult,Agricultural robots; Robots; Home devices; Initial design; Panel studies; Plant components; Potential benefits; Prototype features; Usability evaluation; Young adults; Chemical activation
Design of Hesitation Gestures for Nonverbal Human-Robot Negotiation of Conflicts,2021,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111558095&doi=10.1145%2f3418302&partnerID=40&md5=852a706f4baa4e7533e1bb7447dab7cc,"When the question of who should get access to a communal resource first is uncertain, people often negotiate via nonverbal communication to resolve the conflict. What should a robot be programmed to do when such conflicts arise in Human-Robot Interaction? The answer to this question varies depending on the context of the situation. Learning from how humans use hesitation gestures to negotiate a solution in such conflict situations, we present a human-inspired design of nonverbal hesitation gestures that can be used for Human-Robot Negotiation. We extracted characteristic features of such negotiative hesitations humans use, and subsequently designed a trajectory generator (Negotiative Hesitation Generator) that can re-create the features in robot responses to conflicts. Our human-subjects experiment demonstrates the efficacy of the designed robot behaviour against non-negotiative stopping behaviour of a robot. With positive results from our human-robot interaction experiment, we provide a validated trajectory generator with which one can explore the dynamics of human-robot nonverbal negotiation of resource conflicts.  © 2021 ACM.",hesitation; Human-robot interaction; negotiation; nonverbal communication,Agricultural robots; Machine design; Man machine systems; Conflict situation; Human robots; Human subjects; Non-verbal communications; Non-verbal human; Resource conflict; Trajectory generator; Human robot interaction
Explaining in Time: Meeting Interactive Standards of Explanation for Robotic Systems,2021,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111615742&doi=10.1145%2f3457183&partnerID=40&md5=59eb6991f39b126fbd13e96fcfd2310a,"Explainability has emerged as a critical AI research objective, but the breadth of proposed methods and application domains suggest that criteria for explanation vary greatly. In particular, what counts as a good explanation, and what kinds of explanation are computationally feasible, has become trickier in light of oqaque ""black box""systems such as deep neural networks. Explanation in such cases has drifted from what many philosophers stipulated as having to involve deductive and causal principles to mere ""interpretation,""which approximates what happened in the target system to varying degrees. However, such post hoc constructed rationalizations are highly problematic for social robots that operate interactively in spaces shared with humans. For in such social contexts, explanations of behavior, and, in particular, justifications for violations of expected behavior, should make reference to socially accepted principles and norms. In this article, we show how a social robot's actions can face explanatory demands for how it came to act on its decision, what goals, tasks, or purposes its design had those actions pursue and what norms or social constraints the system recognizes in the course of its action. As a result, we argue that explanations for social robots will need to be accurate representations of the system's operation along causal, purposive, and justificatory lines. These explanations will need to generate appropriate references to principles and norms - explanations based on mere ""interpretability""will ultimately fail to connect the robot's behaviors to its appropriate determinants. We then lay out the foundations for a cognitive robotic architecture for HRI, together with particular component algorithms, for generating explanations and engaging in justificatory dialogues with human interactants. Such explanations track the robot's actual decision-making and behavior, which themselves are determined by normative principles the robot can describe and use for justifications.  © 2021 ACM.",architectural requirements; Explainability; normative HRI,Agricultural robots; Curricula; Decision making; Deep neural networks; Machine design; Philosophical aspects; Robotics; Black boxes; Cognitive robotics; Interpretability; Research objectives; Robotic systems; Social constraints; Social context; Target systems; Social robots
The Perceptual Belief Problem: Why Explainability Is a Tough Challenge in Social Robotics,2021,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111601983&doi=10.1145%2f3461781&partnerID=40&md5=fcb16dd923f7fb7c3e8ee87747437245,"The explainability of robotic systems depends on people's ability to reliably attribute perceptual beliefs to robots, i.e., what robots know (or believe) about objects and events in the world based on their perception. However, the perceptual systems of robots are not necessarily well understood by the majority of people interacting with them. In this article, we explain why this is a significant, difficult, and unique problem in social robotics. The inability to judge what a robot knows (and does not know) about the physical environment it shares with people gives rise to a host of communicative and interactive issues, including difficulties to communicate about objects or adapt to events in the environment. The challenge faced by social robotics researchers or designers who want to facilitate appropriate attributions of perceptual beliefs to robots is to shape human-robot interactions so that people understand what robots know about objects and events in the environment. To meet this challenge, we argue, it is necessary to advance our knowledge of when and why people form incorrect or inadequate mental models of robots' perceptual and cognitive mechanisms. We outline a general approach to studying this empirically and discuss potential solutions to the problem.  © 2021 ACM.",belief attribution; common ground; explainability; Human-robot interaction; intentional stance; intentionality; mental state attribution; predictability; social robotics; understandability,Agricultural robots; Robotics; Cognitive mechanisms; Mental model; Perceptual system; Physical environments; Robotic systems; Social robotics; Social robots
Building the Foundation of Robot Explanation Generation Using Behavior Trees,2021,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111579260&doi=10.1145%2f3457185&partnerID=40&md5=9bc542f85fe7fcadc7bcb479ad33db68,"As autonomous robots continue to be deployed near people, robots need to be able to explain their actions. In this article, we focus on organizing and representing complex tasks in a way that makes them readily explainable. Many actions consist of sub-actions, each of which may have several sub-actions of their own, and the robot must be able to represent these complex actions before it can explain them. To generate explanations for robot behavior, we propose using Behavior Trees (BTs), which are a powerful and rich tool for robot task specification and execution. However, for BTs to be used for robot explanations, their free-form, static structure must be adapted. In this work, we add structure to previously free-form BTs by framing them as a set of semantic sets {goal, subgoals, steps, actions} and subsequently build explanation generation algorithms that answer questions seeking causal information about robot behavior. We make BTs less static with an algorithm that inserts a subgoal that satisfies all dependencies. We evaluate our BTs for robot explanation generation in two domains: a kitting task to assemble a gearbox, and a taxi simulation. Code for the behavior trees (in XML) and all the algorithms is available at github.com/uml-robotics/robot-explanation-BTs.  © 2021 ACM.",Behavior explanation; behavior trees; robot explanation generation; robot transparency; state summarization,Agricultural robots; Computer aided software engineering; Forestry; Semantics; Taxicabs; Behavior trees; Complex actions; Complex task; Generation algorithm; Robot behavior; Semantic sets; Static structures; Sub-actions; End effectors
Introduction to the Special Issue on Explainable Robotic Systems,2021,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111555721&doi=10.1145%2f3461597&partnerID=40&md5=1c29243dd6cfb85e42a043bc5cc8c662,[No abstract available],,
On the Safety of Mobile Robots Serving in Public Spaces: Identifying gaps in en ISO 13482:2014 and calling for a new standard,2021,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111573430&doi=10.1145%2f3442678&partnerID=40&md5=9e48ba1c51b694b5fbc63ebc6e39e925,"Since 2014, a specific standard has been dedicated for the safety certification of personal care robots, which operate in close proximity to humans. These robots serve as information providers, object transporters, personal mobility carriers, and security patrollers. In this article, we point out the shortcomings concerning EN ISO 13482:2014, which encompasses guidelines regarding the safety and design of personal care robots. In particular, we argue that the current standard is not suitable for guaranteeing people's safety when these robots operate in public spaces. Specifically, the standard lacks requirements to protect pedestrians and bystanders. The guideline implicitly assumes that private spaces, such as households and offices, present the same hazards as in public spaces. We highlight the existence of at least three properties pertaining to robots' use in public spaces. These properties include (1) crowds, (2) social norms and proxemics rules, and (3) people's misbehaviours. We discuss how these properties impact robots' safety. This article aims to raise stakeholders' awareness on individuals' safety when robots are deployed in public spaces. This could be achieved by integrating the gaps present in EN ISO 13482:2014 or by creating a new dedicated standard.  © 2021 ACM.",Human-robot interaction; ISO standard; mobile robot; personal care robot; public spaces; safety; service robots,Agricultural robots; Machine design; Robots; Close proximity; Information provider; Personal care; Personal mobility; Private spaces; Public space; Safety and design; Safety certification; ISO Standards
Explainable Embodied Agents through Social Cues: A Review,2021,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111559802&doi=10.1145%2f3457188&partnerID=40&md5=da76b9030ddcc89853cecc646b1993cb,"The issue of how to make embodied agents explainable has experienced a surge of interest over the past 3 years, and there are many terms that refer to this concept, such as transparency and legibility. One reason for this high variance in terminology is the unique array of social cues that embodied agents can access in contrast to that accessed by non-embodied agents. Another reason is that different authors use these terms in different ways. Hence, we review the existing literature on explainability and organize it by (1) providing an overview of existing definitions, (2) showing how explainability is implemented and how it exploits different social cues, and (3) showing how the impact of explainability is measured. Additionally, we present a list of open questions and challenges that highlight areas that require further investigation by the community. This provides the interested reader with an overview of the current state of the art.  © 2021 ACM.",accountability; embodied social agents; explainability; explainable agency; expressive behavior; intelligibility; interpretability; legibility; predictability; robots; Transparency,Embodied agent; Social cues; State of the art; Agricultural robots
"Generating Legible and Glanceable Swarm Robot Motion through Trajectory, Collective Behavior, and Pre-attentive Processing Features",2021,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111563341&doi=10.1145%2f3442681&partnerID=40&md5=a437edb98403ef086bd4628a83ff0c3f,"As swarm robots begin to share the same space with people, it is critical to design legible swarm robot motion that clearly and rapidly communicates the intent of the robots to nearby users. To address this, we apply concepts from intent-expressive robotics, swarm intelligence, and vision science. Specifically, we leverage the trajectory, collective behavior, and density of swarm robots to generate motion that implicitly guides people's attention toward the goal of the robots. Through online evaluations, we compared different types of intent-expressive motions both in terms of legibility as well as glanceability, a measure we introduce to gauge an observer's ability to predict robots' intent pre-attentively. The results show that the collective behavior-based motion has the best legibility performance overall, whereas, for glanceability, trajectory-based legible motion is most effective. These results suggest that the optimal solution may involve a combination of these legibility cues based on the scenario and the desired properties of the motion.  © 2021 Owner/Author.",glanceability; human-multirobot interaction; Legible swarm robot motion,Agricultural robots; Data communication equipment; Machine design; Robots; Trajectories; Collective behavior; Expressive robotics; On-line evaluation; Optimal solutions; Pre-attentive; Swarm robots; Trajectory-based; Vision science; Swarm intelligence
"I See What You Did There"": Understanding People's Social Perception of a Robot and Its Predictability",2021,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111562019&doi=10.1145%2f3461534&partnerID=40&md5=df891c0777e4fda506b17c0caa3668db,"Unpredictability in robot behaviour can cause difficulties in interacting with robots. However, for social interactions with robots, a degree of unpredictability in robot behaviour may be desirable for facilitating engagement and increasing the attribution of mental states to the robot. To generate a better conceptual understanding of predictability, we looked at two facets of predictability, namely, the ability to predict robot actions and the association of predictability as an attribute of the robot. We carried out a video human-robot interaction study where we manipulated whether participants could either see the cause of a robot's responsive action or could not see this, because there was no cause, or because we obstructed the visual cues. Our results indicate that when the cause of the robot's responsive actions was not visible, participants rated the robot as more unpredictable and less competent, compared to when it was visible. The relationship between seeing the cause of the responsive actions and the attribution of competence was partially mediated by the attribution of unpredictability to the robot. We argue that the effects of unpredictability may be mitigated when the robot identifies when a person may not be aware of what the robot wants to respond to and uses additional actions to make its response predictable.  © 2021 Owner/Author.",human-robot interaction; Predictability; responsive actions; social perception,Agricultural robots; Behavioral research; Conceptual understanding; Mental state; Robot actions; Social interactions; Social perception; Visual cues; Social robots
Investigation of Model for Initial Phase of Communication: Analysis of Humans Interaction by Robot,2021,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185928431&doi=10.1145%2f3439719&partnerID=40&md5=55c05fcee2a03b95e9965a4a4a266de8,"We propose an agent model that determines its behavior from an internal state and a spatial relationship with a target to generate approaching and avoiding behaviors in encounter scenes. This model is based on the relationship with an opponent rather than with a scenario. The agent moves to increase the utility value obtained from the preferences for both aggressive and passive involvement. We analyzed the behavioral and utterance data of human–human interactions based only on two-dimensional position information by simple-shaped robots. The rate of participants’ behavior following the model was significantly higher than that of a random walker. Based on this result, we estimated the internal state during the interactions from the behavior of the participants and analyzed it. The words uttered by one member of a pair correlated with the internal state estimated from the behavior of the other member of the pair. The frequency of the internal states observed from the participants who were recommended to interact with the partner was different from that observed from the participants who did not receive such recommendations. These results suggest that a model with two preferences can approximate a human’s internal state in encounter scenes. © 2021 Association for Computing Machinery.",agent model; Initial phase of communication; spatial interaction,Agent modeling; Communication analysis; Human-human interactions; Humaninteraction; Initial phase of communication; Initial phasis; Internal state; Spatial interaction; Spatial relationships; Utility values; Human robot interaction
A taxonomy of social errors in human-robot interaction,2021,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106723180&partnerID=40&md5=e52b402e4c33ab39db7f35d977027f38,"Robotic applications have entered various aspects of our lives, such as health care and educational services. In such Human-robot Interaction (HRI), trust and mutual adaption are established and maintained through a positive social relationship between a user and a robot. This social relationship relies on the perceived competence of a robot on the social-emotional dimension. However, because of technical limitations and user heterogeneity, current HRI is far from error-free, especially when a system leaves controlled lab environments and is applied to in-the-wild conditions. Errors in HRI may either degrade a user's perception of a robot's capability in achieving a task (defined as performance errors in this work) or degrade a user's perception of a robot's socio-affective competence (defined as social errors in this work). The impact of these errors and effective strategies to handle such an impact remains an open question. We focus on social errors in HRI in this work. In particular, we identify the major attributes of perceived socio-affective competence by reviewing human social interaction studies and HRI error studies. This motivates us to propose a taxonomy of social errors in HRI. We then discuss the impact of social errors situated in three representative HRI scenarios. This article provides foundations for a systematic analysis of the social-emotional dimension of HRI. The proposed taxonomy of social errors encourages the development of user-centered HRI systems, designed to offer positive and adaptive interaction experiences and improved interaction outcomes.  © 2021 Copyright held by the owner/author(s).",Affective computing; human-robot interaction; social norms; social robotics; socio-affective competence,Agricultural robots; Behavioral research; Educational robots; Errors; Man machine systems; Social aspects; Taxonomies; Educational services; Emotional dimensions; Human robot Interaction (HRI); Human social interactions; Perceived competence; Robotic applications; Social relationships; Technical limitations; Social robots
Mental Models of a Mobile Shoe Rack,2021,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106687454&partnerID=40&md5=deb2e2c8d52c5a90ba2261bac57c8369,"Most people do not have direct access to knowledge about the inner workings of robots. Instead, they must develop mental models of the robot, a process that is not well understood. This article presents findings from a long-term, in-the-wild, qualitative, hypothesis-generating study of the mental model formation process. The focus was on how (qualitatively) users form mental models of the robot-specifically its perceptual capabilities, rules of behavior, and communication with other humans. Participants of diverse ages had multiple interactions with the robot over six weeks in a non-laboratory setting. The robot's rules of behavior were changed every two weeks. A novel, non-anthropomorphic robot was created for the study with a realistic use case: Storing people's shoes during a yoga class. This article reports findings from a case study analysis of 28 interviews conducted over six weeks with six participants. These findings are organized into six topics: (1) variability in the rate at which mental models are updated to be more predictive, (2) types of reasoning and hypothesizing about the robot, (3) borrowing from existing mental models and use of imagination, (4) attributing sensing capabilities where there are no visible sensors, (5) judgments about whether the robot is autonomous or teleoperated, and (6) experimenting with the robot. Specific suggestions for future research are given throughout, culminating in a set of study design recommendations. This work demonstrates the fruitfulness of long-term, in-the-wild studies of human-robot interaction, of which mental model formation is a foundational aspect.  © 2021 Copyright held by the owner/author(s).",in-the-wild studies; long-term studies; Mental models; natural settings; non-anthropomorphic robots; privacy; robotic furniture; transparency,Agricultural robots; Anthropomorphic robots; Cognitive systems; Human robot interaction; Case study analysis; Foundational aspects; Mental model; Multiple interactions; Study design; Teleoperated; Visible sensors; Formation process; In-the-wild study; Long term study; Model formations; Natural setting; Non-anthropomorphic robot; Privacy; Robotic furniture; User form; Social robots; Anthropomorphic robots
Understanding Cultural Preferences for Social Robots,2021,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106730296&partnerID=40&md5=1e7c28704ada94f4339ce4f868490044,"This article presents a study of cultural differences affecting the acceptance and design preferences of social robots. Based on a survey with 794 participants from Germany and the three Arab countries of Egypt, Jordan, and Saudi Arabia, we discuss how culture influences the preferences for certain attributes. We look at social roles, abilities and appearance, emotional awareness and interactivity of social robots, as well as the attitude toward automation. Preferences were found to differ not only across cultures, but also within countries with similar cultural backgrounds. Our findings also show a nuanced picture of the impact of previously identified culturally variable factors, such as attitudes toward traditions and innovations. While the participants' perspectives toward traditions and innovations varied, these factors did not fully account for the cultural variations in their perceptions of social robots. In conclusion, we believe that more real-life practices emerging from the situated use of robots should be investigated. Besides focusing on the impact of broader cultural values such as those associated with religion and traditions, future studies should examine how users interact, or avoid interaction, with robots within specific contexts of use.  ©2021 Copyright held by the owner/author(s).",cross-cultural study; cultural robotics; Human-robot interaction; social robots; technology acceptance,Agricultural robots; Machine design; Surveys; Social robots; Arab countries; Cultural backgrounds; Cultural difference; Cultural value; Design preferences; Emotional awareness; Interactivity; Variable factors; Cross-cultural study; Cultural robotic; Humans-robot interactions; Saudi Arabia; Social robots; Social roles; Technology acceptance; Social robots; Machine design
Automatic Speech Recognition for Indoor HRI Scenarios,2021,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106711397&partnerID=40&md5=08942d52041b7e2dafb96019ed9e9027,"This article presents a stand-alone automatic speech recognition system that accounts for listener movement, time-varying reverberation effects, environmental noise, and user position information for beamforming approaches in an HRI setting. We raise the importance of replacing the classical black-box integration of automatic speech recognition technology in HRI applications with the incorporation of the acoustic environment representation and modeling, and of the target source direction. Test data were recorded on a real robot under various moving conditions. For addressing the time-varying acoustic channel problem and incorporating environmental effect during training, clean speech samples were passed through estimated static channel responses and noise was added. Beamforming is investigated regarding oracle source tracking using, for instance, image processing. The proposed strategy is interesting for the robotics community, because it allows the development of voice-based HRI with limited training data and without relying on third-party technologies or Internet access eliminating the need to upload data to the cloud. In our mobile HRI scenario, the resulting speech recognition engine provided an average word error rate that is at least 19% and 34% lower than publicly available speech recognition APIs with the playback (i.e., loudspeaker) and human testing modalities, respectively.  © 2021 Copyright held by the owner/author(s).",ASR; Beamforming; DNN-HMM; indoor environments; time-varying acoustic channel,Acoustic noise; Agricultural robots; Beamforming; Educational robots; Image processing; Reverberation; Speech; Acoustic environment; Automatic speech recognition; Automatic speech recognition system; Automatic Speech Recognition Technology; Limited training data; Position information; Reverberation effects; Speech recognition engine; Speech recognition
Investigation of model for initial phase of communication,2021,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106672966&partnerID=40&md5=ef4bf9d8033bbafefc85fc2356f9173e,"We propose an agent model that determines its behavior from an internal state and a spatial relationship with a target to generate approaching and avoiding behaviors in encounter scenes. This model is based on the relationship with an opponent rather than with a scenario. The agent moves to increase the utility value obtained from the preferences for both aggressive and passive involvement. We analyzed the behavioral and utterance data of human-human interactions based only on two-dimensional position information by simple-shaped robots. The rate of participants' behavior following the model was significantly higher than that of a random walker. Based on this result, we estimated the internal state during the interactions from the behavior of the participants and analyzed it. The words uttered by one member of a pair correlated with the internal state estimated from the behavior of the other member of the pair. The frequency of the internal states observed from the participants who were recommended to interact with the partner was different from that observed from the participants who did not receive such recommendations. These results suggest that a model with two preferences can approximate a human's internal state in encounter scenes.  © 2021 Copyright held by the owner/author(s).",agent model; Initial phase of communication; spatial interaction,Agricultural robots; Agent model; Human-human interactions; Internal state; Random walkers; Spatial relationships; Utility values; Human robot interaction
A Minority of One against a Majority of Robots,2021,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106758987&partnerID=40&md5=1e52b960dd63dc277961d4d1b47671b7,"Studies have shown that people conform their answers to match those of group members even when they believe the group's answer to be wrong [2]. In this experiment, we test whether people conform to groups of robots and whether the robots cause informational conformity (believing the group to be correct), normative conformity (feeling peer pressure), or both. We conducted an experiment in which participants (N = 63) played a subjective game with three robots. We measured humans' conformity to robots by how many times participants changed their preliminary answers to match the group of robots' in their final answer. Participants in conditions that were given more information about the robots' answers conformed significantly more than those who were given less, indicating that informational conformity is present. Participants in conditions where they were aware they were a minority in their answers conformed more than those who were unaware they were a minority. Additionally, they also report feeling more pressure to change their answers from the robots, and the amount of pressure they reported was correlated to the frequency they conformed, indicating normative conformity. Therefore, we conclude that robots can cause both informational and normative conformity in people.  © 2021 Copyright held by the owner/author(s).",Human-robot interaction; informational conformity; normative conformity; peer pressure,Agricultural robots; Group members; Peer pressure; Condition; Humans-robot interactions; Informational conformity; Normative conformity; Robots; Human robot interaction
Human-collective collaborative target selection,2021,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106744517&partnerID=40&md5=1425420016c58a6caa8c1aef7be3aecc,"Robotic collectives are composed of hundreds or thousands of distributed robots using local sensing and communication that encompass characteristics of biological spatial swarms, colonies, or a combination of both. Interactions between the individual entities can result in emergent collective behaviors. Human operators in future disaster response or military engagement scenarios are likely to deploy semi-autonomous collectives to gather information and execute tasks within a wide area, while reducing the exposure of personnel to danger. This article presents and evaluates two action selection models in an experiment consisting of a single human operator supervising four simulated collectives. The action selection models have two parts: (1) a best-of-n decision-making model that attempts to choose the highest-quality target from a set of n targets and (2) a quorum sensing task sequencing model that enables autonomous target site occupation. An original biologically inspired insect colony decision model is compared to a bias-reducing model that attempts to reduce environmental bias, which can negatively influence collective best-of-n decisions when poorer-quality targets are easier to evaluate than higher-quality targets. The collective decision-making models are compared in both supervised and unsupervised trials. The bias-reducing model without human supervision is slower than the original model but is 57% more accurate for decisions where evaluating the optimal target is more difficult. Human-collective teams using the bias-reducing model require less operator influence and achieve 25% higher accuracy with difficult decisions compared to the teams using the original model.  ©2021 Copyright held by the owner/author(s).",collective decision making; Human-swarm interaction; swarm intelligence,Agricultural robots; Biomimetics; Decision making; Environmental microbiology; Personnel; Quality control; Action selection model; Biologically inspired; Collective behavior; Collective decision making; Decision making models; Distributed robots; Human supervision; Operator influence; Behavioral research
Making Appearances: How Robots Should Approach People,2021,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100940303&doi=10.1145%2f3385121&partnerID=40&md5=0ca6908326a378d29ff08a43ea26b8dc,"To prepare for a future in which robots are more commonplace, it is important to know what robot behaviors people find socially normative. Previous work suggests that for robots to be accepted by people, the robot should adhere to the prevalent social norms, such as those related to approaching people. However, we do not expect that socially normative approach behaviors for robots can be translated on a one-on-one basis from people to robots, because currently robots have unique and different features to humans, including (but not limited to) wheels, sounds, and shapes. The two studies presented in this article go beyond the state-of-the-art and focus on socially normative approach behaviors for robots. In the first study, we compared people's responses to violations of personal space done by robots compared to people. In the second study, we explored what features (sound, size, speed) of a robot approaching people have an effect on acceptance. Findings indicate that people are more lenient toward violations of a social norm by a robot as compared to a person. Also, we found that robots can use their unique features to mitigate the negative effects of norm violations by communicating intent. © 2021 Owner/Author.",functional noise; height; human-robot collaboration; human-robot interaction; interpersonal distance; personal space invasion; proxemics; robot approach; robot navigation; social norms; Social robots; velocity,Agricultural robots; Norm violation; Personal spaces; Robot behavior; Social norm; State of the art; Unique features; Social robots
"Designing Personas for Expressive Robots: Personality in the New Breed of Moving, Speaking, and Colorful Social Home Robots",2021,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100946743&doi=10.1145%2f3424153&partnerID=40&md5=64b7541e4e6e714e3c45f65822748f81,"Imbuing robots with personality has been shown to be an effective design approach in HRI, promoting user trust and acceptance. We explore personality design in a non-anthropomorphic voice-assisted home robot. Our design approach developed three distinct robot personas: Butler, Buddy, and Sidekick, intended to differ in proactivity and emotional impact. Persona differences were signaled to users by a combination of humanoid (speech, intonation), and indirect cues (colors and movement). We use Big Five personality theory to evaluate perceived differences between personas in an exploratory Wizard of Oz study. Participants were largely able to recognize underlying personality traits expressed through these cue combinations in ways that were consistent with our design goals. The proactive Buddy persona was judged as more Extravert than the more passive Sidekick persona, and the Butler persona was perceived as more Conscientious and less Neurotic than either Buddy or Butler personas. Users also had clear preferences between different personas; they wanted robots that mimicked but accentuated their own personality. Results suggest that future designs might exploit abstract cues to signal personality traits. © 2021 ACM.",Human robot interaction; non-humanoid; personality,Agricultural robots; Anthropomorphic robots; Behavioral research; Social robots; Cue combination; Design approaches; Design goal; Future designs; Personality theory; Personality traits; Pro activities; Wizard-of-oz studies; Machine design
"Effect Confirmed, Patient Dead: A Commentary on Hoffman &zhao's Primer for Conducting Experiments in HRI",2021,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100919222&doi=10.1145%2f3439714&partnerID=40&md5=07bef326ca0d40209c2332258c7fb236,"This article is a commentary on Hoffman 8 Zhao's ""A Primer for Conducting Experiments in Human-robot Interaction.""I argue that a too-narrow view of HRI methodology fails to address the dynamic systems properties of interaction. Furthermore, the focus on addressing the so-called ""replicability crisis""makes field studies next to impossible, inhibits interdisciplinarity and methodological pluralism, and draws our attention and resources away from the fact that contexts, people, cultures, expectations, and interaction itself may influence how social signals are interpreted. Therefore, in spite of its great benefits, the ""Primer""may not be taken as an instruction on ""how to carry out research in HRI""in general. © 2021 Owner/Author.",interaction; interdisciplinarity; Methodology; power analysis; replicability crisis,Agricultural robots; Field studies; Interdisciplinarity; Replicability; Social signals; Human robot interaction
The Power of Theory,2021,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101832181&doi=10.1145%2f3439716&partnerID=40&md5=3661c12f2606848e0d915c84a4f1b80c,"Hoffman & Zhao's paper ""A Primer for Conducting Experiments in Human-Robot Interaction""is an excellent paper overall with many practical suggestions throughout. We expect it to be used as a source for how to run human-subject experiments in the field of human-robot interaction, especially for people not trained directly in psychology.  © 2021 Copyright held by the owner/author(s).",Human robot interaction; psychology,Agricultural robots; Behavioral research; Man machine systems; Human subject experiments; Human robot interaction
"The Complexity of Human Social Interactions Calls for Mixed Methods in HRI: Comment on ""a Primer for Conducting Experiments in Human-robot Interaction,"" by G. Hoffman and X. Zhao",2021,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100920990&doi=10.1145%2f3439715&partnerID=40&md5=65c62c3be7c967bf88613c627f57fa9c,"In this research note, we offer a comment on the ""A Primer for Conducting Experiments in Human-robot Interaction,""by G. Hoffman and X. Zhao, suggesting that due to the complexity of human social reality quantitative methods should be integrated into a mixed method approach. © 2021 ACM.",Empirical research in HRI; mixed methods; responsible robotics,Agricultural robots; Man machine systems; Human social interactions; Mixed method; Quantitative method; Social reality; Social robots
The influence of robot number on robot group perception-A call for action,2020,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092712113&doi=10.1145%2f3394899&partnerID=40&md5=a062fa27683ace1338cdc9ac9d287825,"Research on robot groups has often applied psychological principles underlying group processes between humans to interactions with and between robots. However, such research has failed to test empirically whether these principles indeed apply to the robot context. For instance, the notion of a social group may be interpreted differently when facing human versus robot groups. Basic research on this issue is missing. Therefore, the present experiment aimed at integrating social psychological theorizing and research on robot groups by utilizing the principles of group entitativity. We examined the effect of robot number and similarity on the perception of these robots as a (social) group. To do so, participants saw pictures of one to ten robots, appearing low or high in similarity. Results showed that the aspects eliciting the perception of a social ""group""in humans seem to differ from the factors evoking robot group perception. According to our findings, at least three robots seem necessary for the perception of a robot ""group""to emerge. Social psychological research, however, has proposed that two persons suffice to elicit the notion of a human social group. Basic research is needed to substantiate assumptions drawn from social psychological theorizing before translating it into human-robot context. © 2020 ACM.",entitativity; group perception; Robot groups,Agricultural robots; Economic and social effects; Group process; Human robots; Psychological research; Robot group; Robot numbers; Social groups; Social robots
Where Do You Think You're Going?: Characterizing spatial mental models from planned routes,2020,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092709227&doi=10.1145%2f3385008&partnerID=40&md5=0e955dd72fe25ed1f04674ee5d9fd411,"Route planning is a critical behavior for human-intelligent agent (H-IA) team mobility. The scientific community has made major advances in improving route planner optimality and speed. However, human factors, such as the ability to predict and understand teammates' actions and goals, are necessary for trust development in H-IA teams. Trust is especially critical when agents' behaviors do not match human team members' expectations, or the human cannot understand the agent's underlying reasoning process. To address this issue, the artificial intelligence community has pushed toward creating human-like agent behaviors using machine learning. The problem with this approach is that we do not yet have a clear understanding of what constitutes human-like behavior across the breadth of tasks that H-IA teams undertake. This article describes an investigation and comparison of human and agent route planning behaviors, the interplay between humans and agents in collaborative planning, and the role of trust in this collaborative process. Finally, we propose a data-driven methodology for characterizing and visualizing differences among routes planned by humans and agents. This methodology provides a means to advance compatible mental model metrics and theory by informing targeted transparency manipulations, thereby improving the speed and quality of routes produced by H-IA teams. © 2020 ACM.",autonomous systems; GPS systems; Human-autonomy teaming; human-robot interaction; mental models; navigation systems; route planning; spatial cognition; spatial navigation; spatial planning,Agricultural robots; Artificial intelligence; Cognitive systems; Collaborative planning; Collaborative process; Critical behavior; Human-like agents; Reasoning process; Route planning; Scientific community; Spatial mental models; Behavioral research
"Embodiment, presence, and their intersections",2020,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092721004&doi=10.1145%2f3389210&partnerID=40&md5=1e347dbf1233a55ec55a5a0f407b7c2b,"Subjective experience of human control over remote, artificial, or virtual limbs has traditionally been investigated from two separate angles: presence research originates from teleoperation, aiming to capture to what extent the user feels like actually being in the remote or virtual environment. Embodiment captures to what extent a virtual or artificial limb is perceived as one's own limb. Unfortunately, the two research fields have not interacted much. This survey intends to provide a coherent overview of the literature at the intersection of these two fields to further that interaction. Two rounds of systematic research in topic-related data bases resulted in 414 related articles, 14 of which satisfy the deliberately strict inclusion criteria: 2 theoretical frameworks that highlighted intersections and 12 experimental studies that evaluated subjective measures for both concepts. Considering the surrounding literature as well, theoretical and experimental potential of embodiment and presence are discussed and suggestions to apply them in teleoperation research are derived. While increased publication activity is observed between 2016 and 2018, potentially caused by affordable virtual reality technologies, various open questions remain. To tackle them, human-in-the-loop experiments and three guiding principles for teleoperation system design (mechanical fidelity, spatial bodily awareness, and self-identification) are suggested. © 2020 Owner/Author.",Embodiment; human-robot interaction; presence; teleoperation,Agricultural robots; Remote control; Guiding principles; Human-in-the-loop; Publication activities; Subjective experiences; Systematic research; Teleoperation systems; Theoretical framework; Virtual reality technology; Virtual reality
Towards effective interface designs for collaborative HRI in manufacturing,2020,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089016586&doi=10.1145%2f3385009&partnerID=40&md5=576751ea93fc4b0260992e912f909233,"We present a comprehensive framework and test methodology for the evaluation of human-machine interfaces (HMI) and human-robot interactions (HRI) in collaborative manufacturing applications. An overview of the challenges that face current- and next-generation collaborative robot systems is presented, specifically focused on the interactions between man and machine, and a series of objectively quantitative and subjectively qualitative metrics are given to guide the development and assessment of interfaces and interactions. A generalized set of guidelines for the design of HMI is also proposed to address these challenges and thereby enable effective and intuitive diagnostics and error corrections when process failures occur. These guidelines are aimed at aiding researchers in developing effective interface and interaction technologies, maximizing operator situation awareness in human-robot collaborative manufacturing teams, promoting effective process and system diagnostics reporting, and enabling faster responses to equipment or application errors. © 2020 Public Domain.",benchmarking; data sets; peformance measures; repeatability studies; Test methods and metrics; use cases,Agricultural robots; Error correction; Man machine systems; Manufacture; Collaborative manufacturing; Collaborative robots; Human Machine Interface; Human robot Interaction (HRI); Interaction technology; Interface designs; Situation awareness; System diagnostics; Social robots
"Destruction, catharsis, and emotional release in human-robot interaction",2020,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090123724&doi=10.1145%2f3385007&partnerID=40&md5=cc3a21969080c092bcb5e39dc955946d,"The intersection between social, technical, and economic factors biases new product development to focus on utilitarian value. However, objects that serve alternative goals, behaviors and emotions have accompanied humankind for millennia. This article speculates about robotic objects for one non-utilitarian behavior and its implications: destruction. Robots and objects for destruction have a shared history of embodiment and heavily rely on their embodiment for interaction. Yet the topic of destruction is not very common in the field of human-robot interaction (HRI). Thus, we (1) present a survey of ethnographic investigations that show modes of HRI related to destruction, and (2) develop speculative concepts of interaction that demonstrate these ideas in HRI. By exemplifying a broad range of speculative uses of destruction in HRI and grounding it in literature, we hope this theoretical and conceptual article will bring a fresh perspective on alternative interactions with robots. © 2020 Owner/Author.",catharsis; creation; destruction; embodied interaction; HRI; human-object interaction; human-robot interaction; Interaction design; material culture; social robots; speculative design,Agricultural robots; Man machine systems; Product development; Economic factors; Human robot Interaction (HRI); New product development; Human robot interaction
Measuring the perceived social intelligence of robots,2020,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092724959&doi=10.1145%2f3415139&partnerID=40&md5=2756a6b588510e50b57f3d42553c1aa3,"Robotic social intelligence is increasingly important. However, measures of human social intelligence omit basic skills, and robot-specific scales do not focus on social intelligence. We combined human robot interaction concepts of beliefs, desires, and intentions with psychology concepts of behaviors, cognitions, and emotions to create 20 Perceived Social Intelligence (PSI) Scales to comprehensively measure perceptions of robots with a wide range of embodiments and behaviors. Participants rated humanoid and non-humanoid robots interacting with people in five videos. Each scale had one factor and high internal consistency, indicating each measures a coherent construct. Scales capturing perceived social information processing skills (appearing to recognize, adapt to, and predict behaviors, cognitions, and emotions) and scales capturing perceived skills for identifying people (appearing to identify humans, individuals, and groups) correlated strongly with social competence and constituted the Mind and Behavior factors. Social presentation scales (appearing friendly, caring, helpful, trustworthy, and not rude, conceited, or hostile) relate more to Social Response to Robots Scales and Godspeed Indices, form a separate factor, and predict positive feelings about robots and wanting social interaction with them. For a comprehensive measure, researchers can use all PSI 20 scales for free. Alternatively, they can select the most relevant scales for their projects. © 2020 ACM.",human-computer interaction; human-robot interaction; Social intelligence; socially assistive robotics,Agricultural robots; Anthropomorphic robots; Behavioral research; Economic and social effects; Intelligent robots; Behavior factor; Humanoid robot; Internal consistency; Positive feelings; Social competences; Social information processing; Social intelligence; Social interactions; Social robots
Human perception of social robot s emotional states via facial and thermal expressions,2020,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092731982&doi=10.1145%2f3388469&partnerID=40&md5=8d4cd6341b5316a34a70d106a45c00c9,"Facial and thermal expressions can be used by humans to interpret emotions. While facial expressions can be a voluntary reaction, the change of temperature in the body is often not. Thus, a facial expression may not always be consistent with the emotional state that is expressed by the body temperature. This article aims to study the human perception of the emotional expression and the emotional state of a robot that simultaneously uses its face and body temperature. To this end, a robot, named TherMoody, which has the capability to change its body temperature from 10-55°, was used. Then, 25 combinations (5x5: one facial expression and one thermal expression for anger, joy, fear, sadness, and neutral state) were evaluated by 15 participants. Thermal expressions were designed based on the metaphors of emotions related to temperature. The results show people tend to base their judgment of the robot's emotional expression exclusively in its facial expression, regardless of the change of its body temperature. However, there are combinations where the thermal expression predominates over the facial expression when judging the robot's emotional state. Thus, these combinations may produce the perception that the robot's emotional expression is, in fact, genuine, simulated, masked, or neutralized. © 2020 ACM.",and genuine/simulated/masked/neutralized emotions; body temperature; emotion regulation; face; Robot expressions; social robot,Agricultural robots; Behavioral research; Physiology; Body temperature; Change of temperatures; Emotional expressions; Emotional state; Facial Expressions; Human perception; Neutral state; Social robots
Multimodal Physiological Signals for Workload Prediction in Robot-assisted Surgery,2020,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103776257&doi=10.1145%2f3368589&partnerID=40&md5=8570f0bbf495ade13469e744d59821d7,"Monitoring surgeon workload during robot-assisted surgery can guide allocation of task demands, adapt system interfaces, and assess the robotic system’s usability. Current practices for measuring cognitive load primarily rely on questionnaires that are subjective and disrupt surgical workflow. To address this limitation, a computational framework is demonstrated to predict user workload during telerobotic surgery. This framework leverages wireless sensors to monitor surgeons’ cognitive load and predict their cognitive states. Continuous data across multiple physiological modalities (e.g., heart rate variability, electrodermal, and electroencephalogram activity) were simultaneously recorded for twelve surgeons performing surgical skills tasks on the validated da Vinci Skills Simulator. These surgical tasks varied in difficulty levels, e.g., requiring varying visual processing demand and degree of fine motor control. Collected multimodal physiological signals were fused using independent component analysis, and the predicted results were compared to the ground-truth workload level. Results compared performance of different classifiers, sensor fusion schemes, and physiological modality (i.e., prediction with single vs. multiple modalities). It was found that our multisensor approach outperformed individual signals and can correctly predict cognitive workload levels 83.2% of the time during basic and complex surgical skills tasks. © 2020 Copyright held by the owner/author(s). Publication rights licensed to ACM.",machine learning; multimodal sensing; resilience; robot-assisted surgery; sensor fusion; Workload prediction,Cameras; Independent component analysis; Machine learning; Physiology; Robotic surgery; Surgical equipment; Transplantation (surgical); Cognitive loads; Machine-learning; Multi-modal; Multimodal sensing; Physiological signals; Resilience; Robot-assisted surgery; Sensor fusion; Task demand; Workload predictions; Forecasting
Robot-centric perception of human groups,2020,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091932561&doi=10.1145%2f3375798&partnerID=40&md5=bfcf1bc5a5eda9f5bb344a0ce5c94f00,"The robotics community continually strives to create robots that are deployable in real-world environments. Often, robots are expected to interact with human groups. To achieve this goal, we introduce a new method, the Robot-Centric Group Estimation Model (RoboGEM), which enables robots to detect groups of people. Much of the work reported in the literature focuses on dyadic interactions, leaving a gap in our understanding of how to build robots that can effectively team with larger groups of people. Moreover, many current methods rely on exocentric vision, where cameras and sensors are placed externally in the environment, rather than onboard the robot. Consequently, these methods are impractical for robots in unstructured, human-centric environments, which are novel and unpredictable. Furthermore, the majority of work on group perception is supervised, which can inhibit performance in real-world settings. RoboGEM addresses these gaps by being able to predict social groups solely from an egocentric perspective using color and depth (RGB-D) data. To achieve group predictions, RoboGEM leverages joint motion and proximity estimations. We evaluated RoboGEM against a challenging, egocentric, real-world dataset where both pedestrians and the robot are in motion simultaneously, and show RoboGEM outperformed two state-of-The-Art supervised methods in detection accuracy by up to 30%, with a lower miss rate. Our work will be helpful to the robotics community, and serve as a milestone to building unsupervised systems that will enable robots to work with human groups in real-world environments.  © 2020 ACM.",Group detection; human-robot teaming; robot perception; robots,Agricultural robots; Robotics; Detection accuracy; Dyadic interaction; Estimation models; Human-centric; Real world environments; Real world setting; Social groups; Supervised methods; Social robots
Robot errors in proximate hri how functionality framing affects perceived reliability and trust,2020,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091901668&doi=10.1145%2f3380783&partnerID=40&md5=a2c3ecc427055ac90ef02fdf1cf65455,"Advancements within human-robot interaction generate increasing opportunities for proximate, goal-directed joint action (GDJA). However, robot errors are common and researchers must determine how to mitigate them. In this article, we examine how expectations for robot functionality affect people's perceptions of robot reliability and trust for a robot that makes errors. Here 35 participants (n = 35) performed a collaborative banner-hanging task with an autonomous mobile manipulator (Toyota HSR). Each participant received either a low-or high-functionality framing for the robot. We then measured how participants perceived the robot's reliability and trust prior to, during, and after interaction. Functionality framing changed how robot errors affected participant experiences of robot behavior. People with low expectations experienced positive changes in reliability and trust after interacting with the robot, while those with high expectations experienced a negative change in reliability and no change in trust. The low-expectation group also showed greater trust recovery following the robot's first error compared to the high group. Our findings inform human-robot teaming through: (1) identifying robot presentation factors that can be employed to facilitate trust calibration and (2) establishing the effects of framing, functionality, and the interactions between them to improve dynamic models of human-robot teaming.  © 2020 ACM.",Human robot teaming; joint action; proximate interaction; robot reliability; trust,Agricultural robots; Errors; Manipulators; Reliability; Autonomous mobile manipulator; Goal directed; Human robots; Joint actions; Positive changes; Robot behavior; Social robots
Laying the groundwork for intra-robotic-natural limb coordination,2020,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091956081&doi=10.1145%2f3377329&partnerID=40&md5=2711f5ddfa51a06b23ece599dcb4dc9d,"Supernumerary Robotic Limbs (SRLs) have been successfully applied in bracing and as an assistive technology for people with disabilities. These tasks only require perception internal to the SRL-human system. However, SRLs show promise in applications requiring external perception such as opening a door when one's hands are full. One path toward developing SRLs that accomplish these tasks is to use human-in-The-loop control, thus leveraging the human's superior perception system to help the SRLs. However, the effects on the user of controlling additional limbs are unclear. This article presents an experimental study where humans, wearing two single degree of freedom SRLs, were instructed to minimize the position error between the subject's natural and robotic limbs and the corresponding targets, one for each limb. First, subjects performed worse with their natural limbs when asked to perform the task with two natural and two robotic limbs as opposed to with just their natural limbs, suggesting that shared control could help. Second, subjects moved their natural limbs together followed by moving their SRLs together. This informs both the choice of control scheme for the SRLs and the division of labor within a task. Third, subjects showed significant concurrent use of the natural and robotic limbs.  © 2020 ACM.",Supernumerary robotic limbs,Agricultural robots; Degrees of freedom (mechanics); Assistive technology; Control schemes; Division of labor; Human-in-the-loop control; People with disabilities; Perception systems; Position errors; Single degree of freedoms; Robotics
Challenges in designing a fully autonomous socially assistive robot for people with parkinsons disease,2020,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091943090&doi=10.1145%2f3379179&partnerID=40&md5=87d4c3cef00e1b1b5aa2d6586b451472,"Assistive robots are becoming an increasingly important application platform for research in robotics, AI, and HRI, as there is a pressing need to develop systems that support the elderly and people with disabilities, with a clear path to market. Yet, what remains unclear is whether current autonomous systems are already up to the task or whether additional HRI work is needed to make these systems acceptable and useful. In this article, we report our efforts of developing and evaluating an architecture for a fully autonomous robot designed to assist older adults with Parkinson's disease (PD) in sorting their medications. The main goal for the robot is to aid users in a manner that maintains the autonomy of the user by providing cognitive and social support with varying levels of assistance. We first evaluated the robot with subjects drawn from a pool of university students, which is common practice in experimental work in psychology and HRI. As the results were very positive, we followed up with an evaluation using people with Parkinson's disease, who surprisingly had mostly negative outcomes. We thus report our analysis of the differences in the evaluations and discuss the challenges for HRI posed by the sources of the negative evaluations: (1) designing a robot to adapt to the many routines the participants use at home, (2) unique needs of participants with PD not present in student participants, and (3) the role of familiar technologies in designing and evaluating a new technology. While it is unlikely, given the current state of technology, that fully autonomous assistive robots for older adults will be available in the near term, we believe that our work exposes a critical need in HRI to involve the target population as early as possible in the design process.  © 2020 Owner/Author.",assistive technology; cognitive robot; robot architecture; Social robot,Agricultural robots; Neurodegenerative diseases; Social robots; Application platforms; Assistive robots; Autonomous systems; Parkinson's disease; People with disabilities; Social support; Socially assistive robots; University students; Machine design
Personality traits for a social mediator robot encouraging elderly self-disclosure on loss experiences,2020,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091283890&doi=10.1145%2f3377342&partnerID=40&md5=f0884070ec7edbbf53c4203a187661d5,"To prevent elderly people from being socially isolated, encouraging their self-disclosure takes an important role. We discuss a use case of social robots in which they are deployed as mediators for humans that intermediate remote communication between elderly people and their family members or friends. The goal of this article is to present a design guideline for such social mediator robots based on results obtained from two studies in which a total of 741 elderly people participated. In study 1, we explored topics in dialogues and found that a social mediator robot could well encourage the self-disclosure of the elderly people, particularly in topics of which they usually feel resistance in talking to others (e.g., loss experiences). Thus, we confirmed the feasibility of the social mediator robot. Study 2 pursued the effective personality traits of the social mediator robot. We re-investigated a well-studied research question of matching robot personality to the user. The results provided more detailed knowledge as to similarity-Attraction/repulsion than had been reported previously. Finally, design recommendations were discussed by considering the personality traits of the elderly users as well.  © 2020 ACM.",matching personality; older persons; robot design; self-disclosure; Social mediator robot,Agricultural robots; Machine design; Design recommendations; Elderly people; Elderly users; Personality traits; Remote communication; Research questions; Robot personalities; Self-disclosure; Social robots
The robot makers,2020,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090497600&doi=10.1145%2f3377343&partnerID=40&md5=a0f147322dc149e93941b320bdaac2bd,"This article is an ethnographic exploration of robot anthropomorphism at a robotics company. It draws on a 10-month participatory ethnography among a robotics company, an anthropologist, and a social robotics research lab. In contrast to psychological methods, this anthropological participatory ethnography integrates all stakeholders' insights, offering holistic understandings of robots' in situ operations throughout the fieldwork, data-sharing, interviews, and analysis. In particular, this article unravels employee social constructions of the company's self-driving factory transport vehicles, ""the robots.""These robots are deployed across a variety of warehouses and factories in North America. Our results involve an assessment of six teams at the robotics company's headquarters: Those testing robots, those developing their hardware and software, and those working with customers. We unpack trends of anthropomorphism for each of these teams and across the company.  © 2020 Owner/Author.",Collaborative industrial robots; collaborative manufacturing; ethnography in HRI; expressive motions; human-robot social interaction; legibility; minimal social robots; robot anthropomorphism; robot sociability; social intelligence; social robotics,Agricultural robots; Data Sharing; Robotics; Software testing; Hardware and software; Self drivings; Social constructions; Social robotics; Transport vehicles; Robots
A Conversational robot for older adults with alzheimers disease,2020,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091905555&doi=10.1145%2f3380785&partnerID=40&md5=78cf263c491e61fb54d0fda6dc725711,"Amid the rising cost of Alzheimer's disease (AD), assistive health technologies can reduce care-giving burden by aiding in assessment, monitoring, and therapy. This article presents a pilot study testing the feasibility and effect of a conversational robot in a cognitive assessment task with older adults with AD. We examine the robot interactions through dialogue and miscommunication analysis, linguistic feature analysis, and the use of a qualitative analysis, in which we report key themes that were prevalent throughout the study. While conversations were typically better with human conversation partners (being longer, with greater engagement and less misunderstanding), we found that the robot was generally well liked by participants and that it was able to capture their interest in dialogue. Miscommunication due to issues of understanding and intelligibility did not seem to deter participants from their experience. Furthermore, in automatically extracting linguistic features, we examine how non-Acoustic aspects of language change across participants with varying degrees of cognitive impairment, highlighting the robot's potential as a monitoring tool. This pilot study is an exploration of how conversational robots can be used to support individuals with AD.  © 2020 Owner/Author.",Assistive technology; dementia; dialogue; human-robot interaction,Agricultural robots; Disease control; Linguistics; Neurodegenerative diseases; Alzheimer's disease; Cognitive assessments; Cognitive impairment; Health technology; Linguistic features; Monitoring tools; Qualitative analysis; Robot interactions; Robots
Enhancement and application of a UAV control interface evaluation technique: Modified GEDIS-UAV,2020,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089761147&doi=10.1145%2f3368943&partnerID=40&md5=23da8621fa72d75104c98c8d96448658,"UAV supervisory control interfaces are important for safe operations and mission performance. We reviewed existing UAV interface design and evaluation tools and identified limitations. To address issues with existing methods, we developed an enhanced evaluation tool, the M-GEDIS-UAV. The tool includes detailed criteria for all aspects of UAV control interface design to support operator performance. It also supports quantitative and objective assessment of an interface. We prototyped three UAV information displays, including a digital control display, analog control display, and “massive” data display, as part of a simulated supervisory control interface. Six analysts, including three human factors experts and three novices evaluated the interfaces using the M-GEDIS-UAV. Inter-rater reliability was high for the human factors experts, suggesting training in usability analysis is necessary for tool application. Results also revealed the massive data display to produce significantly lower evaluation scores than the other displays. We concluded that the M-GEDIS-UAV was sensitive to interface manipulations and was most effectively used by human factors experts. Using the M-GEDIS-UAV tool can reveal the majority of design deviations from guidelines early in the design process toward increasing the effectiveness of control interfaces. © 2020 Copyright held by the owner/author(s).",GEDIS-UAV; Human factors experts; Interface design; UAV; Usability assessment,Digital control systems; Human engineering; Reliability analysis; Unmanned aerial vehicles (UAV); Information display; Inter-rater reliabilities; Interface designs; Mission performance; Objective assessment; Operator performance; Supervisory control; Usability assessment; Aircraft control
Trust-aware decision making for human-robot collaboration: Model learning and planning,2020,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086562607&doi=10.1145%2f3359616&partnerID=40&md5=7f6446baea1ba8b38ae9f1a73a142939,"Trust in autonomy is essential for effective human-robot collaboration and user adoption of autonomous systems such as robot assistants. This article introduces a computational model that integrates trust into robot decision making. Specifically, we learn from data a partially observable Markov decision process (POMDP) with human trust as a latent variable. The trust-POMDP model provides a principled approach for the robot to (i) infer the trust of a human teammate through interaction, (ii) reason about the effect of its own actions on human trust, and (iii) choose actions that maximize team performance over the long term. We validated the model through human subject experiments on a table clearing task in simulation (201 participants) and with a real robot (20 participants). In our studies, the robot builds human trust by manipulating low-risk objects first. Interestingly, the robot sometimes fails intentionally to modulate human trust and achieve the best team performance. These results show that the trust-POMDP calibrates trust to improve human-robot team performance over the long term. Further, they highlight that maximizing trust alone does not always lead to the best performance. © 2020 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Human-robot collaboration; Partially observable Markov decision process (POMDP); Trust models,Behavioral research; Decision making; Decision support systems; Markov processes; Robot programming; Autonomous systems; Computational model; Human subject experiments; Human-robot collaboration; Human-robot-team; Partially observable Markov decision process; Team performance; Trust models; Human robot interaction
Transparency about a robot's lack of human psychological capacities: Effects on child-robot perception and relationship formation,2020,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089759274&doi=10.1145%2f3365668&partnerID=40&md5=95a992213c80028bacca0548100cb503,"The increasing sophistication of social robots has intensified calls for transparency about robots' machine nature. Initial research has suggested that providing children with information about robots' mechanical status does not alter children's humanlike perception of, and relationship formation with, social robots. Against this background, our study experimentally investigated the effects of transparency about a robot's lack of human psychological capacities (intelligence, self-consciousness, emotionality, identity construction, social cognition) on children's perceptions of a robot and their relationship to it. Our sample consisted of 144 children aged 8 to 9 years old who interacted with the Nao robot in either a transparent or a control condition. Transparency decreased children's humanlike perception of the robot in terms of animacy, anthropomorphism, social presence, and perceived similarity. Transparency reduced child-robot relationship formation in terms of decreased trust, while children's feelings of closeness toward the robot were not affected. © 2020 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Child-robot interaction; Child-robot relationship formation; Human-robot interaction; Robot ethics; Robotics,Behavioral research; Human robot interaction; Intelligent robots; Robotics; Transparency; Animacy; Child-robot interactions; Child-robot relationship formation; Humanlike perception; Robot ethics; Robot perception; Social cognition; Social presence; Social robots
Robot-Assisted Tower Construction—A Method to Study the Impact of a Robot's Allocation Behavior on Interpersonal Dynamics and Collaboration in Groups,2020,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097577304&doi=10.1145%2f3394287&partnerID=40&md5=f7a5e8f14142d9b1a8ef4107f94266d0,"Research on human-robot collaboration or human-robot teaming, has focused predominantly on understanding and enabling collaboration between a single robot and a single human. Extending human-robot collaboration research beyond the dyad, raises novel questions about how a robot should allocate resources among group members and about what the consequences of such allocation are for a group's social dynamics and outcomes. Methodological advances are needed to answer these questions allow researchers to collect data about a robot's impact not only on interactions with the robot but also on interactions of people with each other. This paper presents Robot Assisted Tower Construction, a novel task that allows researchers to examine the impact of a robot's allocation behavior on the dynamics of a group or team collaborating on a task. By focusing on the question of whether and how a robot's allocation of resources (wooden blocks required for a building task) affects collaboration dynamics and outcomes, a case is provided of how this task can be applied in a laboratory study with 124 participants to collect data about human robot collaboration that involves a group of people. We highlight the kinds of insights the task can yield and how it can be adapted to various human robot collaboration contexts.  © 2020 ACM.",and teams; groups; Human-robot collaboration; human-robot interaction; human-robot teaming; interpersonal dynamics; research method,Agricultural robots; Data acquisition; Dynamics; Group members; Human robots; Human-robot collaboration; Laboratory studies; Novel task; Single robots; Social dynamics; Wooden blocks; Social robots
Learning to Engage with Interactive Systems,2020,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097585563&doi=10.1145%2f3408876&partnerID=40&md5=d160fb77bf4b722988346f78392dcc54,"Physical agents that can autonomously generate engaging, life-like behavior will lead to more responsive and user-friendly robots and other autonomous systems. Although many advances have been made for one-to-one interactions in well-controlled settings, physical agents should be capable of interacting with humans in natural settings, including group interaction. To generate engaging behaviors, the autonomous system must first be able to estimate its human partners' engagement level. In this article, we propose an approach for estimating engagement during group interaction by simultaneously taking into account active and passive interaction, and use the measure as the reward signal within a reinforcement learning framework to learn engaging interactive behaviors. The proposed approach is implemented in an interactive sculptural system in a museum setting. We compare the learning system to a baseline using pre-scripted interactive behaviors. Analysis based on sensory data and survey data shows that adaptable behaviors within an expert-designed action space can achieve higher engagement and likeability.  © 2020 ACM.",adaptive system; engagement; group interaction; human-robot interaction; interactive system; Living architecture; natural setting interaction; open-world interaction; reinforcement learning; robotic arts; robotic sculpture; social robot; voluntary engagement,Agricultural robots; Autonomous agents; Reinforcement learning; Sensory analysis; Autonomous systems; Engagement levels; Group interaction; Interactive behavior; Interactive system; Passive interactions; Physical agents; User friendly; Learning systems
Friend or Foe Understanding Assembly Workers' Acceptance of Human-robot Collaboration,2020,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095743307&partnerID=40&md5=9c2b277e0088b09d8fd4c59ea7d04ca2,"Due to rising demands on productivity and flexibility, assembly processes are currently experiencing a substantial transformation. Workstations where humans and robots work closely together are becoming increasingly popular, as they provide major advantages compared to manual assembly and full automation. Yet, human-robot collaboration (HRC) can only be successful if the workforce is willing to accept it. How assembly workers perceive HRC still has to be properly investigated. An exploratory investigation using a Grounded Theory approach was conducted to identify factors that are likely to influence workers' acceptance of introducing HRC at work. Seventeen workers with various levels of HRC experience from four different manufacturing companies were interviewed. Findings reveal that some workers perceive HRC as a threat, while others regard it as an opportunity. This perception seems to depend both on their thoughts and feelings about the technology, i.e., collaborative robots, and the organizational change associated with the introduction of this technology. Several factors related to the robot (object-related factors), the individual background of the workers (subject-related factors), and the organizational environment (context-related factors) are found to influence workers' thoughts and feelings. Implications for researchers and manufacturing companies are outlined.  © 2020 ACM.",assembly; grounded theory; human-robot collaboration; human-robot interaction; qualitative; robot acceptance; Technology acceptance,Agricultural robots; Manufacture; Assembly process; Assembly workers; Collaborative robots; Grounded theory approach; Human-robot collaboration; Manual assembly; Manufacturing companies; Organizational change; Social robots
Usability studies of an egocentric vision-based robotic wheelchair,2020,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096356922&doi=10.1145%2f3399434&partnerID=40&md5=9c1a5311104df283996a0d44d17e5957,"Motivated by the need to improve the quality of life for the elderly and disabled individuals who rely on wheelchairs for mobility, and who may have limited or no hand functionality at all, we propose an egocentric computer vision based co-robot wheelchair to enhance their mobility without hand usage. The robot is built using a commercially available powered wheelchair modified to be controlled by head motion. Head motion is measured by tracking an egocentric camera mounted on the user's head and faces outward. Compared with previous approaches to hands-free mobility, our system provides a more natural human robot interface because it enables the user to control the speed and direction of motion in a continuous fashion, as opposed to providing a small number of discrete commands. This article presents three usability studies, which were conducted on 37 subjects. The first two usability studies focus on comparing the proposed control method with existing solutions while the third study was conducted to assess the effectiveness of training subjects to operate the wheelchair over several sessions. A limitation of our studies is that they have been conducted with healthy participants. Our findings, however, pave the way for further studies with subjects with disabilities.  © 2020 Copyright held by the owner/author(s).",Assistive technologies; Computer vision; Computer vision tasks; Computing methodologies; Human-centered computing; Interactive systems and tools; Social and professional topics,Agricultural robots; Computer vision; Motion tracking; Robotics; Social robots; Wheelchairs; Direction of motion; Disabled individuals; Hand-functionality; Human-Robot Interface; Powered wheel chairs; Quality of life; Robotic wheelchairs; Usability studies; Visual servoing
A Primer for Conducting Experiments in Human-Robot Interaction,2020,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097575448&doi=10.1145%2f3412374&partnerID=40&md5=e0bc881c561433f2f288a65c8545289f,"We provide guidelines for planning, executing, analyzing, and reporting hypothesis-driven experiments in Human-Robot Interaction (HRI). The intended audience are researchers in the field of HRI who are not trained in empirical research but who are interested in conducting rigorous human-participant studies to support their research. Following the chronological order of research activities and grounded in updated research practices in psychological and behavioral sciences, this primer covers recommended methods and common pitfalls for defining research questions, identifying constructs and hypotheses, choosing appropriate study designs, operationalizing constructs as variables, planning and executing studies, sampling, choosing statistical tools for data analysis, and reporting results.  © 2020 Owner/Author.",Experimental studies; research methods; statistical analysis,Agricultural robots; Behavioral research; Man machine systems; Robot programming; Sampling; Statistical mechanics; Behavioral science; Chronological order; Empirical research; Human robot Interaction (HRI); Research activities; Research questions; Statistical tools; Study design; Human robot interaction
Where to Next the Impact of COVID-19 on Human-Robot Interaction Research,2020,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097195243&doi=10.1145%2f3405450&partnerID=40&md5=4ab65f200571ff4bd776e18d0a79ddd6,"The COVID-19 pandemic will have a profound and long-lasting impact on the entire scientific endeavor. Scientists already are adapting research programs to adapt to changes in what is prioritized - and what is possible; educators are changing the way that the next generation of researchers are trained, and flagship conferences in many fields are being cancelled, postponed, and fundamentally transformed. These broad-reaching changes are particularly impactful to human-oriented domains such as human-robot interaction (HRI). Because in-person human-subject experiments can take a year or more to conduct, the research we will see published in the field in the immediate future may appear to be ""business as usual,""with accounts of laboratory studies with large numbers of in-person participants. The research currently being performed, however, is of course a different story entirely. Studies that were under way when the current crisis began will be truncated, resulting either in work that cannot be published or in work whose true impact is difficult to accurately assess. Yet HRI research performed in the coming years will be changed in fundamentally different ways; the inability to perform - or expect future performance of - in-person human subjects research, especially research involving tactile or multiparty interaction, will change both the dominant methodological techniques employed by HRI researchers and the very research questions that the field chooses to - and is able to - address. These challenges demand that HRI researchers identify precisely how the field can maintain research quality and impact while the ability to conduct human-subject studies is severely impaired for an undetermined amount of time. A natural inclination may be simply to wait the crisis out in the hope of a speedy return to normalcy; however, in this article, we argue that the community can also take this opportunity to reevaluate and refocus how research in this field is conducted and how students are mentored in ways that will yield benefits for years to come after the current crisis has ended.  © 2020 Owner/Author.",COVID-19 impact; Human-robot interaction; research,Agricultural robots; Man machine systems; Business-as-usual; Future performance; Human robot Interaction (HRI); Human subject experiments; Laboratory studies; Multi-party interactions; Research programs; Research questions; Human robot interaction
Editorial: Representation learning in HRI,2019,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089764117&doi=10.1145%2f3366621&partnerID=40&md5=49c67df2f887df6e2ba790659791c480,[No abstract available],,
Blossom: A Handcrafted Open-Source Robot,2019,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082009784&doi=10.1145%2f3310356&partnerID=40&md5=94d5ff8d491883dd58ca5fa3fa54e5a9,"Blossom is an open-source social robotics platform responding to three challenges in human-robot interaction (HRI) research: (1) Designing, manufacturing, and programming social robots requires a high level of technical knowledge; (2) social robot designs are fixed in appearance and movement capabilities, making them hard to adapt to a specific application; and (3) the use of rigid mechanisms and hard outer shells limits the robots' expressive capabilities. Addressing these challenges, Blossom aims at three design objectives: accessibility, flexibility, and expressiveness. The robot's mechanism can be quickly assembled and partially extended by end-users. Blossom's appearance is open-ended through handcrafted fabric exteriors created and customized by users. Smooth organic movements are achieved with tensile mechanisms, elastic components, and a soft exterior cover attached loosely to the body. Blossom's smartphone-based gesture generation requires neither programming nor character animation experience, allowing lay users to create their own behaviors. All elements in the design were conceived with a low barrier-of-entry in mind. The result is an accessible and customizable social robot for researchers. This article details the implementation of Blossom's design and demonstrates the platform's potential through four field deployment case studies.  © 2019 Owner/Author.",craft; craft robotics; handcrafted; open-source; research platform; Robot design; robot toolkit; social robotics; soft robotics; toolkit,Agricultural robots; Animation; Fixed platforms; Industrial research; Machine design; Open source software; Robot programming; Smartphones; User experience; Character animation; Design objectives; Elastic components; Field deployment; Human robot Interaction (HRI); Rigid mechanisms; Robot designs; Social robotics; Social robots
Empathic Robot for Group Learning: A Field Study,2019,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068536369&doi=10.1145%2f3300188&partnerID=40&md5=38936e838e88debca3eecfa94296497b,"This work explores a group learning scenario with an autonomous empathic robot. We address two research questions: (1) Can an autonomous robot designed with empathic competencies foster collaborative learning in a group context? (2) Can an empathic robot sustain positive educational outcomes in long-term collaborative learning interactions with groups of students? To answer these questions, we developed an autonomous robot with empathic competencies that is able to interact with a group of students in a learning activity about sustainable development. Two studies were conducted. The first study compares learning outcomes in children across three conditions: learning with an empathic robot; learning with a robot without empathic capabilities; and learning without a robot. The results show that the autonomous robot with empathy fosters meaningful discussions about sustainability, which is a learning outcome in sustainability education. The second study features groups of students who interact with the robot in a school classroom for 2 months. The long-term educational interaction did not seem to provide significant learning gains, although there was a change in game-actions to achieve more sustainability during game-play. This result reflects the need to perform more long-term research in the field of educational robots for group learning.  © 2019 ACM.",collaborative learning; education; empathy; group learning; human-robot interaction; learning gains; Social robotics,Agricultural robots; Robots; Students; Sustainable development; Collaborative learning; Field studies; Group learning; Learning Activity; Learning gain; Learning outcome; Research questions; Sustainability education; Educational robots
Communicating Dominance in a Nonanthropomorphic Robot Using Locomotion,2019,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85080345696&doi=10.1145%2f3310357&partnerID=40&md5=fa76938253f6ec3cce5116a2b380ece5,"Dominance is a key aspect of interpersonal relationships. To what extent do nonverbal indicators related to dominance status translate to a nonanthropomorphic robot? An experiment (N = 25) addressed whether a mobile robot's motion style can influence people's perceptions of its status. Using concepts from improv theater literature, we developed two motion styles across three scenarios (robot makes lateral motions, approaches, and departs) to communicate a robot's dominance status through nonverbal expression. In agreement with the literature, participants described a motion style that was fast, in the foreground, and more animated as higher status than a motion style that was slow, in the periphery, and less animated. Participants used fewer negative emotion words to describe the robot with the purportedly high-status movements versus the purportedly low-status movements, but used more negative emotion words to describe the robot when it made departing motions that occurred in the same style. This result provides evidence that guidelines from improvisational theater for using nonverbal expression to perform interpersonal status can be applied to influence perception of a nonanthropomorphic robot's status, thus suggesting that useful models for more complicated behaviors might similarly be derived from performance literature and theory.  © 2019 ACM.",dominance; human-robot interaction; motion path; Nonanthropomorphic robot; status; theater,Agricultural robots; Interpersonal relationship; Lateral motion; Motion styles; Robots
Editorial-The HRI Spring: Shaping Our Future from a Foundation of Diverse Scholarship,2019,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097817580&doi=10.1145%2f3317314&partnerID=40&md5=fcfaa7ba905778dda22aac56aaa30df3,[No abstract available],,
Reflecting on the Presence of Science Fiction Robots in Computing Literature,2019,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096565432&doi=10.1145%2f3303706&partnerID=40&md5=7c97b0e722cfadbe4b13bca9069837ca,"Depictions of robots and AIs in popular science fiction movies and shows have the potential to showcase visions of Human-Robot Interaction (HRI) to the general public and computer science researchers alike. In contrast, studies on the referral, usage, and appropriation of these portrayals by computer scientists in their research publications is an academic void at present. However, such investigations are critical to better understand the potential utility and latent shortcomings of science fiction robots for future HRI research, innovation, and education. To address this research gap, this study investigates the overall presence, nature, and frequency of referrals of 18 popular science fiction robots in the Association for Computing Machinery (ACM) Digital Library. These robots were either portrayed in various movies and subsequently inducted into the Robot Hall of Fame, created by Carnegie Mellon University in 2003, or were top-ranked in a user-curated Internet Movie Database (IMDB) list. To do so, we performed full-text search and retrieval queries of all 18 robots in the ACM Digital Library. In total, we identified 121 relevant mentions, across 102 individual publications, in a time span from 1973 to 2017. These 121 mentions were then qualitatively analysed to determine the nature of the robot mentions. Our results indicate that the robot attributes of voice or dialogue were emerging as a popularly mentioned element. In addition, we find that research papers of philosophical nature mention sci-fi robots more frequently than papers of technical or theoretical nature. We also observe that the dystopian element of science fiction is under-utilised, with the majority of robot mentions exhibiting neutral or utopian characteristics. In conclusion, we speculate on our results and present possible avenues of future HRI research on the topic.  © 2019 ACM.",ACM digital library; AI; computing literature; robots; science fiction; text mining; visions of the future,Agricultural robots; Digital libraries; Human computer interaction; Human robot interaction; Machinery; Motion pictures; Philosophical aspects; Carnegie Mellon University; Computer scientists; Computing machinery; Full-text search; Human robot Interaction (HRI); Internet movie database; Potential utility; Science fictions; Educational robots
Development and Validation of the Self-Efficacy in Human-Robot-Interaction Scale (SE-HRI),2018,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85080434620&doi=10.1145%2f3139352&partnerID=40&md5=752a4ee95b4ed69da50afdde6025542f,"This methodological article discusses the influence of individuals' beliefs about their abilities to use and control robotic technologies on their evaluation of human-robot-interaction (HRI). We conducted three surveys to develop and validate a new measure of Self-Efficacy in HRI. Exploratory factor analysis revealed a two-factorial (factors perceived self-efficacy and loss of control) solution with good reliability (Study 1, n = 201). Confirmatory factor analysis did not confirm the two-factorial structure. Instead, it revealed a better model fit for a one-factorial solution for a German (Study 2, n = 450) and an English version (Study 3, n = 209) of the scale with good indices for convergent and divergent validity. The final questionnaire with 18 items was used in two experimental studies (Study 4, n = 120). We found that interacting with a robot increased self-efficacy and that individual changes in self-efficacy predict more positive evaluations within a student sample, but not a sample of seniors. Interviews with seniors from this study suggested shortening the scale, and revising the instructions and answering scheme. The revised scale was again subject to confirmatory factor analysis (Study 5, n = 198), confirming the one-factorial solution for the German and the English version of the scale. We discuss potential use cases for the scale in HRI research. © 2018 ACM.",experimental study; human-robot-interaction; scale development; scale validation; Self-efficacy,
Understanding the ACM THRI Review Process,2018,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086087560&doi=10.1145%2f3289154&partnerID=40&md5=fe978f648abe1c8b37713df9bab9e13f,[No abstract available],,
The Effect of Personalization in Longer-Term Robot Tutoring,2018,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075242686&doi=10.1145%2f3283453&partnerID=40&md5=1ea045e56e5ff353d66124757a475010,"The benefits of personalized social robots must be evaluated in real-world educational contexts over periods of time longer than a single session to understand their full potential to impact learning outcomes. In this work, we describe a personalization system designed for longer-term personalization that orders curriculum based on an adaptive Hidden Markov Model (HMM) that evaluates students' skill proficiencies. We present a study investigating the effectiveness of this system in a five-session interaction with a robot tutor, taking place over the course of 2 weeks. Our system is evaluated in the context of native Spanish-speaking first-graders interacting with a social robot tutor while completing an English Language Learning educational task. Participants either received lessons: (1) ordered by our adaptive HMM personalization system which selects a lesson based on a skill that the individual participant needs more practice with (""personalized condition"") or (2) ordered randomly from among the lessons the participant had not yet seen (""non-personalized condition""). We found that participants who received personalized lessons from the robot tutor outperformed participants who received non-personalized lessons on a post-test by 2.0 standard deviations on average, corresponding to a mean learning gain in the 98th percentile. © 2018 ACM.",English Language Learning (ELL); Human-robot interaction; personalization; tutoring,
Facilitating Human-Mobile Robot Communication via Haptic Feedback and Gesture Teleoperation,2018,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086121932&doi=10.1145%2f3243503&partnerID=40&md5=0fe0a907e24b39fc924aab71725b4c1d,"In this article, we present a bi-directional communication scheme that facilitates interaction between a person and a mobile robot that follows the person. A person-following robot can assist people in many applications including load carrying, elder care, and emotional support. However, commercially available personal robot systems usually have limited sensing and actuation capabilities. They are not expected to function perfectly in complex environments, and human intervention is required when the robot fails. We propose to use a holdable mechatronic device to reduce the user's effort in communication and enable natural interaction during the intervention. Our design of the holdable device consists of two parts: a haptic interface that displays touch cues to convey the robot's failure status via asymmetric vibrations, and a command interface for teleoperating the robot follower with hand gestures. We experimentally evaluated the device and the communication strategy in two sets of user studies with a controlled environment and a physical robot follower. Results show that with the proposed method, users are able to perform their tasks better, respond to robot failure events faster, and adjust walking speed according to the robot's limitations. We also demonstrate that users can successfully teleoperate the robot to avoid obstacles when navigating in challenging environments. © 2018 ACM.",Human robot interaction; human-following robot; mobile robots,
Planning with Verbal Communication for Human-Robot Collaboration,2018,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057718442&doi=10.1145%2f3203305&partnerID=40&md5=e91a98c760ec56700c4f23d0fb35a4dc,"Human collaborators coordinate effectively their actions through both verbal and non-verbal communication. We believe that the the same should hold for human-robot teams. We propose a formalism that enables a robot to decide optimally between taking a physical action toward task completion and issuing an utterance to the human teammate. We focus on two types of utterances: verbal commands, where the robot asks the human to take a physical action, and state-conveying actions, where the robot informs the human about its internal state, which captures the information that the robot uses in its decision making. Human subject experiments show that enabling the robot to issue verbal commands is the most effective form of communicating objectives, while retaining user trust in the robot. Communicating information about the robot's state should be done judiciously, since many participants questioned the truthfulness of the robot statements when the robot did not provide sufficient explanation about its actions. © 2018 ACM.",Human-robot collaboration; partially observable Markov decision process; planning under uncertainty; verbal communication,
Human-Guided Object Mapping for Task Transfer,2018,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076919473&doi=10.1145%2f3277905&partnerID=40&md5=05d404bdd68d3d4e1fffc857ae0c37a4,"When transferring a learned task to an environment containing new objects, a core problem is identifying the mapping between objects in the old and new environments. This object mapping is dependent on the task being performed and the roles objects play in that task. Prior work assumes (i) the robot has access to multiple new demonstrations of the task or (ii) the primary features for object mapping have been specified. We introduce an approach that is not constrained by either assumption but rather uses structured interaction with a human teacher to infer an object mapping for task transfer. We describe three experiments: an extensive evaluation of assisted object mapping in simulation, an interactive evaluation incorporating demonstration and assistance data from a user study involving 10 participants, and an offline evaluation of the robot's confidence during object mapping. Our results indicate that human-guided object mapping provided a balance between mapping performance and autonomy, resulting in (i) up to 2.25× as many correct object mappings as mapping without human interaction, and (ii) more efficient transfer than requiring the human teacher to re-demonstrate the task in the new environment, correctly inferring the object mapping across 93.3% of the tasks and requiring at most one interactive assist in the typical case. © 2018 ACM.",object mapping; Task transfer,
Interaction Algorithm Effect on Human Experience with Reinforcement Learning,2018,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076955206&doi=10.1145%2f3277904&partnerID=40&md5=f1f1e1fdfb0f44593b50dab5fe9f8585,"A goal of interactive machine learning (IML) is to enable people with no specialized training to intuitively teach intelligent agents how to perform tasks. Toward achieving that goal, we are studying how the design of the interaction method for a Bayesian Q-Learning algorithm impacts aspects of the human's experience of teaching the agent using human-centric metrics such as frustration in addition to traditional ML performance metrics. This study investigated two methods of natural language instruction: critique and action advice. We conducted a human-in-the-loop experiment in which people trained two agents with different teaching methods but, unknown to each participant, the same underlying reinforcement learning algorithm. The results show an agent that learns from action advice creates a better user experience compared to an agent that learns from binary critique in terms of frustration, perceived performance, transparency, immediacy, and perceived intelligence. We identified nine main characteristics of an IML algorithm's design that impact the human's experience with the agent, including using human instructions about the future, compliance with input, empowerment, transparency, immediacy, a deterministic interaction, the complexity of the instructions, accuracy of the speech recognition software, and the robust and flexible nature of the interaction algorithm. © 2018 ACM.",human factors; Human-agent interaction; natural language interface; reinforcement learning; sentiment,
Robot Classification of Human Interruptibility and a Study of Its Effects,2018,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083485621&doi=10.1145%2f3277902&partnerID=40&md5=78603f5f3afd3db39c424810a591c7a3,"As robots become increasingly prevalent in human environments, there will inevitably be times when the robot needs to interrupt a human to initiate an interaction. Our work introduces the first interruptibility-aware mobile-robot system, which uses social and contextual cues online to accurately determine when to interrupt a person. We evaluate multiple non-temporal and temporal models on the interruptibility classification task, and show that a variant of Conditional Random Fields (CRFs), the Latent-Dynamic CRF, is the most robust, accurate, and appropriate model for use on our system. Additionally, we evaluate different classification features and show that the observed demeanor of a person can help in interruptibility classification; but in the presence of detection noise, robust detection of object labels as a visual cue to the interruption context can improve interruptibility estimates. Finally, we deploy our system in a large-scale user study to understand the effects of interruptibility-awareness on human-task performance, robot-task performance, and on human interpretation of the robot's social aptitude. Our results show that while participants are able to maintain task performance, even in the presence of interruptions, interruptibility-awareness improves the robot's task performance and improves participant social perceptions of the robot. © 2018 ACM.",conditional random fields; Interruptibility,
Introduction to the Special Issue on Artificial Intelligence and Human-Robot Interaction,2018,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083522824&doi=10.1145%2f3279995&partnerID=40&md5=68af5c34c8f5b45efe063dd9bc42df4a,[No abstract available],,
Learning and Personalizing Socially Assistive Robot Behaviors to Aid with Activities of Daily Living,2018,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069687816&doi=10.1145%2f3277903&partnerID=40&md5=31ddb68278bfad2fe82f577c5e48221e,"Socially assistive robots can autonomously provide activity assistance to vulnerable populations, including those living with cognitive impairments. To provide effective assistance, these robots should be capable of displaying appropriate behaviors and personalizing them to a user's cognitive abilities. Our research focuses on the development of a novel robot learning architecture that uniquely combines learning from demonstration (LfD) and reinforcement learning (RL) algorithms to effectively teach socially assistive robots personalized behaviors. Caregivers can demonstrate a series of assistive behaviors for an activity to the robot, which it uses to learn general behaviors via LfD. This information is used to obtain initial assistive state-behavior pairings using a decision tree. Then, the robot uses an RL algorithm to obtain a policy for selecting the appropriate behavior personalized to the user's cognition level. Experiments were conducted with the socially assistive robot Casper to investigate the effectiveness of our proposed learning architecture. Results showed that Casper was able to learn personalized behaviors for the new assistive activity of tea-making, and that combining LfD and RL algorithms significantly reduces the time required for a robot to learn a new activity. © 2018 ACM.",Human-robot interaction; robot behavior learning; socially assistive robots,
Adapting a General-Purpose Social Robot for Paediatric Rehabilitation through in Situ Design,2018,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058103543&doi=10.1145%2f3203304&partnerID=40&md5=bb8d782f9929ce70b48bd80d65bafbc7,"Socially assistive robots (SARs) offer great promise for improving outcomes in paediatric rehabilitation. However, the design of software and interactive capabilities for SARs must be carefully considered in the context of their intended clinical use. While previous work has explored specific roles and functionalities to support paediatric rehabilitation, few have considered the design of such capabilities in the context of ongoing clinical deployment. In this article, we present a two-phase in situ design process for SARs in health care, emphasising stakeholder engagement and on-site development. We explore this in the context of developing the humanoid social robot NAO as a socially assistive rehabilitation aid for children with cerebral palsy. We present and evaluate our design process, outcomes achieved, and preliminary results from ongoing clinical testing with 9 patients and 5 therapists over 14 sessions. We argue that our in situ design methodology has been central to the rapid and successful deployment of our system. © 2018 Owner/Author.",health care; In situ design; rehabilitation; socially assistive robots,
Some Brief Thoughts on the Past and Future of Human-Robot Interaction,2018,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083079065&doi=10.1145%2f3209769&partnerID=40&md5=7503b3a377740ee8facc13f203147323,[No abstract available],Human-robot interaction,
Humane Robots-from Robots with a Humanoid Body to Robots with an Anthropomorphic Mind,2018,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056588768&doi=10.1145%2f3208954&partnerID=40&md5=93f0704f1082150db58edd188b4d613f,[No abstract available],anthropomorphism; cognitive robotics; Human-robot interaction; natural collaboration,
The Science of Human-Robot Interaction,2018,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065762501&doi=10.1145%2f3209701&partnerID=40&md5=5858b6b446833135ca6939e4c978351e,[No abstract available],human-centered computing; Human-robot interaction,
"Closed-Loop Global Motion Planning for Reactive, Collision-Free Execution of Learned Tasks",2018,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083696254&doi=10.1145%2f3209045&partnerID=40&md5=61d2ecf6067c8a36bbc45e69ba9096c6,"We present a robot motion planning approach for performing a learned task while reacting to the movement of obstacles and task-relevant objects. We employ a closed-loop, sampling-based motion planner operating multiple times a second that senses obstacles and task-relevant objects and generates collision-free motion plans based on a learned-task model. The task model is learned from expert demonstrations prior to task execution and is represented as a hidden Markov model. During task execution, our motion planner quickly searches in the Cartesian product of the task model and a probabilistic roadmap for a collision-free plan with features most similar to the demonstrations given the current locations of the task-relevant objects. We accelerate replanning using a fast bidirectional search and by biasing the sampling distribution using information from the learned-task model. We show the efficacy of our approach with the Baxter robot performing two tasks. © 2018 Owner/Author.",assistive robotics; asymptotically optimal motion planning; Interactive motion planning,
"A Brave, Creative, and Happy HRI",2018,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083710981&doi=10.1145%2f3209540&partnerID=40&md5=a083ce238112e800c0d7b2a39c3617f8,[No abstract available],human - robot interaction; Robots,
Hacking the Human Bias in Robotics,2018,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083717537&doi=10.1145%2f3208974&partnerID=40&md5=db00e3ca620671e14b2d91e011309fdc,[No abstract available],Artificial intelligence; bias; professional responsibility; robot ethics,
ACM Transactions on Human-Robot Interaction,2018,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083697958&doi=10.1145%2f3209977&partnerID=40&md5=951f64d84c1046cd45c67d86f10ef2f8,[No abstract available],,
Haptic Dimensions of Human-Robot Interaction,2018,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057765493&doi=10.1145%2f3209768&partnerID=40&md5=2488cb211679633db84666cde9a20844,[No abstract available],Wearable haptics,
Robots in the Wild,2018,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058083033&doi=10.1145%2f3208975&partnerID=40&md5=d8fdd1edcb9d646b68720f41d1af3e8b,[No abstract available],field research; Human-robot interaction; robots in groups and teams,
On Relevance,2018,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083235553&doi=10.1145%2f3209770&partnerID=40&md5=04ab00abbc3d65edd2ff33afdb14deec,[No abstract available],artificial intelligence; data sets; Human-robot interaction; inclusiveness; machine learning; real-world problems; x-index,
Reframing Assistive Robots to Promote Successful Aging,2018,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063988621&doi=10.1145%2f3203303&partnerID=40&md5=1c06003e9548141ae72918299465f3bc,"We are living in an exciting time, as people are living longer, more active lives. This is reshaping how we think about aging. Rather than viewing aging as a problem to be fixed (i.e., a deficit model of aging), many aging researchers are viewing aging as a developmental stage of life to be celebrated and supported, that is, ""successful aging."" In this article, we embrace this approach and consider it in the context of assistive robot design in an aim to steer the conversation away from deficit models that have limited robot design possibilities. To explore an alternative design approach to the study of aging in human-robot interaction (HRI), we invited five aging researchers (three geriatricians, one gerontologist, and one epidemiologist) and nine older adults to participate in our research. In the study, participants illustrated their interpretations of aging and suggested potential assistive robots. We found that while all participants perceived the importance of potential disabilities due to aging, they considered potential disabilities as only one aspect of the experience of aging. They highlighted other key themes to consider in designing robots to support successful aging, such as older adults' autonomy and resilience. We discuss these findings for the HRI community and call for ""robots for successful aging."" © 2018 ACM.",assistive robots; health care robotics; participatory design; Successful aging,
The Increasingly Fascinating Opportunity for Human-Robot-AI Interaction,2018,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081156971&doi=10.1145%2f3209541&partnerID=40&md5=a5ba5e031bb4face4ef08b2baf423ca6,[No abstract available],Autonomous mobile service robots; indoor localization and navigation; symbiotic autonomy; verbalization,
Performance-Aware Trust Modeling within a Human-Multi-Robot Collaboration Setting,2024,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85197350639&doi=10.1145%2f3660648&partnerID=40&md5=44006ec657a8f667b9dab7903b5d7247,"In this study, a novel time-driven mathematical model for trust is developed considering human-multi-robot performance for a Human-Robot Collaboration (HRC) framework. For this purpose, a model is developed to quantify human performance considering the effects of physical and cognitive constraints and factors such as muscle fatigue and recovery, muscle isometric force, human (cognitive and physical) workload, workloads due to the robots' mistakes, and task complexity. The performance of multi-robot in the HRC setting is modeled based upon the rate of task assignment and completion as well as the mistake probabilities of the individual robots. The human trust in HRC setting with single and multiple robots is modeled over different operation regions, namely unpredictable region, predictable region, dependable region, and faithful region. The relative performance difference between the human operator and the robot is used to analyze the effect on the human operator's trust in robots' operation. The developed model is simulated for a manufacturing workspace scenario considering different task complexities and involving multiple robots to complete shared tasks. The simulation results indicate that for a constant multi-robot performance in operation, the human operator's trust in robots' operation improves whenever the comparative performance of the robots improves with respect to the human operator performance. The impact of robot hypothetical learning capabilities on human trust in the same HRC setting is also analyzed. The results confirm that a hypothetical learning capability allows robots to reduce human workloads, which improves human performance. The simulation result analysis confirms that the human operator's trust in the multi-robot operation increases faster with the improvement of the multi-robot performance when the robots have a hypothetical learning capability. An empirical study was conducted involving a human operator and two collaborator robots with two different performance levels in a software-based HRC setting. The experimental results closely followed the pattern of the developed mathematical models when capturing human trust and performance in terms of human-multi-robot collaboration. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",human performance; Human-Robot Collaboration (HRC); multi-robot performance; trust,Industrial robots; Muscle; Human operator; Human performance; Human-robot collaboration; Multi-robot performance; Multirobots; Performance; Robot operations; Robot performance; Trust; Multipurpose robots
Designing Socially Assistive Robots Exploring Israeli and German Designers' Perceptions,2024,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85197356837&doi=10.1145%2f3657646&partnerID=40&md5=347e23e474680b3b56da04e5aa0a57b5,"Socially assistive robots (SARs) are becoming more prevalent in everyday life, emphasizing the need to make them socially acceptable and aligned with users' expectations. Robots' appearance impacts users' behaviors and attitudes toward them. Therefore, product designers choose visual qualities to give the robot a character and to imply its functionality and personality. In this work, we sought to investigate the effect of cultural differences on Israeli and German designers' perceptions of SARs' roles and appearance in four different contexts: A service robot for an assisted living/retirement residence facility, a medical assistant robot for a hospital environment, a COVID-19 officer robot, and a personal assistant robot for domestic use. The key insight is that although Israeli and German designers share similar perceptions of visual qualities for most of the robotics roles, we found differences in the perception of the COVID-19 officer robot's role and, by that, its most suitable visual design. This work indicates that context and culture play a role in users' perceptions and expectations; therefore, they should be taken into account when designing new SARs for diverse contexts.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Context-driven design; professional designers; socially assistive robot; visual qualities,COVID-19; Robots; Assistant robot; Context-driven design; Product designers; Professional designers; Robot appearance; Socially assistive robots; User attitudes; User behaviors; User expectations; Visual qualities; Machine design
Charting User Experience in Physical Human-Robot Interaction,2024,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85197348148&doi=10.1145%2f3659058&partnerID=40&md5=69653a8cfe2dba918b2f109e940b62a4,"Robots increasingly interact with humans through touch, where people are touching or being touched by robots. Yet, little is known about how such interactions shape a user's experience. To inform future work in this area, we conduct a systematic review of 44 studies on physical human-robot interaction (pHRI). Our review examines the parameters of the touch (e.g., the role of touch, location), the experimental variations used by researchers, and the methods used to assess user experience. We identify five facets of user experience metrics from the questionnaire items and data recordings for pHRI studies. We highlight gaps and methodological issues in studying pHRI and compare user evaluation trends with the Human-Computer Interaction (HCI) literature. Based on the review, we propose a conceptual model of the pHRI experience. The model highlights the components of such touch experiences to guide the design and evaluation of physical interactions with robots and inform future user experience questionnaire development. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",haptics; Physical human-robot interaction; systematic review; tactile human-robot interaction; user experience,Human computer interaction; Machine design; Man machine systems; User interfaces; Conceptual model; Haptics; Humans-robot interactions; Interaction experiences; Interaction studies; Physical humanrobot interaction (phri); Systematic Review; Tactile human-robot interaction; User evaluations; Users' experiences; Human robot interaction
Multi-Dimensional Evaluation of an Augmented Reality Head-Mounted Display User Interface for Controlling Legged Manipulators,2024,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85197359632&doi=10.1145%2f3660649&partnerID=40&md5=388af63e96aeb6f094eb4a0bbd346768,"Controlling assistive robots can be challenging for some users, especially those lacking relevant experience. Augmented Reality (AR) User Interfaces (UIs) have the potential to facilitate this task. Although extensive research regarding legged manipulators exists, comparatively little is on their UIs. Most existing UIs leverage traditional control interfaces such as joysticks, Hand-Held (HH) controllers and 2D UIs. These interfaces not only risk being unintuitive, thus discouraging interaction with the robot partner, but also draw the operator's focus away from the task and towards the UI. This shift in attention raises additional safety concerns, particularly in potentially hazardous environments where legged manipulators are frequently deployed. Moreover, traditional interfaces limit the operators' availability to use their hands for other tasks. Towards overcoming these limitations, in this article, we provide a user study comparing an AR Head-Mounted Display (HMD) UI we developed for controlling a legged manipulator against off-the-shelf control methods for such robots. This user study involved 27 participants and 135 trials, from which we gathered over 405 completed questionnaires. These trials involved multiple navigation and manipulation tasks with varying difficulty levels using a Boston Dynamics's Spot, a 7 df Kinova robot arm and a Robotiq 2F-85 gripper that we integrated into a legged manipulator. We made the comparison between UIs across multiple dimensions relevant to a successful human-robot interaction. These dimensions include cognitive workload, technology acceptance, fluency, system usability, immersion and trust. Our study employed a factorial experimental design with participants undergoing five different conditions, generating longitudinal data. Due to potential unknown distributions and outliers in such data, using parametric methods for its analysis is questionable, and while non-parametric alternatives exist, they may lead to reduced statistical power. Therefore, to analyse the data that resulted from our experiment, we chose Bayesian data analysis as an effective alternative to address these limitations. Our results show that AR UIs can outpace HH-based control methods and reduce the cognitive requirements when designers include hands-free interactions and cognitive offloading principles into the UI. Furthermore, the use of the AR UI together with our cognitive offloading feature resulted in higher usability scores and significantly higher fluency and Technology Acceptance Model scores. Regarding immersion, our results revealed that the response values for the AR Immersion questionnaire associated with the AR UI are significantly higher than those associated with the HH UI, regardless of the main interaction method with the former, i.e., hand gestures or cognitive offloading. Derived from the participants' qualitative answers, we believe this is due to a combination of factors, of which the most important is the free use of the hands when using the HMD, as well as the ability to see the real environment without the need to divert their attention to the UI. Regarding trust, our findings did not display discernible differences in reported trust scores across UI options. However, during the manipulation phase of our user study, where participants were given the choice to select their preferred UI, they consistently reported higher levels of trust compared to the navigation category. Moreover, there was a drastic change in the percentage of participants that selected the AR UI for completing this manipulation stage after incorporating the cognitive offloading feature. Thus, trust seems to have mediated the use and non-use of the UIs in a dimension different from the ones considered in our study, i.e., delegation and reliance. Therefore, our AR HMD UI for the control of legged manipulators was found to improve human-robot interaction across several relevant dimensions, underscoring the critical role of UI design in the effective and trustworthy utilisation of robotic systems. © 2024 Copyright held by the owner/author(s).",augmented reality; Bayesian data analysis; cognitive offloading; cognitive workload; fluency; head-mounted display; Human-robot interaction; immersion; legged manipulators; system usability; technology acceptance; trust; user interfaces,Data handling; Helmet mounted displays; Human robot interaction; Information analysis; Man machine systems; Manipulators; User interfaces; Virtual reality; Bayesian data analysis; Cognitive offloading; Cognitive workloads; Fluency; Head-mounted-displays; Humans-robot interactions; Immersion; Legged manipulator; System usability; Technology acceptance; Trust; Augmented reality
Batch Active Learning of Reward Functions from Human Preferences,2024,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85196961933&doi=10.1145%2f3649885&partnerID=40&md5=78a9618767351f1f25f9088d3a86f3d6,"Data generation and labeling are often expensive in robot learning. Preference-based learning is a concept that enables reliable labeling by querying users with preference questions. Active querying methods are commonly employed in preference-based learning to generate more informative data at the expense of parallelization and computation time. In this article, we develop a set of novel algorithms, batch active preference-based learning methods, that enable efficient learning of reward functions using as few data samples as possible while still having short query generation times and also retaining parallelizability. We introduce a method based on determinantal point processes for active batch generation and several heuristic-based alternatives. Finally, we present our experimental results for a variety of robotics tasks in simulation. Our results suggest that our batch active learning algorithm requires only a few queries that are computed in a short amount of time. We showcase one of our algorithms in a study to learn human users’ preferences. © 2024 Copyright held by the owner/author(s).",,Learning algorithms; Learning systems; Active Learning; Computation time; Data generation; Data labelling; Labelings; Learning methods; Novel algorithm; Parallelizations; Preference-based; Reward function; Heuristic methods
RoSI: A Model for Predicting Robot Social Influence,2024,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85197360648&doi=10.1145%2f3641515&partnerID=40&md5=ba57b98fa785b4a28851e655843eb295,"A wide range of studies in Human-Robot Interaction (HRI) has shown that robots can influence the social behavior of humans. This phenomenon is commonly explained by the Media Equation. Fundamental to this theory is the idea that when faced with technology (like robots), people perceive it as a social agent with thoughts and intentions similar to those of humans. This perception guides the interaction with the technology and its predicted impact. However, HRI studies have also reported examples in which the Media Equation has been violated, that is when people treat the influence of robots differently from the influence of humans. To address this gap, we propose a model of Robot Social Influence (RoSI) with two contributing factors. The first factor is a robot's violation of a person's expectations, whether the robot exceeds expectations or fails to meet expectations. The second factor is a person's social belonging with the robot, whether the person belongs to the same group as the robot or a different group. These factors are primary predictors of robots' social influence and commonly mediate the influence of other factors. We review HRI literature and show how RoSI can explain robots' social influence in concrete HRI scenarios. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",belonging; expectation; Human-robot interaction; social influence,Economic and social effects; Man machine systems; Social behavior; Belonging; Contributing factor; Expectation; Humans-robot interactions; Interaction studies; Media equation; Social agents; Social behaviour; Social influence; Human robot interaction
Towards the Legibility of Multi-robot Systems,2024,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85197348643&doi=10.1145%2f3647984&partnerID=40&md5=7d61b323d464da2a2356b6805e33a954,"Communication is crucial for human-robot collaborative tasks. In this context, legibility studies movement as the means of implicit communication between robotic systems and a human observer. This concept has been explored mostly for manipulators and humanoid robots. In contrast, little information is available in the literature about legibility of multi-robot systems or swarms, where simplicity and non-anthropomorphism of robots, along with the complexity of their interactions and aggregated behavior impose different challenges that are not encountered in single-robot scenarios. This article investigates legibility of multi-robot systems. Hence, we extend the definition of legibility, incorporating information about high-level goals in terms of the coordination objective of the group of robots, to previous results that focused solely on the legibility of spatial goals. A set of standard multi-robot algorithms corresponding to different coordination objectives are implemented and their legibility is evaluated in a user study, where participants observe the behavior of the multi-robot system in a virtual reality setup and are asked to identify the system's spatial goal and coordination objective. The results of the study confirmed that coordination objectives are discernible by the users, hence multi-robot systems can be controlled to be legible, in terms of spatial goal and coordination objective. © 2024 Copyright held by the owner/author(s).",human-aware multi-robot systems; human-centered robotics; Multi-robot systems; swarms,Anthropomorphic robots; Human robot interaction; Industrial robots; Manipulators; Robot learning; Virtual reality; Collaborative tasks; Human centered robotics; Human robots; Human-aware; Human-aware multi-robot system; Implicit communications; Multi-robot systems; Robotic systems; Swarm; Multipurpose robots
Impact of Haptic Feedback in High Latency Teleoperation for Space Applications,2024,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85197357621&doi=10.1145%2f3651993&partnerID=40&md5=03f408472e1b2005204e748fb936851a,"Remote manipulation is a key enabler for upcoming space activities such as in-orbit servicing and manufacture (IOSM). However, due to the large distances involved, these systems encounter unavoidable signal delays, which can lead to poor performance and users adopting a disjointed, ""move-and-wait""style of operation. We use a robot arm teleoperated with a haptic controller to test the impact of haptic feedback on delayed (up to 2.6 s: Earth-Moon communications) teleoperation performance for two example IOSM-style tasks.This user study showed that increased latency reduced performance in all metrics recorded. In real-time teleoperation, haptic feedback showed improvements in success rate, accuracy, contact force, velocity, and trust, but, of these, only the improvements to contact forces and moving velocity were also seen at higher latencies. Accuracy and trust improvements were lost, or even reversed, at higher latencies. Results varied between the two tasks, highlighting the need for further research into the range of task types to be encountered in teleoperated space activities. This study also provides a framework by which to explore how features other than haptic feedback can impact both performance and trust in delayed teleoperation. © 2024 Copyright held by the owner/author(s).",haptic feedback; latency; Teleoperation; trust; user study,Feedback; Moon; Orbits; Space applications; Contact forces; Force/velocity; Haptic feedbacks; In-orbit servicing; Latency; Performance; Space activities; Teleoperated; Trust; User study; Remote control
Do Humans Trust Robots that Violate Moral Trust?,2024,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85197342545&doi=10.1145%2f3651992&partnerID=40&md5=89af309c018ae21f4a92a4271ce4fbae,"The increasing use of robots in social applications requires further research on human-robot trust. The research on human-robot trust needs to go beyond the conventional definition that mainly focuses on how human-robot relations are influenced by robot performance. The emerging field of social robotics considers optimizing a robot's personality a critical factor in user perceptions of experienced human-robot interaction (HRI). Researchers have developed trust scales that account for different dimensions of trust in HRI. These trust scales consider one performance aspect (i.e., the trust in an agent's competence to perform a given task and their proficiency in executing the task accurately) and one moral aspect (i.e., trust in an agent's honesty in fulfilling their stated commitments or promises) for human-robot trust. The question that arises here is to what extent do these trust aspects affect human trust in a robot? The main goal of this study is to investigate whether a robot's undesirable behavior due to the performance trust violation would affect human trust differently than another similar undesirable behavior due to a moral trust violation. We designed and implemented an online human-robot collaborative search task that allows distinguishing between performance and moral trust violations by a robot. We ran these experiments on Prolific and recruited 100 participants for this study. Our results showed that a moral trust violation by a robot affects human trust more severely than a performance trust violation with the same magnitude and consequences. © 2024 Copyright held by the owner/author(s).",human- robot interaction; human-robot trust; Moral trust; moral trust violation; multidimensional trust; performance trust; performance trust violation; trust violation in HRI,Economic and social effects; Man machine systems; Human robots; Human-robot trust; Humans-robot interactions; Moral trust; Moral trust violation; Multi-dimensional trust; Performance; Performance trust; Performance trust violation; Trust violation in human-robot interaction; Human robot interaction
Robots as Mental Well-being Coaches: Design and Ethical Recommendations,2024,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85197351372&doi=10.1145%2f3643457&partnerID=40&md5=b6f36f00866fc80a267227d7dfd03a37,"The last decade has shown a growing interest in robots as well-being coaches. However, insightful guidelines for the design of robots as coaches to promote mental well-being have not yet been proposed. This article details design and ethical recommendations based on a qualitative analysis drawing on a grounded theory approach, which was conducted with a three-step iterative design process which included user-centered design studies involving robotic well-being coaches, namely: (1) a user-centred design study conducted with 11 participants consisting of both prospective users who had participated in a Brief Solution-Focused Practice study with a human coach, as well as coaches of different disciplines, (2) semi-structured individual interview data gathered from 20 participants attending a Positive Psychology intervention study with the robotic well-being coach Pepper, and (3) a user-centred design study conducted with 3 participants of the Positive Psychology study as well as 2 relevant well-being coaches. After conducting a thematic analysis and a qualitative analysis, we collated the data gathered into convergent and divergent themes, and we distilled from those results a set of design guidelines and ethical considerations. Our findings can inform researchers and roboticists on the key aspects to take into account when designing robotic mental well-being coaches. © 2024 Copyright held by the owner/author(s).",design guidelines; ethical considerations; Human-robot interaction; robot design; social robots,Ethical technology; Human robot interaction; Machine design; Man machine systems; Design guideline; Design studies; Detail design; Ethical considerations; Humans-robot interactions; Positive psychology; Qualitative analysis; Robot designs; Social robots; Well being; User centered design
Children's Acceptance of a Domestic Social Robot: How It Evolves over Time,2024,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85197287653&doi=10.1145%2f3638066&partnerID=40&md5=d3993c85a2f560030cedab3249fbd56b,"Little is known about children's long-Term acceptance of social robots; whether different types of users exist; and what reasons children have not to use a robot. Moreover, the literature is inconclusive about how the measurement of children's robot acceptance (i.e., self-report or observational) affects the findings. We relied on both self-report and observational data from a six-wave panel study among 321 children aged eight to nine, who were given a Cozmo robot to play with at home over the course of 8 weeks. Children's robot acceptance decreased over time, with the strongest drop after 2-4 weeks. Children rarely rejected the robot (i.e., they did not stop using it already prior to actual adoption). They rather discontinued its use after initial adoption or alternated between using and not using the robot. The competition of other toys and lacking motivation to play with Cozmo emerged as the strongest reasons for not using the robot. Self-report measures captured patterns of robot acceptance well but seemed suboptimal for precise assessments of robot use.  © 2024 Copyright held by the owner/author(s).",child-robot interaction; Human-machine interaction; social robotics; technology acceptance; technology use,Child-robot interactions; Human machine interaction; Measurements of; Observational data; Panel studies; Robot acceptances; Social robotics; Social robots; Technology acceptance; Technology use; Human robot interaction
SARI: Shared Autonomy across Repeated Interaction,2024,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189044764&doi=10.1145%2f3651994&partnerID=40&md5=ac412bac43429ef998269026ca3a1695,"Assistive robot arms try to help their users perform everyday tasks. One way robots can provide this assistance is shared autonomy. Within shared autonomy, both the human and robot maintain control over the robot's motion: As the robot becomes confident it understands what the human wants, it intervenes to automate the task. But how does the robot know these tasks in the first place? State-of-The-Art approaches to shared autonomy often rely on prior knowledge. For instance, the robot may need to know the human's potential goals beforehand. During long-Term interaction these methods will inevitably break down-sooner or later the human will attempt to perform a task that the robot does not expect. Accordingly, in this article we formulate an alternate approach to shared autonomy that learns assistance from scratch. Our insight is that operators repeat important tasks on a daily basis (e.g., opening the fridge, making coffee). Instead of relying on prior knowledge, we therefore take advantage of these repeated interactions to learn assistive policies. We introduce SARI, an algorithm that recognizes the human's task, replicates similar demonstrations, and returns control when unsure. We then combine learning with control to demonstrate that the error of our approach is uniformly ultimately bounded. We perform simulations to support this error bound, compare our approach to imitation learning baselines, and explore its capacity to assist for an increasing number of tasks. Finally, we conduct three user studies with industry-standard methods and shared autonomy baselines, including a pilot test with a disabled user. Our results indicate that learning shared autonomy across repeated interactions matches existing approaches for known tasks and outperforms baselines on new tasks. See videos of our user studies here: https://youtu.be/3vE4omSvLvc.  © 2024 Copyright held by the owner/author(s).",Human-robot interaction; imitation learning; shared autonomy,Control theory; Assistive robots; Humans-robot interactions; Imitation learning; Learn+; Prior-knowledge; Robot arms; Robot motion; Shared autonomy; State-of-the-art approach; User study; Human robot interaction
A Survey on Dialogue Management in Human-robot Interaction,2024,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195244828&doi=10.1145%2f3648605&partnerID=40&md5=43c8aac71335d6b2eaeece7674649400,"As social robots see increasing deployment within the general public, improving the interaction with those robots is essential. Spoken language offers an intuitive interface for the human-robot interaction (HRI), with dialogue management (DM) being a key component in those interactive systems. Yet, to overcome current challenges and manage smooth, informative, and engaging interaction, a more structural approach to combining HRI and DM is needed. In this systematic review, we analyze the current use of DM in HRI and focus on the type of dialogue manager used, its capabilities, evaluation methods, and the challenges specific to DM in HRI. We identify the challenges and current scientific frontier related to the DM approach, interaction domain, robot appearance, physical situatedness, and multimodality.  © 2024 Copyright held by the owner/author(s).",dialogue management; social robots; Spoken interaction,Man machine systems; 'current; Dialogue management; General publics; Humans-robot interactions; Interactive system; Intuitive interfaces; Social robots; Spoken interaction; Spoken languages; Structural approach; Human robot interaction
Augmenting Human Teams with Robots in Knowledge Work Settings: Insights from the Literature,2024,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191273865&doi=10.1145%2f3649884&partnerID=40&md5=b7be045772af9fb4a3e30ff62eeba439,"Recent developments in large language models open doors for Artificial Intelligence and robots to augment knowledge workers and teams in a variety of domains, such as customer service, data science, legal work, and software development. In this article, we review 317 articles from multiple disciplines and summarize the insights in a theoretical framework linking key robot attributes to human perceptions and behaviors. The robot attributes include embodiment, nonverbal and verbal communication, perceived gender and race, emotions, perceived personality, and competence. The outcomes include human perceptions, acceptance, engagement, compliance, trust, and willingness to help. We identify four differences between one human and one robot settings and team settings and use them as the springboard to generalize insights from the literature review to the design and impact of a robot in assisting humans in knowledge work teams. We report two high-level observations around the interplay among robot attributes and context dependent designs and discuss their implications. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Generative AI; human robot team; Human-robot interaction; robot design,Behavioral research; Intelligent robots; Machine design; Software design; Customer-service; Generative AI; Human perception; Human-robot-team; Humans-robot interactions; Knowledge work; Knowledge workers; Language model; Open doors; Robot designs; Human robot interaction
PRogramAR: Augmented Reality End-User Robot Programming,2024,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189108787&doi=10.1145%2f3640008&partnerID=40&md5=6bb0da03bed14d2bb8627473f48e128c,"The field of end-user robot programming seeks to develop methods that empower non-expert programmers to task and modify robot operations. In doing so, researchers may enhance robot flexibility and broaden the scope of robot deployments into the real world. We introduce PRogramAR (Programming Robots using Augmented Reality), a novel end-user robot programming system that combines the intuitive visual feedback of augmented reality (AR) with the simplistic and responsive paradigm of trigger-Action programming (TAP) to facilitate human-robot collaboration. Through PRogramAR, users are able to rapidly author task rules and desired reactive robot behaviors, while specifying task constraints and observing program feedback contextualized directly in the real world. PRogramAR provides feedback by simulating the robot's intended behavior and providing instant evaluation of TAP rule executability to help end users better understand and debug their programs during development. In a system validation, 17 end users ranging from ages 18 to 83 used PRogramAR to program a robot to assist them in completing three collaborative tasks. Our results demonstrate how merging the benefits of AR and TAP using elements from prior robot programming research into a single novel system can successfully enhance the robot programming process for non-expert users.  © 2024 Copyright held by the owner/author(s).",Additional Key Words and PhrasesEnd-user robot programming; Augmented Reality (AR); Human-Robot Collaboration (HRC); Human-Robot Interaction (HRI); Trigger-Action Programming (TAP),Augmented reality; Human robot interaction; Man machine systems; Program debugging; Visual communication; Visual servoing; Additional key word and phrasesend-user robot programming; Augmented reality; End-users; Human-robot collaboration; Human-robot interaction; Humans-robot interactions; Key words; Programming robots; Trigger-action programming; Robot programming
The Perception of Agency,2024,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189088345&doi=10.1145%2f3640011&partnerID=40&md5=b69c284dffca3675df57b4c2a958322f,"The perception of agency in human robot interaction has become increasingly important as robots become more capable and more social. There are, however, no accepted or consistent methods of measuring perceived agency; researchers currently use a wide range of techniques and surveys. We provide a definition of perceived agency, and from that definition we create and psychometrically validate a scale to measure perceived agency. We then perform a scale evaluation by comparing the PA scale constructed in experiment 1 to two other existing scales. We find that our PA and PA-R (Perceived Agency-Rasch) scales provide a better fit to empirical data than existing measures. We also perform scale validation by showing that our scale shows the hypothesized relationship between perceived agency and morality.  © 2024 Copyright held by the owner/author(s).",Additional Key Words and PhrasesPerceived agency; agency; human robot interaction,Man machine systems; Additional key word and phrasesperceived agency; Agency; Empirical data; Humans-robot interactions; Key words; Scale validation; Human robot interaction
Human Understanding and Perception of Unanticipated Robot Action in the Context of Physical Interaction,2024,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189097661&doi=10.1145%2f3643458&partnerID=40&md5=2eb8a09757d46db87669fd88777f2d18,"Anticipating a future scenario where the robot initiates its own actions and behaves voluntarily when collaborating with humans, our research focuses on human understanding and perception of unanticipated robot actions during physical human-robot interaction. While the current literature searches for key factors that make the human-robot collaboration successful, the question of how people experience the robot’s unanticipated action as cooperative or uncooperative seems to remain open. We designed a game-based experiment (N = 35) where the participant played a “catch-falling-coins” game by moving a robotic arm. Our experiment introduced unanticipated robot actions in an “active session” where the robot targeted higher-valued coins without first informing the participants. Through semi-structured interviews and statistical analysis of questionnaires (Big Five Personality Test, SAM, NARS and CH33), we examined the participants’ understanding of the robot’s “intention” and their positive or negative perception of the robot as cooperative or uncooperative. Among the participants who understood that the robot’s “intention” was to catch the higher-valued coins, the majority of them reported a positive perception of the robot (cooperative or helpful) while this was not the case among those who did not understand the robot’s intention. We also observed relevant relationships between some personality traits and a person’s understanding of the robot’s intention. Qualitative analysis of the interviews allowed us to structure the process of perception change during the game into three phases: confusion, investigation, and adaptation. We believe that our research contributes to the study of human perception, and particularly to the relationship between a human’s understanding of unanticipated robot actions and their positive or negative perception of the robot. © 2024 Association for Computing Machinery. All rights reserved.",,Man machine systems; 'current; Human perception; Human understanding; Human-robot collaboration; Key factors; Literature search; Physical humanrobot interaction (phri); Physical interactions; Research focus; Robot actions; Human robot interaction
Interaction-Shaping Robotics: Robots That Influence Interactions between Other Agents,2024,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189071275&doi=10.1145%2f3643803&partnerID=40&md5=3bf0c52cd919e4cac3887c9d581028fc,"Work in Human–Robot Interaction (HRI) has investigated interactions between one human and one robot as well as human–robot group interactions. Yet the field lacks a clear definition and understanding of the influence a robot can exert on interactions between other group members (e.g., human-to-human). In this article, we define Interaction-Shaping Robotics (ISR), a subfield of HRI that investigates robots that influence the behaviors and attitudes exchanged between two (or more) other agents. We highlight key factors of interaction-shaping robots that include the role of the robot, the robot-shaping outcome, the form of robot influence, the type of robot communication, and the timeline of the robot’s influence. We also describe three distinct structures of human–robot groups to highlight the potential of ISR in different group compositions and discuss targets for a robot’s interaction-shaping behavior. Finally, we propose areas of opportunity and challenges for future research in ISR. © 2024 Association for Computing Machinery. All rights reserved.",Human–robot interaction; interaction-shaping robotics; multiparty interactions; shaping interactions; social influence,Economic and social effects; Group interaction; Group members; Human robots; Humans-robot interactions; Interaction-shaping robotic; Multiparty interaction; Robot group; Shaping interaction; Social influence; Subfields; Human robot interaction
Perception and Action Augmentation for Teleoperation Assistance in Freeform Telemanipulation,2024,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189078348&doi=10.1145%2f3643804&partnerID=40&md5=ac8b56d35c2679b83ebf2f2462f14503,"Teleoperation enables controlling complex robot systems remotely, providing the ability to impart human expertise from a distance. However, these interfaces can be complicated to use as it is difficult to contextualize information about robot motion in the workspace from the limited camera feedback. Thus, it is required to study the best manner in which assistance can be provided to the operator that reduces interface complexity and effort required for teleoperation. Some techniques that provide assistance to the operator while freeform teleoperating include: (1) perception augmentation, like augmented reality visual cues and additional camera angles, increasing the information available to the operator; (2) action augmentation, like assistive autonomy and control augmentation, optimized to reduce the effort required by the operator while teleoperating. In this article, we investigate: (1) which aspects of dexterous telemanipulation require assistance; (2) the impact of perception and action augmentation in improving teleoperation performance; and (3) what factors impact the usage of assistance and how to tailor these interfaces based on the operators’ needs and characteristics. The findings from this user study and resulting post-study surveys will help identify task-based and user-preferred perception and augmentation features for teleoperation assistance. © 2024 Association for Computing Machinery. All rights reserved.",AR visual cues; Freeform telemanipulation; shared autonomous control,Augmented reality; Cameras; Manipulators; AR visual cue; Autonomous control; Freeform telemanipulation; Freeforms; Human expertise; Perception and actions; Robots system; Shared autonomous control; Telemanipulation; Visual cues; Remote control
Affinity Diagramming with a Robot,2024,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189109198&doi=10.1145%2f3641514&partnerID=40&md5=c11056141746bb32f078d4a098e59bc9,"We investigate what it might look like for a robot to work with a human on a need-finding design task using an affinity diagram. While some recent projects have examined how human-robot teams might explore solutions to design problems, human-robot collaboration in the sensemaking aspects of the design process has not been studied. Designers use affinity diagrams to make sense of unstructured information by clustering paper notes on a work surface. To explore human-robot collaboration on a sensemaking design activity, we developed HIRO, an autonomous robot that constructs affinity diagrams with humans. In a within-user study, 56 participants affinity-diagrammed themes to characterize needs in quotes taken from real-world user data, once alone and once with HIRO. Users spent more time on the task with HIRO than alone, without strong evidence for corresponding effects on cognitive load. In addition, a majority of participants said they preferred to work with HIRO. From post-interaction interviews, we identified eight themes leading to four guidelines for robots that collaborate with humans on sensemaking design tasks: (1) account for the robot's speed, (2) pursue mutual understanding rather than just correctness, (3) identify opportunities for constructive disagreements, and (4) use other modes of communication in addition to physical materials.  © 2024 Copyright held by the owner/author(s).",Additional Key Words and Phraseshuman-robot collaboration; affinity diagramming; design cognition; human-robot collaborative design,Human engineering; Human robot interaction; Additional key word and phraseshuman-robot collaboration; Affinity diagram; Affinity diagramming; Collaborative design; Design cognition; Design tasks; Human robots; Human-robot collaborative design; Key words; Sense making; Machine design
Conflict Avoidance in Social Navigation—a Survey,2024,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189049639&doi=10.1145%2f3647983&partnerID=40&md5=204500ad8cb5718de6af302f6c6460d1,"A major goal in robotics is to enable intelligent mobile robots to operate smoothly in shared human-robot environments. One of the most fundamental capabilities in service of this goal is competent navigation in this “social” context. As a result, there has been a recent surge of research on social navigation; and especially as it relates to the handling of conflicts between agents during social navigation. These developments introduce a variety of models and algorithms, however as this research area is inherently interdisciplinary, many of the relevant papers are not comparable and there is no shared standard vocabulary. This survey aims at bridging this gap by introducing such a common language, using it to survey existing work, and highlighting open problems. It starts by defining the boundaries of this survey to a limited, yet highly common type of social navigation—conflict avoidance. Within this proposed scope, this survey introduces a detailed taxonomy of the conflict avoidance components. This survey then maps existing work into this taxonomy, while discussing papers using its framing. Finally, this article proposes some future research directions and open problems that are currently on the frontier of social navigation to aid ongoing and future research. © 2024 Association for Computing Machinery. All rights reserved.",human-robot interactions; mobile robots; Social navigation,Human robot interaction; Intelligent robots; Navigation; Taxonomies; Common languages; Conflict avoidance; Human robots; Humans-robot interactions; Intelligent mobile robot; Model and algorithms; Research areas; Robot environment; Social context; Social navigation; Mobile robots
Towards an Integrative Framework for Robot Personality Research,2024,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189070309&doi=10.1145%2f3640010&partnerID=40&md5=1567487e778bacc355ebb153a65fa36c,"Within human-robot interaction (HRI), research on robot personality has largely drawn on trait theories and models, such as the Big Five and OCEAN. We argue that reliance on trait models in HRI has led to a limited understanding of robot personality as a question of stable traits that can be designed into a robot plus how humans with certain traits respond to particular robots. However, trait-based approaches exist alongside other ways of understanding personality, including approaches focusing on more dynamic constructs such as adaptations and narratives. We suggest that a deep understanding of robot personality is only possible through a cross-disciplinary effort to integrate these different approaches. We propose an Integrative Framework for Robot Personality Research (IF), wherein robot personality is defined not as a property of the robot, nor of the human perceiving the robot, but as a complex assemblage of components at the intersection of robot design and human factors. With the IF, we aim to establish a common theoretical grounding for robot personality research that incorporates personality constructs beyond traits and treats these constructs as complementary and fundamentally interdependent.  © 2024 Copyright held by the owner/author(s).",Additional Key Words and PhrasesHuman-robot interaction; personality levels; robot personality,Machine design; Man machine systems; Additional key word and phraseshuman-robot interaction; Big five; Humans-robot interactions; Integrative framework; Key words; Personality level; Robot interactions; Robot personalities; Theory and models; Trait theory; Human robot interaction
Variable Autonomy through Responsible Robotics: Design Guidelines and Research Agenda,2024,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189094216&doi=10.1145%2f3636432&partnerID=40&md5=7f55abfdeffd5660f17ee7c0e8695000,"Physically embodied artificial agents, or robots, are being incorporated into various practical and social contexts, from self-driving cars for personal transportation to assistive robotics in social care. To enable these systems to better perform under changing conditions, designers have proposed to endow robots with varying degrees of autonomous capabilities and the capacity to move between them-an approach known as variable autonomy. Researchers are beginning to understand how robots with fixed autonomous capabilities influence a person's sense of autonomy, social relations, and, as a result, notions of responsibility; however, addressing these topics in scenarios where robot autonomy dynamically changes is underexplored. To establish a research agenda for variable autonomy that emphasises the responsible design and use of robotics, we conduct a developmental review. Based on a sample of 42 papers, we provide a synthesised definition of variable autonomy to connect currently disjointed research efforts, detail research approaches in variable autonomy to strengthen the empirical basis for subsequent work, characterise the dimensions of variable autonomy, and present design guidelines for variable autonomy research based on responsible robotics.  © 2024 Copyright held by the owner/author(s).",Additional Key Words and PhrasesVariable autonomy; literature review; responsible innovation; responsible robotics,Design; Additional key word and phrasesvariable autonomy; Artificial agents; Autonomous capability; Key words; Literature reviews; Research agenda; Responsible innovation; Responsible robotic; Robotic design; Social context; Robots
Data-driven Communicative Behaviour Generation: A Survey,2024,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189100837&doi=10.1145%2f3609235&partnerID=40&md5=d80aea0c26fe2b978e399d3efb45ae76,"The development of data-driven behaviour generating systems has recently become the focus of considerable attention in the fields of human-Agent interaction and human-robot interaction. Although rule-based approaches were dominant for years, these proved inflexible and expensive to develop. The difficulty of developing production rules, as well as the need for manual configuration to generate artificial behaviours, places a limit on how complex and diverse rule-based behaviours can be. In contrast, actual human-human interaction data collected using tracking and recording devices makes humanlike multimodal co-speech behaviour generation possible using machine learning and specifically, in recent years, deep learning. This survey provides an overview of the state of the art of deep learning-based co-speech behaviour generation models and offers an outlook for future research in this area.  © 2024 Copyright held by the owner/author(s).",Additional Key Words and PhrasesDatasets; data-driven behaviour generation; neural networks,Human robot interaction; Learning systems; Additional key word and phrasesdataset; Behavior generation; Data driven; Data-driven behavior generation; Generating system; Human-agent interaction; Humans-robot interactions; Interaction robot; Key words; Neural-networks; Deep learning
Forging Productive Human-Robot Partnerships Through Task Training,2024,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189081120&doi=10.1145%2f3611657&partnerID=40&md5=9c2858d355c8f765c49eebd813110a43,"Productive human-robot partnerships are vital to successful integration of assistive robots into everyday life. Although prior research has explored techniques to facilitate collaboration during human-robot interaction, the work described here aims to forge productive partnerships prior to human-robot interaction, drawing upon team-building activities' aid in establishing effective human teams. Through a 2 (group membership: ingroup and outgroup) ×3 (robot error: main task errors, side task errors, and no errors) online study (N=62), we demonstrate that (1) a non-social pre-task exercise can help form ingroup relationships; (2) an ingroup robot is perceived as a better, more committed teammate than an outgroup robot (despite the two behaving identically); and (3) participants are more tolerant of negative outcomes when working with an ingroup robot. We discuss how pre-task exercises may serve as an active task failure mitigation strategy.  © 2024 Copyright held by the owner/author(s).",Additional Key Words and PhrasesHuman-robot interaction; group membership; robot error,Human robot interaction; Man machine systems; Mergers and acquisitions; Additional key word and phraseshuman-robot interaction; Assistive robots; Group memberships; Human robots; Humans-robot interactions; Key words; Robot error; Robot interactions; Task trainings; Team building; Errors
Effortless Polite Telepresence using Intention Recognition,2024,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189097555&doi=10.1145%2f3636433&partnerID=40&md5=a7517d3fcbde2bc17be1f258d9f537d0,"Telepresence technology creates the opportunity for people that were traditionally left out of the workforce to work remotely. In the service industry, a pool of novice remote workers could teleoperate robots to perform short work stints to fill in the gaps left by the dwindling workforce. A hurdle is that consistently talking appropriately and politely imposes a severe mental burden on such novice operators and the quality of the service may suffer. In this study, we propose a teleoperation support system that lets novice remote workers talk freely without considering appropriateness and politeness while maintaining the quality of the service. The proposed system exploits intent recognition to transform casual utterances into predefined appropriate and polite utterances. We conducted a within-subject user study where 23 participants played the role of novice remote operators controlling a guardsman robot in charge of monitoring customers' behaviors. We measured the workload with and without using the proposed support system using NASA task load index questionnaires. The workload was significantly lower (p< .001) when using the proposed support system (M = 46.07, SD = 14.36) than when not using it (M = 62.74, SD = 12.70). The effect size was large (Cohen's d = 1.23).  © 2024 Copyright held by the owner/author(s).",Additional Key Words and PhrasesTeleoperation; service robot; user study,Behavioral research; Personnel; Remote control; Robots; Visual communication; Additional key word and phrasesteleoperation; Intent recognition; Intention recognition; Key words; Remote workers; Service industries; Service robots; Support systems; Telepresence; User study; NASA
Understanding Human Dynamic Sampling Objectives to Enable Robot-Assisted Scientific Decision Making,2024,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189078450&doi=10.1145%2f3623383&partnerID=40&md5=c3a4624f609e2d983009cad3cbc87f7f,"Truly collaborative scientific field data collection between human scientists and autonomous robot systems requires a shared understanding of the search objectives and tradeoffs faced when making decisions. Therefore, critical to developing intelligent robots to aid human experts is an understanding of how scientists make such decisions and how they adapt their data collection strategies when presented with new information in situ. In this study, we examined the dynamic data collection decisions of 108 expert geoscience researchers using a simulated field scenario. Human data collection behaviors suggested two distinct objectives: An information-based objective to maximize information coverage and a discrepancy-based objective to maximize hypothesis verification. We developed a highly simplified quantitative decision model that allows the robot to predict potential human data collection locations based on the two observed human data collection objectives. Predictions from the simple model revealed a transition from information-based to discrepancy-based objective as the level of information increased. The findings will allow robotic teammates to connect experts' dynamic science objectives with the adaptation of their sampling behaviors and, in the long term, enable the development of more cognitively compatible robotic field assistants.  © 2024 Copyright held by the owner/author(s).",Additional Key Words and PhrasesDecision making; human cognitive model; robot-Assisted scientific exploration,Behavioral research; Data acquisition; Intelligent robots; Search engines; Additional key word and phrasesdecision making; Cognitive model; Data collection; Dynamic sampling; Human cognitive model; Human data; Human dynamics; Key words; Robot-assisted scientific exploration; Scientific exploration; Decision making
Brain-Behavior Relationships of Trust in Shared Space Human-Robot Collaboration,2024,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189082898&doi=10.1145%2f3632149&partnerID=40&md5=b515bcead50cc1894e14d2f48d916c6b,"Trust in human-robot collaboration is an essential consideration that relates to operator performance, utilization, and experience. While trust's importance is understood, the state-of-The-Art methods to study trust in automation, like surveys, drastically limit the types of insights that can be made. Improvements in measuring techniques can provide a granular understanding of influencers like robot reliability and their subsequent impact on human behavior and experience. This investigation quantifies the brain-behavior relationships associated with trust manipulation in shared space human-robot collaboration to advance the scope of metrics to study trust. Thirty-eight participants, balanced by sex, were recruited to perform an assembly task with a collaborative robot under reliable and unreliable robot conditions. Brain imaging, psychological and behavioral eye-Tracking, quantitative and qualitative performance, and subjective experiences were monitored. Results from this investigation identify specific information processing and cognitive strategies that result in identified trust-related behaviors that were found to be sex specific. The use of covert measurements of trust can reveal insights that humans cannot consciously report, thus shedding light on processes systematically overlooked by subjective measures. Our findings connect a trust influencer (robot reliability) to upstream cognition and downstream human behavior and are enabled by the utilization of granular metrics.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesHuman-robot interaction; eye-Tracking; fnirs; sex; survey,Behavioral research; Brain mapping; Human robot interaction; Additional key word and phraseshuman-robot interaction; Essential considerations; Eye-tracking; Fnirs; Human behaviors; Human-robot collaboration; Key words; Robot interactions; Sex; Shared spaces; Eye tracking
Scarecrows in Oz: The Use of Large Language Models in HRI,2024,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85188537614&doi=10.1145%2f3606261&partnerID=40&md5=b17820b234faf440d8e0f7d87ae3ba9b,"The proliferation of Large Language Models (LLMs) presents both a critical design challenge and a remarkable opportunity for the field of Human-Robot Interaction (HRI). While the direct deployment of LLMs on interactive robots may be unsuitable for reasons of ethics, safety, and control, LLMs might nevertheless provide a promising baseline technique for many elements of HRI. Specifically, in this article, we argue for the use of LLMs as Scarecrows: ""brainless,""straw-man black-box modules integrated into robot architectures for the purpose of quickly enabling full-pipeline solutions, much like the use of ""Wizard of Oz""(WoZ) and other human-in-The-loop approaches. We explicitly acknowledge that these Scarecrows, rather than providing a satisfying or scientifically complete solution, incorporate a form of the wisdom of the crowd and, in at least some cases, will ultimately need to be replaced or supplemented by a robust and theoretically motivated solution. We provide examples of how Scarecrows could be used in language-capable robot architectures as useful placeholders and suggest initial reporting guidelines for authors, mirroring existing guidelines for the use and reporting of WoZ techniques.  © 2024 Copyright held by the owner/author(s).",Additional Key Words and PhrasesLarge Language Models; robot cognitive architectures; Robot Ethics,Computational linguistics; Ethical technology; Machine design; Additional key word and phraseslarge language model; Cognitive architectures; Critical design; Humans-robot interactions; Key words; Language model; Robot architecture; Robot cognitive architecture; Robot ethics; Wizard of Oz; Human robot interaction
Is Someone There or Is That the TV? Detecting Social Presence Using Sound,2023,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181681608&doi=10.1145%2f3611658&partnerID=40&md5=4623d012052d4b609e54ec122d7394e0,"Social robots in the home will need to solve audio identification problems to better interact with their users. This article focuses on the classification between (a) natural conversation that includes at least one co-located user and (b) media that is playing from electronic sources and does not require a social response, such as television shows. This classification can help social robots detect a user's social presence using sound. Social robots that are able to solve this problem can apply this information to assist them in making decisions, such as determining when and how to appropriately engage human users. We compiled a dataset from a variety of acoustic environments that contained either natural or media audio, including audio that we recorded in our own homes. Using this dataset, we performed an experimental evaluation on a range of traditional machine learning classifiers and assessed the classifiers' abilities to generalize to new recordings, acoustic conditions, and environments. We conclude that a C-Support Vector Classification (SVC) algorithm outperformed other classifiers. Finally, we present a classification pipeline that in-home robots can utilize, and we discuss the timing and size of the trained classifiers as well as privacy and ethics considerations.  © 2023 Copyright held by the owner/author(s).",Additional Key Words and PhrasesHuman-robot interaction; audio analysis; in-home systems,Audio acoustics; Audio systems; Robots; Support vector machines; Acoustic environment; Additional key word and phraseshuman-robot interaction; Audio analysis; Audio identification; Identification problem; In-home system; Key words; Robot interactions; Social presence; Social robots; Classification (of information)
"""Who Said That?"" Applying the Situation Awareness Global Assessment Technique to Social Telepresence",2023,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181484543&doi=10.1145%2f3592801&partnerID=40&md5=2fd7e5b82fda048ab2837a92fe3617ab,"As with all remotely controlled robots, successful teleoperation of social and telepresence robots relies greatly on operator situation awareness; however, existing situation awareness measurements, most being originally created for military purposes, are not adapted to the context of social interaction. We propose an objective technique for telepresence evaluation based on the widely accepted Situation Awareness Global Assessment Technique, adjusted to suit social contexts. This was trialled in a between-subjects participant study (n = 56) comparing the effect of mono and spatial (binaural) audio feedback on operator situation awareness during robot teleoperation in a simulated social telepresence scenario. Subjective data were also recorded, including questions adapted from Witmer and Singer's Presence Questionnaire, as well as qualitative feedback from participants. No significant differences in situation awareness measurements were detected; however, correlations observed between measures call for further research. This study and its findings are a potential starting point for the development of social situation awareness assessment techniques, which can inform future social and telepresence robot design decisions.  © 2023 Copyright held by the owner/author(s).",Additional Key Words and PhrasesTelepresence; robot teleoperation; situation awareness; social robotics; spatial audio,Economic and social effects; Feedback; Human robot interaction; Machine design; Visual communication; Additional key word and phrasestelepresence; Key words; Robot teleoperation; Situation awareness; Situation awareness global assessment techniques; Social robotics; Social robots; Spatial audio; Telepresence; Telepresence robots; Remote control
Nonverbal Sound in Human-Robot Interaction: A Systematic Review,2023,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176838230&doi=10.1145%2f3583743&partnerID=40&md5=bb7075d93f2ce87f9f771e6d898eea39,"Nonverbal sound offers great potential to enhance robots' interactions with humans, and a growing body of research has begun to explore nonverbal sound for tasks such as sound source localization, explicit communication, and improving sociability. However, nonverbal sound has a broad interpretation and design space that can draw from areas such as machine learning, music theory, and foley. We sought to identify and compare use cases and approaches for nonverbal sound in human-robot interaction through a systematic review. A search of sound and robotics-related publisher databases yielded 148 peer-reviewed articles presenting systems, studies, and taxonomies. Differences in taxonomy and overlap of terminology with adjacent research fields such as speech, gaze, and gesture posed difficulties for the search, which we attempted to address through a multi-stage search process. Based on the reviewed articles, we developed a pair of taxonomies using scientific communication principles and analyzed study designs and measures for the creation of nonverbal robot sound. We discuss recommendations for the field, including the use of the new taxonomies; methods for design, generation, and validation; and paths for future research. Roboticists may benefit from incorporating nonverbal sound as a key component in multimodal human-robot interaction.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesNonverbal sound; human-robot interaction; systematic review,Machine design; Man machine systems; Music; Taxonomies; Additional key word and phrasesnonverbal sound; Design spaces; Explicit communication; Growing bodies; Humans-robot interactions; Key words; Nonverbals; Robot interactions; Sound source localization; Systematic Review; Human robot interaction
"Robots' ""woohoo"" and ""argh"" Can Enhance Users' Emotional and Social Perceptions: An Exploratory Study on Non-lexical Vocalizations and Non-linguistic Sounds",2023,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181492140&doi=10.1145%2f3626185&partnerID=40&md5=d74582dbfc06963632ac8fe49ef63216,"As robots have become more pervasive in our everyday life, social aspects of robots have attracted researchers' attention. Because emotions play a crucial role in social interactions, research has been conducted on conveying emotions via speech. Our study sought to investigate the synchronization of multimodal interaction in human-robot interaction (HRI). We conducted a within-subjects exploratory study with 40 participants to investigate the effects of non-speech sounds (natural voice, synthesized voice, musical sound, and no sound) and basic emotions (anger, fear, happiness, sadness, and surprise) on user perception with emotional body gestures of an anthropomorphic robot (Pepper). While listening to a fairytale with the participant, a humanoid robot responded to the story with recorded emotional non-speech sounds and gestures. Participants showed significantly higher emotion recognition accuracy from the natural voice than from other sounds. The confusion matrix showed that happiness and sadness had the highest emotion recognition accuracy, which is in line with previous research. The natural voice also induced higher trust, naturalness, and preference compared to other sounds. Interestingly, the musical sound mostly showed lower perception ratings, even compared to no sound. Results are discussed with design guidelines for emotional cues from social robots and future research directions.  © 2023 Copyright held by the owner/author(s).",Additional Key Words and PhrasesHuman-robot interaction; emotion perception; non-speech sounds; robot voice,Behavioral research; Emotion Recognition; Human robot interaction; Machine design; Music; Social aspects; Speech recognition; Additional key word and phraseshuman-robot interaction; Emotion perception; Exploratory studies; Key words; Musical sounds; Non speech; Non-speech sound; Robot interactions; Robot voice; Speech sounds; Anthropomorphic robots
Sounding Robots: Design and Evaluation of Auditory Displays for Unintentional Human-robot Interaction,2023,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181449398&doi=10.1145%2f3611655&partnerID=40&md5=6d026f5b61f8c158281895a2b3055c7f,"Non-verbal communication is important in HRI, particularly when humans and robots do not need to actively engage in a task together, but rather they co-exist in a shared space. Robots might still need to communicate states such as urgency or availability, and where they intend to go, to avoid collisions and disruptions. Sounds could be used to communicate such states and intentions in an intuitive and non-disruptive way. Here, we propose a multi-layer classification system for displaying various robot information simultaneously via sound. We first conceptualise which robot features could be displayed (robot size, speed, availability for interaction, urgency, and directionality); we then map them to a set of audio parameters. The designed sounds were then evaluated in five online studies, where people listened to the sounds and were asked to identify the associated robot features. The sounds were generally understood as intended by participants, especially when they were evaluated one feature at a time, and partially when they were evaluated two features simultaneously. The results of these evaluations suggest that sounds can be successfully used to communicate robot states and intended actions implicitly and intuitively.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesSonification; Auditory Display; Design Evaluation; Non-verbal communication; unintentional Human-Robot Interaction,Classification (of information); Machine design; Man machine systems; Additional key word and phrasessonification; Auditory display; Design and evaluations; Design evaluation; Humans-robot interactions; Key words; Non-verbal communications; Robot designs; Shared spaces; Unintentional human-robot interaction; Human robot interaction
New Design Potentials of Non-mimetic Sonification in Human-Robot Interaction,2023,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181709724&doi=10.1145%2f3611646&partnerID=40&md5=d898be78be80696ec6876f8f4cfdcd97,"With the increasing use and complexity of robotic devices, the requirements for the design of human-robot interfaces are rapidly changing and call for new means of interaction and information transfer. On that scope, the discussed project - being developed by the Hybrid Things Lab at the University of Applied Sciences Augsburg and the Design Research Lab at Bauhaus-Universität Weimar - takes a first step in characterizing a novel field of research, exploring the design potentials of non-mimetic sonification in the context of human-robot interaction. Featuring an industrial seven-axis manipulator and collecting multiple information (for instance, the position of the end-effector, joint positions and forces) during manipulation, these datasets are being used for creating a novel augmented audible presence and thus allowing new forms of interaction. As such, this article considers(1)research parameters for non-mimetic sonification (such as pitch, volume, and timbre);(2)a comprehensive empirical pursuit, including setup, exploration, and validation;(3)the overall implications of integrating these findings into a unifying human-robot interaction process. The relation between machinic and auditory dimensionality is of particular concern.  © 2023 Copyright held by the owner/author(s).",Additional Key Words and PhrasesHuman-robot interaction; auditory display; augmented presence; industrial robot; interaction design; non-mimetic; sonification,Industrial manipulators; Industrial research; Machine design; Man machine systems; Additional key word and phraseshuman-robot interaction; Auditory display; Augmented presence; Humans-robot interactions; Interaction design; Key words; Mimetics; Non-mimetic; Robot interactions; Sonifications; Human robot interaction
The Effects of Natural Sounds and Proxemic Distances on the Perception of a Noisy Domestic Flying Robot,2023,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85167727346&doi=10.1145%2f3579859&partnerID=40&md5=ea4fac54941c4e0564e4d6f7ef42cb04,"When flying robots are used in close-range interaction with humans, the noise they generate, also called consequential sound, is a critical parameter for user acceptance. We conjecture that there is a benefit in adding natural sounds to noisy domestic drones. To test our hypothesis experimentally, we carried out a mixed-methods research study (N = 56) on reported user perception of a sonified domestic flying robot with three sound conditions at three distances. The natural sounds studied were, respectively, added to the robot's inherent noises during flying; namely, a birdsong and a rain sound, plus a control condition of no added sound. The distances studied were set according to proxemics; namely, near, middle, and far. Our results show that adding birdsong or rain sound affects the participants' perceptions, and the proxemic distances play a nonnegligible role. For instance, we found that participants liked the bird condition the most when the drone was at far, while they disliked the same sound the most when at near. We also found that participants' perceptions strongly depended on their associations and interpretations deriving from previous experience. We derived six concrete design recommendations.  © 2023 Copyright held by the owner/author(s).",Additional Key Words and PhrasesClose-range human-drone interaction; consequential sound; domestic flying robots; natural sounds,Human robot interaction; Rain; Additional key word and phrasesclose-range human-drone interaction; Close range; Condition; Consequential sound; Domestic flying robot; Flying robots; Key words; Mixed-methods research; Natural sounds; Users' acceptance; Drones
Introduction to the Special Issue on Sound in Human-Robot Interaction,2023,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181525947&doi=10.1145%2f3632185&partnerID=40&md5=dfe192669939a2bca77780acc4b3830e,"INTRODUCTION: Sound is an important interaction modality and a large part of human interaction happens in the aural domain. While research in Human-Robot Interaction (HRI) has long explored spoken language for interacting with humans, sound as a broader-and, to a significant degree, nonlexical (i.e., without words)-medium has been given comparably less attention. Yet, the range of sounds that robots can produce is vast, encompassing, among others, mechanical noise, music, and utterances that mimic human and animal vocalizations with varying degrees of realism. The sound of a robot's machinery can shape our perceptions and expectations [11, 17], music serves as a medium for robots to engage and communicate [18], and shared musical experiences can strengthen the bond between humans and robots [5]. Sonifications may enhance the legibility of movement and gestures [4, 7, 15] and beep sounds may be used to communicate emotions [2, 14]. Getting closer to themargins of language, robotsmay take inspiration from non-lexical fillers such as ""uh"" [13, 16] and backchannels such as ""mhmm"" [8, 12]. More generally, pitch, intensity, and other human prosodic variations may be drawn on in robot sound design [3, 10]. The information that can be extracted from sound in a robot's environment is equally rich. Beyond the recognition of semantic content, robots use, for example, sound source localization to gain a better understanding of their environment [9], or analyze a human's voice timbre and tone to distinguish speakers [6] and detect emotion [1]. © 2023 Copyright held by the owner/author(s).",,Emotion Recognition; Machine design; Man machine systems; Music; Semantics; Back channels; Human sounds; Humaninteraction; Humans-robot interactions; Large parts; Mechanical noise; Prosodics; Sonifications; Sound designs; Spoken languages; Human robot interaction
Probing Aesthetics Strategies for Robot Sound: Complexity and Materiality in Movement Sonification,2023,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85170233153&doi=10.1145%2f3585277&partnerID=40&md5=c94c6e6b87fa76872c8e9c104d439a76,"This article presents three studies where we probe aesthetics strategies of sound produced by movement sonification of a Pepper robot by mapping its movements to sound models.We developed two sets of sound models. The first set was made by two sound models, a sawtooth-based one and another based on feedback chains, for investigating how the perception of synthesized robot sounds would depend on their design complexity. We implemented the second set of sound models for probing the ""materiality""of sound made by a robot in motion. This set consisted of a sound synthesis based on an engine highlighting the robot's internal mechanisms, a metallic sound synthesis highlighting the robot's typical appearance, and a whoosh sound synthesis highlighting the movement.We conducted three studies. The first study explores how the first set of sound models can influence the perception of expressive gestures of a Pepper robot through an online survey. In the second study, we carried out an experiment in a museum installation with a Pepper robot presented in two scenarios: (1) while welcoming patrons into a restaurant and (2) while providing information to visitors in a shopping center. Finally, in the third study, we conducted an online survey with stimuli similar to those used in the second study.Our findings suggest that participants preferred more complex sound models for the sonification of robot movements. Concerning the materiality, participants liked better subtle sounds that blend well with the ambient sound (i.e., less distracting) and soundscapes in which sound sources can be identified. Also, sound preferences varied depending on the context in which participants experienced the robot-generated sounds (e.g., as a live museum installation vs. an online display).  © 2023 Copyright held by the owner/author(s).",Additional Key Words and PhrasesRobot sound; HRI; sonification; sound design,Machine design; Museums; Additional key word and phrasesrobot sound; HRI; Key words; Online surveys; Sawteeth; Sonifications; Sound designs; Sound modeling; Sound-synthesis; Synthesised; Robots
Which Voice for which Robot? Designing Robot Voices that Indicate Robot Size,2023,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181447180&doi=10.1145%2f3632124&partnerID=40&md5=ddd781e282b53a96d3ba493b52214529,"Many social robots will have the capacity to interact via speech in the future, and thus they will have to have a voice. However, so far it is unclear how we can create voices that fit their robotic speakers. In this article, we explore how robot voices can be designed to fit the size of the respective robot. We therefore investigate the acoustic correlates of human voices and body size. In Study I, we analyzed 163 speech samples in connection with their speakers' body size and body height. Our results show that specific acoustic parameters are significantly associated with body height, and to a lesser degree to body weight, but that different features are relevant for female and male voices. In Study II, we tested then for female and male voices to what extent the acoustic features identified can be used to create voices that are reliably associated with the size of robots. The results show that the acoustic features identified provide reliable clues to whether a large or a small robot is speaking.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",acoustic analysis; Additional Key Words and PhrasesRobot voice; body size,Anthropometry; Acoustic analysis; Acoustic correlates; Acoustic features; Acoustic parameters; Additional key word and phrasesrobot voice; Body sizes; Human bodies; Human voice; Key words; Social robots; Robots
The Sound of Swarm. Auditory Description of Swarm Robotic Movements,2023,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181488242&doi=10.1145%2f3596203&partnerID=40&md5=aaee61ec1c62a3fb9c678cb722177946,"Movements of robots in a swarm can be mapped to sounds, highlighting the group behavior through the coordinated and simultaneous variations of musical parameters across time. The vice versa is also possible: Sound parameters can be mapped to robotic motion parameters, giving instructions through sound. In this article, we first develop a theoretical framework to relate musical parameters such as pitch, timbre, loudness, and articulation (for each time) with robotic parameters such as position, identity, motor status, and sensor status. We propose a definition of musical spaces as Hilbert spaces and musical paths between parameters as elements of bigroupoids, generalizing existing conceptions of musical spaces. The use of Hilbert spaces allows us to build up quantum representations of musical states, inheriting quantum computing resources, already used for robotic swarms. We present the theoretical framework and then some case studies as toy examples. In particular, we discuss a 2D video and matrix simulation with two robo-caterpillars; a 2D simulation of 10 robo-ants with Webots; a 3D simulation of three robo-fish in an underwater search-and-rescue mission.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesSonification; musical spaces; nature-inspired swarms; quantum computing; robotic spaces,Biomimetics; Hilbert spaces; Quantum computers; Quantum optics; Robotics; Swarm intelligence; Toys; Additional key word and phrasessonification; Key words; Musical parameters; Musical space; Nature-inspired swarm; Quantum Computing; Robotic movements; Robotic space; Swarm robotics; Theoretical framework; Vector spaces
The Power of Robot-mediated Play: Forming Friendships and Expressing Identity,2023,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181472764&doi=10.1145%2f3611656&partnerID=40&md5=844ce0cacb90480d0d1eede1e4b57f0f,"Tele-operated collaborative robots are used by many children for academic learning. However, as child-directed play is important for social-emotional learning, it is also important to understand how robots can facilitate play. In this article, we present findings from an analysis of a national, multi-year case study, where we explore how 53 children in grades K-12 (n = 53) used robots for self-directed play activities. The contributions of this article are as follows. First, we present empirical data on novel play scenarios that remote children created using their tele-operated robots. These play scenarios emerged in five categories of play: physical, verbal, visual, extracurricular, and wished-for play. Second, we identify two unique themes that emerged from the data - robot-mediated play as a foundational support of general friendships and as a foundational support of self-expression and identity. Third, our work found that robot-mediated play provided benefits similar to in-person play. Findings from our work will inform novel robot and HRI design for tele-operated and social robots that facilitate self-directed play. Findings will also inform future interdisciplinary studies on robot-mediated play.  © 2023 Copyright held by the owner/author(s).",inclusion; learning; Play; social robots; telerobots,Machine design; Collaborative robots; Emotional learning; Learning; Play; Power; Self-directed; Social robots; Teleoperated; Teleoperated robots; Telerobot; Robots
"Virtual, Augmented, and Mixed Reality for Human-robot Interaction: A Survey and Virtual Design Element Taxonomy",2023,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181521829&doi=10.1145%2f3597623&partnerID=40&md5=b6c2f053b78e185969555cf880d8824a,"Virtual, Augmented, and Mixed Reality for Human-Robot Interaction (VAM-HRI) has been gaining considerable attention in HRI research in recent years. However, the HRI community lacks a set of shared terminology and framework for characterizing aspects of mixed reality interfaces, presenting serious problems for future research. Therefore, it is important to have a common set of terms and concepts that can be used to precisely describe and organize the diverse array of work being done within the field. In this article, we present a novel taxonomic framework for different types of VAM-HRI interfaces, composed of four main categories of virtual design elements (VDEs). We present and justify our taxonomy and explain how its elements have been developed over the past 30 years as well as the current directions VAM-HRI is headed in the coming decade. © 2023 Copyright held by the owner/author(s)",augmented reality; human-computer interaction; Human-robot interaction; interface design; mixed reality; robots; virtual reality,Augmented reality; Human computer interaction; Human robot interaction; Man machine systems; Mixed reality; Taxonomies; Current direction; Design elements; Humans-robot interactions; Interaction interface; Interface designs; Mixed reality; Mixed reality interfaces; Survey design; Virtual design; Machine design
"Introduction to the Special Issue on ""Designing the Robot Body: Critical Perspectives on Affective Embodied Interaction""",2023,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164361666&doi=10.1145%2f3594713&partnerID=40&md5=bf5665f955e6e8de534af3c6105cafcb,[No abstract available],,
It Takes Two: Using Co-creation to Facilitate Child-Robot Co-regulation,2023,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181508742&doi=10.1145%2f3593812&partnerID=40&md5=7982c5f167a97684c06f338778b3d3e8,"While interacting with a social robot, children have a need to express themselves and have their expressions acknowledged by the robot—a need that is often unaddressed by the robot, due to its limitations in understanding the expressions of children. To keep the child-robot interaction manageable, the robot takes control, undermining children’s ability to co-regulate the interaction. Co-regulation is important for having a fulfilling social interaction. We developed a co-creation activity that aims to facilitate more co-regulation. Children are enabled to create sound effects, gestures, and light animations for the robot to use during their conversation. A crucial additional feature is that children are able to coordinate their involvement of the co-creation process. Results from a user study (n = 59 school children, 7–11 years old) showed that the co-creation activity successfully facilitated co-regulation by improving children’s agency. It also positively affected the acceptance of the robot. We furthermore identified five distinct profiles detailing the different needs and motivations children have for the level of involvement they chose during the co-creation process. © 2023 Copyright held by the owner/author(s)",Child-robot interaction; co-creation; co-regulation; user study,Child-robot interactions; Co-creation; Co-regulation; Creation process; Gesture animation; Is-enabled; Social interactions; Social robots; Sound effects; User study
Fielded Human-Robot Interaction for a Heterogeneous Team in the DARPA Subterranean Challenge,2023,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164242060&doi=10.1145%2f3588325&partnerID=40&md5=4c5331bedebff2a2df3140a538b9acf9,"Human supervision of multiple fielded robots is a challenging task which requires a thoughtful design and implementation of both the underlying infrastructure and the human interface. It also requires a skilled human able to manage the workload and understand when to trust the autonomy, or manually intervene. We present an end-to-end system for human-robot interaction with a heterogeneous team of robots in complex, communication-limited environments. The system includes the communication infrastructure, autonomy interaction, and human interface elements. Results of the DARPA Subterranean Challenge Final Systems Competition are presented as a case study of the design and analyze the shortcomings of the system.  © 2023 Copyright held by the owner/author(s).",Additional Key Words and PhrasesAutonomous systems; field robotics; graphical user interface; multi-agent,Human robot interaction; Machine design; Man machine systems; Multi agent systems; Additional key word and phrasesautonomous system; Communication infrastructure; Design and implementations; End-to-end systems; Field robotics; Human Interface; Human supervision; Humans-robot interactions; Key words; Multi agent; Graphical user interfaces
A Computational Model of Coupled Human Trust and Self-confidence Dynamics,2023,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164236859&doi=10.1145%2f3594715&partnerID=40&md5=4fe77912f90bf0d7adb9fc7844cd74fc,"Autonomous systems that can assist humans with increasingly complex tasks are becoming ubiquitous. Moreover, it has been established that a human's decision to rely on such systems is a function of both their trust in the system and their own self-confidence as it relates to executing the task of interest. Given that both under- and over-reliance on automation can pose significant risks to humans, there is motivation for developing autonomous systems that could appropriately calibrate a human's trust or self-confidence to achieve proper reliance behavior. In this article, a computational model of coupled human trust and self-confidence dynamics is proposed. The dynamics are modeled as a partially observable Markov decision process without a reward function (POMDP/R) that leverages behavioral and self-report data as observations for estimation of these cognitive states. The model is trained and validated using data collected from 340 participants. Analysis of the transition probabilities shows that the proposed model captures the probabilistic relationship between trust, self-confidence, and reliance for all discrete combinations of high and low trust and self-confidence. The use of the proposed model to design an optimal policy to facilitate trust and self-confidence calibration is a goal of future work.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesHuman cognitive modeling; computational modeling; human self-confidence; human trust in automation; partially observable Markov decision process,Behavioral research; Computation theory; Computational methods; Markov processes; Additional key word and phraseshuman cognitive modeling; Cognitive model; Complex task; Computational modelling; Human decisions; Human self-confidence; Human trust in automation; Key words; Over reliance; Partially observable Markov decision process; Dynamics
Affective Corners as a Problematic for Design Interactions,2023,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173058583&doi=10.1145%2f3596452&partnerID=40&md5=db744558787bd118b559b7df1c5cf65b,"Domestic robots are already commonplace in many homes, while humanoid companion robots like Pepper are increasingly becoming part of different kinds of care work. Drawing on fieldwork at a robotics lab, as well as our personal encounters with domestic robots, we use here the metaphor of “hard-to-reach corners” to explore the socio-technical limitations of companion robots and our differing abilities to respond to these limitations. This article presents “hard-to-reach-corners” as a problematic for design interaction, offering them as an opportunity for thinking about context and intersectional aspects of adaptation. © 2023 Copyright held by the owner/author(s)",affect; design; Social robotics,Human robot interaction; Affect; Companion robot; Domestic robots; Robotics labs; Social robotics; Sociotechnical; Technical limitations; Economic and social effects
From Robotics to Prosthetics: What Design and Engineering Can Do Better Together,2023,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164238728&doi=10.1145%2f3588323&partnerID=40&md5=58e1b2d10b6a561ec281357ec0ad7676,"This paper discusses how the disciplines of Design and Engineering are jointly addressing disability and somehow affecting its very interpretation. The discussion focuses on high-tech prostheses, where robotic devices substitute human body parts. The application of robotic technologies to prosthetics has a relatively long history. Nevertheless, only in the last decade have we witnessed applications reach the market and become available for a large base of users who were offered prostheses with superior motor and sensory performance. The process of bringing ever more advanced technologies to fruition by prosthetic users is fully ongoing today, with some promising solutions coming from robotics (such as, e.g. AI techniques or soft robotics materials) to be transferred to human use. In this transfer process, technology alone is insufficient to warrant success, and the need for a close collaboration between the Engineering domain and the Design disciplines is apparent. We address this point with specific reference to a case study, i.e. the transformation of an innovative but by-now established technology in the industrial robotics field (the ""Pisa/IIT SoftHand"") into a prosthetic hand (the ""SoftHand Pro""). Besides obvious technical considerations about size, connections, control, and so on, which can be addressed with a thorough technical revision of the design, what makes the profound difference between the two devices is that, as a prosthesis, the SoftHand is intended as a human body part, and not as an external tool. To reach its ultimate goals, the hand should become a part of the human user, with his body and mind. The empirical approach and tools of Designers afford the possibility to enrich the re-design process, considering the final user at the centre of the process, in a sort of renewed humanistic approach. The paper reflects this multidisciplinary approach and is structured as follows: the first part describes a cultural framework for the use of high-technology upper limb prostheses. This culture is defined through two significant relations (Users & Society; Users & Device). Inputs come from desk research conducted in different fields, ranging from Social Psychology to Medicine and Rehabilitation area. In this scenario, it is possible to extract design insights applicable to the design brief. The introduction of a robotic prosthetic hand (SoftHand Pro) and a related, single-user case study follow. The aim here is also to illustrate a process where engineering innovations are facilitated by tools from the Design field in the attempt to make the whole process coherently centred on users. Involved are all aspects, from material technology to the covering and finishing of the prosthetic device. The resulting, final prototype of the SoftHand Pro is finally presented.  © 2023 Copyright held by the owner/author(s).",Additional Key Words and PhrasesRobotics; design process; disability culture; prostheses; social perception; users' needs,Design; Prosthetics; Social psychology; Additional key word and phrasesrobotic; Body parts; Case-studies; Design-process; Disability culture; Human bodies; Key words; Prosthetic hands; Social perception; User need; Robotics
How Do We Perceive Our Trainee Robots? Exploring the Impact of Robot Errors and Appearance When Performing Domestic Physical Tasks on Teachers' Trust and Evaluations,2023,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164246541&doi=10.1145%2f3582516&partnerID=40&md5=f7b3f907610e7e12db23846abdd081ff,"To be successful, robots that can learn new tasks from humans should interact effectively with them while being trained, and humans should be able to trust the robots' abilities after teaching. Typically, when human learners make mistakes, their teachers tolerate those errors, especially when students exhibit acceptable progress overall. But how do errors and appearance of a trainee robot affect human teachers' trust while the robot is generally improving in performing a task? First, an online survey with 173 participants investigated perceived severity of robot errors in performing a cooking task. These findings were then used in an interactive online experiment with 138 participants, in which the participants were able to remotely teach their food preparation preferences to trainee robots with two different appearances. Compared with an untidy-looking robot, a tidy-looking robot was rated as more professional, without impacting participants' trust. Furthermore, while larger errors at the end of iterative training had a greater impact, even a small error could significantly reduce trust in a trainee robot performing the domestic physical task of food preparation, regardless of the robot's appearance. The present study extends human-robot interaction knowledge about teachers' perception of trainee robots, particularly when teachers observe them accomplishing domestic physical tasks.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesRobot errors; perceived robot attributes; social learning; trust,Human robot interaction; Personnel training; Additional key word and phrasesrobot error; Food preparation; Human teachers; Key words; Learn+; Online surveys; Perceived robot attribute; Social learning; Teachers'; Trust; Errors
FABRIC: A Framework for the Design and Evaluation of Collaborative Robots with Extended Human Adaptation,2023,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159752147&doi=10.1145%2f3585276&partnerID=40&md5=6f49e808d880ab4de1a1b37f61e6b9c0,"A limitation for collaborative robots (cobots) is their lack of ability to adapt to human partners, who typically exhibit an immense diversity of behaviors. We present an autonomous framework as a cobot's real-time decision-making mechanism to anticipate a variety of human characteristics and behaviors, including human errors, toward a personalized collaboration. Our framework handles such behaviors in two levels: (1) short-term human behaviors are adapted through our novel Anticipatory Partially Observable Markov Decision Process (A-POMDP) models, covering a human's changing intent (motivation), availability, and capability; (2) long-term changing human characteristics are adapted by our novel Adaptive Bayesian Policy Selection (ABPS) mechanism that selects a short-term decision model, e.g., an A-POMDP, according to an estimate of a human's workplace characteristics, such as her expertise and collaboration preferences. To design and evaluate our framework over a diversity of human behaviors, we propose a pipeline where we first train and rigorously test the framework in simulation over novel human models. Then, we deploy and evaluate it on our novel physical experiment setup that induces cognitive load on humans to observe their dynamic behaviors, including their mistakes, and their changing characteristics such as their expertise. We conduct user studies and show that our framework effectively collaborates non-stop for hours and adapts to various changing human behaviors and characteristics in real-time. That increases the efficiency and naturalness of the collaboration with a higher perceived collaboration, positive teammate traits, and human trust. We believe that such an extended human-adaptation is a key to the long-term use of cobots.  © 2023 Copyright held by the owner/author(s).",Additional Key Words and PhrasesCollaborative robots; anticipatory decision-making; evaluating human adaptation; human-robot collaboration; user studies,Behavioral research; Decision making; Machine design; Markov processes; Additional key word and phrasescollaborative robot; Anticipatory decision-making; Collaborative robots; Decisions makings; Evaluating human adaptation; Human adaptation; Human behaviors; Human-robot collaboration; Key words; User study; Robots
Field Trial of a Shopworker Robot with Friendly Guidance and Appropriate Admonishments,2023,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164245778&doi=10.1145%2f3575805&partnerID=40&md5=ab3aa3001fa6167ce66a77c331ed215a,"We developed an admonishing service for a shopworker robot and conducted a field trial to investigate the impressions of a shop's staff and customers. Applying the admonishing service in a real-world robot is difficult due to the high risk of rejection by society. We wanted to achieve an acceptable admonishing service while simultaneously avoiding the impression of a forceful request from a machine. We proposed a harmonized design that provided friendly and admonishing services. First, we interviewed a shop's staff to learn their strategies for both friendly and admonishing services. From our evaluation of the interview results, we derived three design principles: friendly impressions, zero erroneous admonishments, and polite requests. Based on the design principles, we implemented our harmonized design on a social robot that guides customers to product locations and admonishes those who are not wearing face masks. We conducted a 13-day field trial in a retail shop and interviewed the customers and shopworkers to learn their impressions of our robot. The results of the field trial imply that our harmonized design approach is successful. Both the customers and the shop staff had overall positive impressions of the robot, its admonishing and friendly services, and expressed an intention to use it in the future. Furthermore, we studied the robot's autonomous service-providing capability in the field and conducted an evaluation with hired participants to deepen our study of the robot's mask recognition capability.  © 2023 Copyright held by the owner/author(s).",Additional Key Words and PhrasesShopworker robot; admonishment; field trial; guiding,Machine design; Product design; Robots; Additional key word and phrasesshopworker robot; Admonishment; Design Principles; Field trial; Guiding; Key words; Learn+; Product location; Real-world; Social robots; Sales
Core Challenges of Social Robot Navigation: A Survey,2023,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159835052&doi=10.1145%2f3583741&partnerID=40&md5=fddd9ee80ea9b626fb91ec128e7d1919,"Robot navigation in crowded public spaces is a complex task that requires addressing a variety of engineering and human factors challenges. These challenges have motivated a great amount of research resulting in important developments for the fields of robotics and human-robot interaction over the past three decades. Despite the significant progress and the massive recent interest, we observe a number of significant remaining challenges that prohibit the seamless deployment of autonomous robots in crowded environments. In this survey article, we organize existing challenges into a set of categories related to broader open problems in robot planning, behavior design, and evaluation methodologies. Within these categories, we review past work and offer directions for future research. Our work builds upon and extends earlier survey efforts by (a) taking a critical perspective and diagnosing fundamental limitations of adopted practices in the field and (b) offering constructive feedback and ideas that could inspire research in the field over the coming decade.  © 2023 Copyright held by the owner/author(s).",Additional Key Words and PhrasesSocial robot navigation; benchmarking; motion planning; motion prediction; multiagent systems; social robotics,Human robot interaction; Machine design; Motion estimation; Motion planning; Navigation; Robot programming; Additional key word and phrasessocial robot navigation; Complex task; Humans-robot interactions; Key words; Motion prediction; Motion-planning; Public space; Robot navigation; Social robotics; Social robots; Multi agent systems
First-Hand Impressions: Charting and Predicting User Impressions of Robot Hands,2023,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150452269&doi=10.1145%2f3580592&partnerID=40&md5=288db6a0e7959ac2e4bca414fcd8f463,"Designing robotic hands has been an active area of research and innovation in the last decade. However, little is known about how people perceive robot hands and react to being touched by them. To inform hand design for social robots, we created a database of 73 robot hands and ran two user studies. In the first study, 160 online users rated the hands in our database. Variations in user ratings mostly centered on the perceived Comfortableness, Interestingness, and Industrialness of the hands. In a second lab-based study, users evaluated seven physical hands and had similar ratings to results from the online study. Furthermore, we did not find a significant difference in user ratings before and after the users were touched by the hands. We provide regression models that can predict user ratings from the hand features (e.g., number of fingers) and an online interface for using our robot hand database and predictive models.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesRobotic hands; human-robot interaction; touch; user experience; user study,Human robot interaction; Machine design; Regression analysis; Robotic arms; Active area; Additional key word and phrasesrobotic hand; Hand-design; Humans-robot interactions; Key words; Robot hand; Touch; User rating; User study; Users' experiences; Database systems
Design Metaphors for Understanding User Expectations of Socially Interactive Robot Embodiments,2023,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164235077&doi=10.1145%2f3550489&partnerID=40&md5=3759b51b298e624061796a2f5f98e725,"The physical design of a robot suggests expectations of that robot's functionality for human users and collaborators. When those expectations align with the robot's true capabilities, users are more likely to adopt the technologies for their intended use. However, the relationship between expectations and socially interactive robot design is not well understood. This article applies the concept of design metaphors to robot design and contributes the Metaphors for Understanding Functional and Social Anticipated Affordances dataset of 165 extant robots and the expectations users place on them. We used Mechanical Turk to crowd-source user expectation over three user studies. The first study (N = 382) associated crowd-sourced design metaphors to different robot embodiments. The second study (N = 803) assessed initial social expectations of robot embodiments. The final study (N = 805) addressed the degree of abstraction of the design metaphors and the functional expectations projected on robot embodiments. We performed analyses to gain insights into how design metaphors can be used to understand social and functional expectations of robots and how these data can be visualized to be useful for study designers and robot designers. Together, these results can serve to guide robot designers toward aligning user expectations with true robot capabilities, facilitating positive human-robot interaction.  © 2023 Copyright held by the owner/author(s).",Additional Key Words and PhrasesSocially interactive robots; robot morphology; social perceptions,Crowdsourcing; Human robot interaction; Morphology; Additional key word and phrasessocially interactive robot; Design metaphors; Interactive robot; Key words; Physical design; Robot designers; Robot designs; Robot morphology; Social perception; User expectations; Machine design
"Applying ""Designerly Framing"" to Understand Assisted Feeding as Social Aesthetic Bodily Experiences",2023,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164241771&doi=10.1145%2f3583742&partnerID=40&md5=8294d469ebde3d8444ea39e2537f3a42,"What could it mean to take a designerly perspective of a robotic eating aid to get a more holistic understanding of meals as social and embodied experiences? In this article, we provide a new perspective of bodily experiences of assisted feeding. We apply ""designerly framing""in the context of meals and Human Robot Interaction (HRI) and contribute with insights for researchers with backgrounds other than design into how ""designerly framing""can foreground social and aesthetic use. The study focuses on experiences of assisted feeding of five people with impairments in their arms or hands. All of the subjects have long-term experience of meal assistance, and four also have experience of using a robotic eating aid. The data collection comprises seven interview sessions held in peoples homes, a functional analysis of the meal experience, and a workshop held at a design agency. The ""designerly framing""is also supported by a theoretical framework describing different types of use to open the meal as a design space. This complements and extends existing knowledge on acceptance and abandonment of assistive technology (AT) and assistive robotics for the meal.  © 2023 Copyright held by the owner/author(s).",accessibility; Additional Key Words and PhrasesRobotic eating aid; aesthetic bodily use; designerly framing; experience; HRI; meal assistance; practical use; qualities; social bodily use,Human robot interaction; Machine design; Accessibility; Additional key word and phrasesrobotic eating aid; Designerly framing; Esthetic bodily use; Experience; Humans-robot interactions; Key words; Meal assistance; Practical use; Quality; Social bodily use; Feeding
"Avoiding the Abject and Seeking the Script: Perceived Mind, Morality, and Trust in a Persuasive Social Robot",2023,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164235921&doi=10.1145%2f3572036&partnerID=40&md5=f5073cdfb4c888427bd4c4c5825d66ec,"Social robots are being groomed for human influence, including the implicit and explicit persuasion of humans. Humanlike characteristics are understood to enhance robots' persuasive impact; however, little is known of how perceptions of two key human capacities - mind and morality - function in robots' persuasive potential. This experiment tests the possibility that perceived robot mind and morality will correspond with greater persuasive impact, moderated by relational trustworthiness for a moral appeal and by capacity trustworthiness for a logical appeal. Via an online survey, a humanoid robot asks participants to help it learn to overcome CAPTCHA puzzles to access important online spaces - either on grounds that it is logical or moral to do so. Based on three performance indicators and one self-report indicator of compliance, analysis indicates that (a) seeing the robot as able to perceive and act on the world selectively improves compliance, and (b) perceiving agentic capacity diminishes compliance, though capacity trustworthiness can moderate that reduction. For logical appeals, social-moral mental capacities promote compliance, moderated by capacity trustworthiness. Findings suggest that, in this compliance scenario, the accessibility of schemas and scripts for engaging robots as social-moral actors may be central to whether/how perceived mind, morality, and trust function in machine persuasion.  © 2023 Copyright held by the owner/author(s).",abjection; Additional Key Words and PhrasesSocial robots; CAPTCHA; mentalizing; moral agency; persuasion; schema,Electronic mail filters; Abjection; Additional key word and phrasessocial robot; CAPTCHAs; Human influences; Key words; Mentalizing; Moral agency; Persuasion; Schema; Social robots; Anthropomorphic robots
From Inanimate Object to Agent: Impact of Pre-beginnings on the Emergence of Greetings with a Robot,2023,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161692363&doi=10.1145%2f3575806&partnerID=40&md5=1d5036b3220a7b874e80a32a450579a4,"The very first moments of co-presence, during which a robot appears to a participant for the first time, are often ""off-the-record""in the data collected from human-robot experiments (video recordings, motion tracking, methodology sections, etc.). Yet, this ""pre-beginning""phase, well documented in the case of human-human interactions, is not an interactional vacuum: It is where interactional work from participants can take place so the production of a first speaking turn (like greeting the robot) becomes relevant and expected. We base our analysis on an experiment that replicated the interaction opening delays sometimes observed in laboratory or ""in-the-wild""human-robot interaction studies - where robots can require time before springing to life after they are in co-presence with a human. Using an ethnomethodological and multimodal conversation analytic methodology (EMCA), we identify which properties of the robot's behavior were oriented to by participants as creating the adequate conditions to produce a first greeting. Our findings highlight the importance of the state in which the robot originally appears to participants: as an immobile object or, instead, as an entity already involved in preexisting activity. Participants' orientations to the very first behaviors manifested by the robot during this ""pre-beginning""phase produced a priori unpredictable sequential trajectories, which configured the timing and the manner in which the robot emerged as a social agent. We suggest that these first instants of co-presence are not peripheral issues with respect to human-robot experiments but should be thought about and designed as an integral part of those. © 2023 Copyright held by the owner/author(s).",anthropomorphism; computers are social actors; conversation analysis; ethnomethodology; greetings; Pre-beginning; robot latencies; social agent,Human robot interaction; Man machine systems; Anthropomorphism; Co-presence; Computers are social actors; Conversation analysis; Ethnomethodology; Greeting; Human-robot experiments; Pre-beginning; Robot latency; Social agents; Video recording
Concerning Trends in Likert Scale Usage in Human-robot Interaction: Towards Improving Best Practices,2023,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163414261&doi=10.1145%2f3572784&partnerID=40&md5=bd565e7dd0eaef64e50c297992633a26,"As robots become more prevalent, the importance of the field of human-robot interaction (HRI) grows accordingly. As such, we should endeavor to employ the best statistical practices in HRI research. Likert scales are commonly used metrics in HRI to measure perceptions and attitudes. Due to misinformation or honest mistakes, many HRI researchers do not adopt best practices when analyzing Likert data. We conduct a review of psychometric literature to determine the current standard for Likert scale design and analysis. Next, we conduct a survey of five years of the International Conference on Human-Robot Interaction (HRIc) (2016 through 2020) and report on incorrect statistical practices and design of Likert scales [1, 2, 3, 5, 7]. During these years, only 4 of the 144 papers applied proper statistical testing to correctly designed Likert scales. We additionally conduct a survey of best practices across several venues and provide a comparative analysis to determine how Likert practices differ across the field of Human-robot Interaction. We find that a venue's impact score negatively correlates with number of Likert-related errors and acceptance rate, and total number of papers accepted per venue positively correlates with the number of errors. We also find statistically significant differences between venues for the frequency of misnomer and design errors. Our analysis suggests there are areas for meaningful improvement in the design and testing of Likert scales. Based on our findings, we provide guidelines and a tutorial for researchers for developing and analyzing Likert scales and associated data. We also detail a list of recommendations to improve the accuracy of conclusions drawn from Likert data.  © 2023 Association for Computing Machinery.",Additional Key Words and PhrasesMetrics for HRI; Likert scales; statistical practices,Errors; Machine design; Man machine systems; 'current; Additional key word and phrasesmetric for human-robot interaction; Best practices; Design and analysis; Humans-robot interactions; Key words; Likert data; Likert scale; Statistical practice; Statistical testing; Human robot interaction
What Can a Robot's Skin Be? Designing Texture-changing Skin for Human-Robot Social Interaction,2023,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164243232&doi=10.1145%2f3532772&partnerID=40&md5=2d6dcc669eb432526933d8c8c28e4de1,"Biological skin has numerous functions like protection, sensing, expression, and regulation. On the contrary, a robot's skin is usually regarded as a passive and static separation between the body and environment. In this article, we explore the design opportunities of a robot's skin as a socially expressive medium. Inspired by living organisms, we discuss the roles of interactive robotic skin from four perspectives: expression, perception, regulation, and mechanical action. We focus on the expressive function of skin to sketch design concepts and present a flexible technical method for embodiment. The proposed method integrates pneumatically actuated dynamic textures on soft skin, with forms and kinematic patterns generating a variety of visual and haptic expressions. We demonstrate the proposed design space with six texture-changing skin prototypes and discuss their expressive capacities.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesRobot skin; bio-inspired design; human-robot interaction; pneumatics; soft robots; texture change,Human robot interaction; Man machine systems; Textures; Additional key word and phrasesrobot skin; Bio-inspired designs; Human robots; Humans-robot interactions; Key words; Living organisms; Robot skin; Social interactions; Soft robot; Texture change; Machine design
15 Years of (Who)man Robot Interaction: Reviewing the H in Human-Robot Interaction,2023,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163177354&doi=10.1145%2f3571718&partnerID=40&md5=351cb67109cbfd029b7cefa891aba879,"Recent work identified a concerning trend of disproportional gender representation in research participants in Human-Computer Interaction (HCI). Motivated by the fact that Human-Robot Interaction (HRI) shares many participant practices with HCI, we explored whether this trend is mirrored in our field. By producing a dataset covering participant gender representation in all 684 full papers published at the HRI conference from 2006-2021, we identify current trends in HRI research participation. We find an over-representation of men in research participants to date, as well as inconsistent and/or incomplete gender reporting, which typically engages in a binary treatment of gender at odds with published best practice guidelines. We further examine if and how participant gender has been considered in user studies to date, in-line with current discourse surrounding the importance and/or potential risks of gender based analyses. Finally, we complement this with a survey of HRI researchers to examine correlations between who is doing with the who is taking part, to further reflect on factors which seemingly influence gender bias in research participation across different sub-fields of HRI. Through our analysis, we identify areas for improvement, but also reason for optimism, and derive some practical suggestions for HRI researchers going forward.  © 2023 Copyright held by the owner/author(s).",Additional Key Words and PhrasesGender; inclusivity; participant recruitment; systematic review; user study methodologies,Human computer interaction; Human engineering; Man machine systems; 'current; Additional key word and phrasesgender; Humans-robot interactions; Inclusivity; Key words; Participant recruitment; Research participation; Systematic Review; User study; User study methodology; Human robot interaction
"Judging a Socially Assistive Robot by Its Cover: The Effect of Body Structure, Outline, and Color on Users' Perception",2023,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164235972&doi=10.1145%2f3571717&partnerID=40&md5=f48c47362bec63e709fb2e9669fd09ea,"Socially assistive robots (SARs) aim to provide assistance through social interaction. Previous studies contributed to understanding users' perceptions and preferences regarding existing commercially available SARs. Yet very few studies regarding SARs' appearance used designated SAR designs, and even fewer evaluated isolated visual qualities (VQs). In this work, we aim to assess the effect of isolated VQs systematically. To achieve this, we first conducted market survey and deconstructed the VQs attributed to SARs. Then, a reconstruction of body structure, outline, and color scheme was done, resulting in the creation of 30 new SAR models that differ in their VQs, allowing us to isolate one character at a time. We used these new designs to evaluate users' preferences and perceptions in two empirical studies. Our empirical findings link VQs with perceptions of SAR characteristics. These can lead to forming guidelines for the industrial design processes of new SARs to match user expectations.  © 2023 Copyright held by the owner/author(s).",Additional Key Words and PhrasesSocially assistive robots; design; taxonomy; user experience; visual qualities,Machine design; Quality control; Robots; Structural design; User interfaces; Additional key word and phrasessocially assistive robot; Assistive robots; Body structure; Key words; Social interactions; Socially assistive robots; User perceptions; User's preferences; Users' experiences; Visual qualities; Product design
Perception-Motion Coupling in Active Telepresence: Human Behavior and Teleoperation Interface Design,2023,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164238055&doi=10.1145%2f3571599&partnerID=40&md5=c57aa136d32f7f1ae64056b80de7672b,"Teleoperation enables complex robot platforms to perform tasks beyond the scope of the current state-of-the-art robot autonomy by imparting human intelligence and critical thinking to these operations. For seamless control of robot platforms, it is essential to facilitate optimal situational awareness of the workspace for the operator through active telepresence cameras. However, the control of these active telepresence cameras adds an additional degree of complexity to the task of teleoperation. In this paper we present our results from the user study that investigates: (1) how the teleoperator learns or adapts to performing the tasks via active cameras modeled after camera placements on the TRINA humanoid robot; (2) the perception-action coupling operators implement to control active telepresence cameras, and (3) the camera preferences for performing the tasks. These findings from the human motion analysis and post-study survey will help us determine desired design features for robot teleoperation interfaces and assistive autonomy.  © 2023 Copyright held by the owner/author(s).",active telepresence; Additional Key Words and PhrasesPerception-action coupling; robot teleoperation,Anthropomorphic robots; Behavioral research; Control theory; Human robot interaction; Intelligent robots; Machine design; Remote control; Visual communication; Visual servoing; Active telepresence; Additional key word and phrasesperception-action coupling; Human behaviors; Key words; Motion Coupling; Perception motion; Robot platform; Robot teleoperation; Teleoperation interface; Telepresence; Cameras
Comparing Norm-Based and Role-Based Strategies for Robot Communication of Role-Grounded Moral Norms,2023,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164234919&doi=10.1145%2f3571719&partnerID=40&md5=52bda029030706a8a868e1a837444b2f,"Because robots are perceived as moral agents, they must behave in accordance with human systems of morality. This responsibility is especially acute for language-capable robots because moral communication is a method for building moral ecosystems. Language capable robots must not only make sure that what they say adheres to moral norms; they must also actively engage in moral communication to regulate and encourage human compliance with those norms. In this work, we describe four experiments (total N=316) across which we systematically evaluate two different moral communication strategies that robots could use to influence human behavior: a norm-based strategy grounded in deontological ethics, and a role-based strategy grounded in role ethics. Specifically, we assess the effectiveness of robots that use these two strategies to encourage human compliance with norms grounded in expectations of behavior associated with certain social roles. Our results suggest two major findings, demonstrating the importance of moral reflection and moral practice for effective moral communication: First, opportunities for reflection on ethical principles may increase the efficacy of robots' role-based moral language; and second, following robots' moral language with opportunities for moral practice may facilitate role-based moral cultivation.  © 2023 Copyright held by the owner/author(s).",Additional Key Words and PhrasesHuman-robot interaction; moral communication; role ethics,Ethical technology; Additional key word and phraseshuman-robot interaction; Communication strategy; Human-systems; Key words; Moral agents; Moral communication; Robot communication; Robot interactions; Role ethic; Role-based; Behavioral research
Affective Robots Need Therapy,2023,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164234792&doi=10.1145%2f3543514&partnerID=40&md5=24eb3c93aa31e598c74d262d5ae0cd47,"Emotion researchers have begun to converge on the theory that emotions are psychologically and socially constructed. A common assumption in affective robotics is that emotions are categorical brain-body states that can be confidently modeled. But if emotions are constructed, then they are interpretive, ambiguous, and specific to an individual's unique experience. Constructivist views of emotion pose several challenges to affective robotics: first, it calls into question the validity of attempting to obtain objective measures of emotion through rating scales or biometrics. Second, ambiguous subjective data poses a challenge to computational systems that need structured and definite data to operate. How can a constructivist view of emotion be rectified with these challenges?In this article, we look to psychotherapy for ontological, epistemic, and methodological guidance. These fields (1) already understand emotions to be intrinsically embodied, relative, and metaphorical and (2) have built up substantial knowledge informed by everyday practice. It is our hope that by using interpretive methods inspired by therapeutic approaches, HRI researchers will be able to focus on the practicalities of designing effective embodied emotional interactions.  © 2023 Copyright held by the owner/author(s).",Additional Key Words and PhrasesEmotion; affective; constructivism; embodied; therapeutic care,Computation theory; Additional key word and phrasesemotion; Affective; Computational system; Constructivism; Embodied; Emotional interactions; Key words; Objective measure; Rating scale; Therapeutic care; Robotics
Nonverbal Cues in Human-Robot Interaction: A Communication Studies Perspective,2023,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151247235&doi=10.1145%2f3570169&partnerID=40&md5=ae0da2a0f365f0846562edb092893531,"Communication between people is characterized by a broad range of nonverbal cues. Transferring these cues into the design of robots and other artificial agents that interact with people may foster more natural, inviting, and accessible experiences. In this article, we offer a series of definitive nonverbal codes for human-robot interaction (HRI) that address the five human sensory systems (visual, auditory, haptic, olfactory, and gustatory) drawn from the field of communication studies. We discuss how these codes can be translated into design patterns for HRI using a curated sample of the communication studies and HRI literatures. As nonverbal codes are an essential mode in human communication, we argue that integrating robotic nonverbal codes in HRI will afford robots a feeling of ""aliveness""or ""social agency""that would otherwise be missing. We end with suggestions for research directions to stimulate work on nonverbal communication within the field of HRI and improve communication between people and robots.  © 2023 Association for Computing Machinery.",Additional Key Words and PhrasesRobotics; communication studies; human-robot interaction; nonverbal codes; nonverbal communication,Machine design; Man machine systems; Additional key word and phrasesrobotic; Artificial agents; Communication study; Human sensory system; Humans-robot interactions; Key words; Non-verbal communications; Nonverbal code; Nonverbal cues; Nonverbals; Human robot interaction
Inviting Robot Touch (By Design),2023,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164241474&doi=10.1145%2f3549533&partnerID=40&md5=4fa5077e865b1ec520795d7f88b749d0,"What is the role of touch in inviting social interaction with robots? Forms of functional haptics in collaboration and socially assistive robots, for example, indicate one pathway. But what of more naturalistic and affective forms of touch that are more inviting, that encourage pro-social behaviours? This is a tale of three loops. First, the haptic feedback loop, where human-human touch still remains underexplored, and where human-machine touch is produced through mechanical engineering as ""force display""and perceived by the user as tactile. Second, the affective feedback loop, courtesy of Höök and Dumouchel and Damiano, where technical systems influence, and are influenced by, a human user corporeally. Bringing these loops together encourages interaction design to consider how touch and affect may more effectively invite a range of users to interact with social robots, and their role in the perception of Artificial Empathy.  © 2023 Association for Computing Machinery.",Additional Key Words and PhrasesHaptics; affect; embodiment; non-verbal communication; social robotics; touch,Economic and social effects; Feedback; Robots; Additional key word and phraseshaptic; Affect; Embodiment; Feedback loops; Haptics; Key words; Non-verbal communications; Social interactions; Social robotics; Touch; Machine design
Towards a Soft Science of Soft Robots. A Call for a Place for Aesthetics in Soft Robotics Research,2023,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147219313&doi=10.1145%2f3533681&partnerID=40&md5=6ba1c9f55f3433c0d16ba47a1cd334e0,The position paper presents an argument that aesthetic theory and practice are pertinent to the rapidly expanding field of soft robotics. Soft robotics as an aesthetic phenomenon is introduced and contextualized drawing on the author's own research and practice. The potential of aesthetic perspectives and approaches in developing and implementing soft robotics are highlighted with a focus on technical design and human-robot interaction.  © 2023 Copyright held by the owner/author(s).,Additional Key Words and PhrasesSoft robotics; aesthetics; human-robot interaction; robotic art,Machine design; Man machine systems; Additional key word and phrasessoft robotic; Esthetic; Humans-robot interactions; Key words; Position papers; Robotic arts; Robotics research; Soft robot; Soft robotics; Soft science; Human robot interaction
In the Arms of a Robot: Designing Autonomous Hugging Robots with Intra-Hug Gestures,2023,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164244102&doi=10.1145%2f3526110&partnerID=40&md5=52481a6182afceb7810370124e92ac52,"Hugs are complex affective interactions that often include gestures like squeezes. We present six new guidelines for designing interactive hugging robots, which we validate through two studies with our custom robot. To achieve autonomy, we investigated robot responses to four human intra-hug gestures: holding, rubbing, patting, and squeezing. A Total of 32 users each exchanged and rated 16 hugs with an experimenter-controlled HuggieBot 2.0. The robot's inflated torso's microphone and pressure sensor collected data of the subjects' demonstrations that were used to develop a perceptual algorithm that classifies user actions with 88% accuracy. Users enjoyed robot squeezes, regardless of their performed action, they valued variety in the robot response, and they appreciated robot-initiated intra-hug gestures. From average user ratings, we created a probabilistic behavior algorithm that chooses robot responses in real time. We implemented improvements to the robot platform to create HuggieBot 3.0 and then validated its gesture perception system and behavior algorithm with 16 users. The robot's responses and proactive gestures were greatly enjoyed. Users found the robot more natural, enjoyable, and intelligent in the last phase of the experiment than in the first. After the study, they felt more understood by the robot and thought robots were nicer to hug.  © 2023 Copyright held by the owner/author(s).",Additional Key Words and PhrasesSocial-physical human-robot interaction; behavioral algorithm; haptic sensing; user study,Behavioral research; Human robot interaction; Additional key word and phrasessocial-physical human-robot interaction; Affective interaction; Behavioral algorithms; Haptic sensing; Key words; Physical humanrobot interaction (phri); Probabilistic behavior; User action; User rating; User study; Intelligent robots
Embodied Expressive Gestures in Telerobots: A Tale of Two Users,2023,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164236245&doi=10.1145%2f3570908&partnerID=40&md5=3908ad2d313e75eb1e5f8f525b2b7a9e,"Despite their technical advancements, commercially available telerobots are limited in social interaction capabilities for both pilot and local users, specifically in nonverbal communication. Our group hypothesizes that the introduction of expressive gesturing and tangible interaction capabilities (e.g., handshakes, fist bumps) will enhance telerobotic interactions and increase social connection between users. To investigate the affordances to social connection that gestures and tangible interactions provide in telerobot-mediated interactions, we designed and integrated a lightweight manipulator terminating in an anthropomorphic end effector onto a commercially available telerobot (Anybots QB 2.0). Through virtual reality tracking of the pilot user's arm and hand, expressive gestures and social contact interactions are recreated via the manipulator, enabling a pilot user and a local user to engage in a tangible exchange. To assess the usability and effectiveness of the gesturing system, we present evaluations from both the local and pilot user perspectives. First, we present a validation study to assess usability of the control system by the pilot user. Our results demonstrate that pilot user interactions can be replicated with a greater than 80% pass rate and mean ease of use rating of (out of 10) with brief training. Finally, we present a user study to assess the social impacts of (1) using the telerobot without the manipulator from both the pilot user and local user perspectives and (2) using the control system and telerobotic manipulator from both the pilot user and local user perspectives. Results demonstrate that the robot with the manipulator elicited a more positive social experience than the robot without the arm for local users but no significant difference in conditions for pilot users. Future work will focus on improving the pilot user experience to support social contact interactions.  © 2023 Copyright held by the owner/author(s).",Additional Key Words and PhrasesTelerobotics; expressive gestures; human-robot interaction; mobile remote presence; robot design; social connection; tangible interactions,Control systems; Economic and social effects; Industrial robots; Machine design; Man machine systems; Manipulators; User interfaces; Virtual reality; Additional key word and phrasestelerobotic; Expressive gestures; Humans-robot interactions; Key words; Mobile remote presence; Robot designs; Social connection; Tangible interaction; Telerobot; Users perspective; Human robot interaction
Multiple Roles of Multimodality Among Interacting Agents,2023,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164237637&doi=10.1145%2f3549955&partnerID=40&md5=87f11404a7bda092278636d97ce89173,"The term multimodality has come to take on several somewhat different meanings depending on the underlying theoretical paradigms and traditions along with the purpose and context of use. The term is closely related to embodiment, which, in turn, is also used in several different ways. In this article, we elaborate on this connection and propose that a pragmatic and pluralistic stance is appropriate for multimodality. We further propose a distinction between first- and second-order effects of multimodality - what is achieved by multiple modalities in isolation and the opportunities that emerge when several modalities are entangled. This highlights questions regarding ways to cluster or interchange different modalities, for example, through redundancy or degeneracy. Apart from discussing multimodality with respect to an individual agent, we further look to more distributed agents and situations in which social aspects become relevant. In robotics, understanding the various uses and interpretations of these terms can prevent miscommunication when designing robots as well as increase awareness of the underlying theoretical concepts. Given the complexity of the different ways in which multimodality is relevant in social robotics, this can provide the basis for negotiating appropriate meanings of the term on a case-by-case basis.  © 2023 Copyright held by the owner/author(s).",Additional Key Words and PhrasesMultimodality; embodiment; robotics; sensors,Social aspects; Additional key word and phrasesmultimodality; Context of use; Embodiment; First order effect; Interacting agents; Key words; Multi-modality; Multiple modalities; Multiples roles; Second order effect; Robotics
Communicating Missing Causal Information to Explain a Robot's Past Behavior,2023,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149028005&doi=10.1145%2f3568024&partnerID=40&md5=8780b009f926a19900e5ef6a9b176f39,"Robots need to explain their behavior to gain trust. Existing research has focused on explaining a robot's current behavior, yet it remains unknown yet challenging how to provide explanations of past actions in an environment that might change after a robot's actions, leading to critical missing causal information due to moved objects.We conducted an experiment (N = 665) investigating how a robot could help participants infer the missing causal information by replaying the past behavior physically, using verbal explanations, and projecting visual information onto the environment. Participants watched videos of the robot replaying its completion of an integrated mobile kitting task. During the replay, the objects are already gone, so participants needed to infer where an object was picked, where a ground obstacle had been, and where the object was placed.Based on the results, we recommend combining physical replay with speech and projection indicators (Replay-Project-Say) to help infer all the missing causal information (picking, navigation, and placement) from the robot's past actions. This condition had the best outcome in both task-based - effectiveness, efficiency, and confidence - and team-based metrics - workload and trust. If one's focus is efficiency, then we recommend projection markers for navigation inferences and verbal markers for placing inferences. © 2023 Copyright held by the owner/author(s).",behavior explanation; Robot explanation; system transparency,Robots; Behavior explanation; Condition; Current behaviors; Kitting; Robot actions; Robot explanation; System transparency; Task-based; Visual information; Efficiency
Designing Robots for Aging: Wisdom as a Critical Lens,2023,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149020994&doi=10.1145%2f3549531&partnerID=40&md5=cbc7fecd39b028e8528abd2f63c98bdf,"Although the concept of wisdom is ancient, empirical research on it has only recently received attention in gerontology. This coincides with a critical turn away from a deficit model of aging, viewing aging as a series of losses, toward a more supportive and developmental model. This article draws on this recent work to consider how wisdom can be a critical lens for human-robot interaction (HRI) researchers and other technology design researchers to pay more attention to the coping strategies that older adults accumulated throughout their lives. We engaged in a 6-month collaborative design process with community-dwelling older adults. The contributions of this article are twofold. First, we found that wisdom as a design concept helps researchers to critically examine how they define knowledge. Wisdom as an accumulation of experiential knowledge of older adults helps researchers rethink the definition of knowledge-valuing computational and technological knowledge-in the field of HRI. Second, wisdom leads researchers to the past experiences of older adults. Although past experiences are as important as current experiences, they are not actively considered in robot design studies for older adults. We hope wisdom as a critical lens could allow researchers to integrate the invisible aspects of older adults' aging experiences into the existing practices of designing robots for aging users. © 2023 Association for Computing Machinery.",Additional Key Words and PhrasesHuman-robot interaction; assistive robots; critical design; older adults; robots; robots for aging; wisdom,Human robot interaction; Additional key word and phraseshuman-robot interaction; Assistive robots; Critical design; Empirical research; Humans-robot interactions; Key words; Older adults; Robot for aging; Robot interactions; Wisdom; Machine design
Best of Both Worlds? Combining Different Forms of Mixed Reality Deictic Gestures,2023,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149024538&doi=10.1145%2f3563387&partnerID=40&md5=6b89b513d6e6ae731f8f219001b8636b,"Mixed Reality provides a powerful medium for transparent and effective human-robot communication, especially for robots with significant physical limitations (e.g., those without arms). To enhance nonverbal capabilities for armless robots, this article presents two studies that explore two different categories of mixed reality deictic gestures for armless robots: A virtual arrow positioned over a target referent (a non-ego-sensitive allocentric gesture) and a virtual arm positioned over the gesturing robot (an ego-sensitive allocentric gesture). In Study 1, we explore the tradeoffs between these two types of gestures with respect to both objective performance and subjective social perceptions. Our results show fundamentally different task-oriented versus social benefits, with non-ego-sensitive allocentric gestures enabling faster reaction time and higher accuracy, but ego-sensitive gestures enabling higher perceived social presence, anthropomorphism, and likability. In Study 2, we refine our design recommendations by showing that in fact these different gestures should not be viewed as mutually exclusive alternatives, and that by using them together, robots can achieve both task-oriented and social benefits. © 2023 Copyright held by the owner/author(s).",Anthropomorphism; Augmented Reality (AR); Deictic gesture; Human-Robot Interaction (HRI); Mixed Reality (MR); nonverbal communication; Social presence,Economic and social effects; Human robot interaction; Machine design; Mixed reality; Anthropomorphism; Augmented reality; Deictic gesture; Human-robot interaction; Humans-robot interactions; Mixed reality; Non-verbal communications; Social presence; Task-oriented; Augmented reality
Robotic Vision for Human-Robot Interaction and Collaboration: A Survey and Systematic Review,2023,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147208152&doi=10.1145%2f3570731&partnerID=40&md5=9707c9de848b7ec18098ebcd22411c74,"Robotic vision, otherwise known as computer vision for robots, is a critical process for robots to collect and interpret detailed information related to human actions, goals, and preferences, enabling robots to provide more useful services to people. This survey and systematic review presents a comprehensive analysis on robotic vision in human-robot interaction and collaboration (HRI/C) over the past 10 years. From a detailed search of 3,850 articles, systematic extraction and evaluation was used to identify and explore 310 papers in depth. These papers described robots with some level of autonomy using robotic vision for locomotion, manipulation, and/or visual communication to collaborate or interact with people. This article provides an in-depth analysis of current trends, common domains, methods and procedures, technical processes, datasets and models, experimental testing, sample populations, performance metrics, and future challenges. Robotic vision was often used in action and gesture recognition, robot movement in human spaces, object handover and collaborative actions, social communication, and learning from demonstration. Few high-impact and novel techniques from the computer vision field had been translated into HRI/C. Overall, notable advancements have been made on how to develop and deploy robots to assist people. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",collaborative actions; computer vision; gesture recognition; human-robot interaction; learning from demonstration; object handover; robot movement in human spaces; Robotic vision; social communication,Gesture recognition; Man machine systems; Robot vision; Visual communication; Visual servoing; Collaborative action; Gestures recognition; Hand over; Humans-robot interactions; Learning from demonstration; Object handover; Robot movement in human space; Robot movements; Robotic vision; Social communications; Human robot interaction
The Effects of Healthcare Robot Empathy Statements and Head Nodding on Trust and Satisfaction: A Video Study,2023,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149021531&doi=10.1145%2f3549534&partnerID=40&md5=a9396082f8be6613ff93e514f466c6d0,"Clinical empathy has been associated with many positive outcomes, including patient trust and satisfaction. Physicians can demonstrate clinical empathy through verbal statements and non-verbal behaviors, such as head nodding. The use of verbal and non-verbal empathy behaviors by healthcare robots may also positively affect patient outcomes. The current study examined whether the use of robot verbal empathy statements and head nodding during a video recorded interaction between a healthcare robot and patient improved participant trust and satisfaction. One hundred participants took part in the experiment, online through Amazon Mechanical Turk. They were randoimnized to watch one of four videos depicting an interaction with a 'patient' and a Nao robot that (1) either made empathetic or neutral statements, and (2) either nodded its head when listening to the patient or did not. Results showed that the use of empathetic statements by the healthcare robot significantly increased participant perceptions of robot empathy, trust and satisfaction, and reduced robot distrust. No significant findings were revealed in relation to robot head nodding. The positive effects of empathy statements support the model of Robot-Patient Communication, which theorizes that robot use of recommended clinical empathy behaviors can improve patient outcomes. The effects of healthcare robot nodding behavior needs to be further investigated. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Empathy; Healthcare robot; Human-robot inter-action; Robot; Social interaction; Trust,Health care; 'current; Amazon's mechanical turks; Empathy; Head nodding; Healthcare robot; Non-verbal behaviours; Robot head; Social interactions; Trust; Human robot interaction
Bonding with a Couchsurfing Robot: The Impact of Common Locus on Human-Robot Bonding In-the-Wild,2023,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149032648&doi=10.1145%2f3563702&partnerID=40&md5=af3bcbf520ae97628de7b763f5007cec,"Due to an increased presence of robots in human-inhabited environments, we observe a growing body of examples in which humans show behavior that is indicative of strong social engagement towards robots that do not possess any life-like realism in appearance or behavior. In response, we focus on the under-explored concept of a common locus as a relevant driver for a robot passing a social threshold. The key principle of common locus is that sharing place and time with a robotic artifact functions as an important catalyst for a perception of shared experiences, which in turn leads to bonding. We present BlockBots, minimal cube-shaped robotic artifacts that are deployed in an unsupervised, open-ended and in-the-field experimental setting aimed to explore the relevance of this concept. Participants host the BlockBot in their domestic environment before passing it on, without necessarily knowing they are taking part in an experiment. Qualitative data suggest that participants make identity and mind attributions to the BlockBot. People that actively maintain a common locus with BlockBot by taking it with them when changing location, on trips and during outdoor activities, project more of these attributes than others. © 2023 Copyright held by the owner/author(s).",abstract robots; bonding; common locus; Human-robot interaction; human-robot relationships; in-the-wild study; qualitative study,Man machine systems; Abstract robot; Bonding; Common locus; Growing bodies; Human robots; Human-robot relationship; Humans-robot interactions; In-the-wild study; Qualitative study; Social engagement; Human robot interaction
Swarm Control for Distributed Construction: A Computational Complexity Perspective,2023,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149023265&doi=10.1145%2f3555078&partnerID=40&md5=f3aec4ddb1d8dd6ff6fc9b144c1a909f,"Over the last 20 years, human interaction with robot swarms has been investigated as a means to mitigate problems associated with the control and coordination of such swarms by either human teleoperation or completely autonomous swarms. Ongoing research seeks to characterize those situations in which such interaction is both viable and preferable. In this article, we contribute to this effort by giving the first computational complexity analyses of problems associated with algorithm, environmental influence, and leader selection methods for the control of swarms performing distributed construction tasks. These analyses are done relative to a simple model in which swarms of deterministic finite-state robots operate in a synchronous error-free manner in 2D grid-based environments. We show that all three of our problems are polynomial-Time intractable in general and remain intractable under a number of plausible restrictions (both individually and in many combinations) on robot controllers, environments, target structures, and sequences of swarm control commands. We also give the first restrictions relative to which these problems are tractable, as well as discussions of the implications of our results for both the design and deployment of swarm control assistance software tools and the human control of swarms. © 2023 Association for Computing Machinery.",Computational complexity; Construction; Human-robot interaction; Swarm robotics,Human robot interaction; Man machine systems; Polynomial approximation; Swarm intelligence; Autonomous swarms; Computational complexity analysis; Deterministics; Environmental influences; Humaninteraction; Humans-robot interactions; Robot swarms; Selection methods; Simple modeling; Swarm robotics; Computational complexity
Embodiment Matters in Social HRI Research: Effectiveness of Anthropomorphism on Subjective and Objective Outcomes,2023,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140894838&doi=10.1145%2f3555812&partnerID=40&md5=ffa050d8c489efb9e8bbc4887a07bd50,"Anthropomorphism, as a design feature of robots, is widely applied to enhance human-robot interaction in the social domain. Current knowledge of how anthropomorphism influences perception, attitudes, and actual behavior is gained via research with depicted and embodied robots. However, it remains unclear how comparable gained insights of both approaches are. Results of a current meta-Analysis suggest that anthropomorphism positively influences subjective and objective measures in case of embodied robots, whereas in case of depicted robots predominantly effects on subjective measures seem to emerge. This follow-up analysis aims to further investigate this difference by using a recoded dataset including data of 41 studies, involving over 3,000 participants. The results illustrate that anthropomorphism investigated via embodied robots indeed facilitates both subjective and objective outcomes. Remarkably, studies concerning effects of anthropomorphism using depicted robots showed positive effects on a subjective level but failed to show any effects on an objective level. In conclusion, the results show that the consequences of anthropomorphism in human-robot interaction depend on how robots are presented to participants. Moreover, they reveal that the transfer of results gained via depicted robots to embodied human-robot interaction might lead to both overestimation on a subjective level and underestimation on an objective level regarding the consequences of anthropomorphism. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Anthropomorphism; Embodiment; Human-robot interaction; Social robots,Economic and social effects; Machine design; Man machine systems; 'current; Anthropomorphism; Design features; Embodiment; Follow up; Humans-robot interactions; Meta-analysis; Social domains; Social robots; Subjective and objective measures; Human robot interaction
Using User-Generated YouTube Videos to Understand Unguided Interactions with Robots in Public Places,2023,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149013429&doi=10.1145%2f3550280&partnerID=40&md5=eef1efe708a9f106acd88492cea87620,"Professional service robots are increasingly being deployed in public places, which thus increases user exposure. However, we lack an empirical understanding of complex encounters taking place in dynamic and often crowded environments as well as how people overcome breakdowns during unguided interaction with a robot in a real-world scenario. In this paper, we conducted a covert, digital ethnographic study analyzing 104 user-generated YouTube videos focusing on people's unguided interactions with robots in several public places. We identified several types of interaction breakdowns pertaining to someone (person-initiated interaction breakdown, IB) or something (environmental disturbances, ED) having a direct, negative effect on an ongoing unguided interaction. Our findings have implications for the design and development of service robots facing multi-user scenarios entertaining active (primary and secondary) users, inactive (commentators and observers) g'users', and Incidentally Co-present Persons (InCoPs). Furthermore, we contribute to and built on the limited prior use of YouTube videos and digital ethnography in HRI research, thereby demonstrating its effectiveness in studying unguided interactions in public places, while supplementing and adding to the existing knowledge base of service robots in public places. © 2023 Association for Computing Machinery.",Digital ethnography; Interaction breakdowns; Robots in public places; Service robots; Unguided interactions,Knowledge based systems; Mobile robots; Digital ethnography; Interaction breakdown; Professional services; Public places; Real-world scenario; Robot in public place; Service robots; Unguided interaction; User-generated; YouTube; Machine design
The Design and Observed Effects of Robot-performed Manual Gestures: A Systematic Review,2023,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140846147&doi=10.1145%2f3549530&partnerID=40&md5=197634fa0c21f1268fb34d82b96ababf,"Communication using manual (hand) gestures is considered a defining property of social robots, and their physical embodiment and presence, therefore, we see a need for a comprehensive overview of the state-of-the-art in social robots that use gestures. This systematic literature review aims to address this need by (1) describing the gesture production process of a social robot, including the design and planning steps, and (2) providing a survey of the effects of robot-performed gestures on human-robot interactions in a multitude of domains. We identify patterns and themes from the existing body of literature, resulting in nine outstanding questions for research on robot-performed gestures regarding: developments in sensor technology and AI, structuring the gesture design and evaluation process, the relationship between physical appearance and gestures, the effects of planning on the overall interaction, standardizing measurements of gesture ""quality,""individual differences, gesture mirroring, whether human-likeness is desirable, and universal accessibility of robots. We also reflect on current methodological practices in studies of robot-performed gestures and suggest improvements regarding replicability, external validity, measurement instruments used, and connections with other disciplines. These outstanding questions and methodological suggestions can guide future work in this field of research.  Copyright © 2023 the owner/author(s).",hand gestures; literature review; Social robot,Machine design; Man machine systems; Robot programming; Design and planning; Hand gesture; Literature reviews; Performed gestures; Production process; Property; Social robots; State of the art; Systematic literature review; Systematic Review; Human robot interaction
"Socially Assistive Robots for Parkinson's Disease: Needs, Attitudes and Specific Applications as Identified by Healthcare Professionals",2023,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149002879&doi=10.1145%2f3570168&partnerID=40&md5=e9d5acddb3e417b658b8835384796cdb,"To explore how socially assistive robots (SARs) may assist the specific needs of individuals with Parkinson's disease (IwPD), we conducted three focus groups with 12 clinicians who treat IwPD. We present a thematic analysis of their perceptions of the needs of the patients, and their own expectations, perceived advantages, disadvantages and concerns regarding the use of SARs for IwPD. Clinicians were positive towards using SARs for IwPD, if used in the patient's home, for motor, communication, emotional, and cognitive needs, especially for practice and for help with activities of daily living. They were concerned that a SAR might be used to replace clinicians' work, and stressed it should only augment the clinicians' work. They thought a SAR may relieve some of the burden experienced by informal caregivers, and identified specific applications for SARs for PD. We asked 18 stakeholders (nine IwPD, nine family members) to rate their level of agreement with the clinicians' statements. The greatest divergence between their views and those of the clinicians was on the topic of using a SAR as a companion, or as a feeding assistant, to which they objected. This work may be used as a basis for future studies designing SARs for IwPD. © 2023 Copyright held by the owner/author(s).",care robots; co-design; focus groups; inclusive design; Participatory design; qualitative methods; socially assistive robots,Machine design; Neurodegenerative diseases; Care robot; Co-designs; Focus groups; Health care professionals; Inclusive design; Parkinson's disease; Participatory design; Qualitative method; Socially assistive robots; Thematic analysis; Robots
Integrating Robot Manufacturer Perspectives into Legible Factory Robot Light Communications,2023,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149017305&doi=10.1145%2f3570732&partnerID=40&md5=3566cc2090ce5c6e3617511585a5c254,"In a world with increasing numbers of robots operating in everyday human spaces, the employees at this robotics company are pioneers, with intelligent point-to-point path planning and autonomous transport operations in 150+ factory and warehouse locations in North America. At the time of research, this robotics company consisted of 250 employees. Unlike other industry models, their robots are designed to operate with people in mixed human-machine spaces, yet no HRI style evaluations had previously been run with their robots. As early observers of how factory workers and transport robot interact, across varied job roles ranging from technology design to customer relations, this work sought to leverage employee knowledge and experiences to identify opportunities for improving the communication capabilities of the robots, resulting in the addition of several robot state communications to their initial software set leveraging both employee- and social robotics literature- sourced ideas for communicating with lights. To achieve this a social robotics researcher spent a summer onsite at the robotics company, getting to know their software stack and culture. Her research activities included: (1) a company-wide survey relative to the robot's light, sound, and motion communications was sent out and analyzed, (2) the development of three new light sets (car-like, sweeping, heartbeat) and five overall states (blocked, at goal, turning, idle), and (3) a user study evaluating the developed light sets relative to the current robot default light patterns, all significantly improving the overall legibility of the targeted robot state communications: at goal, blocked, turning, and idle. Our initial findings advance knowledge in which style of light patterns is best for different communication states, showing that eye-catching lights are best for high urgency states, such as blocked, and subtle lights are best for low urgency states, such as idle. Finally, the latest software release for this robot has deployed a subset of these light patterns to all of their currently operating client sites, i.e., anyone who updates their robots to the latest release will benefit from these research results. This deployment sets the ground for future researchers exploring how end-users at different sites have responded to the new, more communicative light patterns.  © 2023 Association for Computing Machinery.",expressive lights; Factory robotics; social robotics,Human robot interaction; Intelligent robots; Knowledge management; Machine design; Personnel; Public relations; Robot programming; Expressive light; Factory robotic; Human-machine; Light patterns; Robot manufacturers; Social robotics; Technology designs; Transport operations; Warehouse location; Workers'; Motion planning
Not All Robots are Evaluated Equally: The Impact of Morphological Features on Robots' Assessment through Capability Attributions,2023,ACM Transactions on Human-Robot Interaction,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144648377&doi=10.1145%2f3549532&partnerID=40&md5=63211e6d9112ed1c357508133b9eabb1,"Favorable assessments of social robots are addressed in several research and development attempts because positive attitudes and intentions towards technology are regarded as a necessary prerequisite for usage. To predict a favorable evaluation, it is inevitable to understand the appraisal process and determine crucial variables that affect the evaluative and behavioral consequences of HRI. Robotic morphology has been identified as one of these variables. In the present work, we expand previous work by demonstrating that capability attributions associated with robots' morphological features explain variations in evaluations. Based on two large picture-based online studies (Study 1, n = 673; Study 2, n = 586) we show that robots with similar morphological features (e.g., robots with arms and grippers) can be clustered along their assigned capabilities, and that these capabilities (e.g., to manipulate objects) explain evaluations of the robots in terms of acceptance and social attributes (i.e., warmth, competence, and discomfort). We discuss whether these initial assessments are relevant to live interactions and how our results can inform robot design. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Acceptance; Capability attributions; Human-robot interaction; Morphology,Human robot interaction; Machine design; Acceptance; Capability attribution; Humans-robot interactions; Initial assessment; Morphological features; Online studies; Positive attitude; Research and development; Social attributes; Social robots; Morphology
