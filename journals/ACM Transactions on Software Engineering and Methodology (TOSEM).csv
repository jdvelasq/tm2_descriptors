Title,Year,Source title,Link,Abstract,Author Keywords,Index Keywords
A Time-Sensitive Object Model for Real-Time Systems,1995,ACM Transactions on Software Engineering and Methodology (TOSEM),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0029333053&doi=10.1145%2f214013.214021&partnerID=40&md5=c832003c4e9502710d7d337b62f5811a,"Process-oriented models for real-time systems focus on the timing constraints of processes, a focus that can adversely affect resulting designs. Data dependencies between processes create scheduling interactions that limit the times at which processes may execute Processes are then designed to fit available windows in the overall system schedule. “Fitting in” frequently involves fragmenting processes to fit scheduhng windows and/or designing program and data structures for speed rather than for program comprehension. The result is often a system with very sensitive timing that is hard to understand and maintain. As an alternative to process-oriented design. we present time-sensitive objects: a data-oriented model for real-time systems. The time-sensitive object (TSO) model structures systems as time-constrained data, rather than time-ccmstrained processing Object values are extended to object histories in which a sequence of time-constrained values describe the evolution of the object over time. Systems comprise a set of objects and theu- dependencies. The TSO model describes the effects of object operations and the propagation of change among related objects. Periodic objects, a class of objects within the TSO model, are described in detail in this article and compared with traditional periodic processes. Advantages of time-sensitive objects are identified, including greater scheduling independence when processes have data dependencies, more opportumty for concurrency, and greater inherent capabd[ity for detection of and tolerance to timing errors. © 1995, ACM. All rights reserved.",Concurrency; fault tolerance; object models; programming techmques; real-time processing models; timing constraints,Computer simulation; Computer systems programming; Concurrency control; Constraint theory; Data processing; Data structures; Error detection; Fault tolerant computer systems; Object oriented programming; Programming theory; Scheduling; Process oriented models; Real time processing model; Time sensitive object model; Timing constraints; Real time systems
APPL/A: A Language for Software Process Programming,1995,ACM Transactions on Software Engineering and Methodology (TOSEM),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0029333633&doi=10.1145%2f214013.214017&partnerID=40&md5=a1dbc4d77752991755efa0fe8b44326b,[No abstract available],,Ada (programming language); Computer aided design; Computer aided software engineering; Computer programming; Encoding (symbols); High level languages; Management information systems; Multiprocessing systems; Process engineering; Project management; Software prototyping; Consistency management; Engineering oriented applications; Metaprocesses; Multiparadigm programming languages; Software process programming; Transaction management; Computer programming languages
Graph Models for Reachability Analysis of Concurrent Programs,1995,ACM Transactions on Software Engineering and Methodology (TOSEM),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0029280426&doi=10.1145%2f210134.210180&partnerID=40&md5=4c0f108862fc6740ca648fc44004645f,"The problem of analyzing concurrent systems has been investigated by many researchers, and several solutions have been proposed. Among the proposed techniques, reachability analysis—systematic enumeration of reachable states in a finite-state model—is attractive because it is conceptually simple and relatively straightforward to automate and can be used in conjunction with model-checking procedures to check for application-specific as well as general properties. This article shows that the nature of the translation from source code to a modeling formalism is of greater practical importance than the underlying formalism. Features identified as pragmatically important are the representation of internal choice, selection of a dynamic or static matching rule, and the ease of applying reductions. Since combinatorial explosion is the primary impediment to application of reachability analysis, a particular concern in choosing a model is facilitating divide-and-conquer analysis of large programs. Recently, much interest in finite-state verification systems has centered on algebraic theories of concurrency. Algebraic structure can be used to decompose reachability analysis based on a flowgraph model. The semantic equivalence of graph and Petri net-based models suggests that one ought to be able to apply a similar strategy for decomposing Petri nets. We describe how category-theoretic treatments of Petri nets provide a basis for decomposition of Petri net reachability analysis. © 1995, ACM. All rights reserved.",Ada tasking; process algebra; static analysis,Concurrent engineering; Finite element method; Graph theory; Petri nets; Software engineering; Systems engineering; Concurrent programs; Graph models; Computer software selection and evaluation
Authors' Response,1995,ACM Transactions on Software Engineering and Methodology (TOSEM),https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976752238&doi=10.1145%2f210134.210438&partnerID=40&md5=a7fd68c3bbe968166453c8c71b27061d,[No abstract available],,
Aspect: Detecting Bugs with Abstract Dependences,1995,ACM Transactions on Software Engineering and Methodology (TOSEM),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0029280675&doi=10.1145%2f210134.210135&partnerID=40&md5=363816fef1a86d30b65509a9fda36aa5,"Aspect is a static analysis technique for detecting bugs in imperative programs, consisting of an annotation language and a checking tool. Like a type declaration, an Aspect annotation of a procedure is a kind of declarative, partial specification that can be checked efficiently in a modular fashion. But instead of constraining the types of arguments and results, Aspect specifications assert dependences that should hold between inputs and outputs. The checker uses a simple dependence analysis to check code against annotations and can find bugs automatically that are not detectable by other static means, especially errors of omission, which are common, but resistant to type checking. This article explains the basic scheme and shows how it is elaborated to handle data abstraction and aliasing. © 1995, ACM. All rights reserved.",dataflow dependences; partial specification; partial verification,Codes (symbols); Computer programming languages; Constraint theory; Logic programming; Program diagnostics; Software engineering; Detecting bugs; Partial verification; Program debugging
Comments on “the Cost of Selective Recompilation and Environment Processing”,1995,ACM Transactions on Software Engineering and Methodology (TOSEM),https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976830494&doi=10.1145%2f210134.210435&partnerID=40&md5=ad53746111ea183d64dddad7552e2712,[No abstract available],,
Signature Matching: A Tool for Using Software Libraries,1995,ACM Transactions on Software Engineering and Methodology (TOSEM),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0029280449&doi=10.1145%2f210134.210179&partnerID=40&md5=0474a26b2f1ca4c089bd703f18f9011d,"Signature matching is a method for organizing, navigating through, and retrieving from software libraries. We consider two kinds of software library components—functions and modules—and hence two kinds of matching—function matching and module matching. The signature of a function is simply its type; the signature of a module is a multiset of user-defined types and a multiset of function signatures. For both functions and modules, we consider not just exact match but also various flavors of relaxed match. We describe various applications of signature matching as a tool for using software libraries, inspired by the use of our implementation of a function signature matcher written in Standard ML. © 1995, ACM. All rights reserved.",signature matching; software retrieval,Computer programming languages; Database systems; File organization; Interfaces (computer); Libraries; Signature matching; Software libraries; Software engineering
Formalizing Style to Understand Descriptions of Software Architecture,1995,ACM Transactions on Software Engineering and Methodology (TOSEM),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0029390712&doi=10.1145%2f226241.226244&partnerID=40&md5=fb05cf42adf395145c2197f798d744a4,[No abstract available],,Computational linguistics; Computer architecture; Computer hardware description languages; Formal logic; Interfaces (computer); Software architecture; Software engineering
Structuring Z Specifications with Views,1995,ACM Transactions on Software Engineering and Methodology (TOSEM),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0029389271&doi=10.1145%2f226241.226249&partnerID=40&md5=91c2e47ef17dcff870dc9bfb2ef594d4,[No abstract available],,Computer hardware description languages; Formal languages; Interfaces (computer); State space methods; Implicit definition; View structuring; Z specifications; Software engineering
Double Iterative Framework for Flow-Sensitive Interprocedural Data Flow Analysis,1994,ACM Transactions on Software Engineering and Methodology (TOSEM),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0028320694&doi=10.1145%2f174634.174635&partnerID=40&md5=1e6d6e8af488c4382b215560f01d66fb,"Compiler optimization, parallel processing, data flow testing, and symbolic debugging can benefit from interprocedural data flow analysis. However, the live, reaching definition, and most summary data flow problems are theoretically intractable in the interprocedural case. A method is presented that reduces the exponential time bound with the help of an algorithm that solves the problem in polynomial time. Either the resulting sets contain precise results or the missing 1994 results do not cause any problems during their use. We also introduce the double iterative framework, where one procedure is processed at a time. The results of the intraprocedural analysis of procedures then propagates along the edges of the call multi-graph. In this way the intra and interprocedural analyses are executed alternately until there is no change in any result set. This method can be applied to any known interprocedural data flow problem. Here the algorithms for the kill, live variables, and reaching definitions problems are presented. Besides for precision, the algorithms can be used for very large programs, and since inter and intraprocedural analyses can be optimized separately, the method is fast as well. © 1994, ACM. All rights reserved.",data flow analysis; double iterative frameworks,Algorithms; Codes (symbols); Critical path analysis; Data transfer; Graph theory; Iterative methods; Mathematical models; Parallel processing systems; Procedure oriented languages; Program compilers; Program diagnostics; Systems analysis; Double iterative framework; Flow sensitive interprocedural data flow analysis; Multigraph; Program debugging
Object-Oriented Logical Specification of Time-Critical Systems,1994,ACM Transactions on Software Engineering and Methodology (TOSEM),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0028320693&doi=10.1145%2f174634.174636&partnerID=40&md5=1cbd164e058edfaa089f795a674fc6fd,"We define TRIO+, an object-oriented logical language for modular system specification. TRIO+ is based on TRIO, a first-order temporal language that is well suited to the specification of embedded and real-time systems, and that provides an effective support to a variety of validation activities, like specification testing, simulation, and property proof. Unfortunately, TRIO lacks the ability to construct specifications of complex systems in a systematic and modular way. TRIO+ combines the use of constructs for hierarchical system decomposition and object-oriented concepts like inheritance and genericity with an expressive and intuitive graphic notation, yielding a specification language that is formal and rigorous, yet still flexible, readable, general, and easily adaptable to the user's needs. After introducing and motivating the main features of the language, we illustrate its application to a nontrivial case study extracted from a real-life industrial application. © 1994, ACM. All rights reserved.",first-order logic; formal specifications; model-theoretic semantics; object-oriented methodologies; real-time systems; temporal logic,Computational linguistics; Computer graphics; Computer hardware description languages; Critical path analysis; Formal logic; Hierarchical systems; Interfaces (computer); Logic programming; Object oriented programming; Subroutines; Theorem proving; First order logic; Model theoretic semantics; Object oriented logical language TRIO; Object oriented logical specification; Temporal logic; Time critical systems; Real time systems
"A Concurrency Analysis Tool Suite for Ada Programs: Rationale, Design, and Preliminary Experience",1995,ACM Transactions on Software Engineering and Methodology (TOSEM),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0029222886&doi=10.1145%2f201055.201080&partnerID=40&md5=4169c365eeedf0b2bc9268fd6931d8a3,"Cats 1995 is designed to satisfy several criteria: it must analyze implementation-level Ada source code and check user-specified conditions associated with program source code; it must be modularized in a fashion that supports flexible composition with other tool components, including integration with a variety of testing and analysis techniques; and its performance and capacity must be sufficient for analysis of real application programs. Meeting these objectives together is significantly more difficult than meeting any of them alone. We describe the design and rationale of Cats and report experience with an implementation. The issues addressed here are primarily practical concerns for modularizing and integrating tools for analysis of actual source programs. We also report successful application of Cats to major subsystems of a (nontoy) highly concurrent user interface system. © 1995, ACM. All rights reserved.",Ada; concurrency; software development environments; static analysis; tool integration,Codes (symbols); Computer aided software engineering; Distributed computer systems; Parallel processing systems; Program debugging; Reliability; Software engineering; Systems analysis; User interfaces; Concurrency; Static analysis; Tool integration; Verification; Ada (programming language)
The Cost of Selective Recompilation and Environment Processing,1994,ACM Transactions on Software Engineering and Methodology (TOSEM),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0028320066&doi=10.1145%2f174634.174637&partnerID=40&md5=33a7cb80d1625b998c6a7eee74b8dc3d,"When a single software module in a large system is modified, a potentially large number of other modules may have to be recompiled. By reducing both the number of compilations and the amount of input processed by each compilation run, the turnaround time after changes can be reduced significantly. Potential time savings are measured in a medium-sized, industrial software project over a three-year period. The results indicate that a large number of compilations caused by traditional compilation unit dependencies may be redundant. On the available data, a mechanism that compares compiler output saves about 25 percent, smart recompilation saves 50 percent, and smartest recompilation may save up to 80 percent of compilation work. Furthermore, all compilation methods other than smartest recompilation process large amounts of unused environment data. In the project analyzed, the average environment symbols are actually used. Reading only the actually used symbols would reduce total compiler input by about 50 percent. Combining smart recompilation with a reduction in environment processing might double to triple perceived compilation speed and double linker speed, without sacrificing static type safety. © 1994, ACM. All rights reserved.",empirical analysis; environment processing; selective recompilation; separate compilation; smart recompilation; software evolution,Codes (symbols); Computer programming languages; Computer software; Computer systems; Data processing; Large scale systems; Redundancy; Software engineering; Turnaround time; Empirical analysis; Environment processing; Selective recompilation; Separate compilation; Smart compilation; Software configuration management; Software evolution; Program compilers
Software Trustability Analysis,1995,ACM Transactions on Software Engineering and Methodology (TOSEM),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0029233079&doi=10.1145%2f201055.201058&partnerID=40&md5=7273e7f279264439ee5a8c58a302c9c4,"A measure of software dependability called trustability is described. A program p has trustability T if we are at least T confident that p is free of faults. Trustability measurement depends on detectability. The detectability of a method is the probability that it will detect faults, when there are faults present. Detectability research can be used to characterize conditions under which one testing and analysis method is more effective than another. Several detectability results that were only previously described informally, and illustrated by example, are proved. Several new detectability results are also proved. The trustability model characterizes the kind of information that is needed to justify a given level of trustability. When the required information is available, the trustability approach can be used to determine strategies in which methods are combined for maximum effectiveness. It can be used to determine the minimum amount of resources needed to guarantee a required degree of trustability, and the maximum trustability that is achievable with a given amount of resources. Theorems proving several optimization results are given. Applications of the trustability model are discussed. Methods for the derivation of detectability factors, the relationship between trustability and operational reliability, and the relationship between the software development process and trustability are described. © 1995, ACM. All rights reserved.",analysis; dependability; detectability; failure density; statistical; testability; testing; trustability,Computer software portability; Failure analysis; Optimization; Probability; Program debugging; Reliability; Software engineering; Statistics; Theorem proving; Detectability; Failure density; Software trustability; Testability; Verification; Computer software
Program Integration for Languages with Procedure Calls,1995,ACM Transactions on Software Engineering and Methodology (TOSEM),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0029233080&doi=10.1145%2f201055.201056&partnerID=40&md5=5a976914429e2a4839caf866b53e0595,"Given a program Base and two variants, A and B, each created by modifying separate copies of Base, the goal of program integration is to determine whether the modifications interfere, and if they do not, to create an integrated program that incorporates both sets of changes as well as the portions of Base preserved in both variants. Text-based integration techniques, such as the one used by the Unix diff3 utility, are obviously unsatisfactory because one has no guarantees about how the execution behavior of the integrated program relates to the behaviors of Base, A, and B. The first program integration algorithm to provide such guarantees was developed by Horwitz, Prins, and Reps. However, a limitation of that algorithm is that it only applied to programs written in a restricted language—in particular, the algorithm does not handle programs with procedures. This article describes a generalization of the Horwitz-Prins-Reps algorithm that handles programs that consist of multiple 1995 procedures. We show that two straightforward generalizations of the Horwitz-Prins-Reps algorithm yield unsatisfactory results. The key issue in developing a satisfactory algorithm is how to take into account different calling contexts when determining what has changed in the variants A and B. Our solution to this problem involves identifying two different kinds of affected components of A and B: those affected regardless of how the procedure is called, and those affected by a changed or new calling context. The algorithm makes use of interprocedural program slicing to identify these components, as well as components in Base, A, and B with the same behavior. © 1995, ACM. All rights reserved.",Control dependence; data dependence; data-flow analysis; flow-insensitive summary information; program dependence graph; program slicing; semantics-based program integration,Algorithms; Computational linguistics; Computer programming languages; Control systems; Data processing; Data structures; Encoding (symbols); Graph theory; Program processors; Software engineering; Control dependence; Data dependence; Data flow analysis; Flow insensitive summary information; Procedure calls; Program dependence graph; Program integration; Program slicing; Computer software
A Simplified Domain-Testing Strategy,1994,ACM Transactions on Software Engineering and Methodology (TOSEM),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0028479361&doi=10.1145%2f196092.193171&partnerID=40&md5=99f5bb4bf06e4af297359ad12059f559,[No abstract available],domain testing; software testing,Computer software; Error detection; Reliability; Software engineering; Testing; Domain testing strategy; Software testing; Program debugging
Application and Experimental Evaluation of State Space Reduction Methods for Deadlock Analysis in Ada,1994,ACM Transactions on Software Engineering and Methodology (TOSEM),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0028532451&doi=10.1145%2f201024.201038&partnerID=40&md5=e75999c2afcd673865538a49e78a9d56,"An emerging challenge for software engineering is the development of the methods and tools to aid design and analysis of concurrent and distributed software. Over the past few years, a number of analysis methods that focus on Ada tasking have been developed. Many of these methods are based on some form of reachability analysis, which has the advantage of being conceptually simple, but the disadvantage of being computationally expensive. We explore the effectiveness of various Petri net-based techniques for the automated deadlock analysis of Ada programs. Our experiments consider a variety of state space reduction methods both individually and in various combinations. The experiments are applied to a number of classical concurrent programs as well as a set of “real-world” programs. The results indicate that Petri net reduction and reduced state space generation are mutually beneficial techniques, and that combined approaches based on Petri net models are quite effective, compared to alternative analysis approaches. © 1994, ACM. All rights reserved.",Ada tasking; automatic analysis; concurrency analysis; deadlock detection; experimental evaluation; state space explosion,Ada (programming language); Mathematical models; Multiprogramming; Petri nets; Program debugging; Software engineering; State space methods; Ada tasking; Automatic analysis; Concurrency analysis; Deadlock detection; Computer system recovery
A Reduced Test Suite for Protocol Conformance Testing,1994,ACM Transactions on Software Engineering and Methodology (TOSEM),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0028478618&doi=10.1145%2f196092.196088&partnerID=40&md5=21a5d00cb66d6683c513f8691dcceb6a,"Let M be a finite-state machine, and let S be an implementation of M. The protocol-testing problem is the problem of determining if S is a correct implementation of M. One known method for solving this problem, called the W-method, has the disadvantage that it generates a relatively large test set. In this paper, we describe three new versions of this method. We prove that these versions all have the same fault detection capability as the W-method. In addition, we show that in most cases all three generate a smaller number of tests than the W-method. Specifically, suppose M1 and M2 are finite-state machines having n and m states, respectively, where Ml is a specification (M), Mz is an implementation (S), and m n. In addition, suppose they have input alphabet X, where [formula ommited] let a be the total number of strings in a characterization set for Ml, and let be the total number of strings in a transition couer set for Ml. The W-method will generate a test set consisting of [formula ommited] strings. In contrast, our first algorithm will generate a test set containing at most [formula ommited] strings. For our second algorithm, the number of strings will be [formula ommited], and for the third, [formula ommited], When [formula ommited], all three of our algorithms will produce fewer strings than the W-method. Finally, two of our algorithms make use of a heuristic for minimizing the number of strings in a characterization set. We show that the performance ratio for this heuristic has an upper bound of O(log n). © 1994, ACM. All rights reserved.",heuristics,Algorithms; Computer software selection and evaluation; Heuristic methods; Optimization; Performance; Program debugging; Reliability; Sequential machines; Characterization set; Performance ratio; Protocol conformance testing; Test data generators; Verification; Network protocols
Formal Specification and Design of a Message Router,1994,ACM Transactions on Software Engineering and Methodology (TOSEM),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0028531548&doi=10.1145%2f201024.201026&partnerID=40&md5=3e11f1ec815ac358af1e76024541cefd,"Formal derivation refers to a family of design techniques that entail the development of programs which are guaranteed to be correct by construction. Only limited industrial use of such techniques 1994 has been reported in the literature, and there is a great need for methodological developments aimed at facilitating their application to complex problems. This article examines the formal specification and design of a message router in an attempt to identify those methodological elements that are likely to contribute to successful industrial uses of program derivation. Although the message router cannot be characterized as being industrial grade, it is a sophisticated problem that poses significant specification and design challenges—its apparent simplicity is rather deceiving. The main body of the article consists of a complete formal specification of the router and a series of successive refinements that eventually lead to an immediate construction of a correct UNITY program. Each refinement is accompanied by its design rationale and is explained in a manner accessible to a broad audience. We use this example to make the case that program derivation provides a good basis for introducing rigor in the design strategy, regardless of the degrees of formality one is willing to consider. © 1994, ACM. All rights reserved.",formal methods; program derivation; specification refinement; UNITY,Computer software; Computer systems programming; Data communication systems; Software engineering; Theorem proving; Correctness proofs; Message router; Program derivation; Specification refinement; Formal logic
The Larch/Smalltalk Interface Specification Language,1994,ACM Transactions on Software Engineering and Methodology (TOSEM),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0028479040&doi=10.1145%2f196092.195325&partnerID=40&md5=8af83858766d91eca2197de39e1ec0d4,[No abstract available],formal methods; interface specification; Larch/Smalltalk; Smalltalk; specification inheritance; subtype; verification,Codes (symbols); Database systems; Formal logic; Object oriented programming; Program documentation; Software engineering; Systems analysis; User interfaces; Formal methods; Larch/smalltalk interface specification language; Smalltalk; Specification inheritance; Subtype; Verification; Computer hardware description languages
Validating Real-Time Systems by History-Checking TRIO Specifications,1994,ACM Transactions on Software Engineering and Methodology (TOSEM),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0028530476&doi=10.1145%2f201024.201034&partnerID=40&md5=cb368eb684c387779a393bce82d86ad9,"We emphasize the importance of formal executable specifications in the development of real-time systems, as a means to assess the adequacy of the requirements before a costly development process takes place. TRIO is a first-order temporal logic language for executable specification of real-time systems that deals with time in a quantitative way by providing a metric to indicate distance in time between events and length of time intervals. We summarize the language and its model-parametric semantics. Then we present an algorithm to perform history checking, i.e., to check that a history of the system satisfies the specification. This algorithm can be used as a basis for an effective specification testing tool. The algorithm is described; an estimation of its complexity is provided; and the main functionalities of the tool are presented, together with sample test cases. Finally, we draw conclusions and indicate directions of future research. © 1994, ACM. All rights reserved.",first-order logic; formal specifications; model-theoretic semantics; requirements validation,Algorithms; Computational complexity; Computational linguistics; Computer programming languages; Logic programming; Real time systems; Software engineering; First order temporal logic language; Formal specifications; Model theoretic semantics; Requirements validation; Formal logic
An Algebraic Theory of Class Specification,1994,ACM Transactions on Software Engineering and Methodology (TOSEM),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0028421662&doi=10.1145%2f192218.192235&partnerID=40&md5=ae443b909d4167d51c3965c6be4246d2,"The notion of class 1994 as defined in most object-oriented languages is formalized using known techniques from algebraic specifications. Inheritance can be viewed as a relation betweeen classes, which suggests how classes can be arranged in hierarchies. The hierarchies contain two kinds of information: on the one hand, they indicate how programs are structured and how code is shared among classes; on the other hand, they give information about compatible assignment rules, which are based on subtyping. In order to distinguish between code sharing, which is related to implementational aspects, and functional specialization, which is connected to the external behavior of objects, we introduce an algebraic specification-based formalism, by which one can specify the behavior of a class and state when a class inherits another one. It is shown that reusing inheritance can be reduced to specialization inheritance with respect to a virtual class. The class model and the two distinct aspects of inheritance allow the definition of clean interconnection mechanisms between classes leading to new classes which inherit from old classes their correctness and their semantics. © 1994, ACM. All rights reserved.",algebraic specifications; inheritance; interconnection mechanisms; modularity,Algebra; Computational linguistics; Computer programming languages; Data structures; Equivalence classes; Error correction; Formal logic; Algebraic specifications; Class specification; Inheritance; Interconnection mechanisms; Modularity; Object oriented programming
The ASTOOT Approach to Testing Object-Oriented Programs,1994,ACM Transactions on Software Engineering and Methodology (TOSEM),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0028421077&doi=10.1145%2f192218.192221&partnerID=40&md5=3a5ac09939d1e84a4d9dbc697b90bc62,"This article describes a new approach to the unit testing of object-oriented programs, a set of tools based on this approach, and two case studies. In this approach, each test case consists of a tuple of sequences of messages, along with tags indicating whether these sequences should put objects of the class under test into equivalent states and/or return objects that are in equivalent states. Tests are executed by sending the sequences to objects of the class under test, then invoking a user-supplied equivalence-checking mechanism. This approach allows for substantial automation of many aspects of testing, including test case generation, test driver generation, test execution, and test checking. Experimental prototypes of tools for test generation and test execution are described. The test generation tool requires the availability of an algebraic specification of the abstract data type being tested, but the test execution tool can be used when no formal specification is available. Using the test execution tools, case studies involving execution of tens of thousands of test cases, with various sequence lengths, parameters, and combinations of operations were performed. The relationships among likelihood of detecting an error and sequence length, range of parameters, and relative frequency of various operations were investigated for priority queue and sorted-list implementations having subtle errors. In each case, long sequences tended to be more likely to detect the error, provided that the range of parameters was sufficiently large and likelihood of detecting an error tended to increase up to a threshold value as the parameter range increased. © 1994, ACM. All rights reserved.",Abstract data types; algebraic specification; object-oriented programming; software testing,Algorithms; Computability and decidability; Computer programming languages; Data structures; Equivalence classes; Program debugging; Software engineering; Algebraic specification; Program testing; Symbolic execution; Test data generators; Object oriented programming
A Graphical Interval Logic for Specifying Concurrent Systems,1994,ACM Transactions on Software Engineering and Methodology (TOSEM),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0028421478&doi=10.1145%2f192218.192226&partnerID=40&md5=bb588dcf8b6390650aa1cd17450203f9,"This article describes a graphical interval logic that is the foundation of a tool set supporting formal specification and verification of concurrent software systems. Experience has shown that most software engineers find standard temporal logics difficult to understand and use. The objective of this article is to enable software engineers to specify and reason about temporal properties of concurrent systems more easily by providing them with a logic that has an intuitive graphical representation and with tools that support its use. To illustrate the use of the graphical logic, the article provides some specifications for an elevator system and proves several properties of the specifications. The article also describes the tool set and the implementation. © 1994, ACM. All rights reserved.",automated proof-checking; concurrent systems; formal specifications; graphical interval logic; temporal logic; timing diagrams; visual languages,Computer graphics; Computer programming languages; Data structures; Human engineering; Program debugging; Program diagnostics; Software engineering; Systems analysis; Theorem proving; Automated proof checking; Concurrent systems; Graphical interval logic; Temporal logic; Timing diagrams; Visual languages; Formal logic
Distributed Real-Time System Specification and Verification in APTL,1993,ACM Transactions on Software Engineering and Methodology (TOSEM),https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976656144&doi=10.1145%2f158431.158434&partnerID=40&md5=851ab1d5ac6395f1ee71e7129bff3395,"In this article, we propose a language, Asynchronous Propositional Temporal Logic 1993, for the specification and verification of distributed hard real-time sytems. APTL extends the logic TPTL by dealing explicitly with multiple local clocks. We propose a distributed-system model which permits definition of inequalities asserting the temporal precedence of local clock readings. We show the expressiveness of APTL through two nontrivial examples. Our logic can be used to specify and reason about such important properties as bounded clock rate drifting. We then give a 22 tableau-based decision procedure for determining APTL satisfiability, where n is the size (number of bits) of the input formula. © 1993, ACM. All rights reserved.",asynchronous; bounded clock rate drifting; multiclock system model; propositional temporal logic; real-time systems; specification; verification,
A Visual Execution Model for Ada Tasking,1993,ACM Transactions on Software Engineering and Methodology (TOSEM),https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976726520&doi=10.1145%2f158431.158432&partnerID=40&md5=73ceb0ef2eeae5693d817ca7e5fbe604,"A visual execution model for Ada tasking can help programmers attain a deeper understanding of the tasking semantics. It can illustrate subtleties in semantic definitions that are not apparent in natural language design. We describe a contour model of Ada tasking that depicts asynchronous tasks 1993, relationships between the environments in which tasks execute, and the manner in which tasks interact. The use of this high-level execution model makes it possible to see what happens during execution of a program. The paper provides an introduction to the contour model of Ada tasking and demonstrates its use. © 1993, ACM. All rights reserved.",Contour model; visual execution model,
Conjunction as Composition,1993,ACM Transactions on Software Engineering and Methodology (TOSEM),https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976743686&doi=10.1145%2f158431.158438&partnerID=40&md5=8f64afe35ed88365fef25eec57e326a4,"Partial specifications written in many different specification languages can be composed if they are all given semantics in the same domain, or alternatively, all translated into a common style of predicate logic. The common semantic domain must be very general, the particular semantics assigned to each specification language must be conducive to composition, and there must be some means of communication that enables specifications to build on one another. The criteria for success are that a wide variety of specification languages should be accommodated, there should be no restrictions on where boundaries between languages can be placed, and intuitive expectations of the specifier should be met. © 1993, ACM. All rights reserved.",compositional specification; multiparadigm specification; practical specification,
Automated Assistance for Program Restructuring,1993,ACM Transactions on Software Engineering and Methodology (TOSEM),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0027623592&doi=10.1145%2f152388.152389&partnerID=40&md5=c8ae8ca205db14d4155f1d573b875583,"Maintenance tends to degrade the structure of software, ultimately making maintenance more costly. At times, then, it is worthwhile to manipulate the structure of a system to make changes easier. However, manual restructuring is an error-prone and expensive activity. By separating structural manipulations from other maintenance activities, the semantics of a system can be held constant by a tool, assuring that no errors are introduced by restructuring. To allow the maintenance team to focus on the aspects of restructuring and maintenance requiring human judgment, a transformation-based tool can be provided—based on a model that exploits preserving data flow dependence and control flow dependence—to automate the repetitive, error-prone, and computationally demanding aspects of restructuring. A set of automatable transformations is introduced; their impact on structure is described, and their usefulness is demonstrated in examples. A model to aid building meaning-preserving restructuring transformations is described, and its realization in a functioning prototype tool for restructuring Scheme programs is discussed. © 1993, ACM. All rights reserved.",CASE; flow analysis; meaning-preserving transformations; software engineering; software evolution; software maintenance; software restructuring; source-level restructuring,Computer programming; Computer software selection and evaluation; Computer systems programming; Automated assistance; Computer maintenance; Control flow dependence; Program restructuring; Transformation based tool; Software engineering
Coordinating Rule-Based Software Processes with ESP,1993,ACM Transactions on Software Engineering and Methodology (TOSEM),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0027623908&doi=10.1145%2f152388.152393&partnerID=40&md5=e626daca652474cd8eddb650ff45fc60,"ESP is a language for modeling rule-based software processes that take place in a distributed software development environment. It is based on PoliS, an abstract coordination model that relies on Multiple Tuple Spaces, i.e., collections of tuples a la Linda. PoliS extends Linda aiming at the specification and coordination of logically distributed systems. ESP 1993 combines the PoliS mechanisms to deal with concurrency and distribution, with the logic-programming language Prolog, to deal with rules and deduction. Such a combination of a coordination model and a logic language provides a powerful framework in which experiments about rule-based software process programming can be performed and evaluated. © 1993, ACM. All rights reserved.",concurrency; logic programming; multiuser programming environment; rule-based programming; software process; software process modeling,Computer programming; Computer programming languages; Logic programming; Abstract coordination model; Concurrency; Extended shared prolog (ESP); Multiple tuple spaces; Rule-based software processes; Computer software
A Methodology for Controlling the Size of a Test Suite,1993,ACM Transactions on Software Engineering and Methodology (TOSEM),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0027625420&doi=10.1145%2f152388.152391&partnerID=40&md5=5696ea89ba844eb8dc0fa00577d3b325,"This paper presents a technique to select a representative set of test cases from a test suite that provides the same coverage as the entire test suite. This selection is performed by identifying, and then eliminating, the redundant and obsolete test cases in the test suite. The representative set replaces the original test suite and thus, potentially produces a smaller test suite. The representative set can also be used to identify those test cases that should be rerun to test the program after it has been changed. Our technique is independent of the testing methodology and only requires an association between a testing requirement and the test cases that satisfy the requirement. We illustrate the technique using the data flow testing methodology. The reduction that is possible with our technique is illustrated by experimental results. © 1993, ACM. All rights reserved.",hitting set; regression testing; software engineering; software maintenance; test suite reduction,Algorithms; Computer programming; Program debugging; Data flow testing; Methodology; Test cases; Test suite; Software engineering
A Meta-Environment for Generating Programming Environments,1993,ACM Transactions on Software Engineering and Methodology (TOSEM),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0027575871&doi=10.1145%2f151257.151260&partnerID=40&md5=2ca6c9cf159829aa8c455819087153b1,"Over the last decade, considerable progress has been made in solving the problems of automatic generation of programming/development environments, given a formal definition of some programming or specification language. In most cases, research has focused on the functionality and efficiency of the generated environments, and, of course, these aspects will ultimately determine the acceptance of environment generators. However, only marginal attention has been paid to the development process of formal language definitions itself. Assuming that the quality of automatically generated environments will be satisfactory within a few years, the development costs of formal language definitions will then become the next limiting factor determining ultimate success and acceptance of environment generators. In this paper we describe the design and implementation of a meta-environment 1993 based on the formalism ASF + SDF. This meta-environment is currently being implemented as part of the Centaur system and is, at least partly, obtained by applying environment generation techniques to the language definition formalism itself. A central problem is providing fully interactive editing of modular language definitions such that modifications made to the language definition during editing can be translated immediately to modifications in the programming environment generated from the original language definition. Therefore, some of the issues addressed are the treatment of formalisms with user-definable syntax and incremental program generation techniques. © 1993, ACM. All rights reserved.",,Computational linguistics; Computer hardware description languages; Computer programming languages; Formal logic; Report generators; Software engineering; Algebraic specification; Application generators; Application languages; Centaur system; Concrete and abstract syntax; Language definition formalism; Meta environment; Programming environment generation; User definable syntax; Computer programming
Simulating Reactive Systems by Deduction,1993,ACM Transactions on Software Engineering and Methodology (TOSEM),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0027575284&doi=10.1145%2f151257.151259&partnerID=40&md5=6b7c5f6f43c6ac6a81be763df7f8ae69,"Debugging is one of the main uses of simulation. Localizing bugs or finding the reasons for unclear behavior involves going backwards in time, whereas simulation goes forward in time. Therefore, identifying causes with the aid of most existing simulation tools usually requires repeating the simulation several times, each time with reduced holes in the sieve. An alternative is simulation by deduction, a technique in which the steps in the dynamic behavior of the simulated model are deduced by a reasoning system. A simulation system that uses simulation by deduction can give direct answers to questions about the reasons for the simulation results. By recording the support for its deductions, such a system can answer “why” and “why not” questions about the scenario. Another benefit of simulation by deduction is that it enables symbolic simulation, that is, simulating a scenario given only a partial description of the environment and the simulated model. This allows verifying properties of an evolving design at any stage of the design process, and thus checking the consequences of the design decisions made so far. In order to allow deducing as much as possible from partial information, the axiom system has to be minimalistic, i.e., axioms have to require the minimum amount of knowledge of simulation inputs. These ideas were implemented in a system called SIP, which simulates the behavior of reactive systems. SIP is capable of answering “why,” “why not,” and “what if” questions. It also has a limited capability of dealing with partial knowledge. SIP is based on a reasoning system that is responsible for deducing the effects of the external inputs on the state of the simulated model, and recording the support for its deductions. The logical basis for the deduction of a step in SIP is provided by a minimalistic axiom system for statecharts. Although SIP simulates reactive systems described as statecharts, the principle of simulation by deduction is applicable to other types of systems and descriptions, provided only that they have a well-defined formal semantics. © 1993, ACM. All rights reserved.",,Computation theory; Computational linguistics; Data reduction; Flowcharting; Mathematical models; Simulation; Theorem proving; Reactive systems; Simulation support systems; Program debugging
Experimental Results from an Automatic Test Case Generator,1993,ACM Transactions on Software Engineering and Methodology (TOSEM),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0027579085&doi=10.1145%2f151257.151258&partnerID=40&md5=e44c46382007d760b81f3ce495ad2db1,"Constraint-based testing is a novel way of generating test data to detect specific types of common programming faults. The conditions under which faults will be detected are encoded as mathematical systems of constraints in terms of program symbols. A set of tools, collectively called Godzilla, has been implemented that automatically generates constraint systems and solves them to create test cases for use by the Mothra testing system. Experimental results from using Godzilla show that the technique can produce test data that is very close in terms of mutation adequacy to test data that is produced manually, and at substantially reduced cost. Additionally, these experiments have suggested a new procedure for unit testing, where test cases are viewed as throw-away items rather than scarce resources. © 1993, ACM. All rights reserved.",adequacy; constraints; mutation analysis,Algorithms; Codes (symbols); Constraint theory; Program debugging; Report generators; Software engineering; Adequacy; Constraint based testing (CBT); Godzilla; Mothra testing system; Mutation analysis; Computer programming
Retrieving Reusable Software by Sampling Behavior,1993,ACM Transactions on Software Engineering and Methodology (TOSEM),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0027627536&doi=10.1145%2f152388.152392&partnerID=40&md5=210f3b99f9cd4273cfafdd81a4b116d6,"A new method, called behavior sampling, is proposed for automated retrieval of reusable components from software libraries. Behavior sampling exploits the property of software that distinguished it from other forms of test: executability. Basic behavior sampling identifies relevant routines by executing candidates on a searcher-supplied sample of operational inputs and by comparing their output to output provided by the searcher. The probabilistic basis for behavior sampling is described, and experimental results are reported that suggest that basic behavior sampling exhibits high precision when used with small samples. Extensions to basic behavior sampling are proposed to improve its recall and to make it applicable to the retrieval of abstract data types and object classes. © 1993, ACM. All rights reserved.",behavior sampling; software libraries; software retrieval; software reuse,Algorithms; Computer software; Information analysis; Information retrieval systems; Libraries; Automated retrieval; Probabilistic bases; Reusable software; Sampling behavior; Software libraries; Software engineering
Functional Specification of Time-Sensitive Communicating Systems,1993,ACM Transactions on Software Engineering and Methodology (TOSEM),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0027353263&doi=10.1145%2f151299.151302&partnerID=40&md5=8228ef7a32d003f0ab6ea9d38e8d68aa,"A formal model and a logical framework for the functional specification of time-sensitive communicating systems and their interacting components are outlined. The specification method is modular with respect to sequential composition, parallel composition, and communication feedback. Nondeterminism is included by underspecification. The application of the specification method to timed communicating functions is demonstrated. Abstractions from time are studied. In particular, a rational is given for the chosen concepts of the functional specification technique. The relationship between system models based on nondeterminism and system models based on explicit time notions is investigated. Forms of reasoning are considered. The alternating bit protocol is used as a running example. © 1993, ACM. All rights reserved.",functional system models; real-time systems; specification,Computer hardware description languages; Computer software; Data communication systems; Formal logic; Mathematical models; Network protocols; Real time systems; Systems analysis; Experimentation; Functional specification models; Specification; Verification; Distributed computer systems
Parallel and Distributed Incremental Attribute Evaluation Algorithms for Multiuser Software Development Environments,1993,ACM Transactions on Software Engineering and Methodology (TOSEM),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0027348001&doi=10.1145%2f151299.151312&partnerID=40&md5=5b8cbb3982e91791155735797f418a9f,"The problem of change propagation in multiuser software development environments distributed across a local-area network is addressed. The program is modeled as an attributed parse tree segmented among multiple user processes and changes are modeled as subtree replacements requested asynchronously by individual users. Change propagation is then implemented using decentralized incremental evaluation of an attribute grammar that defines the static semantic properties of the programming language. Building up to our primary result, we first present algorithms that support parallel evaluation on a centralized tree in response to single edits using a singe editing cursor and multiple edits with multiple editing cursors. Then we present our algorithm for parallel evaluation on a decentralized tree. We also present a protocol to guarantee reliability of the evaluation algorithm as components of the decentralized tree become unavailable due to failures and return to availability. © 1993, ACM. All rights reserved.",attribute grammar; change propagation; distributed; incremental algorithm; parallel; reliability,Computational grammars; Computational linguistics; Computer aided software engineering; Computer system recovery; Data structures; Failure analysis; File editors; Interfaces (computer); Network protocols; Parallel processing systems; Reliability; Trees (mathematics); Change propagation; Incremental attribute evaluation algorithms; Multiuser software development environments; Algorithms
Interprocedural static analysis of sequencing constraints,1992,ACM Transactions on Software Engineering and Methodology (TOSEM),https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976834591&doi=10.1145%2f125489.122822&partnerID=40&md5=a145467861127e154ac4b65861d8f5aa,"This paper describes a system that automatically performs static interprocedural sequencing analysis from programmable constraint specifications. We describe the algorithms used for interprocedural analysis, relate the problems arising from the analysis of real-world programs, and show how these difficulties were overcome. Finally, we sketch the architecture of our prototype analysis system 1992 and describe our experiences to date with its use, citing performance and error detection characteristics. © 1992, ACM. All rights reserved.",error detection; interprocedural data flow analysis; sequencing constraints,
The automated production control documentation system: A case study in cleanroom software engineering,1992,ACM Transactions on Software Engineering and Methodology (TOSEM),https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976777568&doi=10.1145%2f125489.122826&partnerID=40&md5=eb22e62f3121e9fcf15d0279eb5a755c,"A prototype software system was developed for the U.S. Naval Underwater Systems Center1992 as a demonstration of the Cleanroom Software Engineering methodology. The Cleanroom method is a team approach to the incremental development of software under statistical quality control. Cleanroom's formal methods of Box Structure specification and design, functional verification, and statistical testing were used by a four-person team to develop the Automated Production Control Documentation(APCODOC) system, a relational database application. As is typical in Cleanroom developments, correctness of design and code were ensured through team reviews. Eighteen errors were found during functional verification of the design, and nineteen errors were found during walkthrough of the 1820 lines of FOXBASE code. The software was not executed by developers prior to independent testing (i.e., there was no debugging). There were no errors in compilation, no failures during statistical certification testing, and the software was certified at the target levels of reliability and confidence. Team members attribute the ultimate error-free compilation and failure-free execution of the software to the rigor of the methodology and the intellectual control afforded by the team approach. © 1992, ACM. All rights reserved.",box structures; cleanroom software engineering; statistical quality control; statistical testing,
Investigations of the software testing coupling effect,1992,ACM Transactions on Software Engineering and Methodology (TOSEM),https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976744369&doi=10.1145%2f125489.125473&partnerID=40&md5=00d26ce8674b4d31ef6c86558749cd84,"Fault-based testing strategies test software by focusing on specific, common types of faults. The coupling effect hypothesizes that test data sets that detect simple types of faults are sensitive enough to detect more complex types of faults. This paper describes empirical investigations into the coupling effect over a specific class of software faults. All of the results from this investigation support the validity of the coupling effect. The major conclusion from this investigation is the fact that by explicitly testing for simple faults, we are also implicitly testing for more complicated faults, giving us confidence that fault-based testing is an effective way to test software. © 1992, ACM. All rights reserved.",fault-based testing; mutation; software testing; unit testing,
Markov Analysis of Software Specifications,1993,ACM Transactions on Software Engineering and Methodology (TOSEM),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0027347917&doi=10.1145%2f151299.151326&partnerID=40&md5=971d3a9252669ee9771b8e30b5d3bbd6,"A procedure for modeling software usage with the finite state, discrete parameter Markov chain is described. It involves rigorous analysis of the specification before design and coding begin. Many benefits emerge from this process, including the ability to synthesize a macro level usage distribution from a micro level understanding of how the software will be used. This usage distribution becomes the basis for a statistical test of the software, which is fundamental to the Cleanroom development process. Some analytical results known for Markov chains that have meaningful implications and interpretations for the software development process are described. © 1993, ACM. All rights reserved.",box structure method; certification; Cleanroom; Markov chain; software specification; statistical test; stochastic process; usage distribution,Computational complexity; Computer hardware description languages; Data structures; Finite automata; Numerical analysis; Parameter estimation; Program debugging; Program documentation; Quality assurance; Random processes; Software engineering; Statistical tests; Box structure method; Certification; Cleanroom software engineering; Markov analysis; Software specifications; Stochastic process; Usage distribution; Computer software
The Pan language-based editing system,1992,ACM Transactions on Software Engineering and Methodology (TOSEM),https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976717315&doi=10.1145%2f125489.122804&partnerID=40&md5=404ea96ce51a02301f10aa2824db0321,"Powerful editing systems for developing complex software documents are difficult to engineer. Besides requiring efficient incremental algorithms and complex data structures, such editors must accommodate flexible editing styles, provide a consistent, coherent, and powerful user interface, support individual variations and projectwide configurations, maintain a sharable database of information concerning the documents being edited, and integrate smoothly with the other tools in the environment. Pan is a language-based editing and browsing system that exhibits these characteristics. This paper surveys the design and engineering of Pan, paying particular attention to a number of issues that pervade the system: incremental checking and analysis, information retention in the presence of change, tolerance for errors and anomalies, and extension facilities. © 1992, ACM. All rights reserved.",coherent user interfaces; colander; contextual constraint; extension facilities; grammatical abstraction; interactive programming environment; Ladle; logic programming; logical constraint grammar; Pan; reason maintenance; syntax-recognizing editor; tolerance for errors and anomalies,
A reference architecture for the component factory,1992,ACM Transactions on Software Engineering and Methodology (TOSEM),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0040519313&doi=10.1145%2f125489.122823&partnerID=40&md5=56fa149e33237e81fe4ce4bf41b011ba,"Software reuse can be achieved through an organization that focuses on utilization of life cycle products from previous developments. The component factory is both an example of the more general concepts of experience and domain factory and an organizational unit worth being considered independently. The critical features of such an organization are flexibility and continuous improvement. In order to achieve these features we can represent the architecture of the factory at different levels of abstraction and define a reference architecture from which specific architectures can be derived by instantiation. A reference architecture is an implementation and organization independent representation of the component factory and its environment. The paper outlines this reference architecture, discusses the instantiation process, and presents some examples of specific architectures by comparing them in the framework of the reference model. © 1992, ACM. All rights reserved.",component factory; experience factory; reference architecture; reusability,Computer software reusability; Reusability; Continuous improvements; Experience factory; Instantiation process; Levels of abstraction; Organizational units; Reference architecture; Reference models; Architecture
Reconciling Environment Integration and Software Evolution,1992,ACM Transactions on Software Engineering and Methodology (TOSEM),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0026887938&doi=10.1145%2f131736.131744&partnerID=40&md5=b48167992e10fe2bb7142938bf1f2081,"Common software design approaches complicate both tool integration and software evolution when applied in the development of integrated environments. We illustrate this by tracing the evolution of three different designs for a simple integrated environment as representative changes are made to the requirements. We present an approach that eases integration and evolution by preserving tool independence in the face of integration. We design tool integration relationships as separate components called mediators, and we design tools to implicitly invoke mediators that integrate them. Mediators separate tools from each other, while implicit invocation allows tools to remain independent of mediators. To enable the use of our approach on a range of platforms, we provide a formalized model and requirements for implicit invocation mechanisms. We apply this model both to analyze existing mechanisms and in the design of a mechanism for C++. © 1992, ACM. All rights reserved.",abstract behavior type; behavior abstraction; component independence; environment integration; event mechanism; implicit invocation; integrated environment; mediator; mediator/event design; software evolution; tool integration,C (programming language); Computer programming; Computer software; Design; Models; Software engineering; Utility programs; C++ programming language; Component independence; Environment integration; Implicit invocation; Mediators; Software evolution; Computer aided software engineering
Detection of Linear Errors via Domain Testing,1992,ACM Transactions on Software Engineering and Methodology (TOSEM),https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976828902&doi=10.1145%2f136586.136590&partnerID=40&md5=13d7f02d1a1f8579c211b6715259b0f0,"Domain testing attempts to find errors in the numeric expressions affecting the flow of control through a program. Intuitively, domain testing provides a systematic form of boundary value testing for the conditional statements within a program. Several forms of domain testing have been proposed, all dealing with the detection of linear errors in linear functions. Perturbation analysis has been previously developed as a measure of the volume of faults, from within a selected space of possible faults, left undetected by a test set. It is adapted here to errors and error spaces. The adapted form is used to show that the different forms of domain testing are closer in error detection ability than had been supposed. They may all be considered effective for finding linear errors in linear predicate functions. A simple extension is proposed, which allows them to detect linear errors in nonlinear predicate functions using only a single additional test point. © 1992, ACM. All rights reserved.",domain testing; perturbation testing,
The Design and Implementation of Hierarchical Software Systems with Reusable Components,1992,ACM Transactions on Software Engineering and Methodology (TOSEM),https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976850194&doi=10.1145%2f136586.136587&partnerID=40&md5=39e751eadfe00a7810b0933f78ded625,"We present a domain-independent model of hierarchical software system design and construction that is based on interchangeable software components and large-scale reuse. The model unifies the conceptualizations of two independent projects, Genesis and Avoca, that are successful examples of software component/building-block technologies and domain modeling. Building-block technologies exploit large-scale reuse, rely on open architecture software, and elevate the granularity of programming to the subsystem level. Domain modeling formalizes the similarities and differences among systems of a domain. We believe our model is a blueprint for achieving software component technologies in many domains. © 1992, ACM. All rights reserved.",domain modeling; open system architectures; reuse; software building-blocks; software design,
An Experimental Study of Fault Detection in User Requirements Documents,1992,ACM Transactions on Software Engineering and Methodology (TOSEM),https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976738164&doi=10.1145%2f128894.128897&partnerID=40&md5=ac7e0f0eec0ee037d48bad097a3ddfc5,"This paper describes a software engineering experiment designed to confirm results from an earlier project which measured fault detection rates in user requirements documents 1992. The experiment described in this paper involves the creation of a standardized URD with a known number of injected faults of specific type. Nine independent inspection teams were given this URD with instructions to locate as many faults as possible using the N-fold requirements inspection technique developed by the authors. Results obtained from this experiment confirm earlier conclusions about the low rate of fault detection in requirements documents using formal inspections and the advantages to be gained using the N-fold inspection method. The experiment also provides new results concerning variability in inspection team performance and the relative difficulty of locating different classes of URD faults. © 1992, ACM. All rights reserved.",fault detection; inspections; user requirements,
OBSERV—A Prototyping Language and Environment,1992,ACM Transactions on Software Engineering and Methodology (TOSEM),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0026886255&doi=10.1145%2f131736.131751&partnerID=40&md5=1e590cc2ff2b24d1a68ea3df914be419,"The OBSERV methodology for software development is based on rapid construction of an executable specification, or prototype, of a systems, which may be examined and modified repeatedly to achieve the desired functionality. The objectives of OBSERV also include facilitating a smooth transition to a target system, and providing means for reusing specification, design, and code of systems and subsystems. We are particularly interested in handling embedded systems, which are likely to have concurrency and have some real-time requirements. The OBSERV prototyping language combines several paradigms to express the behavior of a system. The object-oriented approach provides the basic mechanism for building a system from a collection of objects, with well-defined interfaces between them. We use finite-state machines to model the behavior of individual objects. At a lower level, activities that occur within objects, either upon entry to a state or in transition between thus allowing a nonprocedural description. The environment provided to a prototype builder is as important as the language. We have made an attempt to provide flexible tools for executing or simulating the prototype being built, as well as for browsing and static checking. The first implementation of the tools was window based but not graphic. A graphic front end, name CRUISE, was developed afterwards. A simulation sequence focuses on a single object, which can be as complex as necessary, possibly the entire system, and expects all the interactions between it and the outside world to be achieved by communication between the simulator and the user. The simulator allows the user to easily switch back and forth from one object to another, simulating each object in isolation. To enable testing the behavior of a prototype in a realistic environment, it is possible to construct objects that imitate the environment objects. We also allow simulation of systems with missing pieces, by calling upon the user to simulate any such missing piece by himself. © 1992, ACM. All rights reserved.",browsers; concurrency; CRUISE; embedded systems; graphical user interface; interactive programming environments; logic programming; modeling with finite state machines; object-oriented approach; OBSERV; real time systems; simulator; software reuse; static checker,Computer simulation languages; Interactive computer graphics; Logic programming; Machine oriented languages; Object oriented programming; Real time systems; Software engineering; User interfaces; Utility programs; CRUISE graphical user interface; Finite state machines; Interactive programming environments; OBSERV prototyping language and environment; Rapid prototyping; Computer aided software engineering
Structural Testing of Rule-Based Expert Systems,1992,ACM Transactions on Software Engineering and Methodology (TOSEM),https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976814208&doi=10.1145%2f128894.128896&partnerID=40&md5=f880fe7578695c6e1ae791d50e4d7677,"Testing of rule-based expert systems has become a high priority for many organizations as the use of such systems proliferates. Traditional software teting techniques apply to some components of rule-based systems, e.g., the inference engine. However, to structurally test the rule base component requires new techniques or adaptations of existing ones. This paper describes one such adaptation: an extension of data flow path selection in which a graphical representation of a rule base is defined and evaluated. This graphical form, called a logical path graph, captures logical paths through a rule base. These logical paths create precisely the abstractions needed in the testing process. An algorithm for the construction of logical path graphs are analyzed. © 1992, ACM. All rights reserved.",basis path testing; data flow path selection; expert systems; rule bases; structured testing,
A Program Integration Algorithm that Accommodates Semantics-Preserving Transformations,1992,ACM Transactions on Software Engineering and Methodology (TOSEM),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0026890189&doi=10.1145%2f131736.131756&partnerID=40&md5=4a4dcdde27b18fbea1f589f03abe1e0d,"Given a program Base and two variants, A and B, each created by modifying separate copies of Base, the goal of program integration is to determine whether the modifications interfere, and if they do not, to create an integrated program that includes both sets of changes as well as the portions of Base preserved in both variants. Text-based integration techniques, such as the one used by the Unix diff3 utility, are obviously unsatisfactory because one has no guarantees about how the execution behavior of the integrated program relates to the behaviors of Base, A, and B. The first program-integration algorithm to provide such guarantees was developed by Horwitz et al.[13]. However, a limitation of that algorithm is that it incorporates no notion of semantics-preserving transformations. This limitation causes the algorithm to be overly conservative in its definition of interference. For example, if one variant changes the way a computation is performed 1992 while the other variant adds code that uses the result of the computation, the algorithm would classify those changes as interfering. This paper describes a new integration algorithm that is able to accommodate semantics-preserving transformations. © 1992, ACM. All rights reserved.",coarsest partition; control dependence; data dependence; data-flow analysis; flow dependence; program dependence graph; program integration; program representation graph; static-single-assignment form,Computer programming; Computer programming languages; Computer software; Data handling; Data structures; Encoding (symbols); File editors; Modification; Program processors; Software engineering; Program integration; Semantics preserving transformations; Software package Base; Algorithms
Building Integrated Software Development Environments. Part I: Tool Specification,1992,ACM Transactions on Software Engineering and Methodology (TOSEM),https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976653492&doi=10.1145%2f128894.128895&partnerID=40&md5=e18d502376a45ef543e41df0cfbde915,"The conceptual modeling approach of the IPSEN 1992 project for building highly integrated environments is based on using attributed graphs to model and implement arbitrary object structures, in particular all kinds of software documents and their relationships. A language based on graph grammars, called PROGRESS (PROgrammed Graph REwriting SyStems), and a suitable method for the application of this language, called graph grammar engineering, have been developed over the last ten years. This language and method are being extensively used for specifying the complex graph structures of internal document representations as well as for specifying the functionality of all tools (editors, browsers, analyzers, debuggers) working on these internal rpresentations. This paper explains the language and the method for applying the language based on a pragmatic nontrivial example of a software production process and its corresponding documents. In particular, it is shown why and how a graph grammar-based strongly typed language is perfectly suitable to formally specify highly integrated software tools. In addition, it is shown that the implementation of these tools (i.e., an environment composed of these tools) is systematically being derived from the formal specifications. © 1992, ACM. All rights reserved.",attribute grammars; attributed graphs; environment generators; graph grammars,
Computing Similarity in a Reuse Library System: An AI-Based Approach,1992,ACM Transactions on Software Engineering and Methodology (TOSEM),https://www.scopus.com/inward/record.uri?eid=2-s2.0-0026884536&doi=10.1145%2f131736.131739&partnerID=40&md5=382d14138ff913368041355633ef7cff,"This paper presents an AI based library system for software reuse, called AIRS, that allows a developer to browse a software library in search of components that best meet some stated requirement. A component is described by a set of 1992 pairs. A feature represents a classification criterion, and is defined by a set of related terms. The system allows to represent packages (logical units that group a set of components) which are also described in terms of features. Candidate reuse components and packages are selected from the library based on the degree of similarity between their descriptions and a given target description. Similarity is quantified by a nonnegative magnitude (distance) proportional to the effort required to obtain the target given a candidate. Distances are computed by comparator functions based on the subsumption, closeness, and package relations. We present a formalization of the concepts on which the AIRS system is based. The functionality of a prototype implementation of the AIRS system is illustrated by application to two different software libraries: a set of Ada packages for data structure manipulation, and a set of C components for use in Command, Control, and Information Systems. Finally, we discuss some of the ideas we are currently exploring to automate the construction of AIRS classification libraries. © 1992, ACM. All rights reserved.",facet classification; similarity-based retrieval,Ada (programming language); Artificial intelligence; C (programming language); Components; Computer programming; Computer software; Data description; Data structures; Information retrieval; Performance; AIRS; Facet classification; Reuse components; Reuse library; Similarity; Similarity based retrieval; Software libraries; Software reuse; Computer aided software engineering
On Statecharts with Overlapping,1992,ACM Transactions on Software Engineering and Methodology (TOSEM),https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976769596&doi=10.1145%2f136586.136589&partnerID=40&md5=6ba15e9f2d3d5c923608e59ec83a603f,"The problem of extending the language of statecharts to include overlapping states is considered. The need for such an extension is motivated and the subtlety of the problem is illustrated by exhibiting the shortcomings of naive approaches. The syntax and formal semantics of our extension are then presented, showing in the process that the definitions for conventional statecharts constitute a special case. Our definitions are rather complex, a fact that we feel points to the inherent difficulty of such an extension. We thus prefer to leave open the question of whether or not it should be adopted in practice. © 1992, ACM. All rights reserved.",higraphs; reactive systems; statecharts; visual language,
