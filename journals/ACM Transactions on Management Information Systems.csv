Title,Year,Source title,Link,Abstract,Author Keywords,Index Keywords
"The netflix recommender system: Algorithms, business value, and innovation",2015,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84953409830&doi=10.1145%2f2843948&partnerID=40&md5=1fd4643d597dc44a206461cfb1cbfc15,"This article discusses the various algorithms that make up the Netflix recommender system, and describes its business purpose. We also describe the role of search and related algorithms, which for us turns into a recommendations problem as well. We explain the motivations behind and review the approach that we use to improve the recommendation algorithms, combining A/B testing focused on improving member retention andmedium term engagement, as well as offline experimentation using historical member engagement data. We discuss some of the issues in designing and interpreting A/B tests. Finally, we describe some current areas of focused innovation, which include making our recommender system global and language aware.",,Algorithms; A/b testing; Business value; Member retentions; Netflix; Offline; Recommendation algorithms; Related algorithms; Recommender systems
On the role of structural holes in requirements identification: An exploratory study on open-source software development,2015,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84942051662&doi=10.1145%2f2795235&partnerID=40&md5=709e372a4e638f1eae6bd74aa472160b,"Requirements identification is a human-centric activity that involves interaction among multiple stakeholders. Traditional requirements engineering (RE) techniques addressing stakeholders' social interaction are mainly part of a centralized process intertwined with a specific phase of software development. However, in open-source software (OSS) development, stakeholders' social interactions are often decentralized, iterative, and dynamic. Little is known about new requirements identification in OSS and the stakeholders' organizational arrangements supporting such an activity. In this article, we investigate the theory of structural hole from the context of contributing new requirements in OSS projects. Structural hole theory suggests that stakeholders positioned in the structural holes in their social network are able to produce new ideas. In this study, we find that structural hole positions emerge in stakeholders' social network and these positions are positively related to contributing a higher number of new requirements. We find that along with structural hole positions, stakeholders' role is also an important part in identifying new requirements. We further observe that structural hole positions evolve over time, thereby identifying requirements to realize enriched features. Our work advances the fundamental understanding of the RE process in a decentralized environment and opens avenues for improved techniques supporting this process. © 2015 ACM 2158-656X/2015/09-ART10 $15.00.",Brokerage; Open-source requirements engineering; Requirements identification; Social capital; Social information foraging theory; Stakeholders' social network; Structural hole,Economic and social effects; Iterative methods; Open systems; Requirements engineering; Social networking (online); Software design; Brokerage; Open sources; Social capitals; Social information; Structural holes; Open source software
Minimizing organizational user requirement while meeting security constraints,2015,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84942056106&doi=10.1145%2f2811269&partnerID=40&md5=4434a7a754fd01cf6cde872e311cdeb6,"Large systems are complex and typically need automatic configuration to be managed effectively. In any organization, numerous tasks have to be carried out by employees. However, due to security needs, it is not feasible to directly assign any existing task to the first available employee. In order to meet many additional security requirements, constraints such as separation of duty, cardinality and binding have to be taken into consideration. Meeting these requirements imposes extra burden on organizations, which, however, is unavoidable in order to ensure security. While a trivial way of ensuring security is to assign each user to a single task, business organizations would typically like to minimize their costs and keep staffing requirements to a minimum. To meet these contradictory goals, we define the problem of Cardinality Constrained-Mutually Exclusive Task Minimum User Problem (CMUP), which aims to find the minimum users that can carry out a set of tasks while satisfying the given security constraints. We show that the CMUP problem is equivalent to a constrained version of the weak chromatic number problem in hypergraphs, which is NP-hard. We, therefore, propose a greedy solution. Our experimental evaluation shows that the proposed algorithm is both efficient and effective. © 2015 ACM 2158-656X/2015/09-ART12 $15.00.",Constraints; Hypergraph; User task assignments; Weak chromatic number problem,Personnel; Automatic configuration; Business organizations; Chromatic number; Constraints; Experimental evaluation; Hypergraph; Security requirements; Task assignment; Constraint satisfaction problems
RT @news: An analysis of News agency ego networks in a microblogging environment,2015,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84945175305&doi=10.1145%2f2811270&partnerID=40&md5=a16501745dceed986b67c2457bdce30c,"News agencies regularly use Twitter to publicize and increase readership of their articles. Although substantial research on the spread of news on Twitter exists, there hasn't been much focus on the study of the spread of news articles. In this study, we present an innovative methodology involving weighted ego networks to understand how news agencies propagate news articles using their Twitter handle. We propose a set of measures to compare the propagation process of different news agencies by studying important aspects such as volume, extent of spread, conversion rate, multiplier effect, lifespan, hourly response, and audience participation. Using a dataset of tweets collected over a period of 6 months, we apply our methodology and suggest a framework to help news agencies gauge their performance on social media and also provide critical insights into the phenomenon of news article propagation on Twitter. © 2015 ACM 2158-656X/2015/09-ART11 $15.00.",Article propagation; Microblogging; News propagation; Twitter,Hardware; Information systems; Conversion rates; Innovative methodologies; Microblogging; Multiplier effects; News agencies; News articles; Propagation process; Twitter; Social networking (online)
"Relationships among minimum requirements, facebook likes, and groupon deal outcomes",2015,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84945186708&doi=10.1145%2f2764919&partnerID=40&md5=80e2495b82addb8c70e0d1d75195ce7a,"Daily deal coupons have gained a prominent foothold on the web. The earliest and largest player is Groupon. Originally,Groupon dealswere a mix of deals with a minimum requirement (MR) of coupon sales before a deal became effective and of deals without a minimum requirement (NMR). Eventually, Groupon stopped using MR deals. For Groupon and its retailer customers, might this decision have actually resulted in negative impacts for both parties (fewer coupons sold and lower revenue)? The structure of Groupon deals (including a ""Facebook like"" option) together with electronic access to the necessary data offered the opportunity to empirically investigate these questions.We analyzed relationships among MR, Facebook likes (FL), quantity of coupons sold, and total revenue, performing the analysis across the four largest retail categories. Using timestamped empirical data, we completed a propensity score analysis of causal effects. We find that the presence of MR increases Facebook likes, quantity of coupons sold, and total revenue at the time point when the MR is met and at subsequent 2-hour intervals over the horizon of deals. A key finding is that the initial differences observed when MR is met not only continue but also actually increase over the life of the deals. © 2015 ACM.",Daily deal sites; e-WOM; Groupon; Propensity score analysis (PSA); Social media,Sales; Daily deal sites; Electronic access; Empirical data; Groupon; Minimum requirements; Over the horizons; Propensity score; Social media; Social networking (online)
Who's next? Scheduling personalization services with variable service times,2015,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84937886534&doi=10.1145%2f2764920&partnerID=40&md5=93f57d6925ae302ddf0b232d84e17264,"Online personalization has become quite prevalent in recent years, with firms able to derive additional profits from such services. As the adoption of such services grows, firms implementing such practices face some operational challenges. One important challenge lies in the complexity associated with the personalization process and how to deploy available resources to handle such complexity. The complexity is exacerbated when a site faces a large volume of requests in a short amount of time, as is often the case for e-commerce and content delivery sites. In such situations, it is generally not possible for a site to provide perfectly personalized service to all requests. Instead, a firm can provide differentiated service to requests based on the amount of profiling information available about the visitor. We consider a scenario where the revenue function is concave, capturing the diminishing returns from personalization effort. Using a batching approach, we determine the optimal scheduling policy (i.e., time allocation and sequence of service) for a batch that accounts for the externality cost incurred when a request is provided service before other waiting requests. The batching approach leads to sunk costs incurred when visitors wait for the next batch to begin. An optimal admission control policy is developed to prescreen new request arrivals. We show how the policy can be implemented efficiently when the revenue function is complex and there are a large number of requests that can be served in a batch. Numerical experiments show that the proposed approach leads to substantial improvements over a linear approximation of the concave revenue function. Interestingly, we find that the improvements in firm profits are not only (or primarily) due to the different service times that are obtained when using the nonlinear personalization function - there is a ripple effect on the admission control policy that incorporates these optimized service times, which contributes even more to the additional profits than the service time optimization by itself. © 2015 ACM.",Admission control policy; Batch processing; Hierarchical decision tables; Online personalization; Resource allocation,Batch data processing; Decision tables; Profitability; Resource allocation; Scheduling; Social networking (online); Admission control policies; Differentiated Services; Hierarchical decisions; Linear approximations; Operational challenges; Personalization services; Personalizations; Profiling informations; Economics
Understanding business ecosystem dynamics: A data-driven approach,2015,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84937791797&doi=10.1145%2f2724730&partnerID=40&md5=41818c2b9b9b2daadf81593e4e36e7ea,"Business ecosystems consist of a heterogeneous and continuously evolving set of entities that are interconnected through a complex, global network of relationships. However, there is no well-established methodology to study the dynamics of this network. Traditional approaches have primarily utilized a single source of data of relatively established firms; however, these approaches ignore the vast number of relevant activities that often occur at the individual and entrepreneurial levels. We argue that a data-driven visualization approach, using both institutionally and socially curated datasets, can provide important complementary, triangulated explanatory insights into the dynamics of interorganizational networks in general and business ecosystems in particular. We develop novel visualization layouts to help decision makers systemically identify and compare ecosystems. Using traditionally disconnected data sources on deals and alliance relationships (DARs), executive and funding relationships (EFRs), and public opinion and discourse (POD), we empirically illustrate our data-driven method of data triangulation and visualization techniques through three cases in the mobile industry Google's acquisition of Motorola Mobility, the coopetitive relation between Apple and Samsung, and the strategic partnership between Nokia and Microsoft. The article concludes with implications and future research opportunities. © 2015 ACM 2158-656X/2015/05-ART6 $15.00.",Business ecosystem; Data triangulation; Information visualization; Interorganizational networks,Complex networks; Decision making; Dynamics; Ecology; Ecosystems; Information systems; Social aspects; Surveying; Triangulation; Visualization; Business ecosystem; Data-driven approach; Information visualization; Inter-organizational network; Research opportunities; Strategic partnership; Traditional approaches; Visualization technique; Data visualization
Role-based process view derivation and composition,2015,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84937856035&doi=10.1145%2f2744207&partnerID=40&md5=0643ed44977705a799d3b9189ea55b21,"The process view concept deploys a partial and temporal representation to adjust the visible view of a business process according to various perception constraints of users. Process view technology is of practical use for privacy protection and authorization control in process-oriented business management. Owing to complex organizational structure, it is challenging for large companies to accurately specify the diverse perception of different users over business processes. Aiming to tackle this issue, this article presents a rolebased process viewmodel to incorporate role dependencies into process view derivation. Compared to existing process view approaches, ours particularly supports runtime updates to the process view perceivable to a user with specific view merging operations, thereby enabling the dynamic tracing of process perception. A series of rules and theorems are established to guarantee the structural consistency and validity of process view transformation. A hypothetical case is conducted to illustrate the feasibility of our approach, and a prototype is developed for the proof-of-concept purpose. © 2015 ACM.",Business process view; Collaborative business process; Process perception,Information systems; Authorization controls; Business management; Business Process; Collaborative business process; Organizational structures; Privacy protection; Proof of concept; Temporal representations; Hardware
Stakeholder analyses of firm-related web forums: Applications in stock return prediction,2015,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84927514705&doi=10.1145%2f2675693&partnerID=40&md5=5b801b7132d123185d6fc10a86b154e8,"In this study, we present stakeholder analyses of firm-related web forums. Prior analyses of firm-related forums have considered all participants in the aggregate, failing to recognize the potential for diversity within the populations. However, distinctive groups of forum participants may represent various interests and stakes in a firm worthy of consideration. To perform the stakeholder analyses, the Stakeholder Analyzer system for firm-relatedweb forums is developed following the design science paradigm of information systems research. The design of the system and its approach to stakeholder analysis is guided by two kernel theories, the stakeholder theory of the firm and the systemic functional linguistic theory. A stakeholder analysis identifies distinctive groups of forum participants with shared characteristics expressed in discussion and evaluates their specific opinions and interests in the firm. Stakeholder analyses are performed in six major firm-related forums hosted on Yahoo Finance over a 3-month period. The relationships between measures extracted from the forums and subsequent daily firm stock returns are examined using multiple linear regression models, revealing statistically significant indicators of firm stock returns in the discussions of the stakeholder groups of each firm with stakeholder-model-adjusted R2 values reaching 0.83. Daily stock return prediction is also performed for 31 trading days, and stakeholder models correctly predicted the direction of return on 67% of trading days and generated an impressive 17% return in simulated trading of the six firm stocks. These evaluations demonstrate that the stakeholder analyses provided more refined assessments of the firm-related forums, yielding measures at the stakeholder group level that better explain and predict daily firm stock returns than aggregate forum-level information. © 2015 ACM.",Sentiment analysis; Social media analytics; Stakeholder analysis; Stock prediction; Web forums,Aggregates; Commerce; Design; Forecasting; Information systems; Linear regression; Regression analysis; Social networking (online); Sentiment analysis; Social media analytics; Stakeholder analysis; Stock predictions; Web Forums; Investments
A network behavior-based botnet detection mechanism using PSO and K-means,2015,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84927557531&doi=10.1145%2f2676869&partnerID=40&md5=5811ef23e835d80e1d73386738a078f4,"In today's world, Botnet has become one of the greatest threats to network security. Network attackers, or Botmasters, use Botnet to launch the Distributed Denial of Service (DDoS) to paralyze large-scale websites or steal confidential data from infected computers. They also employ ""phishing"" attacks to steal sensitive information (such as users' accounts and passwords), send bulk email advertising, and/or conduct click fraud. Even though detection technology has been much improved and some solutions to Internet security have been proposed and improved, the threat of Botnet still exists. Most of the past studies dealing with this issue used either packet contents or traffic flow characteristics to identify the invasion of Botnet. However, there still exist many problems in the areas of packet encryption and data privacy, simply because Botnet can easily change the packet contents and flow characteristics to circumvent the Intrusion Detection System (IDS). This study combines Particle Swarm Optimization (PSO) and K-means algorithms to provide a solution to remedy those problems and develop, step by step, a mechanism for Botnet detection. First, three important network behaviors are identified: long active communication behavior (ActBehavior), connection failure behavior (FailBehavior), and network scanning behavior (ScanBehavior). These behaviors are defined according to the relevant prior studies and used to analyze the communication activities among the infected computers. Second, the features of network behaviors are extracted from the flow traces in the network layer and transport layer of the network equipment. Third, PSO and K-means techniques are used to uncover the host members of Botnet in the organizational network. This study mainly utilizes the flow traces of a campus network as an experiment. The experimental findings show that this proposed approach can be employed to detect the suspicious Botnet members earlier than the detection application systems. In addition, this proposed approach is easy to implement and can be further used and extended in the campus dormitory network, home networks, and the mobile 3G network. © 2015 ACM.",Botnet; K-means clustering; Network traffic analysis; Particle swarm optimization,Computer crime; Data privacy; Denial-of-service attack; Home networks; Intrusion detection; Malware; Mobile security; Network layers; Particle swarm optimization (PSO); Personal communication systems; Security of data; Botnet; Communication activities; Distributed denial of service; Intrusion Detection Systems; K-means clustering; Network traffic analysis; Organizational network; Traffic flow characteristics; Network security
Investigating task coordination in globally dispersed teams: A structural contingency perspective,2015,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930146230&doi=10.1145%2f2688489&partnerID=40&md5=fefa25e97838a158c94761c8c81d9d7c,"Task coordination poses significant challenges for globally dispersed teams (GDTs). Although various task coordination mechanisms have been proposed for such teams, there is a lack of systematic examination of the appropriate coordination mechanisms for different teams based on the nature of their task and the context under which they operate. Prior studies on collocated teams suggest matching their levels of task dependence to specific task coordination mechanisms for effective coordination. This research goes beyond the earlier work by also considering additional contextual factors of GDT (i.e., temporal dispersion and time constraints) in deriving their optimal IT-mediated task coordination mechanisms. Adopting the structural contingency theory, we propose optimal IT-mediated task coordination portfolios to fit the different levels of task dependence, temporal dispersion, and perceived time constraint of GDTs. The proposed fit is tested throughasurvey and profile analysis of95 globally dispersed software development teamsina largefinancial organization. We find, as hypothesized, that the extent of fit between the actual IT-mediated task coordination portfolios used by the surveyed teams and their optimal portfolios proposed here is positively related to their task coordination effectiveness that in turn impacts the team's efficiency and effectiveness. The implications for theory and practice are discussed. © 2015 ACM.",Fit analysis; Globally dispersed team; IT-mediated task coordination portfolio; Perceived time constraint; Task dependence; Temporal dispersion,Dispersions; Software design; Software testing; Fit analysis; Globally-dispersed teams; Task coordination; Task dependence; Temporal dispersion; Time constraints; Human resource management
A case study of data quality in text mining clinical progress notes,2015,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84927514407&doi=10.1145%2f2669368&partnerID=40&md5=eaa99bca06c1af7c6a4048d1c903c5ad,"Text analytic methods are often aimed at extracting useful information from the vast array of unstructured, free format text documents that are created by almost all organizational processes. The success of any text mining application rests on the quality of the underlying data being analyzed, including both predictive features and outcome labels. In this case study, some focused experiments regarding data quality are used to assess the robustness of Statistical Text Mining (STM) algorithms when applied to clinical progress notes. In particular, the experiments consider the impacts of task complexity (by removing signals), training set size, and target outcome quality. While this research is conducted using a dataset drawn from the medical domain, the data quality issues explored are of more general interest. © 2015 ACM.",Clinical progress notes; Data quality; Electronic health records; Feature selection; Health informatics; Machine learning; Noisy text analysis; Predictive model quality; Text mining,Artificial intelligence; Data reduction; Feature extraction; Learning systems; Clinical progress notes; Data quality; Electronic health record; Health informatics; Predictive modeling; Text analysis; Text mining; Data mining
Editorial: Business Process intelligence: Connecting data and processes,2015,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929169663&doi=10.1145%2f2685352&partnerID=40&md5=1552dcecd762d2161d147522b721c561,"This introduction to the special issue on Business Process Intelligence (BPI) discusses the relation between data and processes. The recent attention for Big Data illustrates that organizations are aware of the potential of the torrents of data generated by today's information systems. However, at the same time, organizations are struggling to extract value from this overload of data. Clearly, there is a need for data scientists able to transform event data into actionable information. To do this, it is crucial to take a process perspective. The ultimate goal of BPI is not to improve information systems or the recording of data; instead the focus should be in improving the process. For example, we may want to aim at reducing costs, minimizing response times, and ensuring compliance. This requires a ""confrontation"" between process models and event data. Recent advances in process mining allow us to automatically learn process models showing the bottlenecks from ""raw"" event data. Moreover, given a normative model, we can use conformance checking to quantify and understand deviations. Automatically learned models may also be used for prediction and recommendation. BPI is rapidly developing as a field linking data science to business process management. This article aims to provide an overview thereby paving the way for the other contributions in this special issue. © 2015 ACM.",Business Process Intelligence; Compliance checking; Performance analysis; Process mining; Process modeling,Administrative data processing; Compliance control; Data mining; Enterprise resource management; Information systems; Business Process Intelligence; Compliance checking; Performance analysis; Process mining; Process Modeling; Big data
Ontology-based mapping for automated document management: A concept-based technique for word mismatch and ambiguity problems in document clustering,2015,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84924649643&doi=10.1145%2f2688488&partnerID=40&md5=619e4a5a93cb9ba6ad19d8566e4bbf7d,"Document clustering is crucial to automated document management, especially for the fast-growing volume of textual documents available digitally. Traditional lexicon-based approaches depend on document content analysis and measure overlap of the feature vectors representing different documents, which cannot effectively address word mismatch or ambiguity problems. Alternative query expansion and local context discovery approaches are developed but suffer from limited efficiency and effectiveness, because the large number of expanded terms create noise and increase the dimensionality and complexity of the overall feature space. Several techniques extend lexicon-based analysis by incorporating latent semantic indexing but produce less comprehensible clustering results and questionable performance. We instead propose a conceptbased document representation and clustering (CDRC) technique and empirically examine its effectiveness using 433 articles concerning information systems and technology, randomly selected from a popular digital library. Our evaluation includes two widely used benchmark techniques and shows that CDRC outperforms them. Overall, our results reveal that clustering documents at an ontology-based, concept-based level is more effective than techniques using lexicon-based document features and can generate more comprehensible clustering results. © 2015 ACM.",Document clustering; Document-category management; Knowledge management; Ontology-supported document clustering,Cluster analysis; Clustering algorithms; Digital libraries; Information retrieval; Information services; Information use; Knowledge management; Semantics; Alternative queries; Clustering documents; Document Clustering; Document Representation; Document-category managements; Information systems and technologies; Latent Semantic Indexing; Ontology-supported document clustering; Ontology
Getting to the shalls: Facilitating Sensemaking in Requirements Engineering,2015,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84922586949&doi=10.1145%2f2629351&partnerID=40&md5=75f7072e7b433c87a0f125baa4feb14b,"Sensemaking in Requirements Engineering (RE) relies on knowledge transfer, communication, and negotiation of project stakeholders. It is a critical and challenging aspect of Information Systems (IS) development. One of the most fundamental aspects of RE is the specification of traceable, unambiguous, and operationalizable functional and nonfunctional requirements. This remains a nontrivial task in the face of the complexity inherent in RE due to the lack of well-documented, systematic procedures that facilitate a structured analysis of the qualitative data from stakeholder interviews, observations, and documents that are typically the input to this activity. This research develops a systematic and traceable procedure, for non-functional requirements the Grounded and Linguistic-Based Requirements Analysis Procedure (GLAP), which can fill this gap by incorporating perspectives from Grounded Theory Method, linguistic analysis of language quality, Volere typology, and the Nonfunctional Requirements Framework without significantly deviating from existing practice. The application of GLAP is described along with empirical illustrations using RE data from a redesign initiative of a library website of a public university in the United States. An outlook is given on further work and necessary evaluation steps. © 2015 ACM.",Grounded theory methodology; Linguistic analysis; Nonfunctional Requirements Framework; Qualitative analysis,Computational linguistics; Knowledge management; Linguistics; Requirements engineering; Grounded theory; Grounded theory methods; Linguistic analysis; Non-functional requirements; Project stakeholders; Qualitative analysis; Requirements analysis; Structured analysis; Quality control
Complications with complexity in requirements,2015,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84922578188&doi=10.1145%2f2629375&partnerID=40&md5=7b9249eb90a82c9b8df0004e929de6b9,"Requirements engineering must recognize the difference between complicated and complex problems. The former can lead to successful solutions. The latter should be avoided because they often lead to failure. As a starting point for distinguishing between complicated and complex, this article offers six characteristics of complex problems, with examples from economics, logistics, forecasting, among others. These characteristics make it easier and more systematic to recognize complexity during requirements elicitation and formulation. © 2015 ACM 2158-656X/2015/01-ART13 $15.00.",Complex; Complicated; Information systems; Requirements; Software,Computer software; Hardware; Information systems; Complex; Complex problems; Complicated; Requirements; Requirements elicitation; Requirements engineering
Process mining for clinical processes: A comparative analysis of four australian hospitals,2015,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84925629090&doi=10.1145%2f2629446&partnerID=40&md5=4953ed0c1169ca7d0616e073b516d66d,"Business process analysis and process mining, particularly within the health care domain, remain underutilized. Applied research that employs such techniques to routinely collected health care data enables stakeholders to empirically investigate care as it is delivered by different health providers. However, crossorganizational mining and the comparative analysis of processes present a set of unique challenges in terms of ensuring population and activity comparability, visualizing the mined models, and interpreting the results. Without addressing these issues, health providers will find it difficult to use process mining insights, and the potential benefits of evidence-based process improvement within health will remain unrealized. In this article, we present a brief introduction on the nature of health care processes, a review of process mining in health literature, and a case study conducted to explore and learn how health care data and crossorganizational comparisons with process-mining techniques may be approached. The case study applies process-mining techniques to administrative and clinical data for patients who present with chest pain symptoms at one of four public hospitals in South Australia. We demonstrate an approach that provides detailed insights into clinical (quality of patient health) and fiscal (hospital budget) pressures in the delivery of health care. We conclude by discussing the key lessons learned from our experience in conducting business process analysis and process mining based on the data from four different hospitals. © 2015 ACM.",Comparative analysis; Data preparation; Health care delivery; Patient pathways; Process mining,Budget control; Health care; Hospitals; Business process analysis; Comparative analysis; Cross-organizational; Data preparation; Delivery of health care; Patient pathways; Process Improvement; Process mining; Data mining
On the discovery of declarative control flows for artful processes,2015,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84923644985&doi=10.1145%2f2629447&partnerID=40&md5=1c048e15272ab19cbe8f86ab6acfb9c8,"Artful processes are those processes in which the experience, intuition, and knowledge of the actors are the key factors in determining the decision making. They are typically carried out by the ""knowledge workers,"" such as professors, managers, and researchers. They are often scarcely formalized or completely unknown a priori. Throughout this article, we discuss how we addressed the challenge of discovering declarative control flows in the context of artful processes. To this extent, we devised and implemented a two-phase algorithm, named MINERful. The first phase builds a knowledge base, where statistical information extracted from logs is represented. During the second phase, queries are evaluated on that knowledge base, in order to infer the constraints that constitute the discovered process. After outlining the overall approach and offering insight on the adopted process modeling language, we describe in detail our discovery technique. Thereupon, we analyze its performances, both from a theoretical and an experimental perspective. A user-driven evaluation of the quality of results is also reported on the basis of a real case study. Finally, a study on the fitness of discovered models with respect to synthetic and real logs is presented. © 2015 ACM.",Artful processes; Control-flow discovery; Declarative process model; MailOfMine; Process mining,Decision making; Human resource management; Knowledge based systems; Modeling languages; Quality control; Artful process; Control flows; Declarative process models; MailOfMine; Process mining; Process control
Mining agents' goals in agent-oriented business processes,2015,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84925642700&doi=10.1145%2f2629448&partnerID=40&md5=88f03a7bc9325c3354ff540109b1c8eb,"When designing a business process, individual agents are assigned to perform tasks based on certain goals (i.e., designed process goals). However, based on their own interests, real-world agents often have different goals (i.e., agents' goals) and thus may behave differently than designed, often resulting in reduced effectiveness or efficiencies of the executed process. Moreover, existing business process research lacks effective methods for discovering agents' goals in the actual execution of the designed business processes. To address this problem, we propose an agent-oriented goal mining approach to modeling, discovering, and analyzing agents' goals in executed business processes using historical event logs and domain data. To the best of our knowledge, our research is the first to adopt the agents' goal perspective to study inconsistencies between the design and execution of business processes. Moreover, it also provides a useful tool for stakeholders to discover real-world agents' actual goals and thus provides insights for improving the task assignment mechanism or business process design in general. © 2014 ACM.",Agent-oriented business process; Belief-desire-intention model; Goal mining,Human computer interaction; Intelligent agents; Management science; Agent-oriented; Belief-desire-intention models; Business Process; Business process design; Event logs; Individual agent; Process goals; Task assignment; Process design
Compliance checking of organizational interactions,2015,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84925683662&doi=10.1145%2f2629630&partnerID=40&md5=673ccb21c3b5d97259d162de1fc1b541,"In business environments, different sorts of regulations are imposed to restrict the behavior of both public and private organizations, ranging from legal regulations to internal policies. Regulatory compliance is important for the safety of individual actors as well as the overall business environment. However, complexity derives from not only the contents of the regulations but also their interdependencies. As such, the verification of whether actors are able to comply with the combined regulations cannot be done by checking each regulation separately. To these ends, we introduce a normative structure Norm Nets (NNs) for modeling sets of interrelated regulations and setting a basis for compliance checking of organizational interactions against interrelated regulations. NNs support a modular design by providing the constructs to represent regulations and the relationships between them. Additionally, we propose a computational mechanism to reason about regulatory compliance by mapping NNs to Colored Petri Nets (CPNs). We show that compliance checking of both individual actors' behavior and the collective behavior of the business environment can be achieved automatically using state space analysis techniques of CPNs. The approach is illustrated with a case study from the domain of international trade. © 2014 ACM.",Colored Petri Nets; Norm compliance; Regulated business environments,Compliant mechanisms; International trade; Laws and legislation; Petri nets; Regulatory compliance; Business environments; Collective behavior; Colored Petri Nets; Norm compliance; Normative structures; Organizational interactions; Private organizations; State-space analysis; Compliance control
Impediments to regulatory compliance of requirements in contractual systems engineering projects: A case study,2014,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84995575864&doi=10.1145%2f2629432&partnerID=40&md5=961d7499f4c29e6256c8d85ee2a6f301,"Large-scale contractual systems engineering projects often need to comply with myriad government regulations and standards as part of contractual obligations. A key activity in the requirements engineering (RE) process for such a project is to demonstrate that all relevant requirements have been elicited from the regulatory documents and have been traced to the contract as well as to the target system components. That is, the requirements have met regulatory compliance. However, there are impediments to achieving this level of compliance due to such complexity factors as voluminous contract, large number of regulatory documents, and multiple domains of the system. Little empirical research has been conducted in the scientific community on identifying these impediments. Knowing these impediments is a driver for change in the solutions domain (i.e., creating improved or new methods, tools, processes, etc.) to deal with such impediments. Through a case study of an industrial RE project, we have identified a number of key impediments to achieving regulatory compliance in a large-scale, complex, systems engineering project. This project is an upgrade of a rail infrastructure system. The key contribution of the article is a number of hitherto uncovered impediments described in qualitative and quantitative terms. The article also describes an artefact model, depicting key artefacts and relationships involved in such a compliance project. This model was created from data gathered and observations made in this compliance project. In addition, the article describes emergent metrics on regulatory compliance of requirements that can possibly be used for estimating the effort needed to achieve regulatory compliance of system requirements. © 2014 ACM.","Compliance of requirements; D.2.1 [software engineering]: requirements/specifications - elicitation methods; D.2.8 [software engineering]: metrics - complexity measures, product metrics; D.2.9 [software engineering]: management - Software quality assurance (SQA); Effort estimation; Impediments; K.5.2 [legal aspects of computing:]: governmental issues; Legal requirements elicitation; Rail infrastructure system; Regulatory compliance; Requirements Engineering; Systems engineering",Computer software selection and evaluation; Laws and legislation; Quality assurance; Requirements engineering; Software engineering; Standards; Systems engineering; Complexity measures; Compliance of requirements; D.2.9 [software engineering]: managements; Effort Estimation; Elicitation methods; Impediments; Legal aspects; Legal requirements; Rail infrastructure; Regulatory compliance
Process analytics approach for R&D project selection,2014,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84908004642&doi=10.1145%2f2629436&partnerID=40&md5=7bf36b3e1c874e1dfb46438f9c0729c7,"R&D project selection plays an important role in government funding agencies, as allocation of billions of dollars among the proposals deemed highly influential and contributive solely depend on it. Efficacious assignment of reviewers is one of the most critical processes that controls the quality of the entire project selection and also has a serious implication on business profit. Current methods that focus on workflow automation are more efficient than manual assignment; however, they are not effective, as they fail to consider the real insight of core tasks. Other decision models that analyze core tasks are effective but inefficient when handling large amounts of submissions, and they suffer from irrelevant assignment. Furthermore, they largely ignore real deep insight of back-end data such as quality of the reviewers (e.g., quality and citation impact of their produced research) and the effect of social relationships in project selection processes that are essential for identifying reviewers for interdisciplinary proposal evaluation. In light of these deficiencies, this research proposes a novel hybrid process analytics approach to decompose the complex reviewer assignment process into manageable subprocesses and applies data-driven decision models cum process analytics systematically from a triangular perspective via the research analytics framework to achieve high operational efficiencies and high-quality assignment. It also analyzes big data from scientific databases and generates visualized decision-ready information to support effective decision making. The proposed approach has been implemented to aid the project selection process of the largest funding agency in China and has been tested. The test results show that the proposed approach has the potential to add great benefits, including cost saving, improved effectiveness, and increased business value. © 2014 ACM.",Big data; Process analytics; Research analytics; Reviewer assignment,Big data; Decision making; Quality control; Data driven decision; Government funding; Operational efficiencies; Process analytics; Project selection; Reviewer assignment; Scientific database; Social relationships; Advanced Analytics
Classification models for RFID-based real-time detection of process events in the supply chain: An empirical study,2014,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84949146725&doi=10.1145%2f2629449&partnerID=40&md5=2a66d0821e192616bf838267770a77b4,"RFID technology allows the collecting of fine-grained real-time information on physical processes in the supply chain that often cannot be monitored using conventional approaches. However, because of the phenomenon of false-positive reads, RFID data streams resemble noisy analog measurements rather than the desired recordings of activities within a business process. The present study investigates the use of data mining techniques for filtering and aggregating raw RFID data. We consider classifiers based on logistic regression, decision trees, and artificial neural networks using attributes derived from low-level reader data. In addition, we present a custom-made algorithm for generating decision rules using artificial attributes and an iterative training procedure. We evaluate the classifiers using a massive set of data on pallet movements collected under real-world conditions at one of the largest retailers worldwide. The results clearly indicate high classification performance of the classification models, with the rule-based classifier outperforming all others. Moreover, we show that utilizing the full spectrum of data generated by the reader hardware leads to superior performance compared with the approaches based on timestamp and antenna information proposed in prior research. © 2014 ACM.",Artificial neural networks; Decision trees; False-positive reads; Logistic regression; Machine learning; RFID; Time series analysis,Decision Making; Neural Networks; Regression Analysis; Trees; Antennas; Data mining; Data reduction; Data streams; Decision trees; Forestry; Iterative methods; Learning systems; Logistic regression; Neural networks; Radio frequency identification (RFID); Signal detection; Supply chains; Time series analysis; Trees (mathematics); Artificial attributes; Classification models; Classification performance; Conventional approach; False positive; Real-time detection; Real-time information; Rule-based classifier; Classification (of information)
An analytical framework for understanding knowledge-sharing processes in online QandA communities,2014,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84919666639&doi=10.1145%2f2629445&partnerID=40&md5=47d2600d5b6fdc9042dd29c4599eae5c,"Online communities have become popular knowledge sources for both individuals and organizations. Computer-mediated communication research shows that communication patterns play an important role in the collaborative efforts of online knowledge-sharing activities. Existing research is mainly focused on either user egocentric positions in communication networks or communication patterns at the community level. Very few studies examine thread-level communication and process patterns and their impacts on the effectiveness of knowledge sharing. In this study, we fill this research gap by proposing an innovative analytical framework for understanding thread-level knowledge sharing in online Q&A communities based on dialogue act theory, network analysis, and process mining. More specifically, we assign a dialogue act tag for each post in a discussion thread to capture its conversation purpose and then apply graph and process mining algorithms to examine knowledge-sharing processes. Our results, which are based on a real support forum dataset, show that the proposed analytical framework is effective in identifying important communication, conversation, and process patterns that lead to helpful knowledge sharing in online Q&A communities © 2014 ACM.",,Data mining; Graph algorithms; Communication pattern; Computer mediated communication; Effectiveness of knowledge; Knowledge sources; Knowledge-sharing; On-line communities; Online knowledge sharing; Process patterns; Knowledge management
A dataflow perspective for business process integration,2014,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84907982047&doi=10.1145%2f2629450&partnerID=40&md5=165accf343a61f84252208fe5d237e51,"Business process integration has become prevalent as business increasingly crosses organizational boundaries. To address the issue of protecting organizations' competitive knowledge and private information while also enabling business-to-business (B2B) collaboration, past research has focused mainly on customized public and private process design, as well as structural correctness of the integrated workflow. However, a dataflow perspective is important for business process integration. This article presents a data-flow perspective using workflow management and mathematical techniques to address data exchange problems in independent multistakeholder business process integration in dynamic circumstances. The research is conducted following a design science paradigm. We build artifacts that include interorganizational workflow concepts, a workflow model, and a public dataset calculation method. The use of the proposed artifacts is illustrated by applying them to a real-world case in the Shenzhen (Chaiwan) port. The utility of the artifacts is evaluated through interviews with practitioners in industry. We conclude that this research complements the control-flow perspective in the interorganizational workflow management area and also contributes to B2B information-sharing literature; further, the dataflow formalism can help practitioners to formally provide the right data at the right time in dynamic circumstances. © 2014 ACM.",Dataflow; Process integration,Electronic data interchange; Process design; Work simplification; Business process integration; Business to business; Dataflow; Inter-Organizational workflow; Organizational boundaries; Private information; Process integration; Workflow managements; Information management
Improving business value assurance in large-scale IT projects - A quantitative method based on founded requirements assessment,2014,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84907580962&doi=10.1145%2f2638544&partnerID=40&md5=e24ba780e1f574495666cc25a7982561,"The probability of IT project failures can be mitigated more successfully when discovered early. To support a more insightful management of IT projects, which may also facilitate an early detection of IT project failures, transparency regarding a project's cash flows shall be increased. Therefore, an appropriate analysis of a project's benefits, costs, requirements, their respective risks and interdependencies is inevitable. However, to date, in requirements engineering only few methods exist that appropriately consider these factors when estimating the ex ante project business case. Furthermore, empirical studies reveal that a lot of risk factors emerge during the runtime of projects why the ex ante valuation of IT projects even with respect to requirements seems insufficient. Therefore, using the Action Design Research approach, we design, apply, and evaluate a practicable method for value-based continuous IT project steering especially for large-scale IT projects. © 2014 ACM.",Action design research; Business value of IT; IT project controlling; Requirements engineering; Value assurance,Project management; Requirements engineering; Business value; Business value of it; Design research; Empirical studies; IT project; Project business; Quantitative method; Value assurance; Risk assessment
The requirements problem for adaptive systems,2014,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84907583131&doi=10.1145%2f2629376&partnerID=40&md5=022cdc30a8e5d827ae8625d5dc739233,"Requirements Engineering (RE) focuses on eliciting, modeling, and analyzing the requirements and environment of a system-to-be in order to design its specification. The design of the specification, known as the Requirements Problem (RP), is a complex problem-solving task because it involves, for each new system, the discovery and exploration of, and decision making in a new problem space. A system is adaptive if it can detect deviations between its runtime behavior and its requirements, specifically situations where its behavior violates one or more of its requirements. Given such a deviation, an Adaptive System uses feedback mechanisms to analyze these changes and decide, with or without human intervention, how to adjust its behavior as a result. We are interested in defining the Requirements Problem for Adaptive Systems (RPAS). In our case, we are looking for a configurable specification such that whenever requirements fail to be fulfilled, the system can go through a series of adaptations that change its configuration and eventually restore fulfilment of the requirements. From a theoretical perspective, this article formally shows the fundamental differences between standard RE (notably Zave and Jackson [1997]) and RE for Adaptive Systems (see the seminal work by Fickas and Feather [1995], to Letier and van Lamsweerde [2004], and up to Whittle et al. [2010]). The main contribution of this article is to introduce the RPAS as a new RP class that is specific to Adaptive Systems. We relate the RPAS to RE research on the relaxation of requirements, the evaluation of their partial satisfaction, and the monitoring and control of requirements, all topics of particular interest in research on adaptive systems [de Lemos et al. 2013]. From an engineering perspective, we define a protoframework for solving RPAS, which illustrates features needed in future frameworks for adaptive software systems. © 2014 ACM.",Adaptive systems; Requirements engineering; Requirements modelling language; Requirements problem; Requirements problem for adaptive systems,Adaptive systems; Decision making; Feedback; Modeling languages; Requirements engineering; Specifications; Adaptive software systems; Adaptive system use; Complex problem solving tasks; Engineering perspective; Feedback mechanisms; Monitoring and control; Requirements modelling; Requirements problem; Adaptive control systems
"Editorial: ""Complexity of systems evolution: Requirements engineering perspective""",2014,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84922581363&doi=10.1145%2f2629597&partnerID=40&md5=efed92ec705531e9861dd8a08a074906,"This introduction discusses the changing nature of complexity associated with requirements engineering (RE) tasks and how it has shifted from managing internal complexity to adapting and leveraging upon external and dynamic complexity. We note several significant drivers in the requirements knowledge that have resulted in this change and discuss in light of complexity theory how the RE research community can respond to this. We observe several research challenges associated with ""new complexity"" and highlight how the articles included in the special issue advance the field by defining complexity more accurately, observing more vigilantly new sources of complexity, and suggesting new ways to manage complexity in terms of economic assessments, knowledge flows, and modeling for adaptability. © 2015 ACM 2158-656X/2015/01-ART12 $15.00.",Agility; Complexity; Edge computing; Requirements engineering; Requirements knowledge; System evolution,Hardware; Information systems; Agility; Complexity; Edge computing; Requirements knowledge; System evolution; Requirements engineering
Situated boundary spanning: An empirical investigation of requirements engineering practices in product family development,2014,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84922945428&doi=10.1145%2f2629395&partnerID=40&md5=8d4e0d98ef21711dc8c7a03c73c86ac3,"Requirements Engineering (RE) faces considerable challenges that are often related to boundaries between various stakeholders involved in the software development process. These challenges may be addressed by boundary spanning practices. We examine how boundary spanning can be adapted to address RE challenges in Product Family Development (PFD), a context that involves complex RE. We study two different development approaches, namely, conventional and agile PFD, because these present considerably different challenges. Our findings from a multisite case study present boundary spanning as a solution to improve the quality of RE processes and highlight interesting differences in how boundary spanner roles and boundary objects are adapted in conventional and agile PFD. © 2014 ACM.",Agile development; Boundary spanning; Product family development; Requirements engineering,Requirements engineering; Software engineering; Agile development; Boundary spanning; Boundary-spanning practices; Development approach; Empirical investigation; Product family development; Situated boundaries; Software development process; Software design
Swineflu: The use of twitter as an early warning and risk communication tool in the 2009 Swine Flu Pandemic,2014,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84905728097&doi=10.1145%2f2597892&partnerID=40&md5=8edb0be472018c52c8e6e585fe8317c5,"The need to improve population monitoring and enhance surveillance of infectious diseases has never been more pressing. Factors such as air travel act as a catalyst in the spread of new and existing viruses. The unprecedented user-generated activity on social networks over the last few years has created real-time streams of personal data that provide an invaluable tool for monitoring and sampling large populations. Epidemic intelligence relies on constant monitoring of online media sources for early warning, detection, and rapid response; however, the real-time information available in social networks provides a new paradigm for the early warning function. The communication of risk in any public health emergency is a complex task for governments and healthcare agencies. This task is made more challenging in the current situation when the public has access to a wide range of online resources, ranging from traditional news channels to information posted on blogs and social networks. Twitter's strength is its two-way communication nature - both as an information source but also as a central hub for publishing, disseminating and discovering online media. This study addresses these two challenges by investigating the role of Twitter during the 2009 swine flu pandemic by analysing data collected from the SN, and by Twitter using the opposite way for dissemination information through the network. First, we demonstrate the role of the social network for early warning by detecting an upcoming spike in an epidemic before the official surveillance systems by up to two weeks in the U.K. and up to two to three weeks in the U.S. Second, we illustrate how online resources are propagated through Twitter at the time of theWHO's declaration of the swine flu ""pandemic"". Our findings indicate that Twitter does favour reputable t bogus information can still leak into the network. © 2014 ACM.",Data mining; Epidemic intelligence; Global health and well-being; Monitoring spread of disease; Real-time data management and public health response; Social media; Swine flu 2009 analysis; Twitter,Complex networks; Data mining; Epidemiology; Information dissemination; Information management; Public health; Viruses; Epidemic intelligence; Global health; Health response; Social media; Spread of disease; Swine flu 2009 analysis; Twitter; Social networking (online)
Building the nation's cyber security workforce: Contributions from the CAE colleges and universities,2014,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84905739731&doi=10.1145%2f2629636&partnerID=40&md5=4a08c3be9df670e8fbb9acbf9da5e67a,"This article presents a view of the necessary size and composition of the US national cyber security workforce, and considers some of the contributions that the government-designated Centers of Academic Excellence (CAE) might make to it. Over the last dozen years about 200 million taxpayer dollars have gone into funding many of these CAEs, with millions explicitly targeted to help them build capacity. The most visible intended output has been in the form of around 125 Scholarship for Service (SFS) students per year going mostly into the workforce of the federal government. Surely the output capacity of these 181 colleges and universities is greater than that, and should be helping to protect the rest of US citizens and taxpayers. We take a need-based look at what the nation's workforce should look like, and then consider some possibilities of what the CAE schools could be doing to help to close the gaps between that perceived need and the supply and demand. © 2014 ACM.",Economics; Management; Security,Economics; Education; Management; Societies and institutions; Taxation; Academic excellence; Colleges and universities; Cyber security; Federal governments; Security; Supply and demand; National security
Stock prediction by searching for similarities in candlestick charts,2014,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84905750500&doi=10.1145%2f2591672&partnerID=40&md5=dc3fa0c0ed99326fc4602d2c5eee23de,"The aim of stock prediction is to effectively predict future stock market trends (or stock prices), which can lead to increased profit. One major stock analysis method is the use of candlestick charts. However, candlestick chart analysis has usually been based on the utilization of numerical formulas. There has been no work taking advantage of an image processing technique to directly analyze the visual content of the candlestick charts for stock prediction. Therefore, in this study we apply the concept of image retrieval to extract seven different wavelet-based texture features from candlestick charts. Then, similar historical candlestick charts are retrieved based on different texture features related to the query chart, and the ""future"" stock movements of the retrieved charts are used for stock prediction. To assess the applicability of this approach to stock prediction, two datasets are used, containing 5-year and 10-year training and testing sets, collected from the Dow Jones Industrial Average Index (INDU) for the period between 1990 and 2009. Moreover, two datasets (2010 and 2011) are used to further validate the proposed approach. The experimental results show that visual content extraction and similarity matching of candlestick charts is a new and useful analytical method for stock prediction. More specifically, we found that the extracted feature vectors of 30, 90, and 120, the number of textual features extracted from the candlestick charts in the BMP format, are more suitable for predicting stock movements, while the 90 feature vector offers the best performance for predicting short- and medium-term stock movements. That is, using the 90 feature vector provides the lowest MAPE (3.031%) and Theil's U (1.988%) rates in the twenty-year dataset, and the best MAPE (2.625%, 2.945%) and Theil's U (1.622%, 1.972%) rates in the two validation datasets (2010 and 2011). © 2014 ACM.",Candlestick chart; Data mining; Stock prediction,Data mining; Financial markets; Graphic methods; Image processing; Textures; Analytical method; Candlestick chart; Different wavelets; Dow Jones Industrial averages; Image processing technique; Similarity-matching; Stock predictions; Training and testing; Forecasting
"An interactive, web-based high performance modeling environment for computational epidemiology",2014,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84905718008&doi=10.1145%2f2629692&partnerID=40&md5=0be2dcd7c5f5675752be88f4f9fce601,"We present an integrated interactive modeling environment to support public health epidemiology. The environment combines a high resolution individual-based model with a user-friendly Web-based interface that allows analysts to access the models and the analytics backend remotely from a desktop or a mobile device. The environment is based on a loosely coupled service-oriented-architecture that allows analysts to explore various counterfactual scenarios. As the modeling tools for public health epidemiology are getting more sophisticated, it is becoming increasingly difficult for noncomputational scientists to effectively use the systems that incorporate such models. Thus an important design consideration for an integrated modeling environment is to improve ease of use such that experimental simulations can be driven by the users. This is achieved by designing intuitive and user-friendly interfaces that allow users to design and analyze a computational experiment and steer the experiment based on the state of the system. A key feature of a system that supports this design goal is the ability to start, stop, pause, and roll back the disease propagation and intervention application process interactively. An analyst can access the state of the system at any point in time and formulate dynamic interventions based on additional information obtained through state assessment. In addition, the environment provides automated services for experiment set-up and management, thus reducing the overall time for conducting end-to-end experimental studies. We illustrate the applicability of the system by describing computational experiments based on realistic pandemic planning scenarios. The experiments are designed to demonstrate the system's capability and enhanced user productivity. © 2014 ACM.",Computational epidemiology; Computational steering; Interactive computations; Network-based epidemiological modeling; Service oriented architectures; Usability; User productivity,Diseases; Epidemiology; Experiments; Information services; Interface states; Mobile devices; Multimedia systems; Productivity; Quality of service; Service oriented architecture (SOA); Computational epidemiologies; Computational steering; Epidemiological modeling; Interactive computation; Usability; User productivity; Public health
Postmarketing drug safety surveillance using publicly available health-consumer-contributed content in social media,2014,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84899048828&doi=10.1145%2f2576233&partnerID=40&md5=8f58c6d412346b11462d8864a106dac1,"Postmarketing drug safety surveillance is important because many potential adverse drug reactions cannot be identified in the premarketing review process. It is reported that about 5% of hospital admissions are attributed to adverse drug reactions and many deaths are eventually caused, which is a serious concern in public health. Currently, drug safety detection relies heavily on voluntarily reporting system, electronic health records, or relevant databases. There is often a time delay before the reports are filed and only a small portion of adverse drug reactions experienced by health consumers are reported. Given the popularity of social media, many health social media sites are now available for health consumers to discuss any healthrelated issues, including adverse drug reactions they encounter. There is a large volume of health-consumercontributed content available, but little effort has been made to harness this information for postmarketing drug safety surveillance to supplement the traditional approach. In this work, we propose the association rule mining approach to identify the association between a drug and an adverse drug reaction. We use the alerts posted by Food and Drug Administration as the gold standard to evaluate the effectiveness of our approach. The result shows that the performance of harnessing health-related social media content to detect adverse drug reaction is good and promising. © 2014 ACM.",Adverse drug reactions; Drug safety signal detection; Health-consumer-contributed content; Postmarketing surveillance; Social media,Monitoring; Pharmacodynamics; Adverse drug reactions; Drug safety; Electronic health record; Food and Drug Administration; Health-consumer-contributed content; Hospital admissions; Social media; Traditional approaches; Health
Predicting stability of open-source software systems using combination of bayesian classifiers,2014,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84899074097&doi=10.1145%2f2555596&partnerID=40&md5=0c9c488eb3073aba5e6e8b411227ac80,"The use of free and Open-Source Software (OSS) systems is gaining momentum. Organizations are also now adopting OSS, despite some reservations, particularly about the quality issues. Stability of software is one of the main features in software quality management that needs to be understood and accurately predicted. It deals with the impact resulting from software changes and argues that stable components lead to a cost-effective software evolution. Changes are most common phenomena present in OSS in comparison to proprietary software. This makes OSS system evolution a rich context to study and predict stability. Our objective in this work is to build stability prediction models that are not only accurate but also interpretable, that is, able to explain the link between the architectural aspects of a software component and its stability behavior in the context of OSS. Therefore, we propose a new approach based on classifiers combination capable of preserving prediction interpretability. Our approach is classifier-structure dependent. Therefore, we propose a particular solution for combining Bayesian classifiers in order to derive a more accurate composite classifier that preserves interpretability. This solution is implemented using a genetic algorithm and applied in the context of an OSS large-scale system, namely the standard Java API. The empirical results show that our approach outperforms state-of-the-art approaches from both machine learning and software engineering. © 2014 ACM.",Bayesian classifiers; Genetic algorithm; Software stability prediction,Computer software selection and evaluation; Forecasting; Genetic algorithms; Quality management; Software engineering; Bayesian classifier; Classifiers combinations; Open source software systems; Open-source softwares; Proprietary software; Software quality management; Software stabilities; State-of-the-art approach; Stability
Septic shock prediction for patients with missing data,2014,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84899106645&doi=10.1145%2f2591676&partnerID=40&md5=34ee22f7b5a7998329d5dbe97a23622e,"Sepsis and septic shock are common and potentially fatal conditions that often occur in intensive care unit (ICU) patients. Early prediction of patients at risk for septic shock is therefore crucial to minimizing the effects of these complications. Potential indications for septic shock risk span a wide range of measurements, including physiological data gathered at different temporal resolutions and gene expression levels, leading to a nontrivial prediction problem. Previous works on septic shock prediction have used small, carefully curated datasets or clinical measurements that may not be available for many ICU patients. The recent availability of a large, rich ICU dataset called MIMIC-II has provided the opportunity for more extensive modeling of this problem. However, such a large clinical dataset inevitably contains a substantial amount of missing data. We investigate how different imputation selection criteria and methods can overcome the missing data problem. Our results show that imputation methods in conjunction with predictive modeling can lead to accurate septic shock prediction, even if the features are restricted primarily to noninvasive measurements. Our models provide a generalized approach for predicting septic shock in any ICU patient. © 2014 ACM.",Data mining; Imputation methods; Missing data; Sepsis,Data mining; Gene expression; Intensive care units; Clinical measurements; Gene expression levels; Imputation methods; Missing data; Missing data problem; Noninvasive measurements; Predictive modeling; Sepsis; Forecasting
Risk mitigation decisions for it security,2014,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84899115896&doi=10.1145%2f2576757&partnerID=40&md5=1073a9a7357a5e86f9a6636220884b42,"Enterprises must manage their information risk as part of their larger operational risk management program. Managers must choose how to control for such information risk. This article defines the flow risk reduction problem and presents a formal model using a workflow framework. Three different control placement methods are introduced to solve the problem, and a comparative analysis is presented using a robust test set of 162 simulations. One year of simulated attacks is used to validate the quality of the solutions. We find that the math programming control placement method yields substantial improvements in terms of risk reduction and risk reduction on investment when compared to heuristics that would typically be used by managers to solve the problem. The contribution of this research is to provide managers with methods to substantially reduce information and security risks, while obtaining significantly better returns on their security investments. By using a workflow approach to control placement, which guides the manager to examine the entire infrastructure in a holistic manner, this research is unique in that it enables information risk to be examined strategically. © 2014 ACM.",Controls; Information risk management; Workflows,Control; Enterprise resource management; Investments; Risk management; Security of data; Comparative analysis; Information risk managements; Operational risk managements; Placement methods; Risk mitigation decisions; Security investments; Simulated attacks; Work-flows; Managers
Building online trust in a culture of confucianism: The impact of process flexibility and perceived control,2014,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84899102251&doi=10.1145%2f2576756&partnerID=40&md5=3c626209e328af0ab23d2c9ee7f50831,"The success of e-commerce companies in a Confucian cultural context takes more than advanced IT and process design that have proven successful in Western countries. The example of eBay's failure in China indicates that earning the trust of Chinese consumers is essential to success, yet the process of building that trust requires something different from that in the Western culture. This article attempts to build a theoretical model to explore the relationship between the Confucian culture and online trust.We introduce two new constructs, namely process flexibility and perceived control, as particularly important factors in online trust formation in the Chinese cultural context. A survey was conducted to test the proposed theoretical model. This study offers a new explanation for online trust formation in the Confucian context. The findings of this article can provide guidance for companies hoping to successfully navigate the Chinese online market in the future. © 2014 ACM.",Confucianism; Culture; E-commerce; Online market; Perceived control; Process flexibility; Trust,Cell culture; Design; Electronic commerce; Industry; Confucianism; Cultural context; Online markets; Process flexibility; Provide guidances; Theoretical modeling; Trust; Western countries; Process control
I can help you change! An empathic virtual agent delivers behavior change health interventions,2013,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891769876&doi=10.1145%2f2544103&partnerID=40&md5=01cfe4a0f4c2d74096db54bbf9cdad9d,"We discuss our approach to developing a novel modality for the computer-delivery of Brief Motivational Interventions (BMIs) for behavior change in the form of a personalized On-Demand VIrtual Counselor (ODVIC), accessed over the internet. ODVIC is a multimodal Embodied Conversational Agent (ECA) that empathically delivers an evidence-based behavior change intervention by adapting, in real-time, its verbal and nonverbal communication messages to those of the user's during their interaction. We currently focus our work on excessive alcohol consumption as a target behavior, and our approach is adaptable to other target behaviors (e.g., overeating, lack of exercise, narcotic drug use, non-adherence to treatment). We based our current approach on a successful existing patient-centered brief motivational intervention for behavior change-the Drinker's Check-Up (DCU)-whose computer-delivery with a text-only interface has been found effective in reducing alcohol consumption in problem drinkers. We discuss the results of users' evaluation of the computer-based DCU intervention delivered with a text-only interface compared to the same intervention delivered with two different ECAs (a neutral one and one with some empathic abilities). Users rate the three systems in terms of acceptance, perceived enjoyment, and intention to use the system, among other dimensions. We conclude with a discussion of how our positive results encourage our long-term goals of on-demand conversations, anytime, anywhere, with virtual agents as personal health and well-being helpers. © 2013 ACM.",Affective computing; Alcohol interventions; Behavior change; Brief motivational interviewing intervention; Computer-based interventions; Embodied conversational agent; Empathy modeling; Health informatics; Healthy lifestyles; Information systems; Intelligent virtual agent; Multimodal communication,Communication; Human computer interaction; Information systems; Intelligent virtual agents; Medical computing; Affective Computing; Behavior change; Brief motivational interviewing intervention; Computer-based interventions; Embodied conversational agent; Health informatics; Healthy lifestyles; Multimodal communications; Patient treatment
Mining deviations from patient care pathways via electronic medical record system audits,2013,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891786408&doi=10.1145%2f2544102&partnerID=40&md5=ae9e537e7b1e9e43981d99ca2818ebe5,"In electronic medical record (EMR) systems, administrators often provide EMR users with broad access privileges, which may leave the system vulnerable to misuse and abuse. Given that patient care is based on a coordinated workflow, we hypothesize that care pathways can be represented as the progression of a patient through a system and introduce a strategy to model the patient's flow as a sequence of accesses defined over a graph. Elements in the sequence correspond to features associated with the access transaction (e.g., reason for access). Based on this motivation, we model patterns of patient record usage, which may indicate deviations from care workflows. We evaluate our approach using several months of data from a large academic medical center. Empirical results show that this framework finds a small portion of accesses constitute outliers from such flows. We also observe that the violation patterns deviate for different types of medical services. Analysis of our results suggests greater deviation from normal access patterns by nonclinical users. We simulate anomalies in the context of real accesses to illustrate the efficiency of the proposed method for different medical services. As an illustration of the capabilities of our method, it was observed that the area under the receiver operating characteristic (ROC) curve for the Pediatrics service was found to be 0.9166. The results suggest that our approach is competitive with, and often better than, the existing state-of-the-art in its outlier detection performance. At the same time, our method is more efficient, by orders of magnitude, than previous approaches, allowing for detection of thousands of accesses in seconds. © 2013 ACM.",Anomaly detection; Audit; Data mining; Electronic medical record systems; Graph-based analysis; Temporal systems,Data mining; Flow graphs; Management; Statistics; Anomaly detection; Audit; Electronic medical record system; Graph-based; Temporal systems; Medical computing
Business benefits or incentive maximization? Impacts of the medicare EHR incentive program at acute care hospitals,2013,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891753040&doi=10.1145%2f2543900&partnerID=40&md5=242e4b89859c3d07d044b449e770b90d,"This study investigates the influence of the Medicare EHR Incentive Program on EHR adoption at acute care hospitals and the impact of EHR adoption on operational and financial efficiency/effectiveness. It finds that even before joining the incentive program, adopter hospitals had more efficient and effective Medicare operations than those of non-adopters. Adopters were also financially more efficient. After joining the program, adopter hospitals treated significantly more Medicare patients by shortening their stay durations, relative to their own non-Medicare patients and also to patients at non-adopter hospitals, even as their overall capacity utilization remained relatively unchanged. The study concludes that many of these hospitals had implemented EHR even before the initiation of the incentive program. It further infers that they joined this program with opportunistic intentions of tapping into incentive payouts which they maximized by taking on more Medicare patients. These findings give credence to critics of the program who have questioned its utility and alleged that it serves only to reward existing users of EHR technologies. © 2013 ACM.",Electronic health records; Electronic medical records; Information technology adoption; Information technology implementation; Medicare EHR Incentive Program; Opportunism; Organizational change; Perverse incentives,Hospitals; Information technology; Joining; Medical computing; Patient treatment; Electronic health record; Electronic medical record; Incentive programs; Information technology adoption; Information technology implementations; Opportunism; Organizational change; Perverse incentives; Health insurance
Modeling throughput of emergency departments via time series: An expectation maximization algorithm,2013,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891806718&doi=10.1145%2f2544105&partnerID=40&md5=777b5c3c2a72b2bea41b7f48a3708123,"In this article, the expectation maximization (EM) algorithm is applied for modeling the throughput of emergency departments via available time-series data. The dynamics of emergency department throughput is developed and evaluated, for the first time, as a stochastic dynamic model that consists of the noisy measurement and first-order autoregressive (AR) stochastic dynamic process. By using the EM algorithm, the model parameters, the actual throughput, as well as the noise intensity, can be identified simultaneously. Four real-world time series collected from an emergency department in West London are employed to demonstrate the effectiveness of the introduced algorithm. Several quantitative indices are proposed to evaluate the inferred models. The simulation shows that the identified model fits the data very well. © 2013 ACM.",EM algorithm; Emergency department; Healthcare; Modeling; Time series data,Computer simulation; Emergency rooms; Health care; Maximum principle; Models; Stochastic systems; Throughput; Time series; EM algorithms; Emergency departments; Expectation-maximization algorithms; Noisy measurements; Quantitative indices; Real-world time series; Stochastic dynamics; Time-series data; Algorithms
Smart health and wellbeing,2013,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891816409&doi=10.1145%2f2555810.2555811&partnerID=40&md5=7708509af6ee5790cf6ee49851246283,"Healthcare informatics has drawn substantial attention in recent years. Current work on healthcare informatics is highly interdisciplinary involving methodologies from computing, engineering, information science, behavior science, management science, social science, as well as many different areas in medicine and public health. Three major tracks, (i) systems, (ii) analytics, and (iii) human factors, can be identified. The systems track focuses on healthcare system architecture, framework, design, engineering, and application; the analytics track emphasizes data/information processing, retrieval, mining, analytics, as well as knowledge discovery; the human factors track targets the understanding of users or context, interface design, and user studies of healthcare applications. In this article, we discuss some of the latest development and introduce several articles selected for this special issue. We envision that the development of computing-oriented healthcare informatics research will continue to grow rapidly. The integration of different disciplines to advance the healthcare and wellbeing of our society will also be accelerated. © 2013 ACM.",,Health care; Human engineering; Information science; Management science; Social sciences computing; Behavior science; Health care application; Health care informatics; Health-care system; Interface designs; Latest development; User study; Wellbeing; Behavioral research
Embodying care in matilda: An affective communication robot for emotional wellbeing of older people in Australian residential care facilities,2013,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891800072&doi=10.1145%2f2544104&partnerID=40&md5=58c90cb17038873f0b7ae8efc7030004,"Ageing population is at the center of the looming healthcare crisis in most parts of the developed and developing world. Australia, like most of the western world, is bracing up for the looming ageing population crisis, spiraling healthcare costs, and expected serious shortage of healthcare workers. Assistive service and companion (social) robots are being seen as one of the ways for supporting aged care facilities to meet this challenge and improve the quality of care of older people including mental and physical health outcomes, as well as to support healthcare workers in personalizing care. In this article, the authors report on the design and implementation of first-ever field trials of Matilda, a human-like assistive communication (service and companion) robot for improving the emotional well-being of older people in three residential care facilities in Australia involving 70 participants. The research makes several unique contributions including Matilda's ability to break technology barriers, positively engage older people in group and one-to-one activities, making these older people productive and useful, helping them become resilient and cope better through personalization of care, and finally providing them sensory enrichment through Matilda's multimodal communication capabilities. © 2013 ACM.",Affective communication robot; Aged care; Nursing home personhood; Personalization of care; Well-being,Communication; Developing countries; Health care; Housing; Robots; Affective communication; Aged care; Nursing homes; Personalizations; Well-being; Machine design
Does knowledge management matter? the empirical evidence from market-based valuation,2013,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885655662&doi=10.1145%2f2500750&partnerID=40&md5=9eefd18ebb9375f931930cf054111672,"Information technology is inseparable from contemporary knowledge management (KM). Although anecdotal evidence and individual case studies suggest that effective knowledge management initiatives contribute to superior firm performance, other kinds of empirical investigations are scarce, and more to the point, most of them are based on perceptions of survey participants embedded in the firms being studied. Moreover, studies analyzing the question of whether superior KM performance can predict superior market-based valuation appear to be virtually nonexistent. Findings of such studies would be of value to those who champion and direct a firm's KM efforts, and to the firm's strategists, planners, and operational managers. Here, we empirically examine the relationship between KM performance and firm valuation; the former is assessed by international panels of independent KM experts and the latter is evaluated in terms of market-based measures. Based on data spanning eight years, the results show that superior KM performance has a statistically significant positive association with firm valuation in terms of Tobin's q, price-to-book ratio, and price-to-sales ratio. This study contributes to themanagement literature by using independent expert judges and archival data to substantiate the notion that KM competencies are an important ingredient in a firm's performance as indicated by market-based valuation. © 2013 ACM.",Knowledge chain theory; Knowledge management; Knowledge management performance; Market performance; Market valuation; Organizational learning theory; Resource-based view; Tobin's q,Commerce; Industry; Information technology; Knowledge management; Surveys; Knowledge chain; Market performance; Market valuations; Organizational learning; Resource-based view; Tobin's q; Resource valuation
Bayesian inference in trust networks,2013,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885668703&doi=10.1145%2f2489790&partnerID=40&md5=e4e6d769390f85a508dba8838b865a78,"Trust has emerged as a major impediment to the success of electronic markets and communities where interaction with the strangers is the norm. Social Networks and Online Communities enable interaction with complete strangers, and open up new commercial, political, and social possibilities. But those promises are rarely achieved because it is difficult to trust the online contacts. A common approach to remedy this problem is to compute trust values for the new contacts from the existing trust values in the network. There are two main methods: aggregation and transitivity. Yet, neither method provides satisfactory results because trust networks are sparse and transitivity may not hold. This article develops a Bayesian formulation of the problem, where trust is defined as a conditional probability, and a Bayesian Network analysis is employed to compute the unknown trust values in terms of the known trust values. The algorithms used to propagate conditional probabilities through the network are theoretically sound and based on a long-standing literature on probability propagation in Bayesian networks. Moreover, the context information that is typically ignored in trust literature is included here as a major factor in computing new trust values. These changes have led to significant improvements over existing approaches in the accuracy of computed trust, and with some modifications to the algorithm, in its reach. Real data acquired from Advogato network is used to do extensive testing, and the results confirm the practical value of a theoretically sound Bayesian approach. © 2013 ACM.",Bayesian networks; Social networks; Trust inference; Trust networks; Trust propagation,Algorithms; Inference engines; Social networking (online); Bayesian formulation; Conditional probabilities; Context information; On-line communities; Probability propagation; Trust inferences; Trust networks; Trust propagation; Bayesian networks
Detecting deceptive chat-based communication using typing behavior and message cues,2013,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885595173&doi=10.1145%2f2499962.2499967&partnerID=40&md5=98ae00b74d2c73bb7cdd023c290a9601,"Computer-mediated deception is prevalent and may have serious consequences for individuals, organizations, and society. This article investigates several metrics as predictors of deception in synchronous chatbased environments, where participants must often spontaneously formulate deceptive responses. Based on cognitive load theory, we hypothesize that deception influences response time, word count, lexical diversity, and the number of times a chat message is edited. Using a custom chatbot to conduct interviews in an experiment, we collected 1,572 deceitful and 1,590 truthful chat-based responses. The results of the experiment confirm that deception is positively correlated with response time and the number of edits and negatively correlated to word count. Contrary to our prediction, we found that deception is not significantly correlated with lexical diversity. Furthermore, the age of the participant moderates the influence of deception on response time. Our results have implications for understanding deceit in chat-based communication and building deception-detection decision aids in chat-based systems. © 2013 ACM.",Chat; Deception detection; Decision support system; Typing bahavior,Artificial intelligence; Decision support systems; Bahavior; Chat; Chatbot; Cognitive load theory; Deception detection; Decision aids; Experiment confirm; Typing behaviors; Communication
A random walk model for item recommendation in social tagging systems,2013,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885626865&doi=10.1145%2f2490860&partnerID=40&md5=b565bb23ab3973ad3f01e29b1e2f1f75,"Social tagging, as a novel approach to information organization and discovery, has been widely adopted in many Web 2.0 applications. Tags contributed by users to annotate a variety of Web resources or items provide a new type of information that can be exploited by recommender systems. Nevertheless, the sparsity of the ternary interaction data among users, items, and tags limits the performance of tag-based recommendation algorithms. In this article, we propose to deal with the sparsity problem in social tagging by applying random walks on ternary interaction graphs to explore transitive associations between users and items. The transitive associations in this article refer to the path of the link between any two nodes whose length is greater than one. Taking advantage of these transitive associations can allow more accurate measurement of the relevance between two entities (e.g., user-item, user-user, and item-item). A PageRank-like algorithm has been developed to explore these transitive associations by spreading users' preferences on an item similarity graph and spreading items' influences on a user similarity graph. Empirical evaluation on three real-world datasets demonstrates that our approach can effectively alleviate the sparsity problem and improve the quality of item recommendation. © 2013 ACM.",Random walk; Recommender systems; Social tagging; Sparsity,Algorithms; Random processes; Recommender systems; Social networking (online); World Wide Web; Empirical evaluations; Information organization; Random Walk; Social tagging; Social tagging systems; Sparsity; Tag-based recommendations; Transitive associations; Association reactions
An inference engine for estimating outside states of clinical test items,2013,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887958374&doi=10.1145%2f2517084&partnerID=40&md5=2d01232ab03dcc275c435e9ebc68d2c2,"Common periodical health check-ups include several clinical test items with affordable cost. However, these standard tests do not directly indicate signs of most lifestyle diseases. In order to detect such diseases, a number of additional specific clinical tests are required, which increase the cost of the health check-up. This study aims to enrich our understanding of the common health check-ups and proposes a way to estimate the signs of several lifestyle diseases based on the standard tests in common examinations without performing any additional specific tests. In this manner, we enable a diagnostic process, where the physician may prefer to perform or avoid a costly test according to the estimation carried out through a set of common affordable tests. To that end, the relation between standard and specific test results is modeled with a multivariate kernel density estimate. The condition of the patient regarding a specific test is assessed following a Bayesian framework. Our results indicate that the proposed method achieves an overall estimation accuracy of 84%. In addition, an outstanding estimation accuracy is achieved for a subset of high-cost tests.Moreover, comparison with standard artificial intelligence methods suggests that our algorithm outperforms the conventional methods. Our contributions are as follows: (i) promotion of affordable health check-ups, (ii) high estimation accuracy in certain tests, (iii) generalization capability due to ease of implementation on different platforms and institutions, (iv) flexibility to apply to various tests and potential to improve early detection rates. © 2013 ACM.",Algorithms; Management,Algorithms; Artificial intelligence; Costs; Diagnosis; Estimation; Health; Management; Artificial intelligence methods; Bayesian frameworks; Conventional methods; Diagnostic process; Generalization capability; Lifestyle disease; Multivariate kernel densities; Overall estimation; Statistical tests
Accurate and efficient algorithms that adapt to privacy-enhanced video for improved assistive monitoring,2013,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887858341&doi=10.1145%2f2523025.2523026&partnerID=40&md5=25c23f854eae6f64598c59ab809720d2,"Automated monitoring algorithms operating on live video streamed from a home can effectively aid in several assistive monitoring goals, such as detecting falls or estimating daily energy expenditure. Use of video raises obvious privacy concerns. Several privacy enhancements have been proposed such as modifying a person in video by introducing blur, silhouette, or bounding-box. Person extraction is fundamental in video-based assistive monitoring and degraded in the presence of privacy enhancements; however, privacy enhancements have characteristics that can opportunistically be adapted to. We propose two adaptive algorithms for improving assistive monitoring goal performance with privacy-enhanced video: specific-color hunter and edge-void filler. A nonadaptive algorithm, foregrounding, is used as the default algorithm for the adaptive algorithms. We compare nonadaptive and adaptive algorithms with 5 common privacy enhancements on the effectiveness of 8 automated monitoring goals. The nonadaptive algorithm performance on privacy-enhanced video is degraded from raw video. However, adaptive algorithms can compensate for the degradation. Energy estimation accuracy in our tests degraded from 90.9% to 83.9%, but the adaptive algorithms significantly compensated by bringing the accuracy up to 87.1%. Similarly, fall detection accuracy degraded from 1.0 sensitivity to 0.86 and from 1.0 specificity to 0.79, but the adaptive algorithms compensated accuracy back to 0.92 sensitivity and 0.90 specificity. Additionally, the adaptive algorithms were computationally more efficient than the nonadaptive algorithm, averaging 1.7% more frames processed per second. © 2013 ACM.",Algorithms; Experimentation; Human Factors; Measurement; Reliability,Algorithms; Human engineering; Measurements; Monitoring; Reliability; Assistive monitoring; Automated monitoring; Automated monitoring algorithms; Default algorithms; Energy estimation; Energy expenditure; Experimentation; Nonadaptive algorithm; Adaptive algorithms
A fitness-utility model for design science research,2013,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885618077&doi=10.1145%2f2499962.2499963&partnerID=40&md5=303ed0d8bccca6e52b2d09430c1c2387,"Current thinking in design science research (DSR) defines the usefulness of the design artifact in a relevant problem environment as the primary research goal. Here we propose a complementary evaluation model for DSR. Drawing from evolutionary economics, we define a fitness-utility model that better captures the evolutionary nature of design improvements and the essential DSR nature of searching for a satisfactory design across a fitness landscape. Our goal is to move DSR tomoremeaningful evaluations of design artifacts for sustainable impacts. A key premise of this new thinking is that the evolutionary fitness of a design artifact is more valuable than its immediate usefulness. We conclude with a discussion of the strengths and challenges of the fitness-utility model for the performance of rigorous and relevant DSR. © 2013 ACM.",Design science research; Evolutionary economics; Fitness; Research paradigms; Sustainability; Utility,Research; Sustainable development; Design improvements; Design science researches (DSR); Design-science researches; Evaluation modeling; Evolutionary Economics; Fitness; Sustainable impacts; Utility; Design
Distributed privacy-preserving decision support system for highly imbalanced clinical data,2013,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887897024&doi=10.1145%2f2517310&partnerID=40&md5=8ce95fa6682625f1e917f22878e9f9e5,"When a medical practitioner encounters a patient with rare symptoms that translates to rare occurrences in the local database, it is quite valuable to draw conclusions collectively from such occurrences in other hospitals. However, for such rare conditions, there will be a huge imbalance in classes among the relevant base population. Due to regulations and privacy concerns, collecting data from other hospitals will be problematic. Consequently, distributed decision support systems that can use just the statistics of data from multiple hospitals are valuable. We present a system that can collectively build a distributed classification model dynamically without the need of patient data from each site in the case of imbalanced data. The system uses a voting ensemble of experts for the decision model. The imbalance condition and number of experts can be determined by the system. Since only statistics of the data and no raw data are required by the system, patient privacy issues are addressed. We demonstrate the outlined principles using the Nationwide Inpatient Sample (NIS) database. Results of experiments conducted on 7,810,762 patients from 1050 hospitals show improvement of 13.68% to 24.46% in balanced prediction accuracy using our model over the baseline model, illustrating the effectiveness of the proposed methodology. © 2013 ACM.",Algorithms,Algorithms; Artificial intelligence; Decision support systems; Hospital data processing; Hospitals; Decision modeling; Distributed classification; Distributed decision support systems; Medical practitioner; Patient privacies; Prediction accuracy; Privacy concerns; Privacy preserving; Data privacy
Real options and system dynamics for information technology investment decisions: Application to RFID adoption in retail,2013,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887984578&doi=10.1145%2f2517309&partnerID=40&md5=a972d745637fb05a3bc27d79af2485fa,"We propose a unique combination of system dynamics and real options into a robust and innovative model for analyzing return on investments in IT. Real options modeling allows a cost benefit analysis to take into account managerial flexibilities when there is uncertainty in the investment, while system dynamics can build a predictive model, in which one can simulate different real-life and hypothetical scenarios in order to provide measurements that can be used in the real options model. Our return on the investment model combines these long-established quantitative techniques in a novel manner. This study applies this robust hybrid model to a challenging IT investment problem: adoption of RFID in retail. Item-level RFID is the next generation of identification technology in the retail sector. Our method can help managers to overcome the complexity and uncertainties in the investment timing of this technology. We analyze the RFID considerations in retail decision-making using real data compiled from a Delphi study. Our model demonstrates how the cost and benefits of such an investment change over time. The results highlight the variable cost of RFID tags as the key factor in the decision process concerning whether to immediately adopt or postpone the use of RFID in retail. Our exploratory work suggests that it is possible to combine merchandising and pricing issues in addition to the traditional supply chain management issues in studying any multifaceted problem in retail. © 2013 ACM.",Economics; Management; Performance,Cost benefit analysis; Economics; Information technology; Management; Managers; Radio frequency identification (RFID); Sales; Supply chain management; System theory; Uncertainty analysis; Cost and benefits; Identification technology; Information technology investments; Investment timing; Managerial flexibility; Performance; Predictive modeling; Quantitative techniques; Investments
A systematic review of business and information technology alignment,2013,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878560537&doi=10.1145%2f2445560.2445564&partnerID=40&md5=6437fa49ca4aa2c5d0ad301d7b41e722,"Business organizations have become heavily dependent on information technology (IT) services. The process of alignment is defined as the mutual synchronization of business goals and IT services. However, achieving mature alignment between business and IT is difficult due to the rapid changes in the business and IT environments. This article provides a systematic review of studies on the alignment of business and IT. The research articles reviewed are based on topics of alignment, the definition of alignment, history, alignment challenges, phases of alignment, alignment measurement approaches, the importance of alignment in business industries, how software engineering helps in better alignment, and the role of the business environment in aligning business with IT. It aims to present a thorough understanding of business-IT alignment and to provide a list of future research directions regarding alignment. To perform the systematic review, we used the guidelines developed by Kitchenham for reviewing the available research papers relevant to our topic. © 2013 ACM.",Alignment measurement; Alignment phases; Business and IT alignment; Business environment modeling; Business issues; IT issues; IT support; Literature; Systematic review,Alignment; Computer supported cooperative work; Research; Software engineering; Alignment measurement; Business and it alignments; Business environments; Business issues; IT issues; It supports; Literature; Systematic Review; Information technology
Network effects in health information exchange growth,2013,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878554689&doi=10.1145%2f2445560.2445561&partnerID=40&md5=4170c247ba5464a81348629cd8534e54,"The importance of the Healthcare Information Exchange (HIE) in increasing healthcare quality and reducing risks and costs has led to greater interest in identifying factors that enhance adoption and meaningful use of HIE by healthcare providers. In this research we study the interlinked network effects between two different groups of physicians - primary care physicians and specialists - as significant factors in increasing the growth of each group in an exchange. An analytical model of interlinked and intragroup influences on adoption is developed using the Bass diffusion model as a basis. Adoption data on 1,060 different primary and secondary care physicians over 32 consecutive months was used to test the model. The results indicate not only the presence of interlinked effects, but also that their influence is stronger than that of the intragroup. Further, the influence of primary care physicians on specialists is stronger than that of specialists on primary care physicians. We also provide statistical evidence that the new model performs better than the conventional Bass model, and the assumptions of diffusion symmetry in the market are statistically valid. Together, the findings provide important guidelines on triggers that enhance the overall growth of HIE and potential marketing strategies for HIE services. © 2013 ACM.",Bass model; Diffusion of innovation; Health information exchange,Commerce; Health care; Bass Diffusion Model; Bass model; Diffusion of innovations; Health care providers; Health information exchanges; Healthcare quality; Information exchanges; Statistical evidence; Information dissemination
The impact of SOA implementation on IT-business alignment: A system dynamics approach,2013,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878551717&doi=10.1145%2f2445560.2445563&partnerID=40&md5=5809e76dfc8acc8e6f67b95c6437e854,"With firms facing intense rivalry, globalization, and time-to-market pressures, the need for organizational agility assumes greater importance. One of the primary vehicles for achieving organizational agility is the use of agile information systems [IS] and the close alignment of information technologies [IT] with business. However, IS is often viewed as an impediment to organization agility. Recently, service-oriented architecture [SOA] has emerged as a prominent IS agility-enhancing technology. The fundamental question of how SOA can enhance organization agility and foster closer alignment between IT and business has not been adequately addressed. The dynamic interaction among external business environmental factors, organizational agility, and IS architecture makes the process of keeping IT and business aligned more complex. This study uses a design science approach to build a system dynamics model to examine the effect of employing alternative SOA implementation strategies in various organizational and external business environments on the IT business alignment and IS cost. The results provide insights into the shaping of IT-business alignment. Additionally, the system dynamics model serves as a tool for supporting managerial decisions related to SOA implementation. © 2013 ACM.",Information systems agility; IT-business alignment; Service-oriented architecture; System dynamics modeling,Design; Information services; Information systems; Service oriented architecture (SOA); System theory; Agile information system; Business environments; Environmental factors; IT-business alignments; Organizational agility; System dynamics approach; System dynamics model; System dynamics modeling; Alignment
A dispatch-mediated communication model for emergency response systems,2013,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878530969&doi=10.1145%2f2445560.2445562&partnerID=40&md5=f1ffe3a0e3a5f7e79321819ce711424b,"The current state of emergency communication is dispatch-mediated (the messages from the scene are directed towards the responders and agencies through the dispatch agency). These messages are logged in electronic documents called incident reports, which are useful in monitoring the incident, off-site supervision, resource allocation, and post-incident analysis. However, these messages do not adhere to any particular structure, and there is no set format. The lack of standards creates a problem for sharing information among systems and responders and has a detrimental impact on systems interoperability. In this article, we develop a National Information Exchange Model (NIEM) and Universal Core (UCORE) compliant messaging model, considering message structures and formats, to foster message standardization. © 2013 ACM.",Design science; Dispatch-mediated communication; Emergency response systems; Message classification; Message queuing; Message structuring,Design; Information theory; Communication models; Design science; Emergency response systems; Information exchanges; Message queuing; Message structuring; Sharing information; Systems interoperability; Interoperability
"The ""mail-Order-Bride"" (MOB) phenomenon in the cyberworld: An interpretive investigation",2013,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887954612&doi=10.1145%2f2524263&partnerID=40&md5=77b057baa063eccd2148b2968d09b171,"Information technology (IT) is often an enabler in bringing people together. In the context of this study, IT helps connect matchmaking service providers with those looking for love, particularly when a male seeks to meet and possibly marry a female from another country: a process which results in over 16,500 such 'mailorder- bride' (MOB) marriages a year in the United States alone. Past research in business disciplines has been largely silent about the way in which this process unfolds, the perspectives of the participants at different points of time, and the role of IT underlying the MOB matchmaking service. Adopting an interpretivist stance, and utilizing some of the methodological guidelines associated with the Grounded Theory Methodology (GTM), we develop a process model which highlights: a) the key states of the process through which the relationship between the MOB seeker (the man) and the MOB (the woman) unfolds, b) the transitions between states, and c) the triggering conditions for the transitions from one state to another. This study also highlights key motivations of the individuals participating in the MOB process, the effect of power and the role it plays in the dynamics of the relationships, the status of women and how their status evolves during the MOB process, and the unique affordance provided by IT as the relationships evolve. © 2013 ACM.",Human Factors; Management; Theory,Human engineering; Information systems; Management; Affordances; Grounded theory; Interpretivist; Mail orders; Matchmaking services; Methodological guidelines; Process Modeling; Theory; Hardware
Product comparison networks for competitive analysis of online word-of-mouth,2013,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873674299&doi=10.1145%2f2407740.2407744&partnerID=40&md5=8cf388e898fca389f6f1bb54d2bdfada,"Enabled by Web 2.0 technologies social media provide an unparalleled platform for consumers to share their product experiences and opinions-through word-of-mouth (WOM) or consumer reviews. It has become increasingly important to understand how WOM content and metrics thereof are related to consumer purchases and product sales. By integrating network analysis with text sentiment mining techniques, we propose product comparison networks as a novel construct, computed from consumer product reviews. To test the validity of these product ranking measures, we conduct an empirical study based on a digital camera dataset from Amazon.com. The results demonstrate significant linkage between network-based measures and product sales, which is not fully captured by existing review measures such as numerical ratings. The findings provide important insights into the business impact of social media and user-generated content, an emerging problem in business intelligence research. From a managerial perspective, our results suggest that WOM in social media also constitutes a competitive landscape for firms to understand and manipulate. © 2013 ACM.",Consumer reviews; Network analysis; Sentiment analysis; Social media; Text mining; Word-of-mouth,Competitive intelligence; Consumer products; Data mining; Electric network analysis; Purchasing; Consumer reviews; Sentiment analysis; Social media; Text mining; Word-of-mouth; Statistical tests
"Contrasting multiple social network autocorrelations for binary outcomes, with applications to technology adoption",2013,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873675798&doi=10.1145%2f2407740.2407742&partnerID=40&md5=e9a70fb2b37772d0e23d984b7ee57df8,"The rise of socially targeted marketing suggests that decisions made by consumers can be predicted not only from their personal tastes and characteristics, but also from the decisions of people who are close to them in their networks. One obstacle to consider is that there may be several different measures for closeness that are appropriate, either through different types of friendships, or different functions of distance on one kind of friendship, where only a subset of these networks may actually be relevant. Another is that these decisions are often binary and more difficult to model with conventional approaches, both conceptually and computationally. To address these issues, we present a hierarchical auto-probit model for individual binary outcomes that uses and extends the machinery of the auto-probit method for binary data. We demonstrate the behavior of the parameters estimated by the multiple network-regime auto-probit model (m-NAP) under various sensitivity conditions, such as the impact of the prior distribution and the nature of the structure of the network. We also demonstrate several examples of correlated binary data outcomes in networks of interest to information systems, including the adoption of caller ring-back tones, whose use is governed by direct connection but explained by additional network topologies. © 2013 ACM.",Autocorrelation model; Bayesian method; Diffusion; Social network,Bayesian networks; Behavioral research; Diffusion; Electric network topology; Machinery; Mathematical models; Social networking (online); Autocorrelation models; Bayesian methods; Binary data; Conventional approach; In networks; Network topology; Parameters estimated; Prior distribution; Social Networks; Targeted marketing; Technology adoption; Autocorrelation
Business intelligence and analytics: Research directions,2013,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873614757&doi=10.1145%2f2407740.2407741&partnerID=40&md5=c3e3cad6ce312874af851d239ce7cdd5,"Business intelligence and analytics (BIA) is about the development of technologies, systems, practices, and applications to analyze critical business data so as to gain new insights about business and markets. The new insights can be used for improving products and services, achieving better operational efficiency, and fostering customer relationships. In this article, we will categorize BIA research activities into three broad research directions: (a) big data analytics, (b) text analytics, and (c) network analytics. The article aims to review the state-of-the-art techniques and models and to summarize their use in BIA applications. For each research direction, we will also determine a few important questions to be addressed in future research. © 2013 ACM.",Business analytics; Business intelligence,Competitive intelligence; Public relations; Big datum; Business analytics; Critical business; Customer relationships; Operational efficiencies; Products and services; Research activities; Research directions; State-of-the-art techniques; Text analytics; Research
"Fast, scalable, and context-sensitive detection of trending topics in microblog post streams",2013,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873631170&doi=10.1145%2f2407740.2407743&partnerID=40&md5=7af7f9f81ec84a6b490cc76676bb6100,"Social networks, such as Twitter, can quickly and broadly disseminate news and memes across both realworld events and cultural trends. Such networks are often the best sources of up-to-the-minute information, and are therefore of considerable commercial and consumer interest. The trending topics that appear first on these networks represent an answer to the age-old query ""what are people talking about?"" Given the incredible volume of posts (on the order of 45,000 or more per minute), and the vast number of stories about which users are posting at any given time, it is a formidable problem to extract trending stories in real time. In this article, we describe a method and implementation for extracting trending topics from a highvelocity real-time stream of microblog posts. We describe our approach and implementation, and a set of experimental results that show that our system can accurately find ""hot"" stories from high-rate Twitterscale text streams. © 2013 ACM.",Microblogs; Scalability; Trending topics; Twitter,Query processing; Scalability; Consumer interests; Context-sensitive; Cultural trends; High rate; High velocity; Micro-blog; Microblogs; Real time; Real-time streams; Real-world; Social Networks; Text streams; Trending topics; Twitter; Social networking (online)
"Business Intelligence and Analytics education, and program development: A unique opportunity for the Information Systems discipline",2012,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84869408424&doi=10.1145%2f2361256.2361257&partnerID=40&md5=47a58e55b464f6eb25b0c4cb58e129ec,"""Big Data,"" huge volumes of data in both structured and unstructured forms generated by the Internet, social media, and computerized transactions, is straining our technical capacity to manage it. More importantly, the new challenge is to develop the capability to understand and interpret the burgeoning volume of data to take advantage of the opportunities it provides in many human endeavors, ranging from science to business. Data Science, and in business schools, Business Intelligence and Analytics (BI&A) are emerging disciplines that seek to address the demands of this new era. Big Data and BI&A present unique challenges and opportunities not only for the research community, but also for Information Systems (IS) programs at business schools. In this essay, we provide a brief overview of BI&A, speculate on the role of BI&A education in business schools, present the challenges facing IS departments, and discuss the role of IS curricula and program development, in delivering BI&A education. We contend that a new vision for the IS discipline should address these challenges. © 2012 ACM.",Big Data; Business intelligence & analytics; Data mining; Data warehousing; Text analytics,Curricula; Data mining; Data warehouses; Information systems; Big datum; Business schools; Information systems programs; Program development; Research communities; Social media; Technical capacity; Text analytics; Information management
Credit rating change modeling using news and financial ratios,2012,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84869386443&doi=10.1145%2f2361256.2361259&partnerID=40&md5=626612a6f8ee6483b977cdb114ade91b,"Credit ratings convey credit risk information to participants in financial markets, including investors, issuers, intermediaries, and regulators. Accurate credit rating information plays a crucial role in supporting sound financial decision-making processes. Most previous studies on credit rating modeling are based on accounting and market information. Text data are largely ignored despite the potential benefit of conveying timely information regarding a firm's outlook. To leverage the additional information in news full-text for credit rating prediction, we designed and implemented a news full-text analysis system that provides firm-level coverage, topic, and sentiment variables. The novel topic-specific sentiment variables contain a large fraction of missing values because of uneven news coverage. The missing value problem creates a new challenge for credit rating prediction approaches. We address this issue by developing a missingtolerant multinomial probit (MT-MNP) model, which imputes missing values based on the Bayesian theoretical framework. Our experiments using seven and a half years of real-world credit ratings and news full-text data show that (1) the overall news coverage can explain future credit rating changes while the aggregated news sentiment cannot; (2) topic-specific news coverage and sentiment have statistically significant impact on future credit rating changes; (3) topic-specific negative sentiment has a more salient impact on future credit rating changes compared to topic-specific positive sentiment; (4) MT-MNP performs better in predicting future credit rating changes compared to support vector machines (SVM). The performance gap as measured by macroaveraging F-measure is small but consistent. © 2012 ACM.",Credit rating changes; Latent dirichlet allocation; Missing-tolerant multinomial probit; News coverage; News sentiment; SVM; Topic-specific news coverage; Topic-specific news sentiment,Autocorrelation; Character recognition; Commerce; Forecasting; Industry; Risk assessment; Statistics; Support vector machines; Credit ratings; Latent Dirichlet allocation; Multinomials; News coverage; News sentiment; SVM; Topic-specific news sentiment; Rating
Do vendors' pricing decisions fully reflect information in online reviews?,2012,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84869401895&doi=10.1145%2f2361256.2361261&partnerID=40&md5=e99ffbeb8e33c583c3603f1b83a458c6,"By using online retail data collected from Amazon, Barnes & Nobel, and Pricegrabber, this paper investigates whether online vendors' pricing decisions fully reflect the information contained in various components of customers' online reviews. The findings suggest that there is inefficiency in vendors' pricing decisions. Specifically, vendors do not appear to fully understand the incremental predictive power of online reviews in forecasting future sales when they adjust their prices. However, they do understand demand persistence. Interestingly, vendors reduce price if the actual demand is higher than the expected demand (positive demand shock). This phenomenon is attributed to the advertising effect suggested in previous literature and the intense competitiveness of e-Commerce. Finally, we document that vendors do not change their prices directly in response to online reviews; their response to online reviews is through forecasting consumer's future demand. © 2012 ACM.",Market efficiency; Mishkin test; Online pricing; Online reviews; Rational expectation; Vendor decisions,Competition; Sales; Market efficiency; Mishkin test; Online pricing; Rational expectations; Vendor decisions; Costs
Using a network analysis approach for organizing social bookmarking tags and enabling web content discovery,2012,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84869402593&doi=10.1145%2f2361256.2361260&partnerID=40&md5=8ac8bbfd47d0958b96296b57502b2829,"This article describes an innovative approach to reorganizing the tag space generated by social bookmarking services. The objective of this work is to enable effective search and discovery of Web content using social bookmarking tags. Tags are metadata generated by users for Web content annotation. Their potential as effective Web search and discovery tool is hindered by challenges such as, the tag space being untidy due to ambiguity, and hidden or implicit semantics. Using a novel analytics approach, we conducted network analyses on tags and discovered that tags are generated for different purposes and that there are inherent relationships among tags. Our approach can be used to extract the purposes of tags and relationships among the tags and this information can be used as facets to add structure and hierarchy to reorganize the flat tag space. The semantics of relationships and hierarchy in our proposed faceted model of tags enable searches on annotated Web content in an effective manner. We describe the implementation of a prototype system called FASTS to demonstrate feasibility and effectiveness of our approach. © 2012 ACM.",Content discovery; Facets; Social bookmarking; Tags,Image retrieval; Metadata; Semantics; Analysis approach; Content discovery; Facets; Implicit semantics; Innovative approaches; Prototype system; Social bookmarking; Tags; Web content; Web searches; Websites
"Who is retweeting the tweeters? Modeling, originating, and promoting behaviors in the twitter network",2012,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84869421989&doi=10.1145%2f2361256.2361258&partnerID=40&md5=d8b56a9b7cbf6bebd7b617d1a4d9b9a3,"Real-time microblogging systems such as Twitter offer users an easy and lightweight means to exchange information. Instead of writing formal and lengthy messages, microbloggers prefer to frequently broadcast several short messages to be read by other users. Only when messages are interesting, are they propagated further by the readers. In this article, we examine user behavior relevant to information propagation through microblogging. We specifically use retweeting activities among Twitter users to define and model originating and promoting behavior. We propose a basic model for measuring the two behaviors, a mutual dependency model, which considers the mutual relationships between the two behaviors, and a range-based model, which considers the depth and reach of users' original tweets. Next, we compare the three behavior models and contrast them with the existing work on modeling influential Twitter users. Last, to demonstrate their applicability, we further employ the behavior models to detect interesting events from sudden changes in aggregated information propagation behavior of Twitter users. The results will show that the proposed behavior models can be effectively applied to detect interesting events in the Twitter stream, compared to the baseline tweet-based approaches. © 2012 ACM.",Behavior modeling; Event detection; Information propagation; Originating behavior; Promoting behavior; Retweet; Twitter; Weak retweet,Social networking (online); Behavior modeling; Event detection; Information propagation; Originating behavior; Promoting behavior; Retweet; Twitter; Weak retweet; Behavioral research
Analyzing online review helpfulness using a Regressional ReliefF-enhanced text mining method,2012,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872115050&doi=10.1145%2f2229156.2229158&partnerID=40&md5=e5b4e669620406a5dddb38aa596b5e28,"Within the emerging context of Web 2.0 social media, online customer reviews are playing an increasingly important role in disseminating information, facilitating trust, and promoting commerce in the e-marketplace. The sheer volume of customer reviews on the web produces information overload for readers. Developing a system that can automatically identify the most helpful reviews would be valuable to businesses that are interested in gathering informative and meaningful customer feedback. Because the target variable-review helpfulness-is continuous, common feature selection techniques from text classification cannot be applied. In this article, we propose and investigate a text mining model, enhanced using the Regressional ReliefF (RReliefF) feature selection method, for predicting the helpfulness of online reviews from Amazon.com. We find that RReliefF significantly outperforms two popular dimension reduction methods. This study is the first to investigate and compare different dimension reduction techniques in the context of applying text regression for predicting online review helpfulness. Another contribution is that our analysis of the keywords selected by RReliefF reveals meaningful feature groupings. © 2012 ACM.",Business intelligence; Dimension reduction; Online reviews; Regressional ReliefF; Text mining,Classification (of information); Competitive intelligence; Data mining; Information dissemination; Intelligent agents; Customer feedback; Customer review; Dimension reduction; Dimension reduction method; Dimension reduction techniques; E-marketplaces; Feature grouping; Feature selection methods; Information overloads; Online customers; ReliefF; Selection techniques; Social media; Text classification; Text mining; Web 2.0; Text processing
"""Enforced"" vs. ""casual"" transparency - Findings from it-supported financial advisory encounters",2012,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872075541&doi=10.1145%2f2229156.2229161&partnerID=40&md5=911501504b11d38f9636aa10e4a63046,"In sales-oriented service encounters like financial advice, the client may perceive information and interest asymmetries as a lack of transparency regarding the advisor's activities. In this article, we will discuss two design iterations of a supportive tabletop application that we built to increase process and information transparency as compared to the traditional pen and paper encounters. While the first iteration's design was ""enforcing"" transparency and therefore proved to be a failure [Nussbaumer et al. 2011], we built the second iteration on design rationales enabling more ""casual"" transparency. Experimental evaluations show that the redesigned system significantly increases the client's perceived transparency, her perceived control of the encounter and improves her perceived trustworthiness of and satisfaction with the encounter. With these findings, we contribute to (1) insight into the role of transparency advisory encounter design; (2) design solutions for establishing particular facets of transparency and their potential instantiations in tabletop systems; and (3) insight into the process of designing for transparency with socio-technical artifacts that are emergent as a result of design activities. © 2012 ACM.",Collaboration; Concatenation; Design science; Investment advice; Tabletop,Design; Collaboration; Concatenation; Design activity; Design iteration; Design rationale; Design science; Design solutions; Experimental evaluation; Information transparency; Service encounter; Sociotechnical; Tabletop; Tabletop applications; Tabletop systems; Transparency
Process mining: Overview and opportunities,2012,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872058463&doi=10.1145%2f2229156.2229157&partnerID=40&md5=493b90de2b9e9f563333f657c2e19979,"Over the last decade, process mining emerged as a new research field that focuses on the analysis of processes using event data. Classical data mining techniques such as classification, clustering, regression, association rule learning, and sequence/episode mining do not focus on business process models and are often only used to analyze a specific step in the overall process. Process mining focuses on end-to-end processes and is possible because of the growing availability of event data and new process discovery and conformance checking techniques. Process models are used for analysis (e.g., simulation and verification) and enactment by BPM/WFM systems. Previously, process models were typically made by hand without using event data. However, activities executed by people, machines, and software leave trails in so-called event logs. Process mining techniques use such logs to discover, analyze, and improve business processes. Recently, the Task Force on Process Mining released the Process Mining Manifesto. This manifesto is supported by 53 organizations and 77 process mining experts contributed to it. The active involvement of end-users, tool vendors, consultants, analysts, and researchers illustrates the growing significance of process mining as a bridge between data mining and business process modeling. The practical relevance of process mining and the interesting scientific challenges make process mining one of the ""hot"" topics in Business Process Management (BPM). This article introduces process mining as a new research field and summarizes the guiding principles and challenges described in the manifesto. © 2012 ACM.",Business intelligence; Business Process Management; Data mining; Process mining,Competitive intelligence; Enterprise resource management; Management science; Research; Systems engineering; Business Process; Business process management; Business process model; Business process modeling; Conformance checking; Data mining techniques; End-to-end process; End-users; Guiding principles; Process Discovery; Process mining; Process model; Research fields; Task force; Data mining
Optimal adapter creation for process composition in synchronous vs. asynchronous communication,2012,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872073728&doi=10.1145%2f2229156.2229160&partnerID=40&md5=ab82e26fa6d382d7a38d6f9d520f8d5a,"A key issue in process-aware e-commerce collaboration is to orchestrate business processes of multiple business partners throughout a supply chain network in an automated and seamless way. Since each partner has its own internal processes with different control flow structures and message interfaces, the real challenge lies in verifying the correctness of process collaboration, and reconciling conflicts in an automated manner to make collaboration successful. The purpose of business process adaptation is to mediate the communication between independent processes to overcome their mismatches and incompatibilities. The goal of this article is to develop and compare efficient approaches of optimal adapter (i.e. one that minimizes the number of messages to be adapted) creation for multiple interacting processes under both synchronous and asynchronous communication. We start with an analysis of interactions of each message pair, and show how to identify incompatible cases and their adaptation elements for both types of communication. Then, we show how to extend this analysis into more general cases involving M messages and N processes (M, N > 2). Further, we present optimal adapter creation algorithms for both scenarios based on our analysis technique. The algorithms were implemented in a Java-based prototype system, and results of two experiments are reported. We compare and discuss the insights gained about adapter creation in these two scenarios. © 2012 ACM.",Adapter creation; Integer programming; Optimal adapter; Performance evaluation; Process compatibility; Process metrics; Synchronous and asynchronous communication,Algorithms; Integer programming; Optimization; Supply chains; Adapter creation; Asynchronous communication; Optimal adapter; Performance evaluation; Process compatibility; Process metrics; Communication
Two new prediction-driven approaches to discrete choice prediction,2012,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872062853&doi=10.1145%2f2229156.2229159&partnerID=40&md5=696f220ef55878dbf03d5f6b157ec9ef,"The ability to predict consumer choices is essential in understanding the demand structure of products and services. Typical discrete choice models that are targeted at providing an understanding of the behavioral process leading to choice outcomes are developed around two main assumptions: the existence of a utility function that represents the preferences over a choice set and the relatively simple and interpretable functional form for the utility function with respect to attributes of alternatives and decision makers. These assumptions lead to models that can be easily interpreted to provide insights into the effects of individual variables, such as price and promotion, on consumer choices. However, these restrictive assumptions might impede the ability of such theory-driven models to deliver accurate predictions and forecasts. In this article, we develop novel approaches targeted at providing more accurate choice predictions. Specifically, we propose two prediction-driven approaches: pairwise preference learning using classification techniques and ranking function learning using evolutionary computation. We compare our proposed approaches with a multiclass classification approach, as well as a standard discrete choice model. Our empirical results show that the proposed approaches achieved significantly higher choice prediction accuracy. © 2012 ACM.",Choice prediction; Data mining; Discrete choice modeling; Genetic programming; Utility theory,Data mining; Decision theory; Genetic programming; Accurate prediction; Behavioral process; Classification technique; Consumer choice; Decision makers; Demand structures; Discrete choice; Discrete choice models; Functional forms; Multi-class classification; Prediction accuracy; Preference learning; Products and services; Ranking functions; Utility functions; Utility theory; Forecasting
Design science and the accumulation of knowledge in the information systems discipline,2012,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859725573&doi=10.1145%2f2151163.2151164&partnerID=40&md5=6dbbc150c9190954253930a73aa667eb,"Design science has emerged as an important research paradigm in the information systems (IS) discipline, andmuch has been written on how it should be conducted and evaluated (e.g.,Hevner et al. [2004];Walls et al. [1992]; Vaishnavi and Kuechler [2007]; Kuechler and Vaishnavi [2008]; Peffers et al. [2007]; Iivari [2010]; Pigneur [2011]). We contend that, as a socio-technical discipline, IS research must address the interaction between design and behavior. We begin with a background discussion of what we mean by IS research and the nature of the relationship between design and behavioral approaches to IS research. We discuss the nature of design, design science, and IT artifacts within information systems research and describe the importance of linking design and behavioral perspectives. We illustrate several key points using selected articles recently published in ACM Transactions on Management Information Systems [Schmidt-Rauch and Schwabe 2011; Lau et al. 2011]. We conclude with a vision of IS research in which the capabilities and affordances of IT artifacts are incorporated into behavioral studies; the results of behavioral studies are utilized in the development and evaluation of IT artifacts; and both behavioral and design perspectives are used to address the important problems of our constituent community. © 2012 ACM.",Design; Design science; IS artifacts; IS behavioral research performance; Performance; Philosophy of IS,Design; Information systems; Knowledge management; Research; Design science; IS artifacts; Performance; Philosophy of IS; Research performance; Behavioral research
Impact of data characteristics on recommender systems performance,2012,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859722999&doi=10.1145%2f2151163.2151166&partnerID=40&md5=0f04b9207a410831e4d1eb7fe3e5b610,"This article investigates the impact of rating data characteristics on the performance of several popular recommendation algorithms, including user-based and item-based collaborative filtering, as well as matrix factorization. We focus on three groups of data characteristics: rating space, rating frequency distribution, and rating value distribution. A sampling procedure was employed to obtain different rating data subsamples with varying characteristics; recommendation algorithms were used to estimate the predictive accuracy for each sample; and linear regression-based models were used to uncover the relationships between data characteristics and recommendation accuracy. Experimental results on multiple rating datasets show the consistent and significant effects of several data characteristics on recommendation accuracy. © 2012 ACM.",Accuracy of recommendation algorithms; Algorithms; Collaborative filtering; Data characteristics; Experimentation; Performance; Performance of recommender systems,Algorithms; Recommender systems; Collaborative filtering; Data characteristics; Experimentation; Performance; Recommendation algorithms; Rating
Discovery and diagnosis of behavioral transitions in patient event streams,2012,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859715662&doi=10.1145%2f2151163.2151167&partnerID=40&md5=d6e39c853a01420ecb858ff133bed8ad,"Users with cognitive impairments use assistive technology (AT) as part of a clinical treatment plan. As the AT interface ismanipulated, data streammining techniques are used tomonitor user goals. In this context, realtime data mining aids clinicians in tracking user behaviors as they attempt to achieve their goals. Quality metrics over stream-mined models identify potential changes in user goal attainment, as the user learns his or her personalized emailing system. When the quality of some data-mined models varies significantly from nearby models-as defined by quality metrics-the user's behavior is then flagged as a significant behavioral change. The specific changes in user behavior are then characterized by differencing the data-mined decision tree models. This article describes how model quality monitoring and decision tree differencing can aid in recognition and diagnoses of behavioral changes in a case study of cognitive rehabilitation via emailing. The technique may be more widely applicable to other real-time data-intensive analysis problems. © 2012 ACM.",Behavioral rehabilitation; Clinical treatment plans; Decision trees; Design; Measurement; Patient monitoring; Real-time data mining; Stream mining,Data mining; Decision trees; Design; Measurements; Patient monitoring; Analysis problems; Assistive technology; Behavioral changes; Clinical treatments; Cognitive impairment; Cognitive rehabilitation; Decision tree models; Event streams; Model qualities; Potential change; Quality metrics; Real-time data mining; Stream mining; User behaviors; User goals; Behavioral research
How virtual teams use their virtual workspace to coordinate knowledge,2012,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859705609&doi=10.1145%2f2151163.2151169&partnerID=40&md5=acac75e6b0c1f22d8d7956484a718668,"Virtual team members increasingly rely on virtual workspace tools to coordinate knowledge that each individual brings to the team. How the use of these tools affects knowledge coordination within virtual teams is not well understood. We distinguish between tools as features and the use of the virtual workspace as providing affordances for behaviors. Using situational awareness theory, we hypothesized two affordances of virtual workspaces that facilitate knowledge coordination. Using trading zone theory, we hypothesized two forms of trading zones created by features of virtual workspaces and the impact of these trading zones on the creation of affordances for team members. Members of 54 teams were asked about the affordances of the virtual workspace, and team leaders were asked about specific tools provided to the team. Our hypothesized model was supported: the different forms of trading zones were differentially related to the different affordances and on affordances were related to knowledge coordination satisfaction. Theoretical implications focus on the distinction between features and affordances and on the identification of specific features that affect specific affordances. Practical implications for managers and engineers supporting virtual teams include the utility of becoming knowledgeable about different forms of trading zones that virtual workspaces can provide and understanding the relationship between trading zones and different affordances. © 2012 ACM.",Design; Human factors; Performance; Presence awareness; Situational awareness; Task awareness; Tool categories; Virtual teams,Commerce; Design; Human engineering; Professional aspects; Performance; Presence awareness; Situational awareness; Task awareness; Virtual team; Management
Deciphering word-of-mouth in social media: Text-based metrics of consumer reviews,2012,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859727202&doi=10.1145%2f2151163.2151168&partnerID=40&md5=fe552c64d067b555c20ff3632f449242,"Enabled by Web 2.0 technologies, social media provide an unparalleled platform for consumers to share their product experiences and opinions through word-of-mouth (WOM) or consumer reviews. It has become increasingly important to understand how WOM content and metrics influence consumer purchases and product sales. By integrating marketing theories with text mining techniques, we propose a set of novel measures that focus on sentiment divergence in consumer product reviews. To test the validity of these metrics, we conduct an empirical study based on data from Amazon.com and BN.com (Barnes & Noble). The results demonstrate significant effects of our proposed measures on product sales. This effect is not fully captured by nontextual review measures such as numerical ratings. Furthermore, in capturing the sales effect of review content, our divergence metrics are shown to be superior to and more appropriate than some commonly used textual measures the literature. The findings provide important insights into the business impact of social media and user-generated content, an emerging problem in business intelligence research. From amanagerial perspective, our results suggest that firms should pay special attention to textual content information when managing social media and, more importantly, focus on the right measures. © 2012 ACM.",Consumer reviews; Design; Performance; Sentiment analysis; Social media; Text mining; Word-of-mouth,Competitive intelligence; Consumer products; Design; Purchasing; Performance; Sentiment analysis; Social media; Text mining; Word-of-mouth; Sales
Enhancement of recall within technology-mediated teams through the use of online visual artifacts,2012,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859707841&doi=10.1145%2f2151163.2151165&partnerID=40&md5=9aaca73226a9b2518a6f5f211c03f7c6,"Given the distributed nature of modern organizations, the use of technology-mediated teams is a critical aspect of their success. These teams use various media that are arguably less personal than face-to-face communication. One factor influencing the success of these teams is their ability to develop an understanding of who knows what during the initial team development stage. However, this development of understanding within dispersed teams may be impeded because of the limitations of technology-enabled communication environments. Past research has found that a limited understanding of team member capabilities hinders team performance. As such, this article investigates mechanisms for improving the recall of individuals within dispersed teams. Utilizing the input-process-output model to conceptualize the group interaction process, three input factors-visual artifacts (i.e., a computer-generated image of each team member), team size, and work interruptions-are manipulated to assess their influence on a person's ability to recall important characteristics of their virtual team members. Results show that visual artifacts significantly increase the recall of individuals' information. However, high-urgency interruptions significantly deteriorate the recall of individuals, regardless of the visual artifact or team size. These findings provide theoretical and practical implications on knowledge acquisition and project success within technology-mediated teams. © 2012 ACM.",Experimentation; Human factors; Interruptions; Recall; Team complexity; Team size; Technology-mediated teams; Visual artifact,Communication; Electromagnetic launchers; Human engineering; Knowledge acquisition; Experimentation; Interruptions; Recall; Team complexity; Team size; Technology-mediated teams; Visual artifacts; Technology
Toward a broader vision for information systems,2011,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859723689&doi=10.1145%2f2070710.2070711&partnerID=40&md5=4d1a0aa1fbe372d089ce661937aa970c,"In December of 2009, several founders of the Information Systems (IS) academic discipline gathered for a panel discussion at the International Conference on Information Systems to present their visions for the future of the field, and their comments were summarized in the inaugural issue of TMIS [Davis et al., 2010; J. F. J. Nunamaker et al., 1991]. To assure a robust future, they argued, IS journals, conferences, reviewers, promotion committees, teachers, researchers, and curriculum developers must broaden the scope of IS. This article explores the need for a broader vision to drive future development of the IS discipline. © 2011 ACM.",Design science; Emerging technologies; Health care; Information systems impacts; Information systems vision; Innovations; Model of graduate; Model of professional; New technologies; Professional communications; Publications; Research methods; Systems requirements; University-wide resource,Curricula; Design; Health care; Information systems; Innovation; Design science; Emerging technologies; Professional communication; Publications; research methods; Systems requirements; University-wide resource; Medical computing
From information to operations: Service quality and customer retention,2011,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860581455&doi=10.1145%2f2070710.2070712&partnerID=40&md5=741deae2ec6a20c085a6b559b3b00caa,"In business, information is abundant. Yet, effective use of that information to inform and drive business operations is a challenge. Our industry-university collaborative project draws from a rich dataset of commercial demographics, transaction history, product features, and Service Quality Index (SQI) factors on shipping transactions at FedEx. We apply inductive methods to understand and predict customer churn in a noncontractual setting. Results identify several SQI variables as important determinants of churn across a variety of analytic approaches. Building on this we propose the design of a Business Intelligence (BI) dashboard as an innovative approach for increasing customer retention by identifying potential churners based on combinations of predictor variables such as demographics and SQI factors. This empirical study contributes to BI research and practice by demonstrating the application of data analytics to the fundamental business operations problem of customer churn. © 2011 ACM.",Business intelligence; Customer churn; Data analytics; Data mining; Pattern discovery; Service quality index,Competitive intelligence; Data mining; Population statistics; Quality of service; Sales; Analytic approach; Business operation; Collaborative projects; Customer churn; Customer retention; Data analytics; Data sets; Empirical studies; Inductive method; Innovative approaches; Pattern discovery; Predictor variables; Product feature; Service Quality; Customer satisfaction
Live-chat agent assignments to heterogeneous E-customers under imperfect classification,2011,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860563674&doi=10.1145%2f2070710.2070715&partnerID=40&md5=2225837ceebaa5aaee192bb18b0b934d,"Many e-commerce firms provide live-chat capability on their Web sites to promote product sales and to offer customer support.With increasing traffic on e-commerceWeb sites, providing such live-chat services requires a good allocation of service resources to serve the customers.When resources are limited, firms may consider employing priority-processing and reserving resources for high-value customers. In this article, we model a reserve-based priority-processing policy for e-commerce systems that have imperfect customer classification. Two policy decisions considered in the model are: (1) the number of agents exclusively reserved for highvalue customers, and (2) the configuration of the classification system. We derive explicit expressions for average waiting times of high-value and low-value customer classes and define a total waiting cost function. Through numerical analysis, we study the impact of these two policy decisions on average waiting times and total waiting costs. Our analysis finds that reserving agents for high-value customers may have negative consequences for such customers under imperfect classification. Further, we study the interaction between the two policy decisions and discuss how one decision should be modified with respect to a change in the other one in order to keep the waiting costs minimized. © 2011 ACM.",Customer classification; Live-chat system; Priority policy; Resource allocation,Computer resource management; Customer satisfaction; Decision making; Electronic commerce; Industry; Numerical analysis; Resource allocation; Average waiting time; Classification system; Customer classification; E-commerce systems; Live-chat system; Policy decisions; Product sales; Service resources; Sales
Six principles for redesigning executive information systems-findings of a survey and evaluation of a prototype,2011,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860587692&doi=10.1145%2f2070710.2070717&partnerID=40&md5=1e5fb672ed741ea6d2cc385b18a6cb77,"Information Systems (IS) meant to help seniormanagers are known as Executive Information Systems (EIS). Despite a five-decade tradition of such IS, many executives still complain that they bear little relevance to managing a company and, even more, fail to accommodate their working style. The increasing acceptance of IS among today's executives and technological advances of the Internet era make the present moment favorable for redesigning EIS. Following the design science paradigm in IS research, this article provides six principles for such a redesign. To do so, we survey executives regarding their requirements and the IS they currently use. We then derive principles for a redesign to fill the gaps. They address diverse areas: a comprehensive information model, functions to better analyze and process information, easy-to-use IS handling, a more flexible IS architecture and data model, a proper information management, and fast prototype implementation. Finally a field test demonstrates and evaluates the utility of our proposal by means of a prototype. © 2011 ACM.",Corporate business intelligence; Design principles; Design requirements; Executive information system (EIS); Working style,Design; Information management; Information theory; Management information systems; Surveys; Comprehensive information; Design Principles; Design requirements; Design science; Executive information systems; Field test; IS architecture; Process information; Prototype implementations; Technological advances; Working styles; Information systems
Designing a social-broadcasting-based business intelligence system,2011,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860592814&doi=10.1145%2f2070710.2070713&partnerID=40&md5=3cda37b590f6f32f6b5ea00de2c90e60,"The rise of social media has fundamentally changed the way information is produced, disseminated, and consumed in the digital age, which has profound economic and business effects. Among many different types of social media, social broadcasting networks such as Twitter in the U.S. and ""Weibo"" in China are particularly interesting from a business perspective. In the case of Twitter, the huge amounts of real-time data with extremely rich text, along with valuable structural information, makes Twitter a great platform to build Business Intelligence (BI) systems. We propose a framework of social-broadcasting-based BI systems that utilizes real-time information extracted from these data with text mining techniques. To demonstrate this framework, we designed and implemented a Twitter-based BI system that forecasts movie box office revenues during the opening weekend and forecasts daily revenue after 4 weeks.We found that incorporating information from Twitter could reduce the Mean Absolute Percentage Error (MAPE) by 44% for the opening weekend and by 36% for total revenue. For daily revenue forecasting, including Twitter information into a baseline model could reduce forecasting errors by 17.5% on average. On the basis of these results, we conclude that social-broadcasting- based BI systems have great potential and should be explored by both researchers and practitioners. © 2011 ACM.",Business intelligence; Forecasting; Social broadcasting; Twitter,Broadcasting; Commerce; Competitive intelligence; Forecasting; Motion pictures; Baseline models; Box office; Business intelligence systems; Business perspective; Digital age; Forecasting error; Mean absolute percentage error; Real-time data; Real-time information; Social media; Structural information; Text mining techniques; Total revenue; Twitter; Social networking (online)
Text mining and probabilistic language modeling for online review spam detection,2011,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859702088&doi=10.1145%2f2070710.2070716&partnerID=40&md5=5fb5d5f95c919d69f63f8863301ec9e9,"In the era of Web 2.0, huge volumes of consumer reviews are posted to the Internet every day. Manual approaches to detecting and analyzing fake reviews (i.e., spam) are not practical due to the problem of information overload. However, the design and development of automated methods of detecting fake reviews is a challenging research problem. The main reason is that fake reviews are specifically composed to mislead readers, so they may appear the same as legitimate reviews (i.e., ham). As a result, discriminatory features that would enable individual reviews to be classified as spam or ham may not be available. Guided by the design science research methodology, the main contribution of this study is the design and instantiation of novel computational models for detecting fake reviews. In particular, a novel text mining model is developed and integrated into a semantic language model for the detection of untruthful reviews. The models are then evaluated based on a real-world dataset collected from amazon.com. The results of our experiments confirm that the proposed models outperform other well-known baseline models in detecting fake reviews. To the best of our knowledge, the work discussed in this article represents the first successful attempt to apply text mining methods and semantic language models to the detection of fake consumer reviews. A managerial implication of our research is that firms can apply our design artifacts to monitor online consumer reviews to develop effective marketing or product design strategies based on genuine consumer feedback posted to the Internet. © 2011 ACM.",Design science; Language models; Review spam; Spam detection; Text mining,Computational linguistics; Mining; Natural language processing systems; Product design; Research; Semantics; Automated methods; Baseline models; Computational model; Data sets; Design and Development; Design artifacts; Design science; Design strategies; Information overloads; Language model; Managerial implications; Online consumer reviews; Probabilistic language; Research problems; Semantic language; Spam detection; Text mining; Web 2.0; Internet
Decision support for containing pandemic propagation,2011,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860597726&doi=10.1145%2f2070710.2070714&partnerID=40&md5=4eaf93e24d3f4f39569a7f006cc58125,"This research addresses complexities inherent in dynamic decision making settings represented by global disasters such as influenza pandemics. By coupling a theoretically grounded Equation-Based Modeling (EBM) approach with more practically nuanced Agent-Based Modeling (ABM) approach we address the inherent heterogeneity of the ""influenza pandemic"" decision space more effectively. In addition to modeling contributions, results and findings of this study have three important policy implications for pandemic containment; first, an effective way of checking the progression of a pandemic is a multipronged approach that includes a combination of pharmaceutical and non-pharmaceutical interventions. Second, mutual aid is effective only when regions that have been affected by the pandemic are sufficiently isolated from other regions through non-pharmaceutical interventions. When regions are not sufficiently isolated, mutual aid can in fact be detrimental. Finally, intraregion non-pharmaceutical interventions such as school closures are more effective than interregion nonpharmaceutical interventions such as border closures. © 2011 ACM.",Dynamic decision making; Multiagent simulation; Pandemics; Public health; Resource allocation,Decision making; Decision support systems; Public health; Resource allocation; Agent-based modeling; Decision space; Decision supports; Dynamic decision making; Equation-based modeling; Global disasters; Influenza pandemics; Multi agent simulation; Mutual aid; Non-pharmaceutical interventions; Pandemics; Policy implications; Public policy
"From telesales to tele-advisory in travel agencies: Business problems, generic design goals and requirements",2011,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859698033&doi=10.1145%2f2019618.2019623&partnerID=40&md5=41fc45053ed81c3d939e9fd234c1c422,"This article describes a multiloop design approach, demonstrating two concatenated build-and-evaluate loops of a project that aims at enhancing and improving tele-counseling at travel agency call-centers. The socio-technical system design intends to implement and support a collaborative travel counseling concept which equalizes travel agents and customers within a value co-created service encounter to resolve the specific problems in travel agency call-centers. The design is finally guided by four instrumental goals: (1) increase transparency, (2) improve information quality, (3) support joint problem solving, and (4) create advisory experience. While the first build-and-evaluate loop mainly enhanced the workplace picture, the second build-and-evaluate loop primarily revealed organizational demands. This indicates that the proposed design approach enables sequentially broadening the problem space for design and supports iteratively moving forward the utility of the emerging artifact. Design is informed by concatenating insights from the previous loops which enrich understanding of the instrumental goals. The presented goals as well as the design approach are encouraging candidates for additional testing as a base for general design principles. © 2011 ACM.",Concatenation; Design science; Remote travel counseling; Tele-advisory service; Value cocreation,Rating; Co-creation; Concatenation; Design science; Remote travel counseling; Tele-advisory service; Design
A multilabel text classification algorithm for labeling risk factors in sec form 10-K,2011,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860546442&doi=10.1145%2f2019618.2019624&partnerID=40&md5=d508f1567f4120f852989463a71ac15e,"This study develops, implements, and evaluates a multilabel text classification algorithm called the multilabel categorical K-nearest neighbor (ML-CKNN). The proposed algorithm is designed to automatically identify 25 types of risk factors with specific meanings reported in Section 1A of SEC form 10-K. The idea of ML-CKNN is to compute a categorical similarity score for each label by the K-nearest neighbors in that category. ML-CKNN is tailored to achieve the goal of extracting risk factors from 10Ks. The proposed algorithm can perfectly classify 74.94% of risk factors and 98.75% of labels. Moreover, ML-CKNN is empirically shown to outperform ML-KNN and other multilabel algorithms. The extracted risk factors could be valuable to empirical studies in accounting or finance. © 2011 ACM.",Annual reports; Multilabel classification; Risk factors; Text classification; Text mining,Membership functions; Text processing; Annual reports; Multi-label; Risk factors; Text classification; Text mining; Algorithms
Latent subject-centered modeling of collaborative tagging: An application in social search,2011,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859729088&doi=10.1145%2f2019618.2019621&partnerID=40&md5=f9a396947f17f396cb73a5834c351381,"Collaborative tagging or social bookmarking is a main component of Web 2.0 systems and has been widely recognized as one of the key technologies underpinning next-generation knowledge management platforms. In this article, we propose a subject-centered model of collaborative tagging to account for the ternary cooccurrences involving users, items, and tags in such systems. Extending the well-established probabilistic latent semantic analysis theory for knowledge representation, our model maps the user, item, and tag entities into a common latent subject space that captures the ""wisdom of the crowd"" resulted from the collaborative tagging process. To put this model into action, we have developed a novel way to estimate the probabilistic subject- centeredmodel approximately in a highly efficient manner taking advantage of amatrix factorization method. Our empirical evaluation shows that our proposed approach delivers substantial performance improvement on the knowledge resource recommendation task over the state-of-the-art standard and tag-aware resource recommendation algorithms. © 2011 ACM.",Collaborative tagging; Item recommendation; Social search; Subject-centered modeling; Tag-based recommendation,Knowledge representation; Collaborative tagging; Empirical evaluations; Factorization methods; Item recommendation; Key technologies; Knowledge management platform; Knowledge resource; Model maps; Performance improvements; Probabilistic latent semantic analysis; Resource recommendation; Social bookmarking; Social search; Tag-based; Web 2.0; User interfaces
Eliciting a sense of virtual community among knowledge contributors,2011,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860596289&doi=10.1145%2f2019618.2019620&partnerID=40&md5=ebb4aa7d43c7c04c2d0a68fc2bf718dd,"Member-initiated virtual communities for product knowledge sharing and commerce purposes are proliferating as useful alternatives to company information and commerce Web sites. Although such communities are easy to create with the availability of numerous tools, the challenge lies in keeping the community alive and thriving. Key to sustainability is members' Sense Of Virtual Community (SOVC) so that they feel responsible for contributing their knowledge and creating value for others. However, it is unclear what leads to the SOVC among knowledge contributors. Building on appraisal theory, we hypothesize that the fulfillment of contributors' informational, instrumental, entertainment, self-discovery, and social enhancement needs will increase their SOVC. To test the hypotheses, we surveyed knowledge contributors in a beautyproduct- related community to examine the relationship between their needs' fulfillment and SOVC levels. Other than the social enhancement need, all other needs' fulfillment were found to be positively related to SOVC levels. To further understand how the SOVC of knowledge contributors changes over time, we conducted a longitudinal analysis of a panel of these members. We discovered that over time, changes in the perceived fulfillment of their instrumental, entertainment, and self-discovery needs determined the change of their SOVC. The results have implications for future research as well as for the sustainability and value generation from such virtual communities. c© 2011 ACM.",Knowledge contributors; Longitudinal analysis; Needs fulfillment; Sense of virtual community,Hardware; Information systems; Appraisal theory; Knowledge contributors; Longitudinal analysis; Needs fulfillment; Product knowledge; Virtual community; Commerce
Cloud-based malware detection for evolving data streams,2011,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860563303&doi=10.1145%2f2019618.2019622&partnerID=40&md5=fb0ced36db9e536c72e5698f040b01a4,"Data stream classification for intrusion detection poses at least three major challenges. First, these data streams are typically infinite-length,making traditional multipass learning algorithms inapplicable. Second, they exhibit significant concept-drift as attackers react and adapt to defenses. Third, for data streams that do not have any fixed feature set, such as text streams, an additional feature extraction and selection task must be performed. If the number of candidate features is too large, then traditional feature extraction techniques fail. In order to address the first two challenges, this article proposes a multipartition, multichunk ensemble classifier in which a collection of v classifiers is trained from r consecutive data chunks using v-fold partitioning of the data, yielding an ensemble of such classifiers. This multipartition, multichunk ensemble technique significantly reduces classification error compared to existing single-partition, single-chunk ensemble approaches, wherein a single data chunk is used to train each classifier. To address the third challenge, a feature extraction and selection technique is proposed for data streams that do not have any fixed feature set. The technique's scalability is demonstrated through an implementation for the Hadoop MapReduce cloud computing architecture. Both theoretical and empirical evidence demonstrate its effectiveness over other state-of-the-art stream classification techniques on synthetic data, real botnet traffic, and malicious executables. © 2011 ACM.",Data mining; Data streams; Malicious executable; Malware detection; N-gram analysis,Computer viruses; Data communication systems; Feature extraction; Intrusion detection; Learning algorithms; Classification errors; Computing architecture; Data chunks; Data stream; Empirical evidence; Ensemble classifiers; Ensemble techniques; Feature extraction and selection; Feature extraction techniques; Feature sets; Malicious executable; Malicious executables; Malware detection; Map-reduce; Multi-pass; N-gram analysis; Stream classification; Synthetic data; Text streams; Data mining
"Knowledge management revisited: Old dogs, new tricks",2011,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860592274&doi=10.1145%2f2019618.2019619&partnerID=40&md5=233402a48372e686ed715f37b05c3aa3,"The article considers the (old) field of Knowledge Management (KM) and examines the factors that led to its decline over the last several years. It also argues that the demise of knowledge management is premature and that it is time to revisit the field and take it to the ""next level"" of academic research and business practice. Then the article presents a proposal of how to do it by deploying certain ""new tricks,"" that is, various modern developments in the fields related to KM that can make a difference and solve the old problems of knowledge management. © 2011 ACM.",Content management; Content processing and analysis; Knowledge management; Personalization; User-generated content,Knowledge engineering; Academic research; Business practices; Content management; Content processing; Modern development; Personalizations; User-generated content; Knowledge management
Stock price movement prediction using representative prototypes of financial reports,2011,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859703864&doi=10.1145%2f2019618.2019625&partnerID=40&md5=aef83124153ae8dafcfa46aa17673779,"Stock price movement prediction is an appealing topic not only for research but also for commercial applications. Most of prior research separately analyzes the meanings of the qualitative or quantitative features, and does not consider the categorical information when clustering financial reports. Since quantitative or qualitative features contain only partial information, there may be no synergy by considering them individually. It is more appropriate to predict stock price movements by simultaneously taking both quantitative and qualitative features into account. Therefore, in this study, we utilize a weighting scheme to combine both qualitative and quantitative features of financial reports together, and propose a method to predict short-term stock price movements. The proposed method employs the categorical information to localize the clusters and improve the purity of each resultant cluster. We gathered 26,255 reports of companies listed in the S&P 500 index from the EDGAR database and conducted the GICS (Global Industrial Classification System) experiments based on the industry sectors. The empirical evaluation results show that the proposed method outperforms the SVM, näive Bayes, and PFHC methods in terms of accuracy and average profit. © 2011 ACM.",Document clustering; Financial report; Hybrid clustering; Stock price movement prediction,Finance; Profitability; Classification system; Commercial applications; Document Clustering; Empirical evaluations; Financial reports; Hybrid clustering; Industry sectors; Partial information; Qualitative features; Quantitative features; Stock price; Weighting scheme; Forecasting
Who does what: Collaboration patterns in the wikipedia and their impact on article quality,2011,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860580992&doi=10.1145%2f1985347.1985352&partnerID=40&md5=f6ae723c95fe23d47302f0bb4945507a,"The quality of Wikipedia articles is debatable. On the one hand, existing research indicates that not only are people willing to contribute articles but the quality of these articles is close to that found in conventional encyclopedias. On the other hand, the public has never stopped criticizing the quality of Wikipedia articles, and critics never have trouble finding low-quality Wikipedia articles. Why do Wikipedia articles vary widely in quality? We investigate the relationship between collaboration and Wikipedia article quality. We show that the quality of Wikipedia articles is not only dependent on the different types of contributors but also on how they collaborate. Based on an empirical study, we classify contributors based on their roles in editing individualWikipedia articles.We identify various patterns of collaboration based on the provenance or, more specifically, who does what to Wikipedia articles. Our research helps identify collaboration patterns that are preferable or detrimental for article quality, thus providing insights for designing tools and mechanisms to improve the quality of Wikipedia articles. © 2011 ACM.",Article quality; Collaboration pattern; Wikipedia,Hardware; Information systems; Collaboration patterns; Designing tools; Empirical studies; Low qualities; Wikipedia; Websites
Risk hedging in storage grid markets: Do options add value to forwards?,2011,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860579424&doi=10.1145%2f1985347.1985351&partnerID=40&md5=5d09f1e7e901bfe7612f24b559e39503,"Internet storage services allow businesses to move away from maintaining their own internal storage networks. Service providers currently follow a utility pricing model which translates to them absorbing all the risk that arises from the fluctuating storage needs of their customers. The risk borne by the Internet storage service providers has large revenue implications as Internet startups and smaller companies, which face significant demand stochasticity, constitute an important segment of their clientele. We develop an option pricing mechanism to hedge against this risk and evaluate its effectiveness vis-A-vis forward contracts. We obtain the conditions under which options dominate forward contracts and the trade-offs involved when the provider has to decide on appropriate pricing mechanisms. Our empirical study uses publicly obtainable traffic data of Amazon S3 clients to validate the analytical results. We show that providers can significantly benefit from including options in their risk-hedging portfolio, especially when there is less variation in the costs faced by the buyers in building their own data networks as opposed to using cloud services. © 2011 ACM.",Cloud computing; Forward contracts; Grid computing; Online storage; Options,Cloud computing; Commerce; Costs; Economic and social effects; Economics; Grid computing; Internet; Analytical results; Cloud services; Data network; Empirical studies; Forward contract; Grid market; In-buildings; Internal storage; Option pricing; Options; Pricing mechanism; Risk hedging; Service provider; Stochasticity; Storage services; Traffic data; Utility pricing; Web services
Digital goods and markets: Emerging issues and challenges,2011,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860576563&doi=10.1145%2f1985347.1985349&partnerID=40&md5=ed4fc430d57c624e6b592060f0de998d,"This research commentary examines the changing landscape of digital goods, and discusses important emerging issues for IS researchers to explore. We begin with a discussion of the major technological milestones that have shaped digital goods industries such as music, movies, software, books, video games, and recently emerging digital goods. Our emphasis is on economic and legal issues, rather than on design science or sociological issues. We explore how research has been influenced by the major technological milestones and discuss the major findings of prior research. Based on this, we offer a roadmap for future researchers to explore the emergent changes in the digital goods arena, covering different aspects of digital goods industries such as risk management, value chain, legal aspects, transnational and cross-cultural issues. © 2011 ACM.",Cross-cultural issues; Digital goods; Digital goods markets; Disembodiment; Distribution; Distribution channels; Emerging technologies; Piracy; Pricing; Risk management; Value chain,Costs; Design; Research; Risk management; Cross-cultural issues; Digital goods; Disembodiment; Distribution; Distribution channel; Emerging technologies; Piracy; Value chains; Computer crime
Trust in a specific technology: An investigation of its components and measures,2011,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84856341794&doi=10.1145%2f1985347.1985353&partnerID=40&md5=f2cc0380a17e95599fc2cfb337dd11d6,"Trust plays an important role in many Information Systems (IS)-enabled situations. Most IS research employs trust as a measure of interpersonal or person-to-firm relations, such as trust in a Web vendor or a virtual team member. Although trust in other people is important, this article suggests that trust in the Information Technology (IT) itself also plays a role in shaping IT-related beliefs and behavior. To advance trust and technology research, this article presents a set of trust in technology construct definitions and measures.We also empirically examine these constructmeasures using tests of convergent, discriminant, and nomological validity. This study contributes to the literature by providing: (a) a framework that differentiates trust in technology from trust in people, (b) a theory-based set of definitions necessary for investigating different kinds of trust in technology, and (c) validated trust in technology measures useful to research and practice. © 2011 ACM.",Construct development; Trust; Trust in technology,Research; Construct development; Technology research; Trust; Virtual team; Web vendors; Information technology
Explaining U.S. consumer behavior with news sentiment,2011,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860571110&doi=10.1145%2f1985347.1985350&partnerID=40&md5=6a9b8174a8cc0b0381577687e9ca612a,"We introduce a novel dataset with a news sentiment index that was constructed from a selection of over 300,000 newspaper articles from five of the top ten U.S. newspapers by circulation. By constructing ARMA models, we show that news and consumer sentiment, when combined with other macroeconomic variables, achieve statistically significant results to explain changes in private consumption. We make three distinct findings with respect to sentiment in consumption behavior models: first, both consumer and news sentiment add explanatory power and statistical significance to conventional consumer behavior models. Second, consumer sentiment, measured by the University of Michigan Index of Consumer Sentiment, adds more explanatory power and statistical significance than news sentiment when tested individually. Third, news sentiment is able to determine the signs of all coefficients in the model correctly, whereas consumer sentiment does not. In general, we conclude that news sentiment is a useful variable to add in consumer behavior models, especially when coupled with consumer sentiment and other macroeconomic variables. Tested individually, news sentiment is as good a proxy as personal income for explaining private consumption growth when tested individually. © 2011 ACM.",Consumer sentiment; News sentiment; Private consumption,Newsprint; ARMA model; Behavior model; Consumer sentiment; Data sets; Explanatory power; Macroeconomic variables; News sentiment; Personal income; Private consumption; Sentiment index; Statistical significance; University of Michigan; Consumer behavior
Analyzing information systems researchers' productivity and impacts: A perspective on the H index,2011,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860551808&doi=10.1145%2f1985347.1985348&partnerID=40&md5=bac4b52882f90a644874b577d19780cb,"Quantitative assessments of researchers' productivity and impacts are crucial for the information systems (IS) discipline. Motivated by its growing popularity and expanding use, we offer a perspective on the h index, which refers to the number of papers a researcher has coauthored with at least h citations each. We studied a partial list of 232 top IS researchers who received doctoral degrees between 1957 and 2003 and chose Google Scholar as the source for our analyses. At the individual level, we attempted to identify some of the most productive, high-impact researchers, as well as those who exhibited impressive paces of productivity. At the institution level, we revealed some institutions with relatively more productive researchers, as well as institutions that had produced more productive researchers. We also analyzed the overall IS community by examining the primary research areas of productive scholars identified by our analyses. We then compared their h index scores with those of top scholars in several related disciplines. © 2011 ACM.",Analysis of scholars' productivity and impacts; H index; Information systems,Information systems; Productivity; Analysis of scholars' productivity and impacts; Doctoral degrees; Google scholar; H indices; Quantitative assessments; Research
Computer use and wage returns: The complementary roles of it-related human capital and nonroutine tasks,2011,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-81455128758&doi=10.1145%2f1929916.1929922&partnerID=40&md5=d71526da7240ca2a6311d89968aefcd1,"The effect of computer use on individual workers is relatively underresearched in the IS literature. Prior studies on computer use usually treat technologies as a ""black box"" and rarely look into how computers are used in workplaces and why IT-related skills are important there. In this study, we use the data from the Current Population Survey (CPS) and the data on job requirements for over 12,000 occupations from the Dictionary of Occupational Titles (DOT), to examine the complementary roles of skill sets and nonroutine tasks in providing computer-use wage returns for individual workers. We find that computer use is associated with increased levels of interactive and numerical skills required for the general workforce. In addition, workers who use computers at work and possess higher levels of interactive skills receive higher wages. We also find that computer use complements performing nonroutine tasks, particularly nonroutine abstract tasks in contributing to the wage premium. As the tasks become increasingly routine, the impact of computer use on wage returns diminishes. © 2011 ACM.",Computer use; Human capital; Nonroutine tasks; Wage returns,Computer science; Black boxes; Computer use; Human capitals; Nonroutine tasks; Population survey; Skill sets; Wages
A multimethod study of information quality in wiki collaboration,2011,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80051762643&doi=10.1145%2f1929916.1929920&partnerID=40&md5=61877300db6df2c2f0ae78459d5665b3,"In this article, the author presents the results of a two-phase, multimethod study of wiki-based collaboration in an attempt to better understand how peer-produced collaboration is done well in wiki environments. Phase 1 involves an in-depth case study of the collaborative processes surrounding the development of the Wikipedia article on the 2007 Virginia Tech massacre. The rich data collected are used to develop an initial set of testable hypotheses of factors that enhance the quality of peer-produced information in wiki environments. Phase 2 tests these theories through a quantitative analysis of the collaborative features associated with 188 similar articles that Wikipedia considered for recognition as their best (i.e., the top 0.1%). Four collaborative features are examined for their effects on quality: volume of contributor activity, type of contributor activity, number of anonymous contributors, and top contributor experience. Volume of contributor activity is the only feature that is unsupported, a particularly interesting result because previous literature connects that factor most clearly to success in wiki-based collaboration. Implications are discussed. © 2011 ACM.",Anonymity; Collaboration; Electronic collaboration; Electronic communities; Information quality; Multimethod studies; Peerproduction; Shaping; Virtual teams; Web 2.0; Wiki; Wikipedia,Data mining; Websites; Anonymity; Collaboration; Electronic collaboration; Information quality; Multi methods; Peerproduction; Shaping; Virtual team; Web 2.0; Wiki; Wikipedia; Web services
"Editorial: Design science, grand challenges, and societal impacts",2011,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859715811&doi=10.1145%2f1929916.1929917&partnerID=40&md5=fbd46cbfda50b6cf67284bbf1ecbac59,[No abstract available],,
A linguistic analysis of group support systems interactions for uncovering social realities of organizations,2011,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-82455182687&doi=10.1145%2f1929916.1929919&partnerID=40&md5=ff2202ae5a4db5445384b68d89889b87,"Talks are actions, and language represents the medium through which we encounter reality, carry out practical reasoning, and construct social actions. This study applies the speech act theory to analyze the data collected in a study by Trauth and Jessup [2000] and confirms previous research findings that both the topic and the group size influence the pattern of discussion, especially when issues are threatening. It also shows that the abundance of speech acts like assertives, directives, and expressives can be accounted for by a few simple recurring patterns, indicating participants are rather close-minded. More important, linguistic analysis helps uncovering defensive speech routines that inhibit the generation of valid information and create self-sealing patterns of escalating error. Linguistic analysis may therefore complement positivist and interpretive analysis by examining if participants' engagement is superficial or profound, if consensus is reached or blocked, and if certain speech acts lead to dysfunctional organizational learning. Hence, in the era of participatory Web in which language is the primary medium for interactive sharing and dynamic collaboration, linguistic analysis can be applied to study the promises and declarations that people rely on to initiate, coordinate, and complete social actions. © 2011 ACM.",Group dynamic; Group support systems; Linguistic analysis; Speech act theory,Decision support systems; Group theory; Groupware; Linguistics; Dynamic collaborations; Group dynamics; Group size; Group support systems; Linguistic analysis; Organizational learning; Practical reasoning; Social reality; Speech act theory; Speech acts; Speech analysis
Visualizing web search results using glyphs: Design and evaluation of a flower metaphor,2011,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855785933&doi=10.1145%2f1929916.1929918&partnerID=40&md5=400103e3897f1e9b9d31e9a4efc6c47e,"While the Web provides a lot of useful information to managers and decision makers in organizations for decision support, it requires a lot of time and cognitive effort for users to sift through a search result list returned by search engines to find useful information. Previous research in information visualization has shown that visualization techniques can help users comprehend information and accomplish information tasksmore efficiently and effectively. However, only a limited number of such techniques have been applied to Web search result visualization withmixed evaluation results. Using a design science approach, this research designed and implemented a glyph (a graphical object that represents the values of multiple dimensions using multiple visual parameters) and a system for visualizing Web search results. A flower metaphor was adopted in the glyph design to represent the characteristics and metadata of Web documents. Following the cognitive fit theory, an experimental study was conducted to evaluate three displays: a numeric display, a glyph display, and a combined display which showed numbers only, glyphs only, and both, respectively. Experimental results showed that the glyph display and the combined display performed better when task complexity was high, and the numeric display and the combined display performed better when task complexity was low. The combined display also received the best perceived usability from the subjects. Based on the findings, the implications of the study to research and practice are discussed and some future research directions are suggested. © 2011 ACM.",Design science; Glyphs; Information visualization; Web searching,Decision support systems; Design; Information systems; Information use; Metadata; Research; Search engines; Visualization; Websites; Cognitive fit theory; Decision makers; Decision supports; Design science; Evaluation results; Experimental studies; Future research directions; Glyphs; Graphical objects; Information visualization; Multiple dimensions; Perceived usability; Search results; Task complexity; Visualization technique; Web document; Web searches; Web-searching; Information retrieval
Uncovering and testing archetypes of effective public sector CIOs,2011,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860583444&doi=10.1145%2f1929916.1929921&partnerID=40&md5=48d611ef583ac3fd92813e0dd3fc9053,"Given the importance of public sector CIOs to government performance and citizens' faith in democracy as an efficient provider of services, it is important to understand what makes some government CIOs more effective than others. QMethod is used to uncover five archetypes of public sector CIOs which are shown to be reliable across two Q sorts. These archetypes include politically-oriented CIO, savvy negotiator, technology optimizer, and skillful communicator. Further analysis using a tournament scoring approach indicates that business-oriented CIOs are the most effective. Applying a stakeholder perspective to interpret the results, it is proposed that business-oriented CIOs understand the value in tracking closely to an organization's business leaders and strategically ignoring other stakeholders in their environment, even politically powerful ones. The development and comparison of archetypes provide a new focus of CIO research by extending from the individual level of the attribute to a combination of attributes (archetypes). © 2011 ACM.",Chief information officer; Information asymmetry; Leadership effectiveness; Public sector; Stakeholder theory; Strategic alignment,Information systems; Chief information officer; Information asymmetry; Leadership effectiveness; Public sector; Stakeholder theory; Strategic alignment; Hardware
Ideas for the future of the IS field,2010,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-83455169934&doi=10.1145%2f1877725.1877727&partnerID=40&md5=d32c036ba87898ffbc7fd8a6de6fa8d5,"Information systems as a field of intellectual inquiry is now approximately 50 years old. It hasmany achievements and extensive research to its credit and has established a large group of researchers and experts worldwide. The field has changed and changed and changed again over the last half century. The question addressed in this inaugural issue article is:Where does IS go from here? This article presents the views of six of the ""fathers of the field"" about its directions in the years ahead. Each coauthor presents two ideas about the future. The topics covered includes continuing support of the work of organizations, emerging technologies, new ways of communicating, expanding the ways IS performs research, expanding its vision both of what IS is and of its impact, its role as a resource, its model of the IS professional and its graduates, and its staying on top of new technologies and new areas of inquiry. © 2010 ACM.",Emerging technologies; Health care; Information systems impacts; Information systems vision; Innovations; Model of graduate; Model of professional; New technologies; Professional communications; Publications; Researchmethods; Systems requirements; University-wide resource,Health care; Information systems; Innovation; Medical computing; Professional aspects; Research; Emerging technologies; Professional communication; Publications; Researchmethods; Systems requirements; University-wide resource; Engineering education
Why give away something for nothing? Investigating virtual goods pricing and permission strategies,2010,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860555276&doi=10.1145%2f1877725.1877729&partnerID=40&md5=ad74d909d50ad0da031206b3bbaf688c,"With the rapid increase of virtual goods created for virtual world exchanges and the record growth of user-to-user transactions in these in-world economies, an important question is how a creator sets prices for a virtual good so as to maximize her profit from her creation. Virtual goods share similar economic properties (such as substantial production cost and negligible marginal cost) with other types of digital goods. However, one aspect that distinguishes a virtual good is that consumers in a virtual world may want to use multiple copies of the identical good at the same time, and such simultaneous use of multiple copies of the identical good increases a consumer's utility. In this research, we focus on the COPY permission of virtual goods.We develop an economic model to examine under what conditions the COPY permission setting leads to the highest profit for the creator of a virtual good, and what the pricing strategies are in a dynamic setting when such permission choices are present. Theoretical and practical implications of the research are discussed. © 2010 ACM.",Digital goods; Permissions; Pricing strategy; Virtual goods; Virtual worlds,Economics; Interactive computer graphics; Profitability; Virtual reality; Digital goods; Permissions; Pricing strategy; Virtual goods; Virtual worlds; Costs
Modeling dynamics in agile software development,2010,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860580767&doi=10.1145%2f1877725.1877730&partnerID=40&md5=8a9d4bdf78ba02dab33b6e1193a33f7c,"Changes in the business environment such as turbulent market forces, rapidly evolving system requirements, and advances in technology demand agility in the development of software systems. Though agile approaches have received wide attention, empirical research that evaluates their effectiveness and appropriateness is scarce. Further, research to-date has investigated individual practices in isolation rather than as an integrated system. Addressing these concerns, we develop a system dynamics simulation model that considers the complex interdependencies among the variety of practices used in agile development. The model is developed on the basis of an extensive review of the literature as well as quantitative and qualitative data collected from real projects in nine organizations. We present the structure of the model focusing on essential agile practices. The validity of the model is established based on extensive structural and behavioral validation tests. Insights gained from experimentation with the model answer important questions faced by development teams in implementing two unique practices used in agile development. The results suggest that due to refactoring, the cost of implementing changes to a system varies cyclically and increases during later phases of development.Delays in refactoring also increase costs and decrease development productivity. Also, the simulation shows that pair programming helps complete more tasks and at a lower cost. The systems dynamics model developed in this research can be used as a tool by IS organizations to understand and analyze the impacts of various agile development practices and project management strategies. © 2010 ACM.",Agile software development; Process modeling; Simulation; System dynamics,Computer simulation; Costs; Project management; Research; Software design; Agile approaches; Agile development; Agile practices; Agile software development; Business environments; Development productivity; Development teams; Empirical research; Evolving systems; Integrated systems; Management strategies; Market forces; Pair-programming; Process Modeling; Qualitative data; Real projects; Refactorings; Simulation; Software systems; System Dynamics; System dynamics simulation; Systems dynamics; Validation test; Agile manufacturing systems
Drivers of information security search behavior: An investigation of network attacks and vulnerability disclosures,2010,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-82255171897&doi=10.1145%2f1877725.1877728&partnerID=40&md5=8a3137d8dc81569f60d810529f00e47c,"More and more people use search engines to seek for various information. This study investigates the search behavior that drives the search for information security knowledge via a search engine. Based on theories in information search and information security behavior we examine the effects of network attacks and vulnerability disclosures on search for information security knowledge by ordinary users. We construct a unique dataset from publicly available sources, and use a dynamic regression model to test the hypotheses empirically.We find that network attacks of current day and one day prior significantly impact the search, while vulnerability disclosure does not significantly affect the search. Implications of the study are discussed. © 2010 ACM.",Dynamic regression; Information search behavior; Information security; Information systems; Network attacks; Search engine; Vulnerability disclosures,Computer crime; Information retrieval; Information systems; Regression analysis; Search engines; Security of data; Statistical tests; Data sets; Dynamic regression models; Information search; Information search behavior; Network attack; Search behavior; Vulnerability disclosure; Network security
Editorial: Welcome to the first issue of ACM TMIS,2010,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860563942&doi=10.1145%2f1877725.1877726&partnerID=40&md5=88ae2002c5f26668e1413ef0209e0d7a,[No abstract available],,
The sustainability of corporate wikis: A time-series analysis of activity patterns,2010,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860598476&doi=10.1145%2f1877725.1877731&partnerID=40&md5=b43feedbaa8eada40a72692647c91502,"While existing theoretical frameworks describe collective technology adoption patterns, they provide little insight regarding the expected patterns of wiki activity within projects. Another impediment to the study of wiki sustainability is the absence of time-series analysis methods that are suitable for the unique patterns of wiki activity logs. The primary goals of this study are to: (i) develop a novel method for analyzing wiki edit activity logs, (ii) reveal the temporal patterns of corporate wiki edit activity, and (iii) study the factors impacting wikis' sustainability. A validation of our proposed method demonstrates that it is superior to the baseline algorithm in the face of noisy data. Our empirical study combines wiki system edit activity logs with a survey of users' perceptions, and explores 33277 distinct wiki applications within one global organization over the first 5 years of wiki operation. Our results reveal six different prototypical wiki activity patterns, and show that most corporate wikis become inactive after a relatively short period. Findings from the user survey show that users of sustainable wikis are more satisfied with the wiki system and its contents, and feel that the wiki provides them with a sense of community and productivity enhancements. © 2010 ACM.",Clustering; Corporate; Edit activity patterns; Lifecycle; Sustainability; Time series; Wiki,Harmonic analysis; Life cycle; Surveys; Sustainable development; Time series; Activity patterns; Clustering; Corporate; Empirical studies; Global organization; Noisy data; Productivity enhancement; Sense of community; Short periods; Technology adoption; Temporal pattern; Theoretical framework; User surveys; Users' perception; Wiki; Websites
A privacy protection technique for publishing data mining models and research data,2010,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-82955228457&doi=10.1145%2f1877725.1877732&partnerID=40&md5=d2c7313b607099c37484a6b460cfc2b6,"Data mining techniques have been widely used in many research disciplines such as medicine, life sciences, and social sciences to extract useful knowledge (such as mining models) from research data. Research data often needs to be published along with the data mining model for verification or reanalysis. However, the privacy of the published data needs to be protected because otherwise the published data is subject to misuse such as linking attacks. Therefore, employing various privacy protection methods becomes necessary. However, these methods only consider privacy protection and do not guarantee that the same mining models can be built from sanitized data. Thus the published models cannot be verified using the sanitized data. This article proposes a technique that not only protects privacy, but also guarantees that the same model, in the form of decision trees or regression trees, can be built from the sanitized data. We have also experimentally shown that other mining techniques can be used to reanalyze the sanitized data. This technique can be used to promote sharing of research data. © 2010 ACM.",Preserving data mining,Data Processing; Decision Making; Forestry; Data privacy; Decision trees; Forestry; Research; Trees (mathematics); Data mining models; Data mining techniques; Life-sciences; Mining techniques; Privacy protection; Reanalysis; Regression trees; Research data; Data mining
A Decision Framework to Recommend Cruising Locations for Taxi Drivers under the Constraint of Booking Information,2022,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133408684&doi=10.1145%2f3490687&partnerID=40&md5=8f1d7dd0204b97626f2117b1240ef1a8,"As the demand for taxi reservation services has increased, increasing the income of taxi drivers with advanced services has attracted attention. In this article, we propose a path decision framework that considers real-time spatial-temporal predictions and traffic network information. The goal is to optimize a taxi driver's profit when considering a reservation. Our framework contains four components. First, we build a grid-based road network graph for modeling traffic network information for speeding up the search process. Next, we conduct two prediction modules that adopt advanced deep learning techniques to guide proper search directions for recommending cruising locations. One module of the taxi demand prediction is used to estimate the pick-up probabilities of passengers in the city. Another one is destination prediction, which can predict the distribution of drop-off probabilities and capture the flow of potential passengers. Finally, we propose the H∗ (Heuristic-star) algorithm, which jointly considers pick-up probabilities, drop-off distribution, road network, distance, and time factors based on the attentive heuristic function to dynamically recommend next cruising locations. Compared with existing route planning methods, the experimental results on a real-world dataset have shown that our proposed approach is more effective and robust. Moreover, our designed search scheme in H* can decrease the computing time and allow the search process to be more efficient. To the best of our knowledge, this is the first work that focuses on guiding a route, which can increase the income of taxi drivers under the constraint of booking information. © 2022 Association for Computing Machinery. All rights reserved.",deep learning; location recommendation; multi-criteria optimization; route decision; Taxi route planner,Deep learning; Drops; Forecasting; Heuristic algorithms; Multiobjective optimization; Pickups; Probability distributions; Roads and streets; Taxicabs; Decision framework; Deep learning; Location recommendation; Multi-criteria optimisation; Multi-criterion optimization; Route decision; Route planner; Taxi drivers; Taxi route planner; Traffic networks; Location
OWSP-Miner: Self-adaptive One-off Weak-gap Strong Pattern Mining,2022,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124643072&doi=10.1145%2f3476247&partnerID=40&md5=eedaceaba0ba98bd27a7f96bed3d0b16,"Gap constraint sequential pattern mining (SPM), as a kind of repetitive SPM, can avoid mining too many useless patterns. However, this method is difficult for users to set a suitable gap without prior knowledge and each character is considered to have the same effects. To tackle these issues, this article addresses a self-adaptive One-off Weak-gap Strong Pattern (OWSP) mining, which has three characteristics. First, it determines the gap constraint adaptively according to the sequence. Second, all characters are divided into two groups: strong and weak characters, and the pattern is composed of strong characters, while weak characters are allowed in the gaps. Third, each character can be used at most once in the process of support (the frequency of pattern) calculation. To handle this problem, this article presents OWSP-Miner, which equips with two key steps: support calculation and candidate pattern generation. A reverse-order filling strategy is employed to calculate the support of a candidate pattern, which reduces the time complexity. OWSP-Miner generates candidate patterns using pattern join strategy, which effectively reduces the candidate patterns. For clarification, time series is employed in the experiments and the results show that OWSP-Miner is not only more efficient but also is easier to mine valuable patterns. In the experiment of stock application, we also employ OWSP-Miner to mine OWSPs and the results show that OWSPs mining is more meaningful in real life. The algorithms and data can be downloaded at https://github.com/wuc567/Pattern-Mining/tree/master/OWSP-Miner. © 2022 Association for Computing Machinery. All rights reserved.",gap constraint; self-adaptive; Sequential pattern mining; time series; weak gap,Data mining; Miners; Candidate patterns; Gap constraint; Pattern Generation; Pattern mining; Prior-knowledge; Reverse order; Self-adaptive; Sequential-pattern mining; Times series; Weak gap; Time series
How Customer Demand Reactions Impact Technology Innovation and Security,2022,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133473547&doi=10.1145%2f3505227&partnerID=40&md5=adc2d298daea73481742b580ea317038,"Innovation is a very important concern for both managers and governmental policy makers. There is an important interplay between security and technology innovation that is largely unrecognized in the literature. This research considers the case where technology innovation in the form of additional product features increases demand through greater functionality. However, the likelihood of a security breach increases with the number of product features as the features interact in unintended ways, thereby increasing the attack surface. Using a two-stage game, we demonstrate how potential demand changes (from direct risk or externalities) impact firm technology innovation strategy. The analysis shows that the type and extent of customer demand reaction has a significant impact on innovative feature development. This research identifies two potential impacts on the level of innovation that can be strategically managed - the impact of externalities on demand and industry risk - explaining how these forces alter the level of innovation in the product ecosystem. Additionally, high-security development is disincentivized, and leads to a type of competitive behavior where the opportunity window for high-security development for all firms is small. © 2022 Association for Computing Machinery. All rights reserved.",direct-risk; externalities; game theory; Innovation; risk; security breach,Engineering research; Customer demands; Direct-risk; Externality; Governmental policies; High securities; Innovation; Level of innovation; Product feature; Security breaches; Technology innovation; Game theory
Anomaly Detection in Cybersecurity Datasets via Cooperative Co-evolution-based Feature Selection,2022,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124756114&doi=10.1145%2f3495165&partnerID=40&md5=a91a2b1c5d896c8fdb7858b57b324a50,"Anomaly detection from Big Cybersecurity Datasets is very important; however, this is a very challenging and computationally expensive task. Feature selection (FS) is an approach to remove irrelevant and redundant features and select a subset of features, which can improve the machine learning algorithms' performance. In fact, FS is an effective preprocessing step of anomaly detection techniques. This article's main objective is to improve and quantify the accuracy and scalability of both supervised and unsupervised anomaly detection techniques. In this effort, a novel anomaly detection approach using FS, called Anomaly Detection Using Feature Selection (ADUFS), has been introduced. Experimental analysis was performed on five different benchmark cybersecurity datasets with and without feature selection and the performance of both supervised and unsupervised anomaly detection techniques were investigated. The experimental results indicate that instead of using the original dataset, a dataset with a reduced number of features yields better performance in terms of true positive rate (TPR) and false positive rate (FPR) than the existing techniques for anomaly detection. For example, with FS, a supervised anomaly detection technique, multilayer perception increased the TPR by over 200% and decreased the FPR by about 97% for the KDD99 dataset. Similarly, with FS, an unsupervised anomaly detection technique, local outlier factor increased the TPR by more than 40% and decreased the FPR by 15% and 36% for Windows 7 and NSL-KDD datasets, respectively. In addition, all anomaly detection techniques require less computational time when using datasets with a suitable subset of features rather than entire datasets. Furthermore, the performance results have been compared with six other state-of-the-art techniques based on a decision tree (J48). © 2022 Association for Computing Machinery. All rights reserved.",Anomaly detection; Big Data; cooperative co-evolution; cybersecurity; feature selection; machine learning,Benchmarking; Decision trees; Feature Selection; Large dataset; Learning algorithms; Anomaly detection; Cooperative co-evolution; Cyber security; False positive rates; Features selection; Machine-learning; Performance; Redundant features; True positive rates; Unsupervised anomaly detection; Anomaly detection
An Evolutive Frequent Pattern Tree-based Incremental Knowledge Discovery Algorithm,2022,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132570424&doi=10.1145%2f3495213&partnerID=40&md5=62486046b7a5fd961d7325a683569002,"To understand current situation in specific scenarios, valuable knowledge should be mined from both historical data and emerging new data. However, most existing algorithms take the historical data and the emerging data as a whole and periodically repeat to analyze all of them, which results in heavy computation overhead. It is also challenging to accurately discover new knowledge in time, because the emerging data are usually small compared to the historical data. To address these challenges, we propose a novel knowledge discovery algorithm based on double evolving frequent pattern trees that can trace the dynamically evolving data by an incremental sliding window. One tree is used to record frequent patterns from the historical data, and the other one records incremental frequent items. The structures of the double frequent pattern trees and their relationships are updated periodically according to the emerging data and a sliding window. New frequent patterns are mined from the incremental data and new knowledge can be obtained from pattern changes. Evaluations show that this algorithm can discover new knowledge from evolving data with good performance and high accuracy. © 2022 Association for Computing Machinery. All rights reserved.",Association rule; data mining; evolutive frequent pattern tree; incremental window; knowledge discovery; public opinion analysis,Social aspects; Trees (mathematics); Discovery algorithm; Evolutive frequent pattern tree; Evolving datum; Frequent Pattern Tree; Historical data; Incremental window; Opinion analysis; Public opinion analyse; Public opinions; Sliding Window; Data mining
A Quantitative Comparative Study of Data-oriented Trust Management Schemes in Internet of Things,2022,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133476069&doi=10.1145%2f3476248&partnerID=40&md5=d8e1dae309b435f396f5148be284fa7c,"In the Internet of Things (IoT) paradigm, all entities in the IoT network, whether home users or industrial things, receive data from other things to make decisions. However, in the decentralized, heterogeneous, and rapidly changing IoT network with billions of devices, deciding about where to get the services or information from is critical, especially because malicious entities can exist in such an unmanaged network. Security provisioning alone cannot solve the issue of service quality or reliability. One way to elevate security and reliability in the IoT network is to bridge the gap of trust between objects, and also between humans and objects, while taking into account the IoT network characteristics. Therefore, a proper trust management system must be established on top of the IoT network service architecture. Trust is related to the manner expected from objects in providing services and recommendations. Recommendations are the basis of decision making in every trust management system. Since trust management ideas in the IoT are still immature, the purpose of this article is to survey, analyze, and compare the approaches that have been taken in building trust management systems for the IoT. We break down the features of such systems by analysis and also do quantitative comparisons by simulation. This article is organized into two main parts. First, studies and approaches in this field are compared from four perspectives: (1) trust computation method, (2) resistance to attacks (3) adherence to the limitations of IoT networks and devices, and (4) performance of the trust management scheme. The second part is quantitative and simulates four major methods in this field and measures their performance. We also make extensive analytical comparisons to demonstrate the similarities and discrepancies of current IoT trust management schemes and extract the essence of a resilient trust management framework. © 2022 Association for Computing Machinery. All rights reserved.",cyber security; data mining; decision making; Internet of things; recommender systems; trust management,Cybersecurity; Data mining; Information management; Internet of things; Network security; Recommender systems; Trusted computing; Comparatives studies; Cyber security; Decentralised; Decisions makings; Home users; Malicious entity; Management scheme; Performance; Trust management; Trust management systems; Decision making
"Introduction to the Special Issue on Pattern-Driven Mining, Analytics, and Prediction for Decision Making, Part II",2022,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133433733&doi=10.1145%2f3512468&partnerID=40&md5=03003a5677812f4cf1986d8899b4e8f2,"NAs the demand for taxi reservation services has increased, increasing the income of taxi driverswith advanced services has attracted attention. In this article, we propose a path decision framework that considers real-time spatial-temporal predictions and traffic network information. The goal is to optimize a taxi driver’s profit when considering a reservation. Our framework contains four components. First, we build a grid-based road network graph formodeling traffic network information for speeding up the search process. Next,we conduct two prediction modules that adopt advanced deep learning techniques to guide proper search directions for recommending cruising locations. One module of the taxi demand prediction is used to estimate the pick-up probabilities of passengers in the city. Another one is destination prediction, which can predict the distribution of drop-off probabilities and capture the flow of potential passengers. Finally, we propose the H* (Heuristic-star) algorithm, which jointly considers pick-up probabilities, drop-off distribution, road network, distance, and time factors based on the attentive heuristic function to dynamically recommend next cruising locations. Compared with existing route planning methods, the experimental results on a real-world dataset have shown that our proposed approach is more effective and robust. Moreover, our designed search scheme in H* can decrease the computing time and allow the search process to be more efficient. To the best of our knowledge, this is the first work that focuses on guiding a route, which can increase the income of taxi drivers under the constraint of booking information. © 2022 Association for Computing Machinery. All rights reserved.",,Decision making; Deep learning; Drops; Heuristic algorithms; Location; Pickups; Probability distributions; Roads and streets; Taxicabs; Decision framework; Decisions makings; Network information; Real- time; Road network; Search process; Spatial temporals; Taxi drivers; Temporal prediction; Traffic networks; Forecasting
Configurable Batch-Processing Discovery from Event Logs,2022,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130866799&doi=10.1145%2f3490394&partnerID=40&md5=722b6379ad160f124d368b67f7beced8,"Batch processing is used in many production and service processes and can help achieve efficiencies of scale; however, it can also increase inventories and introduce process delays. Before organizations can develop good understanding about the effects of batch processing on process performance, they should be able to identify potential batch-processing behavior in business processes. However, in many cases such behavior may not be known; for example, batch processing may be occasionally performed during certain time frames, by specific employees, and/or for particular customers. This article presents a novel approach for the identification of batching behavior from process execution data recorded in event logs. The approach can discover different types of batch-processing behaviors and allows users to configure batch-processing characteristics they are interested in. The approach is implemented and evaluated through experiments with synthetic event logs and case studies with real-life event logs. The evaluation demonstrates that the approach can identify various batch-processing behaviors in the context of business processes. © 2022 Association for Computing Machinery. All rights reserved.",Batch processing; batch processing discovery; event log; process mining,Batch processing; Batch processing discovery; Business Process; Event logs; Process delay; Process mining; Process performance; Production process; Service process; Time frame; Batch data processing
"Integrating Behavioral, Economic, and Technical Insights to Understand and Address Algorithmic Bias: A Human-Centric Perspective",2022,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133428210&doi=10.1145%2f3519420&partnerID=40&md5=11d2ec0dc03ce40d366d5bca3560dfc6,"Many important decisions are increasingly being made with the help of information systems that use artificial intelligence and machine learning models. These computational models are designed to discover useful patterns from large amounts of data, which augment human capabilities to make decisions in various application domains. However, there are growing concerns regarding the ethics challenges faced by these automated decision-making (ADM) models, most notably on the issue of algorithmic bias, in which the models systematically produce less favorable (i.e., unfair) decisions for certain groups of people. In this commentary, we argue that algorithmic bias is not just a technical (e.g., computational or statistical) problem, and its successful resolution requires deep insights into individual and organizational behavior, economic incentives, as well as complex dynamics of the sociotechnical systems in which the ADM models are embedded. We discuss a human-centric, fairness-aware ADM framework that highlights the holistic involvement of human decision makers in each step of ADM. We review the emerging literature on fairness-aware machine learning and then discuss various strategic decisions that humans need to make, such as formulating proper fairness objectives, recognizing fairness-induced trade-offs and implications, utilizing machine learning model outputs, and managing/governing the decisions of ADM models. We further illustrate how these strategic decisions are jointly informed by behavioral, economic, and design sciences. Our discussions reveal a number of future research opportunities uniquely suitable for Management Information Systems (MIS) researchers to pursue. © 2022 Association for Computing Machinery. All rights reserved.",algorithmic bias; augmented decision-making; Automated decision-making; ethics; fairness,Behavioral research; Economic and social effects; Embedded systems; Ethical technology; Information management; Information systems; Information use; Machine learning; Algorithmic bias; Algorithmics; Augmented decision-making; Automated decision making; Behavioral economics; Decision-making modeling; Decisions makings; Fairness; Human-centric; Machine learning models; Decision making
Engineering Trustable and Auditable Choreography-based Systems Using Blockchain,2022,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132179218&doi=10.1145%2f3505225&partnerID=40&md5=6d995adce7decc8b0d45972fdea40f9a,"A key challenge in engineering distributed systems consists in the integration into their development of a decentralised infrastructure allowing the system participants to trust each other. In this article, we face such a challenge by proposing a model-driven methodology and a related framework to support the engineering of trustable and auditable systems. The approach is based on choreography diagrams specified in the Business Process Model and Notation standard, describing the interactions that should occur among the distributed components of systems. We support the whole lifecycle of choreographies, from their modelling to their distributed execution and auditing. The framework, based on blockchain technology, is named ChorChain. More specifically, ChorChain takes as input a BPMN choreography model and automatically translates it into a Solidity smart contract. The smart contract permits us to enforce the interactions among the cooperating components as prescribed by the choreography model. By leveraging on the auditability of blockchain, ChorChain also supports the activity of auditors continuously. In such a way, ChorChain enables auditors to get some degree of assurance on what happens simultaneously with, or shortly after, information disclosure. We assess the feasibility and effectiveness of the proposed methodology and framework through experiments conducted on the Rinkeby Ethereum Testnet. © 2022 Association for Computing Machinery. All rights reserved.",auditing; blockchain; BPMN; choreography; execution; trust,Life cycle; Smart contract; Auditing; Block-chain; BPMN; Choreography; Choreography models; Decentralised; Distributed systems; Execution; Model-driven methodology; Trust; Blockchain
Feature Extraction of High-dimensional Data Based on J-HOSVD for Cyber-Physical-Social Systems,2022,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133411360&doi=10.1145%2f3483448&partnerID=40&md5=ebc957adc52c10761e1c419a2d70a552,"With the further integration of Cyber-Physical-Social systems (CPSSs), there is explosive growth of the data in CPSSs. How to discover effective information or knowledge from CPSSs big data and provide support for subsequent learning tasks has become a core issue. Moreover, modern applications in CPSSs increasingly rely on the processing and analysis of high-dimensional data; the correlation and internal structure of these highdimensional data are gradually becoming more complex, which further makes traditional machine learning algorithms a little inadequate in processing these data. In this article, we propose two general dimension reduction and feature extraction methods for high-dimensional data based on joint tensor decomposition, namely core feature extraction methods and factor feature extraction methods, which can effectively mine out the common components and hidden patterns of high-dimensional data by joint analysis while maintaining the original data structure. We also verified the effectiveness of the methods from both theoretical and practical aspects. Furthermore, we extend the two feature extraction methods to the tensor distance scenario and illustrate that the compressed features extracted by our models can keep the global information of original data well. Finally, we evaluated proposed methods on two benchmark datasets through classification tasks, and experimental results show that the low-dimensional features extracted by the proposed models have higher classification accuracy than the direct classification of the original data, which further verifies the effectiveness and robustness of our methods. © 2022 Association for Computing Machinery. All rights reserved.",big data; Cyber-Physical-Social systems; Data mining; dimensionality reduction; feature extraction; joint tensor decomposition,Big data; Classification (of information); Clustering algorithms; Cyber Physical System; Data reduction; Extraction; Feature extraction; Learning algorithms; Tensors; Cybe-physical-social system; Cyber physicals; Dimensionality reduction; Explosive growth; Feature extraction methods; Features extraction; High dimensional data; Joint tensor decomposition; Social systems; Tensor decomposition; Data mining
Social Media Event Prediction using DNN with Feedback Mechanism,2022,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133461134&doi=10.1145%2f3522759&partnerID=40&md5=b165f26d715d4d43f2d2a5dfb9106917,[No abstract available],event prediction; neural networks; Online social networks,
Two-Stage Competitive Particle Swarm Optimization Based Timing-Driven X-Routing for IC Design Under Smart Manufacturing,2022,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149443299&doi=10.1145%2f3531328&partnerID=40&md5=5da15a4495c59896743af4c24ec68e20,"As timing delay becomes a critical issue in chip performance, there is a burning desire for IC design under smart manufacturing to optimize the delay. As the best connection model for multi-terminal nets, the wirelength and the maximum source-to-sink pathlength of the Steiner minimum tree are the decisive factors of timing delay for routing. In addition, considering that X-routing can get the utmost out of routing resources, this article proposes a Timing-Driven X-routing Steiner Minimum Tree (TD-XSMT) algorithm based on two-stage competitive particle swarm optimization. This work utilizes the multi-objective particle swarm optimization algorithm and redesigns its framework, thus improving its performance. First, a two-stage learning strategy is presented, which balances the exploration and exploitation capabilities of the particle by learning edge structures and pseudo-Steiner point choices. Especially in the second stage, a hybrid crossover strategy is designed to guarantee convergence quality. Second, the competition mechanism is adopted to select particle learning objects and enhance diversity. Finally, according to the characteristics of the discrete TD-XSMT problem, the mutation and crossover operators of the genetic algorithm are used to effectively discretize the proposed algorithm. Experimental results reveal that TSCPSO-TD-XSMT can obtain a smooth trade-off between wirelength and maximum source-to-sink pathlength, and achieve distinguished timing delay optimization.  © 2022 Association for Computing Machinery.",competitive mechanism; IC design; multi-objective particle swarm optimization; Smart manufacturing; Timing-Driven; X-routing,Economic and social effects; Genetic algorithms; Integrated circuits; Multiobjective optimization; Particle swarm optimization (PSO); Timing circuits; Trees (mathematics); Competitive mechanisms; Multi objective particle swarm optimization; Particle swarm; Routings; Smart manufacturing; Steiner minimum trees; Swarm optimization; Timing delay; Timing-driven; X-routing; Integrated circuit design
Scientific Workflows in IoT Environments: A Data Placement Strategy Based on Heterogeneous Edge-Cloud Computing,2022,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136559261&doi=10.1145%2f3531327&partnerID=40&md5=9aa6809f92c4db35f97bc46fcf3e4546,"In Industry 4.0 and Internet of Things (IoT) environments, the heterogeneous edge-cloud computing paradigm can provide a more proper solution to deploy scientific workflows compared to cloud computing or other traditional distributed computing. Owing to the different sizes of scientific datasets and the privacy issue concerning some of these datasets, it is essential to find a data placement strategy that can minimize data transmission time. Some state-of-the-art data placement strategies combine edge computing and cloud computing to distribute scientific datasets. However, the dynamic distribution of newly generated datasets to appropriate datacenters and exiting the spent datasets are still a challenge during workflows execution. To address this challenge, this study not only constructs a data placement model that includes shared datasets within the individual and among multiple workflows across various geographical regions, but also proposes a data placement strategy (DYM-RL-DPS) based on algorithms of two stages. First, during the build-time stage of workflows, we use the discrete particle swarm optimization algorithm with differential evolution to pre-allocate initial datasets to proper datacenters. Then, we reformulate the dynamic datasets distribution problem as a Markov decision process and provide a reinforcement learning-based approach to learn the data placement strategy in the runtime stage of scientific workflows. Through using the heterogeneous edge-cloud computing architecture to simulate IoT environments, we designed comprehensive experiments to demonstrate the superiority of DYM-RL-DPS. The results of our strategy can effectively reduce the data transmission time as compared to other strategies.  © 2022 Association for Computing Machinery.",data-sharing; Heterogeneous edge-cloud computing; IoT environments; scientific workflows,Computer architecture; Data transfer; Edge computing; Geographical regions; Markov processes; Reinforcement learning; Cloud-computing; Data placement; Data Sharing; Data transmission time; Datacenter; Edge clouds; Heterogeneous edge-cloud computing; Internet of thing environment; Placement strategy; Scientific workflows; Internet of things
Computational Intelligence in Security of Digital Twins Big Graphic Data in Cyber-physical Systems of Smart Cities,2022,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136882613&doi=10.1145%2f3522760&partnerID=40&md5=1b88b1a92cddb697354ab3b20dbff44e,"This investigation focuses on the application of computational intelligence to the security of Digital Twins (DTs) graphic data of the Cyber-physical System (CPS). The intricate and diverse physical space of CPS in the smart city is mapped in virtual space to construct the DTs CPS in the smart city. Besides, Differential Privacy Frequent Subgraph-Big Multigraph (DPFS-BM) is employed to ensure data privacy security. Moreover, the analysis and prediction model for the DTs big graphic data (BGD) in the CPS is built based on Differential Privacy-AlexNet (DP-AlexNet). Alexnet successfully solves the gradient dispersion problem of the Sigmoid function of deep network structures. Finally, the comparative analysis approach is utilized to verify the performance of the model reported here by comparing it with Long Short-Term Memory, Convolutional Neural Network, Recurrent Neural Network, original AlexNet, and Multi-Layer Perceptron in a simulation experiment. Through the comparison in the root mean square error, the mean absolute error, the mean absolute percentage error, training time, and test time, the model proposed here outperforms other models regarding errors, time delay, and time consumption. In the same environment, the system performs better with multi-hop paths, extra relays, and a high fading index; in that case, the outage probability is minimal. Therefore, the DP-AlexNet model is suitable for processing BGD. Moreover, its speed acceleration is more apparent than that of other models, with a higher SpeedUp indicator. The research effectively combines data mining and data security, which is of significant value for optimizing the privacy protection technology of frequent subgraph mining on a single multi-graph. Besides, the constructed DTs of CPS can provide excellent accuracy and a prominent acceleration effect on the premise of low errors. In addition, the model reported here can provide reference for the intelligent and digital development of smart cities.  © 2022 Association for Computing Machinery.",big graph data; Cyber-physical system; Deep learning; Digital twins; security,Cyber Physical System; Data mining; Data privacy; E-learning; Embedded systems; Errors; Mean square error; Multilayer neural networks; Network security; Recurrent neural networks; Smart city; Big graph data; Cybe-physical systems; Cyber-physical systems; Deep learning; Differential privacies; Frequent subgraphs; Graph data; Multigraphs; Security; Virtual spaces; Cybersecurity
User-empowered Privacy-preserving Authentication Protocol for Electric Vehicle Charging Based on Decentralized Identity and Verifiable Credential,2022,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133599844&doi=10.1145%2f3532869&partnerID=40&md5=7d76d994a5435cf636ab677d2de054fb,"The use of Electric Vehicles (EVs) has been gaining traction in recent years due to various reasons. While charging their EVs, users expose their identity and personal details, and an adversary being able to identify and track where users charge their EVs is a potential privacy threat. In this article, we propose a user-empowered privacy-preserving authentication protocol for EV charging based on Decentralized Identifier (DID) and Verifiable Credential (VC) to provide Zero-Knowledge Proof (ZKP)-security. The proposed method gives users full control over their identities and allows them to remain anonymous while charging from any station. Giving control over their identities empowers users. At the same time, by making use of the concept of VC, other parties can verify that a user is legitimate and authenticate the user before providing charging services. Hence, the proposed protocol makes the charging service available in a secure way, while empowering users and preserving their privacy.  Copyright © 2022 held by the owner/author(s).",anonymity; blockchain; DID; Electric vehicle charging; privacy; user-empowered authentication; VC,Blockchain; Charging (batteries); Electric traction; Electric vehicles; Privacy-preserving techniques; Anonymity; Authentication protocols; Block-chain; Decentralised; Decentralized identifier; Electric vehicle charging; Privacy; Privacy-preserving authentication; User-empowered authentication; Verifiable credential; Authentication
Performance-Driven X-Architecture Routing Algorithm for Artificial Intelligence Chip Design in Smart Manufacturing,2022,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149434169&doi=10.1145%2f3519422&partnerID=40&md5=fd700c229e7ed4c5c9d1983123ff2ab0,"The new 7-nm Artificial Intelligence (AI) chip is an important milestone recently announced by the IBM research team, with a very important optimization goal of performance. This chip technology can be extended to various business scenarios in the Internet of Things. As the basic model for very large scale integration routing, the Steiner minimal tree can be used in various practical problems, such as wirelength optimization and timing closure. Further considering the X-architecture and the routing resources within obstacles, an effective performance-driven X-architecture routing algorithm for AI chip design in smart manufacturing is proposed to improve the delay performance of the chip. First, a special particle swarm optimization algorithm is presented to solve the discrete length-restricted X-architecture Steiner minimum tree problem in combination with genetic operations, and a particle encoding scheme is presented to encode each particle into an initial routing tree. Second, two lookup tables based on pins and obstacles are established to provide a fast information query for the whole algorithm flow. Third, a strategy of candidate point selection is designed to make the particles satisfy the constraints. Finally, a refinement strategy is implemented to further improve the quality of the final routing tree. Compared with other state-of-the-art algorithms, the proposed algorithm achieves a better total wirelength, which is an important index of performance, thus better satisfying the demand for delay performance of AI chip design in smart manufacturing.  © 2022 Association for Computing Machinery.",Artificial intelligence chip design; particle swarm optimization; refinement strategy; resource relaxation; smart manufacturing; Steiner minimal tree; X-architecture routing,Artificial intelligence; Encoding (symbols); Flow control; Particle swarm optimization (PSO); Table lookup; Trees (mathematics); Artificial intelligence chip design; Chip design; Particle swarm; Particle swarm optimization; Refinement strategy; Resource relaxation; Routings; Smart manufacturing; Steiner minimal tree; Swarm optimization; X architecture; X-architecture routing; Routing algorithms
Integration of DevOps Practices on a Noise Monitor System with CircleCI and Terraform,2022,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149443532&doi=10.1145%2f3505228&partnerID=40&md5=7ca766930e043f0fa3eabcaff89ce84c,"Lowering pollution levels is one of the main principles of Sustainable Development goals dictated by the United Nations. Consequently, developments on noise monitoring contribute in great manner to this purpose, since they give the opportunity to governments and institutions to maintain track on the matter. While developing a software product for this purpose, with the growth in terms of functional and non-functional requirements, elements such as infrastructure, source code, and others also scale up. Consequently if there are not good practices to face the new challenges of the software product, then it could become more complex to refactor, maintain, and scale, causing a decrease on delivery rate and the quality of the product. DevOps is an emerging concept but still hazy, which involves a set of practices that helps organizations to speed up delivery time, improve software quality and collaboration between teams. The aim of this article is to document the implementation of some DevOps practices such as IaC, continuous integration and deployment, code quality control, and collaboration on a noise monitor system to increase the product quality and automation of deployment. The final result is a set of automated pipelines that represents the entire integration and deployment cycle of the software integrated with platforms to improve quality and maintainability of the software components.  © 2022 Association for Computing Machinery.",CI/CD; classification; DeVOps; serverless; sound,Acoustic noise; Codes (symbols); Computer software selection and evaluation; Noise pollution; Quality control; CI/CD; Devops; Monitor system; Noise monitoring; Noise monitors; Pollution level; Serverless; Software products; Terraform; United Nations; Integration
Heterogeneous Energy-aware Load Balancing for Industry 4.0 and IoT Environments,2022,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145346956&doi=10.1145%2f3543859&partnerID=40&md5=2285510bd56c109b2e5fd1e4a100888d,"With the improvement of global infrastructure, Cyber-Physical Systems (CPS) have become an important component of Industry 4.0. Both the application as well as the machine work together to improve the task of interdependencies. Machine learning methods in CPS require the monitoring of computational algorithms, including adopting optimizations, fine-tuning cyber systems, improving resource utilization, as well as reducing vulnerability and also computation time. By leveraging the tremendous parallelism provided by General-Purpose Graphics Processing Units (GPGPU) as well as OpenCL, it is possible to dramatically reduce the execution time of data-parallel programs. However, when running an application with tiny amounts of data on a GPU, GPU resources are wasted because the program may not be able to fully utilize the GPU cores. This is because there is no mechanism for kernels to share a GPU due to the lack of OS support for GPUs. Optimal device selection is required to reduce the high power of the GPU. In this paper, we propose an energy reduction method for heterogeneous clustering. This study focuses on load balancing; resource-aware processor selection based on machine learning is performed using code features. The proposed method identifies energy-efficient kernel candidates (from the employment pool). Then, it selects a pair of kernel candidates from all possibilities that lead to a reduction in both energy consumption as well as execution time. Experimental results show that the proposed kernel approach reduces execution time by 2.23 times compared to a baseline scheduling system. Experiments have also shown that the execution time is 1.2 times faster than state-of-the-art approaches.  © 2022 Association for Computing Machinery.",classification; feature selection; Machine learning; OpenCL; optimization; scheduling,Application programs; Classification (of information); Computer graphics; Embedded systems; Energy efficiency; Energy utilization; Feature extraction; Industry 4.0; Internet of things; Program processors; Cybe-physical systems; Cyber-physical systems; Energy aware; Features selection; Global infrastructure; Load-Balancing; Machine learning methods; Machine-learning; Opencl; Optimisations; Graphics processing unit
Allocation of Resources for Cloud Survivability in Smart Manufacturing,2022,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149439719&doi=10.1145%2f3533701&partnerID=40&md5=5bdc718139770bfa61157899e82c9c68,"With the development of virtualization technology, cloud computing has emerged as a powerful and flexible platform for various services such as online trading. However, there are concerns about the survivability of cloud services in smart manufacturing. Most existing solutions provide a standby Virtual Machine (VM) for each running VM. However, this often leads to huge resource waste because VMs do not always run at full capacity. To reduce resource waste, we propose a smart survivability framework to efficiently allocate resources to standby VMs. Our framework contains two novel aspects: (1) a prediction mechanism to predict the resource utilization of each VM in order to reduce the number of standby VMs; and (2) a nested virtualization technology to refine the granularity of standby VMs. We will use an open-source cloud simulation platform named cloudsim, with real-world data, to verify the feasibility of the proposed framework and evaluate its performance. The proposed Smart Survivable Usable Virtual Machine (SSUVM) will predict resource utilization of VMs on Rack1 periodically. When errors happen in VMs, the framework will allocate standby resources according to the predicted result. The SSUVM will receive the latest running status of the failed VM and its mirror image to recover the VM's work.  © 2022 Association for Computing Machinery.",prediction; resource utilization; smart allocation; Smart manufacture; standby resource; survivability,Cloud computing; Flow control; Network security; Simulation platform; Virtual machine; Virtual reality; Virtualization; Cloud-computing; Flexible platforms; Resource wastes; Resources utilizations; Smart allocation; Smart manufacture; Smart manufacturing; Standby resource; Survivability; Virtualization technologies; Forecasting
An Intelligent Mechanism to Automatically Discover Emerging Technology Trends: Exploring Regulatory Technology,2022,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127586842&doi=10.1145%2f3485187&partnerID=40&md5=29a2565b8524869aa824254cbc876271,"Technology trend analysis uses data relevant to historical performance and extrapolates it to estimate and assess the future potential of technology. Such analysis is used to analyze emerging technologies or predict the growing markets that influence the resulting social or economic development to assist in effective decision-making. Traditional trend analysis methods are time-consuming and require considerable labor. Moreover, the implemented processes may largely rely on the specific knowledge of the domain experts. With the advancement in the areas of science and technology, emerging cross-domain trends have received growing attention for its considerable influence on society and the economy. Consequently, emerging cross-domain predictions that combine or complement various technologies or integrate with diverse disciplines may be more critical than other tools and applications in the same domain. This study uses a design science research methodology, a text mining technique, and social network analysis (SNA) to analyze the development trends concerning the presentation of the product or service information on a company's website. This study applies regulatory technology (RegTech) as a case to analyze and justify the emerging cross-disciplinary trend. Furthermore, an experimental study is conducted using the Google search engine to verify and validate the proposed research mechanism at the end of this study. The study results reveal that, compared with Google Trends and Google Correlate, the research mechanism proposed in this study is more illustrative, feasible, and promising because it reduces noise and avoids the additional time and effort required to perform a further in-depth exploration to obtain the information. © 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM.",computer auditing; fintech; regtech; social network analysis; technology trend analysis; Text mining,Data mining; Economic and social effects; Product design; Search engines; Social networking (online); Social sciences computing; Computer auditing; Cross-domain; Emerging technologies; Growing markets; Historical performance; Intelligent mechanisms; Regtech; Social Network Analysis; Technology trends; Technology trends analysis; Decision making
Identifying the Big Shots-A Quantile-Matching Way in the Big Data Context,2022,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127594040&doi=10.1145%2f3490395&partnerID=40&md5=717c7fa97c478ded3e344de715767978,"The prevalence of big data has raised significant epistemological concerns in information systems research. This study addresses two of them-the deflated p-value problem and the role of explanation and prediction. To address the deflated p-value problem, we propose a multivariate effect size method that uses the log-likelihood ratio test. This method measures the joint effect of all variables used to operationalize one factor, thus overcoming the drawback of the traditional effect size method (θ), which can only be applied at the single variable level. However, because factors can be operationalized as different numbers of variables, direct comparison of multivariate effect size is not possible. A quantile-matching method is proposed to address this issue. This method provides consistent comparison results with the classic quantile method. But it is more flexible and can be applied to scenarios where the quantile method fails. Furthermore, an absolute multivariate effect size statistic is developed to facilitate concluding without comparison. We have tested our method using three different datasets and have found that it can effectively differentiate factors with various effect sizes. We have also compared it with prediction analysis and found consistent results: explanatorily influential factors are usually also predictively influential in a large sample scenario. © 2022 Association for Computing Machinery.",Big data; deflated p-value; multivariate effect size; prediction analysis; quantile matching,Big data; Information systems; Information use; Statistical tests; Data contexts; Deflated p-value; Effect size; Information system research; Matchings; Multivariate effect size; P-values; Prediction analyse; Quantile matching; Value problems; Forecasting
A Multi-attention Collaborative Deep Learning Approach for Blood Pressure Prediction,2022,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127592316&doi=10.1145%2f3471571&partnerID=40&md5=44ffaaea77969e29d3b624344708984f,"We develop a deep learning model based on Long Short-term Memory (LSTM) to predict blood pressure based on a unique data set collected from physical examination centers capturing comprehensive multi-year physical examination and lab results. In the Multi-attention Collaborative Deep Learning model (MAC-LSTM) we developed for this type of data, we incorporate three types of attention to generate more explainable and accurate results. In addition, we leverage information from similar users to enhance the predictive power of the model due to the challenges with short examination history. Our model significantly reduces predictive errors compared to several state-of-the-art baseline models. Experimental results not only demonstrate our model's superiority but also provide us with new insights about factors influencing blood pressure. Our data is collected in a natural setting instead of a setting designed specifically to study blood pressure, and the physical examination items used to predict blood pressure are common items included in regular physical examinations for all the users. Therefore, our blood pressure prediction results can be easily used in an alert system for patients and doctors to plan prevention or intervention. The same approach can be used to predict other health-related indexes such as BMI. © 2021 Association for Computing Machinery.",Blood pressure prediction; deep learning; long short-term memory; physical examination data; recurrent neural networks,Blood; Blood pressure; Brain; Forecasting; Patient treatment; Blood pressure prediction; Data set; Deep learning; Learning approach; Learning models; Model-based OPC; Physical examination data; Predictive power; Pressure predictions; Pressure-based; Long short-term memory
Counteracting Dark Web Text-Based CAPTCHA with Generative Adversarial Learning for Proactive Cyber Threat Intelligence,2022,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127563214&doi=10.1145%2f3505226&partnerID=40&md5=0568597fb81aa68784dbda4578154f0e,"Automated monitoring of dark web (DW) platforms on a large scale is the first step toward developing proactive Cyber Threat Intelligence (CTI). While there are efficient methods for collecting data from the surface web, large-scale dark web data collection is often hindered by anti-crawling measures. In particular, text-based CAPTCHA serves as the most prevalent and prohibiting type of these measures in the dark web. Text-based CAPTCHA identifies and blocks automated crawlers by forcing the user to enter a combination of hard-to-recognize alphanumeric characters. In the dark web, CAPTCHA images are meticulously designed with additional background noise and variable character length to prevent automated CAPTCHA breaking. Existing automated CAPTCHA breaking methods have difficulties in overcoming these dark web challenges. As such, solving dark web text-based CAPTCHA has been relying heavily on human involvement, which is labor-intensive and time-consuming. In this study, we propose a novel framework for automated breaking of dark web CAPTCHA to facilitate dark web data collection. This framework encompasses a novel generative method to recognize dark web text-based CAPTCHA with noisy background and variable character length. To eliminate the need for human involvement, the proposed framework utilizes Generative Adversarial Network (GAN) to counteract dark web background noise and leverages an enhanced character segmentation algorithm to handle CAPTCHA images with variable character length. Our proposed framework, DW-GAN, was systematically evaluated on multiple dark web CAPTCHA testbeds. DW-GAN significantly outperformed the state-of-the-art benchmark methods on all datasets, achieving over 94.4% success rate on a carefully collected real-world dark web dataset. We further conducted a case study on an emergent Dark Net Marketplace (DNM) to demonstrate that DW-GAN eliminated human involvement by automatically solving CAPTCHA challenges with no more than three attempts. Our research enables the CTI community to develop advanced, large-scale dark web monitoring. We make DW-GAN code available to the community as an open-source tool in GitHub. © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Automated CAPTCHA breaking; dark web; generative adversarial networks,Character recognition; Data acquisition; Electronic mail filters; Generative adversarial networks; Image enhancement; Image segmentation; Automated CAPTCHA breaking; Background noise; Breakings; CAPTCHAs; Cyber threats; Dark web; Data collection; Large-scales; Web data; Web texts; Automation
The Evolution of Search: Three Computing Paradigms,2022,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127576159&doi=10.1145%2f3495214&partnerID=40&md5=9e59014ea58b2b7458f3c17ca564e541,"Search is probably the most common activity that humans conduct all the time. A search target can be a concrete item (with a yes or no answer and location information), an abstract concept (such as the most important information on the Web about Xindong Wu), or a plan/path for a specific target with an objective function (like flight scheduling with a minimal travel time), among others. In this article, we propose a Universal Connection Theorem (UCT) to suggest that all physical objects/items in the universe are connected through explicit or implicit relationships. Search is to explore the relationships, using different computing methods, to retrieve relevant objects. Under the UCT theorem, we summarize mainstream search approaches into two categories from the user perspective, deterministic search vs. abstract search, and further distinguish them into three computing paradigms: planning based search, data driven search, and knowledge enhanced search. The planning based paradigm explores search as a planning process in a large search space, by graph traversing with heuristic principles to locate optimal solutions. The data driven paradigm seeks to find objects matching the user's query from a large data repository. Indexing, hashing, information retrieval, and recommendations are typical strategies to tackle the data volumes and select the best answers for users' queries. The knowledge enhanced search does not aim to find matching objects, but to discover and then meet user's search requirements through knowledge mining. The evolution of these three search paradigms, from planning to data engineering and knowledge engineering, provides increasing levels of challenges and opportunities. This article elaborates the respective principles of these paradigms. © 2022 Copyright held by the owner/author(s).",data engineering; knowledge engineering; planning; Search,Abstracting; Computation theory; Data mining; Knowledge engineering; Search engines; Abstract concept; Computing paradigm; Data driven; Data engineering; Flight scheduling; Location information; Objective functions; Plan paths; Search; User query; Travel time
Readmission Prediction for Patients with Heterogeneous Medical History: A Trajectory-Based Deep Learning Approach,2022,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125422458&doi=10.1145%2f3468780&partnerID=40&md5=9bd2472ad7788a7da0594d4e7df80e80,"Hospital readmission refers to the situation where a patient is re-hospitalized with the same primary diagnosis within a specific time interval after discharge. Hospital readmission causes $26 billion preventable expenses to the U.S. health systems annually and often indicates suboptimal patient care. To alleviate those severe financial and health consequences, it is crucial to proactively predict patients' readmission risk. Such prediction is challenging because the evolution of patients' medical history is dynamic and complex. The state-of-the-art studies apply statistical models which use static predictors in a period, failing to consider patients' heterogeneous medical history. Our approach-Trajectory-BAsed DEep Learning (TADEL)-is motivated to tackle the deficiencies of the existing approaches by capturing dynamic medical history. We evaluate TADEL on a five-year national Medicare claims dataset including 3.6 million patients per year over all hospitals in the United States, reaching an F1 score of 87.3% and an AUC of 88.4%. Our approach significantly outperforms all the state-of-the-art methods. Our findings suggest that health status factors and insurance coverage are important predictors for readmission. This study contributes to IS literature and analytical methodology by formulating the trajectory-based readmission prediction problem and developing a novel deep-learning-based readmission risk prediction framework. From a health IT perspective, this research delivers implementable methods to assess patients' readmission risk and take early interventions to avoid potential negative consequences. © 2021 Association for Computing Machinery.",computational design science; deep learning; Hospital readmission; predictive analytics,Deep learning; Diagnosis; Forecasting; Health insurance; Health risks; Hospitals; Patient treatment; Risk assessment; Trajectories; Computational design; Computational design science; Deep learning; Design science; Hospital readmission; Learning approach; Medical history; Specific time; Time interval; Trajectory-based; Predictive analytics
Automating Research Data Management Using Machine-Actionable Data Management Plans,2022,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127565497&doi=10.1145%2f3490396&partnerID=40&md5=ca869181b59acbb4aeb5787ce5c59569,"Many research funders mandate researchers to create and maintain data management plans (DMPs) for research projects that describe how research data is managed to ensure its reusability. A DMP, being a static textual document, is difficult to act upon and can quickly become obsolete and impractical to maintain. A new generation of machine-actionable DMPs (maDMPs) was therefore proposed by the Research Data Alliance to enable automated integration of information and updates. maDMPs open up a variety of use cases enabling interoperability of research systems and automation of data management tasks.In this article, we describe a system for machine-actionable data management planning in an institutional context. We identify common use cases within research that can be automated to benefit from machine-actionability of DMPs. We propose a reference architecture of an maDMP support system that can be embedded into an institutional research data management infrastructure. The system semi-automates creation and maintenance of DMPs, and thus eases the burden for the stakeholders responsible for various DMP elements. We evaluate the proposed system in a case study conducted at the largest technical university in Austria and quantify to what extent the DMP templates provided by the European Commission and a national funding body can be pre-filled. The proof-of-concept implementation shows that maDMP workflows can be semi-automated, thus workload on involved parties can be reduced and quality of information increased. The results are especially relevant to decision makers and infrastructure operators who want to design information systems in a systematic way that can utilize the full potential of maDMPs. © 2021 Copyright held by the owner/author(s).",automation; business processes; Data management plan; enterprise architecture; FAIR; funder template; machine-actionable; RDA; RDM; requirements engineering,Decision making; Information management; Interoperability; Reusability; Business Process; Data management plan; Enterprise Architecture; Funder template; Machine-actionable; Management plans; RDA; RDM; Requirement engineering; Research data managements; Automation
Machine Learning and Survey-based Predictors of InfoSec Non-Compliance,2022,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127612080&doi=10.1145%2f3466689&partnerID=40&md5=1e5b85fe401756f3827b0e8b590509b6,"Survey items developed in behavioral Information Security (InfoSec) research should be practically useful in identifying individuals who are likely to create risk by failing to comply with InfoSec guidance. The literature shows that attitudes, beliefs, and perceptions drive compliance behavior and has influenced the creation of a multitude of training programs focused on improving ones' InfoSec behaviors. While automated controls and directly observable technical indicators are generally preferred by InfoSec practitioners, difficult-to-monitor user actions can still compromise the effectiveness of automatic controls. For example, despite prohibition, doubtful or skeptical employees often increase organizational risk by using the same password to authenticate corporate and external services. Analysis of network traffic or device configurations is unlikely to provide evidence of these vulnerabilities but responses to well-designed surveys might. Guided by the relatively new IPAM model, this study administered 96 survey items from the Behavioral InfoSec literature, across three separate points in time, to 217 respondents. Using systematic feature selection techniques, manageable subsets of 29, 20, and 15 items were identified and tested as predictors of non-compliance with security policy. The feature selection process validates IPAM's innovation in using nuanced self-efficacy and planning items across multiple time frames. Prediction models were trained using several ML algorithms. Practically useful levels of prediction accuracy were achieved with, for example, ensemble tree models identifying 69% of the riskiest individuals within the top 25% of the sample. The findings indicate the usefulness of psychometric items from the behavioral InfoSec in guiding training programs and other cybersecurity control activities and demonstrate that they are promising as additional inputs to AI models that monitor networks for security events. © 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM.",compliance behavior; Information security; machine learning,Automation; Cybersecurity; Feature extraction; Machine learning; Network security; Personnel training; Automated control; Behavioral information security; Compliance behavior; Features selection; Information security behaviours; Machine-learning; Non-compliance; Security research; Technical indicator; Training program; Surveys
A Query Language for Workflow Logs,2022,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127608261&doi=10.1145%2f3482968&partnerID=40&md5=d203163178177654257be9ec8aabc4cb,"A business process (workflow) is an assembly of tasks to accomplish a business goal. Real-world workflow models often demanded to change due to new laws and policies, changes in the environment, and so on. To understand the inner workings of a business process to facilitate changes, workflow logs have the potential to enable inspecting, monitoring, diagnosing, analyzing, and improving the design of a complex workflow. Querying workflow logs, however, is still mostly an ad hoc practice by workflow managers. In this article, we focus on the problem of querying workflow log concerning both control flow and dataflow properties. We develop a query language based on ""incident patterns""to allow the user to directly query workflow logs instead of having to transform such queries into database operations. We provide the formal semantics and a query evaluation algorithm of our language. By deriving an accurate cost model, we develop an optimization mechanism to accelerate query evaluation. Our experiment results demonstrate the effectiveness of the optimization and achieves up to 50× speedup over an adaption of existing evaluation method. © 2021 Association for Computing Machinery.",Business workflow; log; query languages,Environmental regulations; Formal methods; Query processing; Semantics; Business goals; Business Process; Business workflows; Complex workflows; Log; Optimisations; Policy changes; Real-world; Work-flows; Workflow models; Query languages
Establishing Data Provenance for Responsible Artificial Intelligence Systems,2022,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127561926&doi=10.1145%2f3503488&partnerID=40&md5=6f92c85efaca293e15a927c6fe35c90c,"Data provenance, a record that describes the origins and processing of data, offers new promises in the increasingly important role of artificial intelligence (AI)-based systems in guiding human decision making. To avoid disastrous outcomes that can result from bias-laden AI systems, responsible AI builds on four important characteristics: fairness, accountability, transparency, and explainability. To stimulate further research on data provenance that enables responsible AI, this study outlines existing biases and discusses possible implementations of data provenance to mitigate them. We first review biases stemming from the data's origins and pre-processing. We then discuss the current state of practice, the challenges it presents, and corresponding recommendations to address them. We present a summary highlighting how our recommendations can help establish data provenance and thereby mitigate biases stemming from the data's origins and pre-processing to realize responsible AI-based systems. We conclude with a research agenda suggesting further research avenues. © 2022 Copyright held by the owner/author(s).",accountability; artificial intelligence; Data provenance; explainability; fairness; transparency,Artificial intelligence; Data handling; Decision making; 'current; Accountability; Artificial intelligence systems; Data provenance; Explainability; Fairness; Human decision-making; Pre-processing; Research agenda; State of practise; Transparency
Detecting Android Malware and Classifying Its Families in Large-scale Datasets,2022,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127575514&doi=10.1145%2f3464323&partnerID=40&md5=45ce2881d69b905ce0ca379ce364c174,"To relieve the burden of security analysts, Android malware detection and its family classification need to be automated. There are many previous works focusing on using machine (or deep) learning technology to tackle these two important issues, but as the number of mobile applications has increased in recent years, developing a scalable and precise solution is a new challenge that needs to be addressed in the security field. Accordingly, in this article, we propose a novel approach that not only enhances the performance of both Android malware and its family classification, but also reduces the running time of the analysis process. Using large-scale datasets obtained from different sources, we demonstrate that our method is able to output a high F-measure of 99.71% with a low FPR of 0.37%. Meanwhile, the computation time for processing a 300K dataset is reduced to nearly 3.3 hours. In addition, in classification evaluation, we demonstrate that the F-measure, precision, and recall are 97.5%, 96.55%, 98.64%, respectively, when classifying 28 malware families. Finally, we compare our method with previous studies in both detection and classification evaluation. We observe that our method produces better performance in terms of its effectiveness and efficiency. © 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Android malware; deep learning; machine learning; natural language processing; semantic-aware,Classification (of information); Deep learning; Large dataset; Learning algorithms; Malware; Mobile security; Natural language processing systems; Semantics; Android malware; Classification evaluation; Deep learning; F measure; Large-scale datasets; Learning technology; Malware detection; Performance; Securities analysts; Semantic-aware; Android (operating system)
Distance Based Pattern Driven Mining for Outlier Detection in High Dimensional Big Dataset,2022,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121860461&doi=10.1145%2f3469891&partnerID=40&md5=7e6b8bb2c8b4350441bc74a930e93060,"Detection of outliers or anomalies is one of the vital issues in pattern-driven data mining. Outlier detection detects the inconsistent behavior of individual objects. It is an important sector in the data mining field with several different applications such as detecting credit card fraud, hacking discovery and discovering criminal activities. It is necessary to develop tools used to uncover the critical information established in the extensive data. This paper investigated a novel method for detecting cluster outliers in a multidimensional dataset, capable of identifying the clusters and outliers for datasets containing noise. The proposed method can detect the groups and outliers left by the clustering process, like instant irregular sets of clusters (C) and outliers (O), to boost the results. The results obtained after applying the algorithm to the dataset improved in terms of several parameters. For the comparative analysis, the accurate average value and the recall value parameters are computed. The accurate average value is 74.05% of the existing COID algorithm, and our proposed algorithm has 77.21%. The average recall value is 81.19% and 89.51% of the existing and proposed algorithm, which shows that the proposed work efficiency is better than the existing COID algorithm. © 2022 American Institute of Physics Inc.. All rights reserved.",Cluster; data visualization; distance-based; k-means; outliers,Anomaly detection; Crime; Data mining; K-means clustering; Large dataset; Personal computing; Statistics; Average values; Cluster; Credit card frauds; Criminal activities; Distance-based; High-dimensional; Higher-dimensional; Individual objects; K-means; Outlier Detection; Data visualization
Key Factors Affecting User Adoption of Open-Access Data Repositories in Intelligence and Security Informatics: An Affordance Perspective,2021,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123037822&doi=10.1145%2f3460823&partnerID=40&md5=f8238a00c14d1210cfe850a3bea02ade,"Rich, diverse cybersecurity data are critical for efforts by the intelligence and security informatics (ISI) community. Although open-access data repositories (OADRs) provide tremendous benefits for ISI researchers and practitioners, determinants of their adoption remain understudied. Drawing on affordance theory and extant ISI literature, this study proposes a factor model to explain how the essential and unique affordances of an OADR (i.e., relevance, accessibility, and integration) affect individual professionals' intentions to use and collaborate with AZSecure, a major OADR. A survey study designed to test the model and hypotheses reveals that the effects of affordances on ISI professionals' intentions to use and collaborate are mediated by perceived usefulness and ease of use, which then jointly determine their perceived value. This study advances ISI research by specifying three important affordances of OADRs; it also contributes to extant technology adoption literature by scrutinizing and affirming the interplay of essential user acceptance and value perceptions to explain ISI professionals' adoptions of OADRs.  © 2021 Association for Computing Machinery.",affordance theory; Intelligence and security informatics; open-access data repositories; perceived value; technology acceptance; user technology adoption,Cybersecurity; Affordance theories; Affordances; Data repositories; Intelligence and security informatics; Open-access data repository; OpenAccess; Perceived value; Technology acceptance; Technology adoption; User technology adoption; Open Data
Cross-Modality Transfer Learning for Image-Text Information Management,2021,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119961901&doi=10.1145%2f3464324&partnerID=40&md5=374555ce646c4fd30b723d490052201e,"In the past decades, information from all kinds of data has been on a rapid increase. With state-of-the-art performance, machine learning algorithms have been beneficial for information management. However, insufficient supervised training data is still an adversity in many real-world applications. Therefore, transfer learning (TF) was proposed to address this issue. This article studies a not well investigated but important TL problem termed cross-modality transfer learning (CMTL). This topic is closely related to distant domain transfer learning (DDTL) and negative transfer. In general, conventional TL disciplines assume that the source domain and the target domain are in the same modality. DDTL aims to make efficient transfers even when the domains or the tasks are entirely different. As an extension of DDTL, CMTL aims to make efficient transfers between two different data modalities, such as from image to text. As the main focus of this study, we aim to improve the performance of image classification by transferring knowledge from text data. Previously, a few CMTL algorithms were proposed to deal with image classification problems. However, most existing algorithms are very task specific, and they are unstable on convergence. There are four main contributions in this study. First, we propose a novel heterogeneous CMTL algorithm, which requires only a tiny set of unlabeled target data and labeled source data with associate text tags. Second, we introduce a latent semantic information extraction method to connect the information learned from the image data and the text data. Third, the proposed method can effectively handle the information transfer across different modalities (text-image). Fourth, we examined our algorithm on a public dataset, Office-31. It has achieved up to 5% higher classification accuracy than ""non-transfer""algorithms and up to 9% higher than existing CMTL algorithms.  © 2021 Association for Computing Machinery.",cross-modality transfer learning; image-text information management; Machine learning; transfer learning with semantic features,Image classification; Image enhancement; Learning algorithms; Machine learning; Semantics; Text processing; Cross modality; Cross-modality transfer learning; Image texts; Image-text information management; Machine-learning; Semantic features; Text information; Transfer learning; Transfer learning with semantic feature; Information management
TRG-DAtt: The Target Relational Graph and Double Attention Network Based Sentiment Analysis and Prediction for Supporting Decision Making,2021,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159300283&doi=10.1145%2f3462442&partnerID=40&md5=715a0a1a663f843d1ea94c5fdfe2ebfa,"The management of public opinion and the use of big data monitoring to accurately judge and verify all kinds of information are valuable aspects in the enterprise management decision-making process. The sentiment analysis of reviews is a key decision-making tool for e-commerce development. Most existing review sentiment analysis methods involve sequential modeling but do not focus on the semantic relationships. However, Chinese semantics are different from English semantics in terms of the sentence structure. Irrelevant contextual words may be incorrectly identified as cues for sentiment prediction. The influence of the target words in reviews must be considered. Thus, this paper proposes the TRG-DAtt model for sentiment analysis based on target relational graph (TRG) and double attention network (DAtt) to analyze the emotional information to support decision making. First, dependency tree-based TRG is introduced to independently and fully mine the semantic relationships. We redefine and constrain the dependency and use it as the edges to connect the target and context words. Second, we design dependency graph attention network (DGAT) and interactive attention network (IAT) to form the DAtt and obtain the emotional features of the target words and reviews. DGAT models the dependency of the TRG by aggregating the semantic information. Next, the target emotional enhancement features obtained by the DGAT are input to the IAT. The influence of each target word on the review can be obtained through the interaction. Finally, the target emotional enhancement features are weighted by the impact factor to generate the review's emotional features. In this study, extensive experiments were conducted on the car and Meituan review data sets, which contain consumer reviews on cars and stores, respectively. The results demonstrate that the proposed model outperforms the existing models.  © 2021 Association for Computing Machinery.",decision making; dependency relationship; graph attention network; interactive attention network; prediction; Sentiment analysis; target relational graph,Decision making; Semantics; Sentiment analysis; Social aspects; Decisions makings; Dependency graphs; Dependency relationship; Graph attention network; Interactive attention network; Relational graph; Semantic relationships; Sentiment analysis; Target relational graph; Target words; Forecasting
Machine Learning for Automated Industrial IoT Attack Detection: An Efficiency-Complexity Trade-off,2021,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143763118&doi=10.1145%2f3460822&partnerID=40&md5=9a353fd7dfc2a0e3a1c3455f518bdf8a,"Critical city infrastructures that depend on smart Industrial Internet of Things (IoT) devices have been increasingly becoming a target of cyberterrorist or hacker attacks. Although this has led to multiple studies in the recent past, there exists a paucity of literature concerning real-time Industrial IoT attack detection. The goal of this article is to build a machine-learning approach using Industrial IoT sensor readings for accurately tracking down Industrial IoT attacks in real time. We analyze IoT system behavior under a lab-controlled series of attacks on a Secure Water Treatment (SWaT) system. The system is analytically challenging in that it results in sensor readings that resemble waveforms. To that end, we develop a novel early detection method using functional shape analysis (FSA) to extract features from the data that can capture the profile of the waveform. Our results show an efficiency-complexity trade-off between functional and non-functional methods in predicting IoT attacks. © 2021 Association for Computing Machinery.",cybersecurity; functional shape analysis (FSA); Industrial IoT; machine learning,Cybersecurity; Economic and social effects; Efficiency; Internet of things; Personal computing; Water treatment; Attack detection; Cyber security; Functional shape analyse; Industrial internet of thing; Machine-learning; Real- time; Sensor readings; Shape-analysis; Trade off; Waveforms; Machine learning
"Introduction to the Special Issue on Pattern-Driven Mining, Analytics, and Prediction for Decision Making, Part 1",2021,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159270282&doi=10.1145%2f3486960&partnerID=40&md5=c55f7603e1c0ddd9449d59c7eb500091,[No abstract available],,
Mining High Utility Itemsets with Hill Climbing and Simulated Annealing,2021,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159356284&doi=10.1145%2f3462636&partnerID=40&md5=8a946aef62e5171a73f19a19ee200edf,"High utility itemset mining (HUIM) is the task of finding all items set, purchased together, that generate a high profit in a transaction database. In the past, several algorithms have been developed to mine high utility itemsets (HUIs). However, most of them cannot properly handle the exponential search space while finding HUIs when the size of the database and total number of items increases. Recently, evolutionary and heuristic algorithms were designed to mine HUIs, which provided considerable performance improvement. However, they can still have a long runtime and some may miss many HUIs. To address this problem, this article proposes two algorithms for HUIM based on Hill Climbing (HUIM-HC) and Simulated Annealing (HUIM-SA). Both algorithms transform the input database into a bitmap for efficient utility computation and for search space pruning. To improve population diversity, HUIs discovered by evolution are used as target values for the next population instead of keeping the current optimal values in the next population. Through experiments on real-life datasets, it was found that the proposed algorithms are faster than state-of-the-art heuristic and evolutionary HUIM algorithms, that HUIM-SA discovers similar HUIs, and that HUIM-SA evolves linearly with the number of iterations.  © 2021 Association for Computing Machinery.",bitmap; high utility itemsets; Hill climbing; neighbor; simulated annealing,Computational efficiency; Database systems; Evolutionary algorithms; Heuristic algorithms; Bit maps; Exponentials; Heuristics algorithm; High utility itemset minings; High utility itemsets; Hill climbing; Itemset; Neighbor; Search spaces; Transaction database; Simulated annealing
Introduction to the Special Section on Using AI and Data Science to Handle Pandemics and Related Disruptions,2021,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85172699153&doi=10.1145%2f3486969&partnerID=40&md5=8d0de2f641edb4268c17c9b87b51010b,[No abstract available],,
AI-Based Vehicular Network toward 6G and IoT: Deep Learning Approaches,2021,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159364217&doi=10.1145%2f3466691&partnerID=40&md5=0294d75e826d1f4e28335833c2a578ad,"In recent years, vehicular networks have become increasingly large, heterogeneous, and dynamic, making it difficult to meet strict requirements of ultralow latency, high reliability, high security, and massive connections for next generation (6G) networks. Recently, deep learning (DL) has emerged as a powerful artificial intelligence (AI) technique to optimize the efficiency and adaptability of vehicle and wireless communication. However, rapidly increasing absolute numbers of vehicles on the roads are leading to increased automobile accidents, many of which are attributable to drivers interacting with their mobile phones. To address potentially dangerous driver behavior, this study applies deep learning approaches to image recognition to develop an AI-based detection system that can detect potentially dangerous driving behavior. Multiple convolutional neural network (CNN)-based techniques including VGG16, VGG19, Densenet, and Openpose were compared in terms of their ability to detect and identify problematic driving. © 2021 Association for Computing Machinery.",6G; Convolutional neural network; deep learning; internet of things; vehicular network,Accidents; Behavioral research; Convolution; Convolutional neural networks; Deep learning; Image recognition; 6g; Artificial intelligence techniques; Convolutional neural network; Deep learning; High reliability; High securities; Learning approach; Vehicle communications; Vehicular networks; Wireless communications; Internet of things
DSSAE: Deep Stacked Sparse Autoencoder Analytical Model for COVID-19 Diagnosis by Fractional Fourier Entropy,2021,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159196557&doi=10.1145%2f3451357&partnerID=40&md5=8d96760697aefddc18fb2a35945a8a8b,"(Aim) COVID-19 has caused more than 2.28 million deaths till 4/Feb/2021 while it is still spreading across the world. This study proposed a novel artificial intelligence model to diagnose COVID-19 based on chest CT images. (Methods) First, the two-dimensional fractional Fourier entropy was used to extract features. Second, a custom deep stacked sparse autoencoder (DSSAE) model was created to serve as the classifier. Third, an improved multiple-way data augmentation was proposed to resist overfitting. (Results) Our DSSAE model obtains a micro-averaged F1 score of 92.32% in handling a four-class problem (COVID-19, community-acquired pneumonia, secondary pulmonary tuberculosis, and healthy control). (Conclusion) Our method outperforms 10 state-of-the-art approaches. © 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM.",autoencoder; COVID-19; Deep learning; fractional Fourier entropy,Computerized tomography; Deep learning; Entropy; Fourier transforms; Learning systems; Auto encoders; Chest CT; CT Image; Data augmentation; Deep learning; Fractional fourier; Fractional fourier entropy; Intelligence models; Overfitting; Two-dimensional; COVID-19
Privacy and Confidentiality in Process Mining: Threats and Research Challenges,2021,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159352512&doi=10.1145%2f3468877&partnerID=40&md5=015187809f2700db037f4cd032fb5a83,Privacy and confidentiality are very important prerequisites for applying process mining to comply with regulations and keep company secrets. This article provides a foundation for future research on privacy-preserving and confidential process mining techniques. Main threats are identified and related to a motivation application scenario in a hospital context as well as to the current body of work on privacy and confidentiality in process mining. A newly developed conceptual model structures the discussion that existing techniques leave room for improvement. This results in a number of important research challenges that should be addressed by future process mining research. © 2021 Copyright held by the owner/author(s).,confidentiality; privacy; Process mining; research challenges,Data mining; Privacy-preserving techniques; 'current; Application scenario; Conceptual model; Confidentiality; In-process; Mining techniques; Privacy; Privacy preserving; Process mining; Research challenges; Model structures
Novel Machine Learning for Big Data Analytics in Intelligent Support Information Management Systems,2021,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134346228&doi=10.1145%2f3469890&partnerID=40&md5=02cdab6ab69939f0a9d5abd083c48923,"Two-dimensional1 arrays of bi-component structures made of cobalt and permalloy elliptical dots with thickness of 25 nm, length 1 mm and width of 225 nm, have been prepared by a self-aligned shadow deposition technique. Brillouin light scattering has been exploited to study the frequency dependence of thermally excited magnetic eigenmodes on the intensity of the external magnetic field, applied along the easy axis of the elements.Scientific information technology has been developed rapidly. Here, the purposes are to make people's lives more convenient and ensure information management and classification. The machine learning algorithm is improved to obtain the optimized Light Gradient Boosting Machine (LightGBM) algorithm. Then, an Android-based intelligent support information management system is designed based on LightGBM for the big data analysis and classification management of information in the intelligent support information management system. The system is designed with modules of employee registration and login, company announcement notice, attendance and attendance management, self-service, and daily tools with the company as the subject. Furthermore, the performance of the constructed information management system is analyzed through simulations. Results demonstrate that the training time of the optimized LightGBM algorithm can stabilize at about 100s, and the test time can stabilize at 0.68s. Besides, its accuracy rate can reach 89.24%, which is at least 3.6% higher than other machine learning algorithms. Moreover, the acceleration efficiency analysis of each algorithm suggests that the optimized LightGBM algorithm is suitable for processing large amounts of data; its acceleration effect is more apparent, and its acceleration ratio is higher than other algorithms. Hence, the constructed intelligent support information management system can reach a high accuracy while ensuring the error, with apparent acceleration effect. Therefore, this model can provide an experimental reference for information classification and management in various fields. © 2021 Association for Computing Machinery.",accuracy rate; big data analysis; intelligent support information system; lightGBM; Machine learning,Adaptive boosting; Big data; Classification (of information); Data Analytics; Data handling; Data visualization; Machine learning; Nickel alloys; Accuracy rate; Big data analyse; Gradient boosting; Information management systems; Intelligent support; Intelligent support information system; Light gradients; Lightgbm; Machine algorithm; Machine-learning; Information management
AIDCOV: An Interpretable Artificial Intelligence Model for Detection of COVID-19 from Chest Radiography Images,2021,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145435285&doi=10.1145%2f3466690&partnerID=40&md5=13700aab96e971727f5f579ed8c1ed4e,"As the Coronavirus Disease 2019 (COVID-19) pandemic continues to grow globally, testing to detect COVID-19 and isolating individuals who test positive remains the primary strategy for preventing community spread of the disease. Therefore, automatic and accurate detection of COVID-19 using medical imaging modalities, which are more widely available and accessible, can be beneficial as an alternative diagnostic tool. In this study, an Artificial Intelligence model for Detection of COVID-19 (AIDCOV) is developed to classify chest radiography images as belonging to a person with either COVID-19, other infections, or no pneumonia (i.e., normal). The hierarchical structure in AIDCOV captures the dependencies among features and improves model performance while an attention mechanism makes the model interpretable and transparent. We used several publicly available datasets of both computed tomography (CT) and X-ray modalities. The main public dataset for chest X-ray images contains 475 COVID-19 samples, 3949 samples from other viral/bacterial infections, and 1583 normal samples. Our model achieves a mean cross-validation accuracy of 98.4%. AIDCOV has a sensitivity of 99.8%, a specificity of 100%, and an F1-score of 99.8% in detecting COVID-19 from X-ray images on that dataset. Using a large dataset of CT images, our model obtained mean cross-validation accuracy and sensitivity of 98.8% and 99.4%, respectively. Additionally, our interpretable model can distinguish subtle signs of infection within each radiography image. Assuming these results hold up in larger datasets obtained from a variety of patients over the world, AIDCOV can be used in conjunction with or instead of RT-PCR testing (where RT-PCR testing is unavailable) to detect and isolate individuals with COVID-19, prevent onward transmission to the general population and healthcare workers, and highlight the areas in the lungs that show signs of COVID-related damage. © 2021 Association for Computing Machinery.",artificial intelligence; attention; chest CT; chest X-ray; convolutional neural networks; COVID-19; deep learning; pneumonia; transfer learning,Computerized tomography; Convolutional neural networks; Deep learning; Diagnosis; Large dataset; Medical imaging; Transfer learning; X ray radiography; Attention; Chest computed tomography; Chest radiography; Chest X-ray; Convolutional neural network; Deep learning; Intelligence models; Pneumonia; Radiography images; Transfer learning; COVID-19
A New Approach for Mining Correlated Frequent Subgraphs,2021,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159367985&doi=10.1145%2f3473042&partnerID=40&md5=9587aa412e2d290a7802b043265bf8b4,"Nowadays graphical datasets are having a vast amount of applications. As a result, graph mining - mining graph datasets to extract frequent subgraphs - has proven to be crucial in numerous aspects. It is important to perform correlation analysis among the subparts (i.e., elements) of the frequent subgraphs generated using graph mining to observe interesting information. However, the majority of existing works focuses on complexities in dealing with graphical structures, and not much work aims to perform correlation analysis. For instance, a previous work realized in this regard, operated with a very naive raw approach to fulfill the objective, but dealt only on a small subset of the problem. Hence, in this article, a new measure is proposed to aid in the analysis for large subgraphs, mined from various types of graph transactions in the dataset. These subgraphs are immense in terms of their structural composition, and thus parallel the entire set of graphs in real-world. A complete framework for discovering the relations among parts of a frequent subgraph is proposed using our new method. Evaluation results show the usefulness and accuracy of the newly defined measure on real-life graphical datasets. © 2021 Association for Computing Machinery.",Association rules; branching factor; confidence measures; correlation discovery; data mining; graph mining,Correlation methods; Large dataset; Branching factors; Confidence Measure; Correlation analysis; Correlation discovery; Frequent subgraphs; Graph mining; Graphical structures; Interesting information; New approaches; Subgraphs; Data mining
High-Resolution Spatio-Temporal Model for County-Level COVID-19 Activity in the U.S.,2021,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115840313&doi=10.1145%2f3468876&partnerID=40&md5=8896c8f2dc710ce998877b528b46bc43,"We present an interpretable high-resolution spatio-temporal model to estimate COVID-19 deaths together with confirmed cases 1 week ahead of the current time, at the county level and weekly aggregated, in the United States. A notable feature of our spatio-temporal model is that it considers the (1) temporal auto- and pairwise correlation of the two local time series (confirmed cases and deaths from the COVID-19), (2) correlation between locations (propagation between counties), and (3) covariates such as local within-community mobility and social demographic factors. The within-community mobility and demographic factors, such as total population and the proportion of the elderly, are included as important predictors since they are hypothesized to be important in determining the dynamics of COVID-19. To reduce the model's high dimensionality, we impose sparsity structures as constraints and emphasize the impact of the top 10 metropolitan areas in the nation, which we refer to (and treat within our models) as hubs in spreading the disease. Our retrospective out-of-sample county-level predictions were able to forecast the subsequently observed COVID-19 activity accurately. The proposed multivariate predictive models were designed to be highly interpretable, with clear identification and quantification of the most important factors that determine the dynamics of COVID-19. Ongoing work involves incorporating more covariates, such as education and income, to improve prediction accuracy and model interpretability. © 2021 Association for Computing Machinery.",COVID-19; spatio-temporal model; vector autoregressive process,Data mining; Forecasting; Population statistics; 'current; Auto correlation; Auto regressive process; County level; Covariates; Demographic factors; High resolution; Pairwise correlation; Spatio-temporal models; Vector autoregressive process; COVID-19
CoRSAI: A System for Robust Interpretation of CT Scans of COVID-19 Patients Using Deep Learning,2021,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129092270&doi=10.1145%2f3467471&partnerID=40&md5=d1b12ac9dd4653a8e1c8d0c9738da23e,"Analysis of chest CT scans can be used in detecting parts of lungs that are affected by infectious diseases such as COVID-19. Determining the volume of lungs affected by lesions is essential for formulating treatment recommendations and prioritizing patients by severity of the disease. In this article we adopted an approach based on using an ensemble of deep convolutional neural networks for segmentation of slices of lung CT scans. Using our models, we are able to segment the lesions, evaluate patients' dynamics, estimate relative volume of lungs affected by lesions, and evaluate the lung damage stage. Our models were trained on data from different medical centers. We compared predictions of our models with those of six experienced radiologists, and our segmentation model outperformed most of them. On the task of classification of disease severity, our model outperformed all the radiologists. © 2021 Association for Computing Machinery.",Convolutional neural network; COVID-19; deep learning; ensembling; lesion detection; segmentation,Biological organs; Computerized tomography; Convolution; Convolutional neural networks; Deep neural networks; Learning systems; Patient treatment; Chest CT scans; Convolutional neural network; CT-scan; Deep learning; Ensembling; Infectious disease; Lesion detection; Lung CT; Medical center; Segmentation; COVID-19
Write like a pro or an amateur? Effect of medical language formality,2021,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122616695&doi=10.1145%2f3458752&partnerID=40&md5=ba7590aaea55b20050a85e433ca995a7,"Past years have seen rising engagement among caregivers in online health communities. Although studies indicate that this caregiver-generated online health information benefits patients, how such information can be perceived easily and correctly remains unclear. This study aims to fill this gap by exploring mechanisms to improve the perceived helpfulness of online health information. We propose a multi-method framework, including a novel Medical-Enriched DEep Learning (MEDEL) feature extraction method, econometric analyses, and a randomized experiment. The results show that when the medical language of health information is informal, the senior care information is more helpful. Our findings provide a theoretical foundation to understand the influence of language formality on many other business communications. Our proposed multi-method approach can also be generalized to investigate research questions involving complex textual features. Forum sites could leverage our proposed approach to improve the helpfulness of online health information and user satisfaction.  © 2021 Association for Computing Machinery.",Deep learning; Health IT; Multi-method; Randomized experiment; Text mining,Deep learning; Deep learning; Econometric analysis; Feature extraction methods; Health informations; Health IT; Multi methods; Online health communities; Online health information; Randomized experiments; Theoretical foundations; Health
Design and Realisation of Scalable Business Process Management Systems for Deployment in the Cloud,2021,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132161043&doi=10.1145%2f3460123&partnerID=40&md5=3d9c0530de396d8ec51f3f120092b782,"Business Process Management Systems (BPMSs) provide automated support for the execution of business processes in modern organisations. With the emergence of cloud computing, BPMS deployment considerations are shifting from traditional on-premise models to the Software-as-a-Service (SaaS) paradigm, aiming at delivering Business Process Automation as a Service. However, scaling up a traditional BPMS to cope with simultaneous demand from multiple organisations in the cloud is challenging, since its underlying system architecture has been designed to serve a single organisation with a single process engine. Moreover, the complexity in addressing both the dynamic execution environment and the elasticity requirements of users impose further challenges to deploying a traditional BPMS in the cloud. A typical SaaS often deploys multiple instances of its core applications and distributes workload to these application instances via load balancing. But, for stateful and often long-running process instances, standard stateless load balancing strategies are inadequate. In this article, we propose a conceptual design of BPMS capable of addressing dynamically varying demands of end users in the cloud, and present a prototypical implementation using an open source traditional BPMS platform. Both the design and system realisation offer focused strategies on achieving scalability and demonstrates the system capabilities for supporting both upscaling, to address large volumes of user demand or workload, and downscaling, to release underutilised computing resources, in a cloud environment. © 2021 Association for Computing Machinery.",business process engine; Business process management systems; load balancing; scalability; software-as-a-service,Balancing; Conceptual design; Enterprise resource management; Open source software; Platform as a Service (PaaS); Software as a service (SaaS); Automated support; Business Process; Business process engine; Business process management systems; Cloud-computing; Load-Balancing; Process engines; Service paradigm; Software-as-a- Service (SaaS); System deployment; Scalability
Automated feature selection for anomaly detection in network traffic data,2021,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122616587&doi=10.1145%2f3446636&partnerID=40&md5=4512905999742060acac7bba4b26751a,"Variable selection (also known as feature selection) is essential to optimize the learning complexity by prioritizing features, particularly for a massive, high-dimensional dataset like network traffic data. In reality, however, it is not an easy task to effectively perform the feature selection despite the availability of the existing selection techniques. From our initial experiments, we observed that the existing selection techniques produce different sets of features even under the same condition (e.g., a static size for the resulted set). In addition, individual selection techniques perform inconsistently, sometimes showing better performance but sometimes worse than others, thereby simply relying on one of them would be risky for building models using the selected features. More critically, it is demanding to automate the selection process, since it requires laborious efforts with intensive analysis by a group of experts otherwise. In this article, we explore challenges in the automated feature selection with the application of network anomaly detection. We first present our ensemble approach that benefits from the existing feature selection techniques by incorporating them, and one of the proposed ensemble techniques based on greedy search works highly consistently showing comparable results to the existing techniques. We also address the problem of when to stop to finalize the feature elimination process and present a set of methods designed to determine the number of features for the reduced feature set. Our experimental results conducted with two recent network datasets show that the identified feature sets by the presented ensemble and stopping methods consistently yield comparable performance with a smaller number of features to conventional selection techniques.  © 2021 Association for Computing Machinery.",Cybersecurity analytics; Ensemble approach; Feature selection; Network anomaly detection,Anomaly detection; Automated features; Cyber security; Cybersecurity analytic; Ensemble approaches; Features selection; Network anomaly detection; Network traffic; Performance; Selection techniques; Traffic data; Feature extraction
Early exploration of MOOCs in the U.S. Higher education: An absorptive capacity perspective,2021,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122621939&doi=10.1145%2f3456295&partnerID=40&md5=e231ed22e54de0044652c081ebf33a4e,"Advanced information technologies have enabled Massive Open Online Courses (MOOCs), which have the potential to transform higher education around the world. Why are some institutions eager to embrace this technology-enabled model of teaching, while others remain reluctant to jump aboard? Applying the theory of absorptive capacity, we study the role of a university's educational IT capabilities in becoming an early MOOC producer. Examining the history of MOOC offerings by U.S. colleges and universities, we find that prior IT capabilities, such as (1) the use of Web 2.0, social media and other interactive tools for teaching and (2) experience with distance education and hybrid teaching, are positively associated with the early exploration of MOOCs. Interestingly, we also find that the effect of educational IT capabilities is moderated by social integration mechanisms and activation triggers. For example, when instructional IT supporting services are highly decentralized, educational IT capabilities have a greater impact on the probability of a university offering a MOOC. In addition, for colleges facing an adverse environment, such as those experience a decline in college applications, the effect of IT capabilities on the exploration of MOOCs is much stronger.  © 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Absorptive capacity; Distance learning; IT capability; MOOC; Online education,Education computing; Teaching; Absorptive capacity; Advanced informations; Colleges and universities; Distance-learning; High educations; IT Capabilities; Massive open online course; Models of teaching; On-line education; Web 2.0; E-learning
Using Social Media to Analyze Public Concerns and Policy Responses to COVID-19 in Hong Kong,2021,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161132870&doi=10.1145%2f3460124&partnerID=40&md5=555a67974f12f32532f56b70de5f4d57,"The outbreak of COVID-19 has caused huge economic and societal disruptions. To fight against the coronavirus, it is critical for policymakers to take swift and effective actions. In this article, we take Hong Kong as a case study, aiming to leverage social media data to support policymakers' policy-making activities in different phases. First, in the agenda setting phase, we facilitate policymakers to identify key issues to be addressed during COVID-19. In particular, we design a novel epidemic awareness index to continuously monitor public discussion hotness of COVID-19 based on large-scale data collected from social media platforms. Then we identify the key issues by analyzing the posts and comments of the extensively discussed topics. Second, in the policy evaluation phase, we enable policymakers to conduct real-time evaluation of anti-epidemic policies. Specifically, we develop an accurate Cantonese sentiment classification model to measure the public satisfaction with anti-epidemic policies and propose a keyphrase extraction technique to further extract public opinions. To the best of our knowledge, this is the first work which conducts a large-scale social media analysis of COVID-19 in Hong Kong. The analytical results reveal some interesting findings: (1) there is a very low correlation between the number of confirmed cases and the public discussion hotness of COVID-19. The major public concern in the early stage is the shortage of anti-epidemic items. (2) The top-3 anti-epidemic measures with the greatest public satisfaction are daily press conference on COVID-19 updates, border closure, and social distancing rules. © 2021 Association for Computing Machinery.",COVID-19; natural language processing; public opinion; sentiment analysis; social media,Epidemiology; Sentiment analysis; Social aspects; Social networking (online); Hong-kong; Key Issues; Language processing; Natural language processing; Natural languages; Policy makers; Public concern; Public opinions; Sentiment analysis; Social media; COVID-19
Understanding the Impact of COVID-19 on Online Mental Health Forums,2021,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121665574&doi=10.1145%2f3458770&partnerID=40&md5=ea12b9bd7d166752b0a74957bf6113cb,"Like many of the disasters that have preceded it, the COVID-19 pandemic is likely to have a profound impact on people's mental health. Understanding its impact can inform strategies for mitigating negative consequences. This work seeks to better understand the impacts of COVID-19 on mental health by examining how discussions on mental health subreddits have changed in the three months following the WHO's declaration of a global pandemic. First, the rate at which the pandemic is discussed in each community is quantified. Then, volume of activity is measured to determine whether the number of people with mental health concerns has risen, and user interactions are analyzed to determine how they have changed during the pandemic. Finally, the content of the discussions is analyzed. Each of these metrics is considered with respect to a set of control subreddits to better understand if the changes present are specific to mental health subreddits or are representative of Reddit as a whole. There are numerous changes in the three mental health subreddits that we consider, r/Anxiety, r/depression, r/SuicideWatch; there is reduced posting activity in most cases, and there are significant changes in discussion of some topics such as work and anxiety. The results suggest that there is not an overwhelming increase in online mental health support-seeking on Reddit during the pandemic, but that discussion content related to mental health has changed. © 2021 Association for Computing Machinery.",COVID-19; Mental health; time series; topic modeling; user interaction,Community IS; Health concerns; Mental health; Number of peoples; Times series; Topic Modeling; User interaction; COVID-19
University Operations During a Pandemic: A Flexible Decision Analysis Toolkit,2021,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138537238&doi=10.1145%2f3460125&partnerID=40&md5=ab10059b4dfeaaf4b212cf46502e8a2e,"Modeling infection spread during pandemics is not new, with models using past data to tune simulation parameters for predictions. These help in understanding of the healthcare burden posed by a pandemic and responding accordingly. However, the problem of how college/university campuses should function during a pandemic is new for the following reasons: (i) social contact in colleges are structured and can be engineered for chosen objectives; (ii) the last pandemic to cause such societal disruption was more than 100 years ago, when higher education was not a critical part of society; (iii) not much was known about causes of pandemics, and hence effective ways of safe operations were not known; and (iv) today with distance learning, remote operation of an academic institution is possible. As one of the first to address this problem, our approach is unique in presenting a flexible simulation system, containing a suite of model libraries, one for each major component. The system integrates agent-based modeling and the stochastic network approach, and models the interactions among individual entities (e.g., students, instructors, classrooms, residences) in great detail. For each decision to be made, the system can be used to predict the impact of various choices, and thus enables the administrator to make informed decisions. Although current approaches are good for infection modeling, they lack accuracy in social contact modeling. Our agent-based modeling approach, combined with ideas from Network Science, presents a novel approach to contact modeling. A detailed case study of the University of Minnesota's Sunrise Plan is presented. For each decision made, its impact was assessed, and results were used to get a measure of confidence. We believe that this flexible tool can be a valuable asset for various kinds of organizations to assess their infection risks in pandemic-time operations, including middle and high schools, factories, warehouses, and small/medium-sized businesses. © 2021 Association for Computing Machinery.",Agent based modeling; bipartite networks; COVID-19; decision analysis; simulation,Autonomous agents; Computational methods; Decision making; Distance education; Education computing; Risk assessment; Simulation platform; Stochastic models; Stochastic systems; Students; Agent-based model; Analysis toolkits; Bipartite network; Contact modeling; High educations; Safe operation; Simulation; Simulation parameters; Social contacts; University campus; COVID-19
COVID-Safe Spatial Occupancy Monitoring Using OFDM-Based Features and Passive WiFi Samples,2021,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127289210&doi=10.1145%2f3472668&partnerID=40&md5=a576bb7974a566df2e887503e6995fab,"During the COVID-19 pandemic, authorities have been asking for social distancing to prevent transmission of the virus. However, enforcing such distancing has been challenging in tight spaces such as elevators and unmonitored commercial settings such as offices. This article addresses this gap by proposing a low-cost and non-intrusive method for monitoring social distancing within a given space, using Channel State Information (CSI) from passive WiFi sensing. By exploiting the frequency selective behavior of CSI with a Support Vector Machine (SVM) classifier, we achieve an improvement in accuracy over existing crowd counting works. Our system counts the number of occupants with a 93% accuracy rate in an elevator setting and predicts whether the COVID-Safe limit is breached with a 97% accuracy rate. We also demonstrate the occupant counting capability of the system in a commercial office setting, achieving 97% accuracy. Our proposed occupancy monitoring outperforms existing methods by at least 7%. Overall, the proposed framework is inexpensive, requiring only one device that passively collects data and a lightweight supervised learning algorithm for prediction. Our lightweight model and accuracy improvements are necessary contributions for WiFi-based counting to be suitable for COVID-specific applications. © 2021 Association for Computing Machinery.",channel state information; COVID-19; machine learning; social distancing; support vector machine; WiFi,Channel state information; Costs; Elevators; Learning algorithms; Learning systems; Support vector machines; Viruses; Wireless local area networks (WLAN); Accuracy rate; Channel-state information; Commercial settings; Frequency-selective; Low-costs; Machine-learning; Non-intrusive method; Social distancing; Support vectors machine; Wifi; COVID-19
Multi-disease predictive analytics: A clinical knowledge-aware approach,2021,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122619777&doi=10.1145%2f3447942&partnerID=40&md5=0b469ba6b0a83a422f5b9326f4d1681d,"Multi-Disease Predictive Analytics (MDPA) models simultaneously predict the risks of multiple diseases in patients and are valuable in early diagnoses. Patients tend to have multiple diseases simultaneously or develop multiple complications over time, and MDPA models can learn and effectively utilize such correlations between diseases. Data from large-scale Electronic Health Records (EHR) can be used through Multi-Label Learning (MLL) methods to develop MDPA models. However, data-driven approaches for MDPA face the challenge of data imbalance, because rare diseases tend to have much less data than common diseases. Insufficient data for rare diseases makes it difficult to leverage correlations with other diseases. These correlations are studied and recorded in biomedical literature but are rarely utilized in predictive analytics. This article presents a novel method called Knowledge-Aware Approach (KAA) that learns clinical correlations from the rapidly growing body of clinical knowledge. KAA can be combined with any data-driven MLL model for MDPA to refine the predictions of the model. Our extensive experiments, on real EHR data, show that the use of KAA improves the predictive performance of commonly used MDPA models, particularly for rare diseases. KAA is also found to be superior to existing general approaches of combining clinical knowledge with data-driven models. Further, a counterfactual analysis shows the efficacy of KAA in improving physicians' ability to prescribe preventive treatments.  © 2021 Association for Computing Machinery.",Biomedical literature; Diagnosis prediction; Electronic health records; Knowledge graph; Multi-label learning; Rare diseases,Diagnosis; Diseases; Knowledge graph; Learning systems; Medical computing; Predictive analytics; Records management; Analytic modeling; Biomedical literature; Clinical knowledge; Diagnose prediction; Early diagnosis; Knowledge graphs; Large-scales; Learn+; Multi-label learning; Rare disease; Forecasting
Anonymization of daily activity data by using ℓ-diversity privacy model,2021,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122641949&doi=10.1145%2f3456876&partnerID=40&md5=6c2395d0868d4d2763dee76450bb5dde,"In the age of IoT, collection of activity data has become ubiquitous. Publishing activity data can be quite useful for various purposes such as estimating the level of assistance required by older adults and facilitating early diagnosis and treatment of certain diseases. However, publishing activity data comes with privacy risks: Each dimension, i.e., the activity of a person at any given point in time can be used to identify a person as well as to reveal sensitive information about the person such as not being at home at that time. Unfortunately, conventional anonymization methods have shortcomings when it comes to anonymizing activity data. Activity datasets considered for publication are often flat with many dimensions but typically not many rows, which makes the existing anonymization techniques either inapplicable due to very few rows, or else either inefficient or ineffective in preserving utility. This article proposes novel multi-level clustering-based approaches using a non-metric weighted distance measure that enforce ℓ-diversity model. Experimental results show that the proposed methods preserve data utility and are orders more efficient than the existing methods.  © 2021 Association for Computing Machinery.",Anonymization; Clustering; K-anonymity; Privacy; ℓ-diversity,Data privacy; Anonymization; Clusterings; Daily activity; Early diagnosis; K-Anonymity; Older adults; Privacy; Privacy models; Privacy risks; ℓ-diversity; Diagnosis
SymptomID: A Framework for Rapid Symptom Identification in Pandemics Using News Reports,2021,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123166935&doi=10.1145%2f3462441&partnerID=40&md5=ac8b1c349de695b62bb0c12a35808dc4,"The ability to quickly learn fundamentals about a new infectious disease, such as how it is transmitted, the incubation period, and related symptoms, is crucial in any novel pandemic. For instance, rapid identification of symptoms can enable interventions for dampening the spread of the disease. Traditionally, symptoms are learned from research publications associated with clinical studies. However, clinical studies are often slow and time intensive, and hence delays can have dire consequences in a rapidly spreading pandemic like we have seen with COVID-19. In this article, we introduce SymptomID, a modular artificial intelligence-based framework for rapid identification of symptoms associated with novel pandemics using publicly available news reports. SymptomID is built using the state-of-the-art natural language processing model (Bidirectional Encoder Representations for Transformers) to extract symptoms from publicly available news reports and cluster-related symptoms together to remove redundancy. Our proposed framework requires minimal training data, because it builds on a pre-trained language model. In this study, we present a case study of SymptomID using news articles about the current COVID-19 pandemic. Our COVID-19 symptom extraction module, trained on 225 articles, achieves an F1 score of over 0.8. SymptomID can correctly identify well-established symptoms (e.g., ""fever""and ""cough"") and less-prevalent symptoms (e.g., ""rashes,""""hair loss,""""brain fog"") associated with the novel coronavirus. We believe this framework can be extended and easily adapted in future pandemics to quickly learn relevant insights that are fundamental for understanding and combating a new infectious disease. © 2021 Association for Computing Machinery.",BERT; COVID-19; named entity extraction; news articles; novel pandemics; Symptom identification,Clinical research; Extraction; Natural language processing systems; BERT; Clinical study; Infectious disease; Learn+; Named entity extraction; News articles; News reports; Novel pandemic; Rapid identification; Symptom identification; COVID-19
An efficient deep learning paradigm for deceit identification test on EEG signals,2021,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122620479&doi=10.1145%2f3458791&partnerID=40&md5=5896e93a361e18cc82770d933540758c,"Brain-Computer Interface is the collaboration of the human brain and a device that controls the actions of a human using brain signals. Applications of brain-computer interface vary from the field of entertainment to medical. In this article, a novel Deceit Identification Test is proposed based on the Electroencephalogram signals to identify and analyze the human behavior. Deceit identification test is based on P300 signals, which have a positive peak from 300 ms to 1,000 ms of the stimulus onset. The aim of the experiment is to identify and classify P300 signals with good classification accuracy. For preprocessing, a band-pass filter is used to eliminate the artifacts. The feature extraction is carried out using ""symlet""Wavelet Packet Transform (WPT). Deep Neural Network (DNN) with two autoencoders having 10 hidden layers each is applied as the classifier. A novel experiment is conducted for the collection of EEG data from the subjects. EEG signals of 30 subjects (15 guilty and 15 innocent) are recorded and analyzed during the experiment. BrainVision recorder and analyzer are used for recording and analyzing EEG signals. The model is trained for 90% of the dataset and tested for 10% of the dataset and accuracy of 95% is obtained.  © 2021 Association for Computing Machinery.",Brain-computer interface; Deceit identification test; Deep neural network; Electroencephalogram; Wavelet packet transform,Bandpass filters; Behavioral research; Biomedical engineering; Biomedical signal processing; Brain computer interface; Deep neural networks; Multilayer neural networks; Packet networks; Wavelet analysis; Brain signals; Classification accuracy; Deceit identification test; EEG signals; Electroencephalogram signals; Features extraction; Human behaviors; Human brain; Learning paradigms; Wavelet packet transforms; Electroencephalography
Anonymous blockchain-based system for consortium,2021,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111450481&doi=10.1145%2f3459087&partnerID=40&md5=8cf833bf655230490721b6d1adb3f412,"Blockchain records transactions with various protection techniques against tampering. To meet the requirements on cooperation and anonymity of companies and organizations, researchers have developed a few solutions. Ring signature-based schemes allow multiple participants cooperatively to manage while preserving their individuals' privacy. However, the solutions cannot work properly due to the increased computing complexity along with the expanded group size. In this article, we propose a Multi-center Anonymous Blockchain-based (MAB) system, with joint management for the consortium and privacy protection for the participants. To achieve that, we formalize the syntax used by the MAB system and present a general construction based on a modular design. By applying cryptographic primitives to each module, we instantiate our scheme with anonymity and decentralization. Furthermore, we carry out a comprehensive formal analysis of our exemplified scheme. A proof of concept simulation is provided to show the feasibility. The results demonstrate security and efficiency from both theoretical perspectives and practical perspectives.  © 2021 Association for Computing Machinery.",Anonymity; Blockchain; Consortium; Multi-center,Anonymity; Block-chain; Computing complexity; Consortium; Group size; Individual privacy; Multi-center; Privacy protection; Protection techniques; Ring signatures; Blockchain
Graph convolutional network-based model for incident-related congestion prediction: A case study of shanghai expressways,2021,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120527321&doi=10.1145%2f3451356&partnerID=40&md5=36af0d012b65ce268e853c39a8c486db,"Traffic congestion has become a significant obstacle to the development of mega cities in China. Although local governments have used many resources in constructing road infrastructure, it is still insufficient for the increasing traffic demands. As a first step toward optimizing real-time traffic control, this study uses Shanghai Expressways as a case study to predict incident-related congestions. Our study proposes a graph convolutional network-based model to identify correlations in multi-dimensional sensor-detected data, while simultaneously taking into account environmental, spatiotemporal, and network features in predicting traffic conditions immediately after a traffic incident. The average accuracy, average AUC, and average F-1 score of the predictive model are 92.78%, 95.98%, and 88.78%, respectively, on small-scale ground-truth data. Furthermore, we improve the predictive model's performance using semi-supervised learning by including more unlabeled data instances. As a result, the accuracy, AUC, and F-1 score of the model increase by 2.69%, 1.25%, and 4.72%, respectively. The findings of this article have important implications that can be used to improve the management and development of Expressways in Shanghai, as well as other metropolitan areas in China.  © 2021 Association for Computing Machinery.",Expressway; Graph convolutional networks; Incident-related congestion; Predictive model; Self-training,Convolution; Convolutional neural networks; Forecasting; Graph neural networks; Motor transportation; Supervised learning; Case-studies; Congestion prediction; Convolutional networks; Expressway; Graph convolutional network; Incident-related congestion; Megacities; Network-based modeling; Predictive models; Self-training; Traffic congestion
Leveraging individual and collective regularity to profile and segment user locations from mobile phone data,2021,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116333154&doi=10.1145%2f3449042&partnerID=40&md5=c577528ca93ff0d9f04156e92ea959b6,"The dynamic monitoring of home and workplace distribution is a fundamental building block for improving location-based service systems in fast-developing cities worldwide. Inferring these places is challenging; existing approaches rely on labor-intensive and untimely survey data or ad hoc heuristic assignment rules based on the frequency of appearance at given locations. Motivated by the regularities in human behavior, we propose a novel method to infer the home, workplace, and third place based on an individual's spatial-temporal patterns inferred from Call Detail Records. To capture the individual regularity, our method develops, for each person-location, the probability distribution that the person will appear in that location at a specific time of day using geo-temporal travel patterns a panel of individuals. To reveal the collective regularity, we apply eigen-decomposition to the matrix of historical geo-temporal data. Unsupervised machine learning techniques are then used to extract commonalities across locations for different groups of travelers, making inferences, such as home and workplace. Testing the methodology on real-world data with known location labels shows that our method identifies home and workplace with significant accuracy, improving upon the best practices in the literature by 79% and 34%, respectively. The methodology proposed is computationally efficient and is highly scalable to other real-world applications with historical tracking data. It provides a basis to improve location-based services, such as mobile commerce, social events recommendations, and urban transit design.  © 2021 Association for Computing Machinery.",Human mobility; Origin-destination flow; Travel demand management; Unsupervised learning,Behavioral research; Encoding (symbols); Location based services; Machine learning; Mobile commerce; Probability distributions; Telecommunication services; User profile; Dynamic monitoring; Fundamental building blocks; Human mobility; Location-based services; Mobile phone datum; Origin-destination flows; Real-world; Service systems; Travel demand management; User location; Location
Optimizing Ontology Alignment through an Interactive Compact Genetic Algorithm,2021,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108908155&doi=10.1145%2f3439772&partnerID=40&md5=514704318e13f8666d7920c612ba4ae8,"Ontology provides a shared vocabulary of a domain by formally representing the meaning of its concepts, the properties they possess, and the relations among them, which is the state-of-the-art knowledge modeling technique. However, the ontologies in the same domain could differ in conceptual modeling and granularity level, which yields the ontology heterogeneity problem. To enable data and knowledge transfer, share, and reuse between two intelligent systems, it is important to bridge the semantic gap between the ontologies through the ontology matching technique. To optimize the ontology alignment's quality, this article proposes an Interactive Compact Genetic Algorithm (ICGA)-based ontology matching technique, which consists of an automatic ontology matching process based on a Compact Genetic Algorithm (CGA) and a collaborative user validating process based on an argumentation framework. First, CGA is used to automatically match the ontologies, and when it gets stuck in the local optima, the collaborative validation based on the multi-relationship argumentation framework is activated to help CGA jump out of the local optima. In addition, we construct a discrete optimization model to define the ontology matching problem and propose a hybrid similarity measure to calculate two concepts' similarity value. In the experiment, we test the performance of ICGA with the Ontology Alignment Evaluation Initiative's interactive track, and the experimental results show that ICGA can effectively determine the ontology alignments with high quality.  © 2021 ACM.",argumentation framework; collaborative validation; Interactive ontology matching,Alignment; Genetic algorithms; Intelligent systems; Knowledge management; Semantics; Argumentation frameworks; Automatic ontology matching; Automatically match; Collaborative users; Compact genetic algorithm; Compact genetic algorithm (cga); Discrete optimization; Ontology heterogeneities; Ontology
Exploiting Network Fusion for Organizational Turnover Prediction,2021,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108909377&doi=10.1145%2f3439770&partnerID=40&md5=fe74109bd31b0dc641eedc83f2a0139f,"As an emerging measure of proactive talent management, talent turnover prediction is critically important for companies to attract, engage, and retain talents in order to prevent the loss of intellectual capital. While tremendous efforts have been made in this direction, it is not clear how to model the influence of employees' turnover within multiple organizational social networks. In this article, we study how to exploit turnover contagion by developing a Turnover Influence-based Neural Network (TINN) for enhancing organizational turnover prediction. Specifically, TINN can construct the turnover similarity network which is then fused with multiple organizational social networks. The fusion is achieved either through learning a hidden turnover influence network or through integrating the turnover influence on multiple networks. Taking advantage of the Graph Convolutional Network and the Long Short-Term Memory network, TINN can dynamically model the impact of social influence on talent turnover. Meanwhile, the utilization of the attention mechanism improves the interpretability, providing insights into the impact of different networks along time on the future turnovers. Finally, we conduct extensive experiments in real-world settings to evaluate TINN. The results validate the effectiveness of our approach to enhancing organizational turnover prediction. Also, our case studies reveal some interpretable findings, such as the importance of each network or hidden state which potentially impacts future organizational turnovers.  © 2021 ACM.",network fusion; social influence; Talent management; turnover prediction,Economic and social effects; Forecasting; Knowledge management; Attention mechanisms; Convolutional networks; Influence networks; Intellectual capital; Multiple networks; Real world setting; Similarity network; Talent management; Convolutional neural networks
Will Catastrophic Cyber-Risk Aggregation Thrive in the IoT Age? A Cautionary Economics Tale for (Re-)Insurers and Likes,2021,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108904618&doi=10.1145%2f3446635&partnerID=40&md5=98490d791a49b21024e4f0e664fc971f,"Service liability interconnections among networked IT and IoT-driven service organizations create potential channels for cascading service disruptions due to modern cybercrimes such as DDoS, APT, and ransomware attacks. These attacks are known to inflict cascading catastrophic service disruptions worth billions of dollars across organizations and critical infrastructure around the globe. Cyber-insurance is a risk management mechanism that is gaining increasing industry popularity to cover client (organization) risks after a cyber-attack. However, there is a certain likelihood that the nature of a successful attack is of such magnitude that an organizational client's insurance provider is not able to cover the multi-party aggregate losses incurred upon itself by its clients and their descendants in the supply chain, thereby needing to re-insure itself via other cyber-insurance firms. To this end, one question worth investigating in the first place is whether an ecosystem comprising a set of profit-minded cyber-insurance companies, each capable of providing re-insurance services for a service-networked IT environment, is economically feasible to cover the aggregate cyber-losses arising due to a cyber-attack. Our study focuses on an empirically interesting case of extreme heavy tailed cyber-risk distributions that might be presenting themselves to cyber-insurance firms in the modern Internet age in the form of catastrophic service disruptions, and could be a possible standard risk distribution to deal with in the near IoT age. Surprisingly, as a negative result for society in the event of such catastrophes, we prove via a game-theoretic analysis that it may not be economically incentive compatible, even under i.i.d. statistical conditions on catastrophic cyber-risk distributions, for limited liability-taking risk-averse cyber-insurance companies to offer cyber re-insurance solutions despite the existence of large enough market capacity to achieve full cyber-risk sharing. However, our analysis theoretically endorses the popular opinion that spreading i.i.d. cyber-risks that are not catastrophic is an effective practice for aggregate cyber-risk managers, a result established theoretically and empirically in the past. A failure to achieve a working re-insurance market in critically demanding situations after catastrophic cyber-risk events strongly calls for centralized government regulatory action/intervention to promote risk sharing through re-insurance activities for the benefit of service-networked societies in the IoT age.  © 2021 ACM.",Cyber-risk; cyber-risk aggregation; extreme heavy-tail; re-insurance,Aggregates; Commerce; Crime; Game theory; Insurance; Internet of things; Network security; Risk assessment; Risk management; Service industry; Supply chains; Effective practices; Game theoretic analysis; Incentive compatible; Insurance companies; Insurance providers; Management mechanisms; Service disruptions; Service organizations; Malware
Assessing the Moderating Effect of Security Technologies on Employees Compliance with Cybersecurity Control Procedures,2021,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108366804&doi=10.1145%2f3424282&partnerID=40&md5=fa0580349385b5cf37b31fda4fe4afac,"The increase in cybersecurity threats and the challenges for organisations to protect their information technology assets has made adherence to organisational security control processes and procedures a critical issue that needs to be adequately addressed. Drawing insight from organisational theory literature, we develop a multi-theory model, combining the elements of the theory of planned behaviour, competing value framework, and technology - organisational and environmental theory to examine how the organisational mechanisms interact with espoused cultural values and employee cognitive belief to influence cybersecurity control procedures. Using a structured questionnaire, we deployed structural equation modelling (SEM) to analyse the survey data obtained from public sector information technology organisations in Nigeria to test the hypothesis on the relationship of socio-organisational mechanisms and techno-cultural factors with other key determinants of employee security behaviour. The results showed that knowledge of cybersecurity and employee cognitive belief significantly influence the employees' intentions to comply with organisational cybersecurity control mechanisms. The research further noted that the influence of organisational elements such as leadership on employee security behaviour is mediated by espoused cultural values while the impact of employee cognitive belief is moderated by security technologies. For effective cybersecurity compliance, leaders and policymakers are therefore to promote organisational security initiatives that ensure incorporation of cybersecurity principles and practices into job descriptions, routines, and processes. This study contributes to behavioural security research by highlighting the critical role of leadership and cultural values in fostering organisational adherence to prescribed security control mechanisms.  © 2021 ACM.",compliance; Cybersecurity; organisational culture; structural equation modelling,Compliance control; Employment; Environmental technology; Job analysis; Personnel; Process control; Security of data; Surveys; Cognitive beliefs; Control procedures; Principles and practices; Public sector information; Security controls; Security technology; Structural equation modelling (SEM); Theory of planned behaviour; Behavioral research
Using Word Embeddings to Deter Intellectual Property Theft through Automated Generation of Fake Documents,2021,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106980658&doi=10.1145%2f3418289&partnerID=40&md5=db1a063b199aa9587d7e0fddfdc4085d,"Theft of intellectual property is a growing problem - one that is exacerbated by the fact that a successful compromise of an enterprise might only become known months after the hack. A recent solution called FORGE addresses this problem by automatically generating N ""fake""versions of any real document so that the attacker has to determine which of the N + 1 documents that they have exfiltrated from a compromised network is real. In this article, we remove two major drawbacks in FORGE: (i) FORGE requires ontologies in order to generate fake documents - however, in the real world, ontologies, especially good ontologies, are infrequently available. The WE-FORGE system proposed in this article completely eliminates the need for ontologies by using distance metrics on word embeddings instead. (ii) FORGE generates fake documents by first identifying ""target""concepts in the original document and then substituting ""replacement""concepts for them. However, we will show that this can lead to sub-optimal results (e.g., as target concepts are selected without knowing the availability and/or quality of the replacement concepts, they can sometimes lead to poor results). Our WE-FORGE system addresses this problem in two possible ways by performing a joint optimization to select concepts and replacements simultaneously. We conduct a human study involving both computer science and chemistry documents and show that WE-FORGE successfully deceives adversaries.  © 2021 ACM.",AI security; fake document generation,Crime; Embeddings; Intellectual property; Automated generation; Distance metrics; Human study; Joint optimization; Optimal results; Real-world; Target concept; Ontology
Exploring Decomposition for Solving Pattern Mining Problems,2021,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105696779&doi=10.1145%2f3439771&partnerID=40&md5=dde7e03252e4dba1d65f7096e19449db,"This article introduces a highly efficient pattern mining technique called Clustering-based Pattern Mining (CBPM). This technique discovers relevant patterns by studying the correlation between transactions in the transaction database based on clustering techniques. The set of transactions is first clustered, such that highly correlated transactions are grouped together. Next, we derive the relevant patterns by applying a pattern mining algorithm to each cluster. We present two different pattern mining algorithms, one applying an approximation-based strategy and another based on an exact strategy. The approximation-based strategy takes into account only the clusters, whereas the exact strategy takes into account both clusters and shared items between clusters. To boost the performance of the CBPM, a GPU-based implementation is investigated. To evaluate the CBPM framework, we perform extensive experiments on several pattern mining problems. The results from the experimental evaluation show that the CBPM provides a reduction in both the runtime and memory usage. Also, CBPM based on the approximate strategy provides good accuracy, demonstrating its effectiveness and feasibility. Our GPU implementation achieves significant speedup of up to 552× on a single GPU using big transaction databases.  © 2021 ACM.",decomposition; GPU; Pattern mining; scalability,Approximation algorithms; Clustering techniques; Experimental evaluation; GPU implementation; Highly-correlated; Pattern mining algorithms; Relevant patterns; Runtime and memory usage; Transaction database; Data mining
Machine Learning for Identifying Group Trajectory Outliers,2021,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106310427&doi=10.1145%2f3430195&partnerID=40&md5=b22291dc8d74a00812b60ad6a6fd8e23,"Prior works on the trajectory outlier detection problem solely consider individual outliers. However, in real-world scenarios, trajectory outliers can often appear in groups, e.g., a group of bikes that deviates to the usual trajectory due to the maintenance of streets in the context of intelligent transportation. The current paper considers the Group Trajectory Outlier (GTO) problem and proposes three algorithms. The first and the second algorithms are extensions of the well-known DBSCAN and kNN algorithms, while the third one models the GTO problem as a feature selection problem. Furthermore, two different enhancements for the proposed algorithms are proposed. The first one is based on ensemble learning and computational intelligence, which allows for merging algorithms' outputs to possibly improve the final result. The second is a general high-performance computing framework that deals with big trajectory databases, which we used for a GPU-based implementation. Experimental results on different real trajectory databases show the scalability of the proposed approaches.  © 2021 ACM.",data mining; Group trajectory outliers; machine learning,Intelligent computing; Machine learning; Statistics; Trajectories; Ensemble learning; Feature selection problem; High performance computing; Intelligent transportation; Merging algorithms; Real trajectories; Real-world scenario; Trajectory database; Learning algorithms
A Latent Space Modeling Approach to Interfirm Relationship Analysis,2021,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102685385&doi=10.1145%2f3424240&partnerID=40&md5=e73d59547379a040ea127549c01e52bc,"Interfirm relationships are crucial to our understanding of firms' collective and interactive behavior. Many information systems-related phenomena, including the diffusion of innovations, standard alliances, technology collaboration, and outsourcing, involve a multitude of relationships between firms. This study proposes a latent space approach to model temporal change in a dual-view interfirm network. We assume that interfirm relationships depend on an underlying latent space; firms that are close to each other in the latent space are more likely to develop a relationship. We construct the latent space by embedding two dynamic networks of firms in an integrated manner, resulting in a more comprehensive view of an interfirm relationship. We validate our approach by introducing three business measures derived from the latent space model to study alliance formation and stock comovement. We illustrate how the trajectories of firms provide insights into alliance activities. We also show that our proposed measures have strong predictive power on stock comovement. We believe the proposed approach enriches the methodology toolbox of IS researchers in studying interfirm relationships.  © 2021 ACM.",dynamic network; Interfirm relationship; latent space; Markov chain Monte Carlo; multi-view network; network modeling,Information systems; Alliance formation; Diffusion of innovations; Inter-firm relationships; Interactive behavior; Interfirm network; Latent space models; Predictive power; Temporal change; Technology transfer
The Effect of the GDPR on Privacy Policies,2021,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102979515&doi=10.1145%2f3389685&partnerID=40&md5=d47514ce29e28a2ee1a43d5d4a3d858d,"The General Data Protection Regulation (GDPR) is considered by some to be the most important change in data privacy regulation in 20 years. Effective May 2018, the European Union GDPR privacy law applies to any organization that collects and processes the personal information of EU citizens within or outside the EU. In this work, we seek to quantify the progress the GDPR has made in improving privacy policies around the globe. We leverage our data mining tool, PrivacyCheck, to automatically compare three corpora (totaling 550) of privacy policies, pre- A nd post-GDPR. In addition, to evaluate the current level of compliance with the GDPR around the globe, we manually studied the policies within two corpora (450 policies). We find that the GDPR has made progress in protecting user data, but more progress is necessary-particularly in the area of giving users the right to edit and delete their information-to entirely fulfill the GDPR's promise. We also observe that the GDPR encourages sharing user data with law enforcement, and as a result, many policies have facilitated such sharing after the GDPR. Finally, we see that when there is non-compliance with the GDPR, it is often in the form of failing to explicitly indicate compliance, which in turn speaks to an organization's lack of transparency and disclosure regarding their processing and protection of personal information. If Personally Identifiable Information (PII) is the ""currency of the Internet,""these findings mark continued alarm regarding an individual's agency to protect and secure their PII assets. © 2020 ACM.",GDPR; policy; Privacy; PrivacyCheck,Data mining; Laws and legislation; Current levels; Data-mining tools; European union; General data protection regulations; Personal information; Personally identifiable information; Privacy policies; Privacy regulation; Data privacy
Detecting High-Engaging Breaking News Rumors in Social Media,2021,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102983454&doi=10.1145%2f3416703&partnerID=40&md5=bafbda72b0e1649f6f2e9abde2c09264,"Users from all over the world increasingly adopt social media for newsgathering, especially during breaking news. Breaking news is an unexpected event that is currently developing. Early stages of breaking news are usually associated with lots of unverified information, i.e., rumors. Efficiently detecting and acting upon rumors in a timely fashion is of high importance to minimize their harmful effects. Yet, not all rumors have the potential to spread in social media. High-engaging rumors are those written in a manner that ensures achievement of the highest prevalence among the recipients. They are difficult to detect, spread very fast, and can cause serious damage to society. In this article, we propose a new multi-task Convolutional Neural Network (CNN) attention-based neural network architecture to jointly learn the two tasks of breaking news rumors detection and breaking news rumors popularity prediction in social media. The proposed model learns the salient semantic similarities among important features for detecting high-engaging breaking news rumors and separates them from the rest of the input text. Extensive experiments on five real-life datasets of breaking news suggest that our proposed model outperforms all baselines and is capable of detecting breaking news rumors and predicting their future popularity with high accuracy. © 2020 ACM.",breaking news; deep learning; rumor detection; Social media,Convolutional neural networks; Network architecture; Semantics; Harmful effects; High-accuracy; Important features; Popularity predictions; Real life datasets; Semantic similarity; Social media; Unexpected events; Social networking (online)
Design of an Inclusive Financial Privacy Index (INF-PIE): A Financial Privacy and Digital Financial Inclusion Perspective,2021,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102980745&doi=10.1145%2f3403949&partnerID=40&md5=96e848d64e08b1d6ffe053d3506b754d,"Financial privacy is an important part of an individual's privacy, but efforts to enhance financial privacy have often not been given enough prominence by some countries when advancing financial inclusion. This impedes under-served communities from utilizing financial services. This article adopts a design science approach to create an INclusive Financial Privacy IndEx (INF-PIE) from the two perspectives of financial privacy and digital financial inclusion to help ensure financial services for a wide range of populations. This article first examines the privacy policies of Mobile Wallet and Remittance (MWR) apps (a digital financial solution), uses an analytics approach for extracting semi-structured information components; and based on text categorization and topic modeling, creates privacy policy compliance scores. In particular, it analyses the privacy policies using natural language processing techniques such as Term Frequency-Inverse Document Frequency (tf-idf) and Latent Dirichlet Allocation (LDA). This article then develops a digital financial inclusion score through a multivariate analysis of indexes extracted from the global findex dataset using Principal Component Analysis (PCA). Finally, the INF-PIE framework is established to analyze various countries and assess their financial privacy and digital financial inclusion practices. This framework can show how countries' relative data privacy compliance and digital financial inclusion practices underscore their inclusive financial privacy. © 2020 ACM.",compliance score; Digital financial inclusion; digital payments systems; GDPR; inclusive privacy index; mobile wallet and remittance,Multivariant analysis; Natural language processing systems; Privacy by design; Statistics; Text processing; Financial inclusions; Latent dirichlet allocations; Multi variate analysis; NAtural language processing; Privacy compliance; Semi-structured information; Term frequencyinverse document frequency (TF-IDF); Text categorization; Finance
Mathematical Reconciliation of Medical Privacy Policies,2021,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102979493&doi=10.1145%2f3397520&partnerID=40&md5=66db9a3be937440c18abf17f29f04ba0,"Healthcare data are arguably the most private of personal data. This very private information in the wrong hands can lead to identity theft, prescription fraud, insurance fraud, and an array of other crimes. Electronic-health systems such as My Health Record in Australia holds great promise in sharing medical data and improving healthcare quality. But, a key privacy issue in these systems is the misuse of healthcare data by ""authorities.""The recent General Data Protection Regulation (GDPR) introduced in the EU aims to reduce personal-data misuse. But, there are no tools currently available to accurately reconcile a domestic E-health policy against the GDPR to identify discrepancies. Reconciling privacy policies is also non-trivial, because policies are often written in free text, making them subject to human interpretation. In this article, we propose a tool that allows the description of E-health privacy policies, represents them using formal constructs making the policies precise and explicit. Using this formal framework, our tool can automatically reconcile a domestic E-health policy against the GDPR to identify violations and omissions. We use our prototype to illustrate several critical flaws in Australia's My Health Record policy, including a non-compliance with GDPR that allows healthcare providers to access medical records by default. © 2020 ACM.",GDPR; metagraph; my health record; privacy policy,Crime; Data privacy; Health care; Insurance; Electronic health system; Formal framework; General data protection regulations; Health care providers; Healthcare quality; Insurance frauds; Prescription frauds; Private information; Electronic document exchange
Deep Learning for Multi-instance Biometric Privacy,2021,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102981492&doi=10.1145%2f3389683&partnerID=40&md5=b9748324331b1f1be1b2d4feb444d33b,"The fundamental goal of a revocable biometric system is to defend a user's biometrics from being compromised. This research explores the application of deep learning or Convolutional Neural Networks to multi-instance biometrics. Modality features are transformed into revocable templates through the application of random projection. During the user authentication phase, we employ Support Vector Machines, chosen over three other alternative classifiers after carrying out a comparative study. Comparison of the proposed method over other standard deep learning models and performance evaluation before and after revocability have also been discussed. Results demonstrate ability to improve identification accuracy and provide sound template security. The system was validated on three multi-instance iris and fingervein databases. © 2020 ACM.",biometric template security; Convolutional neural networks; machine learning; multi-instance revocable biometrics; random projection,Authentication; Biometrics; Convolutional neural networks; Support vector machines; Biometric systems; Comparative studies; Finger-vein; Identification accuracy; Learning models; Random projections; Template securities; User authentication; Deep learning
"A Multi-Disciplinary Perspective for Conducting Artificial Intelligence-enabled Privacy Analytics: Connecting Data, Algorithms, and Systems",2021,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102982627&doi=10.1145%2f3447507&partnerID=40&md5=dcc637f5de3bf95170bc4cf67ceddac7,"Events such as Facebook-Cambridge Analytica scandal and data aggregation efforts by technology providers have illustrated how fragile modern society is to privacy violations. Internationally recognized entities such as the National Science Foundation (NSF) have indicated that Artificial Intelligence (AI)-enabled models, artifacts, and systems can efficiently and effectively sift through large quantities of data from legal documents, social media, Dark Web sites, and other sources to curb privacy violations. Yet considerable efforts are still required for understanding prevailing data sources, systematically developing AI-enabled privacy analytics to tackle emerging challenges, and deploying systems to address critical privacy needs. To this end, we provide an overview of prevailing data sources that can support AI-enabled privacy analytics; a multi-disciplinary research framework that connects data, algorithms, and systems to tackle emerging AI-enabled privacy analytics challenges such as entity resolution, privacy assistance systems, privacy risk modeling, and more; a summary of selected funding sources to support high-impact privacy analytics research; and an overview of prevailing conference and journal venues that can be leveraged to share and archive privacy analytics research. We conclude this paper with an introduction of the papers included in this special issue. © 2021 ACM.",analytics; artificial intelligence; data; Privacy; systems; theories,Advanced Analytics; Privacy by design; Social networking (online); Assistance system; Data aggregation; Disciplinary perspective; Entity resolutions; Multi-disciplinary research; National Science Foundations; Privacy violation; Technology providers; Artificial intelligence
An Analysis of Complexity of Insider Attacks to Databases,2021,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102982042&doi=10.1145%2f3391231&partnerID=40&md5=ee0a6f3265b3615cf4e0722d758f84aa,"Insider attacks are one of the most dangerous threats to an organization. Unfortunately, they are very difficult to foresee, detect, and defend against due to the trust and responsibilities placed on the employees. In this article, we first define the notion of user intent and construct a model for a common scenario that poses a very high risk for sensitive data stored in the organization's database. We show that the complexity of identifying pseudo-intents of a user in this scenario is coNP-Complete, and launching a harvester insider attack within the boundaries of the defined threat model takes linear time while a targeted threat model is an NP-Complete problem. We also discuss the general defense mechanisms against the modeled threats and show that countering the harvester insider attack takes quadratic time while countering the targeted insider attack can take linear to quadratic time, depending on the strategy chosen. We analyze the adversarial behavior and show that launching an attack with minimum risk is also an NP-Complete problem. Finally, we perform timing experiments with the defense mechanisms on SQL query workloads collected from a national bank to test the feasibility of using these systems in real time. © 2020 ACM.",Complexity analysis; insider threat; query intent; query logs; threat modeling,Data privacy; Database systems; Harvesters; Network security; NP-hard; Query processing; Risk perception; Defense mechanism; Defined threats; Insider attack; Linear time; Minimum risks; Quadratic time; Sensitive datas; Threat modeling; Real time systems
Extraction of Information Content Exchange in Financial Markets by an Entropy Analysis,2021,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102983656&doi=10.1145%2f3419372&partnerID=40&md5=743253156c3028a6d36f3a5af8f4ffdc,"Recently, there has been an explosive interest in the literature about modeling and forecasting volatility in financial markets. Many researches have focused on energy markets and oil volatility index (OVX). In this article, we aim first at showing if there is an exchange of information between two stock time series, and then at evaluating what is the direction of this information flow. In particular, we propose an entropy-based approach that exploits two objective metrics, namely Mutual Information (MI) and Transfer Entropy (TE), that does not require a parametric model and is directly applicable on the data. The experimental outcomes, applied on Brent and WTI crude oil prices and their volatility index for the period from May 10, 2007 till July 03, 2018, demonstrate the effectiveness of the proposed method. © 2020 ACM.",entropy-based analysis; Extraction of information; financial markets; mutual information; volatility,Commerce; Entropy; Entropy based approach; Exchange of information; Extraction of information; Information flows; Modeling and forecasting; Mutual informations; Parametric modeling; Stock time series; Crude oil price
Optimal Employee Recruitment in Organizations under Attribute-Based Access Control,2021,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102979969&doi=10.1145%2f3403950&partnerID=40&md5=f5069881a005fd611395cd629b56f7cc,"For any successful business endeavor, recruitment of a required number of appropriately qualified employees in proper positions is a key requirement. For effective utilization of human resources, reorganization of such workforce assignment is also a task of utmost importance. This includes situations when the under-performing employees have to be substituted with fresh applicants. Generally, the number of candidates applying for a position is large, and hence, the task of identifying an optimal subset becomes critical. Moreover, a human resource manager would also like to make use of the opportunity of retirement of employees to improve manpower utilization. However, the constraints enforced by the security policies prohibit any arbitrary assignment of tasks to employees. Further, the new employees should have the capabilities required to handle the assigned tasks. In this article, we formalize this problem as the Optimal Recruitment Problem (ORP), wherein the goal is to select the minimum number of fresh employees from a set of candidates to fill the vacant positions created by the outgoing employees, while ensuring satisfiability of the specified security conditions. The model used for specification of authorization policies and constraints is Attribute-Based Access Control (ABAC), since it is considered to be the de facto next-generation framework for handling organizational security policies. We show that the ORP problem is NP-hard and propose a greedy heuristic for solving it. Extensive experimental evaluation shows both the effectiveness and efficiency of the proposed solution. © 2021 ACM.",graph coloring; greedy algorithm; Role-based access control (RBAC); statically mutually exclusive roles (SMER) constraint,Access control; Human resource management; NP-hard; Optimization; Security systems; Attribute based access control; Authorization policy; Effectiveness and efficiencies; Experimental evaluation; Greedy heuristics; Optimal subsets; Resource managers; Security policy; Employment
Time-based Gap Analysis of Cybersecurity Trends in Academic and Digital Media,2020,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097344471&doi=10.1145%2f3389684&partnerID=40&md5=5146d7e4ef9238a3b0e5c7877d10b575,"This study analyzes cybersecurity trends and proposes a conceptual framework to identify cybersecurity topics of social interest and emerging topics that need to be addressed by researchers in the field. The insights drawn from this framework allow for a more proactive approach to identifying cybersecurity patterns and emerging threats that will ultimately improve the collective cybersecurity posture of the modern society. To achieve this, cybersecurity-oriented content in both media and academic corpora, disseminated between 2008 and 2018, were morphologically analyzed via text mining. A total of 3,556 academic papers obtained from the top-10 highly reputable cybersecurity academic conferences, and 4,163 news articles collected from the New York Times were processed. The LDA topic modeling followed optimal perplexity and coherence scores resulted in 12 trendy topics. Next, the time-based gap between these trendy topics was analyzed to measure the correlation between media and trendy academic topics. Both convergences and divergences between the two cybersecurity corpora were identified, suggesting a strong time-based correlation between these resources. This framework demonstrates the effective use of automated techniques to provide insights about cybersecurity topics of social interest and emerging trends and informs the direction of future academic research in this field.  © 2020 ACM.",academic context; Cybersecurity trends; digital media; topic modeling; trend analysis,Digital storage; Text mining; Academic conferences; Academic research; Automated techniques; Conceptual frameworks; Emerging topics; Emerging trends; Pro-active approach; Topic Modeling; Security of data
IP Reputation Scoring with Geo-Contextual Feature Augmentation,2020,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097342177&doi=10.1145%2f3419373&partnerID=40&md5=aa603d8b7678c4fbb6e70485604c9a8d,"The focus of this article is to present an effective anomaly detection model for an encrypted network session by developing a novel IP reputation scoring model that labels the incoming session IP address based on the most similar IP addresses in terms of both network and geo-contextual knowledge. We provide empirical evidence that considering not only traditional network information but also geo-contextual information provides better threat assessment. The reputation scores provide a means to quantitatively capture good and bad IP behavior, making our model ideal for detecting malicious network behavior. With network encryption being the most practical solution to data security and privacy today, our approach expands the network administrator's ability to make decisions about IP addresses' trustworthiness in an encrypted session with limited network information.  © 2020 ACM.",Clustering; cybersecurity; geo-contextual; heterogeneous features; IP reputation scoring,Anomaly detection; Cryptography; Network security; Anomaly detection models; Contextual feature; Contextual information; Contextual knowledge; Data security and privacy; Network administrator; Network information; Practical solutions; Internet protocols
Analysis of Cyber Incident Categories Based on Losses,2020,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097342404&doi=10.1145%2f3418288&partnerID=40&md5=82e7cf8a2d813702ebe2d0c19b2d00a1,"The fact that ""cyber risk""is indeed a collective term for various distinct risks creates great difficulty in communications. For example, policyholders of ""cyber insurance""contracts often have a limited or inaccurate understanding about the coverage that they have. To address this issue, we propose a cyber risk categorization method using clustering techniques. This method classifies cyber incidents based on their consequential losses for insurance and risk management purposes. As a result, it also reveals the relationship between the causes and the outcomes of incidents. Our results show that similar cyber incidents, which are often not properly distinguished, can lead to very different losses. We hope that our work can clarify the differences between cyber risks and provide a set of risk categories that is feasible in practice and for future studies.  © 2020 ACM.",cyber insurance; cyber losses; Cyber risk,Information systems; Clustering techniques; Risk categories; Risk categorization; Risk management
An Ensemble of Ensembles Approach to Author Attribution for Internet Relay Chat Forensics,2020,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097341835&doi=10.1145%2f3409455&partnerID=40&md5=1e85503257af2f471a042468895337f3,"With the advances in Internet technologies and services, social media has been gained extreme popularity, especially because these technologies provide potential anonymity, which in turn harbors hacker discussion forums, underground markets, dark web, and so on. Internet relay chat (IRC) is a real-time communication protocol actively used by cybercriminals for hacking, cracking, and carding. Hence, it is particularly urgent to identify the authors of threat messages and malicious activities in IRC. Unfortunately, author identification studies in IRC remain as an underexplored area. In this research, we perform novel IRC text feature extraction methods and propose the first author attribution version of the deep forest (DF) model that is an ensemble of ensembles that utilizes the fusion of ensemble learning techniques. Our approach is supported by autonomic IRC monitoring. Experiments show that our approach is highly effective for author attribution and attains high accuracy even when the number of candidates is large while training data is limited.  © 2020 ACM.",Author attribution; cybersecurity; ensemble learning; internet relay chat (IRC); social network analysis,Computer crime; Forensic engineering; Learning systems; Personal computing; Textile industry; Author identification; Cybercriminals; Discussion forum; Ensemble learning; Internet relay chat; Internet technology; Malicious activities; Real-time communication; Relay control systems
Edge-Based Intrusion Detection for IoT devices,2020,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097349201&doi=10.1145%2f3382159&partnerID=40&md5=540f50074eb57bd376b737659401cba5,"As the Internet of Things (IoT) is estimated to grow to 25 billion by 2021, there is a need for an effective and efficient Intrusion Detection System (IDS) for IoT devices. Traditional network-based IDSs are unable to efficiently detect IoT malware and new evolving forms of attacks like file-less attacks. In this article, we present a system level Device-Edge split IDS for IoT devices. Our IDS profiles IoT devices according to their ""behavior""using system-level information like running process parameters and their system calls in an autonomous, efficient, and scalable manner and then detects anomalous behavior indicative of intrusions. The modular design of our IDS along with a unique device-edge split architecture allows for effective attack detection with minimal overhead on the IoT devices. We have extensively evaluated our system using a dataset of 3,973 traditional IoT malware samples and 8 types of sophisticated file-less attacks recently observed against IoT devices in our testbed. We report the evaluation results in terms of detection efficiency and computational.  © 2020 ACM.",AI; edge; Intrusion detection; IoT security; malware,Computational efficiency; Intrusion detection; Malware; Network security; Anomalous behavior; Attack detection; Detection efficiency; Evaluation results; Internet of thing (IOT); Intrusion Detection Systems; Modular designs; Split architectures; Internet of things
Predictive Cyber Situational Awareness and Personalized Blacklisting,2020,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096850466&doi=10.1145%2f3386250&partnerID=40&md5=cbc84dea27fdf41f7493b4ecbd6422ee,"Cybersecurity adopts data mining for its ability to extract concealed and indistinct patterns in the data, such as for the needs of alert correlation. Inferring common attack patterns and rules from the alerts helps in understanding the threat landscape for the defenders and allows for the realization of cyber situational awareness, including the projection of ongoing attacks. In this article, we explore the use of data mining, namely sequential rule mining, in the analysis of intrusion detection alerts. We employed a dataset of 12 million alerts from 34 intrusion detection systems in 3 organizations gathered in an alert sharing platform, and processed it using our analytical framework. We execute the mining of sequential rules that we use to predict security events, which we utilize to create a predictive blacklist. Thus, the recipients of the data from the sharing platform will receive only a small number of alerts of events that are likely to occur instead of a large number of alerts of past events. The predictive blacklist has the size of only 3% of the raw data, and more than 60% of its entries are shown to be successful in performing accurate predictions in operational, real-world settings.  © 2020 ACM.",attack prediction; Data mining; intrusion detection; situational awareness,Intrusion detection; Accurate prediction; Alert correlation; Attack patterns; Intrusion Detection Systems; Real world setting; Security events; Sharing platforms; Situational awareness; Data mining
Internet-scale Insecurity of Consumer Internet of Things,2020,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097276869&doi=10.1145%2f3394504&partnerID=40&md5=d7391182b548989595e61763983e5d42,"The number of Internet-of-Things (IoT) devices actively communicating across the Internet is continually increasing, as these devices are deployed across a variety of sectors, constantly transferring private data across the Internet. Due to the extensive deployment of such devices, the continuous discovery and persistence of IoT-centric vulnerabilities in protocols, applications, hardware, and the improper management of such IoT devices has resulted in the rampant, uncontrolled spread of malware threatening consumer IoT devices. To this end, this work adopts a novel, macroscopic methodology for fingerprinting Internet-scale compromised IoT devices, revealing crucial cyber threat intelligence on the insecurity of consumer IoT devices. By developing data-driven techniques rooted in machine learning methods and analyzing 3.6 TB of network traffic data, we discover 855,916 compromised IP addresses, with 310,164 fingerprinted as IoT. Further analysis reveals China and Brazil to be hosting the most significant population of compromised IoT devices (100,000 and 55,000, respectively). Additionally, we provide a longitudinal analysis on data from one year ago against this work, revealing the evolving trends of IoT exploitation, such as the increased number of vendors targeted by malware, rising from 50 to 131. Moreover, countries such as China (420% increased infected IoT count) and Indonesia (177% increased infected IoT count) have seen notably high increases in infection rates. Last, we compare our geographic results against Global Cybersecurity Index (GCI) ratings, verifying that countries with high GCI ratings, such as the Netherlands and Germany, had relatively low infection rates. However, upon further inspection, we find that the GCI rate does not accurately represent the consumer IoT market, with countries such as China and Russia being rated with ""high""CGI scores, yet hosting a large population of infected consumer IoT devices.  © 2020 ACM.",data science; Internet-of-Things; IoT forensics; IoT security,Learning systems; Malware; Cyber security; Data driven technique; Infection rates; Internet of Things (IOT); Large population; Longitudinal analysis; Machine learning methods; Network traffic; Internet of things
Trailblazing the Artificial Intelligence for Cybersecurity Discipline: A Multi-Disciplinary Research Roadmap,2020,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097331327&doi=10.1145%2f3430360&partnerID=40&md5=4bec1bd7f6eea489d68f5da6dba45ca4,"Cybersecurity has rapidly emerged as a grand societal challenge of the 21st century. Innovative solutions to proactively tackle emerging cybersecurity challenges are essential to ensuring a safe and secure society. Artificial Intelligence (AI) has rapidly emerged as a viable approach for sifting through terabytes of heterogeneous cybersecurity data to execute fundamental cybersecurity tasks, such as asset prioritization, control allocation, vulnerability management, and threat detection, with unprecedented efficiency and effectiveness. Despite its initial promise, AI and cybersecurity have been traditionally siloed disciplines that relied on disparate knowledge and methodologies. Consequently, the AI for Cybersecurity discipline is in its nascency. In this article, we aim to provide an important step to progress the AI for Cybersecurity discipline. We first provide an overview of prevailing cybersecurity data, summarize extant AI for Cybersecurity application areas, and identify key limitations in the prevailing landscape. Based on these key issues, we offer a multi-disciplinary AI for Cybersecurity roadmap that centers on major themes such as cybersecurity applications and data, advanced AI methodologies for cybersecurity, and AI-enabled decision making. To help scholars and practitioners make significant headway in tackling these grand AI for Cybersecurity issues, we summarize promising funding mechanisms from the National Science Foundation (NSF) that can support long-term, systematic research programs. We conclude this article with an introduction of the articles included in this special issue.  © 2020 ACM.",adversarial machine learning; analytics; artificial intelligence; cyber threat intelligence; Cybersecurity; disinformation; security operations centers,Decision making; Security of data; Control allocation; Funding mechanisms; Innovative solutions; Multi-disciplinary research; National Science Foundations; Systematic research; Threat detection; Vulnerability management; Artificial intelligence
On the Variety and Veracity of Cyber Intrusion Alerts Synthesized by Generative Adversarial Networks,2020,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097328703&doi=10.1145%2f3394503&partnerID=40&md5=17ae8af89c7d6ffdee164aee067f3b84,"Many cyber attack actions can be observed, but the observables often exhibit intricate feature dependencies, non-homogeneity, and potentially rare yet critical samples. This work tests the ability to learn, model, and synthesize cyber intrusion alerts through Generative Adversarial Networks (GANs), which explore the feature space by reconciling between randomly generated samples and data that reflect a mixture of diverse attack behaviors without a priori knowledge. Through a comprehensive analysis using Jensen-Shannon Divergence, Conditional and Joint Entropy, and mode drops and additions, we show that the Wasserstein-GAN with Gradient Penalty and Mutual Information is more effective in learning to generate realistic alerts than models without Mutual Information constraints. We further show that the added Mutual Information constraint pushes the model to explore the feature space more thoroughly and increases the generation of low probability, yet critical, alert features. This research demonstrates the novel and promising application of unsupervised GANs to learn from limited yet diverse intrusion alerts to generate synthetic alerts that emulate critical dependencies, opening the door to proactive, data-driven cyber threat analyses.  © 2020 ACM.",cyberattack characterization; GAN; intrusion alert analysis,Information systems; Adversarial networks; Comprehensive analysis; Cyber intrusion; Intrusion alerts; Jensen-Shannon divergence; Low probability; Mutual informations; Priori knowledge; Security of data
PANDA: Partitioned Data Security on Outsourced Sensitive and Non-sensitive Data,2020,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097336161&doi=10.1145%2f3397521&partnerID=40&md5=cf35091a1c914704796e00353e6d5c57,"Despite extensive research on cryptography, secure and efficient query processing over outsourced data remains an open challenge. This article continues along with the emerging trend in secure data processing that recognizes that the entire dataset may not be sensitive and, hence, non-sensitivity of data can be exploited to overcome limitations of existing encryption-based approaches. We first provide a new security definition, entitled partitioned data security, for guaranteeing that the joint processing of non-sensitive data (in cleartext) and sensitive data (in encrypted form) does not lead to any leakage. Then, this article proposes a new secure approach, entitled query binning (QB), that allows secure execution of queries over non-sensitive and sensitive parts of the data. QB maps a query to a set of queries over the sensitive and non-sensitive data in a way that no leakage will occur due to the joint processing over sensitive and non-sensitive data. In particular, we propose secure algorithms for selection, range, and join queries to be executed over encrypted sensitive and cleartext non-sensitive datasets. Interestingly, in addition to improving performance, we show that QB actually strengthens the security of the underlying cryptographic technique by preventing size, frequency-count, and workload-skew attacks.  © 2020 ACM.",data encryption; Data outsourcing; data partitioning; Intel SGX; multi-party computation; non-sensitive data; output-size attack; scalable cryptography; secret-sharing; sensitive data; workload skew attack,Data privacy; Cryptographic techniques; Frequency counts; Improving performance; Joint processing; Non-sensitive data; Outsourced datum; Secure execution; Security definitions; Cryptography
"Trajectory Outlier Detection: Algorithms, Taxonomies, Evaluation, and Open Challenges",2020,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095136081&doi=10.1145%2f3399631&partnerID=40&md5=cadaa6ddf626c2d20fa0835a45bc6e3c,"Detecting abnormal trajectories is an important task in research and industrial applications, which has attracted considerable attention in recent decades. This work studies the existing trajectory outlier detection algorithms in different industrial domains and applications, including maritime, smart urban transportation, video surveillance, and climate change domains. First, we review several algorithms for trajectory outlier detection. Second, different taxonomies are proposed regarding application-, output-, and algorithm-based levels. Third, evaluation of 10 trajectory outlier detection algorithms is performed on small, large, and big trajectory databases. Finally, future challenges and open issues with regard to trajectory outliers are derived and discussed. This survey offers a general overview of existing trajectory outlier detection algorithms in industrial informatics applications. As a result, mature solutions may be further developed by data mining and machine learning communities.  © 2020 ACM.",data mining; industrial informatics applications; machine learning; Trajectory outlier detection,Anomaly detection; Climate change; Data handling; Data mining; Industrial informatics; Industrial research; Security systems; Signal detection; Statistics; Taxonomies; Urban transportation; Future challenges; Machine learning communities; Outlier detection algorithm; Trajectory database; Video surveillance; Work study; Trajectories
A Weakly Supervised WordNet-Guided Deep Learning Approach to Extracting Aspect Terms from Online Reviews,2020,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095110914&doi=10.1145%2f3399630&partnerID=40&md5=402c012f45f4308e056257930d529e75,"The unstructured nature of online reviews makes it inefficient and inconvenient for prospective consumers to research and use in support of purchase decision making. The aspects of products provide a fine-grained meaningful perspective for understanding and organizing review texts. Traditional aspect term extraction approaches rely on discrete language models that treat words in isolation. Despite that continuous-space language models have demonstrated promise in addressing a wide range of problems, their application in aspect term extraction faces significant challenges. For instance, existing continuous-space language models typically require large collections of labeled data, which remain difficult to obtain in many domains. More importantly, previous methods are largely data driven but overlook the role of human knowledge in guiding model development. To address these limitations, this study designs and develops weakly supervised WordNet-guided deep learning to aspect term extraction. The approach draws on deep-level semantic information from WordNet to guide not only the selection representative seed terms but also the pruning of aspect candidate terms. The weak supervision is provided by a very small set of labeled data. We conduct a comprehensive evaluation of the proposed method using both direct and indirect methods. The evaluation results with Yelp restaurant reviews demonstrate that our proposed method consistently outperforms all baseline methods including discrete models and the state-of-the-art continuous-space language models for aspect term extraction across both direct and indirect evaluations. The research findings have broad research, technical, and practical implications for various stakeholders of online reviews.  © 2020 ACM.",Aspect term extraction; continuous-space language model; deep learning; semantic knowledge; text analytics,Computational linguistics; Decision making; E-learning; Extraction; Labeled data; Ontology; Semantics; Comprehensive evaluation; Continuous spaces; Direct and indirect methods; Evaluation results; Learning approach; Model development; Restaurant reviews; Semantic information; Deep learning
Predicting User Posting Activities in Online Health Communities with Deep Learning,2020,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095123546&doi=10.1145%2f3383780&partnerID=40&md5=12029afe44d87f888d048b16f11427f6,"Online health communities (OHCs) represent a great source of social support for patients and their caregivers. Better predictions of user activities in OHCs can help improve user engagement and retention, which are important to manage and sustain a successful OHC. This article proposes a general framework to predict OHC user posting activities. Deep learning methods are adopted to learn from users' temporal trajectories in both the volumes and content of posts published over time. Experiments based on data from a popular OHC for cancer survivors demonstrate that the proposed approach can improve the performance of user activity predictions. In addition, several topics of users' posts are found to have strong impact on predicting users' activities in the OHC.  © 2020 ACM.",Predictive model; text analytics; trajectory mining; user churn,E-learning; Forecasting; Learning systems; Learning methods; Online health communities; Social support; Temporal trajectories; User activity; User engagement; Deep learning
Cashless Payment Policy and Its Effects on Economic Growth of India: An Exploratory Study,2020,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095122801&doi=10.1145%2f3391402&partnerID=40&md5=9687b8301eb1367cf34243725b173f28,"The present world has moved from cash transactions to cashless transactions. This article examines the impact of implementation of a cashless payment policy on economic development and gradual transition to a cashless economy in India. For this study, the focus is on the time period from 2010 to 2018. The data used for this study are tele transfer, through credit or debit card payment, check payment, and E-money on Indian economic growth. The study has employed the panel vector error correction model, Padroni residual cointegration, and the hypothetical prototypical method. The results show that customers and sellers accept a cashless system policy. In the short period, we have a causality model running from a card system to a check payment and telegraphic transfer system, and a causality model running from a telegraphic payment system to a card payment system. In the long period, there is a positive outcome in using a cashless policy on Indian economic growth. However, the use of a cashless policy on Indian economic development in the short term will be negative, whereas in the long term it will impact positively. Hence, any kind of economic strategy that endorses a cashless payment system cannot have positive impact on the economic development directly.  © 2020 ACM.",and telegraphic transfer; Card payment; check payment; electronic money; VECM,Economic analysis; Error correction; Cash transaction; Causality modeling; Economic growths; Economic strategy; Exploratory studies; Gradual transition; Transfer systems; Vector error correction model; Economic and social effects
Introduction to WITS 2018 Special Issue in TMIS,2020,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095113649&doi=10.1145%2f3404392&partnerID=40&md5=cb1b612dd90b165504a5deff65fd1afc,[No abstract available],,
E-Commerce Product Categorization via Machine Translation,2020,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095128102&doi=10.1145%2f3382189&partnerID=40&md5=9b70fda7f85e9a22636e634d7ebb6f7c,"E-commerce platforms categorize their products into a multi-level taxonomy tree with thousands of leaf categories. Conventional methods for product categorization are typically based on machine learning classification algorithms. These algorithms take product information as input (e.g., titles and descriptions) to classify a product into a leaf category. In this article, we propose a new paradigm based on machine translation. In our approach, we translate a product's natural language description into a sequence of tokens representing a root-to-leaf path in a product taxonomy. In our experiments on two large real-world datasets, we show that our approach achieves better predictive accuracy than a state-of-the-art classification system for product categorization. In addition, we demonstrate that our machine translation models can propose meaningful new paths between previously unconnected nodes in a taxonomy tree, thereby transforming the taxonomy into a directed acyclic graph. We discuss how the resultant taxonomy directed acyclic graph promotes user-friendly navigation, and how it is more adaptable to new products.  © 2020 ACM.",classification; E-commerce; machine translation,Computational linguistics; Computer aided language translation; Directed graphs; Electronic commerce; Forestry; Large dataset; Learning algorithms; Taxonomies; Trees (mathematics); Classification system; Conventional methods; Directed acyclic graph (DAG); Machine translation models; Machine translations; Predictive accuracy; Product information; Real-world datasets; Machine learning
Real or Not?: Identifying Untrustworthy NewsWebsites Using Third-party Partnerships,2020,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095119872&doi=10.1145%2f3382188&partnerID=40&md5=e9e785df782bcdec08fc3b3d824b5b8a,"Untrustworthy content such as fake news and clickbait have become a pervasive problem on the Internet, causing significant socio-political problems around the world. Identifying untrustworthy content is a crucial step in countering them. The current best practices for identification involve content analysis and arduous fact-checking of the content. To complement content analysis, we propose examining websites' third-parties to identify their trustworthiness. Websites utilize third-parties, also known as their digital supply chains, to create and present content and help the website function. Third-parties are an important indication of a website's business model. Similar websites exhibit similarities in the third-parties they use. Using this perspective, we use machine learning and heuristic methods to discern similarities and dissimilarities in third-party usage, which we use to predict trustworthiness of websites. We demonstrate the effectiveness and robustness of our approach in predicting trustworthiness of websites from a database of News, Fake News, and Clickbait websites. Our approach can be easily and cost-effectively implemented to reinforce current identification methods.  © 2020 ACM.",heuristics; machine learning; prediction; untrustworthy websites; Website third-parties,Heuristic methods; Supply chains; Best practices; Business modeling; Content analysis; Digital supply chain; Identification method; Political problems; Third parties; Websites
Utility-Driven Mining of Trend Information for Intelligent System,2020,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095135213&doi=10.1145%2f3391251&partnerID=40&md5=55446ba06f07b3b3d3fa8e7a33fa2782,"Useful knowledge, embedded in a database, is likely to change over time. Identifying the recent changes in temporal data can provide valuable up-to-date information to decision makers. Nevertheless, techniques for mining high-utility patterns (HUPs) seldom consider recency as a criterion to discover patterns. Thus, the traditional utility mining framework is inadequate for obtaining up-to-date insights about real-world data. In this article, we address this issue by introducing a novel framework, named utility-driven mining of recent/trend high-utility patterns (RUPs), in temporal databases for intelligent systems, based on user-specified minimum recency and minimum utility thresholds. The utility-driven RUP algorithm is based on novel global and conditional downward closure properties, and a recency-utility tree. Moreover, it adopts a vertical compact recency-utility list structure to store the information required by the mining process. The developed RUP algorithm recursively discovers recent high-utility patterns. It is also fast and consumes a small amount of memory due to its pattern discovery approach that does not generate candidates. Two improved versions of the algorithm with additional pruning strategies are also designed to speed up the discovery of patterns by reducing the search space. Results of a substantial experimental evaluation show that the proposed algorithm can efficiently identify all recent HUPs in large-scale databases, and that the improved algorithm performs best.  © 2020 ACM.",Economic behavior; high-utility pattern; intelligent system; temporal database; utility mining,Database systems; Decision making; Intelligent systems; Trees (mathematics); Decision makers; Downward closure properties; Experimental evaluation; Large-scale database; List structures; Pattern discovery; Pruning strategy; Temporal Database; Data mining
Algorithms and Applications to Weighted Rank-one Binary Matrix Factorization,2020,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090455930&doi=10.1145%2f3386599&partnerID=40&md5=3cdf3cbb72cecd88aee050b31eaeb7da,"Many applications use data that are better represented in the binary matrix form, such as click-stream data, market basket data, document-term data, user-permission data in access control, and others. Matrix factorization methods have been widely used tools for the analysis of high-dimensional data, as they automatically extract sparse and meaningful features from data vectors. However, existing matrix factorization methods do not work well for the binary data. One crucial limitation is interpretability, as many matrix factorization methods decompose an input matrix into matrices with fractional or even negative components, which are hard to interpret in many real settings. Some matrix factorization methods, like binary matrix factorization, do limit decomposed matrices to binary values. However, these models are not flexible to accommodate some data analysis tasks, like trading off summary size with quality and discriminating different types of approximation errors. To address those issues, this article presents weighted rank-one binary matrix factorization, which is to approximate a binary matrix by the product of two binary vectors, with parameters controlling different types of approximation errors. By systematically running weighted rank-one binary matrix factorization, one can effectively perform various binary data analysis tasks, like compression, clustering, and pattern discovery. Theoretical properties on weighted rank-one binary matrix factorization are investigated and its connection to problems in other research domains are examined. As weighted rank-one binary matrix factorization in general is NP-hard, efficient and effective algorithms are presented. Extensive studies on applications of weighted rank-one binary matrix factorization are also conducted. © 2020 ACM.",clustering; compression; Discrete data; pattern discovery,Access control; Clustering algorithms; Commerce; Data streams; Factorization; Information analysis; NP-hard; Quality control; Approximation errors; Effective algorithms; High dimensional data; Interpretability; Matrix factorizations; Negative components; Pattern discovery; Research domains; Matrix algebra
When Are Cyber Blackouts in Modern Service Networks Likely?,2020,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090469868&doi=10.1145%2f3386159&partnerID=40&md5=b3f9c710ee2e4f415bcb1d77e3cbe11a,"Service liability interconnections among globally networked IT- and IoT-driven service organizations create potential channels for cascading service disruptions worth billions of dollars, due to modern cyber-crimes such as DDoS, APT, and ransomware attacks. A natural question that arises in this context is: What is the likelihood of a cyber-blackout?, where the latter term is defined as the probability that all (or a major subset of) organizations in a service chain become dysfunctional in a certain manner due to a cyber-attack at some or all points in the chain. The answer to this question has major implications to risk management businesses such as cyber-insurance when it comes to designing policies by risk-averse insurers for providing coverage to clients in the aftermath of such catastrophic network events. In this article, we investigate this question in general as a function of service chain networks and different cyber-loss distribution types. We show somewhat surprisingly (and discuss the potential practical implications) that, following a cyber-attack, the effect of (a) a network interconnection topology and (b) a wide range of loss distributions on the probability of a cyber-blackout and the increase in total service-related monetary losses across all organizations are mostly very small. The primary rationale behind these results are attributed to degrees of heterogeneity in the revenue base among organizations and the Increasing Failure Rate property of popular (i.i.d/non-i.i.d) loss distributions, i.e., log-concave cyber-loss distributions. The result will enable risk-averse cyber-risk managers to safely infer the impact of cyber-attacks in a worst-case network and distribution oblivious setting. © 2020 ACM.",cyber-blackout; Service network; systemic risk,Crime; Economics; Failure analysis; Interconnection networks (circuit switching); Network security; Probability distributions; Risk analysis; Risk management; Cyber-attacks; Increasing failure rate; Loss distribution; Modern service; Network interconnections; Service chain; Service disruptions; Service organizations; Malware
Context-Aware Recommendations Based on Deep Learning Frameworks,2020,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090456710&doi=10.1145%2f3386243&partnerID=40&md5=7e0a6c681be0574ec03b04954d4be1d7,"In this article, we suggest a novel deep learning recommendation framework that incorporates contextual information into neural collaborative filtering recommendation approaches. Since context is often represented by dynamic and high-dimensional feature space in multiple applications and services, we suggest to model contextual information in various ways for multiple purposes, such as rating prediction, generating top-k recommendations, and classification of users' feedback. Specifically, based on the suggested framework, we propose three deep context-aware recommendation models based on explicit, unstructured, and structured latent representations of contextual data derived from various contextual dimensions (e.g., time, location, user activity). Offline evaluation on three context-aware datasets confirms that our proposed deep context-aware models surpass state-of-the-art context-aware methods. We also show that utilizing structured latent contexts in the proposed deep recommendation framework achieves significantly better performance than the other context-aware models on all datasets. © 2020 ACM.",Context; context-aware recommendation; deep learning; latent; neural networks,Classification (of information); Collaborative filtering; Recommender systems; Collaborative filtering recommendations; Context-aware models; Context-aware recommendations; Contextual dimensions; Contextual information; High-dimensional feature space; Multiple applications; Top-K recommendations; Deep learning
Security and Privacy Requirements for Cloud Computing in Healthcare: Elicitation and Prioritization from a Patient Perspective,2020,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090459336&doi=10.1145%2f3386160&partnerID=40&md5=ca1f4f1c39a653b753969691c3781191,"Cloud computing promises essential improvements in healthcare delivery performance. However, its wide adoption in healthcare is yet to be seen, one main reason being patients' concerns for security and privacy of their sensitive medical records. These concerns can be addressed through corresponding security and privacy requirements within the system engineering process. Despite a plethora of related research, security and privacy requirements for cloud systems and services have seldomly been investigated methodically so far, whereas their individual priorities to increase the system success probability have been neglected. Against this background, this study applies a systematic requirements engineering process: First, based on a systematic literature review, an extensive initial set of security and privacy requirements is elicited. Second, an online survey based on the best-worst scaling method is designed, conducted, and evaluated to determine priorities of security and privacy requirements. Our results show that confidentiality and integrity of medical data are ranked at the top of the hierarchy of prioritized requirements, followed by control of data use and modification, patients' anonymity, and patients' control of access rights. Availability, fine-grained access control, revocation of access rights, flexible access, clinicians' anonymity, as well as usability, scalability, and efficiency of the system complete the ranking. The level of agreement among patients is rather small, but statistically significant at the 0.01 level. The main contribution of the present research comprises the study method and results highlighting the role of strong security and privacy and excluding any trade-offs with system usability. Enabling a richer understanding of patients' security and privacy requirements for adopting cloud computing in healthcare, these are of particular importance to researchers and practitioners interested in supporting the process of security and privacy engineering for health-cloud solutions. It further represents a supplement that can support time-intensive negotiation meetings between the requirements engineers and patients. © 2020 ACM.",Cloud computing; healthcare; privacy; requirements; security,Cloud computing; Economic and social effects; Health care; Requirements engineering; Best-worst scaling; Healthcare delivery; Requirements engineering process; Security and privacy; Strong securities; System engineering process; System usability; Systematic literature review; Access control
Mining the Local Dependency Itemset in a Products Network,2020,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091405003&doi=10.1145%2f3384473&partnerID=40&md5=a662f77944eb238a90dbc98eb13210e1,"Many studies have been conducted on market basket analysis such as association rules and dependent patterns. These studies mainly focus on mining all significant patterns or patterns directly associated with a given item in a dataset. The problem that has not been addressed is how to mine patterns associated with a given item from the local view. This problem becomes very meaningful when the market basket dataset is huge. To address this problem, in this study, first, a new idea called ""local dependency itemset""is put forward, which refers to patterns associated with the given item. Second, a framework of mining the local dependency itemset is presented. The framework has two steps, which are executed iteratively. One is expanding the local dependency itemset that initially consists of only the given item; the other is updating the local products network. Third, this framework is implemented by three different dependence indicators and a typical local community detection algorithm. The experimental results confirm that the local dependency itemset is meaningful.  © 2020 ACM.",Data mining; local community detection; local dependency itemset; products network,Commerce; Itemset; Local community; Market basket; Market basket analysis; Significant patterns; Iterative methods
A Data-driven Characterization of Modern Android Spyware,2020,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091440725&doi=10.1145%2f3382158&partnerID=40&md5=a6c4037a591450efd1c49da8314b119b,"According to Nokia's 2017 Threat Intelligence Report, 68.5% of malware targets the Android platform; Windows is second with 28%, followed by iOS and other platforms with 3.5%. The Android spyware family UAPUSH was responsible for the most infections, and several of the top 20 most common Android malware were spyware. Simply put, modern spyware steals the basic information needed to fuel more deadly attacks such as ransomware and banking fraud. Not surprisingly, some forms of spyware are also classified as banking trojans (e.g., ACECARD). We present a data-driven characterization of the principal factors that distinguish modern Android spyware (July 2016-July 2017) both from goodware and other Android malware, using both traditional and deep ML. First, we propose an Ensemble Late Fusion (ELF) architecture that combines the results of multiple classifiers' predicted probabilities to generate a final prediction. We show that ELF outperforms several of the best-known traditional and deep learning classifiers. Second, we automatically identify key features that distinguish spyware both from goodware and from other malware. Finally we present a detailed analysis of the factors distinguishing five important families of Android spyware: UAPUSH, PINCER, HEHE, USBCLEAVER, and ACECARD (the last is a hybrid spyware-banking trojan).  © 2020 ACM.",Android; characterization; Machine learning; malware; spyware,Banking; Computer viruses; Deep learning; Android malware; Android platforms; Banking trojans; Key feature; Late fusion; Learning classifiers; Multiple classifiers; Principal factors; Android (operating system)
A Randomized Reputation System in the Presence of Unfair Ratings,2020,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091459465&doi=10.1145%2f3384472&partnerID=40&md5=797c1e2992d8bd3288b493b270620536,"With the increasing popularity of online shopping markets, a significant number of consumers rely on these venues to meet their demands while choosing different products based on the ratings provided by others. Simultaneously, consumers feel confident in expressing their opinions through ratings. As a result, millions of ratings are generated on the web for different products, services, and dealers. Nonetheless, a noticeable number of users post unfair feedback. Recent studies have shown that reputation escalation is emerging as a new service, by which dealers pay to receive good feedback and escalate their ratings in online shopping markets. Therefore, finding robust and reliable ways to distinguish between fake and trustworthy ratings from users is a crucial task for every online shopping market. Moreover, with the dramatic increase in the number of ratings provided by consumers, scalability has arisen as another significant issue in the existing methods of reputation systems. To tackle these issues, we propose a randomized algorithm that calculates the reputation based on a random sample of the ratings. Since the randomly selected sample has a logarithmic size, it guarantees feasible scalability for large-scale online review systems. In addition, the randomness nature of the algorithm makes it robust against unfair ratings. We provide a thorough theoretical analysis of the proposed algorithm and validate its effectiveness through extensive empirical evaluation using real-world and synthetically generated datasets. Our experimental results show that the proposed method provides a high accuracy while running much faster than the existing iterative filtering approaches.  © 2020 ACM.",data aggregation; online rating; randomized algorithm; Reputation system; trust computation,Electronic commerce; Online systems; Scalability; Empirical evaluations; High-accuracy; Iterative filtering; Online reviews; Online shopping market; Randomized Algorithms; Reputation systems; Unfair ratings; Iterative methods
Invested or indebted: Ex-ante and Ex-post reciprocity in online knowledge sharing communities,2020,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078147935&doi=10.1145%2f3371388&partnerID=40&md5=ff5b08971c5384f5d646f569338e1d29,"Online communities that curate knowledge critically depend on high-quality contributions from anonymous expert users. Understanding users’ motivation to contribute knowledge helps practitioners design such websites for optimal user contribution and user benefits. Researchers have studied reciprocity as a motivation for users to share knowledge online. In this study, we focus on two different types of reciprocity as drivers of online contribution: ex-post and ex-ante reciprocity. Ex-post reciprocity refers to users who received help from others in the past and pay back by helping others at present. Using a quasi-experiment performed via the instrumental variable method as the identification strategy, we test whether users who received more answers last week answer more questions in the current week on StackOverflow.com. We find a significant positive relationship between ex-post reciprocity and knowledge contribution, and such a reciprocal motivation diminishes with time. Ex-ante reciprocity refers to people helping others in expectation of future help from others. Using data from StackOverflow.com, we take advantage of a natural experiment with a difference-in-differences analysis and find evidence supporting the existence of ex-ante reciprocity. This study offers a new taxonomy for reciprocity and new insights on how reciprocity drives online knowledge sharing. © 2020 Association for Computing Machinery.",Ex-ante; Ex-post; Knowledge sharing; Q&A website; Reciprocity,Digital storage; Motivation; Websites; Difference-in-differences; Ex antes; Ex-post; Identification strategies; Instrumental variable methods; Knowledge-sharing; Online knowledge sharing; Reciprocity; Knowledge management
Effective selection of a compact and high-quality review set with information preservation,2019,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077400125&doi=10.1145%2f3369395&partnerID=40&md5=ab65e8f043ca6fb369b11afb3e032702,"Consumers increasingly make informed buying decisions based on reading online reviews for products and services. Due to the large volume of available online reviews, consumers hardly have the time and patience to read them all. This article aims to select a compact set of high-quality reviews that can cover a specific set of product features and related consumer sentiments. Selecting such a subset of reviews can significantly save the time spent on reading reviews while preserving the information needed. A unique review selection problem is defined and modeled as a bi-objective combinatorial optimization problem, which is then transformed into a minimum-cost set cover problem that is NP-complete. Several approximation algorithms are then designed, which can sustain performance guarantees in polynomial time. Our effective selection algorithms can also be upgraded to handle dynamic situations. Comprehensive experiments conducted on twelve real datasets demonstrate that the proposed algorithms significantly outperform benchmark methods by generating a more compact review set with much lower computational cost. The number of reviews selected is much smaller compared to the quantity of all available reviews, and the selection efficiency is deeply increased by accelerating strategies, making it very practical to adopt the methods in real-world online applications. © 2019 Association for Computing Machinery.",Approximation algorithms; Dynamic updating; Information preservation; Review selection,Combinatorial optimization; Genetic algorithms; Polynomial approximation; Bi-objective combinatorial optimization; Computational costs; Dynamic updating; Information preservations; On-line applications; Performance guarantees; Products and services; Selection algorithm; Approximation algorithms
Measuring the business value of recommender systems,2019,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077355276&doi=10.1145%2f3370082&partnerID=40&md5=82dbcba3fbb3b0be7fbd918b8f7038a8,"Recommender Systems are nowadays successfully used by all major web sites-from e-commerce to social media-to filter content and make suggestions in a personalized way. Academic research largely focuses on the value of recommenders for consumers, e.g., in terms of reduced information overload. To what extent and in which ways recommender systems create business value is, however, much less clear, and the literature on the topic is scattered. In this research commentary, we review existing publications on field tests of recommender systems and report which business-related performance measures were used in such real-world deployments. We summarize common challenges of measuring the business value in practice and critically discuss the value of algorithmic improvements and offline experiments as commonly done in academic environments. Overall, our review indicates that various open questions remain both regarding the realistic quantification of the business effects of recommenders and the performance assessment of recommendation algorithms in academia. © 2019 Copyright held by the owner/author(s).",Business value; Field tests; Recommendation; Survey,Recommender systems; Surveying; Academic environment; Business value; Field test; Information overloads; Performance assessment; Real world deployment; Recommendation; Recommendation algorithms; Electronic commerce
Interaction models for detecting nodal activities in temporal social media networks,2019,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077373340&doi=10.1145%2f3365537&partnerID=40&md5=b9497e55415c23230daf7905b6b160aa,"Detecting nodal activities in dynamic social networks has strategic importance in many applications, such as online marketing campaigns and homeland security surveillance. How peer-to-peer exchanges in social media can facilitate nodal activity detection is not well explored. Existing models assume network nodes to be static in time and do not adequately consider features from social theories. This research developed and validated two theory-based models, Random Interaction Model (RIM) and Preferential Interaction Model (PIM), to characterize temporal nodal activities in social media networks of human agents. The models capture the network characteristics of randomness and preferential interaction due to community size, human bias, declining connection cost, and rising reachability. The models were compared against three benchmark models (abbreviated as EAM, TAM, and DBMM) using a social media community consisting of 790,462 users who posted over 3,286,473 tweets and formed more than 3,055,797 links during 2013-2015. The experimental results show that both RIM and PIM outperformed EAM and TAM significantly in accuracy across different dates and time windows. Both PIM and RIM scored significantly smaller errors than DBMM did. Structural properties of social networks were found to provide a simple and yet accurate approach to predicting model performances. These results indicate the models' strong capability of accounting for user interactions in realworld social media networks and temporal activity detection. The research should provide new approaches for temporal network activity detection, develop relevant new measures, and report new findings from large social media datasets. © 2019 Copyright held by the owner/author(s).",Business analytics; Dynamic graph modeling; Interaction models; Social media analytics; Social network analysis,Advanced Analytics; Large dataset; Marketing; Business analytics; Dynamic graph; Dynamic social networks; Interaction model; Network characteristics; Peer-to-peer exchanges; Preferential interaction; Social media analytics; Social networking (online)
The economics of cybercrime: The role of broadband and socioeconomic status,2019,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076699937&doi=10.1145%2f3351159&partnerID=40&md5=460c5106a44b30886e122e16afed3c1b,"Under what conditions is the Internet more likely to be used maliciously for criminal activity? This study examines the conditions under which the Internet is associated with cybercriminal offenses. Using comprehensive state-level data in the United States during 2004-2010, our findings show that there is no clear empirical evidence that the Internet penetration rate is related to the number of Internet crime perpetrators; however, cybercriminal activities are contingent upon socioeconomic factors and connection speed. Specifically, a higher income, more education, a lower poverty rate, and a higher inequality are likely to make the Internet penetration be more positively related with cybercrime perpetrators, which are indeed different from the conditions of terrestrial crime in the real world. In addition, as opposed to narrowband, the broadband connections are significantly and positively associated with the number of Internet crime perpetrators, and it amplifies the aforementioned moderating effects of socioeconomic status on Internet crime offenses. Taken together, cybercrime requires more than just a skilled perpetrator, and it requires an infrastructure to facilitate profiteering from the act. © 2019 Association for Computing Machinery.",Broadband; Cybercrime; Economics of crime; Internet penetration; Socioe-conomic status,Crime; Economics; Broadband; Broadband connection; Criminal activities; Cybercrime; Economics of crime; Penetration rates; Socio-economic factor; Socioe-conomic status; Computer crime
Helpfulness assessment of online reviews: The role of semantic hierarchy of product features,2019,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077792063&doi=10.1145%2f3365538&partnerID=40&md5=2ca72eec1d9cbdb5a30de1d79b679e5c,"Effective use of online consumer reviews is hampered by uncertainty about their helpfulness. Despite a growing body of knowledge on indicators of review helpfulness, previous studies have overlooked rich semantic information embedded in review content. Following design science principles, this study introduces a semantic hierarchy of product features by probing the review text. Using the hierarchical framework as a guide, we develop a research model of review helpfulness assessment. In the model, we propose and conceptualize three new factors—breadth, depth, and redundancy, by building on and/or extending product uncertainty, information quality, signaling, and encoding variability theories. The model-testing results lend strong support to the proposed effects of those factors on review helpfulness. They also reveal interesting differences in the effects of redundancy and readability between different types of products. This study embodies knowledge moments of multiple genres of inquiry in design science research, which have multifold research and practical implications. © 2019 Association for Computing Machinery.",Breadth; Depth; Design science; Product feature; Redundancy; Review helpfulness; Semantic hierarchy,Redundancy; Semantics; Breadth; Depth; Design science; Product feature; Semantic hierarchies; Product design
Service-oriented application composition with evolutionary heuristics and multiple criteria,2019,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075618130&doi=10.1145%2f3354288&partnerID=40&md5=6c5df21aeea0ef98ee273c045a46a780,"The need to create and deploy business application systems rapidly has sparked interest in using web services to compose them. When creating mission-critical business applications through web service compositions, in addition to ensuring that functional requirements are met, designers need to consider the end-to-end reliability, security, performance, and overall cost of the application. As the number of available coarse-grain business services grows, the problem of selecting appropriate services quickly becomes combinatorially explosive for realistic-sized business applications. This article develops a business-process-driven approach for composing service-oriented applications. We use a combination of weights to explore the entire QoS criteria landscape through the use of a multi-criteria genetic algorithm (GA) to identify a Pareto-optimal multidimensional frontier that permits managers to trade off conflicting objectives when selecting a set of services. We illustrate the effectiveness of the approach by applying it to a real-world drop-ship business application and compare its performance to another GA-based approach for service composition. © 2019 Association for Computing Machinery.",Multiple criteria decision making; Service-oriented applications,Decision making; Economic and social effects; Genetic algorithms; Pareto principle; Quality of service; Websites; Business application systems; Conflicting objectives; End-to-end reliabilities; Evolutionary heuristics; Functional requirement; Multiple criteria decision making; Service oriented application; Web service composition; Web services
"On robust estimates of correlated risk in cyber-insured it firms: A first look at optimal ai-based estimates under ""small"" data",2019,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075625941&doi=10.1145%2f3351158&partnerID=40&md5=4f15911c8e08c01accd71f68899e6451,"In this article, we comment on the drawbacks of the existing AI-based Bayesian network (BN) cybervulnerability analysis (C-VA) model proposed in Mukhopadhyay et al. (2013) to assess cyber-risk in IT firms, where this quantity is usually a joint distribution of multiple risk (random) variables (e.g., quality of antivirus, frequency of monitoring, etc.) coming from heterogeneous distribution families. As a major modeling drawback, Mukhopadhyay et al. (2013) assume that any pair of random variables in the BN are linearly correlated with each other. This simplistic assumption might not always hold true for general IT organizational environments. Thus, the use of the C-VA model in general will result in loose estimates of correlated IT risk and will subsequently affect cyber-insurance companies in framing profitable coverage policies for IT organizations. To this end, we propose methods to (1) find a closed-form expression for the maximal correlation arising between pairs of discrete random variables, whose value finds importance in getting robust estimates of copula-induced computations of organizational cyber-risk, and (2) arrive at a computationally effective mechanism to compute nonlinear correlations among pairs of discrete random variables in the correlation matrix of the CBBN model (Mukhopadhyay et al. 2013). We also prove that an empirical computation of MC using our method converges rapidly, that is, exponentially fast, to the true correlation value in the number of samples. Our proposed method contributes to a tighter estimate of IT cyber-risk under environments of low-risk data availability and will enable insurers to better assess organizational risks and subsequently underwrite profitable cyber-insurance policies. © 2019 Association for Computing Machinery.",AI; Bayesian network; Copula; Correlation; IT cyber-risk; Sampling,Artificial intelligence; Bayesian networks; Computer viruses; Correlation methods; Insurance; Profitability; Random variables; Risk perception; Sampling; Closed-form expression; Copula; Discrete random variables; Effective mechanisms; Empirical computation; Heterogeneous distributions; Maximal correlation; Non-linear correlations; Risk assessment
Thumbstroke: A virtual keyboard in support of sight-free and one-handed text entry on touchscreen mobile devices,2019,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072202163&doi=10.1145%2f3343858&partnerID=40&md5=6b7cc79456be34dd80fb5f302ec268cf,"The QWERTY keyboard on mobile devices usually requires users’ full visual attention and both hands, which is not always possible. We propose a thumb-stroke-based keyboard, ThumbStroke, to support both sight-free and one-handed text entry. Text entry via ThumbStroke completely relies on the directions of thumb strokes at any place on the screen of a mobile device. It does not require physical press on any specific keys, thus eliminating the need for visual attention and reducing errors due to tiny key size, fat thumbs, limited thumb reachability, and visual occlusion. We empirically evaluated ThumbStroke through a 20-session longitudinal controlled lab experiment. ThumbStroke shows advantages in typing accuracy and user perceptions in comparison to the Escape and QWERTY keyboards and results in faster typing speed than QWERTY in sight-free and one-handed text entry. This study provides novel research contributions to mobile HCI, advancing the design of soft keyboards for one-handed interaction with mobile devices and mobile accessibility. © 2019 held by the owner/author(s).",Keyboard; One-handed; Sight-free; Stroke; Text entry; ThumbStroke,Behavioral research; Keyboard; One-handed; Sight-free; Stroke; Text entry; ThumbStroke; Character recognition
"Introduction to special section based on papers presented at the workshop on information technology and systems, 2017",2019,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074814030&doi=10.1145%2f3342557&partnerID=40&md5=f0a6e6b267f153fa30a95bfbe9e93cf9,[No abstract available],,
Are truthful bidders paying too much? Efficiency and revenue in display ad auctions,2019,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074856479&doi=10.1145%2f3325523&partnerID=40&md5=3d856202eda33728ed98bb90cfb2445c,"Display ad auctions have become the predominant means to allocate user impressions on a website to advertisers. These auctions are conducted in milliseconds online, whenever a user visits a website. The impressions are typically priced via a simple second-price rule. For single-item auctions, this Vickrey payment rule is known to be incentive-compatible. However, it is unclear whether bidders should still bid truthful in an online auction where impressions (or items) arrive dynamically over time and their valuations are not separable, as is the case with campaign targets or budgets. The allocation process might not maximize welfare and the payments can differ substantially from those paid in an offline auction with a Vickrey-Clarke-Groves (VCG) payment rule or also competitive equilibrium prices. We study the properties of the offline problem and model it as a mathematical program. In numerical experiments, we find that the welfare achieved in the online auction process with truthful bidders is high compared to the theoretical worst-case efficiency, but that the bidders pay significantly more on average compared to what they would need to pay in a corresponding offline auction in thin markets with up to four bidders. However, incentives for bid shading in these second-price auctions decrease quickly with additional competition and bidders risk losing. © 2019 Association for Computing Machinery.",Display ad auctions; Efficiency; Incentives in online auctions; Real-time bidding,Budget control; Electronic commerce; Websites; Ad auctions; Competitive equilibrium; Incentive compatible; Mathematical program; Numerical experiments; Online auctions; Real time; Worst-case efficiency; Efficiency
Differential privacy for the vast majority,2019,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074838726&doi=10.1145%2f3329717&partnerID=40&md5=32603c65ef6aa004fcb44fc351de662b,"Differential privacy has become one of the widely used mechanisms for protecting sensitive information in databases and information systems. Although differential privacy provides a clear measure of privacy guarantee, it implicitly assumes that each individual corresponds to a single record in the result of a database query. This assumption may not hold in many database query applications. When an individual has multiple records, strict implementation of differential privacy may cause significant information loss. In this study, we extend the differential privacy principle to situations wheremultiple records in a database are associated with the same individual. We propose a new privacy principle that integrates differential privacy with the Pareto principle in analyzing privacy risk and data utility. When applied to the situations with multiple records per person, the proposed approach can significantly reduce the information loss in the released query results with a relatively small relaxation in the differential privacy guarantee. The effectiveness of the proposed approach is evaluated using three real-world databases. © 2019 Association for Computing Machinery.",Data privacy; Database query; Noise perturbation; Pareto principle,Database systems; Pareto principle; Query processing; Data utilities; Database queries; Differential privacies; Information loss; Noise perturbation; Privacy principle; Real-world database; Sensitive informations; Data privacy
Modeling and analyzing incremental natures of developing software,2019,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074850332&doi=10.1145%2f3333535&partnerID=40&md5=fc266adc895f1c227681263fa4cbd33f,"The basic premise of iterative and evolutionary project management is that a project is divided into early, frequent, and short duration delivery steps. Each step attempts to deliver some real value to stakeholders. The increment size and iteration length usually depend on profitability, finance, deadline, and so on, rather than the functionality of a developing system. It is difficult to guarantee the correctness in every iteration step. In this article, we propose a method of ensuring the correctness of iterative design in terms of deadlock-freedom of the behavior of software. The method first obtains the correct (deadlock-free) atomic subsystems of a system using a decomposition approach. In the iterative development process, the method then requires that one atomic subsystem or the composition of multiple atomic subsystems should be regarded as one increment. Every increment is naturally correct and can be completely independently developed, independently deployed, and independently maintained. The currently released system in each iteration step is naturally guaranteed to be correct. It is not necessary for developers to consider the composition of the increment and the previously released system may cause flaws and errors. We also discuss the approach for ensuring correctness when design modifications are made in an iteration step. Finally, we explore the automatic decomposition of a system into multiple atomic subsystems and present the corresponding algorithm. A case demonstrates these results. © 2019 Copyright held by the owner/author(s).",Decomposition; Decrement; Increment; Iteration,Atoms; Decomposition; Project management; Atomic subsystems; Automatic decomposition; Decomposition approach; Decrement; Design modifications; Increment; Iteration; Iterative development; Iterative methods
Catching them red-handed: Optimizing the nursing homes' rating system,2019,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074816922&doi=10.1145%2f3325522&partnerID=40&md5=1b549adbf8a67a3048c81c6212f499b6,"The Centers for Medicare & Medicaid Services (CMS) launched its nursing home rating system in 2008, which has been widely used among patients, doctors, and insurance companies since then. The system rates nursing homes based on a combination of CMS's inspection results and nursing homes' self-reported measures. Prior research has shown that the rating system is subject to inflation in the self-reporting procedure, leading to biased overall ratings. Given the limited resources CMS has, it is important to optimize the inspection process and develop an effective audit process to detect and deter inflation. We first examine if the domain that CMS currently inspects is the best choice in terms of minimizing the population of nursing homes that can inflate and minimizing the difficulty of detecting such inflators. To do this, we formulate the problem mathematically and test the model by using publicly available CMS data on nursing home ratings. We show that CMS's current choice of inspection domain is not optimal if it intends to minimize the number of nursing homes that can inflate their reports, and CMS will be better off if it inspects the staffing domain instead. We also show that CMS's current choice of inspection domain is only optimal had there been an audit system in place to complement it. We then design an audit system for CMS which will be coupled with its current inspection strategy to either minimize the initial budget required to conduct the audits or to maximize the efficiency of the audit process. To design the audit system, we consider nursing homes' reactions to different audit policies, and conduct a detailed simulation study on the optimal audit parameter settings. Our result suggests that CMS should use a moderate audit policy in order to carefully balance the tradeoff between audit net budget and audit efficiency. © 2019 Association for Computing Machinery.",Audit design; Nursing homes' rating system; Rating inflation,Budget control; Efficiency; Health insurance; Hospitals; Inspection; Inspection process; Inspection strategy; Insurance companies; Nursing homes; Parameter setting; Rating system; Self-reporting; Simulation studies; Nursing
Emoticon analysis for Chinese social media and e-commerce: The azemo system,2019,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065797244&doi=10.1145%2f3309707&partnerID=40&md5=956af8398674dec3f3bf3c39cb56b8b3,"This article presents a novel system, AZEmo, which extracts and classifies emoticons from the ever-growing critical Chinese social media and E-commerce. An emoticon is a meta-communicative pictorial representation of facial expressions, which helps to describe the sender's emotional state. To complement non-verbal communication, emoticons are frequently used in social media websites. However, limited research has been done to effectively analyze the affects of emoticons in a Chinese context. In this study, we developed an emoticon analysis system to extract emoticons from Chinese text and classify them into one of seven affect categories. The system is based on a kinesics model that divides emoticons into semantic areas (eyes, mouths, etc.), with improvements for adaptation in the Chinese context. Machine-learning methods were developed based on feature vector extraction of emoticons. Empirical tests were conducted to evaluate the effectiveness of the proposed system in extracting and classifying emoticons, based on corpora from a video sharing website and an E-commerce website. Results showed the effectiveness of the system in detecting and extracting emoticons from text and in interpreting the affects conveyed by emoticons. © 2019 Association for Computing Machinery.",Affect analysis; Chinese Internet; Emoticon; Social media,Electronic commerce; Learning systems; Semantics; Affect analysis; Chinese internets; Emoticon; Feature vector extraction; Machine learning methods; Non-verbal communications; Pictorial representation; Social media; Social networking (online)
Putting question-answering systems into practice: Transfer learning for efficient domain customization,2019,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062591784&doi=10.1145%2f3309706&partnerID=40&md5=dc4c008b9e1661602d5bfc9eed8bd2f8,"Traditional information retrieval (such as that offered by web search engines) impedes users with information overload from extensive result pages and the need to manually locate the desired information therein. Conversely, question-answering systems change how humans interact with information systems: Users can now ask specific questions and obtain a tailored answer-both conveniently in natural language. Despite obvious benefits, their use is often limited to an academic context, largely because of expensive domain customizations, which means that the performance in domain-specific applications often fails to meet expectations. This article proposes cost-efficient remedies: (i) we leverage metadata through a filtering mechanism, which increases the precision of document retrieval, and (ii) we develop a novel fuse-and-oversample approach for transfer learning to improve the performance of answer extraction. Here, knowledge is inductively transferred from related, yet different, tasks to the domain-specific application, while accounting for potential differences in the sample sizes across both tasks. The resulting performance is demonstrated with actual use cases from a finance company and the film industry, where fewer than 400 question-answer pairs had to be annotated to yield significant performance gains. As a direct implication to management, this presents a promising path to better leveraging of knowledge stored in information systems. ©2019 Association for Computing Machinery.",Deep learning; Domain customization; Machine comprehension; Question answering; Transfer learning,Artificial intelligence; Deep learning; Information retrieval; Information retrieval systems; Information systems; Information use; Knowledge management; Natural language processing systems; Domain customization; Domain-specific application; Information overloads; Potential difference; Question Answering; Question answering systems; Question-answer pairs; Transfer learning; Search engines
A penny is worth a thousand? Investigating the relationship between social media and penny stocks,2019,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065138630&doi=10.1145%2f3309704&partnerID=40&md5=eceddc5e70ae1df77fa0eb7dc9a87580,"Increasingly more investors are seeking information from social media to help make investment decisions. Considering that information on penny stocks is often less reported in traditional media, investors may rely more on social media to obtain such information for investment advice. Although previous research has shown that stock opinions in traditional media is a possible predictor of stock returns, no previous research has considered the effect of the stock opinions in social media on these stocks in terms of future stock performance and the moderation effect of penny stocks. In this research, we studied the relationship between social media and the financial performance of penny stocks. We used the net proportion of positive words in stock articles in social media to help predict the future stock performance for penny stocks. The moderation effect of penny stocks on the net fraction of positive words was found to be significant in short terms, revealing a stronger relationship between social media and stock performance at lower price and market capitalization (MC) levels. Based on the findings, we proposed simple strategies utilizing social media and our measure. The results of our applications will be of interest to individual and institutional investors, shareholders, and regulators. © 2019 Association for Computing Machinery.",Penny stocks; Social media; Stock opinions; Text analytics,Investments; Financial performance; Institutional investors; Investment decisions; Market capitalization; Penny stocks; Social media; Stock opinions; Text analytics; Social networking (online)
A graph-based approach to person name disambiguation inweb,2019,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066066616&doi=10.1145%2f3314949&partnerID=40&md5=f35583515cbe98b61f42d289485ee177,"This article presents a name disambiguation approach to resolve ambiguities between person names and group web pages according to the individuals they refer to. The proposed approach exploits two important sources of entity-centric semantic information extracted from web pages, including personal attributes and social relationships. It takes as input the web pages that are results for a person name search. The web pages are analyzed to extract personal attributes and social relationships. The personal attributes and social relationships are mapped into an undirected weighted graph, called attribute-relationship graph. A graph-based clustering algorithm is proposed to group the nodes representing the web pages, each of which refers to a person entity. The outcome is a set of clusters such that the web pages within each cluster refer to the same person. We show the effectiveness of our approach by evaluating it on large-scale datasets WePS-1, WePS-2, and WePS-3. Experimental results are encouraging and show that the proposed method clearly outperforms several baseline methods and also its counterparts. © 2019 Association for Computing Machinery.",clustering; graph-based name disambiguation; Person name disambiguation; social network analysis,Clustering algorithms; Graphic methods; Large dataset; Semantics; Social aspects; Social networking (online); clustering; Graph-based clustering; Large-scale datasets; Name disambiguation; Person name disambiguations; Semantic information; Social relationships; Undirected weighted graphs; Websites
Website visual design qualities: A threefold framework,2019,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065779545&doi=10.1145%2f3309708&partnerID=40&md5=684eef7bbc9747eee8e8d94245a0a4e8,"The present study aims to contribute to the information systems (IS) literature by developing a new theoretical perspective that integrates three dimensions of artifact visual design quality-namely aesthetic, functional, and symbolic dimensions-in the investigation of website visual design qualities that influence visitors' attitudes and behaviors. Results suggest that website aesthetic, functional, and symbolic qualities positively influence intention to use the website and positive word of mouth and that website aesthetic quality positively influences website functional and symbolic qualities. Results also demonstrate that functional and symbolic qualities mediate the relationships between aesthetic quality and intention to use and positive word of mouth. © 2019 Association for Computing Machinery.",Aesthetic; Functional; Intention; Symbolic; Website design; Word of mouth,Web Design; Aesthetic; Functional; Intention; Symbolic; Word of mouth; Websites
Simulated Annealing-based Ontology Matching,2019,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065772761&doi=10.1145%2f3314948&partnerID=40&md5=3f736c61686e3e9e9e4538aab8c28a06,"Ontology alignment is a fundamental task to reconcile the heterogeneity among various information systems using distinct information sources. The evolutionary algorithms (EAs) have been already considered as the primary strategy to develop an ontology alignment system. However, such systems have two significant drawbacks: they either need a ground truth that is often unavailable, or they utilize the population-based EAs in a way that they require massive computation and memory. This article presents a new ontology alignment system, called SANOM, which uses the well-known simulated annealing as the principal technique to find the mappings between two given ontologies while no ground truth is available. In contrast to population-based EAs, the simulated annealing need not generate populations, which makes it significantly swift and memory-efficient for the ontology alignment problem. This article models the ontology alignment problem as optimizing the fitness of a state whose optimum is obtained by using the simulated annealing. A complex fitness function is developed that takes advantage of various similarity metrics including string, linguistic, and structural similarities. A randomized warm initialization is specially tailored for the simulated annealing to expedite its convergence. The experiments illustrate that SANOM is competitive with the state-of-the-art and is significantly superior to other EA-based systems. © 2019 Association for Computing Machinery.",OAEI; Ontology alignment; SANOM; Simulated annealing,Information use; Ontology; Evolutionary algorithms (EAs); Fitness functions; Information sources; OAEI; Ontology alignment; SANOM; Similarity metrics; Structural similarity; Simulated annealing
"Flexible, efficient, and secure access delegation in cloud computing",2019,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065773139&doi=10.1145%2f3318212&partnerID=40&md5=4c5025e64bf5a5bd5ab3145835a5085e,"The convenience of the cloud-assisted Internet of Things has led to the need for improved protections for the large volumes of data collected from devices around the world and stored on cloud-based servers. Proxy re-encryption (PRE) has been presented as a suitable mechanism for secure transmission and sharing of files within the cloud. However, existing PRE schemes do not support unidirectional data transformation, fine-grained controls, multiple hops, and identity-based encryption simultaneously. To solve these problems, we propose a unidirectional multi-hop identity based-conditional PRE scheme that meets all of the above requirements. Our proposal has the additional benefits of a constant ciphertext size, non-interactivity, and collusion resistance. We also prove that our scheme is secure against adaptive identity chosen-ciphertext attacks in the standard model. © 2019 Association for Computing Machinery.",Conditional proxy re-encryption; Identity-based proxy re-encryption; Internet of things; Multi-usability; Unidirectionality,Internet of things; Metadata; Security of data; Chosen ciphertext attack; Collusion resistance; Conditional proxy re encryptions; Fine-grained control; Identity Based Encryption; Multi-usability; Proxy re encryptions; Unidirectionality; Cryptography
System design through the exploration of contemporary web services,2018,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061214672&doi=10.1145%2f3273932&partnerID=40&md5=ada679897d75aec0d4bdbacdb62a2090,"In this article, we develop a Contemporary Web Service (CWS) repository of system designs, which are encoded as metadata of contemporary web services. We examine if this CWS repository serves as an effective design tool for initial CWS design and as an effective support tool for business users and analysts working together on system design. The CWS repository reduces the cognitive load of both the analyst and the business user as they jointly explore the CWS repository of system designs. It supports an evolutionary approach to system design through rapid selection of appropriate CWS metadata. To accomplish that, we introduce several new design characteristics for the CWS repository. The evaluation results demonstrate that the CWS repository is an effective tool for supporting designers during initial service design, as well as for supporting business users and analysts during system design. © 2018 Association for Computing Machinery.",contemporary web services; Design science research; repository; system design,Metadata; Petroleum reservoir evaluation; Systems analysis; Websites; Business-users; Cognitive loads; Design characteristics; Design-science researches; Evaluation results; Evolutionary approach; repository; Service design; Web services
A meta-model for information systems quality: A mixed study of the financial sector,2018,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054582214&doi=10.1145%2f3230713&partnerID=40&md5=8b29e1a24dae4c9351021a1396f37bab,"Information Systems Quality (ISQ) is a critical source of competitive advantages for organizations. In a scenario of increasing competition on digital services, ISQ is a competitive differentiation asset. In this regard, managing, maintaining, and evolving IT infrastructures have become a primary concern of organizations. Thus, a technical perspective on ISQ provides useful guidance to meet current challenges. The financial sector is paradigmatic, since it is a traditional business, with highly complex business-critical legacy systems, facing a tremendous change due to market and regulation drivers. We carried out a Mixed-Methods study, performing a Delphi-like study on the financial sector. We developed a specific research framework to pursue this vertical study. Data were collected in four phases starting with a high-level randomly stratified panel of 13 senior managers and then a target panel of 124 carefully selected and well-informed domain experts. We have identified and dealt with several quality factors; they were discussed in a comprehensive model inspired by the ISO 25010, 42010, and 12207 standards, corresponding to software quality, software architecture, and software process, respectively. Our results suggest that the relationship among quality, architecture, and process is a valuable technical perspective to explain the quality of an information system. Thus, we introduce and illustrate a novel meta-model, named SQuAP (Software Quality, Architecture, Process), which is intended to give a comprehensive picture of ISQ by abstracting and connecting detailed individual ISO models. © 2018 Association for Computing Machinery.",Delphi study; Information systems quality; Management information systems; Mixed methods; Software architecture; Software process; Software quality,Competition; Finance; Information management; Information services; Information use; Legacy systems; Management information systems; Software architecture; Competitive advantage; Comprehensive model; DELPHI study; IT infrastructures; Mixed method; Research frameworks; Software process; Software Quality; Computer software selection and evaluation
Information systems for a smart electricity grid: Emerging challenges and opportunities,2018,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054582794&doi=10.1145%2f3230712&partnerID=40&md5=5dea6b5967ec8bd3eed425064e4d0879,"The drive for sustainability as evidenced by the Paris Accords is forcing a radical re-examination of the way electricity is produced, managed, and consumed. Research on sustainable smart electricity markets is facilitating the emergence of sustainable energy systems and a revolution in the efficiency and reliability of electricity consumption, production, and distribution. Traditional electricity grids and markets are being disrupted by a range of forces, including the rise of weather-dependent and distributed renewable sources, growing consumer involvement in managing their power consumption and production, and the electrification of transport. These changes will likely bring about complex and dynamic smart electricity markets that rely on analysis of information to inform stakeholders, and on effective integration of stakeholders' actions. We outline a research agenda on how advances in information-intensive processes are fundamental for facilitating these transformations, describe the roles that such processes will play, and discuss Information Systems research challenges necessary to achieve these goals. These challenges span public policy, privacy, and security; market mechanisms; and data-driven decision support. The diverse challenges we outline also underscore that the diverse IS research perspective is instrumental for addressing the complexity and interdisciplinary nature of this research. © 2018 Association for Computing Machinery.",Demand response; Dynamic pricing; Enabling technologies; Energy informatics; Energy policy; Green IS; Price forecasting; Smart electricity markets; Smart grid; Software agents; Supply chain; Sustainability; Trading agent competition,Commerce; Competition; Decision support systems; Digital storage; Electric power generation; Electric power transmission networks; Electric power utilization; Energy policy; Information systems; Information use; Power markets; Software agents; Supply chains; Sustainable development; Demand response; Dynamic pricing; Enabling technologies; Energy informatics; Green is; Price forecasting; Smart grid; Trading Agent Competition; Smart power grids
Person-Job Fit: Adapting the right talent for the right job with joint representation learning,2018,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054558049&doi=10.1145%2f3234465&partnerID=40&md5=9bd6d2804942e4f5a01fb01c5ccfad01,"Person-Job Fit is the process of matching the right talent for the right job by identifying talent competencies that are required for the job. While many qualitative efforts have been made in related fields, it still lacks quantitative ways of measuring talent competencies as well as the job's talent requirements. To this end, in this article, we propose a novel end-to-end data-driven model based on a Convolutional Neural Network (CNN), namely, the Person-Job Fit Neural Network (PJFNN), for matching a talent qualification to the requirements of a job. To be specific, PJFNN is a bipartite neural network that can effectively learn the joint representation of Person-Job fitness from historical job applications. In particular, due to the design of a hierarchical representation structure, PJFNN can not only estimate whether a candidate fits a job but also identify which specific requirement items in the job posting are satisfied by the candidate by measuring the distances between corresponding latent representations. Finally, the extensive experiments on a large-scale real-world dataset clearly validate the performance of PJFNN in terms of Person-Job Fit prediction. Also, we provide effective data visualization to show some job and talent benchmark insights obtained by PJFNN. © 2018 Association for Computing Machinery.",Joint representation learning; Recruitment analysis,Neural networks; Convolutional Neural Networks (CNN); Data-driven model; End to end; Hierarchical representation; Job application; Job postings; Real-world; Recruitment analysis; Data visualization
Machine learning for the developing world,2018,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053510922&doi=10.1145%2f3210548&partnerID=40&md5=527e074954965a5f3ec4d7a4118324b1,"Researchers from across the social and computer sciences are increasingly using machine learning to study and address global development challenges. This article examines the burgeoning field of machine learning for the developing world (ML4D). First, we present a review of prominent literature. Next, we suggest best practices drawn from the literature for ensuring that ML4D projects are relevant to the advancement of development objectives. Finally, we discuss how developing world challenges can motivate the design of novel machine learning methodologies. This article provides insights into systematic differences between ML4D and more traditional machine learning applications. It also discusses how technical complications of ML4D can be treated as novel research questions, how ML4D can motivate new research directions, and where machine learning can be most useful. © 2018 ACM.",Developing countries; Global development,Artificial intelligence; Developing countries; Best practices; Developing world; Global development; Machine learning applications; Research questions; Learning systems
"Ecoxight: Discovery, exploration, and analysis of business ecosystems using interactive visualization",2018,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046999688&doi=10.1145%2f3185047&partnerID=40&md5=77c4bba238a9a528a22dda24737f5a7d,"The term ecosystem is used pervasively in industry, government, and academia to describe the complex, dynamic, hyperconnected nature of many social, economic, and technical systems that exist today. Ecosystems are characterized by a large, dynamic, and heterogeneous set of geospatially distributed entities that are interconnected through various types of relationships. This study describes the design and development of ecoxight, a Web-based visualization platform that provides multiple coordinated views of multipartite, multiattribute, dynamic, and geospatial ecosystem data with novel and rich interaction capabilities to augment decision makers ecosystem intelligence. The design of ecoxight was informed by an extensive multiphase field study of executives. The ecoxight platform not only provides capabilities to interactively explore and make sense of ecosystems but also provides rich visual construction capabilities to help decision makers align their mental model. We demonstrate the usability, utility, and value of our system using multiple evaluation studies with practitioners using socially curated data on the emerging application programming interface ecosystem. We report on our findings and conclude with research implications. Collectively, our study contributes to design science research at the intersection of information systems and strategy and the rapidly emerging field of visual enterprise analytics. © 2018 ACM.",Geospatial data; Multipartite graph; Network visualization; Temporal network,Application programming interfaces (API); Computer systems programming; Data visualization; Decision making; Visualization; Design-science researches; Geo-spatial data; Interactive visualizations; Multipartite graph; Multiple coordinated views; Network visualization; Temporal networks; Web-based visualization; Ecosystems
Implementing and evaluating serendipity in delivering personalized health information,2018,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053557971&doi=10.1145%2f3205849&partnerID=40&md5=c8425ef14f0478d7fc51ad1b71edae32,"Serendipity has been recognized to have the potential of enhancing unexpected information discovery. This study shows that decomposing the concept of serendipity into unexpectedness and interest is a useful way for implementing this concept. Experts' domain knowledge helps in providing serendipitous recommendation, which can be further improved by adaptively incorporating users' real-time feedback. This research also conducts an empirical user-study to analyze the influence of serendipity in a health news delivery context. A personalized filtering system named MedSDFilter was developed, on top of which serendipitous recommendation was implemented using three approaches: random, static-knowledge-based, and adaptive-knowledge-based models. The three different models were compared. The results indicate that the adaptive-knowledge-based method has the highest ability in helping people discover unexpected and interesting contents. The insights of the research will make researchers and practitioners rethink the way in which search engines and recommender systems operate to address the challenges of discovering unexpected and interesting information. The outcome will have implications for empowering ordinary people with more chances of bumping into beneficial information. © 2018 ACM 2158-656X/2018/08-ART7 $15.00",Medical information delivery; Personalized information filtering; Serendipity,Knowledge based systems; Search engines; Information discovery; Interesting information; Knowledge-based methods; Knowledge-based model; Medical information; Personalized healths; Personalized information; Serendipity; Information filtering
A modeling language for conceptual design of systems integration solutions,2018,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053511194&doi=10.1145%2f3185046&partnerID=40&md5=78ded4ef6397bab0a8320de979cf9927,"Systems integration'connecting software systems for cross-functional work'is a significant concern in many large organizations, which continue to maintain hundreds, if not thousands, of independently evolving software systems. Current approaches in this space remain ad hoc, and closely tied to technology platforms. Following a design science approach, and via multiple design-evaluate cycles, we develop Systems Integration Requirements Engineering Modeling Language (SIRE-ML) to address this problem. SIRE-ML builds on the foundation of coordination theory, and incorporates important semantic information about the systems integration domain. The article develops constructs in SIRE-ML, and a merge algorithm that allows both functional managers and integration professionals to contribute to building a systems integration solution. Integration models built with SIRE-ML provide benefits such as ensuring coverage and minimizing ambiguity, and can be used to drive implementation with different platforms such as middleware, services, and distributed objects. We evaluate SIRE-ML for ontological expressiveness and report findings about applicability check with an expert panel. The article discusses implications for future research such as tool building and empirical evaluation, as well as implications for practice. © 2018 ACM 2158-656X/2018/09-ART8 $15.00",Conceptual modeling; Design science; SIRE-ML,Conceptual design; Integral equations; Integration; Middleware; Petroleum reservoir evaluation; Semantics; Space platforms; Conceptual model; Design science; Empirical evaluations; Engineering modeling; Implications for futures; Semantic information; SIRE-ML; Technology platforms; Modeling languages
The state-of-the-art in twitter sentiment analysis: A review and benchmark evaluation,2018,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053528947&doi=10.1145%2f3185045&partnerID=40&md5=afc87a60a545717c103dcafe105dadb4,"Twitter has emerged as a major social media platform and generated great interest from sentiment analysis researchers. Despite this attention, state-of-the-art Twitter sentiment analysis approaches perform relatively poorly with reported classification accuracies often below 70%, adversely impacting applications of the derived sentiment information. In this research, we investigate the unique challenges presented by Twitter sentiment analysis and review the literature to determine how the devised approaches have addressed these challenges. To assess the state-of-the-art in Twitter sentiment analysis, we conduct a benchmark evaluation of 28 top academic and commercial systems in tweet sentiment classification across five distinctive data sets. We perform an error analysis to uncover the causes of commonly occurring classification errors. To further the evaluation, we apply select systems in an event detection case study. Finally, we summarize the key trends and takeaways from the review and benchmark evaluation and provide suggestions to guide the design of the next generation of approaches. © 2018 ACM 2158-656X/2018/08-ART5 $15.00",Benchmark evaluation; Natural language processing; Opinion mining; Sentiment analysis; Social media; Text mining; Twitter,Classification (of information); Data mining; Error analysis; Sentiment analysis; Social networking (online); Text processing; Benchmark evaluation; Classification accuracy; Classification errors; Sentiment classification; Social media; Social media platforms; Text mining; Twitter; Natural language processing systems
Blockchains for business process management - Challenges and opportunities,2018,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042875606&doi=10.1145%2f3183367&partnerID=40&md5=245e0e63066f0726b92d871b062d94db,"Blockchain technology ofers a sizable promise to rethink the way interorganizational business processes are managed because of its potential to realize execution without a central party serving as a single point of trust (and failure). To stimulate research on this promise and the limits thereof, in this article, we outline the challenges and opportunities of blockchain for business process management (BPM). We first refect how blockchains could be used in the context of the established BPM lifecycle and second how they might become relevant beyond. We conclude our discourse with a summary of seven research directions for investigating the application of blockchain technology in the context of BPM. © 2018 ACM.",Blockchain; Business process management; Research challenges,Blockchain; Enterprise resource management; Bpm lifecycle; Business process management; Interorganizational business process; Research challenges; Single point; Administrative data processing
Discovering discontinuity in big financial transaction data,2018,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042502785&doi=10.1145%2f3159445&partnerID=40&md5=540e4370b20d48093c58dc00c22db443,"Business transactions are typically recorded in the company ledger. The primary purpose of such financial information is to accompany a monthly or quarterly report for executives to make sound business decisions and strategies for the next business period. These business strategies often result in transitions that cause underlying infrastructures and components to change, including alteration in the nomenclature system of the business components. As a result, a transaction stream of an affected component would be replaced by another stream with a different component name, resulting in discontinuity of a financial stream of the same component. Recently, advancement in large-scale data mining technologies has enabled a set of critical applications to utilize knowledge extracted from a vast amount of existing data that would otherwise have been unused or underutilized. In financial and services computing domains, recent studies have illustrated that historical financial data could be used to predict future revenues and profits, optimizing costs, among other potential applications. These prediction models rely on long-term availability of the historical data that traces back for multiple years. However, the discontinuity of the financial transaction stream associated with a business component has limited the learning capability of the prediction models. In this article, we propose a set of machine learning-based algorithms to automatically discover component name replacements, using information available in general ledger databases. The algorithms are designed to be scalable for handling massive data points, especially in large companies. Furthermore, the proposed algorithms are generalizable to other domains whose data is time series and shares the same nature as the financial data available in business ledgers. A case study of real-world IBM service delivery retrieved from four different geographical regions is used to validate the efficacy of the proposed methodology. © 2018 ACM.",Classification; Machine learning; Name replacement discovery; Services delivery,Artificial intelligence; Classification (of information); Data handling; Data reduction; Finance; Forecasting; Geographical regions; Learning systems; Business decisions; Business transaction; Critical applications; Financial information; Financial transactions; Learning capabilities; Name replacement discovery; Services delivery; Data mining
"A survey of link recommendation for social networks: Methods, theoretical foundations, and future research directions",2017,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85033239979&doi=10.1145%2f3131782&partnerID=40&md5=27591bb728ab9d22991e3e52a9a3c477,"Link recommendation has attracted significant attention from both industry practitioners and academic researchers. In industry, link recommendation has become a standard and most important feature in online social networks, prominent examples of which include ""People You May Know"" on LinkedIn and ""You May Know"" on Google +. In academia, link recommendation has been and remains a highly active research area. This article surveys state-of-the-art link recommendation methods, which can be broadly categorized into learning-based methods and proximity-based methods. We further identify social and economic theories, such as social interaction theory, that underlie these methods and explain from a theoretical perspective why a link recommendation method works. Finally, we propose to extend link recommendation research in several directions that include utility-based link recommendation, diversity of link recommendation, link recommendation from incomplete data, and experimental study of link recommendation. © 2017 ACM.",Link recommendation; Network formation; Social network,Education; Surveys; Future research directions; Learning-based methods; Link recommendation; Network formation; On-line social networks; Recommendation methods; Social interactions; Theoretical foundations; Social networking (online)
Price shock detection with an influence-based model of social atention,2017,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030716303&doi=10.1145%2f3131781&partnerID=40&md5=12d759bf6406ff5440a26c2ffb43b8bb,"There has been increasing interest in exploring the impact of human behavior on financial market dynamics. One of the important related questions is whether attention from society can lead to significant stock price movements or even abnormal returns. To answer the question, we develop a new measurement of social attention, named periodic cumulative degree of social attention, by simultaneously considering the individual influence and the information propagation in social networks. Based on the vast social network data, we evaluate the new attention measurement by testing its significance in explaining future abnormal returns. In addition, we test the forecasting ability of social attention for stock price shocks, defined by the cumulative abnormal returns. Our results provide significant evidence to support the intercorrelated relationship between the social attention and future abnormal returns. The outperformance of the new approach in predicting price shocks is also confirmed by comparison with several benchmark methods. © 2017 ACM.",Abnormal return; Influence propagation; Price shock; Social attention; Social network; The Chinese stock market,Behavioral research; Commerce; Finance; Financial markets; Information dissemination; Social networking (online); Abnormal returns; Chinese stock market; Cumulative abnormal returns; Forecasting ability; Information propagation; Price shock; Social attention; Stock price movements; Costs
Are all classes created equal? Increasing precision of conceptual modeling grammars,2017,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85033240443&doi=10.1145%2f3131780&partnerID=40&md5=2e544d50a3de1a1346997e9a728e98a5,"Recent decade has seen a dramatic change in the information systems landscape that alters the ways we design and interact with information technologies, including such developments as the rise of business analytics, user-generated content, and NoSQL databases, to name just a few. These changes challenge conceptual modeling research to offer innovative solutions tailored to these environments. Conceptual models typically represent classes (categories, kinds) of objects rather than concrete specific objects, making the class construct a critical medium for capturing domain semantics. While representation of classes may differ between grammars, a common design assumption is what we term different semantics same syntax (D3S). Under D3S, all classes are depicted using the same syntactic symbols. Following recent findings in psychology, we introduce a novel assumption semantics-contingent syntax (SCS) whereby syntactic representations of classes in conceptual models may differ based on their semantic meaning. We propose a core SCS design principle and five guidelines pertinent for conceptual modeling. We believe SCS carries profound implications for theory and practice of conceptual modeling as it seeks to better support modern information environments. © 2017 ACM.",Conceptual modeling; Database design,Semantics; Syntactics; Business analytics; Conceptual model; Database design; Information environment; Innovative solutions; Syntactic representation; Theory and practice; User-generated content; Data mining
Understanding alliance portfolios using visual analytics,2017,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028838744&doi=10.1145%2f3086308&partnerID=40&md5=c4612d9cc00d1158317243b4580d18eb,"In an increasingly global and competitive business landscape, firms must collaborate and partner with others to ensure survival, growth, and innovation. Understanding the evolutionary composition of a firm's relationship portfolio and the underlying formation strategy is a difficult task given the multidimensional, temporal, and geospatial nature of the data. In collaboration with senior executives, we iteratively determine core design requirements and then design and implement an interactive visualization system that enables decision makers to gain both systemic (macro) and detailed (micro) insights into a firm's alliance activities and discover patterns of multidimensional relationship formation. Our system provides both sequential and temporal representation modes, a rich set of additive cross-linked filters, the ability to stack multiple alliance portfolios, and a dynamically updated activity state model visualization to inform decision makers of past and likely future relationship moves. We illustrate our tool with examples of alliance activities of firms listed on the S&P 500. A controlled experiment and real-world evaluation with practitioners and researchers reveals significant evidence of the value of our visual analytic tool. Our design study contributes to design science by addressing a known problem (i.e., alliance portfolio analysis) with a novel solution (interactive, pixel-based multivariate visualization) and to the rapidly emerging area of data-driven visual decision support in corporate strategy contexts. We conclude with implications and future research opportunities. © 2017 ACM.",Alliances; Strategy decision support; Visual analytics,Data visualization; Decision making; Decision support systems; Alliances; Design and implements; Interactive visualization systems; Multivariate visualization; Research opportunities; Strategy decision; Temporal representations; Visual analytics; Visualization
Power and performance estimation for fine-grained server power capping via controlling heterogeneous applications,2017,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028882438&doi=10.1145%2f3086449&partnerID=40&md5=865ff6d4b6cd4437bfc1ad0f60c1342d,"Power capping is a method to save power consumption of servers by limiting performance of the servers. Although users frequently run applications on different virtual machines (VMs) for keeping their performance and having them isolated from the other applications, power capping may degrade performance of all the applications running on the server. We present fine-grained power capping by limiting performance of each application individually. For keeping performance defined in Quality of Service (QoS) requirements, it is important to estimate applications' performance and power consumption after the fine-grained power capping is applied. We propose the estimation method of physical CPU usage when limiting virtual CPU usage of applications on VMs. On servers where multiple VMs run, VM's usage of physical CPU is interrupted by the other VMs, and a hypervisor uses physical CPU to control VMs. These VMs' and hypervisor's behaviors make it difficult to estimate performance and power consumption by straightforward methods, such as linear regression and polynomial regression. The proposed method uses Piecewise Linear Regression to estimate physical CPU usage by assuming that VM's access to physical CPU is not interrupted by the other VMs. Then we estimate how much physical CPU usage is reduced by the interruption. Because physical CPU usage is not stable soon after limiting CPU usage, the proposed method estimates a convergence value of CPU usage after many interruptions are repeated. © 2017 ACM.",Piecewise linear regression; Power consumption; Response time; Server power capping,Electric power utilization; Linear regression; Piecewise linear techniques; Quality of service; Regression analysis; Response time (computer systems); Fine-grained power; Limiting performance; Performance estimation; Piecewise linear regression; Polynomial regression; Power capping; Qualityof-service requirement (QoS); Straight-forward method; Green computing
"Value congruence, trust, and their effects on purchase intention and reservation price",2017,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028821144&doi=10.1145%2f3110939&partnerID=40&md5=55feff037395f303cfd01c674c5bc25f,"We study the roles of value congruence and trust in increasing online shoppers' intention to purchase goods and their reservation prices for these goods. Hypotheses are developed and a controlled experiment is conducted to measure subjects' value congruence with and their trust in online sellers with disparate values, along with their purchase intention and willingness to pay price premiums. Using social exchange theory, we find that, for business-to-consumer (B2C) e-commerce, value congruence increases consumer online trust, and both value congruence and online trust have direct effects on purchase intention and reservation prices. In particular, in the positive value congruence vs. value neutral case, trust has a greater effect than value congruence on purchase intention, but value congruence has a greater effect than trust on reservation price. These findings suggest that trust is essential to a consumer's intention to purchase online but value congruence can induce price premiums from potential buyers for online sellers. This implies that trust is essential to B2C e-commerce, but value congruence can be a more effective instrument for online sellers to achieve competitive advantage through value-based differentiation in the virtual marketplace. © 2017 ACM.",E-commerce; Online market; Price premium; Purchase intention; Reservation price; Trust; Value congruence,Commerce; Competition; Electronic commerce; Purchasing; Sales; Online markets; Price premiums; Purchase intention; Reservation price; Trust; Value congruence; Costs
Introduction to WITS 2015 special issue in TMIS 2017,2017,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028568277&doi=10.1145%2f3108899&partnerID=40&md5=974e754240c0bdb5991dcfb450771383,[No abstract available],,
Apriori rule-based in-app ad selection online algorithm for improving Supply-Side Platform revenues,2017,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027031451&doi=10.1145%2f3086188&partnerID=40&md5=09742b2139aebeb3a622ea83b08f710f,"Today, smartphone-based in-app advertisement forms a substantial portion of the online advertising market. In-app publishers go through ad-space aggregators known as Supply-Side Platforms (SSPs), who, in turn, act as intermediaries for ad-agency aggregators known as demand-side platforms. The SSPs face the twin issue of making ad placement decisions within an order of milliseconds, even though their revenue streams can be optimized only by a careful selection of ads that elicit appropriate user responses regarding impressions, clicks, and conversions. This article considers the SSP's perspective and presents an online algorithm that balances these two issues. Our experimental results indicate that the decision-making time generally ranges between 20 ms and 50 ms and accuracy from 1% to 10%. Further, we conduct statistical analysis comparing the theoretical complexity of the online algorithm with its empirical performance. Empirically, we observe that the time is directly proportional to the number of incoming ads and the number of online rules. © 2017 ACM.",Apriori algorithm; Demand-side platform; Integer linear programming; Online algorithm; Optimization; Rule generation; Supply-side platform,Decision making; Integer programming; Smartphones; Space platforms; Apriori algorithms; Demand-side; Integer Linear Programming; On-line algorithms; Rule generation; Supply sides; Optimization
The role clarity framework to improve requirements gathering,2017,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85033232891&doi=10.1145%2f3083726&partnerID=40&md5=bd9edff78db8a6c5a05b12b2e19bc6f9,"Incorrect and incomplete requirements have been reported as two of the top reasons for information systems (IS) project failures. In order to address these concerns, several IS analysis and design studies have focused on understanding the business needs and organizational factors prior to specifying the requirements. In this research, we add to the existing incremental solutions, such as the work system method and goal-oriented requirements engineering, by proposing the Role Clarity Framework drawn from the theories of “role dynamics” and “goal setting and task performance” in organization studies. The Role Clarity Framework consists of three main concepts related to any organizational role: expectations, activities, and consequences. Based on the interactions among different roles, this framework demonstrates how the business goals and activities of each role, as played out by IS users, are formed and/or changed in the organization. Finally, the Role Clarity Framework helps IS analysts to improve their communication with users and anticipate changes in their requirements, thus improving the gathering of requirements for IS design. © 2018 Association for Computing Machinery. All Rights Reserved.",And Phrases: Organizational role; Business activities; Changes in requirements; Communication between analysts and users; Principles; Role clarity; Role Clarity Framework,Requirements engineering; And Phrases: Organizational role; Business activities; Changes in requirements; Principles; Role clarity; Role Clarity Framework; Engineering research
Resolving ambiguity in sentiment classification: The role of dependency features,2017,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057808153&doi=10.1145%2f3046684&partnerID=40&md5=0bf53c29c056b5f462c1f5686f0eba37,"Sentiment analysis has become popular in business intelligence and analytics applications due to the great need for learning insights from the vast amounts of user generated content on the Internet. One major challenge of sentiment analysis, like most text classification tasks, is finding structures from unstructured texts. Existing sentiment analysis techniques employ the supervised learning approach and the lexicon scoring approach, both of which largely rely on the representation of a document as a collection of words and phrases. The semantic ambiguity (i.e., polysemy) of single words and the sparsity of phrases negatively affect the robustness of sentiment analysis, especially in the context of short social media texts. In this study, we propose to represent texts using dependency features. We test the effectiveness of dependency features in supervised sentiment classification. We compare our method with the current standard practice using a labeled data set containing 170,874 microblogging messages. The combination of unigram features and dependency features significantly outperformed other popular types of features. © 2017 ACM.",And Phrases: Sentiment analysis; Dependency; Feature extraction; Supervised learning; Text mining,Data mining; Feature extraction; Natural language processing systems; Semantics; Sentiment analysis; Supervised learning; Text processing; Dependency; Resolving ambiguities; Semantic ambiguities; Sentiment classification; Supervised learning approaches; Text classification; Text mining; User-generated content; Classification (of information)
Discovering design principles for health behavioral change support systems: A text mining approach,2017,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027047481&doi=10.1145%2f3055534&partnerID=40&md5=1ff4dc154902f089701061ac7ecf8186,"Behavioral Change Support Systems (BCSSs) aim to change users' behavior and lifestyle. These systems have been gaining popularity with the proliferation of wearable devices and recent advances in mobile technologies. In this article, we extend the existing literature by discovering design principles for health BCSSs based on a systematic analysis of users' feedback. Usingmobile diabetes applications as an example of Health BCSSs, we use topicmodeling to discover design principles from online user reviews. We demonstrate the importance of the design principles through analyzing their existence in users' complaints. Overall, the results highlight the necessity of going beyond the techno-centric approach used in current practice and incorporating the social and organizational features into persuasive systems design, as well as integrating with medical devices and other systems in their usage context. © 2017 ACM.",Latent Dirichlet Allocation (LDA); Mobile diabetes apps; Online user reviews; Topic mining,Biomedical equipment; Character recognition; Health; Statistics; Wearable technology; Behavioral changes; Current practices; Design Principles; Latent dirichlet allocations; Mobile Technology; Online users; Systematic analysis; Topic minings; Data mining
Business-to-consumer platform strategy: How vendor certification changes platform and seller incentives,2017,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024501811&doi=10.1145%2f3057273&partnerID=40&md5=d44825aeecbfd8602b538b19b81f72cd,"We build an economic model to study the problem of offering a new, high-certainty channel on an existing business-to-consumer platform such as Taobao and eBay. On this new channel, the platform owner exerts effort to reduce the uncertainty of service quality. Sellers can either sell through the existing low-certainty channel or go through additional screening to sell on this new channel. We model the problem as a Bertrand competition game where sellers compete on price and exert effort to provide better service to consumers. In this game, we consider a reputation spillover effect that refers to the impact of the high-certainty channel on the perceived service quality in the low-certainty channel. Counter-intuitively, we find that low-certainty channel demand will decrease as the reputation spillover effect increases, in the case of low inter-channel competition. Also, low-certainty channel demand increases as the quality uncertainty increases, in the case of intense inter-channel competition. Furthermore, the platform owner should offer a new high-certainty channel when (i) the perceived quality for this channel is sufficiently high, (ii) sellers in this channel are able to efficiently provide quality service, (iii) consumers in this channel are not so sensitive to the quality uncertainty, or (iv) the reputation spillover effect is high. In the one-channel case, the incentives of the platform owner and sellers are aligned for all model parameters. However, this is not the case for the two-channel solution, and our model reveals where tensions will arise between parties. © 2017 ACM.",B2c channel competition; E-commerce infrastructure; Incentive alignment; Online shopping; Reputation spillover; Vetted sellers,Electronic commerce; Channel competitions; Incentive alignment; Online shopping; Reputation spillover; Vetted sellers; Quality of service
Using social sensors for detecting emergency events: A case of power outages in the electrical utility industry,2017,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045853106&doi=10.1145%2f3052931&partnerID=40&md5=3aa4bd06483d521f01c190b468ebdccf,"This article presents a novel approach to detecting emergency events, such as power outages, that utilizes social media users as “social sensors” for virtual detection of such events. The proposed new method is based on the analysis of the Twitter data that leads to the detection of Twitter discussions about these emergency events. The method described in the article was implemented and deployed by one of the vendors in the context of detecting power outages as a part of their comprehensive social engagement platform. It was also field tested on Twitter users in an industrial setting and performed well during these tests. © 2017 ACM.",And Phrases: Social media; Event detection; Power outages; Social sensors; Tweets,Social networking (online); Event detection; Power outage; Social media; Social sensors; Tweets; Outages
"Investigating the Relationships Between the Use Contexts, User Perceived Values, and Loyalty to a Software Product",2017,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019552035&doi=10.1145%2f3057271&partnerID=40&md5=cde55f5bc50153da892342f520368181,"In this study, we propose that software products provide three types of value-utilitarian, hedonic, and social-That impact user loyalty. Although the Technology Acceptance Model (TAM) has focused on the user impacts of utilitarian and hedonic values provided by utilitarian and hedonic software products on system use, the impact of social value provided by the software products in general have been largely ignored. The results of a longitudinal study with actual users of three types of software products show that all three types of software products-utilitarian (Producteev), hedonic (Kerbal), and social (Facebook)-provide significant but varying degrees of all three types of values. Further, the value derived by the users' primary use context moderated the impact of the secondary values provided by the software product to the users on their loyalty for the product. © 2017 ACM.",Hedonic value; Social value; User loyalty; Utilitarian value,Hardware; Information systems; Hedonic values; Longitudinal study; Perceived value; Social values; Software products; Technology acceptance model; User loyalty; Utilitarian value; Economic and social effects
Do health information exchanges deter repetition of medical services?,2017,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019579339&doi=10.1145%2f3057272&partnerID=40&md5=63ca7bb8923175741b47535c1b464df2,"Repetition of medical services by providers is one of the major sources of healthcare costs. The lack of access to previous medical information on a patient at the point of care often leads a physician to perform medical procedures that have already been done. Multiple healthcare initiatives and legislation at both the federal and state levels have mandated Health Information Exchange (HIE) systems to address this problem. This study aims to assess the extent to which HIE could reduce these repetitions, using data from Centers for Medicare & Medicaid Services and a regional HIE organization. A 2-Stage Least Square model is developed to predict the impact of HIE on repetitions of two classes of procedures: diagnostic and therapeutic. The first stage is a predictive analytic model that estimates the duration of tenure of each HIE member-practice. Based on these estimates, the second stage predicts the effect of providers' HIE tenure on their repetition of medical services. The model incorporates moderating effects of a federal quality assurance program and the complexity of medical procedures with a set of control variables. Our analyses show that a practice's tenure with HIE significantly lowers the repetition of therapeutic medical procedures, while diagnostic procedures are not impacted. The medical reasons for the effects observed in each class of procedures are discussed. The results will inform healthcare policymakers and provide insights on the business models of HIE platforms. © 2017 ACM.",2sls model; Diagnostic procedures; Health information exchanges; Healthcare cost; Healthcare quality; Repetition of medical procedures; Therapeutic procedures,Health; Health care; Health insurance; Information dissemination; Laws and legislation; Least squares approximations; Quality assurance; Diagnostic procedure; Health care costs; Health information exchanges; Healthcare quality; Medical procedures; Therapeutic procedures; Diagnosis
Mining resource profiles from event logs,2017,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017600447&doi=10.1145%2f3041218&partnerID=40&md5=a52f733ab2c95bf90b5a527d33a82eee,"In most business processes, several activities need to be executed by human resources and cannot be fully automated. To evaluate resource performance and identify best practices as well as opportunities for improvement, managers need objective information about resource behaviors. Companies often use information systems to support their processes, and these systems record information about process execution in event logs. We present a framework for analyzing and evaluating resource behavior through mining such event logs. The framework provides (1) a method for extracting descriptive information about resource skills, utilization, preferences, productivity, and collaboration patterns; (2) a method for analyzing relationships between different resource behaviors and outcomes; and (3) a method for evaluating the overall resource productivity, tracking its changes over time, and comparing it to the productivity of other resources. To demonstrate the applicability of our framework, we apply it to analyze employee behavior in an Australian company and evaluate its usefulness by a survey among industry managers. © 2017 ACM.",Event log; Evidence-based performance evaluation; Mining resource behavior; Resource profile,Managers; Collaboration patterns; Descriptive information; Event log; Evidence-based; Objective information; Record information; Resource productivity; Resource profile; Productivity
Development and evaluation of a similarity measure for medical event sequences,2017,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054270124&doi=10.1145%2f3070684&partnerID=40&md5=c43bfedf837b91cfc4a56a28fe2fa54d,"We develop a similarity measure for medical event sequences (MESs) and empirically evaluate it using U.S. Medicare claims data. Existing similarity measures do not use unique characteristics of MESs and have never been evaluated on real MESs. Our similarity measure, the Optimal Temporal Common Subsequence for Medical Event Sequences (OTCS-MES), provides a matching component that integrates event prevalence, event duplication, and hierarchical coding, important elements of MESs. The OTCS-MES also uses normalization to mitigate the impact of heavy positive skew of matching events and compact distribution of event prevalence. We empirically evaluate the OTCS-MES measure against two other measures specifically designed for MESs, the original OTCS and Artemis, a measure incorporating event alignment. Our evaluation uses two substantial data sets of Medicare claims data containing inpatient and outpatient sequences with different medical event coding. We find a small overlap in nearest neighbors among the three similarity measures, demonstrating the superior design of the OTCS-MES with its emphasis on unique aspects of MESs. The evaluation also provides evidence about the impact of component weights, neighborhood size, and sequence length. © 2017 ACM.",Medical event sequence; Nearest neighbor search; Otcs-Mes; Similarity measure,Health insurance; Nearest neighbor search; Common subsequence; Event sequence; Hierarchical coding; Medicare claims data; Nearest neighbors; Neighborhood size; Otcs-Mes; Similarity measure; Real time systems
Are sponsored links effective? Investigating the impact of trust in search engine advertising,2017,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011372875&doi=10.1145%2f3023365&partnerID=40&md5=3e20d46c8f7651ecc2bf13d1a07838ea,"As information on the Internet grows exponentially, online users primarily rely on search engines (SEs) to locate e-commerce sites for online shopping. To generate revenue while providing free service to users, SE companies offer sponsored link (SL) placements to e-commerce sites that want to appear in the first SE results page. However, the lack of users' trust in SE advertising indicates that SEs should utilize strategies to project trustworthiness on this mechanism. Despite these insights, the role of users' trust in the operation of SE advertising is still an unexplored territory. To address this issue, a theoretical model was synthesized from the social psychology literature, the marketing literature, and the trust literature to investigate the factors that may pose impacts on the effectiveness of SE advertising by influencing users' perception of both cognitive and emotional trust. A laboratory experiment was conducted. The findings document the importance of incorporating emotional components of trust in the study of online communication by showing that emotional dimension of trust is different from and complementary to cognitive trust in facilitating online communication. The findings also provide valuable implications for practitioners to design and provide more effective SLs that can benefit all parties involved. © 2017 ACM.",Cognitive trust; Emotional trust; Online advertising effectiveness; Trust in online advertising,Behavioral research; Commerce; Electronic commerce; Marketing; Cognitive trust; Emotional dimensions; Emotional trusts; Laboratory experiments; On-line communication; Online advertising; Theoretical modeling; Users' perception; Search engines
A double-layer neural network framework for high-frequency forecasting,2017,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011370899&doi=10.1145%2f3021380&partnerID=40&md5=6e092f4d2eb8977334b829d67a9bdc85,"Nowadays, machine trading contributes significantly to activities in the equity market, and forecasting market movement under high-frequency scenario has become an important topic in finance. A key challenge in high-frequency market forecasting is modeling the dependency structure among stocks and business sectors, with their high dimensionality and the requirement of computational efficiency. As a group of powerful models, neural networks (NNs) have been used to capture the complex structure in many studies. However, most existing applications of NNs only focus on forecasting with daily or monthly data, not with minute-level data that usually contains more noises. In this article, we propose a novel double-layer neural (DNN) network for high-frequency forecasting, with links specially designed to capture dependence structures among stock returns within different business sectors. Various important technical indicators are also included at different layers of the DNN framework. Our model framework allows update over time to achieve the best goodness-of-fit with the most recent data. The model performance is tested based on 100 stocks with the largest capitals from the S&P 500. The results show that the proposed framework outperforms benchmark methods in terms of the prediction accuracy and returns. Our method will help in financial analysis and trading strategy designs. © 2017 ACM.",High-frequency forecasting; Neural networks; S&P 500,Commerce; Complex networks; Computational efficiency; Financial markets; Forecasting; Investments; Network layers; Neural networks; Dependence structures; Dependency structures; High dimensionality; High frequency HF; Network frameworks; Neural networks (NNS); Prediction accuracy; Technical indicator; Electronic trading
On optimal employee assignment in constrained role-based access control systems,2016,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85007578592&doi=10.1145%2f2996470&partnerID=40&md5=69c0c1e6c9c18a48c1b1315b253d6792,"Since any organizational environment is typically resource constrained, especially in terms of human capital, organization managers would like to maximize the utilization of available human resources. However, tasks cannot simply be assigned to arbitrary employees since the employee needs to have the necessary capabilities for executing a task. Furthermore, security policies constrain the assignment of tasks to employees, especially given the other tasks assigned to the same employee. Since role-based access control (RBAC) is the most commonly used access control model for commercial information systems, we limit our attention to consider constraints in RBAC. In this article, we define the Employee Assignment Problem (EAP), which aims to identify an employee to role assignment such that it permits the maximal flexibility in assigning tasks to employees while ensuring that the required security constraints are met. We prove that finding an optimal solution is NP-complete and therefore provide a greedy solution. Experimental evaluation of the proposed approach shows that it is both efficient and effective.",Graph Coloring; Greedy Algorithm; Role-Based Access Control (Rbac); Statically Mutually Exclusive Roles (Smer) Constraint,Combinatorial optimization; Optimization; Access control models; Commercial information systems; Experimental evaluation; Graph colorings; Greedy algorithms; Role-based Access Control; Security constraint; Statically Mutually Exclusive Roles (Smer) Constraint; Access control
Discovering finance keywords via continuous-space language models,2016,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84992222080&doi=10.1145%2f2948072&partnerID=40&md5=92c8a6aaa3debf78a1b878d582ef89fd,"The growing amount of public financial data makes it increasingly important to learn how to discover valuable information for financial decision making. This article proposes an approach to discovering financial keywords from a large number of financial reports. In particular, we apply the continuous bag-of-words (CBOW) model, a well-known continuous-space language model, to the textual information in 10-K financial reports to discover new finance keywords. In order to capture word meanings to better locate financial terms, we also present a novel technique to incorporate syntactic information into the CBOW model. Experimental results on four prediction tasks using the discovered keywords demonstrate that our approach is effective for discovering predictability keywords for post-event volatility, stock volatility, abnormal trading volume, and excess return predictions. We also analyze the discovered keywords that attest to the ability of the proposed method to capture both syntactic and contextual information between words. This shows the success of this method when applied to the field of finance. © 2016, Association for Computing Machinery. All rights reserved.",Continuous-space language model; Finance; Text mining,Computational linguistics; Data mining; Decision making; Information retrieval; Syntactics; Contextual information; Continuous spaces; Financial decisions; Financial reports; Language model; Syntactic information; Text mining; Textual information; Finance
Resource management for business process scheduling in the presence of availability constraints,2016,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84992215152&doi=10.1145%2f2990197&partnerID=40&md5=a4ccdaa2bf38c0bf183ff73016e4e2d0,"In the context of business process management, the resources required by business processes, such as workshop staff, manufacturing machines, etc., tend to follow certain availability patterns, due to maintenance cycles, work shifts and other factors. Such availability patterns heavily influence the efficiency and effectiveness of enterprise resource management. Most existing process scheduling and resource management approaches tend to tune the process structure to seek better resource utilisation, yet neglect the constraints on resource availability. In this article, we investigate the scheduling of business process instances in accordance with resource availability patterns, to find out how enterprise resources can be rationally and sufficiently used. Three heuristic-based planning strategies are proposed to maximise the process instance throughput together with another strategy based on a genetic algorithm. The performance of these strategies has been evaluated by conducting experiments of different settings and analysing the strategy characteristics. © 2016 ACM.",Business process management; Business process scheduling; Resource management; Resource planning,Administrative data processing; Availability; Genetic algorithms; Human resource management; Natural resources management; Resource allocation; Scheduling; Availability constraints; Business Process; Business process management; Enterprise resources; Manufacturing machine; Resource availability; Resource management; Resource planning; Enterprise resource management
Are stakeholders the only source of information for requirements engineers? Toward a taxonomy of elicitation information sources,2016,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84992204727&doi=10.1145%2f2965085&partnerID=40&md5=e25c4d88e066b95e23f7b795f5d3421b,"Requirements elicitation consists in collecting and documenting information about the requirements from a system-to-be and about the environment of that system. Elicitation forms a critical step in the design of any information system, subject to many challenges like information incompleteness, variability, or ambiguity. To deal with these challenges, requirements engineers heavily rely on stakeholders, who turn out to be one of the most significant provider of information during elicitation. Sometimes, this comes at the cost of less attention being paid by engineers to other sources of information accessible in a business. In this article, we try to deal with this issue by studying the different sources of information that can be used by engineers when designing a system. We propose TELIS (a Taxonomy of Elicitation Sources), which can be used during elicitation to review more systematically the sources of information about a system-to-be. TELIS was produced through a series of empirical studies and was partially validated through a real-world case study. Our objective in this article is to increase the awareness of engineers about the other information providers within a business. Ultimately, we believe our taxonomy may help in better dealing with classical elicitation challenges and increase the chances of successful information systems design. © 2016 ACM.",Elicitation; Empirical study; Exploratory research; Information sources; Requirements engineering; Taxonomy,Information systems; Requirements engineering; Taxonomies; Elicitation; Empirical studies; Exploratory research; Information incompleteness; Information provider; Information sources; Requirements elicitation; Sources of informations; Engineers
A web service negotiation management and QoS dependency modeling framework,2016,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84974622999&doi=10.1145%2f2893187&partnerID=40&md5=f7710fbec42d21419963823a5727a7a4,"Information Management Systems that outsource part of the functionality to other (likely unknown) services need an effective way to communicate with these services, so that a mutually beneficial solution can be generated. This includes bargaining for their optimal customizations and the discovery of overlooked potential solutions. In this article, we present an automated negotiation framework for information systems (denoted as WebNeg) that can be used by both the parties for conducting negotiations. WebNeg uses a Genetic Algorithm (GA)-based approach for finding acceptable solutions in multiparty and multiobjective scenarios. The GA is enhanced using a new operator called Norm, which represents the cumulative knowledge of all the parties involved in the negotiation process. Norm incorporates the dependencies of different quality attributes of independently developed component services for the system composition. This enables WebNeg to find a better solution in the context of the current requirements. Experiment results indicate the applicability and improved performance of WebNeg (in comparison with existing similar works) in facilitating the negotiation management involved in a web service-based information composition process. © 2016 ACM.",Genetic algorithm; Negotiation management; Web service,Genetic algorithms; Information management; Websites; Automated negotiations; Information management systems; Multi objective; Negotiation process; Qos dependencies; Quality attributes; System composition; Web service negotiation; Web services
Systemic concentration in sponsored search markets: The role of time window in click-through-rate computation,2016,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050537209&doi=10.1145%2f2934695&partnerID=40&md5=fc0eff2b3d339198ebd44b700c84709f,"Keyword-based search engine advertising markets on the Internet, referred to as Sponsored Search Markets (SSMs), have reduced entry barriers to advertising for niche players. Known empirical research, though scant and emerging, suggests that while these markets provided niche firms with greater access, they do exhibit high levels of concentration-a phenomenon that warrants further study. This research, using agent-based simulation of SSM, investigates the role of “market rules” and “advertiser practices” in generating emergent click share heterogeneity among advertisers in an industry. SSMs often rank ads based on the click-through rate (CTR) that gives rise to reinforcing dynamics at an individual keyword level. In the presence of spillovers arising from advertisers' practice of managing keyword bids with a cost cap operating on the keyword portfolio, these reinforcing dynamics can endogenously generate industry-level concentration. Analysis of counterfactual markets with different window sizes used to compute CTR reveals that industry-level concentration bears an inverted-“U” relationship with window size. © 2016 ACM.",Agent-based models; Auctions; Concentration; Electronic markets; Sponsored search; User response,Autonomous agents; Computational methods; Concentration (process); Electronic trading; Intelligent agents; Marketing; Reinforcement; Search engines; Agent-based model; Auctions; Electronic market; Sponsored searches; User response; Commerce
Is combining contextual and behavioral targeting strategies effective in online advertising?,2016,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960963783&doi=10.1145%2f2883816&partnerID=40&md5=305afae602364af56cada25d83e21464,"Online targeting has been increasingly used to deliver ads to consumers. But discovering how to target the most valuable web visitors and generate a high response rate is still a challenge for advertising intermediaries and advertisers. The purpose of this study is to examine how behavioral targeting (BT) impacts users' responses to online ads and particularly whether BT works better in combination with contextual targeting (CT). Using a large, individual-level clickstream data set of an automobile advertising campaign from an Internet advertising intermediary, this study examines the impact of BT and CT strategies on users' click behavior. The results show that (1) targeting a user with behavioral characteristics that are closely related to ads does not necessarily increase the click through rates (CTRs); whereas, targeting a user with behavioral characteristics that are loosely related to ads leads to a higher CTR, and (2) BT and CT work better in combination. Our study contributes to online advertising design literature and provides important managerial implications for advertising intermediaries and advertisers on targeting individual users. © 2016 ACM.",Behavioral Targeting; Contextual Targeting; Online Advertising; Targeted Advertising,Behavioral research; Advertising campaign; Behavioral characteristics; Behavioral Targeting; Contextual Targeting; Internet advertising; Managerial implications; Online advertising; Targeted advertising; Marketing
A real-time audit mechanism based on the compression technique,2016,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84963877036&doi=10.1145%2f2629569&partnerID=40&md5=60f929011fa84fa6828787ff4afefa11,"Log management and log auditing have become increasingly crucial for enterprises in this era of information and technology explosion. The log analysis technique is useful for discovering possible problems in business processes and preventing illegal-intrusion attempts and data-tampering attacks. Because of the complexity of the dynamically changing environment, auditing a tremendous number of data is a challenging issue. We provide a real-time audit mechanism to improve the aforementioned problems in log auditing. This mechanismwas developed based on the Lempel-Ziv-Welch (LZW) compression technique to facilitate effective compression and provide reliable auditing log entries. The mechanism can be used to predict unusual activities when compressing the log data according to pre-defined auditing rules. Auditors using real-time and continuous monitoring can perceive instantly the most likely anomalies or exceptions that could cause problems.We also designed a user interface that allows auditors to define the various compression and audit parameters, using real log cases in the experiment to verify the feasibility and effectiveness of this proposed audit mechanism. In summary, this mechanism changes the log access method and improves the efficiency of log analysis. This mechanism greatly simplifies auditing so that auditors must only trace the sources and causes of the problems related to the detected anomalies. This greatly reduces the processing time of analytical audit procedures and the manual checking time, and improves the log audit efficiency. © 2016 ACM.",Continuous auditing; Dictionarybased compression; LZW algorithm; Real time log audit,Efficiency; User interfaces; Changing environment; Compression techniques; Continuous auditing; Continuous monitoring; Dictionary-based compressions; Information and technologies; LZW algorithms; Real time; Data compression
Service and financial performance,2016,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960965518&doi=10.1145%2f2875444&partnerID=40&md5=9e6d4404ebb4b56fb03006e4ffb8266d,"Customer service is an important competitive lever for the modern firm. At the same time, the continuous evolution and performance improvements in information technology (IT) capabilities have enabled the utilization of multichannel service delivery strategies. Our research focuses on IT-enabled customer service systems (CSS) and their effect on firm performance. Previous studies have failed to find a consensus on the effect of a new self-service channel on the firm's performance. We argue that the embedded assumptions underpinning the previous research are responsible for these mixed findings. Consequently, using archival data from 169 hotels affiliated with a hotel chain, we designed a longitudinal multichannel study to resolve some of these inconsistencies. Our results illustrate that when firms implement an IT-enabled self-service channel to complement their existing customer service infrastructure, they experience an early negative effect on financial performance due to the disruption of the service processes. Thus, the multichannel CSS generates a positive effect only when the new process becomes a stable part of the organizational procedures. Our findings suggest that researchers evaluate the effect of a technological initiative after the new business process has been stabilized and consider that an additional IT-enabled self-service channel rarely operates in isolation. © 2016 ACM.",Customer service systems; Multichannel; Process disruption,Hotels; Management science; Business Process; Customer service systems; Customer services; Financial performance; Firm Performance; Multi-channel services; Multichannel; Service channels; Sales
Universal artifacts: A new approach to business process management (BPM) systems,2016,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964456733&doi=10.1145%2f2886104&partnerID=40&md5=cfd43408188f701c1f7cf77cd720fbe6,"In most BPM systems (a.k.a. workflow systems), the data for process execution is scattered across databases for enterprise, auxiliary local data stores within the BPM systems, and even file systems (e.g., specification of process models). The interleaving nature of data management and BP execution and the lack of a coherent conceptual data model for all data needed for execution make it hard for (1) providing Business-Processas- A-Service (BPaaS) and (2) effective support for collaboration between business processes. The primary reason is that an enormous effort is required for maintaining both the engines and the data for the client applications. In particular, different modeling languages and different BPM systems make process interoperation one of the toughest challenges. In this article, we formulate a concept of a ""universal artifact,"" which extends artifact-centric models by capturing all needed data for a process instance throughout its execution. A framework called SeGA based on universal artifacts is developed to support separation of data and BP execution, a key principle for BPM systems. We demonstrate in this article that SeGA is versatile enough to fully facilitate not only executions of individual processes (to support BPaaS) but also various collaboration models. Moreover, SeGA reduces the complexity in runtime management including runtime querying, constraints enforcement, and dynamic modification upon collaboration across possibly different BPM systems. ©2016 ACM.",BPaaS; Process collaboration; Workflow systems,Enterprise resource management; Information management; Modeling languages; BPaaS; Business process management; Collaboration models; Conceptual data modeling; Constraints enforcement; Dynamic modifications; Process collaboration; Work-flow systems; Administrative data processing
Peripheral developer participation in open source projects: An empirical analysis,2016,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84957012771&doi=10.1145%2f2820618&partnerID=40&md5=c38c5dacf62797066a60d98c97e3119e,"The success of the Open Source model of software development depends on the voluntary participation of external developers (the peripheral developers), a group that can have distinct motivations from that of project founders (the core developers). In this study, we examine peripheral developer participation by empirically examining approximately 2,600 open source projects. In particular, we hypothesize that peripheral developer participation is higher when the potential for building reputation by gaining recognition from project stakeholders is higher. We consider recognition by internal stakeholders (such as core developers) and external stakeholders (such as end-users and peers). We find a positive association between peripheral developer participation and the potential of stakeholder recognition after controlling for bug reports, feature requests, and other key factors. Our findings provide important insights for OSS founders and corporate managers for open sourcing or OSS adoption decisions. © 2016 ACM.",Code ownership; Open source software; Project management; Software metrics,Open systems; Program debugging; Project management; Software design; Software engineering; Code ownership; Developer participations; Empirical analysis; External stakeholders; Open source projects; Project stakeholders; Software metrics; Voluntary participation; Open source software
"Service ratio-optimal, content coherence-aware data push systems",2016,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84957036252&doi=10.1145%2f2850423&partnerID=40&md5=61f4317597be0bd11143e5aee3ae2b47,"Advertising new information to users via push is the trigger of operation formany contemporary information systems. Furthermore, passive optical networks are expected to extend the reachability of high-quality push services to thousands of clients. The efficiency of a push service is the ratio of successfully informed users. However, pushing only data of high popularity can degrade the thematic coherency of the content. The present work offers a novel, analysis-derived, tunable way for selecting data for push services. The proposed scheme canmaximize the service ratio of a push system with regard to data coherence constraints. Extensive simulations demonstrate the efficiency of the scheme compared to alternative solutions. The proposed scheme is the first to tackle the problem of data coherence-aware, service ratio optimization of push services. © 2016 ACM.",Coherence; Data push; Data selection; Service ratio,Coherent light; Efficiency; Alternative solutions; Data coherence; Data push; Data Selection; Extensive simulations; High quality; Reachability; Service ratio; Passive optical networks
Mining Multimorbidity Trajectories and Co-Medication Effects from Patient Data to Predict Post-Hip Fracture Outcomes,2024,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85197453897&doi=10.1145%2f3665250&partnerID=40&md5=38bfbc7b45a4d28048f4a931b3cd805b,"Hip fractures have profound impacts on patients' conditions and quality of life, even when they receive therapeutic treatments. Many patients face the risk of poor prognosis, physical impairment, and even mortality, especially older patients. Accurate patient outcome estimates after an initial fracture are critical to physicians' decision-making and patient management. Effective predictions might benefit from analyses of patients' multimorbidity trajectories and medication usages. If adequately modeled and analyzed, then they could help identify patients at higher risk of recurrent fractures or mortality. Most analytics methods overlook the onset, co-occurrence, and temporal sequence of distinct chronic diseases in the trajectory, and they also seldom consider the combined effects of different medications. To support effective predictions, we develop a novel deep learning-based method that uses a cross-attention mechanism to model patient progression by obtaining ""contextual information""from multimorbidity trajectories. This method also incorporates a nested self-attention network that captures the combined effects of distinct medications by learning the interactions among medications and how dosages might influence post-fracture outcomes. A real-world patient dataset is used to evaluate the proposed method, relative to six benchmark methods. The comparative results indicate that our method consistently outperforms all the benchmarks in precision, recall, F-measures, and area under the curve. The proposed method is generalizable and can be implemented as a decision support system to identify patients at greater risk of recurrent hip fractures or mortality, which should help clinical decision-making and patient management.  Copyright © 2024 held by the owner/author(s). Publication rights licensed to ACM.",Clinical decision making; decision support systems; deep learning; hip fractures; patient management; patient outcome predictions,Decision making; Decision support systems; Deep learning; Diagnosis; Forecasting; Fracture; Hospital data processing; Learning systems; Clinical decision making; Combined effect; Decision making managements; Deep learning; Hip fracture; Outcome prediction; Patient data; Patient management; Patient outcome prediction; Patients' conditions; Trajectories
A Psycholinguistics-inspired Method to Counter IP Theft Using Fake Documents,2024,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85197389231&doi=10.1145%2f3651313&partnerID=40&md5=0c6254dafd6e9551e710a109ecf8cc17,"Intellectual property (IP) theft is a growing problem. We build on prior work to deter IP theft by generating n fake versions of a technical document so a thief has to expend time and effort in identifying the correct document. Our new SbFAKE framework proposes, for the first time, a novel combination of language processing, optimization, and the psycholinguistic concept of surprisal to generate a set of such fakes. We start by combining psycholinguistic-based surprisal scores and optimization to generate two bilevel surprisal optimization problems (an Explicit one and a simpler Implicit one) whose solutions correspond directly to the desired set of fakes. As bilevel problems are usually hard to solve, we then show that these two bilevel surprisal optimization problems can each be reduced to equivalent surprisal-based linear programs. We performed detailed parameter tuning experiments and identified the best parameters for each of these algorithms. We then tested these two variants of SbFAKE (with their best parameter settings) against the best performing prior work in the field. Our experiments show that SbFAKE is able to more effectively generate convincing fakes than past work. In addition, we show that replacing words in an original document with words having similar surprisal scores generates greater levels of deception.  Copyright © 2024 held by the owner/author(s). Publication rights licensed to ACM.",AI for security; fake document generation,Linear programming; Linguistics; AI for security; Bilevel; Document generation; Fake document generation; Language processing; Optimisations; Optimization problems; Processing optimizations; Simple++; Technical documents; Crime
Co-occurrence Order-preserving Pattern Mining with Keypoint Alignment for Time Series,2024,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85197369213&doi=10.1145%2f3658450&partnerID=40&md5=97483e387a238c04177cb7f662c083ee,"Recently, order-preserving pattern (OPP) mining has been proposed to discover some patterns, which can be seen as trend changes in time series. Although existing OPP mining algorithms have achieved satisfactory performance, they discover all frequent patterns. However, in some cases, users focus on a particular trend and its associated trends. To efficiently discover trend information related to a specific prefix pattern, this article addresses the issue of co-occurrence OPP mining (COP) and proposes an algorithm named COP-Miner to discover COPs from historical time series. COP-Miner consists of three parts: extracting keypoints, preparation stage, and iteratively calculating supports and mining frequent COPs. Extracting keypoints is used to obtain local extreme points of patterns and time series. The preparation stage is designed to prepare for the first round of mining, which contains four steps: obtaining the suffix OPP of the keypoint sub-time series, calculating the occurrences of the suffix OPP, verifying the occurrences of the keypoint sub-time series, and calculating the occurrences of all fusion patterns of the keypoint sub-time series. To further improve the efficiency of support calculation, we propose a support calculation method with an ending strategy that uses the occurrences of prefix and suffix patterns to calculate the occurrences of superpatterns. Experimental results indicate that COP-Miner outperforms the other competing algorithms in running time and scalability. Moreover, COPs with keypoint alignment yield better prediction performance.  Copyright © 2024 held by the owner/author(s). Publication rights licensed to ACM.",co-occurrence pattern; keypoint alignment; order-preserving; Pattern mining; time series,Alignment; Data mining; Iterative methods; Co-occurrence; Co-occurrence pattern; Keypoint alignment; Keypoints; Order preserving; Pattern mining; Pattern mining algorithms; Performance; Times series; User focus; Time series
Estimating Future Financial Development of Urban Areas for Deploying Bank Branches: A Local-Regional Interpretable Model,2024,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85197467842&doi=10.1145%2f3656479&partnerID=40&md5=a161e7d3e4e8cc818caa127dfcfa07c6,"Financial forecasting is an important task for urban development. In this article, we propose a novel deep learning framework to predict the future financial potential of urban spaces. To be more precise, our target is to infer the number of financial institutions in the future for any arbitrary location with environmental and geographical data. We propose a novel local-regional model, the Local-Regional Interpretable Multi-Attention (LIMA) model, which considers multiple aspects of a location—the place itself and its surroundings. Besides, our model offers three kinds of interpretability, providing a superior way for decision makers to understand how the model determines the prediction: critical rules learned from the tree-based module, surrounding locations that are highly correlated with the prediction, and critical regional features. Our module not only takes advantage of a tree-based model, which can effectively extract cross features, but also leverages convolutional neural networks to obtain more complex and inclusive features around the target location. Experimental results on real-world datasets demonstrate the superiority of our proposed LIMA model against existing state-of-the-art methods. The LIMA model has been deployed as a web system for assisting one of the largest bank companies in Taiwan to select locations for building new branches in major cities since 2020. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",attention mechanism; financial development prediction; Multi-task learning,Convolutional neural networks; Decision making; Deep learning; Finance; Learning systems; Location; Urban growth; Attention mechanisms; Attention model; Bank branches; Financial development; Financial development prediction; Financial forecasting; Learning frameworks; Multitask learning; Urban areas; Urban development; Forecasting
Design with Simon's Inner and Outer Environments: Theoretical Foundations for Design Science Research Methods for Digital Science,2024,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189085856&doi=10.1145%2f3640819&partnerID=40&md5=86a4999229d991bfccf0f5c5b8013eff,"Design science research has traditionally been applied to complex real-world problems to produce an artifact to address such problems. Although design science research efforts have been applied traditionally to business or related problems, there is a large set of problems in the area of digital science that also require important, digital artifacts. The digitalization of science has resulted in the need to develop essential, specialized, devices and software before it is feasible for scientists to carry out their work. This research examines digital science to identify its challenges and demonstrate how it can be possible to progress digital science with design science research, thereby establishing digital science as an important area of transdisciplinary inquiry. These areas of research are examined for their synergies and explained by positioning artifact development challenges with respect to Simon's inner and outer environments and the interface between them.  © 2024 Copyright held by the owner/author(s).",Additional Key Words and PhrasesDesign science digital science; computational science; digitalization of science; exhaust artifact; inner environment; outer environment; Simon's boundaries; transdisciplinary design science research,Design; Engineering research; Additional key word and phrasesdesign science digital science; Computational science; Design-science researches; Digitalization of science; Exhaust artifact; Inner environment; Key words; Outer environment; Simon boundary; Transdisciplinary design science research; Computer programming
The Data Product-service Composition Frontier: A Hybrid Learning Approach,2024,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189242944&doi=10.1145%2f3649319&partnerID=40&md5=955b553c7d6452c4d3560d3de32c1b73,"The service dominant logic is a base concept behind modern economies and software products, with service composition being a well-known practice for companies to gain a competitive edge over others by joining differentiated services together, typically assembled according to a number of features. At the other end of the spectrum, product compositions are a marketing device to sell products together in bundles that often augment the value for the customer, e.g., with suggested product interactions, sharing, and so on. Unfortunately, currently each of these two streams - namely, product and service composition - are carried out and delivered individually in splendid isolation: anything is being offered as a product and as a service, disjointly. We argue that the next wave of services computing features more and more service fusion with physical counterparts as well as data around them. Therefore a need emerges to investigate the interactive engagement of both (data) products and services. This manuscript offers a real-life implementation in support of this argument, using (1) genetic algorithms (GA) to shape product-service clusters, (2) end-user feedback to make the GAs interactive with a data-driven fashion, and (3) a hybridized approach which factors into our solution an ensemble machine-learning method considering additional features. All this research was conducted in an industrial environment. With such a cross-fertilized, data-driven, and multi-disciplinary approach, practitioners from both fields may benefit from their mutual state of the art as well as learn new strategies for product, service, and data product-service placement for increased value to the customer as well as the service provider. Results show promise but also highlight plenty of avenues for further research.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesAnomaly detection; literature review; time series; unsupervised,Computation theory; Industrial research; Machine learning; Quality of service; Sales; Additional key word and phrasesanomaly detection; Data products; Key words; Literature reviews; Product and services; Product composition; Product service; Services composition; Times series; Unsupervised; Genetic algorithms
A Process Mining Method for Inter-organizational Business Process Integration,2024,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189104818&doi=10.1145%2f3638062&partnerID=40&md5=1fc973e9efdaa36881f19dd1c75e589e,"Business process integration (BPI) allows organizations to connect and automate their business processes in order to deliver the right economic resources at the right time, place, and price. BPI requires the integration of business processes and their supporting systems across multiple autonomous organizations. However, such integration is complex and can face coordination complexities that occur during the resource exchanges between the partners' processes. This article proposes a new method called Process Mining for Business Process Integration (PM4BPI) that helps process designers to perform BPI by creating new process models that cross the boundaries of multiple organizations from a collection of process event logs. PM4BPI uses federated process mining techniques to detect incompatibilities before the integration of the partners' processes. Then, it applies process adaptation patterns to solve detected incompatibilities. Finally, organizations' processes are merged to build a collaborative process model that crosses the organizations' boundaries. AdaptWF_Net, an extension of a Petri net, is used to design inter-organizational business processes and adaptation patterns. An integrated care pathway is used as a case study to assess the applicability and effectiveness of the proposed method.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",adaptation patterns; Additional Key Words and PhrasesBusiness process integration; inter-organizational business process model; process mining; process modelling,Coordination reactions; Data mining; Enterprise resource management; Adaptation pattern; Additional key word and phrasesbusiness process integration; Business process integration; Business process modeling; Inter-organizational business process model; Interorganizational business process; Key words; Process mining; Process-models; Processes integrations; Petri nets
Agent-Based Model of Initial Token Allocations: Simulating Distributions post Fair Launch,2024,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189434650&doi=10.1145%2f3649318&partnerID=40&md5=d6c9b9e26869474f170c4e077c2d66f6,"With advancements in distributed ledger technologies and smart contracts, tokenized voting rights gained prominence within decentralized finance (DeFi). Voting rights tokens (a.k.a. governance tokens) are fungible tokens that grant individual holders the right to vote upon the fate of a project. The motivation behind these tokens is to achieve decentral control within a decentralized autonomous organization (DAO). Because the initial allocations of these tokens is often undemocratic, the DeFi project and DAO of Yearn Finance experimented with a fair launch allocation where no tokens are pre-mined and all participants have an equal opportunity to receive them. Regardless, research on voting rights tokens highlights the formation of timocracies over time. The consideration is that the tokens’ tradability is the cause of concentration. To examine this proposition, this article uses an agent-based model to simulate and analyze the concentration of voting rights tokens post three fair launch allocation scenarios under different trading modalities. The results show that regardless of the allocation, concentration persistently occurs. It confirms the consideration that the ‘disease’ is endogenous: the cause of concentration is the tokens’ tradability. The findings inform theoretical understandings and practical implications for on-chain governance mediated by tokens. © 2024 Copyright held by the owner/author(s).",agent-based modeling; Blockchain; decentralized finance; fair launch; governance; voting rights tokens,Autonomous agents; Computational methods; Distributed ledger; Finance; Simulation platform; Smart contract; Agent-based model; Block-chain; Decentral control; Decentralised; Decentralized finance; Fair launch; Finance programs; Governance; Initial allocations; Voting right token; Blockchain
From Dissonance to Dialogue: A Token-Based Approach to Bridge the Gap between Manufacturers and Customers,2024,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189088536&doi=10.1145%2f3639058&partnerID=40&md5=4bdbd24fac59d5a8a536ee4c50039097,"This article presents a novel token-based recall communication system, which integrates Enterprise Resource Planning (ERP) systems and blockchain technology to enhance recall communication and cooperation between manufacturers and customers. We employed a design science research methodology to develop a set of design principles and features that support the interoperable system. Our findings demonstrate that we can significantly improve recall coordination, traceability, and co-value creation between involved parties. By focusing on the integration potential of traditional technologies like ERP systems with blockchain and token techniques, we reveal innovative synergies for both social and technical subsystems. The study explores the implications of the proposed system for both theory and practice, offering insights into the advantages and challenges of such integration. The evaluation conducted with industry experts demonstrates high reusability of the design principles.  © 2024 Copyright held by the owner/author(s).",Additional Key Words and PhrasesBlockchain; enterprise resource planning system; information system design; recall communication,Blockchain; Bridges; Design; Enterprise resource planning; Interoperability; Resource allocation; Additional key word and phrasesblockchain; Block-chain; Communications systems; Design Principles; Design-science researches; Enterprise resource planning systems; Information system design; Key words; Recall communication; Research methodologies; Reusability
Non-monotonic Generation of Knowledge Paths for Context Understanding,2024,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189100527&doi=10.1145%2f3627994&partnerID=40&md5=20f216162fda539da281e25dbce71b3d,"Knowledge graphs can be used to enhance text search and access by augmenting textual content with relevant background knowledge. While many large knowledge graphs are available, using them to make semantic connections between entities mentioned in the textual content remains to be a difficult task. In this work, we therefore introduce contextual path generation (CPG), which refers to the task of generating knowledge paths, contextual path, to explain the semantic connections between entities mentioned in textual documents with given knowledge graph. To perform the CPG task well, one has to address its three challenges, namely, path relevance, incomplete knowledge graph, and path well-formedness. This article designs a two-stage framework comprised of the following: (1) a knowledge-enabled embedding matching and learning-To-rank with multi-head self-Attention context extractor to determine a set of context entities relevant to both the query entities and context document, and (2) a non-monotonic path generation method with pretrained transformer to generate high-quality contextual paths. Our experiment results on two real-world datasets show that our best performing CPG model successfully recovers 84.13% of ground truth contextual paths, outperforming the context window baselines. Finally, we demonstrate that the non-monotonic model generates more well-formed paths compared to the monotonic counterpart.  © 2024 Copyright held by the owner/author(s).",Additional Key Words and PhrasesInformation retrieval; contextual path generation; generation model; knowledge graph,Graph theory; Query processing; Semantics; Additional key word and phrasesinformation retrieval; Background knowledge; Contextual path generation; Generation model; Key words; Knowledge graphs; Monotonics; Path-generation; Text search; Textual content; Knowledge graph
Introduction to the Special Issue on IT-enabled Business Management and Decision Making in the (Post) Covid-19 Era,2023,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180113890&doi=10.1145%2f3627995&partnerID=40&md5=034cbcef46a7ddf15f31f9405fffea51,[No abstract available],,
Disentangling Affordances of Online Collaboration Tools for Mutual Aid in Emergencies: Insights from the COVID-19 Lockdown,2023,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179894321&doi=10.1145%2f3593056&partnerID=40&md5=fa29344eb12564ead324c2efa05158c1,"With the uncertain trajectory of COVID-19 conditions worldwide, there lies the potential for emergencies to arise, abruptly yielding mass social and economic disruption. Gaining insight into how digital technologies may be leveraged for effective emergency response is therefore pertinent. Emergency-induced needs may prompt citizens to organize mutual aid initiatives where people give what they can and get what they need in response. An increasingly prominent technology used for emergency response, online collaboration tools (OCTs), enables the appropriate match between the supply of aid and its relevant demand in mutual aid initiatives by mediating information and interactions between participants. Through analysis of mutual aid cases during the 2022 Shanghai COVID-19 lockdown, this study elucidates the benefits OCTs provide through the lens of affordance theory and identifies five key affordances of OCTs for emergency mutual aid: persistent accessibility, iterative modifiability, structured consolidation and retrieval, multisynchronous participation, and multichannel broadcasting. We illustrate how such affordances are actualized and how their enabling features work across information flow processes, specifically highlighting benefits of software minimalism with implications for practitioners and future software design in emergency situations. This study contributes to the body of knowledge on OCTs and affordances by disentangling its role in emergency responses. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",COVID-19; emergency response; Mutual aid; online collaboration tools; software minimalism; technological affordance,Emergency services; Software design; Affordances; Condition; Digital technologies; Emergency response; Gaining insights; Mutual aid; Online collaboration tools; Software minimalism; Technological affordance; Through the lens; COVID-19
Social Determinants of Health and ER Utilization: Role of Information Integration during COVID-19,2023,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179889307&doi=10.1145%2f3583077&partnerID=40&md5=97af2e154d678bd2e950d903b3163b11,"Emergency room (ER) admissions are the front door for the utilization of a community’s health resources and serve as a valuable proxy for a community health system’s capacity. While recent research suggests that social determinants of health (SDOH) are important predictors of patient health outcomes, their impact on ER utilization during the COVID-19 pandemic is not well understood. Further, the role of hospital information integration in moderating the impact of SDOH on ER utilization has not received adequate attention. Utilizing longitudinal claims data from a regional health information exchange spanning 6 years including the COVID-19 period, we study how SDOH affects ER utilization and whether effective integration of patient health information across hospitals can moderate its impact. Our results suggest that a patient’s economic well-being significantly reduces future ER utilization. The magnitude of this relationship is significant when patients are treated at hospitals with high information integration but is weaker when patients receive care at hospitals with lower levels of information integration. Instead, patients’ family and social support can reduce ER utilization when they are treated at hospitals with low information integration. In other words, different dimensions of SDOH are important in low versus high information integration conditions. Furthermore, predictive modeling shows that patient visit type and prior visit history can significantly improve the predictive accuracy of ER utilization. Our research implications support efforts to develop national standards for the collection and sharing of SDOH data and their use and interpretation for clinical decision making by healthcare providers and policy makers. © 2023 Copyright held by the owner/author(s).",COVID-19; emergency room; patient outcomes; social determinants of health; utilization,Clinical research; Decision making; Emergency services; Information retrieval; Integration; Patient treatment; Community health; Health informations; Health Resources; Health systems; Information integration; Patient health; Patient outcome; Recent researches; Social determinants of healths; Utilization; COVID-19
ICT Interactions and COVID-19 – A Theorization Across Two Pandemic Waves,2023,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179888983&doi=10.1145%2f3597938&partnerID=40&md5=2b402319904aa67c1c3908b45a5dd69e,"The COVID-19 pandemic instigated the rapid shift to remote work and virtual interactions, constituting a new normal of professional interaction over information and communication technologies (ICT), such as videoconferencing platforms, email, and mobile devices. While ICT may provide many benefits for remote work, such as flexibility, reductions in travel time, and geographical interaction, ICT may also contribute to increases in job strain, reductions in social interactions, and the decline of mental health. While reliance on ICT for remote work interactions is becoming the new normal of organizational activity, scholarly appreciation of the stages in which employees, particularly educators, enact interactions over ICT is limited. Further, the intensity of the pandemic is unlike anything management scholars have studied before, and previous research into ICT use offers little insight into how ICT behaviors evolve over time. Our research explores how educators enact ICT interactions with students throughout the first two waves of the COVID-19 pandemic. We conducted 24 open-ended interviews with educators to learn about their experiences shifting to ICT for virtual classes. We found that ICT interactions between educators and students are enacted through two sequential, interrelated stages: divergent interaction behaviors and convergent interaction behaviors, with each stage corresponding to the first and second waves of the pandemic. We delineate the phases within each enacting stage of ICT interaction and conjecture that future use of ICT may include iterative cycles of divergent and convergent interaction behaviors, particularly if educators explore how to leverage ICT in more creative ways. Our research presents a theorization of ICT interaction as an increasingly prevalent form of IT-enabled educational interactions and communication. We provide insight into how educators successfully shifted to ICT use for remote work, and we offer implications for the facilitation of hybrid work arrangements in educational settings using ICT. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",COVID-19 pandemic; Education; Grounded theory; Hybridization; Information and communication technologies; IT-enabled interactions; Remote work; Virtual interactions,E-learning; Students; Travel time; Video conferencing; COVID-19 pandemic; Divergents; Grounded theory; Hybridisation; Information and Communication Technologies; Interaction behavior; IT-enabled interaction; Remote work; Technology use; Virtual interactions; COVID-19
A Modular Social Sensing System for Personalized Orienteering in the COVID-19 Era,2023,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179887267&doi=10.1145%2f3615359&partnerID=40&md5=cae392ac3d31579718dabbf8c6ed7eca,"Orienteering or itinerary planning algorithms in tourism are used to optimize travel routes by considering user preference and other constraints, such as time budget or traffic conditions. For these algorithms, it is essential to explore the user preference to predict potential points of interest (POIs) or tourist routes. However, nowadays, user preference has been significantly affected by COVID-19, since health concern plays a key tradeoff role. For example, people may try to avoid crowdedness, even if there is a strong desire for social interaction. Thus, the orienteering or itinerary planning algorithms should optimize routes beyond user preference. Therefore, this article proposes a social sensing system that considers the tradeoff between user preference and various factors, such as crowdedness, personality, knowledge of COVID-19, POI features, and desire for socialization. The experiments are conducted on profiling user interests with a properly 1 trained fastText neural network and a set of specialized Naïve Bayesian Classifiers based on the “Yelp!” dataset. Also, we demonstrate how to approach and integrate COVID-related factors via conversational agents. Furthermore, the proposed system is in a modular design and evaluated in a user study; thus, it can be efficiently adapted to different algorithms for COVID-19-aware itinerary planning. © 2023 Copyright held by the owner/author(s).",COVID-19; itinerary planning; orienteering; personalization; social sensing,Budget control; Classification (of information); User profile; Itinerary planning; Modulars; Orienteering; Personalizations; Planning algorithms; Sensing systems; Social sensing; Time budget; Travel routes; User's preferences; COVID-19
On Predicting ESG Ratings Using Dynamic Company Networks,2023,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85172421078&doi=10.1145%2f3607874&partnerID=40&md5=b503a762b426715da2ee5bcdf0c8d39f,"Environmental, social and governance (ESG) considerations play an increasingly important role due to the growing focus on sustainability globally. Entities, such as banks and investors, utilize ESG ratings of companies issued by specialized rating agencies to evaluate ESG risks of companies. The process of assigning ESG ratings by human analysts is however laborious and time intensive. Developing methods to predict ESG ratings could alleviate such challenges, allow ESG ratings to be generated in a more timely manner, cover more companies, and be more accessible. Most works study the effects of ESG ratings on target variables such as stock prices or financial fundamentals of companies, but few works study how different types of company information can be utilized to predict ESG ratings. Previous works also largely focus on using only the financial information of individual companies to predict ESG ratings, leaving out the different types of inter-company relationship networks. Such inter-company relationship networks are typically dynamic, i.e., they evolve across time. In this paper, we focus on utilizing dynamic inter-company relationships for ESG ratings prediction, and examine the relative importance of different financial and dynamic network information in this prediction task. Our analysis shows that utilizing dynamic inter-company network information, based on common director, common investor and news event-based knowledge graph relationships, can significantly improve ESG rating prediction performance. Robustness checks over different time-periods and different number of time-steps in the future further validate these insights. © 2023 Copyright held by the owner/author(s).",Additional Key Words and PhrasesSustainability; dynamic networks; econometric; ESG; knowledge graphs; machine learning; panel models,"Finance; Forecasting; Machine learning; Sustainable development; Additional key word and phrasessustainability; Company network; Dynamic network; Econometric; Environmental, social and governance; Key words; Knowledge graphs; Machine-learning; Panel model; Work study; Knowledge graph"
"Exploiting Connections among Personality, Job Position, and Work Behavior: Evidence from Joint Bayesian Learning",2023,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85172418945&doi=10.1145%2f3607875&partnerID=40&md5=8765793dade8f6ea210507e600330345,"Personality has been considered as a driving factor for work engagement, which significantly affects people's role performance at work. Although existing research has provided some intuitive understanding of the connection between personality traits and employees' work behaviors, it still lacks effective quantitative tools for modeling personality traits, job position characteristics, and employee work behaviors simultaneously.To this end, in this article, we introduce a data-driven joint Bayesian learning approach, Joint-PJB, to discover explainable joint patterns from massive personality and job-position-related behavioral data. Specifically, Joint-PJB is designed with the knowledgeable guidance of the four-quadrant behavioral model, namely, DISC (Dominance, Influence, Steadiness, Conscientiousness). Based on the real-world data collected from a high-Tech company, Joint-PJB aims to highlight personality-job-behavior joint patterns from personality traits, job responsibilities, and work behaviors. The model can measure the matching degree between employees and their work behaviors given their personality and job position characteristics. We find a significant negative correlation between this matching degree and employee turnover intention. Moreover, we also showcase how the identified patterns can be utilized to support real-world talent management decisions. Both case studies and quantitative experiments verify the effectiveness of Joint-PJB for understanding people's personality traits in different job contexts and their impact on work behaviors. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesBayesian learning; personality traits; work behavior,Behavioral research; Additional key word and phrasesbayesian learning; Bayesian learning; Driving factors; Key words; Matching degree; Performance; Personality traits; Real-world; Work behavior; Work engagements; Personnel
Using Toulmin's Argumentation Model to Enhance Trust in Analytics-Based Advice Giving Systems,2023,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85172415128&doi=10.1145%2f3580479&partnerID=40&md5=a6390a55eaa09add67b75879325cdee4,"Ecommerce websites increasingly provide predictive analytics-based advice (PAA), such as advice about future potential price reductions. Establishing consumer-Trust in these advice-giving systems imposes unique and novel challenges. First, PAA about future alternatives that can benefit the consumer appears to inherently contradict the business goal of selling a product quickly and at high profit margins. Second, PAA is based on mathematical models that are non-Transparent to the user. Third, PAA advice is inherently uncertain, and can be perceived as subjectively imposed in algorithms. Utilizing Toulmin's argumentation-model, we investigate the influence of advice-justification statements in overcoming these difficulties. Based on three experimental studies, in which respondents are provided with the advice of PAA systems, we show evidence for the different roles Toulmin's statement-Types play in enhancing various trusting-beliefs in PAA systems. Provision of warrants is mostly associated with enhanced competence beliefs; rebuttals with integrity beliefs; backings both competence and benevolence; and data statements enhance competence, integrity, and benevolence beliefs. Implications of the findings for research and practice are provided. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesTrust; analytics advice; predictive recommendations; Toulmin's model; trusting beliefs,Electronic commerce; Additional key word and phrasestrust; Advice giving; Analytic advice; Argumentation model; E-commerce websites; Key words; Predictive recommendation; Toulmin; Toulmin models; Trusting beliefs; Predictive analytics
Enabling Efficient Deduplication and Secure Decentralized Public Auditing for Cloud Storage: A Redactable Blockchain Approach,2023,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85172418357&doi=10.1145%2f3578555&partnerID=40&md5=88ed1fe368ea24321765b7490eac89c6,"Public auditing and data deduplication are integral considerations in providing efficient and secure cloud storage services. Nevertheless, the traditional data deduplication models that support public auditing can endure the enormous waste of storage and computation resources induced through data redundancy and repeated audit work by multiple tenants on trusted third-party auditor (TPA). In this work, we introduce blockchain-based secure decentralized public auditing in a decentralized cloud storage with an efficient deduplication model. We employ blockchain to take on the task of centralized TPA, which also mitigates the implications of malicious blockchain miners by using the concept of a decentralized autonomous organization (DAO). Specifically, we employ the idea of redactability for blockchain to handle often neglected security issues that would adversely affect the integrity of stored auditing records on blockchain in decentralized auditing models. However, the proposed model also employs an efficient deduplication scheme to attain adequate storage savings while preserving the users from data loss due to duplicate faking attacks. Moreover, the detailed concrete security analysis demonstrates the computational infeasibility of the proposed model against proof-of-ownership, duplicate faking attack (DFA), collusion attack, storage free-riding attack, data privacy, and forgery attack with high efficiency. Finally, the comprehensive performance analysis shows the scalability and feasibility of the proposed model. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesPublic auditability; data deduplication; decentralized cloud storage (IPFS); redactable blockchain,Blockchain; Computational efficiency; Network security; Privacy-preserving techniques; Additional key word and phrasespublic auditability; Block-chain; Cloud storages; Data de duplications; Decentralised; Decentralized cloud storage (IPFS); Deduplication; Key words; Public auditing; Redactable blockchain; Cloud storage
Peer-To-Peer (P2P) Lending Risk Management: Assessing Credit Risk on Social Lending Platforms Using Textual Factors,2023,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85172413994&doi=10.1145%2f3589003&partnerID=40&md5=9c2a61e92e9f761705c8564134722c91,"Peer-To-peer (P2P) lending platforms offer Internet users the possibility to borrow money from peers without the intervention of traditional financial institutions. Due to the anonymity on such social lending platforms, determining the creditworthiness of borrowers is of high importance. Beyond the disclosure of traditional financial variables that enable risk assessment, peer-To-peer lending platforms offer the opportunity to reveal additional information on the loan purpose. We investigate whether this self-disclosed information is used to show reliability and to outline creditworthiness of platform participants. We analyze more than 70,000 loans funded at a leading social lending platform. We show that linguistic and content-based factors help to explain a loan's probability of default and that content-based factors are more important than linguistic variables. Surprisingly, not every information provided by borrowers underlines creditworthiness. Instead, certain aspects rather indicate a higher probability of default. Our study provides important insights on information disclosure in the context of peer-To-peer lending, shows how to increase performance in credit scoring and is highly relevant for the stakeholders on social lending platforms. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesPeer-To-Peer (P2P) lending; credit risk; credit scoring; FinTech; risk management; social lending,Finance; Linguistics; Peer to peer networks; Risk assessment; Additional key word and phrasespeer-to-peer (P2P) lending; Content-based; Credit risks; Credit scoring; Key words; Peer to peer (P2P); Peer-to-peer lending; Probability of defaults; Risks management; Social lending; Risk management
Situational Factor Determinants of the Allocation of Decision Rights to Edge Computers,2023,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85172417572&doi=10.1145%2f3582081&partnerID=40&md5=ba132217abc5348df4cfd79a05994afb,"Internet of Things (IoT) designers frequently must determine whether action-oriented decisions should be made by edge computers or whether they should be made only by central servers combining input from all edge computers. An important example of this design problem occurs in fire protection IoT, where individual edge computers attached to sensors might be empowered to make decisions (have decision rights) about how to manage the fire. Alternatively, decision rights could be held exclusively by a central server isolated from the fire, because the designer is concerned damage to edge computers could cause them to act unreliably. This research models this allocation of decision rights to identify the relative influence of various decision factors. We first model the allocation of decision rights under the following assumptions: (1) The central server cannot make an error the edge computer cannot make; (2) the central server cannot update the edge computer with its information in a timely manner; and (3) the central server cannot reverse an action initiated by the edge computer to explore the factors impacting decision rights conferral. We then relax each of these three assumptions. We show how relaxing each assumption radically changes the factors impacting decision rights conferral. We also show that allowing the central server to update information on the edge computer or reverse the edge computer's decision making can result in overall lower system performance. We then perform a series of numerical experiments to understand how changing various parameters affect the problem. We show for the general real-world scenario, the key factor influencing the decision is the ability of the edge computer to detect false alarms. We also show magnitude of loss and ratio of real to false incidents have a linear and logarithmic relationship to the reliability of the edge computer. © 2023 Copyright held by the owner/author(s).",Additional Key Words and PhrasesIoT; edge computer; fire protection systems; numerical experiment,Edge computing; Fire extinguishers; Fire protection; Internet of things; Action-oriented; Additional key word and phrasesiot; Central servers; Decision right; Design problems; Edge computer; Fire protection system; Key words; Numerical experiments; Situational factors; Decision making
Incorporating Multiple Knowledge Sources for Targeted Aspect-based Financial Sentiment Analysis,2023,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163219274&doi=10.1145%2f3580480&partnerID=40&md5=fb4f9a2e82dded40e3fdfdffd4ab451d,"Combining symbolic and subsymbolic methods has become a promising strategy as research tasks in AI grow increasingly complicated and require higher levels of understanding. Targeted Aspect-based Financial Sentiment Analysis (TABFSA) is an example of such complicated tasks, as it involves processes like information extraction, information specification, and domain adaptation. However, little is known about the design principles of such hybrid models leveraging external lexical knowledge. To fill this gap, we define anterior, parallel, and posterior knowledge integration and propose incorporating multiple lexical knowledge sources strategically into the fine-Tuning process of pre-Trained transformer models for TABFSA. Experiments on the Financial Opinion mining and Question Answering challenge (FiQA) Task 1 and SemEval 2017 Task 5 datasets show that the knowledge-enabled models systematically improve upon their plain deep learning counterparts, and some outperform state-of-The-Art results reported in terms of aspect sentiment analysis error. We discover that parallel knowledge integration is the most effective and domain-specific lexical knowledge is more important according to our ablation analysis. © 2023 Copyright held by the owner/author(s).",Additional Key Words and PhrasesFinancial sentiment analysis; deep learning; knowledge enabled system; neural networks; transformer models,Deep learning; Finance; Knowledge management; Learning systems; Additional key word and phrasesfinancial sentiment analyse; Deep learning; Key words; Knowledge enabled system; Knowledge integration; Knowledge sources; Lexical knowledge; Neural-networks; Sentiment analysis; Transformer modeling; Sentiment analysis
Optimizing Substance Use Treatment Selection Using Reinforcement Learning,2023,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152636255&doi=10.1145%2f3563778&partnerID=40&md5=9aab855d2c3231a6d192d9f3c23296dc,"Substance use disorder (SUD) exacts a substantial economic and social cost in the United States, and it is crucial for SUD treatment providers to match patients with feasible, effective, and affordable treatment plans. The availability of large SUD patient datasets allows for machine learning techniques to predict patient-level SUD outcomes, yet there has been almost no research on whether machine learning can be used to optimize or personalize which treatment plans SUD patients receive. We use contextual bandits (a reinforcement learning technique) to optimally map patients to SUD treatment plans, based on dozens of patient-level and geographic covariates. We also use near-optimal policies to incorporate treatments' time-intensiveness and cost into our recommendations, to aid treatment providers and policymakers in allocating treatment resources. Our personalized treatment recommendation policies are estimated to yield higher remission rates than observed in our original dataset, and they suggest clinical insights to inform future research on data-driven SUD treatment matching.  © 2023 Association for Computing Machinery.",Contextual bandits; reinforcement learning; substance use,Clinical research; Large dataset; Learning algorithms; Patient treatment; Contextual banditti; Economic costs; Machine learning techniques; Machine-learning; Patient datasets; Reinforcement learnings; Social cost; Substance use; Treatment plans; Treatment selection; Reinforcement learning
COVIDAL: A Machine Learning Classifier for Digital COVID-19 Diagnosis in German Hospitals,2023,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153311918&doi=10.1145%2f3567431&partnerID=40&md5=5872af0b07c6ab76a1e76293c5b33877,"For the fight against the COVID-19 pandemic, it is particularly important to map the course of infection, in terms of patients who have currently tested SARS-CoV-2 positive, as accurately as possible. In hospitals, this is even more important because resources have become scarce. Although polymerase chain reaction (PCR) and point of care (POC) antigen testing capacities have been massively expanded, they are often very time-consuming and cost-intensive and, in some cases, lack appropriate performance. To meet these challenges, we propose the COVIDAL classifier for AI-based diagnosis of symptomatic COVID-19 subjects in hospitals based on laboratory parameters. We evaluate the algorithm's performance by unique multicenter data with approximately 4,000 patients and an extraordinary high ratio of SARS-CoV-2-positive patients. We analyze the influence of data preparation, flexibility in optimization targets, as well as the selection of the test set on the COVIDAL outcome. The algorithm is compared with standard AI, PCR, POC antigen testing and manual classifications of seven physicians by a decision theoretic scoring model including performance metrics, turnaround times and cost. Thereby, we define health care settings in which a certain classifier for COVID-19 diagnosis is to be applied. We find sensitivities, specificities, and accuracies of the COVIDAL algorithm of up to 90 percent. Our scoring model suggests using PCR testing for a focus on performance metrics. For turnaround times, POC antigen testing should be used. If balancing performance, turnaround times, and cost is of interest, as, for example, in the emergency department, COVIDAL is superior based on the scoring model.  © 2023 Association for Computing Machinery.",COVID-19 diagnosis; Machine learning; multicenter data,Balancing; Diagnosis; E-learning; Hospitals; Machine learning; Polymerase chain reaction; Cost-intensive; COVID-19 diagnose; Learning classifiers; Machine-learning; Multicenter data; Performance; Performance metrices; Point of care; Scoring models; Turn-around time; COVID-19
Don't Need All Eggs in One Basket: Reconstructing Composite Embeddings of Customers from Individual-Domain Embeddings,2023,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152624482&doi=10.1145%2f3578710&partnerID=40&md5=c8e01a92d6c4b5f41bf53afeb99426eb,"Although building a 360-degree comprehensive view of a customer has been a long-standing goal in marketing, this challenge has not been successfully addressed in many marketing applications because fractured customer data stored across different ""silos""are hard to integrate under ""one roof""for several reasons. Instead of integrating customer data, in this article we propose to integrate several domain-specific partial customer views into one consolidated or composite customer profile using a Deep Learning-based method that is theoretically grounded in Kolmogorov's Mapping Neural Network Existence Theorem. Furthermore, our method needs to securely access domain-specific or siloed customer data only once for building the initial customer embeddings. We conduct extensive studies on two industrial applications to demonstrate that our method effectively reconstructs stable composite customer embeddings that constitute strong approximations of the ground-truth composite embeddings obtained from integrating the siloed raw customer data. Moreover, we show that these data-security preserving reconstructed composite embeddings not only perform as well as the original ground-truth embeddings but significantly outperform partial embeddings and state-of-the-art baselines in recommendation and consumer preference prediction tasks.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",360-degree view of customer; composite customer embedding; customer preference prediction; Deep Learning,Commerce; Computational complexity; Data integration; Deep learning; Sales; 360-degree view of customer; Composite customer embedding; Customer data; Customer preference prediction; Customer preferences; Deep learning; Domain embedding; Domain specific; Embeddings; Ground truth; Embeddings
Introduction to the Special Issue on Design and Data Science Research in Healthcare,2023,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152735437&doi=10.1145%2f3579646&partnerID=40&md5=949d024577bca02dc3fcb748be0e9bfd,[No abstract available],,
Introduction to the Special Issue on Smart Systems for Industry 4.0 and IoT,2023,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149447159&doi=10.1145%2f3583985&partnerID=40&md5=b9486ef47acaf1abd34d8ac55e6d96ae,[No abstract available],,
Application Massive Data Processing Platform for Smart Manufacturing Based on Optimization of Data Storage,2023,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149441157&doi=10.1145%2f3508395&partnerID=40&md5=2eaa41bb4d4467407b51fabe3b21dfe3,"The aim of smart manufacturing is to reduce manpower requirements of the production line by applying technology of huge amounts of data to the manufacturing industry. Smart manufacturing is also called Industry 4.0, and the platform for processing huge amounts of data has an indispensable role. The massive data processing platform is like the brain of the entire factory, receiving all data from production line sensors via edge computing, processing, and analyzing, and finally making feedback decisions. With the innovation of production technology, the data that the platform needs to process has become diverse and complex, and the amount has become increasingly large. As well, many precision manufacturing industries have begun to enter the field of Industry 4.0. In addition to the accuracy and availability of data processing, there is emphasis on the real-time nature of data processing. After the sensor receives the data, the platform must provide feedback within a short period of time. This article proposes a massive data processing platform based on the Lambda architecture, which has the coexistence of stream processing and batch processing to meet real-time feedback needs of high-precision manufacturing. To verify the effectiveness of the optimization, it is based on real data from the manufacturing industry. To generate a large amount of test data to confirm the optimization of the storage of pictures. The results show that it optimizes the storage and optimization of the image data generated by the Automated Optical Inspection technology used in manufacturing today and optimizes the query for data storage. It also reduces the consumption of a large amount of memory as expected, and the query for Hive reduced the time spent.  © 2022 Association for Computing Machinery.",Hadoop; Industry 4.0; massive data processing platform; Smart manufacturing,Batch data processing; Data handling; Digital storage; Feedback; Flow control; Metadata; Data storage; Hadoop; Manufacturing industries; Manufacturing IS; Massive data; Massive data processing platform; Optimisations; Processing platform; Production line; Smart manufacturing; Industry 4.0
A Multi-Label Classification with an Adversarial-Based Denoising Autoencoder for Medical Image Annotation,2023,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146648130&doi=10.1145%2f3561653&partnerID=40&md5=50f87347c0879af2efac76c31c0a036b,"Medical image annotation aims to automatically describe the content of medical images. It helps doctors to understand the content of medical images and make better informed decisions like diagnoses. Existing methods mainly follow the approach for natural images and fail to emphasize the object abnormalities, which is the essence of medical images annotation. In light of this, we propose to transform the medical image annotation to a multi-label classification problem, where object abnormalities are focused directly. However, extant multi-label classification studies rely on arduous feature engineering, or do not solve label correlation issues well in medical images. To solve these problems, we propose a novel deep learning model where a frequent pattern mining component and an adversarial-based denoising autoencoder component are introduced. Extensive experiments are conducted on a real retinal image dataset to evaluate the performance of the proposed model. Results indicate that the proposed model significantly outperforms image captioning baselines and multi-label classification baselines.  © 2023 Association for Computing Machinery.",adversarial learning; Deep learning; denoising autoencoder; medical image annotation; multi-label classification,Classification (of information); Deep learning; Image analysis; Image annotation; Learning systems; Medical imaging; Adversarial learning; Auto encoders; De-noising; Deep learning; Denoising autoencoder; Feature engineerings; Informed decision; Medical image annotation; Multi-label classifications; Natural images; Diagnosis
A Human-in-the-Loop Segmented Mixed-Effects Modeling Method for Analyzing Wearables Data,2023,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152625162&doi=10.1145%2f3564276&partnerID=40&md5=22de4c3c01e97b5a1c745036749ef9a6,"Wearables are an important source of big data, as they provide real-time high-resolution data logs of health indicators of individuals. Higher-order associations between pairs of variables is common in wearables data. Representing higher-order association curves as piecewise linear segments in a regression model makes them more interpretable. However, existing methods for identifying the change points for segmented modeling either overfit or have low external validity for wearables data containing repeated measures. Therefore, we propose a human-in-the-loop method for segmented modeling of higher-order pairwise associations between variables in wearables data. Our method uses the smooth function estimated by a generalized additive mixed model to allow the analyst to annotate change point estimates for a segmented mixed-effects model, and thereafter employs Brent's constrained optimization procedure to fine-tune the manually provided estimates. We validate our method using three real-world wearables datasets. Our method not only outperforms state-of-the-art modeling methods in terms of prediction performance but also provides more interpretable results. Our study contributes to health data science in terms of developing a new method for interpretable modeling of wearables data. Our analysis uncovers interesting insights on higher-order associations for health researchers.  © 2023 Copyright held by the owner/author(s).",explainability; human-in-the-loop method; interpretable modeling; segmented mixed-effects regression; Smart health; wearables,Piecewise linear techniques; Regression analysis; Explainability; High-order; Higher-order; Human-in-the-loop; Human-in-the-loop method; Interpretable modeling; Mixed effects; Segmented mixed-effect regression; Smart health; Wearables; Constrained optimization
AI-augmented Business Process Management Systems: A Research Manifesto,2023,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150297003&doi=10.1145%2f3576047&partnerID=40&md5=5d511377734b00287b63864b88006725,"AI-augmented Business Process Management Systems (ABPMSs) are an emerging class of process-aware information systems, empowered by trustworthy AI technology. An ABPMS enhances the execution of business processes with the aim of making these processes more adaptable, proactive, explainable, and context-sensitive. This manifesto presents a vision for ABPMSs and discusses research challenges that need to be surmounted to realize this vision. To this end, we define the concept of ABPMS, we outline the lifecycle of processes within an ABPMS, we discuss core characteristics of an ABPMS, and we derive a set of challenges to realize systems with these characteristics.  © 2023 Copyright held by the owner/author(s).",augmented business process; business automation; Business process management; explainability; trustworthy AI,Administrative data processing; Enterprise resource management; Life cycle; Augmented business process; Business automation; Business Process; Business process management; Business process management systems; Explainability; Process management; Process-aware information systems; Trustworthy AI; Information management
MediCoSpace: Visual Decision-Support for Doctor-Patient Consultations using Medical Concept Spaces from EHRs,2023,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152634412&doi=10.1145%2f3564275&partnerID=40&md5=d854bde47da99a3486a54b99fa3ff9bb,"Healthcare systems are under pressure from an aging population, rising costs, and increasingly complex conditions and treatments. Although data are determined to play a bigger role in how doctors diagnose and prescribe treatments, they struggle due to a lack of time and an abundance of structured and unstructured information. To address this challenge, we introduce MediCoSpace, a visual decision-support tool for more efficient doctor-patient consultations. The tool links patient reports to past and present diagnoses, diseases, drugs, and treatments, both for the current patient and other patients in comparable situations. MediCoSpace uses textual medical data, deep-learning supported text analysis and concept spaces to facilitate a visual discovery process. The tool is evaluated by five medical doctors. The results show that MediCoSpace facilitates a promising, yet complex way to discover unlikely relations and thus suggests a path toward the development of interactive visual tools to provide physicians with more holistic diagnoses and personalized, dynamic treatments for patients.  © 2023 Copyright held by the owner/author(s).",electronic health records; interaction design; natural language processing; Visual analytics,Decision support systems; Deep learning; Geometry; Natural language processing systems; Visual languages; Concept space; Decision supports; Electronic health; Electronic health record; Health records; Interaction design; Language processing; Natural language processing; Natural languages; Visual analytics; Diagnosis
Examining Disease Multimorbidity in U.S. Hospital Visits Before and During COVID-19 Pandemic: A Graph Analytics Approach,2023,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152617383&doi=10.1145%2f3564274&partnerID=40&md5=560786db417f501af6288943a60c96e2,"Enduring effects of the COVID-19 pandemic on healthcare systems can be preempted by identifying patterns in diseases recorded in hospital visits over time. Disease multimorbidity or simultaneous occurrence of multiple diseases is a growing global public health challenge as populations age and long-term conditions become more prevalent. We propose a graph analytics framework for analyzing disease multimorbidity in hospital visits. Within the framework, we propose a graph model to explain multimorbidity as a function of prevalence, category, and chronic nature of the underlying disease. We apply our model to examine and compare multimorbidity patterns in public hospitals in Arizona, U.S., during five six-month time periods before and during the pandemic. We observe that while multimorbidity increased by 34.26% and 41.04% during peak pandemic for mental disorders and respiratory disorders respectively, the gradients for endocrine diseases and circulatory disorders were not significant. Multimorbidity for acute conditions is observed to be decreasing during the pandemic while multimorbidity for chronic conditions remains unchanged. Our graph analytics framework provides guidelines for empirical analysis of disease multimorbidity using electronic health records. The patterns identified using our proposed graph model informs future research and healthcare policy makers for pre-emptive decision making.  © 2023 Association for Computing Machinery.",COVID-19 analysis; disease co-occurrence networks; electronic health records; exponential random graph modeling; Graph analytics,Decision making; Graph theory; Health care; Hospitals; Records management; Co-occurrence networks; Condition; COVID-19 analyse; Disease co-occurrence network; Electronic health; Electronic health record; Exponential random graph model; Graph-analytic; Health records; Hospital visits; COVID-19
Near-Infrared Spectroscopy for Bladder Monitoring: A Machine Learning Approach,2023,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152616605&doi=10.1145%2f3563779&partnerID=40&md5=5441af764481e8c0d66c094483805436,"Patients living with neurogenic bladder dysfunction can lose the sensation of their bladder filling. To avoid over-distension of the urinary bladder and prevent long-term damage to the urinary tract, the gold standard treatment is clean intermittent catheterization at predefined time intervals. However, the emptying schedule does not consider actual bladder volume, meaning that catheterization is performed more often than necessary, which can lead to complications such as urinary tract infections. Time-consuming catheterization also interferes with patients' daily routines and, in the case of an empty bladder, uses human and material resources unnecessarily. To enable individually tailored and volume-responsive bladder management, we design a model for the continuous monitoring of bladder volume. During our design science research process, we evaluate the model's applicability and usefulness through interviews with affected patients, prototyping, and application to a real-world in vivo dataset. The developed prototype predicts bladder volume based on relevant sensor data (i.e., near-infrared spectroscopy and acceleration) and the time elapsed since the previous micturition. Our comparison of several supervised state-of-the-art machine and deep learning models reveals that a long short-term memory network architecture achieves a mean absolute error of 116.7 ml that can improve bladder management for patients.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",deep learning; design science research; Machine learning; supervised learning; urinary bladder management,Design; Diagnosis; Infrared devices; Learning systems; Near infrared spectroscopy; Network architecture; Software prototyping; Supervised learning; Deep learning; Design-science researches; Gold standards; Long-term damages; Machine learning approaches; Machine-learning; Neurogenic bladders; Urinary bladder; Urinary bladder management; Urinary tract; Deep learning
Blockchain Use Case in Ballistics and Crime Gun Tracing and Intelligence: Toward Overcoming Gun Violence,2023,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146500712&doi=10.1145%2f3571290&partnerID=40&md5=e965200436a585a14836c7234f99b2b3,"In the United States and around the world, gun violence has become a long-standing public safety concern and a security threat, due to violent gun-related crimes, injuries, and fatalities. Although legislators and lawmakers have attempted to mitigate its threats through legislation, research on gun violence confirms the need for a comprehensive approach to gun violence prevention. This entails addressing the problem in as many ways as possible, such as through legislation, new technological advancements, re-engineering supply, and administrative protocols, among others. The research focuses on the technological, supply, and administrative aspects, in which we propose a manner of managing gun-related data efficiently from the point of manufacture/sale, as well as at points of transfers between secondary sellers for the improvement of criminal investigation processes. Making data more readily available with greater integrity will facilitate successful investigations and prosecutions of gun crimes. Currently, there is no single and uniform platform for firearm manufacturers, dealers, and other stakeholders involved in firearm sales, dissemination, management, and investigation. With the help of Blockchain technology, gun registry, ownership, transfers, and, most importantly, investigations, when crimes occur, can all be managed efficiently, breaking the cycle of gun violence. The identification of guns, gun tracing, and identification of gun owners/possessors rely on accuracy, integrity, and consistency in related systems to influence gun crime investigation processes. Blockchain technology, which uses a consensus-based approach to improve processes and transactions, is demonstrated in this study as a way to enhance these procedures. To the best of our knowledge, this is the first study to explore and demonstrate the utility of Blockchain for gun-related criminal investigations using a design science approach.  © 2023 Association for Computing Machinery.",Blockchain; crisis management; gun violence; smart contract,Blockchain; Crime; Block-chain; Criminal investigation; Crisis management; Gun crime; Gun violence; Investigation process; Public safety; Safety concerns; Security threats; Technological advancement; Smart contract
Tackling the Accuracy-Interpretability Trade-off: Interpretable Deep Learning Models for Satellite Image-based Real Estate Appraisal,2023,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150254190&doi=10.1145%2f3567430&partnerID=40&md5=7c321bd5ad431d45e0d09ede3f888500,"Deep learning models fuel many modern decision support systems, because they typically provide high predictive performance. Among other domains, deep learning is used in real-estate appraisal, where it allows extending the analysis from hard facts only (e.g., size, age) to also consider more implicit information about the location or appearance of houses in the form of image data. However, one downside of deep learning models is their intransparent mechanic of decision making, which leads to a trade-off between accuracy and interpretability. This limits their applicability for tasks where a justification of the decision is necessary. Therefore, in this article, we first combine different perspectives on interpretability into a multi-dimensional framework for a socio-technical perspective on explainable artificial intelligence. Second, we measure the performance gains of using multi-view deep learning, which leverages additional image data (satellite images) for real estate appraisal. Third, we propose and test a novel post hoc explainability method called Grad-Ram. This modified version of Grad-Cam mitigates the intransparency of convolutional neural networks for predicting continuous outcome variables. With this, we try to reduce the accuracy-interpretability trade-off of multi-view deep learning models. Our proposed network architecture outperforms traditional hedonic regression models by 34% in terms of MAE. Furthermore, we find that the used satellite images are the second most important predictor after square feet in our model and that the network learns interpretable patterns about the neighborhood structure and density.  © 2023 Association for Computing Machinery.",accuracy-interpretability trade-off; convolutional neural network; Grad-Ram; hedonic pricing; Interpretability; real estate appraisal,Convolution; Convolutional neural networks; Costs; Decision making; Decision support systems; Deep neural networks; Economic and social effects; Economics; Learning systems; Regression analysis; Satellites; Accuracy-interpretability trade-off; Convolutional neural network; Grad-ram; Hedonic pricing; Interpretability; Learning models; Real estate appraisals; Satellite images; Trade off; Network architecture
An Architecture Using Payment Channel Networks for Blockchain-based Wi-Fi Sharing,2023,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138641920&doi=10.1145%2f3529097&partnerID=40&md5=4aef5206bcbcf5f0e388e15818479f2c,"Enabling Internet access while taking load of mobile networks, the concept of Wi-Fi sharing holds much potential. While trust-based concepts require a trusted intermediary and cannot prevent malicious behavior, for example, conducted through fake profiles, security-based approaches lack adequate accounting mechanisms and coverage. Against this backdrop, we develop a Wi-Fi sharing architecture based on blockchain technology and payment channel networks. Our contribution is twofold: First, we present a comprehensive collection of design principles for workable Wi-Fi sharing networks. Second, we propose and evaluate a reference architecture that augments current approaches with adequate accounting mechanisms and facilitates performance, scalability, security, and participant satisfaction.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",architecture; blockchain; payment channel networks; Wi-Fi sharing,Architecture; Blockchain; Network architecture; Network security; Wireless local area networks (WLAN); Architecture-based; Block-chain; Channel network; Design Principles; Internet access; Malicious behavior; Payment channel network; Sharing architectures; Sharing network; Wi-fi sharing; Wi-Fi
LiMS-Net: A Lightweight Multi-Scale CNN for COVID-19 Detection from Chest CT Scans,2023,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150297522&doi=10.1145%2f3551647&partnerID=40&md5=c55b4f2ddfc490142132e5557a834f89,"Recent years have witnessed a rise in employing deep learning methods, especially convolutional neural networks (CNNs) for detection of COVID-19 cases using chest CT scans. Most of the state-of-the-art models demand a huge amount of parameters which often suffer from overfitting in the presence of limited training samples such as chest CT data and thereby, reducing the detection performance. To handle these issues, in this paper, a lightweight multi-scale CNN called LiMS-Net is proposed. The LiMS-Net contains two feature learning blocks where, in each block, filters of different sizes are applied in parallel to derive multi-scale features from the suspicious regions and an additional filter is subsequently employed to capture discriminant features. The model has only 2.53M parameters and therefore, requires low computational cost and memory space when compared to pretrained CNN architectures. Comprehensive experiments are carried out using a publicly available COVID-19 CT dataset and the results demonstrate that the proposed model achieves higher performance than many pretrained CNN models and state-of-the-art methods even in the presence of limited CT data. Our model achieves an accuracy of 92.11% and an F1-score of 92.59% for detection of COVID-19 from CT scans. Further, the results on a relatively larger CT dataset indicate the effectiveness of the proposed model.  © 2023 Association for Computing Machinery.",chest CT scan; COVID-19; Deep learning; lightweight CNN; LiMS-Net,Computerized tomography; Convolutional neural networks; Deep learning; Learning systems; ART model; Chest CT scans; Convolutional neural network; CT data; Deep learning; Learning methods; Lightweight convolutional neural network; LiMS-net; Multi-scales; State of the art; COVID-19
Resource Allocation for Heterogeneous Computing Tasks in Wirelessly Powered MEC-enabled IIOT Systems,2023,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150304760&doi=10.1145%2f3571291&partnerID=40&md5=4d1c110c6113f50a9ca30dbaa7a2bcc0,"Integrating wireless power transfer with mobile edge computing (MEC) has become a powerful solution for increasingly complicated and dynamic industrial Internet of Things (IIOT) systems. However, the traditional approaches overlooked the heterogeneity of the tasks and the dynamic arrival of energy in wirelessly powered MEC-enabled IIOT systems. In this article, we formulate the problem of maximizing the product of the computing rate and the task execution success rate for heterogeneous tasks. To manage energy harvesting adaptively and select appropriate computing modes, we devise an online resource allocation and computation offloading approach based on deep reinforcement learning. We decompose this approach into two stages: an offloading decision stage and a stopping decision stage. The purpose of the offloading decision stage is to select the computing mode and dynamically allocate the computation round length for each task after learning from the channel state information and the task experience. This stage allows the system to support heterogeneous computing tasks. Subsequently, in the second stage, we adaptively adjust the number of fading slots devoted to energy harvesting in each round in accordance with the status of each fading slot. Simulation results show that our proposed algorithm can better allocate resources for heterogeneous tasks and reduce the ratio of failed tasks and energy consumption when compared with several existing algorithms.  © 2023 Association for Computing Machinery.",heterogeneous computing tasks; Industrial Internet of Things; mobile edge computing; resource allocation; wireless power transfer,Channel state information; Computation offloading; Computing power; Deep learning; Energy harvesting; Energy transfer; Energy utilization; Inductive power transmission; Mobile edge computing; Reinforcement learning; Resource allocation; Channel-state information; Computation offloading; Energy; Heterogeneous computing tasks; Industrial internet of thing; Online resources; Reinforcement learnings; Resources allocation; Task executions; Traditional approaches; Internet of things
Research Challenges for the Design of Human-Artificial Intelligence Systems (HAIS),2023,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150289662&doi=10.1145%2f3549547&partnerID=40&md5=3fc8988d5e0b2edbdeaa105e56488ad6,"Artificial intelligence (AI) capabilities are increasingly common components of all socio-technical information systems that integrate human and machine actions. The impacts of AI components on the design and use of application systems are evolving rapidly as improved deep learning techniques and fresh big data sources afford effective and efficient solutions for broad ranges of applications. New goals and requirements for Human-AI System (HAIS) functions and qualities are emerging, whereas the boundaries between human and machine behaviors continue to blur. This research commentary identifies and addresses the design science research (DSR) challenges facing the field of Information Systems as the demand for human-machine synergies in Human-Artificial Intelligence Systems surges in all application areas. The design challenges of HAIS are characterized by a taxonomy of eight C's - composition, complexity, creativity, confidence, controls, conscience, certification, and contribution. By applying a design science research frame to structure and investigate HAIS design, implementation, use, and evolution, we propose a forward-thinking agenda for relevant and rigorous information systems research contributions.  © 2023 Copyright held by the owner/author(s).",and contribution); Artificial intelligence (AI); certification; complexity; confidence; conscience; controls; creativity; design science research (DSR); HAIS 8C Model; human cognition; Human-AI systems (HAIS); research challenges (composition; socio-technical systems,Behavioral research; Deep learning; Design; Information use; Learning systems; Machine components; AI systems; And contribution); Artificial intelligence; Artificial intelligence systems; Certification; Complexity; Confidence; Conscience; Creativity; Design science research; Design-science researches; Human cognition; Human-AI system 8c model; Human-artificial intelligence system; Research challenge (composition; Research challenges; Sociotechnical systems; Information systems
Smart System: Joint Utility and Frequency for Pattern Classification,2023,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149437655&doi=10.1145%2f3531480&partnerID=40&md5=156eba1206a2c5862caa4ec1eccf736e,"Nowadays, the environments of smart systems for Industry 4.0 and Internet of Things are experiencing fast industrial upgrading. Big data technologies such as design making, event detection, and classification are developed to help manufacturing organizations to achieve smart systems. By applying data analysis, the potential values of rich data can be maximized, which will help manufacturing organizations to finish another round of upgrading. In this article, we propose two new algorithms with respect to big data analysis, namely UFCgen and UFCfast. Both algorithms are designed to collect three types of patterns to help people determine the market positions for different product combinations. We compare these algorithms on various types of datasets, both real and synthetic. The experimental results show that both algorithms can successfully achieve pattern classification by utilizing three different types of interesting patterns from all candidate patterns based on user-specified thresholds of utility and frequency. Furthermore, the list-based UFCfast algorithm outperforms the levelwise-based UFCgen algorithm in terms of both execution time and memory consumption.  © 2022 Association for Computing Machinery.",Big data; pattern classification; smart manufacturing; utility measure,Classification (of information); Data handling; Flow control; Internet of things; Pattern recognition; Data technologies; Events classification; Events detection; Industrial upgrading; Joint utilities; Manufacturing organizations; Patterns classification; Smart manufacturing; Smart System; Utility measure; Big data
The Core Industry Manufacturing Process of Electronics Assembly Based on Smart Manufacturing,2023,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149440083&doi=10.1145%2f3529098&partnerID=40&md5=1a8b4f5ddc4f7759287161aaf2bc6352,"This research takes a case study approach to show the development of a diverse adoption and product strategy distinct from the core manufacturing industry process. It explains the development status in all aspects of smart manufacturing, via the example of ceramic circuit board manufacturing and electronic assembly, and outlines future smart manufacturing plans and processes. The research proposed two experiments using artificial intelligence and deep learning to demonstrate the problems and solutions regarding methods in manufacturing and factory facilities, respectively. In the first experiment, a Bayesian network inference is used to find the cause of the problem of metal residues between electronic circuits through key process and quality correlations. In the second experiment, a convolutional neural network is used to identify false defects that were overinspected during automatic optical inspection. This improves the manufacturing process by enhancing the yield rate and reducing cost. The contributions of the study built in circuit board production. Smart manufacturing, with the application of a Bayesian network to an Internet of Things setup, has addressed the problem of residue and redundant conductors on the edge of the ceramic circuit board pattern, and has improved and prevented leakage and high-frequency interference. The convolutional neural network and deep learning were used to improve the accuracy of the automatic optical inspection system, reduce the current manual review ratio, save labor costs, and provide defect classification as a reference for preprocess improvement.  © 2022 Association for Computing Machinery.",artificial intelligence; Bayesian network; industry manufacturing process; neural network; Smart manufacturing,Convolution; Convolutional neural networks; Cost reduction; Defects; Industrial research; Learning systems; Optical data processing; Wages; Automatic optical inspection; Bayesia n networks; Case study approach; Circuit boards; Convolutional neural network; Electronic assemblies; Industry manufacturing process; Manufacturing process; Neural-networks; Smart manufacturing; Deep learning
"Read the News, Not the Books: Forecasting Firms' Long-term Financial Performance via Deep Text Mining",2023,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150248895&doi=10.1145%2f3533018&partnerID=40&md5=3830e7ee0f6a6efe242c9c94d1c559cd,"In this paper, we show textual data from firm-related events in news articles can effectively predict various firm financial ratios, with or without historical financial ratios. We exploit state-of-the-art neural architectures, including pseudo-event embeddings, Long Short-Term Memory Networks, and attention mechanisms. Our news-powered deep learning models are shown to outperform standard econometric models operating on precise accounting historical data. We also observe forecasting quality improvement when integrating textual and numerical data streams. In addition, we provide in-depth case studies for model explainability and transparency. Our forecasting models, model attention maps, and firm embeddings benefit various stakeholders with quality predictions and explainable insights. Our proposed models can be applied both when numerically historical data is or is not available.  © 2023 Association for Computing Machinery.",deep learning; forecasting; Natural language processing,Deep learning; Embeddings; Finance; Natural language processing systems; Deep learning; Deep text; Embeddings; Financial performance; Financial ratios; Historical data; Language processing; Natural language processing; Natural languages; Textual data; Forecasting
The Opportunity in Difficulty: A Dynamic Privacy Budget Allocation Mechanism for Privacy-Preserving Multi-dimensional Data Collection,2023,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150245665&doi=10.1145%2f3569944&partnerID=40&md5=8323d4db43c3b9405f8d128c01a81641,"Data collection under local differential privacy (LDP) has been gradually on the stage. Compared with the implementation of LDP on the single attribute data collection, that on multi-dimensional data faces great challenges as follows: (1) Communication cost. Multivariate data collection needs to retain the correlations between attributes, which means that more complex privatization mechanisms will result in more communication costs. (2) Noise scale. More attributes have to share the privacy budget limited by data utility and privacy-preserving level, which means that less privacy budget can be allocated to each of them, resulting in more noise added to the data. In this work, we innovatively reverse the complex multi-dimensional attributes, i.e., the major negative factor that leads to the above difficulties, to act as a beneficial factor to improve the efficiency of privacy budget allocation, so as to realize a multi-dimensional data collection under LDP with high comprehensive performance. Specifically, we first present a Multivariate k-ary Randomized Response (kRR) mechanism, called Multi-kRR. It applies the RR directly to each attribute to reduce the communication cost. To deal with the impact of a large amount of noise, we propose a Markov-based dynamic privacy budget allocation mechanism Markov-kRR, which determines the present privacy budget (flipping probability) of an attribute related to the state of the previous attributes. Then, we fix the threshold of flipping times in Markov-kRR and propose an improved mechanism called MarkFixed-kRR, which can obtain more optimized utility by choosing the suitable threshold. Finally, extensive experiments demonstrate the efficiency and effectiveness of our proposed methods.  © 2023 Association for Computing Machinery.",joint distribution estimation; local differential privacy; Multi-dimensional data collection,Budget control; Clustering algorithms; Data acquisition; Data mining; Dynamics; Efficiency; Privacy-preserving techniques; Budget allocation; Data collection; Differential privacies; Distribution estimation; Joint distribution estimation; Joint distributions; Local differential privacy; Multi-dimensional data collection; Multidimensional data; Randomized response; Privatization
Design of a Novel Information System for Semi-automated Management of Cybersecurity in Industrial Control Systems,2023,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150242719&doi=10.1145%2f3546580&partnerID=40&md5=d81ccb87ec138b30ba5409645f34a608,"There is an urgent need in many critical infrastructure sectors, including the energy sector, for attaining detailed insights into cybersecurity features and compliance with cybersecurity requirements related to their Operational Technology (OT) deployments. Frequent feature changes of OT devices interfere with this need, posing a great risk to customers. One effective way to address this challenge is via a semi-automated cyber-physical security assurance approach, which enables verification and validation of the OT device cybersecurity claims against actual capabilities, both pre- and post-deployment. To realize this approach, this article presents new methodology and algorithms to automatically identify cybersecurity-related claims expressed in natural language form in ICS device documents. We developed an identification process that employs natural language processing (NLP) techniques with the goal of semi-automated vetting of detected claims against their device implementation. We also present our novel NLP components for verifying feature claims against relevant cybersecurity requirements. The verification pipeline includes components such as automated vendor identification, device document curation, feature claim identification utilizing sentiment analysis for conflict resolution, and reporting of features that are claimed to be supported or indicated as unsupported. Our novel matching engine represents the first automated information system available in the cybersecurity domain that directly aids the generation of ICS compliance reports.  © 2023 Association for Computing Machinery.",Cybersecurity; CYVET; industrial control systems; natural language processing; vetting system,Automation; Control systems; Cybersecurity; Information management; Information systems; Information use; Intelligent control; Automated management; Cyber security; CYVET; Industrial control systems; Language processing; Natural language processing; Natural languages; Novel information; Operational technologies; Vetting systems; Sentiment analysis
Time Series Prediction Using Deep Learning Methods in Healthcare,2023,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150260888&doi=10.1145%2f3531326&partnerID=40&md5=896f2e22f7a5323ccffd0c24349bbe7f,"Traditional machine learning methods face unique challenges when applied to healthcare predictive analytics. The high-dimensional nature of healthcare data necessitates labor-intensive and time-consuming processes when selecting an appropriate set of features for each new task. Furthermore, machine learning methods depend heavily on feature engineering to capture the sequential nature of patient data, oftentimes failing to adequately leverage the temporal patterns of medical events and their dependencies. In contrast, recent deep learning (DL) methods have shown promising performance for various healthcare prediction tasks by specifically addressing the high-dimensional and temporal challenges of medical data. DL techniques excel at learning useful representations of medical concepts and patient clinical data as well as their nonlinear interactions from high-dimensional raw or minimally processed healthcare data.In this article, we systematically reviewed research works that focused on advancing deep neural networks to leverage patient structured time series data for healthcare prediction tasks. To identify relevant studies, we searched MEDLINE, IEEE, Scopus, and ACM Digital Library for relevant publications through November 4, 2021. Overall, we found that researchers have contributed to deep time series prediction literature in 10 identifiable research streams: DL models, missing value handling, addressing temporal irregularity, patient representation, static data inclusion, attention mechanisms, interpretation, incorporation of medical ontologies, learning strategies, and scalability. This study summarizes research insights from these literature streams, identifies several critical research gaps, and suggests future research opportunities for DL applications using patient time series data.  © 2023 Association for Computing Machinery.",deep learning methods; healthcare predictive analytics; patient time series; Systematic review,Deep neural networks; Digital libraries; Health care; Hospital data processing; Learning systems; Time series; Deep learning method; Healthcare predictive analytic; High-dimensional; Higher-dimensional; Learning methods; Machine learning methods; Patient time series; Systematic Review; Time series prediction; Times series; Predictive analytics
Roadside Unit-based Unknown Object Detection in Adverse Weather Conditions for Smart Internet of Vehicles,2023,ACM Transactions on Management Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149437528&doi=10.1145%2f3554923&partnerID=40&md5=09d16bb8cd0290704f0ab929e08b4ddc,"For Internet of Vehicles applications, reliable autonomous driving systems usually perform the majority of their computations on the cloud due to the limited computing power of edge devices. The communication delay between cloud platforms and edge devices, however, can cause dangerous consequences, particularly for latency-sensitive object detection tasks. Object detection tasks are also vulnerable to significantly degraded model performance caused by unknown objects, which creates unsafe driving conditions. To address these problems, this study develops an orchestrated system that allows real-time object detection and incrementally learns unknown objects in a complex and dynamic environment. A you-only-look-once-based object detection model in edge computing mode uses thermal images to detect objects accurately in poor lighting conditions. In addition, an attention mechanism improves the system's performance without significantly increasing model complexity. An unknown object detector automatically classifies and labels unknown objects without direct supervision on edge devices, while a roadside unit (RSU)-based mechanism is developed to update classes and ensure a secure driving experience for autonomous vehicles. Moreover, the interactions between edge devices, RSU servers, and the cloud are designed to allow efficient collaboration. The experimental results indicate that the proposed system learns uncategorized objects dynamically and detects instances accurately.  © 2023 Association for Computing Machinery.",attention mechanism; cloud computing; deep neural network; Internet of vehicles; roadside unit (RSU); thermal infrared images; unknown object detection,Autonomous vehicles; Complex networks; Computing power; Explosives; Infrared imaging; Object detection; Object recognition; Roadsides; Attention mechanisms; Cloud-computing; Internet of vehicle; Objects detection; Roadside unit; Roadside units; Thermal infrared images; Unit-based; Unknown object detection; Unknown objects; Deep neural networks
