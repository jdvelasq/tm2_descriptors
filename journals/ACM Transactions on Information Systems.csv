Title,Year,Source title,Link,Abstract,Author Keywords,Index Keywords
MWI-sum: A multilingual summarizer based on frequent weighted itemsets,2015,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84943646467&doi=10.1145%2f2809786&partnerID=40&md5=b114016e84d7f8f9c98f35353a3a6638,"Multidocument summarization addresses the selection of a compact subset of highly informative sentences, i.e., the summary, from a collection of textual documents. To perform sentence selection, two parallel strategies have been proposed: (a) apply general-purpose techniques relying on datamining or information retrieval techniques, and/or (b) perform advanced linguistic analysis relying on semantics-based models (e.g., ontologies) to capture the actual sentence meaning. Since there is an increasing need for processing documents written in different languages, the attention of the research community has recently focused on summarizers based on strategy (a). This article presents a novelmultilingual summarizer, namely MWI-Sum (Multilingual Weighted Itemsetbased Summarizer), that exploits an itemset-based model to summarize collections of documents ranging over the same topic. Unlike previous approaches, it extracts frequent weighted itemsets tailored to the analyzed collection and uses them to drive the sentence selection process. Weighted itemsets represent correlations among multiple highly relevant terms that are neglected by previous approaches. The proposed approach makes minimal use of language-dependent analyses. Thus, it is easily applicable to document collections written in different languages. Experiments performed on benchmark and real-life collections, English-written and not, demonstrate that the proposed approach performs better than state-of-the-art multilingual document summarizers. © 2015 ACM.",Frequent weighted itemset mining; Multilingual summarization; Text mining,Data mining; Linguistics; Natural language processing systems; Semantics; Document collection; Itemset mining; Linguistic analysis; Multi-document summarization; Multilingual documents; Multilingual summarization; Research communities; Text mining; Computational linguistics
A document retrieval model based on digital signal filtering,2015,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84943631235&doi=10.1145%2f2809787&partnerID=40&md5=1dbec9fd126724b9448b50f58177b581,"Information retrieval (IR) systems are designed, in general, to satisfy the information need of a user who expresses it by means of a query, by providing him with a subset of documents selected from a collection and ordered by decreasing relevance to the query. Such systems are based on IR models, which define how to represent the documents and the query, as well as how to determine the relevance of a document for a query. In this article, we present a new IR model based on concepts taken from both IR and digital signal processing (like Fourier analysis of signals and filtering). This allows the whole IR process to be seen as a physical phenomenon, where the query corresponds to a signal, the documents correspond to filters, and the determination of the relevant documents to the query is done by filtering that signal. Tests showed that the quality of the results provided by this IR model is comparable with the state-of-the-art. © 2015 ACM.",DFT; Digital filtering; Discrete Fourier transform; Retrieval models,Discrete Fourier transforms; Fourier analysis; Information retrieval; Information retrieval systems; Search engines; Analysis of signal; DFT; Digital filtering; Document Retrieval; Physical phenomena; Relevant documents; Retrieval models; State of the art; Signal processing
KNET: A general framework for learning word embedding using morphological knowledge,2015,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84941559918&doi=10.1145%2f2797137&partnerID=40&md5=418131fb7db8e96b062262dc22c7d610,"Neural network techniques are widely applied to obtain high-quality distributed representations of words (i.e., word embeddings) to address textmining, information retrieval, and natural language processing tasks. Most recent efforts have proposed several efficient methods to learn word embeddings from context such that they can encode both semantic and syntactic relationships between words. However, it is quite challenging to handle unseen or rare words with insufficient context. Inspired by the study on the word recognition process in cognitive psychology, in this article, we propose to take advantage of seemingly less obvious but essentially important morphological knowledge to address these challenges. In particular, we introduce a novel neural network architecture called KNET that leverages both words' contextual information and morphological knowledge to learn word embeddings. Meanwhile, this new learning architecture is also able to benefit from noisy knowledge and balance between contextual information and morphological knowledge. Experiments on an analogical reasoning task and a word similarity task both demonstrate that the proposed KNET framework can greatly enhance the effectiveness of word embeddings. © 2015 ACM.",Morphological knowledge; Neural network; Word embedding,Embeddings; Natural language processing systems; Network architecture; Semantics; Cognitive psychology; Contextual information; Distributed representation; Learning architectures; Morphological knowledge; NAtural language processing; Neural network techniques; Word embedding; Neural networks
Latent discriminative models for social emotion detection with emotional dependency,2015,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84941565056&doi=10.1145%2f2749459&partnerID=40&md5=e1aa549086cf2fa791b81a40ba8d5f50,"Sentiment analysis of such opinionated online texts as reviews and comments has received increasingly close attention, yet most of the work is intended to deal with the detection of authors' emotion. In contrast, this article presents our study of the social emotion detection problem, the objective of which is to identify the evoked emotions of readers by online documents such as news articles. A novel Latent Discriminative Model (LDM) is proposed for this task. LDM works by introducing intermediate hidden variables to model the latent structure of input text corpora. To achieve this, it defines a joint distribution over emotions and latent variables, conditioned on the observed text documents. Moreover, we assume that social emotions are not independent but correlated with one another, and the dependency of them is capable of providing additional guidance to LDM in the training process. The inclusion of this emotional dependency into LDM gives rise to a new Emotional Dependency-based LDM (eLDM). We evaluate the proposed models through a series of empirical evaluations on two real-world corpora of news articles. Experimental results verify the effectiveness of LDM and eLDM in social emotion detection. © 2015 ACM.",Discriminative model; Emotion detection; Opinion mining and sentiment analysis; Social,Computer networks; Information systems; Discriminative models; Emotion detection; Empirical evaluations; Joint distributions; Latent structures; On-line documents; Opinion mining; Social; Sentiment analysis
Identifying opportunities for valuable encounters: Toward context-aware social matching systems,2015,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84941568674&doi=10.1145%2f2751557&partnerID=40&md5=34804dfd6043bda0422dff7231b0d9f1,"Mobile social matching systems have the potential to transform the way we make new social ties, but only if we are able to overcome the many challenges that exist as to how systems can utilize contextual data to recommend interesting and relevant people to users and facilitate valuable encounters between strangers. This article outlines how context andmobility influence people's motivations tomeet new people and presents innovative design concepts for mediating mobile encounters through context-aware social matching systems. Findings from two studies are presented. The first, a survey study (n=117) explored the concept of contextual rarity of shared user attributes as a measure to improve desirability in mobile social matches. The second, an interview study (n = 58) explored people's motivations to meet others in various contexts. From these studies we derived a set of novel context-aware social matching concepts, including contextual sociability and familiarity as an indicator of opportune social context; contextual engagement as an indicator of opportune personal context; and contextual rarity, oddity, and activity partnering as an indicator of opportune relational context. The findings of these studies establish the importance of different contextual factors and frame the design space of context-aware social matching systems. © 2015 ACM.",Context-aware social matching; Context-awareness; Introduction systems; Social discovery; Social recommender systems,Computer networks; Information systems; Context- awareness; Context-Aware; Contextual factors; Innovative design; Interview study; Social discovery; Social matching systems; Social recommender systems; Motivation
Deep dependency substructure-based learning for multidocument summarization,2015,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84941549686&doi=10.1145%2f2766447&partnerID=40&md5=962277f664dedfa45840e8a641e86ed2,"Most extractive style topic-focused multidocument summarization systems generate a summary by ranking textual units in multiple documents and extracting a proper subset of sentences biased to the given topic. Usually, the textual units are simply represented as sentences or n-grams, which do not carry deep syntactic and semantic information. This article presents a novel extractive topic-focused multidocument summarization framework. The framework proposes a new kind of more meaningful and informative units named frequent Deep Dependency Sub-Structure (DDSS) and a topic-sensitive Multi-Task Learning (MTL) model for frequent DDSS ranking. Given a document set, first, we parse all the sentences into deep dependency structures with a Head-driven Phrase Structure Grammar (HPSG) parser and mine the frequent DDSSs after semantic normalization. Then we employ a topic-sensitive MTL model to learn the importance of these frequent DDSSs. Finally, we exploit an Integer Linear Programming (ILP) formulation and use the frequent DDSSs as the essentials for summary extraction. Experimental results on two DUC datasets demonstrate that our proposed approach can achieve state-of-the-art performance. Both the DDSS information and the topic-sensitive MTL model are validated to be very helpful for topic-focused multidocument summarization. © 2015 ACM.",Deep dependency sub-structure; Document summarization; Learning; Multi-task,Computational grammars; Deep learning; Formal languages; Integer programming; Learning systems; Linearization; Multi-task learning; Natural language processing systems; Semantics; Syntactics; Dependency structures; Document summarization; Head-driven phrase structure grammars; Integer Linear Programming; Learning; Multi-document summarization; State-of-the-art performance; Sub-structures; Data mining
Fast forward index methods for pseudo-relevance feedback retrieval,2015,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930965848&doi=10.1145%2f2744199&partnerID=40&md5=86d8905ca20f1670789490e2cf93c31c,"The inverted index is the dominant indexing method in information retrieval systems. It enables fast return of the list of all documents containing a given query term. However, for retrieval schemes involving query expansion, as in pseudo-relevance feedback (PRF), the retrieval time based on an inverted index increases linearly with the number of expansion terms. In this regard, we have examined the use of a forward index, which consists of the mapping of each document to its constituent terms. We propose a novel forward index-based reranking scheme to shorten the PRF retrieval time. In our method, a first retrieval of the original query is performed using an inverted index, and then a forward index is employed for the PRF part. We have studied several new forward indexes, including using a novel spstring data structure and the weighted variable bit-block compression (wvbc) signature. With modern hardware such as solid-state drives (SSDs) and sufficiently large main memory, forward index methods are particularly promising. We find that with the whole index stored in main memory, PRF retrieval using a spstring or wvbc forward index excels in time efficiency over an inverted index, being able to obtain the same levels of performance measures at shorter times. © 2015 Association for Computing Machinery.",Forward index; Information retrieval; Inverted index; Time efficiency,Digital storage; Efficiency; Indexing (of information); Information retrieval; Information retrieval systems; Natural language processing systems; Forward index; Indexing methods; Inverted indices; Performance measure; Pseudo relevance feedback; Pseudo-relevance feedbacks; Solid state drives; Time efficiencies; Search engines
The query change model: Modeling Session search as a Markov decision process,2015,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930651241&doi=10.1145%2f2747874&partnerID=40&md5=e890b0e99308aa90800282c7815d1aa8,"Modern information retrieval (IR) systems exhibit user dynamics through interactivity. These dynamic aspects of IR, including changes found in data, users, and systems, are increasingly being utilized in search engines. Session search is one such IR task-document retrieval within a session. During a session, a user constantly modifies queries to find documents that fulfill an information need. Existing IR techniques for assisting the user in this task are limited in their ability to optimize over changes, learn with a minimal computational footprint, and be responsive. This article proposes a novel query change retrieval model (QCM), which uses syntactic editing changes between consecutive queries,aswellasthe relationship between query changes and previously retrieved documents, to enhance session search. We propose modeling session search as a Markov decision process (MDP). We consider two agents in this MDP: the user agent and the search engine agent. The user agent's actions are query changes that we observe, and the search engine agent's actions are term weight adjustments as proposed in this work. We also investigate multiple query aggregation schemes and their effectivenesson session search. Experiments show that our approach is highly effective and outperforms top session search systems in TREC 2011 and TREC 2012. © 2015 ACM.",Markov decision process; QCM; Query change model; Session search,Information retrieval; Markov processes; Natural language processing systems; Change modeling; Document Retrieval; Dynamic aspects; Markov Decision Processes; Multiple queries; Retrieval models; Retrieved documents; Session search; Search engines
Understanding and supporting cross-device web search for exploratory tasks with Mobile Touch Interactions,2015,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929170524&doi=10.1145%2f2738036&partnerID=40&md5=e85655dfc9c89c6f52198179178e6980,"Mobile devices enable people to look for information at the moment when their information needs are triggered. While experiencing complex information needs that require multiple search sessions, users may utilize desktop computers to fulfill information needs started on mobile devices. Under the context of mobileto-desktop web search, this article analyzes users' behavioral patterns and compares them to the patterns in desktop-to-desktop web search. Then, we examine several approaches of using Mobile Touch Interactions (MTIs) to infer relevant content so that such content can be used for supporting subsequent search queries on desktop computers. The experimental data used in this article was collected through a user study involving 24 participants and six properly designed cross-device web search tasks. Our experimental results show that (1) users' mobile-to-desktop search behaviors do significantly differ from desktop-to-desktop search behaviors in terms of information exploration, sense-making and repeated behaviors. (2) MTIs can be employed to predict the relevance of click-through documents, but applying document-level relevant content based on the predicted relevance does not improve search performance. (3) MTIs can also be used to identify the relevant text chunks at a fine-grained subdocument level. Such relevant information can achieve better search performance than the document-level relevant content. In addition, such subdocument relevant information can be combined with document-level relevance to further improve the search performance. However, the effectiveness of these methods relies on the sufficiency of click-through documents. (4) MTIs can also be obtained from the Search Engine Results Pages (SERPs). The subdocument feedbacks inferred from this set of MTIs even outperform the MTI-based subdocument feedback from the click-through documents. © 2015 ACM.",Cross-device web search; Exploratory web search; Mobile Touch Interactions; Relevance feedback,Human computer interaction; Information retrieval; Mobile telecommunication systems; Personal computers; Search engines; Behavioral patterns; Complex information; Information exploration; Relevance feedback; Search engine results pages; Search performance; Touch interaction; Web searches; Websites
Belief dynamics and biases in web search,2015,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929171867&doi=10.1145%2f2746229&partnerID=40&md5=9ac3ad83dbb6d07d1650575ca3be17be,"We investigate how beliefs about the efficacy of medical interventions are influenced by searchers' exposure to information on retrieved Web pages. We present a methodology for measuring participants' beliefs and confidence about the efficacy of treatment before, during, and after search episodes. We consider interventions studied in the Cochrane collection of meta-analyses. We extract related queries from search engine logs and consider the Cochrane assessments as ground truth. We analyze the dynamics of belief over time and show the influence of prior beliefs and confidence at the end of sessions. We present evidence for confirmation bias and for anchoring-and-adjustment during search and retrieval. Then, we build predictive models to estimate postsearch beliefs using sets of features about behavior and content. The findings provide insights about the influence of Web content on the beliefs of people and have implications for the design of search systems. © 2015 ACM.",Belief dynamics; Cognitive biases; Search interaction,Dynamics; Information retrieval systems; Search engines; Social networking (online); Websites; Cognitive bias; Confirmation bias; Medical intervention; Meta analysis; Predictive models; Search and retrieval; Search interaction; Sets of features; World Wide Web
A Pólya urn document language model for improved information retrieval,2015,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929191017&doi=10.1145%2f2746231&partnerID=40&md5=dce58297c95dd4a3ed384cd5ddc35130,"The multinomial language model has been one of the most effective modelsofretrieval for more thanadecade. However, the multinomial distribution does not model one important linguistic phenomenon relating to term dependency - that is, the tendency of a term to repeat itself within a document (i.e., word burstiness). In this article, we model document generation as a random process with reinforcement (a multivariate Pólya process) and develop a Dirichlet compound multinomial language model that captures word burstiness directly. We show that the new reinforced language model can be computed as efficiently as current retrieval models, and with experiments on an extensive set of TREC collections, we show that it significantly outperforms the state-of-the-art language model for a number of standard effectiveness metrics. Experiments also show that the tuning parameter in the proposed model is more robust than that in the multinomial language model. Furthermore, we develop a constraint for the verbosity hypothesis and show that the proposed model adheres to the constraint. Finally, we show that the new language model essentially introduces a measure closely related to idf, which gives theoretical justification for combining the term and document event spaces in tf-idf type schemes. © 2015 ACM.",Language models; Pólya urn; Retrieval functions; Smoothing,Random processes; Reinforcement; Document generation; Document language models; Effectiveness metrics; Language model; Linguistic phenomena; Multinomial distributions; Smoothing; State of the art; Computational linguistics
Selective search: Efficient and effective search of large textual collections,2015,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929224547&doi=10.1145%2f2738035&partnerID=40&md5=2e6679ba92976c7d4ca481eb0d4f7ef8,"The traditional search solution for large collections divides the collection into subsets (shards), and processes the query against all shards in parallel (exhaustive search). The search cost and the computational requirements of this approach are often prohibitively high for organizations with few computational resources. This article investigates and extends an alternative: selective search, an approach that partitions the dataset based on document similarity to obtain topic-based shards, and searches only a few shards that are estimated to contain relevant documents for the query. We propose shard creation techniques that are scalable, efficient, self-reliant, and create topic-based shards with low variance in size, and high density of relevant documents. The experimental results demonstrate that the effectiveness of selective search is on par with that of exhaustive search, and the corresponding search costs are substantially lower with the former. Also, the majority of the queries perform as well or better with selective search. An oracle experiment that uses optimal shard ranking for a query indicates that selective search can outperform the effectiveness of exhaustive search. Comparison with a query optimization technique shows higher improvements in efficiency with selective search. The overall best efficiencyisachieved when the two techniques are combinedinan optimized selective search approach. © 2015 ACM.",Distributed information retrieval; Document collection organization; Large-scale text search; Partitioned search; Resource selection; Selective search,Information systems; Distributed information retrieval; Document collection; Partitioned search; Resource selection; Selective search; Text search; Computer networks
A general SIMD-based approach to accelerating compression algorithms,2015,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929144244&doi=10.1145%2f2735629&partnerID=40&md5=31bf179fc73b1bbd85e767361a8bc240,"Compression algorithms are important for data-oriented tasks, especially in the era of ""Big Data."" Modern processors equipped with powerful SIMD instruction sets provide us with an opportunity for achieving better compression performance. Previous research has shown that SIMD-based optimizations can multiply decoding speeds. Following these pioneering studies, we propose a general approach to accelerate compression algorithms. By instantiating the approach, we have developed several novel integer compression algorithms, called Group-Simple, Group-Scheme, Group-AFOR, and Group-PFD, and implemented their corresponding vectorized versions. We evaluate the proposed algorithms on two public TREC datasets, a Wikipedia dataset, and a Twitter dataset. With competitive compression ratios and encoding speeds, our SIMD-based algorithms outperform state-of-the-art nonvectorized algorithms with respect to decoding speeds. © 2015 ACM.",Index compression; Integer encoding; Inverted index; SIMD,Big data; Decoding; Encoding (symbols); Compression algorithms; Compression performance; Index compression; Integer encoding; Inverted indices; Modern processors; SIMD; SIMD instructions; Algorithms
Dynamic user modeling in social media systems,2015,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84925352213&doi=10.1145%2f2699670&partnerID=40&md5=96b2953cfd70f90f6a98a3aad5c708ce,"Social media provides valuable resources to analyze user behaviors and capture user preferences. This article focuses on analyzing user behaviors in social media systems and designing a latent class statistical mixture model, named temporal context-aware mixture model (TCAM), to account for the intentions and preferences behind user behaviors. Based on the observation that the behaviors of a user in social media systems are generally influenced by intrinsic interest as well as the temporal context (e.g., the public's attention at that time), TCAM simultaneously models the topics related to users' intrinsic interests and the topics related to temporal context and then combines the influences from the two factors to model user behaviors in a unified way. Considering that users' interests are not always stable and may change over time, we extend TCAM to a dynamic temporal context-aware mixture model (DTCAM) to capture users' changing interests. To alleviate the problem of data sparsity, we exploit the social and temporal correlation information by integrating a social-temporal regularization framework into the DTCAM model. To further improve the performance of our proposed models (TCAM and DTCAM), an item-weighting scheme is proposed to enable them to favor items that better represent topics related to user interests and topics related to temporal context, respectively. Based on our proposed models, we design a temporal context-aware recommender system (TCARS). To speed up the process of producing the top-k recommendations from large-scale social media data, we develop an efficient query-processing technique to support TCARS. Extensive experiments have been conducted to evaluate the performance of our models on four real-world datasets crawled from different social media sites. The experimental results demonstrate the superiority of our models, compared with the state-of-the-art competitor methods, by modeling user behaviors more precisely and making more effective and efficient recommendations. © 2015 ACM 1046-8188/2015/03-ART10 $15.00.",Algorithms; Design; Experimentation; Performance,Algorithms; Design; Mixtures; Social networking (online); Context-aware recommender systems; Dynamic user modeling; Experimentation; Performance; Query processing techniques; Temporal correlations; Temporal regularization; Top-K recommendations; Behavioral research
Browsing hierarchy construction by minimum evolution,2015,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929168029&doi=10.1145%2f2714574&partnerID=40&md5=78b8b7232471c6ad876ef664bceefcc3,Hierarchies serve as browsing tools to access information in document collections. This article explores techniques to derive browsing hierarchies that can be used as an information map for task-based search. It proposes a novel minimum-evolution hierarchy construction framework that directly learns semantic distances from training data and from users to construct hierarchies. The aim is to produce globally optimized hierarchical structures by incorporating user-generated task specifications into the general learning framework. Both an automatic version of the framework and an interactive version are presented. A comparison with state-of-the-art systems and a user study jointly demonstrate that the proposed framework is highly effective. © 2015 ACM.,Browsing hierarchy construction; Complex search; Information organization; Minimum evolution,Computer networks; Information systems; Complex searches; Document collection; Hierarchical structures; Information organization; Minimum evolution; Semantic distance; State-of-the-art system; Task specifications; Semantics
Task-based information interaction evaluation: The viewpoint of program theory,2015,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84926320780&doi=10.1145%2f2699660&partnerID=40&md5=6596c43d70ddec3a1ca5307eaac7f68b,"Evaluation is central in research and development of information retrieval (IR). In addition to designing and implementing new retrieval mechanisms, one must also show through rigorous evaluation that they are effective. A major focus in IR is IR mechanisms' capability of ranking relevant documents optimally for the users, given a query. Searching for information in practice involves searchers, however, and is highly interactive. When human searchers have been incorporated in evaluation studies, the results have of ten suggested that better ranking does not necessarily lead to better search task, or work task, performance. Therefore, it is not clear which system or interface features should be developed to improve the effectiveness of human task performance. In the present article, we focus on the evaluation of task-based information interaction (TBII). We give special emphasis to learning tasks to discuss TBII in more concrete terms. Information interaction is here understood as behavioral and cognitive activities related to task planning, searching information items, selecting between them, working with them, and synthesizing and reporting. These five generic activities contribute to task performance and outcome and can be supported by information systems. In an attempt toward task-based evaluation, we introduce program theory as the evaluation framework. Such evaluation can investigate whether a program consisting of TBII activities and tools works and how it works and, further, provides a causal description of program (in)effectiveness. Our goal in the present article is to structure TBII on the basis of the five generic activities and consider the evaluation of each activity using the program theory framework. Finally, we combine these activity-based program theories in an overall evaluation framework for TBII. Such an evaluation is complex due to the large number of factors affecting information interaction. Instead of presenting tested program theories, we illustrate how the evaluation of TBII should be accomplished using the program theory framework in the evaluation of systems and behaviors, and their interactions, comprehensively in context. © 2015 ACM.",Experimentation; Human factors; Theory,Algorithms; Human engineering; Information retrieval systems; Evaluation framework; Experimentation; Information interaction; Research and development; Retrieval mechanisms; Searching for informations; Task-based information; Theory; Software testing
Metrics and algorithms for routing questions to user communities,2015,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84925379770&doi=10.1145%2f2724706&partnerID=40&md5=00f7ffb2f29fb44cb2f1bf978725ee65,"An online community consists of a group of users who share a common interest, background, or experience, and their collective goal is to contribute toward the welfare of the community members. Several websites allow their users to create and manage niche communities, such as Yahoo! Groups, Facebook Groups, Google+ Circles, and WebMD Forums. These community services also exist within enterprises, such as IBM Connections. Question answering within these communities enables their members to exchange knowledge and information with other community members. However, the onus of finding the right community for question asking lies with an individual user. The overwhelming number of communities necessitates the need for a good question routing strategy so that new questions get routed to an appropriately focused community and thus get resolved in a reasonable time frame. In this article, we consider the novel problem of routing a question to the right community and propose a framework for selecting and ranking the relevant communities for a question. We propose several novel features for modeling the three main entities of the system: questions, users, and communities. We propose features such as language attributes, inclination to respond, user familiarity, and difficulty of a question; based on these features, we propose similarity metrics between the routed question and the system entities. We introduce a Cutoff-Aggregation (CA) algorithm that aggregates the entity similarity within a community to compute that community's relevance. We introduce two k-nearest-neighbor (knn) algorithms that are a natural instantiation of the CA algorithm, which are computationally efficient and evaluate several ranking algorithms over the aggregate similarity scores computed by the two knn algorithms. We propose clustering techniques to speed up our recommendation framework and show how pipelining can improve the model performance. We demonstrate the effectiveness of our framework on two large real-world datasets. © 2015 ACM 1046-8188/2015/03-ART14 $15.00.",Algorithms; Experimentation; Human factors,Aggregates; Clustering algorithms; Human computer interaction; Human engineering; Internet; Knowledge management; Nearest neighbor search; Social networking (online); Clustering techniques; Community services; Computationally efficient; Entity similarities; Experimentation; K nearest neighbor algorithm; On-line communities; Real-world datasets; Algorithms
User activity patterns during information search,2015,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84926390997&doi=10.1145%2f2699656&partnerID=40&md5=d871161f5825e5df3546a19a92f69a70,"Personalization of support for information seeking depends crucially on the information retrieval system's knowledge of the task that led the person to engage in information seeking. Users work during information search sessions to satisfy their task goals, and their activity is not random. To what degree are there patterns in the user activity during information search sessions? Do activity patterns reflect the user's situation as the user moves through the search task under the influence of his or her task goal? Do these patterns reflect aspects of different types of information-seeking tasks? Could such activity patterns identify contexts within which information seeking takes place? To investigate these questions, we model sequences of user behaviors in two independent user studies of information search sessions (N = 32 users, 128 sessions, and N = 40 users, 160 sessions). Two representations of user activity patterns are used. One is based on the sequences of page use; the other is based on a cognitive representation of information acquisition derived from eye movement patterns in service of the reading process. One of the user studies considered journalism work tasks; the other concerned background research in genomics using search tasks taken from the TREC Genomics Track. The search tasks differed in basic dimensions of complexity, specificity, and the type of information product (intellectual or factual) needed to achieve the overall task goal. The results show that similar patterns of user activity are observed at both the cognitive and page use levels. The activity patterns at both representation layers are able to distinguish between task types in similar ways and, to some degree, between tasks of different levels of difficulty. We explore relationships between the results and task difficulty and discuss the use of activity patterns to explore events within a search session. User activity patterns can be at least partially observed in server-side search logs. A focus on patterns of user activity sequences may contribute to the development of information systems that better personalize the user's search experience. © 2015 ACM.",Design; Human factors; Performance,Behavioral research; Design; Eye movements; Human engineering; Information retrieval; Information use; Activity patterns; Eye movement patterns; Information acquisitions; Information products; Information search; Information seeking; Performance; Personalizations; Search engines
Overview of the special issue on contextual search and recommendation,2015,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84926348673&doi=10.1145%2f2691351&partnerID=40&md5=28b96ae7421315a962ef5e453f99458c,"Information systems that leverage contextual knowledge about their users and their search situations, such as histories, demographics, surroundings, constraints, or devices, can provide tailored search experiences and higher-quality task outcomes. The interactions that occur during these complex tasks provide context that can be leveraged by search systems to support users? broader information-seeking activities. Next-generation recommender systems face analogous challenges, including integrating signals from user exploration to update recommendations in real time. Recently, people have recognized the potential value of contextual information for recommendation in e-commerce, travel, and mobile applications. There has been an increasing research interest on contextual recommendation, and different contextual information; for example, user intent, time, companion, weather, location, objects around, season, and temperature have all been studied in various recommender system settings.",,Electronic commerce; Knowledge management; Mobile commerce; Real time systems; Recommender systems; Contextual information; Contextual knowledge; Contextual recommendations; Information seeking; Mobile applications; Potential values; Research interests; Search system; Search engines
A comparative analysis of interleaving methods for aggregated search,2015,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84923362774&doi=10.1145%2f2668120&partnerID=40&md5=68cc2d8c3598e0585944feb417ca5df7,"A result page of a modern search engine often goes beyond a simple list of ""10 blue links."" Many specific user needs (e.g., News, Image, Video) are addressed by so-called aggregated or vertical search solutions: specially presented documents, often retrieved from specific sources, that stand out from the regular organic Web search results. When it comes to evaluating ranking systems, such complex result layouts raise their own challenges. This is especially true for so-called interleaving methods that have arisen as an important type of online evaluation: by mixing results from two different result pages, interleaving can easily break the desired Web layout in which vertical documents are grouped together, and hence hurt the user experience. We conduct an analysis of different interleaving methods as applied to aggregated search engine result pages. Apart from conventional interleaving methods, we propose two vertical-aware methods: one derived from the widely used Team-Draft Interleaving method by adjusting it in such a way that it respects vertical document groupings, and another based on the recently introduced Optimized Interleaving framework. We show that our proposed methods are better at preserving the user experience than existing interleaving methods while still performing well as a tool for comparing ranking systems. For evaluating our proposed vertical-aware interleaving methods, we use real-world click data as well as simulated clicks and simulated ranking systems. © 2015 ACM.",,Search engines; Comparative analysis; Interleaving methods; On-line evaluation; Ranking system; Search engine results; Simple lists; Vertical searches; Web search results; User experience
Unsupervised visual and textual information fusion in CBMIR using graph-based methods,2015,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84923325625&doi=10.1145%2f2699668&partnerID=40&md5=dedee60ac39e16712f82c87a3742a9ce,"Multimedia collections are more than ever growing in size and diversity. Effective multimedia retrieval systems are thus critical to access these datasets from the end-user perspective and in a scalable way. We are interested in repositories of image/text multimedia objects and we study multimodal information fusion techniques in the context of content-based multimedia information retrieval. We focus on graph-based methods, which have proven to provide state-of-the-art performances. We particularly examine two such methods: cross-media similarities and random-walk-based scores. From a theoretical viewpoint, we propose a unifying graph-based framework, which encompasses the two aforementioned approaches. Our proposal allows us to highlight the core features one should consider when using a graph-based technique for the combination of visual and textual information.We compare cross-media and random-walk-based results using three different real-world datasets. From a practical standpoint, our extended empirical analyses allow us to provide insights and guidelines about the use of graph-based methods for multimodal information fusion in content-based multimedia information retrieval. © 2015 ACM.",Content-based multimedia information retrieval; Cross-media similarity; Graph-based methods; Information fusion; Random walk; Visual reranking,Information fusion; Information retrieval; Random processes; Content-based multimedia; Cross-media; Graph-based methods; Random Walk; Re-ranking; Graphic methods
Web query reformulation via joint modeling of latent topic dependency and term context,2015,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84923360715&doi=10.1145%2f2699666&partnerID=40&md5=4df2b26195fa1b9105eaca91845432cd,"An important way to improve users' satisfaction in Web search is to assist them by issuing more effective queries. One such approach is query reformulation, which generates new queries according to the current query issued by users. A common procedure for conducting reformulation is to generate some candidate queries first, then a scoring method is employed to assess these candidates. Currently, most of the existing methods are context based. They rely heavily on the context relation of terms in the history queries and cannot detect and maintain the semantic consistency of queries. In this article, we propose a graphical model to score queries. The proposed model exploits a latent topic space, which is automatically derived from the query log, to detect semantic dependency of terms in a query and dependency among topics. Meanwhile, the graphical model also captures the term context in the history query by skip-bigram and n-gram language models. In addition, our model can be easily extended to consider users' history search interests when we conduct query reformulation for different users. In the task of candidate query generation, we investigate a social tagging data resource-Delicious bookmark-to generate addition and substitution patterns that are employed as supplements to the patterns generated from query log data. © 2015 ACM.",Graphical model; Query log; Social tagging; Web query reformulation,Graphic methods; Natural language processing systems; Semantics; Social networking (online); GraphicaL model; N-gram language models; Query logs; Query reformulation; Semantic consistency; Social tagging; Substitution patterns; Users' satisfactions; Information retrieval
TASC: A transformation-aware soft cascading approach for multimodal video copy detection,2015,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84923346648&doi=10.1145%2f2699662&partnerID=40&md5=273064d411aa2e9f6f4df3cf08875048,"How to precisely and efficiently detect near-duplicate copies with complicated audiovisual transformations from a large-scale video database is a challenging task. To cope with this challenge, this article proposes a transformation-aware soft cascading (TASC) approach for multimodal video copy detection. Basically, our approach divides query videos into some categories and then for each category designs a transformationaware chain to organize several detectors in a cascade structure. In each chain, efficient but simple detectors are placed in the forepart, whereas effective but complex detectors are located in the rear. To judge whether two videos are near-duplicates, a Detection-on-Copy-Units mechanism is introduced in the TASC, which makes the decision of copy detection depending on the similarity between their most similar fractions, called copy units (CUs), rather than the video-level similarity. Following this, we propose a CU search algorithm to find a pair of CUs from two videos and a CU-based localization algorithm to find the precise locations of their copy segments that are with the asserted CUs as the center. Moreover, to address the problem that the copies and noncopies are possibly linearly inseparable in the feature space, the TASC also introduces a flexible strategy, called soft decision boundary, to replace the single threshold strategy for each detector. Its basic idea is to automatically learn two thresholds for each detector to examine the easy-to-judge copies and noncopies, respectively, and meanwhile to train a nonlinear classifier to further check those hard-to-judge ones. Extensive experiments on three benchmark datasets showed that the TASC can achieve excellent copy detection accuracy and localization precision with a very high processing efficiency. © 2015 ACM.",,Computer networks; Information systems; Benchmark datasets; Cascade structures; Flexible strategies; Linearly inseparable; Localization algorithm; Nonlinear classifiers; Threshold strategy; Video copy detection; Query processing
"Who, where, when, and what: A nonparametric Bayesian approach to context-aware recommendation and search for Twitter users",2015,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84923814459&doi=10.1145%2f2699667&partnerID=40&md5=7c0da1bd6d681409dc889a8e205d04b6,"Micro-blogging services and location-based social networks, such as Twitter, Weibo, and Foursquare, enable users to post short messages with timestamps and geographical annotations. The rich spatial-temporalsemantic information of individuals embedded in these geo-annotated short messages provides exciting opportunity to develop many context-aware applications in ubiquitous computing environments. Example applications include contextual recommendation and contextual search. To obtain accurate recommendations and most relevant search results, it is important to capture users' contextual information (e.g., time and location) and to understand users' topical interests and intentions. While time and location can be readily captured by smartphones, understanding user's interests and intentions calls for effective methods in modeling user mobility behavior. Here, user mobility refers to who visits which place at what time for what activity. That is, user mobility behavior modeling must consider user (Who), spatial (Where), temporal (When), and activity (What) aspects. Unfortunately, no previous studies on user mobility behavior modeling have considered all of the four aspects jointly, which have complex interdependencies. In our preliminary study, we propose the first solution named W4 (short for Who, Where, When, and What) to discover user mobility behavior from the four aspects. In this article, we further enhance W4 and propose a nonparametric Bayesian model named EW4 (short for Enhanced W4 ). EW4 requires no parameter tuning and achieves better results over W4 in our experiments. Given some of the four aspects of a user (e.g., time), our model is able to infer information of the other aspects (e.g., location and topical words). Thus, our model has a variety of context-aware applications, particularly in contextual search and recommendation. Experimental results on two real-world datasets show that the proposed model is effective in discovering users' spatial-temporal topics. The model also significantly outperforms state-of-the-art baselines for various tasks including location prediction for tweets and requirement-aware location recommendation. © 2015 ACM.",Context-aware; Geographical topic modeling; Graphical model; Recommendation and search; Requirement-aware location recommendation; Spatiotemporal; Twitter,Bayesian networks; Behavioral research; Location; Social networking (online); Ubiquitous computing; Context-Aware; GraphicaL model; Recommendation and search; Requirement-Aware; Spatiotemporal; Topic Modeling; Twitter; Location based services
Induced sorting suffixes in external memory,2015,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84923627180&doi=10.1145%2f2699665&partnerID=40&md5=64abff83d95637379ae6db738129f4c2,"We present in this article an external memory algorithm, called disk SA-IS (DSA-IS), to exactly emulate the induced sorting algorithm SA-IS previously proposed for sorting suffixes in RAM. DSA-IS is a new diskfriendly method for sequentially retrieving the preceding character of a sorted suffix to induce the order of the preceding suffix. For a size-n string of a constant or integer alphabet, given the RAM capacity Ω((nW )0.5), where W is the size of each I/O buffer that is large enough to amortize the overhead of each access to disk, both the CPU time and peak disk use of DSA-IS are O(n). Our experimental study shows that on average, DSA-IS achieves the best time and space results of all of the existing external memory algorithms based on the induced sorting principle. © 2015 ACM.",,Random access storage; CPU time; External memory; External memory algorithms; Integer alphabets; Sorting algorithm; Cryptography
Profile-based summarisation for web site navigation,2015,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84923813212&doi=10.1145%2f2699661&partnerID=40&md5=0ffa6fbdf7d95046bd6f15bf9de688d1,"Information systems that utilise contextual information have the potential of helping a user identify relevant information more quickly and more accurately than systems that work the same for all users and contexts. Contextual information comes in a variety of types, often derived from records of past interactions between a user and the information system. It can be individual or group based. We are focusing on the latter, harnessing the search behaviour of cohorts of users, turning it into a domain model that can then be used to assist other users of the same cohort. More specifically, we aim to explore how such a domain model is best utilised for profile-biased summarisation of documents in a navigation scenario in which such summaries can be displayed as hover text as a user moves the mouse over a link. The main motivation is to help a user find relevant documentsmore quickly. Given the fact that the Web in general has been studied extensively already, we focus our attention on Web sites and similar document collections. Such collections can be notoriously difficult to search or explore. The process of acquiring the domain model is not a research interest here; we simply adopt a biologically inspired method that resembles the idea of ant colony optimisation. This has been shown to work well in a variety of application areas. The model can be built in a continuous learning cycle that exploits search patterns as recorded in typical query log files. Our research explores different summarisation techniques, some of which use the domain model and some that do not. We perform taskbased evaluations of these different techniques-thus of the impact of the domain model and profile-biased summarisation-in the context of Web site navigation. ©2015 ACM.",Browsing; Group profiling; Log analysis; Multi-document summarisation (MDS); Navigation; Single-document summarisation (SDS); Term association networks,Ant colony optimization; Biomimetics; Information systems; Information use; Navigation; Websites; Browsing; Group profiling; Log analysis; Multi-document; Single-document summarisation (SDS); Term associations; Mammals
Two-stage document length normalization for information retrieval,2015,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84923328913&doi=10.1145%2f2699669&partnerID=40&md5=606c9cea7ccaf5a36f1563723f1195ba,"The standard approach for term frequency normalization is based only on the document length. However, it does not distinguish the verbosity from the scope, these being the twomain factors determining the document length. Because the verbosity and scope have largely different effects on the increase in term frequency, the standard approach can easily suffer from insufficient or excessive penalization depending on the specific type of long document. To overcome these problems, this article proposes two-stage normalization by performing verbosity and scope normalization separately, and by employing different penalization functions. In verbosity normalization, each document is prenormalized by dividing the term frequency by the verbosity of the document. In scope normalization, an existing retrieval model is applied in a straightforward manner to the prenormalized document, finally leading us to formulate our proposed verbosity normalized (VN) retrieval model. Experimental results carried out on standard TREC collections demonstrate that the VN model leads to marginal but statistically significant improvements over standard retrieval models. © 2015 ACM.",Document length normalization; Retrieval heuristics; Scope normalization; Term frequency; Verbosity normalization,Computer networks; Information systems; Document length normalization; Retrieval heuristics; Scope normalization; Term Frequency; Verbosity normalization; Information retrieval
Stochastic query covering for fast approximate document retrieval,2015,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84923592856&doi=10.1145%2f2699671&partnerID=40&md5=1644581c9391a664d62ddf59884aea37,"We design algorithms that, given a collection of documents and a distribution over user queries, return a small subset of the document collection in such a way that we can efficiently provide high-quality answers to user queries using only the selected subset. This approach has applications when space is a constraint or when the query-processing time increases significantly with the size of the collection. We study our algorithms through the lens of stochastic analysis and prove that even though they use only a small fraction of the entire collection, they can provide answers to most user queries, achieving a performance close to the optimal. To complement our theoretical findings, we experimentally show the versatility of our approach by considering two important cases in the context of Web search. In the first case, we favor the retrieval of documents that are relevant to the query, whereas in the second case we aim for document diversification. Both the theoretical and the experimental analysis provide strong evidence of the potential value of query covering in diverse application scenarios. © 2015 ACM.",,Stochastic systems; Collection of documents; Diverse applications; Document collection; Document Retrieval; Experimental analysis; Potential values; Stochastic analysis; Through the lens; Information retrieval
Exploiting representations from statistical machine translation for cross-language information retrieval,2014,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84909957922&doi=10.1145%2f2644807&partnerID=40&md5=132050c91b2d617d5ea303810cc94437,"This work explores how internal representations of modern statistical machine translation systems can be exploited for cross-language information retrieval. We tackle two core issues that are central to query translation: how to exploit context to generate more accurate translations and how to preserve ambiguity that may be present in the original query, thereby retaining a diverse set of translation alternatives. These two considerations are often in tension since ambiguity in natural language is typically resolved by exploiting context, but effective retrieval requires striking the right balance. We propose two novel query translation approaches: the grammar-based approach extracts translation probabilities from translation grammars, while the decoder-based approach takes advantage of n-best translation hypotheses. Both are context-sensitive, in contrast to a baseline context-insensitive approach that uses bilingual dictionaries for word-by-word translation. Experimental results show that by ""opening up"" modern statistical machine translation systems, we can access intermediate representations that yield high retrieval effectiveness. By combining evidence from multiple sources, we demonstrate significant improvements over competitive baselines on standard cross-language information retrieval test collections. In addition to effectiveness, the efficiency of our techniques are explored as well. © 2014 ACM.",,Computational linguistics; Computer aided language translation; Information retrieval; Bilingual dictionary; Cross language information retrieval; Grammar based approach; Intermediate representations; Internal representation; Retrieval effectiveness; Statistical machine translation; Statistical machine translation system; Search engines
Matrix factorization with explicit trust and distrust side information for improved social recommendation,2014,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84909952758&doi=10.1145%2f2641564&partnerID=40&md5=7a3b75bd48a57c1f2578320cc6aebe24,"With the advent of online social networks, recommender systems have became crucial for the success of many online applications/services due to their significance role in tailoring these applications to user-specific needs or preferences. Despite their increasing popularity, in general, recommender systems suffer from data sparsity and cold-start problems. To alleviate these issues, in recent years, there has been an upsurge of interest in exploiting social information such as trust relations among users along with the rating data to improve the performance of recommender systems. The main motivation for exploiting trust information in the recommendation process stems from the observation that the ideas we are exposed to and the choices we make are significantly influenced by our social context. However, in large user communities, in addition to trust relations, distrust relations also exist between users. For instance, in Epinions, the concepts of personal ""web of trust"" and personal ""block list"" allow users to categorize their friends based on the quality of reviews into trusted and distrusted friends, respectively. Hence, it will be interesting to incorporate this new source of information in recommendation as well. In contrast to the incorporation of trust information in recommendation which is thriving, the potential of explicitly incorporating distrust relations is almost unexplored. In this article, we propose a matrix factorization-based model for recommendation in social rating networks that properly incorporates both trust and distrust relationships aiming to improve the quality of recommendations and mitigate the data sparsity and cold-start users issues. Through experiments on the Epinions dataset, we show that our new algorithm outperforms its standard trust-enhanced or distrustenhanced counterparts with respect to accuracy, thereby demonstrating the positive effect that incorporation of explicit distrust information can have on recommender systems. © 2014 ACM.",Matrix factorization; Recommender systems; Social relationships,Factorization; Online systems; Recommender systems; Social aspects; Social networking (online); Cold start problems; Matrix factorizations; On-line applications; On-line social networks; Performance of recommender systems; Quality of recommendations; Social information; Social relationships; Matrix algebra
Understanding intrinsic diversity in web search: Improving whole-session relevance,2014,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84909953299&doi=10.1145%2f2629553&partnerID=40&md5=53629d5963eb02ea22c00777ccfe297a,"Current research on Web search has focused on optimizing and evaluating single queries. However, a significant fraction of user queries are part of more complex tasks [Jones and Klinkner 2008] which span multiple queries across one or more search sessions [Liu and Belkin 2010; Kotov et al. 2011]. An ideal search engine would not only retrieve relevant results for a user's particular query but also be able to identify when the user is engaged in a more complex task and aid the user in completing that task [Morris et al. 2008; Agichtein et al. 2012]. Toward optimizing whole-session or task relevance, we characterize and address the problem of intrinsic diversity (ID) in retrieval [Radlinski et al. 2009], a type of complex task that requires multiple interactions with current search engines. Unlike existing work on extrinsic diversity [Carbonell and Goldstein 1998; Zhai et al. 2003; Chen and Karger 2006] that deals with ambiguity in intent across multiple users, ID queries often have little ambiguity in intent but seek content covering a variety of aspects on a shared theme. In such scenarios, the underlying needs are typically exploratory, comparative, or breadth-oriented in nature. We identify and address three key problems for ID retrieval: identifying authentic examples of ID tasks from post-hoc analysis of behavioral signals in search logs; learning to identify initiator queries that mark the start of an ID search task; and given an initiator query, predicting which content to prefetch and rank. 2014 Copyright held by the Owner/Author. Publication rights licensed to ACM.",Diversity; Proactive search; Search behavior; Search session analysis,Search engines; Signal analysis; Websites; Diversity; Intrinsic diversity; Multiple interactions; Multiple queries; Multiple user; Proactive search; Search behavior; Search sessions; Information retrieval
Patent query formulation by synthesizing multiple sources of relevance evidence,2014,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84909957414&doi=10.1145%2f2651363&partnerID=40&md5=0284f9bac71f8ade61c19a07dd8dff90,"Patent prior art search is a task in patent retrieval with the goal of finding documents which describe prior art work related to a query patent. A query patent is a full patent application composed of hundreds of terms which does not represent a single focused information need. Fortunately, other relevance evidence sources (i.e., classification tags and bibliographical data) provide additional details about the underlying information need. In this article, we propose a unified framework that integrates multiple relevance evidence components for query formulation. We first build a query model from the textual fields of a query patent. To overcome the term mismatch, we expand this initial query model with the term distribution of documents in the citation graph, modeling old and recent domain terminology. We build an IPC lexicon and perform query expansion using this lexicon incorporating proximity information. We performed an empirical evaluation on two patent datasets. Our results show that employing the temporal features of documents has a precision enhancing effect, while query expansion using IPC lexicon improves the recall of the final rank list. © 2014 ACM.",Citation analysis; Patent search; Proximity; Query expansion,Classification (of information); Expansion; Bibliographical datum; Citation analysis; Empirical evaluations; Patent applications; Patent search; Proximity; Query expansion; Term distributions; Patents and inventions
Cache design of SSD-based search engine architectures: An experimental study,2014,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84909979524&doi=10.1145%2f2661629&partnerID=40&md5=114dfb024a2ccb26978030d9ed3203c4,"Caching is an important optimization in search engine architectures. Existing caching techniques for search engine optimization are mostly biased towards the reduction of random accesses to disks, because random accesses are known to be much more expensive than sequential accesses in traditional magnetic hard disk drive (HDD). Recently, solid-state drive (SSD) has emerged as a new kind of secondary storage medium, and some search engines like Baidu have already used SSD to completely replace HDD in their infrastructure. One notable property of SSD is that its random access latency is comparable to its sequential access latency. Therefore, the use of SSDs to replace HDDs in a search engine infrastructuremay void the cache management of existing search engines. In this article, we carry out a series of empirical experiments to study the impact of SSD on search engine cache management. Based on the results, we give insights to practitioners and researchers on how to adapt the infrastructure and caching policies for SSD-based search engines. © 2014 ACM.",Cache; Query processing; Search engine; Solid-state drive,Hard disk storage; Query processing; Cache; Empirical experiments; Magnetic hard disk drives; Search engine optimizations; Secondary storage; Sequential access; Solid state drives; Solid state drives (SSD); Search engines
Browse-to-Search: Interactive exploratory search with visual entities,2014,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84909983212&doi=10.1145%2f2630420&partnerID=40&md5=cf1fab1b50f14f9a33933d11795b1485,"With the development of image search technology, users are no longer satisfied with searching for images using just metadata and textual descriptions. Instead, more search demands are focused on retrieving images based on similarities in their contents (textures, colors, shapes etc.). Nevertheless, one image may deliver rich or complex content and multiple interests. Sometimes users do not sufficiently define or describe their seeking demands for images even when general search interests appear, owing to a lack of specific knowledge to express their intents. A new form of information seeking activity, referred to as exploratory search, is emerging in the research community, which generally combines browsing and searching content together to help users gain additional knowledge and form accurate queries, thereby assisting the users with their seeking and investigation activities. However, there have been few attempts at addressing integrated exploratory search solutions when image browsing is incorporated into the exploring loop. In this work, we investigate the challenges of understanding users' search interests from the images being browsed and infer their actual search intentions. We develop a novel system to explore an effective and efficient way for allowing users to seamlessly switch between browse and search processes, and naturally complete visual-based exploratory search tasks. The system, called Browse-to-Search enables users to specify their visual search interests by circling any visual objects in the webpages being browsed, and then the system automatically forms the visual entities to represent users' underlying intent. One visual entity is not limited by the original image content, but also encapsulated by the textual-based browsing context and the associated heterogeneous attributes. We use large-scale image search technology to find the associated textual attributes from the repository. Users can then utilize the encapsulated visual entities to complete search tasks. The Browse-to-Search system is one of the first attempts to integrate browse and search activities for a visual-based exploratory search, which is characterized by four unique properties: (1) in session - searching is performed during browsing session and search results naturally accompany with browsing content; (2) in context - the pages being browsed provide text-based contextual cues for searching; (3) in focus - users can focus on the visual content of interest without worrying about the difficulties of query formulation, and visual entities will be automatically formed; and (4) intuitiveness - a touch and visual search-based user interface provides a natural user experience. We deploy the Browse-to-Search system on tablet devices and evaluate the system performance using millions of images. We demonstrate that it is effective and efficient in facilitating the user's exploratory search compared to the conventional image search methods and, more importantly, provides users with more robust results to satisfy their exploring experience. © 2014 ACM.",Exploratory search; Gesture; Interactive visual search; Multimedia browsing; Multimedia information systems; User interaction,Information use; Safety devices; Textures; User experience; User interfaces; Exploratory search; Gesture; Multimedia browsing; MultiMedia Information Systems; User interaction; Visual search; Search engines
Georeferencing wikipedia documents using data from social media sources,2014,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84904724242&doi=10.1145%2f2629685&partnerID=40&md5=f28d858689f17c2b53c883c3f6f1ffcb,"Social media sources such as Flickr and Twitter continuously generate large amounts of textual information (tags on Flickr and short messages on Twitter). This textual information is increasingly linked to geographical coordinates, which makes it possible to learn how people refer to places by identifying correlations between the occurrence of terms and the locations of the corresponding social media objects. Recent work has focused on how this potentially rich source of geographic information can be used to estimate geographic coordinates for previously unseen Flickr photos or Twitter messages. In this article, we extend this work by analysing to what extent probabilistic language models trained on Flickr and Twitter can be used to assign coordinates to Wikipedia articles. Our results show that exploiting these language models substantially outperforms both (i) classical gazetteer-based methods (in particular, using Yahoo! Placemaker and Geonames) and (ii) language modelling approaches trained on Wikipedia alone. This supports the hypothesis that social media are important sources of geographic information, which are valuable beyond the scope of individual applications.",Geographic information retrieval; Language models; Semistructured Data,Computational linguistics; Geographic coordinates; Geographic information; Geographic information retrieval; Geographical coordinates; Language model; Probabilistic language; Semi structured data; Textual information; Social networking (online)
XXS: Efficient XPath evaluation on compressed XML documents,2014,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84904761762&doi=10.1145%2f2629554&partnerID=40&md5=0cb07a69735823a447f041c1b9cd393d,"The eXtensible Markup Language (XML) is acknowledged as the de facto standard for semistructured data representation and data exchange on the Web and many other scenarios. A well-known shortcoming of XML is its verbosity, which increases manipulation, transmission, and processing costs. Various structure-blind and structure-conscious compression techniques can be applied to XML, and some are even access-friendly, meaning that the documents can be efficiently accessed in compressed form. Direct access is necessary to implement the query languages XPath and XQuery, which are the standard ones to exploit the expressiveness of XML. While a good deal of theoretical and practical proposals exist to solve XPath/XQuery operations on XML, only a few ones are well integrated with a compression format that supports the required access operations on the XML data. In this work we go one step further and design a compression format for XML collections that boosts the performance of XPath queries on the data. This is done by designing compressed representations of the XML data that support some complex operations apart from just accessing the data, and those are exploited to solve key components of the XPath queries. Our system, called XXS, is aimed at XML collections containing natural language text, which are compressed to within 35%-50% of their original size while supporting a large subset of XPath operations in time competitive with, and many times outperforming, the best state-of-The-Art systems that work on uncompressed representations.",Compression; Self-index; Semistructured data; XML; XPath,Compaction; Electronic data interchange; Query languages; Complex operations; Compression techniques; Extensible Mark-Up language (XML); Natural language text; Self-index; Semi structured data; State-of-the-art system; XPath; XML
Topic modeling for wikipedia link disambiguation,2014,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84904755752&doi=10.1145%2f2633044&partnerID=40&md5=ace0f12c738bd82a48f7aa7f779471dd,"Many articles in the online encyclopedia Wikipedia have hyperlinks to ambiguous article titles; these ambiguous links should be replaced with links to unambiguous articles, a process known as disambiguation. We propose a novel statistical topic model based on link text, which we refer to as the Link Text Topic Model (LTTM), that we use to suggest new link targets for ambiguous links. To evaluate our model, we describe a method for extracting ground truth for this link disambiguation task from edits made to Wikipedia in a specific time period. We use this ground truth to demonstrate the superiority of LTTM over other existing link- and content-based approaches to disambiguating links in Wikipedia. Finally, we build a web service that uses LTTM to make suggestions to human editors wanting to fix ambiguous links in Wikipedia.",Link disambiguation; Topic modeling; Wikipedia,Web services; Content-based approach; Ground truth; Hyperlinks; Link disambiguation; Online encyclopedia; Specific time; Topic Modeling; Wikipedia; Hypertext systems
Content-based video copy detection benchmarking at TRECVID,2014,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84904764868&doi=10.1145%2f2629531&partnerID=40&md5=8d250a435d6e424601877dc0573f5702,"This article presents an overview of the video copy detection benchmarkwhichwas run over a period of 4 years (2008-2011) as part of the TREC Video Retrieval (TRECVID) workshop series. The main contributions of the article include i) an examination of the evolving design of the evaluation framework and its components (system tasks, data, measures); ii) a high-level overview of results and best-performing approaches; and iii) a discussion of lessons learned over the four years. The content-based copy detection (CCD) benchmark worked with a large collection of synthetic queries, which is atypical for TRECVID, as was the use of a normalized detection cost framework. These particular evaluation design choices are motivated and appraised.",Evaluation; Multimedia; TRECVID; Video copy detection,Computer networks; Information systems; Content-based copy detections; Evaluation; Evaluation design; Evaluation framework; Evolving design; Multimedia; TRECVID; Video copy detection; Content based retrieval
LCARS: A spatial item recommender system,2014,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84904754995&doi=10.1145%2f2629461&partnerID=40&md5=5e60b9f36c74053a6292051486400f46,"Newly emerging location-based and event-based social network services provide us with a new platform to understand users' preferences based on their activity history. A user can only visit a limited number of venues/events and most of them are within a limited distance range, so the user-item matrix is very sparse, which creates a big challenge to the traditional collaborative filtering-based recommender systems. The problem becomes even more challenging when people travel to a new city where they have no activity information. In this article, we propose LCARS, a location-content-Aware recommender system that offers a particular user a set of venues (e.g., restaurants and shopping malls) or events (e.g., concerts and exhibitions) by giving consideration to both personal interest and local preference. This recommender system can facilitate people's travel not only near the area in which they live, but also in a city that is new to them. Specifically, LCARS consists of two components: offline modeling and online recommendation. The offline modeling part, called LCA-LDA, is designed to learn the interest of each individual user and the local preference of each individual city by capturing item cooccurrence patterns and exploiting item contents. The online recommendation part takes a querying user along with a querying city as input, and automatically combines the learned interest of the querying user and the local preference of the querying city to produce the topk recommendations. To speed up the online process, a scalable query processing technique is developed by extending both the Threshold Algorithm (TA) and TA-Approximation algorithm.We evaluate the performance of our recommender system on two real datasets, that is, DoubanEvent and Foursquare, and one large-scale synthetic dataset. The results show the superiority of LCARS in recommending spatial items for users, especially when traveling to new cities, in terms of both effectiveness and efficiency. Besides, the experimental analysis results also demonstrate the excellent interpretability of LCARS.",Cold start; Location-based service; Probabilistic generative model; Recommender system; TA algorithm,Approximation algorithms; Location based services; Activity informations; Cold start; Effectiveness and efficiencies; Experimental analysis; Generative model; Scalable Query Processing; Social network services; Top-K recommendations; Recommender systems
Trust prediction via belief propagation,2014,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84904766216&doi=10.1145%2f2629530&partnerID=40&md5=101a0d4088b56f3701b7f5dec092488e,"The prediction of trust relationships in social networks plays an important role in the analytics of the networks. Although various link prediction algorithms for general networks may be adapted for this purpose, the recent notion of ""trust propagation"" has been shown to effectively capture the trust-formation mechanisms and resulted in an effective prediction algorithm. This article builds on the concept of trust propagation and presents a probabilistic trust propagation model. Our model exploits the modern framework of probabilistic graphical models, more specifically, factor graphs. Under this model, the trust prediction problem can be formulated as a statistical inference problem and we derive the belief propagation algorithm as a solver for trust prediction. The model and algorithm are tested using datasets from Epinions and Ciao, by which performance advantages over the previous algorithms are demonstrated.",Link prediction; Social network; Trust propagation,Forecasting; Social networking (online); Belief propagation; Belief propagation algorithm; Link prediction; Model and algorithms; Prediction algorithms; Probabilistic graphical models; Statistical inference; Trust propagation; Algorithms
Social-sensed image search,2014,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84899620034&doi=10.1145%2f2590974&partnerID=40&md5=1455dd72c0fc3683a0c3fd5ddea2b8ce,"Although Web search techniques have greatly facilitate users' information seeking, there are still quite a lot of search sessions that cannot provide satisfactory results, which are more serious in Web image search scenarios. How to understand user intent from observed data is a fundamental issue and of paramount significance in improving image search performance. Previous research efforts mostly focus on discovering user intent either from clickthrough behavior in user search logs (e.g., Google), or from social data to facilitate vertical image search in a few limited social media platforms (e.g., Flickr). This article aims to combine the virtues of these two information sources to complement each other, that is, sensing and understanding users' interests from social media platforms and transferring this knowledge to rerank the image search results in general image search engines. Toward this goal, we first propose a novel social-sensed image search framework, where both social media and search engine are jointly considered. To effectively and efficiently leverage these two kinds of platforms, we propose an example-based user interest representation and modeling method, where we construct a hybrid graph from social media and propose a hybrid random-walk algorithm to derive the user-image interest graph. Moreover, we propose a social-sensed image reranking method to integrate the user-image interest graph from social media and search results from general image search engines to rerank the images by fusing their social relevance and visual relevance. We conducted extensive experiments on real-world data from Flickr and Google image search, and the results demonstrated that the proposed methods can significantly improve the social relevance of image search results while maintaining visual relevance well. © 2014 ACM 1046-8188/2014/04-ART8 15.00.",Hybrid random walk; Image ranking; Image search; Social media,Behavioral research; Image rankings; Image search; Image search engine; Information sources; Random Walk; Random walk algorithms; Social media; Social media platforms; Search engines
Modeling term associations for probabilistic information retrieval,2014,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84899626748&doi=10.1145%2f2590988&partnerID=40&md5=da1aed3f741f7d3ad9e8cd137914ad82,"Traditionally, in many probabilistic retrieval models, query terms are assumed to be independent. Although such models can achieve reasonably good performance, associations can exist among terms from a human being's point of view. There are some recent studies that investigate how to model term associations/ dependencies by proximity measures. However, the modeling of term associations theoretically under the probabilistic retrieval framework is still largely unexplored. In this article, we introduce a new concept cross term, to model term proximity, with the aim of boosting retrieval performance. With cross terms, the association of multiple query terms can be modeled in the same way as a simple unigram term. In particular, an occurrence of a query term is assumed to have an impact on its neighboring text. The degree of the query-term impact gradually weakens with increasing distance from the place of occurrence. We use shape functions to characterize such impacts. Based on this assumption, we first propose a bigram CRoss TErm Retrieval (CRTER2) model as the basis model, and then recursively propose a generalized n-gram CRoss TErm Retrieval (CRTERn) model for n query terms, where n > 2. Specifically, a bigram cross term occurs when the corresponding query terms appear close to each other, and its impact can be modeled by the intersection of the respective shape functions of the query terms. For an n-gram cross term, we develop several distance metrics with different properties and employ them in the proposed models for ranking. We also show how to extend the language model using the newly proposed cross terms. Extensive experiments on a number of TREC collections demonstrate the effectiveness of our proposed models. ©2014 ACM 1046-8188/2014/04-ART7 15.00.",BM25; Cross term; Kernel; N-gram; Probabilistic information retrieval; Term association,Information retrieval; BM25; Cross-terms; Kernel; N-gram; Probabilistic information; Term associations; Computational linguistics
Efficient index-based snippet generation,2014,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84899650312&doi=10.1145%2f2590972&partnerID=40&md5=f7611e5436ee0b525b4d3f40a5d6adac,"Ranked result lists with query-dependent snippets have become state of the art in text search. They are typically implemented by searching, at query time, for occurrences of the query words in the top-ranked documents. This document-based approach has three inherent problems: (i) when a document is indexed by terms which it does not contain literally (e.g., related words or spelling variants), localization of the corresponding snippets becomes problematic; (ii) each query operator (e.g., phrase or proximity search) has to be implemented twice, on the index side in order to compute the correct result set, and on the snippetgeneration side to generate the appropriate snippets; and (iii) in a worst case, the whole document needs to be scanned for occurrences of the query words, which could be problematic for very long documents. We present a new index-based method that localizes snippets by information solely computed from the index and that overcomes all three problems. Unlike previous index-based methods, we show how to achieve this at essentially no extra cost in query processing time, by a technique we call operator inversion. We also show how our index-based method allows the caching of individual segments instead of complete documents, which enables a significantly larger cache hit-ratio as compared to the document-based approach. We have fully integrated our implementation with the CompleteSearch engine © 2014 ACM 1046-8188/2014/04-ART6 15.00.",Advanced search; Caching; Document summarization; Efficiency; Snippets,Computer networks; Efficiency; Information systems; Advanced search; Caching; Document summarization; Fully integrated; Query operators; Snippet generation; Snippets; State of the art; Natural language processing systems
"Theoretical, qualitative, and quantitative analyses of small-document approaches to resource selection",2014,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84899644609&doi=10.1145%2f2590975&partnerID=40&md5=a92cdb86a7a6195ab856b6f0f947ffb0,"In a distributed retrieval setup, resource selection is the problem of identifying and ranking relevant sources of information for a given user's query. For better usage of existing resource selection techniques, it is desirable to know what the fundamental differences between them are and in what settings one is superior to others. However, little is understood still about the actual behavior of resource selection methods. In this work, we focus on small-document approaches to resource selection that rank and select sources based on the ranking of their documents. We pose a number of research questions and approach them by three types of analyses. First, we present existing small-document techniques in a unified framework and analyze them theoretically. Second, we propose using a qualitative analysis to study the behavior of different smalldocument approaches. Third, we present a novel experimental methodology to evaluate small-document techniques and to validate the results of the qualitative analysis. This way, we answer the posed research questions and provide insights about small-document methods in general and about each technique in particular. ©2014 ACM 1046-8188/2014/04-ART9 15.00.",Distributed information retrieval; Resource selection; Small-document model,Information systems; Distributed information retrieval; Distributed retrieval; Experimental methodology; Qualitative analysis; Research questions; Resource selection; Sources of informations; Unified framework; Computer networks
Suffix array construction in external memory using d-critical substrings,2014,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893343967&doi=10.1145%2f2518175&partnerID=40&md5=efe7874db6170b1f7eb9b6cd1786ae5c,"We present a new suffix array construction algorithm that aims to build, in external memory, the suffix array for an input string of length n measured in the magnitude of tens of Giga characters over a constant or integer alphabet. The core of this algorithm is adapted from the framework of the original internal memory SA-DS algorithm that samples fixed-size d-critical substrings. This new external-memory algorithm, called EM-SA-DS, uses novel cache data structures to construct a suffix array in a sequential scanning manner with good data spatial locality: data is read from or written to disk sequentially. On the assumed externalmemory model with RAM capacity σ((nB)0.5), disk capacity O(n), and size of each I/O block B, allmeasured in log n-bit words, the I/O complexity of EM-SA-DS is O(n/B). This work provides a general cache-based solution that could be further exploited to develop external-memory solutions for other suffix-array-related problems, for example, computing the longest-common-prefix array, using a modern personal computer with a typical memory configuration of 4GB RAM and a single disk. © 2014 ACM.",External memory; Sorting algorithm; Suffix array,Algorithms; Personal computers; Random access storage; Construction algorithms; External memory; External memory algorithms; Integer alphabets; Memory configuration; Sequential scanning; Sorting algorithm; Suffix arrays; Cache memory
Cost-aware collaborative filtering for travel tour recommendations,2014,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893356110&doi=10.1145%2f2559169&partnerID=40&md5=239d7ce2ddaa8e8850313c20768412d7,"Advances in tourism economics have enabled us to collect massive amounts of travel tour data. If properly analyzed, this data could be a source of rich intelligence for providing real-time decision making and for the provision of travel tour recommendations. However, tour recommendation is quite different from traditional recommendations, because the tourist's choice is affected directly by the travel costs, which includes both financial and time costs. To that end, in this article, we provide a focused study of cost-aware tour recommendation. Along this line, we first propose two ways to represent user cost preference. One way is to represent user cost preference by a two-dimensional vector. Another way is to consider the uncertainty about the cost that a user can afford and introduce a Gaussian prior to model user cost preference. With these two ways of representing user cost preference, we develop different cost-aware latent factor models by incorporating the cost information into the probabilistic matrix factorization (PMF) model, the logistic probabilistic matrix factorization (LPMF) model, and the maximum margin matrix factorization (MMMF) model, respectively. When applied to real-world travel tour data, all the cost-aware recommendation models consistently outperform existing latent factor models with a significant margin. © 2014 ACM.",Cost-aware collaborative filtering; Tour recommendation,Collaborative filtering; Cost information; Cost-aware; Gaussian priors; Latent factor models; Matrix factorizations; Probabilistic matrix factorizations; Real-time decision making; Tour recommendation; Costs
Document score distribution models for query performance inference and prediction,2014,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893415817&doi=10.1145%2f2559170&partnerID=40&md5=938758821c085365d17bf9701a0cad8c,"Modelling the distribution of document scores returned from an information retrieval (IR) system in response to a query is of both theoretical and practical importance. One of the goals of modelling document scores in this manner is the inference of document relevance. There has been renewed interest of late in modelling document scores using parameterised distributions. Consequently, a number of hypotheses have been proposed to constrain the mixture distribution from which document scores could be drawn. In this article, we show how a standard performance measure (i.e., average precision) can be inferred from a document score distribution using labelled data. We use the accuracy of the inference of average precision as a measure for determining the usefulness of a particular model of document scores. We provide a comprehensive study which shows that certain mixtures of distributions are able to infer average precision more accurately than others. Furthermore, we analyse a number of mixture distributions with regard to the recall-fallout convexity hypothesis and show that the convexity hypothesis is practically useful. Consequently, based on one of the best-performing score-distribution models, we develop some techniques for query-performance prediction (QPP) by automatically estimating the parameters of the document scoredistribution model when relevance information is unknown. We present experimental results that outline the benefits of this approach to query-performance prediction. © 2014 ACM.",Query performance; Score distributions,Mixtures; Distribution models; Mixture distributions; Practical importance; Query performance; Query-performance predictions; Score distributions; Standard performance; Forecasting
Indexing word sequences for ranked retrieval,2014,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893352010&doi=10.1145%2f2559168&partnerID=40&md5=5612ba4d39a05f8e3baed51c02484cb9,"Formulating and processing phrases and other term dependencies to improve query effectiveness is an important problem in information retrieval. However, accessing word-sequence statistics using inverted indexes requires unreasonable processing time or substantial space overhead. Establishing a balance between these competing space and time trade-offs can dramatically improve system performance. In this article, we present and analyze a new index structure designed to improve query efficiency in dependency retrieval models. By adapting a class of (δ)-approximation algorithms originally proposed for sketch summarization in networking applications, we show how to accurately estimate statistics important in term-dependency models with low, probabilistically bounded error rates. The space requirements for the vocabulary of the index is only logarithmically linked to the size of the vocabulary. Empirically, we show that the sketch index can reduce the space requirements of the vocabulary component of an index of n-grams consisting of between 1 and 4 words extracted from the GOV2 collection to less than 0.01% of the space requirements of the vocabulary of a full index. We also show that larger n-gram queries can be processed considerably more efficiently than in current alternatives, such as positional and next-word indexes. © 2014 ACM.",Indexing; Scalability; Sketching; Term-dependency models,Approximation algorithms; Scalability; Inverted indices; Networking applications; Query efficiency; Ranked retrieval; Retrieval models; Sketching; Space requirements; Term dependency; Indexing (of information)
Learning to recommend descriptive tags for questions in social forums,2014,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893367414&doi=10.1145%2f2559157&partnerID=40&md5=371a00f61f13a21d28918e98cad16fa1,"Around 40% of the questions in the emerging social-oriented question answering forums have at most one manually labeled tag, which is caused by incomprehensive question understanding or informal tagging behaviors. The incompleteness of question tags severely hinders all the tag-based manipulations, such as feeds for topic-followers, ontological knowledge organization, and other basic statistics. This article presents a novel scheme that is able to comprehensively learn descriptive tags for each question. Extensive evaluations on a representative real-world dataset demonstrate that our scheme yields significant gains for question annotation, and more importantly, the whole process of our approach is unsupervised and can be extended to handle large-scale data. © 2014 ACM.",Knowledge organization; Question annotation; Social QA,Computer networks; Information systems; Knowledge organization; Large-scale datum; Question annotation; Question Answering; Real-world; Social QA; Tag-based; Whole process; Knowledge management
Improving text classification accuracy by training label cleaning,2013,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84890369394&doi=10.1145%2f2516889&partnerID=40&md5=cff20d3db6fc07c527bf31bbd7772e9e,"In text classification (TC) and other tasks involving supervised learning, labelled data may be scarce or expensive to obtain. Semisupervised learning and active learning are two strategies whose aim is maximizing the effectiveness of the resulting classifiers for a given amount of training effort. Both strategies have been actively investigated for TC in recent years. Much less research has been devoted to a third such strategy, training label cleaning (TLC), which consists in devising ranking functions that sort the original training examples in terms of how likely it is that the human annotator has mislabelled them. This provides a convenient means for the human annotator to revise the training set so as to improve its quality. Working in the context of boosting-based learning methods for multilabel classification we present three different techniques for performing TLC and, on three widely used TC benchmarks, evaluate them by their capability of spotting training documents that, for experimental reasons only, we have purposefully mislabelled. We also evaluate the degradation in classification effectiveness that these mislabelled texts bring about, and to what extent training label cleaning can prevent this degradation. © 2013 ACM 1046-8188/2013/11-ART19 15.00.",Supervised learning; Synthetic noise; Text classification; Training label cleaning; Training label noise,Classification (of information); Information retrieval systems; Supervised learning; Text processing; Learning methods; Multi-label classifications; Ranking functions; Semi- supervised learning; Synthetic noise; Text classification; Three different techniques; Training documents; Cleaning
"Fidelity, soundness, and efficiency of interleaved comparison methods",2013,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84889560866&doi=10.1145%2f2536736.2536737&partnerID=40&md5=4a6b42bad8d4b300743d39c19c790a0e,"Ranker evaluation is central to the research into search engines, be it to compare rankers or to provide feedback for learning to rank. Traditional evaluation approaches do not scale well because they require explicit relevance judgments of document-query pairs, which are expensive to obtain. A promising alternative is the use of interleaved comparison methods, which compare rankers using click data obtained when interleaving their rankings. In this article, we propose a framework for analyzing interleaved comparison methods. An interleaved comparison method has fidelity if the expected outcome of ranker comparisons properly corresponds to the true relevance of the ranked documents. It is sound if its estimates of that expected outcome are unbiased and consistent. It is efficient if those estimates are accurate with only little data. We analyze existing interleaved comparison methods and find that, while sound, none meet our criteria for fidelity. We propose a probabilistic interleave method, which is sound and has fidelity. We show empirically that, by marginalizing out variables that are known, it is more efficient than existing interleaved comparison methods. Using importance sampling we derive a sound extension that is able to reuse historical data collected in previous comparisons of other ranker pairs. © 2013 ACM 1046-8188/2013/11-ART18 15.00.",Clicks; Importance sampling; Information retrieval; Interleaved comparison; Interleaving; Online evaluation,Information retrieval; Search engines; Clicks; Comparison methods; Evaluation approach; Interleaved comparison; Interleaving; Learning to rank; On-line evaluation; Relevance judgment; Importance sampling
Social link prediction in online social tagging systems,2013,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84890319675&doi=10.1145%2f2516891&partnerID=40&md5=16633ba8391a380334b6d36bee6a83af,"Social networks have become a popular medium for people to communicate and distribute ideas, content, news, and advertisements. Social content annotation has naturally emerged as a method of categorization and filtering of online information. The unrestricted vocabulary users choose from to annotate content has often lead to an explosion of the size of space in which search is performed. In this article, we propose latent topic models as a principled way of reducing the dimensionality of such data and capturing the dynamics of collaborative annotation process. We propose three generative processes to model latent user tastes with respect to resources they annotate with metadata. We show that latent user interests combined with social clues from the immediate neighborhood of users can significantly improve social link prediction in the online music social media site Last.fm. Most link prediction methods suffer from the high class imbalance problem, resulting in low precision and/or recall. In contrast, our proposed classification schemes for social link recommendation achieve high precision and recall with respect to not only the dominant class (nonexistence of a link), but also with respect to sparse positive instances, which are the most vital in social tie prediction. © 2013 ACM 1046-8188/2013/11-ART19 15.00.",Annotation; Collaborative tagging; Graphical models; Last.fm; Link prediction; Link recommendation; Machine learning; Social bookmarking; Social media; Topic models; Unsupervised learning,E-learning; Forecasting; Information filtering; Learning systems; Unsupervised learning; Annotation; Collaborative tagging; GraphicaL model; Last.fm; Link prediction; Link recommendation; Social bookmarking; Social media; Topic model; Social networking (online)
Efficient video stream monitoring for near-duplicate detection and localization in a large-scale repository,2013,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84890339898&doi=10.1145%2f2516890&partnerID=40&md5=918b77bb6ff31ca09f6602def34e82a4,"In this article, we study the efficiency problem of video stream near-duplicate monitoring in a large-scale repository. Existing stream monitoring methods are mainly designed for a short video to scan over a query stream; they have difficulty being scalable for a large number of long videos. We present a simple but effective algorithm called incremental similarity update to address the problem. That is, a similarity upper bound between two videos can be calculated incrementally by leveraging the prior knowledge of the previous calculation. The similarity upper bound takes a lightweight computation to filter out unnecessary time-consuming computation for the actual similarity between two videos, making the search process more efficient.We integrate the algorithm with inverted indexing to obtain a candidate list from the repository for the given query stream. Meanwhile, the algorithm is applied to scan each candidate for locating exact nearduplicate subsequences.We implement several state-of-the-art methods for comparison in terms of accuracy, execution time, and memory consumption. Experimental results demonstrate the proposed algorithm yields comparable accuracy, compact memory size, and more efficient execution time. © 2013 ACM 1046-8188/2013/11-ART21 15.00.",Content-based retrieval; Inverted indexing; Near-duplicate; Video copy,Algorithms; Knowledge management; Video streaming; Effective algorithms; Inverted indexing; Memory consumption; Monitoring methods; Near- duplicates; Near-duplicate detection; State-of-the-art methods; Video copies; Indexing (of information)
Effective and robust query-based stemming,2013,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84890352583&doi=10.1145%2f2536736.2536738&partnerID=40&md5=251358d8d9ea7a8b5a11846a43522fd5,"Stemming is a widely used technique in information retrieval systems to address the vocabulary mismatch problem arising out of morphological phenomena. The major shortcoming of the commonly used stemmers is that they accept the morphological variants of the query words without considering their thematic coherence with the given query, which leads to poor performance. Moreover, for many queries, such approaches also produce retrieval performance that is poorer than no stemming, thereby degrading the robustness. The main goal of this article is to present corpus-based fully automatic stemming algorithms which address these issues. A set of experiments on six TREC collections and three other non-English collections containing news and web documents shows that the proposed query-based stemming algorithms consistently and significantly outperform four state of the art strong stemmers of completely varying principles. Our experiments also confirm that the robustness of the proposed query-based stemming algorithms are remarkably better than the existing strong baselines. © 2013 ACM 1046-8188/2013/11-ART18 15.00.",Corpus; Phrases; Stemming; Suffix,Algorithms; Experiments; Information retrieval systems; Corpus; Mismatch problems; Morphological variants; Phrases; Retrieval performance; Stemming; Stemming algorithms; Suffix; Natural language processing systems
The impacts of structural difference and temporality of tweets on retrieval effectiveness,2013,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84890420812&doi=10.1145%2f2500751&partnerID=40&md5=144011089dbc44f7ae2e760c50491f5c,"To explore the information seeking behaviors in microblogosphere, the microblog track at TREC 2011 introduced a real-time ad-hoc retrieval task that aims at ranking relevant tweets in reverse-chronological order. We study this problem via a two-phase approach: 1) retrieving tweets in an ad-hoc way; 2) utilizing the temporal information of tweets to enhance the retrieval effectiveness of tweets. Tweets can be categorized into two types. One type consists of short messages not containing any URL of a Web page. The other type has at least one URL of a Web page in addition to a short message. These two types of tweets have different structures. In the first phase, to address the structural difference of tweets, we propose a method to rank tweets using the divide-and-conquer strategy. Specifically, we first rank the two types of tweets separately. This produces two rankings, one for each type. Then we merge these two rankings of tweets into one ranking. In the second phase, we first categorize queries into several types by exploring the temporal distributions of their top-retrieved tweets from the first phase; then we calculate the time-related relevance scores of tweets according to the classified types of queries; finally we combine the time scores with the IR scores from the first phase to produce a ranking of tweets. Experimental results achieved by using the TREC 2011 and TREC 2012 queries over the TREC Tweets2011 collection show that: (i) our way of ranking the two types of tweets separately and then merging them together yields better retrieval effectiveness than ranking them simultaneously; (ii) our way of incorporating temporal information into the retrieval process yields further improvements, and (iii) our method compares favorably with state-of-the-art methods in retrieval effectiveness. © 2013 ACM 1046-8188/2013/11-ART21 15.00.",Ad-hoc retrieval of tweets; Learning to rank; Query temporal categorization,Information systems; Ad-hoc retrieval tasks; Ad-hoc retrievals; Information seeking behaviors; Learning to rank; Retrieval effectiveness; State-of-the-art methods; Structural differences; Temporal categorizations; Websites
Mining pure high-order word associations via information geometry for information retrieval,2013,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84894562039&doi=10.1145%2f2493175.2493177&partnerID=40&md5=fee3eae4e2e7e45b3c7fb86fa44f5dc4,"The classical bag-of-word models for information retrieval (IR) fail to capture contextual associations between words. In this article, we propose to investigate pure high-order dependence among a number of words forming an unseparable semantic entity, that is, the high-order dependence that cannot be reduced to the random coincidence of lower-order dependencies. We believe that identifying these pure high-order dependence patterns would lead to a better representation of documents and novel retrieval models. Specifically, two formal definitions of pure dependence-unconditional pure dependence (UPD) and conditional pure dependence (CPD)-are defined. The exact decision on UPD and CPD, however, is NP-hard in general.We hence derive and prove the sufficient criteria that entail UPD and CPD, within the well-principled information geometry (IG) framework, leading to a more feasible UPD/CPD identification procedure. We further develop novel methods for extracting word patterns with pure high-order dependence. Our methods are applied to and extensively evaluated on three typical IR tasks: text classification and text retrieval without and with query expansion. © 2013 ACM.",Information geometry; Pure high-order dependence; Text classification; Text retrieval; Word association,
Studying the clustering paradox and scalability of search in highly distributed environments,2013,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878608184&doi=10.1145%2f2457465.2457468&partnerID=40&md5=8cd4f66d7f0765f71d423f4150c8c58a,"With the ubiquitous production, distribution and consumption of information, today's digital environments such as the Web are increasingly large and decentralized. It is hardly possible to obtain central control over information collections and systems in these environments. Searching for information in these information spaces has brought about problems beyond traditional boundaries of information retrieval (IR) research. This article addresses one important aspect of scalability challenges facing information retrieval models and investigates a decentralized, organic view of information systems pertaining to search in large-scale networks. Drawing on observations from earlier studies, we conduct a series of experiments on decentralized searches in large-scale networked information spaces. Results show that how distributed systems interconnect is crucial to retrieval performance and scalability of searching. Particularly, in various experimental settings and retrieval tasks, we find a consistent phenomenon, namely, the Clustering Paradox, in which the level of network clustering (semantic overlay) imposes a scalability limit. Scalable searches are well supported by a specific, balanced level of network clustering emerging from local system interconnectivity. Departure from that level, either stronger or weaker clustering, leads to search performance degradation, which is dramatic in large-scale networks. © 2013 ACM.",Decentralized search; Efficiency; Information network; Information retrieval; Large-scale distributed system; Loose coupling; Network clustering; Scalability; Self-organization,Efficiency; Information retrieval; Information services; Semantics; Decentralized searches; Information networks; Large-scale distributed system; Loose couplings; Network Clustering; Self organizations; Scalability
Sparse hashing for fast multimedia search,2013,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878612787&doi=10.1145%2f2457465.2457469&partnerID=40&md5=8b8f933fd860ed16a674da075f9a0287,"Hash-based methods achieve fast similarity search by representing high-dimensional data with compact binary codes. However, both generating binary codes and encoding unseen data effectively and efficiently remain very challenging tasks. In this article, we focus on these tasks to implement approximate similarity search by proposing a novel hash based method named sparse hashing (SH for short). To generate interpretable (or semantically meaningful) binary codes, the proposed SH first converts original data into low-dimensional data through a novel nonnegative sparse coding method. SH then converts the low-dimensional data into Hamming space (i.e., binary encoding low-dimensional data) by a new binarization rule. After this, training data are represented by generated binary codes. To efficiently and effectively encode unseen data, SH learns hash functions by taking a-priori knowledge into account, such as implicit group effect of the features in training data, and the correlations between original space and the learned Hamming space. SH is able to perform fast approximate similarity search by efficient bit XOR operations in the memory of a modern PC with short binary code representations. Experimental results show that the proposed SH significantly outperforms state-of-the-art techniques. © 2013 ACM.",Hashing; Indexing; Multimedia search; Sparse coding,Encoding (symbols); Hash functions; Indexing (of information); Code representation; Hashing; High dimensional data; Multimedia search; Non-negative sparse coding; Similarity search; Sparse coding; State-of-the-art techniques; Binary codes
Transfer joint embedding for cross-domain named entity recognition,2013,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878591830&doi=10.1145%2f2457465.2457467&partnerID=40&md5=88543d0159d3a469df4f767470004e8a,"Named Entity Recognition (NER) is a fundamental task in information extraction from unstructured text. Most previous machine-learning-based NER systems are domain-specific, which implies that they may only perform well on some specific domains (e.g., Newswire) but tend to adapt poorly to other related but different domains (e.g., Weblog). Recently, transfer learning techniques have been proposed to NER. However, most transfer learning approaches to NER are developed for binary classification, while NER is a multiclass classification problem in nature. Therefore, one has to first reduce the NER task to multiple binary classification tasks and solve them independently. In this article, we propose a new transfer learning method, named Transfer Joint Embedding (TJE), for cross-domain multiclass classification, which can fully exploit the relationships between classes (labels), and reduce domain difference in data distributions for transfer learning. More specifically, we aim to embed both labels (outputs) and high-dimensional features (inputs) from different domains (e.g., a source domain and a target domain) into a unified low-dimensional latent space, where 1) each label is represented by a prototype and the intrinsic relationships between labels can be measured by Euclidean distance; 2) the distance in data distributions between the source and target domains can be reduced; 3) the source domain labeled data are closer to their corresponding label-prototypes than others. After the latent space is learned, classification on the target domain data can be done with the simple nearest neighbor rule in the latent space. Furthermore, in order to scale up TJE, we propose an efficient algorithm based on stochastic gradient descent (SGD). Finally, we apply the proposed TJE method for NER across different domains on the ACE 2005 dataset, which is a benchmark in Natural Language Processing (NLP). Experimental results demonstrate the effectiveness of TJE and show that TJE can outperform state-of-the-art transfer learning approaches to NER. © 2013 ACM.",Multiclass classification; Named entity recognition; Transfer learning,Algorithms; Learning systems; Natural language processing systems; High-dimensional features; Multi-class classification; Multiclass classification problems; Named entity recognition; NAtural language processing; Stochastic gradient descent; Transfer learning; Transfer learning methods; Intelligent agents
Modeling reformulation using query distributions,2013,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878614473&doi=10.1145%2f2457465.2457466&partnerID=40&md5=020786da1af919c2e39ededd123e919f,"Query reformulation modifies the original query with the aim of better matching the vocabulary of the relevant documents, and consequently improving ranking effectiveness. Previous models typically generate words and phrases related to the original query, but do not consider how these words and phrases would fit together in actual queries. In this article, a novel framework is proposed that models reformulation as a distribution of actual queries, where each query is a variation of the original query. This approach considers an actual query as the basic unit and thus captures important query-level dependencies between words and phrases. An implementation of this framework that only uses publicly available resources is proposed, which makes fair comparisons with other methods using TREC collections possible. Specifically, this implementation consists of a query generation step that analyzes the passages containing query words to generate reformulated queries and a probability estimation step that learns a distribution for reformulated queries by optimizing the retrieval performance. Experiments on TREC collections show that the proposed model can significantly outperform previous reformulation models. © 2013 ACM.",Information retrieval; Passage analysis; Query reformulation; Query segmentation; Query substitution,Information retrieval; Information systems; Passage analysis; Probability estimation; Query distributions; Query generation; Query reformulation; Relevant documents; Retrieval performance; TREC collection; Probability distributions
About learning models with multiple query-dependent features,2013,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84894584768&doi=10.1145%2f2493175.2493176&partnerID=40&md5=0ab898ef64ce670f7a9afcd2a67d8e56,"Several questions remain unanswered by the existing literature concerning the deployment of querydependent features within learning to rank. In this work, we investigate three research questions in order to empirically ascertain best practices for learning-to-rank deployments. (i) Previous work in data fusion that pre-dates learning to rank showed that while different retrieval systems could be effectively combined, the combination of multiple models within the same system was not as effective. In contrast, the existing learning-to-rank datasets (e.g., LETOR), often deploy multiple weighting models as query-dependent features within a single system, raising the question as to whether such a combination is needed. (ii) Next, we investigate whether the training of weighting model parameters, traditionally required for effective retrieval, is necessary within a learning-to-rank context. (iii) Finally, we note that existing learning-to-rank datasets use weighting model features calculated on different fields (e.g., title, content, or anchor text), even though such weighting models have been criticized in the literature. Experiments addressing these three questions are conducted on Web search datasets, using various weighting models as query-dependent and typical query-independent features, which are combined using three learning-to-rank techniques. In particular, we show and explain why multiple weighting models should be deployed as features. Moreover, we unexpectedly find that training the weighting model's parameters degrades learned model's effectiveness. Finally, we show that computing a weighting model separately for each field is less effective than more theoretically-sound field-based weighting models. © 2013 ACM.",Field-based weighting models; Learning to rank; Samples,Acoustic fields; Data fusion; Information retrieval; Learning systems; Sampling; Search engines; Anchor texts; Best practices; Learning models; Multiple queries; Research questions; Retrieval systems; Web searches; Weighting model; Learning to rank
Practical linear-time O(1)-workspace suffix sorting for constant alphabets,2013,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893957402&doi=10.1145%2f2493175.2493180&partnerID=40&md5=180017787aceb12db023c079166b4378,"This article presents an O(n)-time algorithm called SACA-K for sorting the suffixes of an input string T[0, n-1] over an alphabet A[0, K-1]. The problem of sorting the suffixes of T is also known as constructing the suffix array (SA) for T. The theoretical memory usage of SACA-K is nlog K+nlog n+Klog nbits. Moreover, we also have a practical implementation for SACA-K that uses n bytes +(n+ 256) words and is suitable for strings over any alphabet up to full ASCII, where a word is log nbits. In our experiment, SACA-K outperforms SA-IS that was previously the most time- and space-efficient linear-time SA construction algorithm (SACA). SACA-K is around 33% faster and uses a smaller deterministic workspace of K words, where the workspace is the space needed beyond the input string and the output SA. Given K = O(1), SACA-K runs in linear time and O(1) workspace. To the best of our knowledge, such a result is the first reported in the literature with a practical source code publicly available. © 2013 ACM.",Linear time; O(1)-workspace; Sorting algorithm; Suffix array,Information systems; Construction algorithms; Linear time; Memory usage; Sorting algorithm; Space efficient; Suffix arrays; Suffix sorting; Time algorithms; Computer networks
Fast candidate generation for real-time tweet search with bloom filter chains,2013,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84894598573&doi=10.1145%2f2493175.2493178&partnerID=40&md5=364cfdc2ac90128a9c438d7118c49dc6,"The rise of social media and other forms of user-generated content have created the demand for real-time search: against a high-velocity stream of incoming documents, users desire a list of relevant results at the time the query is issued. In the context of real-time search on tweets, this work explores candidate generation in a two-stage retrieval architecture where an initial list of results is processed by a second-stage rescorer to produce the final output. We introduce Bloom filter chains, a novel extension of Bloom filters that can dynamically expand to efficiently represent an arbitrarily long and growing list of monotonically-increasing integers with a constant false positive rate. Using a collection of Bloom filter chains, a novel approximate candidate generation algorithm called BWAND is able to perform both conjunctive and disjunctive retrieval. Experiments show that our algorithm is many times faster than competitive baselines and that this increased performance does not require sacrificing end-to-end effectiveness. Our results empirically characterize the trade-off space defined by output quality, query evaluation speed, and memory footprint for this particular search architecture. © 2013 ACM.",Bloom filters; Efficiency; Scalability; Top-k retrieval; Tweet search,Economic and social effects; Efficiency; Scalability; Bloom filters; Candidate generation; False positive rates; Real-time searches; Search architecture; Top-k retrieval; Tweet search; User-generated content; Data structures
"Behavioral dynamics on the web: Learning, modeling, and prediction",2013,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84894515411&doi=10.1145%2f2493175.2493181&partnerID=40&md5=87bb085085a8a437eba8833addeddf46,"The queries people issue to a search engine and the results clicked following a query change over time. For example, after the earthquake in Japan in March 2011, the query japan spiked in popularity and people issuing the query were more likely to click government-related results than they would prior to the earthquake. We explore the modeling and prediction of such temporal patterns in Web search behavior. We develop a temporal modeling framework adapted from physics and signal processing and harness it to predict temporal patterns in search behavior using smoothing, trends, periodicities, and surprises. Using current and past behavioral data, we develop a learning procedure that can be used to construct models of users' Web search activities. We also develop a novel methodology that learns to select the best prediction model from a family of predictive models for a given query or a class of queries. Experimental results indicate that the predictive models significantly outperform baseline models that weight historical evidence the same for all queries. We present two applications where new methods introduced for the temporal modeling of user behavior significantly improve upon the state of the art. Finally, we discuss opportunities for using models of temporal dynamics to enhance other areas of Web search and information retrieval. © 2013 ACM.",Behavioral analysis; Predictive behavioral models,Behavioral research; Earthquakes; Forecasting; Information retrieval; Learning systems; Search engines; Signal processing; Websites; Behavioral analysis; Behavioral dynamics; Learning procedures; Modeling and predictions; Novel methodology; Predictive behavioral models; Temporal modeling; Web search behavior; Predictive analytics
Discovering tasks from search engine query logs,2013,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84894573532&doi=10.1145%2f2493175.2493179&partnerID=40&md5=46d69e4420f0060d7d8875d1c1739acb,"Although Web search engines still answer user queries with lists of ten blue links to webpages, people are increasingly issuing queries to accomplish their daily tasks (e.g., finding a recipe, booking a flight, reading online news, etc.). In this work, we propose a two-step methodology for discovering tasks that users try to perform through search engines. First, we identify user tasks from individual user sessions stored in search engine query logs. In our vision, a user task is a set of possibly noncontiguous queries (within a user search session), which refer to the same need. Second, we discover collective tasks by aggregating similar user tasks, possibly performed by distinct users. To discover user tasks, we propose query similarity functions based on unsupervised and supervised learning approaches.We present a set of query clustering methods that exploit these functions in order to detect user tasks. All the proposed solutions were evaluated on a manually-built ground truth, and two of them performed better than state-of-the-art approaches. To detect collective tasks, we propose four methods that cluster previously discovered user tasks, which in turn are represented by the bag-of-words extracted from their composing queries. These solutions were also evaluated on another manually-built ground truth. © 2013 ACM.",Collective task discovery; Collective tasks; Query clustering; Query log analysis; User search intent; User search session boundaries; User task discovery; User tasks,Query processing; Search engines; Websites; Collective task discovery; Collective tasks; Query clustering; Query log analysis; Search intents; Search sessions; User task discovery; User tasks; Information retrieval
Efficient fuzzy search in large text collections,2013,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878528757&doi=10.1145%2f2457465.2457470&partnerID=40&md5=dfcbac8a3afb8ef6d7a9276f20d4c14b,"We consider the problem of fuzzy full-text search in large text collections, that is, full-text search which is robust against errors both on the side of the query as well as on the side of the documents. Standard inverted-index techniques work extremely well for ordinary full-text search but fail to achieve interactive query times (below 100 milliseconds) for fuzzy full-text search even on moderately-sized text collections (above 10 GBs of text). We present new preprocessing techniques that achieve interactive query times on large text collections (100 GB of text, served by a single machine). We consider two similarity measures, one where the query terms match similar terms in the collection (e.g., algorithm matches algoritm or vice versa) and one where the query terms match terms with a similar prefix in the collection (e.g., alori matches algorithm). The latter is important when we want to display results instantly after each keystroke (search as you type). All algorithms have been fully integrated into the CompleteSearch engine. © 2013 ACM 1046-8188/2013/05-ART10 $15.00.",Approximate dictionary search; Approximate text search; Error tolerant autocomplete; Fuzzy search; HYB index; Inverted index,Information systems; Approximate dictionary search; Approximate text searches; Error tolerant; Fuzzy searches; HYB index; Inverted indices; Algorithms
Regularized latent semantic indexing: A new approach to large-scale topic modeling,2013,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873891666&doi=10.1145%2f2414782.2414787&partnerID=40&md5=9a8a4b83d7fb6d176f82627e93882b0b,"Topic modeling provides a powerful way to analyze the content of a collection of documents. It has become a popular tool in many research areas, such as text mining, information retrieval, natural language processing, and other related fields. In real-world applications, however, the usefulness of topic modeling is limited due to scalability issues. Scaling to larger document collections via parallelization is an active area of research, but most solutions require drastic steps, such as vastly reducing input vocabulary. In this article we introduce Regularized Latent Semantic Indexing (RLSI)-including a batch version and an online version, referred to as batch RLSI and online RLSI, respectively-to scale up topic modeling. Batch RLSI and online RLSI are as effective as existing topic modeling techniques and can scale to larger datasets without reducing input vocabulary. Moreover, online RLSI can be applied to stream data and can capture the dynamic evolution of topics. Both versions of RLSI formalize topic modeling as a problem of minimizing a quadratic loss function regularized by L1 and/or L2 norm. This formulation allows the learning process to be decomposed into multiple suboptimization problems which can be optimized in parallel, for example, via MapReduce.We particularly propose adopting L1 norm on topics and 2 norm on document representations to create a model with compact and readable topics and which is useful for retrieval. In learning, batch RLSI processes all the documents in the collection as a whole, while online RLSI processes the documents in the collection one by one. We also prove the convergence of the learning of online RLSI. Relevance ranking experiments on three TREC datasets show that batch RLSI and online RLSI perform better than LSI, PLSI, LDA, and NMF, and the improvements are sometimes statistically significant. Experiments on a Web dataset containing about 1.6 million documents and 7 million terms, demonstrate a similar boost in performance. © 2013 ACM 1046-8188/2013/01- ART2 s15.00.",Distributed learning; Online learning; Regularization; Sparse methods; Topic modeling,Data mining; Experiments; Indexing (of information); Information retrieval systems; Natural language processing systems; Text processing; Distributed learning; Online learning; Regularization; Sparse methods; Topic Modeling; E-learning
Approximate recall confidence intervals,2013,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873884780&doi=10.1145%2f2414782.2414784&partnerID=40&md5=14f573ddbacf47a8ad67276634a859bc,"Recall, the proportion of relevant documents retrieved, is an important measure of effectiveness in information retrieval, particularly in the legal, patent, and medical domains. Where document sets are too large for exhaustive relevance assessment, recall can be estimated by assessing a random sample of documents, but an indication of the reliability of this estimate is also required. In this article, we examine several methods for estimating two-tailed recall confidence intervals. We find that the normal approximation in current use provides poor coverage in many circumstances, even when adjusted to correct its inappropriate symmetry. Analytic and Bayesian methods based on the ratio of binomials are generally more accurate but are inaccurate on small populations. The method we recommend derives beta-binomial posteriors on retrieved and unretrieved yield, with fixed hyperparameters, and a Monte Carlo estimate of the posterior distribution of recall. We demonstrate that this method gives mean coverage at or near the nominal level, across several scenarios, while being balanced and stable. We offer advice on sampling design, including the allocation of assessments to the retrieved and unretrieved segments, and compare the proposed beta-binomial with the officially reported normal intervals for recent TREC Legal Track iterations. © 2013 ACM 1046-8188/2013/01-ART2 s15.00.",Posterior distributions; Probabilistic models,Bayesian networks; Estimation; Probability distributions; Bayesian methods; Confidence interval; Document sets; Hyperparameters; Measure of effectiveness; Medical domains; Monte Carlo estimates; Normal approximation; Posterior distributions; Probabilistic models; Random sample; Relevance assessments; Relevant documents; Sampling design; Small population; Monte Carlo methods
The effect of social and physical detachment on information need,2013,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873911298&doi=10.1145%2f2414782.2414786&partnerID=40&md5=448d956b52d53cdd993ee689cdbd2a9a,"The information need of users and the documents which answer this need are frequently contingent on the different characteristics of users. This is especially evident during natural disasters, such as earthquakes and violent weather incidents, which create a strong transient information need. In this article, we investigate how the information need of users, as expressed by their queries, is affected by their physical detachment, as estimated by their physical location in relation to that of the event, and by their social detachment, as quantified by the number of their acquaintances who may be affected by the event. Drawing on large-scale data from ten major events, we show that social and physical detachment levels of users are a major influence on their search engine queries. We demonstrate how knowing social and physical detachment levels can assist in improving retrieval for two applications: identifying search queries related to events and ranking results in response to event-related queries. We find that the average precision in identifying relevant search queries improves by approximately 18%, and that the average precision of ranking that uses detachment information improves by 10%. Using both types of detachment achieved a larger gain in performance than each of them separately. © 2013 ACM 1046-8188/2013/01-ART2 s15.00.",Distance; Information; Need; Physical; Social,Search engines; Distance; Information; Need; Physical; Social; Information science
X-Class: Associative classification of XML documents by structure,2013,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873410735&doi=10.1145%2f2414782.2414785&partnerID=40&md5=37252a878fb9bd97da7e7530b94a5e69,"The supervised classification of XML documents by structure involves learning predictive models in which certain structural regularities discriminate the individual document classes. Hitherto, research has focused on the adoption of prespecified substructures. This is detrimental for classification effectiveness, since the a priori chosen substructures may not accord with the structural properties of the XML documents. Therein, an unexplored question is how to choose the type of structural regularity that best adapts to the structures of the available XML documents. We tackle this problem through X-Class, an approach that handles all types of tree-like substructures and allows for choosing the most discriminatory one. Algorithms are designed to learn compact rule-based classifiers in which the chosen substructures discriminate the classes of XML documents. X-Class is studied across various domains and types of substructures. Its classification performance is compared against several rule-based and SVM-based competitors. Empirical evidence reveals that the classifiers induced by X-Class are compact, scalable, and at least as effective as the established competitors. In particular, certain substructures allow the induction of very compact classifiers that generally outperform the rule-based competitors in terms of effectiveness over all chosen corpora of XML data. Furthermore, such classifiers are substantially as effective as the SVM-based competitor, with the additional advantage of a high-degree of interpretability. © 2013 ACM 1046-8188/2013/01-ART2 s15.00.",Structural XML classification; XML mining; XML transactional modeling,XML; Associative classification; Classification performance; Interpretability; Predictive models; Rule based; Rule-based classifier; Structural regularity; Supervised classification; Xml classifications; XML data; XML mining; Information retrieval systems
Enriching documents with examples: A corpus mining approach,2013,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873891290&doi=10.1145%2f2414782.2414783&partnerID=40&md5=24e43becbede97f07a7acd87673cdbe4,"Software developers increasingly rely on information from the Web, such as documents or code examples on application programming interfaces (APIs), to facilitate their development processes. However, API documents often do not include enough information for developers to fully understand how to use the APIs, and searching for good code examples requires considerable effort. To address this problem, we propose a novel code example recommendation system that combines the strength of browsing documents and searching for code examples and returns API documents embedded with high-quality code example summaries mined from the Web. Our evaluation results show that our approach provides code examples with high precision and boosts programmer productivity. © 2013 ACM 1046-8188/2013/01-ART2 s15.00.",API document; Clustering; Code search; Ranking,Application programs; API document; Clustering; Code search; Development process; Evaluation results; Good code; High precision; High quality; Programmer productivity; Ranking; Software developer; Application programming interfaces (API)
Efficient entity translation mining: A parallelized graph alignment approach,2012,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871250455&doi=10.1145%2f2382438.2382444&partnerID=40&md5=40fd824f9dab3d0ecc956f70710f1519,"This article studies the problem of mining entity translation, specifically, mining English and Chinese name pairs. Existing efforts can be categorized into (a) transliteration-based approaches that leverage phonetic similarity and (b) corpus-based approaches that exploit bilingual cooccurrences. These approaches suffer from inaccuracy and scarcity, respectively. In clear contrast, we use under-leveraged resources of monolingual entity cooccurrences crawled from entity search engines, which are represented as two entityrelationship graphs extracted from two language corpora, respectively. Our problem is then abstracted as finding correct mappings across two graphs. To achieve this goal, we propose a holistic approach to exploiting both transliteration similarity and monolingual cooccurrences. This approach, which builds upon monolingual corpora, complements existing corpus-based work requiring scarce resources of parallel or comparable corpus while significantly boosting the accuracy of transliteration-based work. In addition, by parallelizing the mapping process on multicore architectures, we speed up the computation by more than 10 times per unit accuracy. We validated the effectiveness and efficiency of our proposed approach using real-life datasets. © 2012 ACM.",,Software architecture; Comparable corpora; Entity-relationship; Holistic approach; Mapping process; Multicore architectures; Parallelizing; Per unit; Real life datasets; Scarce resources; Two-graphs; Search engines
Sentimental spidering: Leveraging opinion information in focused crawlers,2012,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871244231&doi=10.1145%2f2382438.2382443&partnerID=40&md5=b86948490c2a1bb010793f7e5b69d993,"Despite the increased prevalence of sentiment-related information on the Web, there has been limited work on focused crawlers capable of effectively collecting not only topic-relevant but also sentiment-relevant content. In this article, we propose a novel focused crawler that incorporates topic and sentiment information as well as a graph-based tunneling mechanism for enhanced collection of opinion-rich Web content regarding a particular topic. The graph-based sentiment (GBS) crawler uses a text classifier that employs both topic and sentiment categorization modules to assess the relevance of candidate pages. This information is also used to label nodes in web graphs that are employed by the tunneling mechanism to improve collection recall. Experimental results on two test beds revealed that GBS was able to provide better precision and recall than seven comparison crawlers. Moreover, GBS was able to collect a large proportion of the relevant content after traversing far fewer pages than comparison methods. GBS outperformed comparison methods on various categories of Web pages in the test beds, including collection of blogs, Web forums, and social networking Web site content. Further analysis revealed that both the sentiment classification module and graph-based tunneling mechanism played an integral role in the overall effectiveness of the GBS crawler. © 2012 ACM.",,Equipment testing; Websites; Comparison methods; Focused crawler; Graph-based; Overall effectiveness; Precision and recall; Sentiment classification; Text classifiers; Tunneling mechanism; Web content; Web Forums; Web graphs; Web site contents; Graphic methods
Aggregation methods for proximity-based opinion retrieval,2012,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871251184&doi=10.1145%2f2382438.2382445&partnerID=40&md5=6ffbff8eb25510a384181f26d0cfac2b,"The enormous amount of user-generated data available on the Web provides a great opportunity to understand, analyze, and exploit people's opinions on different topics. Traditional Information Retrieval methods consider the relevance of documents to a topic but are unable to differentiate between subjective and objective documents. Opinion retrieval is a retrieval task in which not only the relevance of a document to the topic is important but also the amount of opinion expressed in the document about the topic. In this article, we address the blog post opinion retrieval task and propose methods that rank blog posts according to their relevance and opinionatedness toward a topic. We propose estimating the opinion density at each position in a document using a general opinion lexicon and kernel density functions. We propose and investigate different models for aggregating the opinion density at query terms positions to estimate the opinion score of every document. We then combine the opinion score with the relevance score based on a probabilistic justification. Experimental results on the BLOG06 dataset show that the proposed method provides significant improvement over the standard TREC baselines. The proposed models also achieve much higher performance compared to all state of the art methods. © 2012 ACM.",,Blogs; Aggregation methods; Data sets; Kernel density function; Query terms; Relevance score; State-of-the-art methods; Information retrieval
Detecting fake medical web sites using recursive trust labeling,2012,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871254978&doi=10.1145%2f2382438.2382441&partnerID=40&md5=d26048678694f53fa063444911f6caa0,"Fake medicalWeb sites have become increasingly prevalent. Consequently, much of the health-related information and advice available online is inaccurate and/or misleading. Scores of medical institution Web sites are for organizations that do not exist and more than 90% of online pharmacy Web sites are fraudulent. In addition to monetary losses exacted on unsuspecting users, these fake medical Web sites have severe public safety ramifications. According to a World Health Organization report, approximately half the drugs sold on the Web are counterfeit, resulting in thousands of deaths. In this study, we propose an adaptive learning algorithm called recursive trust labeling (RTL). RTL uses underlying content and graph-based classifiers, coupled with a recursive labeling mechanism, for enhanced detection of fake medical Web sites. The proposed method was evaluated on a test bed encompassing nearly 100 million links between 930,000Web sites, including 1,000 known legitimate and fake medical sites. The experimental results revealed that RTL was able to significantly improve fake medical Web site detection performance over 19 comparison content and graph-based methods, various meta-learning techniques, and existing adaptive learning approaches, with an overall accuracy of over 94%. Moreover, RTL was able to attain high performance levels even when the training dataset composed of as little as 30 Web sites. With the increased popularity of eHealth and Health 2.0, the results have important implications for online trust, security, and public safety. © 2012 ACM.",,Equipment testing; Health; Learning algorithms; Adaptive learning; Adaptive learning algorithm; Ehealth; Graph-based; Graph-based methods; Medical institutions; Meta-learning techniques; Online trust; Performance level; Public safety; Site detection; Training dataset; World Health Organization; Websites
An online learning framework for refining recency search results with user click feedback,2012,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871253648&doi=10.1145%2f2382438.2382439&partnerID=40&md5=d52b8971133e7361f6e0535571a12cb6,"Traditional machine-learned ranking systems for Web search are often trained to capture stationary relevance of documents to queries, which have limited ability to track nonstationary user intention in a timely manner. In recency search, for instance, the relevance of documents to a query on breaking news often changes significantly over time, requiring effective adaptation to user intention. In this article, we focus on recency search and study a number of algorithms to improve ranking results by leveraging user click feedback. Our contributions are threefold. First, we use commercial search engine sessions collected in a random exploration bucket for reliable offline evaluation of these algorithms, which provides an unbiased comparison across algorithms without online bucket tests. Second, we propose an online learning approach that reranks and improves the search results for recency queries near real-time based on user clicks. This approach is very general and can be combined with sophisticated click models. Third, our empirical comparison of a dozen algorithms on real-world search data suggests importance of a few algorithmic choices in these applications, including generalization across different query-document pairs, specialization to popular queries, and near real-time adaptation of user clicks for reranking. © 2012 ACM.",,Algorithms; Search engines; Empirical comparison; Nonstationary; Offline evaluation; Online learning; Ranking system; Re-ranking; Real-time adaptation; Recency queries; Search results; User intention; Web searches; E-learning
Stability of recommendation algorithms,2012,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871230673&doi=10.1145%2f2382438.2382442&partnerID=40&md5=afeef904507666c76c1ab1f6bce3c32e,"The article explores stability as a new measure of recommender systems performance. Stability is defined to measure the extent to which a recommendation algorithm provides predictions that are consistent with each other. Specifically, for a stable algorithm, adding some of the algorithm's own predictions to the algorithm's training data (for example, if these predictions were confirmed as accurate by users) would not invalidate or change the other predictions. While stability is an interesting theoretical property that can provide additional understanding about recommendation algorithms, we believe stability to be a desired practical property for recommender systems designers as well, because unstable recommendations can potentially decrease users' trust in recommender systems and, as a result, reduce users' acceptance of recommendations. In this article, we also provide an extensive empirical evaluation of stability for six popular recommendation algorithms on four real-world datasets. Our results suggest that stability performance of individual recommendation algorithms is consistent across a variety of datasets and settings. In particular, we find that model-based recommendation algorithms consistently demonstrate higher stability than neighborhood-based collaborative filtering techniques. In addition, we perform a comprehensive empirical analysis of many important factors (e.g., the sparsity of original rating data, normalization of input data, the number of new incoming ratings, the distribution of incoming ratings, the distribution of evaluation data, etc.) and report the impact they have on recommendation stability. © 2012 ACM.",,Algorithms; Forecasting; Recommender systems; Collaborative filtering techniques; Data sets; Empirical analysis; Empirical evaluations; Input datas; Real-world datasets; Recommendation algorithms; Stability performance; Stable algorithms; Training data; Users' acceptance; System stability
Detecting and tracking topics and events from web search logs,2012,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871191904&doi=10.1145%2f2382438.2382440&partnerID=40&md5=63989daf78f2b7ad01d2857fa95f6f70,"Recent years have witnessed increased efforts on detecting topics and events from Web search logs, since this kind of data not only capture web content but also reflect the users' activities. However, the majority of existing work is focused on exploiting clustering techniques for topic and event detection. Due to the huge size and the evolving nature of Web data, existing clustering approaches are limited to meet the realtime demand. To that end, in this article, we propose a method called LETD to detect evolving topics in a timely manner. Also, we design the techniques to extract events from topics and to infer the evolving relationship among the events. For topic detection, we first provide a measurement to select the important URLs, which are most likely to describe a real-life topic. Then, starting from these selected URLs, we exploit the local expansion method to find other topic-related URLs. Moreover, in the LETD framework, we design algorithms based on Random Walk and Markov Random Fields (MRF), respectively. Because the LETD method exploits a divide-and-conquer strategy to process the data, it is more efficient than existing methods based on clustering techniques. To better illustrate the LETD framework, we develop a demo system StoryTeller which can discover hot topics and events, infer the evolving relationships among events, and visualize information in a storytelling way. This demo system can provide a global view of the topic development and help users target the interesting events more conveniently. Finally, experimental results on real-world Microsoft click-through data have shown that StoryTeller can find real-life hot topics and meaningful evolving relationships among events, and has also demonstrated the efficiency and effectiveness of the LETD method. © 2012 ACM.",,Image retrieval; Information retrieval systems; Clickthrough data; Clustering approach; Clustering techniques; Divide and conquer; Event detection; Global view; Local expansion; Markov Random Fields; MicroSoft; Random Walk; Real time; Topic detection; Web content; Web data; Web search logs; Websites
The nonverbal structure of patient case discussions in multidisciplinary medical team meetings,2012,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867319404&doi=10.1145%2f2328967.2328970&partnerID=40&md5=0583cedb18d39623a2dd43c782b8f254,"Meeting analysis has a long theoretical tradition in social psychology, with established practical ramifications in computer science, especially in computer supported cooperative work. More recently, a good deal of research has focused on the issues of indexing and browsing multimedia records of meetings. Most research in this area, however, is still based on data collected in laboratories, under somewhat artificial conditions. This article presents an analysis of the discourse structure and spontaneous interactions at real-life multidisciplinary medical team meetings held as part of the work routine in a major hospital. It is hypothesized that the conversational structure of these meetings, as indicated by sequencing and duration of vocalizations, enables segmentation into individual patient case discussions. The task of segmenting audio-visual records of multidisciplinary medical team meetings is described as a topic segmentation task, and a method for automatic segmentation is proposed. An empirical evaluation based on hand labelled data is presented, which determines the optimal length of vocalization sequences for segmentation, and establishes the competitiveness of the method with approaches based on more complex knowledge sources. The effectiveness of Bayesian classification as a segmentation method, and its applicability to meeting segmentation in other domains are discussed. © 2012 ACM.",Audio analysis; Dialogue segmentation; Meeting analysis; Multidisciplinary medical team meetings; Search of spontaneous speech,Information systems; Audio analysis; Audio-visual; Automatic segmentations; Bayesian classification; Discourse structure; Empirical evaluations; Knowledge sources; Medical teams; Meeting analysis; Patient case; Segmentation methods; Social psychology; Spontaneous interaction; Spontaneous speech; Competition
Special issue on searching speech,2012,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867324515&doi=10.1145%2f2328967.2328968&partnerID=40&md5=be8eff43d15312ca524e48f9ece29bcb,[No abstract available],,
"Sibyl, a factoid question-answering system for spoken documents",2012,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865235825&doi=10.1145%2f2328967.2328972&partnerID=40&md5=e7da382e2efdd72426fa25a20d5f86ae,"In this article, we present a factoid question-answering system, Sibyl, specifically tailored for question answering (QA) on spoken-word documents. This work explores, for the first time, which techniques can be robustly adapted from the usual QA on written documents to the more difficult spoken document scenario. More specifically, we study new information retrieval (IR) techniques designed or speech, and utilize several levels of linguistic information for the speech-based QA task. These include named-entity detection with phonetic information, syntactic parsing applied to speech transcripts, and the use of coreference resolution. Sibyl is largely based on supervised machine-learning techniques, with special focus on the answer extraction step, and makes little use of handcrafted knowledge. Consequently, it should be easily adaptable to other domains and languages. Sibyl and all its modules are extensively evaluated on the European Parliament Plenary Sessions English corpus, comparing manual with automatic transcripts obtained by three different automatic speech recognition (ASR) systems that exhibit significantly different word error rates. This data belongs to the CLEF 2009 track for QA on speech transcripts. The main results confirm that syntactic information is very useful for learning to rank question candidates, improving results on both manual and automatic transcripts, unless the ASR quality is very low. At the same time, our experiments on coreference resolution reveal that the state-of-the-art technology is not mature enough to be effectively exploited for QA with spoken documents. Overall, the performance of Sibyl is comparable or better than the state-of-the-art on this corpus, confirming the validity of our approach. © 2012 ACM.",Question answering; Spoken document retrieval,Artificial intelligence; Linguistics; Natural language processing systems; Answer extraction; Automatic speech recognition system; Coreference resolution; European Parliament; Learning to rank; Linguistic information; Machine learning techniques; Phonetic information; Plenary sessions; Question Answering; Question answering systems; Speech transcripts; Spoken document; Spoken document retrieval; State-of-the-art technology; Syntactic information; Syntactic parsing; Word error rate; Written documents; Speech recognition
Direct posterior confidence for out-of-vocabulary spoken term detection,2012,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865274432&doi=10.1145%2f2328967.2328969&partnerID=40&md5=8b42daa0b5b427215051603b66659da2,"Spoken term detection (STD) is a key technology for spoken information retrieval. As compared to the conventional speech transcription and keyword spotting, STD is an open-vocabulary task and has to address out-of-vocabulary (OOV) terms. Approaches based on subword units, for example phones, are widely used to solve the OOV issue; however, performance on OOV terms is still substantially inferior to that of in-vocabulary (INV) terms. The performance degradation on OOV terms can be attributed to a multitude of factors. One particular factor we address in this article is the unreliable confidence estimation caused by weak acoustic and language modeling due to the absence of OOV terms in the training corpora. We propose a direct posterior confidence derived from a discriminative model, such as multilayer perceptron (MLP). The new confidence considers a wide-range acoustic context which is usually important for speech recognition and retrieval; moreover, it localizes on detected speech segments and therefore avoids the impact of long-span word context which is usually unreliable for OOV term detection. In this article, we first develop an extensive discussion about the modeling weakness problem associated with OOV terms, and then propose our approach to address this problem based on direct poster confidence. Our experiments carried out on spontaneous and conversational multiparty meeting speech, demonstrate that the proposed technique provides a significant improvement in STD performance as compared to conventional lattice-based confidence, in particular for OOV terms. Furthermore, the new confidence estimation approach is fused with other advanced techniques for OOV treatment, such as stochastic pronunciation modeling and discriminative confidence normalization. This leads to an integrated solution for OOV term detection that results in a large performance improvement. © 2012 ACM.",Speech recognition; Spoken term detection; Spontaneous speech search,Computational linguistics; Speech recognition; Speech transmission; Confidence estimation; Discriminative models; Integrated solutions; Key technologies; Keyword spotting; Language modeling; Long span; Multi layer perceptron; Performance degradation; Performance improvements; Problem-based; Pronunciation modeling; Speech segments; Speech transcriptions; Spontaneous speech; Subword units; Training corpus; Word contexts; Information retrieval systems
Comparison of methods for language-dependent and language-independent query-by-example spoken term detection,2012,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865263089&doi=10.1145%2f2328967.2328971&partnerID=40&md5=10edfa4abbeaa409402b31959d2a84e5,"This article investigates query-by-example (QbE) spoken term detection (STD), in which the query is not entered as text, but selected in speech data or spoken. Two feature extractors based on neural networks (NN) are introduced: the first producing phone-state posteriors and the second making use of a compressive NN layer. They are combined with three different QbE detectors: while the Gaussian mixture model/hidden Markov model (GMM/HMM) and dynamic time warping (DTW) both work on continuous feature vectors, the third one, based on weighted finite-state transducers (WFST), processes phone lattices. QbE STD is compared to two standard STD systems with text queries: acoustic keyword spotting and WFST-based search of phone strings in phone lattices. The results are reported on four languages (Czech, English, Hungarian, and Levantine Arabic) using standard metrics: equal error rate (EER) and two versions of popular figureof-merit (FOM). Language-dependent and language-independent cases are investigated; the latter being particularly interesting for scenarios lacking standard resources to train speech recognition systems. While the DTW and GMM/HMM approaches produce the best results for a language-dependent setup depending on the target language, the GMM/HMM approach performs the best dealing with a language-independent setup. As far as WFSTs are concerned, they are promising as they allow for indexing and fast search. © 2012 ACM.",Bottleneck features; DTW-based query-by-example; GMM/HMM-based query-by-example; Keyword spotting; Query-by-example; WFST-based query-by-example,Detectors; Markov processes; Speech recognition; Telephone sets; Bottleneck features; Comparison of methods; Dynamic time warping; Equal error rate; Fast search; Feature extractor; Feature vectors; Gaussian Mixture Model; Hungarians; Keyword spotting; Markov model; Query-by-example; Speech data; Speech recognition systems; Standard metrics; Target language; Text query; Weighted finite-state transducers; Telephone systems
Oracle in image search: A content-based approach to performance prediction,2012,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863771833&doi=10.1145%2f2180868.2180875&partnerID=40&md5=0aebe2a26f79eddf3701b4d48fe1a818,"This article studies a novel problem in image search. Given a text query and the image ranking list returned by an image search system, we propose an approach to automatically predict the search performance. We demonstrate that, in order to estimate the mathematical expectations of Average Precision (AP) and Normalized Discounted Cumulative Gain (NDCG), we only need to predict the relevance probability of each image. We accomplish the task with a query-adaptive graph-based learning based on the images' ranking order and visual content. We validate our approach with a large-scale dataset that contains the image search results of 1, 165 queries from 4 popular image search engines. Empirical studies demonstrate that our approach is able to generate predictions that are highly correlated with the real search performance. Based on the proposed image search performance prediction scheme, we introduce three applications: image metasearch, multilingual image search, and Boolean image search. Comprehensive experiments are conducted to validate our approach. © 2012 ACM.",Graph-based learning; Image search; Search performance prediction,Forecasting; Graphic methods; Content-based approach; Data sets; Empirical studies; Graph-based learning; Highly-correlated; Image search; Image search engine; Image search system; Mathematical expectation; Metasearch; Performance prediction; Search performance; Text query; Visual content; Search engines
Predicting query performance by query-drift estimation,2012,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863752368&doi=10.1145%2f2180868.2180873&partnerID=40&md5=5b21e858cc3c308f367ac1d172c0bf86,"Predicting query performance, that is, the effectiveness of a search performed in response to a query, is a highly important and challenging problem. We present a novel approach to this task that is based on measuring the standard deviation of retrieval scores in the result list of the documents most highly ranked. We argue that for retrieval methods that are based on document-query surface-level similarities, the standard deviation can serve as a surrogate for estimating the presumed amount of query drift in the result list, that is, the presence (and dominance) of aspects or topics not related to the query in documents in the list. Empirical evaluation demonstrates the prediction effectiveness of our approach for several retrieval models. Specifically, the prediction quality often transcends that of current state-of-the-art prediction methods. © 2012 ACM.",Query drift; Query-performance prediction; Score distribution,Information retrieval; Statistics; Empirical evaluations; Prediction methods; Prediction quality; Query drift; Query performance; Retrieval methods; Retrieval models; Score distribution; Standard deviation; Forecasting
Approaches to exploring category information for question retrieval in community question-answer archives,2012,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863753626&doi=10.1145%2f2180868.2180869&partnerID=40&md5=930d4fdd2c09d981508bc9462918536a,"Community Question Answering (CQA) is a popular type of service where users ask questions and where answers are obtained from other users or from historical question-answer pairs. CQA archives contain large volumes of questions organized into a hierarchy of categories. As an essential function of CQA services, question retrieval in a CQA archive aims to retrieve historical question-answer pairs that are relevant to a query question. This article presents several new approaches to exploiting the category information of questions for improving the performance of question retrieval, and it applies these approaches to existing question retrieval models, including a state-of-the-art question retrieval model. Experiments conducted on real CQA data demonstrate that the proposed techniques are effective and efficient and are capable of outperforming a variety of baseline methods significantly. © 2012 ACM.",Categorization; Clusterbased retrieval; Question search; Question-answering services,Natural language processing systems; Baseline methods; Categorization; Cluster-based retrieval; Question Answering; Question search; Question-answer pairs; Retrieval models; Type of services; Query processing
A measurement framework for evaluating emulators for digital preservation,2012,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863765861&doi=10.1145%2f2180868.2180876&partnerID=40&md5=69018097ee4264e1deee0f2f89e1bf28,"Accessible emulation is often the method of choice for maintaining digital objects, specifically complex ones such as applications, business processes, or electronic art. However, validating the emulator's ability to faithfully reproduce the original behavior of digital objects is complicated. This article presents an evaluation framework and a set of tests that allow assessment of the degree to which system emulation preserves original characteristics and thus significant properties of digital artifacts. The original system, hardware, and software properties are described. Identical environment is then recreated via emulation. Automated user input is used to eliminate potential confounders. The properties of a rendered form of the object are then extracted automatically or manually either in a target state, a series of states, or as a continuous stream. The concepts described in this article enable preservation planners to evaluate how emulation affects the behavior of digital objects compared to their behavior in the original environment. We also review how these principles can and should be applied to the evaluation of migration and other preservation strategies as a general principle of evaluating the invocation and faithful rendering of digital objects and systems. The article concludes with design requirements for emulators developed for digital preservation tasks. © 2012 ACM.",Automated testing; Characteristics of rendered content; Digital preservation; Emulation; Preserving interactive content; System properties,Information systems; Automated testing; Characteristics of rendered content; Digital preservation; Emulation; Preserving interactive content; System property; Digital storage
A probabilistic model to combine tags and acoustic similarity for music retrieval,2012,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863753650&doi=10.1145%2f2180868.2180870&partnerID=40&md5=036c0b8bad80ed6c9cf97ab937a3eb7b,"The rise of the Internet has led the music industry to a transition from physical media to online products and services. As a consequence, current online music collections store millions of songs and are constantly being enriched with new content. This has created a need for music technologies that allow users to interact with these extensive collections efficiently and effectively. Music search and discovery may be carried out using tags, matching user interests and exploiting content-based acoustic similarity. One major issue in music information retrieval is how to combine such noisy and heterogeneous information sources in order to improve retrieval effectiveness. With this aim in mind, the article explores a novel music retrieval framework based on combining tags and acoustic similarity through a probabilistic graph-based representation of a collection of songs. The retrieval function highlights the path across the graph that most likely observes a user query and is used to improve state-of-the-art music search and discovery engines by delivering more relevant ranking lists. Indeed, by means of an empirical evaluation, we show how the proposed approach leads to better performances than retrieval strategies which rank songs according to individual information sources alone or which use a combination of them. © 2012 ACM.",Acoustic similarity; Graph structure; Music discovery; Music information retrieval; Probabilistic model; Tags,Information systems; Graph structures; Music discovery; Music information retrieval; Probabilistic models; Tags; Information retrieval
Exploring question selection bias to identify experts and potential experts in community question answering,2012,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863742647&doi=10.1145%2f2180868.2180872&partnerID=40&md5=4b0b1f4b156dcdf3de08559147fbe4ed,"Community Question Answering (CQA) services enable their users to exchange knowledge in the form of questions and answers. These communities thrive as a result of a small number of highly active users, typically called experts, who provide a large number of high-quality useful answers. Expert identification techniques enable community managers to take measures to retain the experts in the community. There is further value in identifying the experts during the first few weeks of their participation as it would allow measures to nurture and retain them. In this article we address two problems: (a) How to identify current experts in CQA+ and (b) How to identify users who have potential of becoming experts in future (potential experts)? In particular, we propose a probabilistic model that captures the selection preferences of users based on the questions they choose for answering. The probabilistic model allows us to run machine learning methods for identifying experts and potential experts. Our results over several popular CQA datasets indicate that experts differ considerably from ordinary users in their selection preferences; enabling us to predict experts with higher accuracy over several baseline models. We show that selection preferences can be combined with baseline measures to improve the predictive performance even further. © 2012 ACM.",Community question answering; Expert identification; Question selection process,Learning systems; Baseline models; Data sets; High quality; Identification techniques; Machine learning methods; Predictive performance; Probabilistic models; Question Answering; Question selection; Knowledge management
Authorship attribution based on specific vocabulary,2012,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863735149&doi=10.1145%2f2180868.2180874&partnerID=40&md5=27d8053e74580a0ae90edab86ec75070,"In this article we propose a technique for computing a standardized Z score capable of defining the specific vocabulary found in a text (or part thereof) compared to that of an entire corpus. Assuming that the term occurrence follows a binomial distribution, this method is then applied to weight terms (words and punctuation symbols in the current study), representing the lexical specificity of the underlying text. In a final stage, to define an author profile we suggest averaging these text representations and then applying them along with a distance measure to derive a simple and efficient authorship attribution scheme. To evaluate this algorithm and demonstrate its effectiveness, we develop two experiments, the first based on 5,408 newspaper articles (Glasgow Herald) written in English by 20 distinct authors and the second on 4,326 newspaper articles (La Stampa) written in Italian by 20 distinct authors. These experiments demonstrate that the suggested classification scheme tends to perform better than the Delta rule method based on the most frequent words, better than the chi-square distance based on word profiles and punctuation marks, better than the KLD scheme based on a predefined set of words, and better than the naïve Bayes approach. © 2012 ACM.",Authorship attribution; Lexical statistics; Text classifiaction,Experiments; Authorship attribution; Bayes approach; Binomial distribution; Classification scheme; Distance measure; Distance-based; Punctuation marks; Term occurrences; Text classifiaction; Text representation; Word profiles; Z-scores; Newsprint
Peer-to-peer information retrieval: An overview,2012,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863750805&doi=10.1145%2f2180868.2180871&partnerID=40&md5=3711967bf03004e90441ecfb83ac360a,"Peer-to-peer technology is widely used for file sharing. In the past decade a number of prototype peer-to-peer information retrieval systems have been developed. Unfortunately, none of these has seen widespread realworld adoption and thus, in contrast with file sharing, information retrieval is still dominated by centralized solutions. In this article we provide an overview of the key challenges for peer-to-peer information retrieval and the work done so far. We want to stimulate and inspire further research to overcome these challenges. This will open the door to the development and large-scale deployment of real-world peer-to-peer information retrieval systems that rival existing centralized client-server solutions in terms of scalability, performance, user satisfaction, and freedom. © 2012 ACM.",Algorithms; Design; Performance; Reliability,Algorithms; Design; Information retrieval; Information retrieval systems; Reliability; Client server; File Sharing; Large-scale deployment; Peer-to-peer information retrieval; Peer-to-peer technologies; Performance; User satisfaction; Distributed computer systems
High-performance processing of text queries with tunable pruned term and term pair indexes,2012,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859018496&doi=10.1145%2f2094072.2094077&partnerID=40&md5=2952567505dfd6c9905c941e55c282bf,"Term proximity scoring is an established means in information retrieval for improving result quality of full-text queries. Integrating such proximity scores into efficient query processing, however, has not been equally well studied. Existing methods make use of precomputed lists of documents where tuples of terms, usually pairs, occur together, usually incurring a huge index size compared to term-only indexes. This article introduces a joint framework for trading off index size and result quality, and provides optimization techniques for tuning precomputed indexes towards either maximal result quality or maximal query processing performance under controlled result quality, given an upper bound for the index size. The framework allows to selectively materialize lists for pairs based on a query log to further reduce index size. Extensive experiments with two large text collections demonstrate runtime improvements of more than one order of magnitude over existing text-based processing techniques with reasonable index sizes. © 2012 ACM.",Index tuning; Performance; Proximity score; Text retrieval,Query processing; High-performance processing; Index tuning; Optimization techniques; Performance; Processing performance; Processing technique; Proximity score; Query logs; Runtimes; Text collection; Text query; Text retrieval; Upper Bound; Information retrieval
Multiple testing in statistical analysis of systems-based information retrieval experiments,2012,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859011471&doi=10.1145%2f2094072.2094076&partnerID=40&md5=1eba63b57a0cf525a485a005720b461c,"High-quality reusable test collections and formal statistical hypothesis testing together support a rigorous experimental environment for information retrieval research. But as Armstrong et al. [2009b] recently argued, global analysis of experiments suggests that there has actually been little real improvement in ad hoc retrieval effectiveness over time. We investigate this phenomenon in the context of simultaneous testing of many hypotheses using a fixed set of data. We argue that the most common approaches to significance testing ignore a great deal of information about the world. Taking into account even a fairly small amount of this information can lead to very different conclusions about systems than those that have appeared in published literature.We demonstrate how to model a set of IR experiments for analysis both mathematically and practically, and show that doing so can cause p-values from statistical hypothesis tests to increase by orders of magnitude. This has major consequences on the interpretation of experimental results using reusable test collections: it is very difficult to conclude that anything is significant once we have modeled many of the sources of randomness in experimental design and analysis. © 2012 ACM.",Effectiveness evaluation; Experimental design; Information retrieval; Statistical analysis; Test collections,Design of experiments; Experiments; Statistical methods; Statistical tests; Statistics; Ad Hoc retrieval; Effectiveness evaluation; Experimental design and analysis; Experimental environment; Global analysis; High quality; Information retrieval research; Multiple testing; Orders of magnitude; P-values; Significance testing; Statistical hypothesis test; Statistical hypothesis testing; Test Collection; Information retrieval
Word-based self-indexes for natural language text,2012,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859060626&doi=10.1145%2f2094072.2094073&partnerID=40&md5=2dd6fbc864f6e1a8cc07b6634df14b73,"The inverted index supports efficient full-text searches on natural language text collections. It requires some extra space over the compressed text that can be traded for search speed. It is usually fast for single-word searches, yet phrase searches require more expensive intersections. In this article we introduce a different kind of index. It replaces the text using essentially the same space required by the compressed text alone (compression ratio around 35%). Within this space it supports not only decompression of arbitrary passages, but efficient word and phrase searches. Searches are orders of magnitude faster than those over inverted indexes when looking for phrases, and still faster on single-word searches when little space is available. Our new indexes are particularly fast at counting the occurrences of words or phrases. This is useful for computing relevance of words or phrases. We adapt self-indexes that succeeded in indexing arbitrary strings within compressed space to deal with large alphabets. Natural language texts are then regarded as sequences of words, not characters, to achieve word-based self-indexes. We design an architecture that separates the searchable sequence from its presentation aspects. This permits applying case folding, stemming, removing stopwords, etc. as is usual on inverted indexes. © 2012 ACM.",Compressed data structures; Inverted indexes; Self-indexes,Compression ratio (machinery); Data structures; Full-text search; Inverted indexes; Inverted indices; Large alphabets; Natural language text; Orders of magnitude; Search speed; Self-indexes; Natural language processing systems
"Summarizing figures, tables, and algorithms in scientific publications to augment search results",2012,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859012828&doi=10.1145%2f2094072.2094075&partnerID=40&md5=cdadba1bc02d23b6e326742f12c383dd,"Increasingly, special-purpose search engines are being built to enable the retrieval of document-elements like tables, figures, and algorithms [Bhatia et al. 2010; Liu et al. 2007; Hearst et al. 2007]. These search engines present a thumbnail view of document-elements, some document metadata such as the title of the papers and their authors, and the caption of the document-element. While some authors in some disciplines write carefully tailored captions, generally, the author of a document assumes that the caption will be read in the context of the text in the document. When the caption is presented out of context as in a document-elementsearch- engine result, it may not contain enough information to help the end-user understand what the content of the document-element is. Consequently, end-users examining document-element search results would want a short ""synopsis"" of this information presented along with the document-element. Having access to the synopsis allows the end-user to quickly understand the content of the document-element without having to download and read the entire document as examining the synopsis takes a shorter time than finding information about a document element by downloading, opening and reading the file. Furthermore, it may allow the end-user to examine more results than they would otherwise. In this paper, we present the first set of methods to extract this useful information (synopsis) related to document-elements automatically. We use Naïve Bayes and support vector machine classifiers to identify relevant sentences from the document text based on the similarity and the proximity of the sentences with the caption and the sentences in the document text that refer to the document-element. We compare the two classification methods and study the effects of different features used. We also investigate the problem of choosing the optimum synopsissize that strikes a balance between the information content and the size of the generated synopses. A user study is also performed to measure how the synopses generated by our proposed method compare with other state-of-the-art approaches. © 2012 ACM.",Classification; Document-element; Summarization; Synopses,Algorithms; Classification (of information); Metadata; Search engines; Document-element; End users; Information contents; Scientific publications; Search results; State-of-the-art approach; Summarization; Synopses; Two classification; User study; Information retrieval systems
Static index pruning in web search engines: Combining term and document popularities with query views,2012,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859010976&doi=10.1145%2f2094072.2094074&partnerID=40&md5=195ada60f8804a290eaa5be92c9cd306,"Static index pruning techniques permanently remove a presumably redundant part of an inverted file, to reduce the file size and query processing time. These techniques differ in deciding which parts of an index can be removed safely; that is, without changing the top-ranked query results. As defined in the literature, the query view of a document is the set of query terms that access to this particular document, that is, retrieves this document among its top results. In this paper, we first propose using query views to improve the quality of the top results compared against the original results. We incorporate query views in a number of static pruning strategies, namely term-centric, document-centric, term popularity based and document access popularity based approaches, and show that the new strategies considerably outperform their counterparts especially for the higher levels of pruning and for both disjunctive and conjunctive query processing. Additionally,we combine the notions of term and document access popularity to form new pruning strategies, and further extend these strategies with the query views. The new strategies improve the result quality especially for the conjunctive query processing, which is the default and most common search mode of a search engine. © 2012 ACM.",Query view; Static inverted index pruning,Query languages; Query processing; Search engines; Conjunctive queries; Document access; File sizes; Inverted files; Pruning strategy; Pruning techniques; Query results; Query terms; Query view; Static inverted index pruning; Information retrieval systems
Large-scale validation and analysis of interleaved search evaluation,2012,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859070397&doi=10.1145%2f2094072.2094078&partnerID=40&md5=9f2e2f761cf82daeb55b24900fa08a5f,"Interleaving is an increasingly popular technique for evaluating information retrieval systems based on implicit user feedback. While a number of isolated studies have analyzed how this technique agrees with conventional offline evaluation approaches and other online techniques, a complete picture of its efficiency and effectiveness is still lacking. In this paper we extend and combine the body of empirical evidence regarding interleaving, and provide a comprehensive analysis of interleaving using data from two major commercial search engines and a retrieval system for scientific literature. In particular, we analyze the agreement of interleaving with manual relevance judgments and observational implicit feedback measures, estimate the statistical efficiency of interleaving, and explore the relative performance of different interleaving variants. We also show how to learn improved credit-assignment functions for clicks that further increase the sensitivity of interleaving. © 2012 ACM.",Clicks; Interleaving; Judgments; Online evaluation; Search engine; Sensitivity,Information retrieval systems; Search engines; Clicks; Interleaving; Judgments; On-line evaluation; Sensitivity; Information retrieval
GRAS: An effective and efficient stemming algorithm for information retrieval,2011,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84555196913&doi=10.1145%2f2037661.2037664&partnerID=40&md5=0fc3d4a79ba16c537efead8030e17e5a,"A novel graph-based language-independent stemming algorithm suitable for information retrieval is proposed in this article. The main features of the algorithm are retrieval effectiveness, generality, and computational efficiency. We test our approach on seven languages (using collections from the TREC, CLEF, and FIRE evaluation platforms) of varying morphological complexity. Significant performance improvement over plain word-based retrieval, three other language-independent morphological normalizers, as well as rule-based stemmers is demonstrated. © 2011 ACM.",Corpus; Stemming; Suffix,Algorithms; Information retrieval; Natural language processing systems; Corpus; Evaluation platforms; Graph-based; Morphological complexity; Performance improvements; Retrieval effectiveness; Rule based; Stemming; Stemming algorithms; Suffix; Computational efficiency
Upper-bound approximations for dynamic pruning,2011,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84555202447&doi=10.1145%2f2037661.2037662&partnerID=40&md5=735d10cab71812354862cd6760ab1c86,"Dynamic pruning strategies for information retrieval systems can increase querying efficiency without decreasing effectiveness by using upper bounds to safely omit scoring documents that are unlikely to make the final retrieved set. Often, such upper bounds are pre-calculated at indexing time for a given weighting model. However, this precludes changing, adapting or training the weighting model without recalculating the upper bounds. Instead, upper bounds should be approximated at querying time from various statistics of each term to allow on-the-fly adaptation of the applied retrieval strategy. This article, by using uniform notation, formulates the problem of determining a term upper-bound given a weighting model and discusses the limitations of existing approximations. Moreover, we propose an upper-bound approximation using a constrained nonlinear maximization problem. We prove that our proposed upper-bound approximation does not impact the retrieval effectiveness of several modern weighting models from various different families. We also show the applicability of the approximation for the Markov Random Field proximity model. Finally, we empirically examine how the accuracy of the upper-bound approximation impacts the number of postings scored and the resulting efficiency in the context of several large Web test collections. © 2011 ACM.",Dynamic pruning; Upper bounds,Information retrieval systems; Search engines; Markov Random Fields; Maximization problem; On-the-fly; Pruning strategy; Retrieval effectiveness; Retrieval strategies; Test Collection; Upper Bound; Upper bounds; Weighting model; Information retrieval
"Query modeling for entity search based on terms, categories, and examples",2011,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84555190544&doi=10.1145%2f2037661.2037667&partnerID=40&md5=46be470dd3820c0acc735c147e92e4f5,"Users often search for entities instead of documents, and in this setting, are willing to provide extra input, in addition to a series of query terms, such as category information and example entities. We propose a general probabilistic framework for entity search to evaluate and provide insights in the many ways of using these types of input for query modeling. We focus on the use of category information and show the advantage of a category-based representation over a term-based representation, and also demonstrate the effectiveness of category-based expansion using example entities. Our best performing model shows very competitive performance on the INEX-XER entity ranking and list completion tasks. © 2011 ACM.",Entity retrieval; Generative probabilistic model; Query expansion; Query modeling,Entity ranking; Entity retrieval; Entity search; Generative probabilistic model; List completion; Probabilistic framework; Query expansion; Query terms; Information systems
Ranking function adaptation with boosting trees,2011,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84555190553&doi=10.1145%2f2017661.2037663&partnerID=40&md5=a510a6d517fd6419b376c68de28956ad,"Machine-learned ranking functions have shown successes in Web search engines. With the increasing demands on developing effective ranking functions for different search domains, we have seen a big bottleneck, that is, the problem of insufficient labeled training data, which has significantly slowed the development and deployment of machine-learned ranking functions for different domains. There are two possible approaches to address this problem: (1) combining labeled training data from similar domains with the small targetdomain labeled data for training or (2) using pairwise preference data extracted from user clickthrough log for the target domain for training. In this article, we propose a new approach called tree-based ranking function adaptation (Trada) to effectively utilize these data sources for training cross-domain ranking functions. Tree adaptation assumes that ranking functions are trained with the Stochastic Gradient Boosting Trees method-a gradient boosting method on regression trees. It takes such a ranking function from one domain and tunes its tree-based structure with a small amount of training data from the target domain. The unique features include (1) automatic identification of the part of the model that needs adjustment for the new domain and (2) appropriate weighing of training examples considering both local and global distributions. Based on a novel pairwise loss function that we developed for pairwise learning, the basic tree adaptation algorithm is also extended (Pairwise Trada) to utilize the pairwise preference data from the target domain to further improve the effectiveness of adaptation. Experiments are performed on real datasets to show that tree adaptation can provide better-quality ranking functions for a new domain than other methods. © 2011 ACM.",Boosting regression trees; Domain adaptation; Learning to rank; User feedback; Web search ranking,Automation; Forestry; Information Retrieval; Internet; Mathematics; Regression Analysis; Trees; Automation; Forestry; Information retrieval; Search engines; World Wide Web; Domain adaptation; Learning to rank; Regression trees; User feedback; Web searches; Trees (mathematics)
Correlation-based retrieval for heavily changed near-duplicate videos,2011,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84455168546&doi=10.1145%2f2037661.2037666&partnerID=40&md5=b546f49f3d2c76a26232b091cbf05095,"The unprecedented and ever-growing number of Web videos nowadays leads to the massive existence of near-duplicate videos. Very often, some near-duplicate videos exhibit great content changes, while the user perceives little information change, for example, color features change significantly when transforming a color video with a blue filter. These feature changes contribute to low-level video similarity computations, making conventional similarity-based near-duplicate video retrieval techniques incapable of accurately capturing the implicit relationship between two near-duplicate videos with fairly large content modifications. In this paper, we introduce a new dimension for near-duplicate video retrieval. Different from existing nearduplicate video retrieval approaches which are based on video-content similarity, we explore the correlation between two videos. The intuition is that near-duplicate videos should preserve strong information correlation in spite of intensive content changes. More effective retrieval with stronger tolerance is achieved by replacing video-content similarity measures with information correlation analysis. Theoretical justification and experimental results prove the effectiveness of correlation-based near-duplicate retrieval. © 2011 ACM.",Correlation-based retrieval; Near-duplicate video; Similarity-based retrieval,Image retrieval; Color features; Color video; Content modification; Correlation-based retrieval; Implicit relationships; Information correlation; Near-duplicate video; New dimensions; Similarity measure; Similarity-based retrieval; Video retrieval; Video retrieval techniques; Video similarity; Web video; Correlation methods
Recommendation systems with complex constraints: A course recommendation perspective,2011,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-81155153211&doi=10.1145%2f2037661.2037665&partnerID=40&md5=387bc9e8d4aeb43b28fbac571629cdd1,"We study the problem of making recommendations when the objects to be recommended must also satisfy constraints or requirements. In particular, we focus on course recommendations: the courses taken by a student must satisfy requirements (e.g., take two out of a set of five math courses) in order for the student to graduate. Our work is done in the context of the CourseRank system, used by students to plan their academic program at Stanford University. Our goal is to recommend to these students courses that not only help satisfy constraints, but that are also desirable (e.g., popular or taken by similar students). We develop increasingly expressive models for course requirements, and present a variety of schemes for both checking if the requirements are satisfied, and for making recommendations that take into account the requirements. We show that some types of requirements are inherently expensive to check, and we present exact, as well as heuristic techniques, for those cases. Although our work is specific to course requirements, it provides insights into the design of recommendation systems in the presence of complex constraints found in other applications. © 2011 ACM.",Complex constraints; Package recommendations; Recommender systems,College buildings; Curricula; Heuristic methods; Model checking; Recommender systems; Academic program; Complex constraints; Course requirements; Heuristic techniques; Other applications; Package recommendations; Stanford University; Students
Exploring the Music Similarity Space on the Web,2011,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024283247&doi=10.1145%2f1993036.1993038&partnerID=40&md5=fc421b9aade2562ca9e1b923af9fdc50,"This article comprehensively addresses the problem of similarity measurement between music artists via text-based features extracted from Web pages. To this end, we present a thorough evaluation of different term-weighting strategies, normalizationmethods, aggregation functions, and similarity measurement techniques. In large-scale genre classification experiments carried out on real-world artist collections, we analyze several thousand combinations of settings/parameters that influence the similarity calculation process, and investigate in which way they impact the quality of the similarity estimates. Accurate similarity measures for music are vital for many applications, such as automated playlist generation, music recommender systems, music information systems, or intelligent user interfaces to access music collections by means beyond text-based browsing. Therefore, by exhaustively analyzing the potential of text-based features derived from artist-related Web pages, this article constitutes an important contribution to context-based music information research. © 2011, ACM. All rights reserved.",Algorithms; evaluation; Experimentation; Measurement; Music information retrieval; term space; Web contentmining,
Fast construction of the HYB index,2011,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80051526081&doi=10.1145%2f1993036.1993040&partnerID=40&md5=bfde36bf2ed806148c4ecc930d28c6d3,"As shown in a series of recent works, the HYB index is an alternative to the inverted index (INV) that enables very fast prefix searches, which in turn is the basis for fast processing of many other types of advanced queries, including autocompletion, faceted search, error-tolerant search, database-style select and join, and semantic search. In this work we show that HYB can be constructed at least as fast as INV, and often up to twice as fast. This is because HYB, by its nature, requires only a half-inversion of the data and allows an efficient in-place instead of the traditional merge-based index construction. We also pay particular attention to the cache efficiency of the in-memory posting accumulation, an issue that has not been addressed in previous work, and show that our simple multilevel posting accumulation scheme yields much fewer cache misses compared to related approaches. Finally, we show that HYB supports fast dynamic index updates more easily than INV. © 2011 ACM.",Autocompletion; HYB index; Index construction; Indexing; Inverted index,Query languages; Semantics; Autocompletion; Cache efficiency; Cache Miss; Error tolerant; Faceted search; Fast dynamics; HYB index; Index construction; Index update; Inverted indices; Semantic search; Query processing
Content redundancy in YouTube and its application to video tagging,2011,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80051541279&doi=10.1145%2f1993036.1993037&partnerID=40&md5=131a84752a3daee5cb47a8083e525f3d,"The emergence of large-scale social Web communities has enabled users to share online vast amounts of multimedia content. An analysis of YouTube reveals a high amount of redundancy, in the form of videos with overlapping or duplicated content. We use robust content-based video analysis techniques to detect overlapping sequences between videos. Based on the output of these techniques, we present an in-depth study of duplication and content overlap in YouTube, and analyze various dependencies between content overlap and meta data such as video titles, views, video ratings, and tags. As an application, we show that content-based links provide useful information for generating new tag assignments. We propose different tag propagation methods for automatically obtaining richer video annotations. Experiments on video clustering and classification as well as a user evaluation demonstrate the viability of our approach. © 2011 ACM.",Automatic tagging; Content-based links; Data organization; Neighbor-based tagging; Tag propagation; Video duplicates,Redundancy; Automatic tagging; Content-based; Data organization; Neighbor-based tagging; Tag propagation; Video duplicates; User interfaces
Exploring the music similarity space on the Web,2011,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80051515057&doi=10.1145%2f1993036&partnerID=40&md5=daf08d0da3fb3688a57e675874357086,"This article comprehensively addresses the problem of similarity measurement between music artists via text-based features extracted from Web pages. To this end, we present a thorough evaluation of different term-weighting strategies, normalization methods, aggregation functions, and similarity measurement techniques. In large-scale genre classification experiments carried out on real-world artist collections, we analyze several thousand combinations of settings/parameters that influence the similarity calculation process, and investigate in which way they impact the quality of the similarity estimates. Accurate similarity measures for music are vital for many applications, such as automated playlist generation, music recommender systems, music information systems, or intelligent user interfaces to access music collections by means beyond text-based browsing. Therefore, by exhaustively analyzing the potential of text-based features derived from artist-related Web pages, this article constitutes an important contribution to context-based music information research. © 2011 ACM.",Evaluation; Music information retrieval; Term space; Web content mining,Information retrieval; World Wide Web; Aggregation functions; Context-based; Evaluation; Genre classification; Intelligent User Interfaces; Music collection; Music information; Music information retrieval; Music recommender systems; Music similarity; Normalization methods; Similarity calculation; Similarity estimate; Similarity measure; Similarity measurements; Term space; Text-based features; Web content mining; User interfaces
Toward a semantic granularity model for domain-specific information retrieval,2011,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80051488571&doi=10.1145%2f1993036.1993039&partnerID=40&md5=40933f66fe820d8908e432cbb90d296f,"Both similarity-based and popularity-based document ranking functions have been successfully applied to information retrieval (IR) in general. However, the dimension of semantic granularity also should be considered for effective retrieval. In this article, we propose a semantic granularity-based IR model that takes into account the three dimensions, namely similarity, popularity, and semantic granularity, to improve domain-specific search. In particular, a concept-based computational model is developed to estimate the semantic granularity of documents with reference to a domain ontology. Semantic granularity refers to the levels of semantic detail carried by an information item. The results of our benchmark experiments confirm that the proposed semantic granularity based IR model performs significantly better than the similaritybased baseline in both a bio-medical and an agricultural domain. In addition, a series of user-oriented studies reveal that the proposed document ranking functions resemble the implicit ranking functions exercised by humans. The perceived relevance of the documents delivered by the granularity-based IR system is significantly higher than that produced by a popular search engine for a number of domain-specific search tasks. To the best of our knowledge, this is the first study regarding the application of semantic granularity to enhance domain-specific IR. © 2011 ACM.",Document ranking; Domain ontology; Domain-specific search; Granular computing; Information retrieval,Granular computing; Ontology; Search engines; Semantics; Benchmark experiments; Bio-medical; Computational model; Concept-based; Document ranking; Domain ontologies; Domain specific; Domain-specific information; Implicit ranking; Information items; IR models; Search tasks; Three dimensions; User oriented; Information retrieval
"Identifying, indexing, and ranking chemical formulae and chemical names in digital documents",2011,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80051508034&doi=10.1145%2f1961209.1961215&partnerID=40&md5=aa51ee19f8f332e0ccc74acbe5c693b7,"End-users utilize chemical search engines to search for chemical formulae and chemical names. Chemical search engines identify and index chemical formulae and chemical names appearing in text documents to support efficient search and retrieval in the future. Identifying chemical formulae and chemical names in text automatically has been a hard problem that has met with varying degrees of success in the past. We propose algorithms for chemical formula and chemical name tagging using Conditional Random Fields (CRFs) and Support Vector Machines (SVMs) that achieve higher accuracy than existing (published) methods. After chemical entities have been identified in text documents, they must be indexed. In order to support userprovided search queries that require a partial match between the chemical name segment used as a keyword or a partial chemical formula, all possible (or a significant number of) subformulae of formulae that appear in any document and all possible subterms (e.g., ""methyl"") of chemical names (e.g., ""methylethyl ketone"") must be indexed. Indexing all possible subformulae and subterms results in an exponential increase in the storage and memory requirements as well as the time taken to process the indices. We propose techniques to prune the indices significantly without reducing the quality of the returned results significantly. Finally, we propose multiple query semantics to allow users to pose different types of partial search queries for chemical entities. We demonstrate empirically that our search engines improve the relevance of the returned results for search queries involving chemical entities. © 2011 ACM.",Chemical formula; Chemical name; Conditional Random Fields; Entity extraction; Hierarchical text segmentation; Independent frequent subsequence; Index pruning; Query models; Ranking; Similarity search; Support Vector Machines,Content based retrieval; Indexing (of information); Information retrieval systems; Ketones; Search engines; Semantics; Support vector machines; Chemical formulae; Conditional random field; Entity extraction; Independent frequent subsequence; Index pruning; Query model; Ranking; Similarity search; Support vector; Text segmentation; Information retrieval
Improving recommender systems by incorporating social contextual information,2011,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80051490270&doi=10.1145%2f1961209.1961212&partnerID=40&md5=6447a796aadce52cd2e0c27596647cda,"Due to their potential commercial value and the associated great research challenges, recommender systems have been extensively studied by both academia and industry recently. However, the data sparsity problem of the involved user-item matrix seriously affects the recommendation quality. Many existing approaches to recommender systems cannot easily deal with users who have made very few ratings. In view of the exponential growth of information generated by online users, social contextual information analysis is becoming important for many Web applications. In this article, we propose a factor analysis approach based on probabilistic matrix factorization to alleviate the data sparsity and poor prediction accuracy problems by incorporating social contextual information, such as social networks and social tags. The complexity analysis indicates that our approach can be applied to very large datasets since it scales linearly with the number of observations. Moreover, the experimental results show that our method performs much better than the state-of-the-art approaches, especially in the circumstance that users have made few ratings. © 2011 ACM.",Collaborative filtering; Matrix factorization; Recommender systems; Social network; Tags,Computer aided network analysis; Factorization; Recommender systems; User interfaces; Collaborative filtering; Complexity analysis; Contextual information; Data sparsity; Data sparsity problems; Exponential growth; Factor analysis; Large datasets; Matrix factorizations; Online users; Prediction accuracy; Research challenges; Social network; Social Networks; State-of-the-art approach; Tags; User-item matrix; WEB application; Matrix algebra
Diagnostic evaluation of information retrieval models,2011,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80051496446&doi=10.1145%2f1961209.1961210&partnerID=40&md5=53d67fe803e3431f7377251eea46631c,"Developing effective retrieval models is a long-standing central challenge in information retrieval research. In order to develop more effective models, it is necessary to understand the deficiencies of the current retrieval models and the relative strengths of each of them. In this article, we propose a general methodology to analytically and experimentally diagnose the weaknesses of a retrieval function, which provides guidance on how to further improve its performance. Our methodology is motivated by the empirical observation that good retrieval performance is closely related to the use of various retrieval heuristics. We connect the weaknesses and strengths of a retrieval function with its implementations of these retrieval heuristics, and propose two strategies to check how well a retrieval function implements the desired retrieval heuristics. The first strategy is to formalize heuristics as constraints, and use constraint analysis to analytically check the implementation of retrieval heuristics. The second strategy is to define a set of relevance-preserving perturbations and perform diagnostic tests to empirically evaluate how well a retrieval function implements retrieval heuristics. Experiments show that both strategies are effective to identify the potential problems in implementations of the retrieval heuristics. The performance of retrieval functions can be improved after we fix these problems. © 2011 ACM.",Constraints; Diagnostic evaluation; Formal models; Retrieval heuristics; TF-IDF weighting,Information retrieval; Constraints; Diagnostic evaluation; Formal model; Retrieval heuristics; TF-IDF weighting; Function evaluation
Effects of usage-based feedback on video retrieval: A simulation-based study,2011,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80051523613&doi=10.1145%2f1961209.1961214&partnerID=40&md5=1f9db484917aa458c4a1d859d20bb06c,"We present a model for exploiting community-based usage information for video retrieval, where implicit usage information from past users is exploited in order to provide enhanced assistance in video retrieval tasks, and alleviate the effects of the semantic gap problem. We propose a graph-based model for all types of implicit and explicit feedback, in which the relevant usage information is represented. Our model is designed to capture the complex interactions of a user with an interactive video retrieval system, including the representation of sequences of user-system interaction during a search session. Building upon this model, four recommendation strategies are defined and evaluated. An evaluation strategy is proposed based on simulated user actions, which enables the evaluation of our recommendation strategies over a usage information pool obtained from 24 users performing four different TRECVid tasks. Furthermore, the proposed simulation approach is used to simulate usage information pools with different characteristics, with which the recommendation approaches are further evaluated on a larger set of tasks, and their performance is studied with respect to the scalability and quality of the available implicit information. © 2011 ACM.",Collaborative filtering; Evaluation model; Human-computer interaction; Implicit feedback,Computer simulation; Human computer interaction; Knowledge management; Lakes; Semantics; User interfaces; Collaborative filtering; Complex interaction; Evaluation models; Evaluation strategies; Explicit feedback; Graph-based models; Human-computer; Implicit feedback; Interactive video retrieval system; Search sessions; Semantic gap; Simulation approach; Simulation-based; TRECVID; User action; User-system interaction; Video retrieval; Image retrieval
Concept-based information retrieval using explicit semantic analysis,2011,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80051504927&doi=10.1145%2f1961209.1961211&partnerID=40&md5=b33b33076ee0edac8e0b74746a07d8e4,"Information retrieval systems traditionally rely on textual keywords to index and retrieve documents. Keyword-based retrieval may return inaccurate and incomplete results when different keywords are used to describe the same concept in the documents and in the queries. Furthermore, the relationship between these related keywords may be semantic rather than syntactic, and capturing it thus requires access to comprehensive human world knowledge. Concept-based retrieval methods have attempted to tackle these difficulties by using manually built thesauri, by relying on term cooccurrence data, or by extracting latent word relationships and concepts from a corpus. In this article we introduce a new concept-based retrieval approach based on Explicit Semantic Analysis (ESA), a recently proposed method that augments keywordbased text representation with concept-based features, automatically extracted from massive human knowledge repositories such as Wikipedia. Our approach generates new text features automatically, and we have found that high-quality feature selection becomes crucial in this setting to make the retrieval more focused. However, due to the lack of labeled data, traditional feature selection methods cannot be used, hence we propose new methods that use self-generated labeled training data. The resulting system is evaluated on several TREC datasets, showing superior performance over previous state-of-the-art results. © 2011 ACM.",Concept-based retrieval; Explicit semantic analysis; Feature selection; Semantic search,Feature extraction; Information retrieval systems; Knowledge representation; Search engines; Semantics; Concept-based; Concept-based retrieval; Data sets; Explicit semantics; Feature selection methods; High quality; Human knowledge; Keyword-based retrieval; Labeled data; Labeled training data; Semantic search; Term co-occurrence; Text feature; Text representation; Wikipedia; World knowledge; Information retrieval
Contextual video recommendation by multimodal relevance and user feedback,2011,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80051538000&doi=10.1145%2f1961209.1961213&partnerID=40&md5=ef6f6a59b5cffedb39882beca5fa171a,"With Internet delivery of video content surging to an unprecedented level, video recommendation, which suggests relevant videos to targeted users according to their historical and current viewings or preferences, has become one of most pervasive online video services. This article presents a novel contextual video recommendation system, called VideoReach, based on multimodal content relevance and user feedback. We consider an online video usually consists of different modalities (i.e., visual and audio track, as well as associated texts such as query, keywords, and surrounding text). Therefore, the recommended videos should be relevant to current viewing in terms of multimodal relevance. We also consider that different parts of videos are with different degrees of interest to a user, as well as different features and modalities have different contributions to the overall relevance. As a result, the recommended videos should also be relevant to current users in terms of user feedback (i.e., user click-through). We then design a unified framework for VideoReach which can seamlessly integrate both multimodal relevance and user feedback by relevance feedback and attention fusion. VideoReach represents one of the first attempts toward contextual recommendation driven by video content and user click-through, without assuming a sufficient collection of user profiles available. We conducted experiments over a large-scale real-world video data and reported the effectiveness of VideoReach. © 2011 ACM.",Image retrieval; Relevance feedback; Video recommendation,Image retrieval; Video recording; Audio track; Internet delivery; Multi-modal; Online video; Relevance feedback; Unified framework; User feedback; User profile; Video contents; Video data; Video recommendation; Feedback
Ranking Function Adaptation with Boosting Trees,2011,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025390489&doi=10.1145%2f2037661.2037663&partnerID=40&md5=2eab797924549f2d9085d0dd6c9b2dc0,"Machine-learned ranking functions have shown successes in Web search engines. With the increasing demands on developing effective ranking functions for different search domains, we have seen a big bottleneck, that is, the problem of insufficient labeled training data, which has significantly slowed the development and deployment of machine-learned ranking functions for different domains. There are two possible approaches to address this problem: (1) combining labeled training data from similar domains with the small targetdomain labeled data for training or (2) using pairwise preference data extracted from user clickthrough log for the target domain for training. In this article, we propose a new approach called tree-based ranking function adaptation (Trada) to effectively utilize these data sources for training cross-domain ranking functions. Tree adaptation assumes that ranking functions are trained with the Stochastic Gradient Boosting Trees method-a gradient boosting method on regression trees. It takes such a ranking function from one domain and tunes its tree-based structure with a small amount of training data from the target domain. The unique features include (1) automatic identification of the part of the model that needs adjustment for the new domain and (2) appropriate weighing of training examples considering both local and global distributions. Based on a novel pairwise loss function that we developed for pairwise learning, the basic tree adaptation algorithm is also extended (Pairwise Trada) to utilize the pairwise preference data from the target domain to further improve the effectiveness of adaptation. Experiments are performed on real datasets to show that tree adaptation can provide better-quality ranking functions for a new domain than other methods. © 2011, ACM. All rights reserved.",Algorithms; boosting regression trees; Design; domain adaptation; learning to rank; Performance; user feedback; Web search ranking,
Utilizing inter-passage and inter-document similarities for reranking search results,2010,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80051500892&doi=10.1145%2f1877766.1877769&partnerID=40&md5=3cabe9060cb23eb6581b7a3a38198098,"We present a novel language-model-based approach to reranking search results; that is, reordering the documents in an initially retrieved list so as to improve precision at top ranks. Our model integrates whole-document information with that induced from passages. Specifically, interpassage, inter-document, and query-based similarities, which constitute a rich source of information, are combined in our model. Empirical evaluation shows that the precision-at-top-ranks performance of our model is substantially better than that of the initial ranking upon which reranking is performed. Furthermore, the performance is substantially better than that of a commonly used passage-based document ranking method that does not exploit inter-item similarities. Our model also generalizes and outperforms a recently proposed reranking method that utilizes interdocument similarities, but which does not exploit passage-based information. Finally, the model's performance is superior to that of a state-of-the-art pseudo-feedback-based retrieval approach. © 2010 ACM.",Ad hoc retrieval; Document centrality; Inter-document similarities; Interpassage similarities; Passage centrality; Passage-based retrieval; Reranking,Search engines; Ad Hoc retrieval; Document centrality; Inter-document similarities; Interpassage similarities; Passage centrality; Passage-based retrieval; Re-ranking; State feedback
"Extraction, characterization and utility of prototypical communication groups in the blogosphere",2010,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80051521905&doi=10.1145%2f1877766.1877772&partnerID=40&md5=1bf682615243324fa63bfc32e2a34e20,"This article analyzes communication within a set of individuals to extract the representative prototypical groups and provides a novel framework to establish the utility of such groups. Corporations may want to identify representative groups (which are indicative of the overall communication set) because it is easier to track the prototypical groups rather than the entire set. This can be useful for advertising, identifying ""hot"" spots of resource consumption as well as in mining representative moods or temperature of a community. Our framework has three parts: extraction, characterization, and utility of prototypical groups. First, we extract groups by developing features representing communication dynamics of the individuals. Second, to characterize the overall communication set, we identify a subset of groups within the community as the prototypical groups. Third, we justify the utility of these prototypical groups by using them as predictors of related external phenomena; specifically, stock market movement of technology companies and political polls of Presidential candidates in the 2008 U.S. elections. We have conducted extensive experiments on two popular blogs, Engadget and Huffington Post. We observe that the prototypical groups can predict stock market movement/political polls satisfactorily with mean error rate of 20.32%. Further, our method outperforms baseline methods based on alternative group extraction and prototypical group identification methods. We evaluate the quality of the extracted groups based on their conductance and coverage measures and develop metrics: predictivity and resilience to evaluate their ability to predict a related external time-series variable (stock market movement/political polls). This implies that communication dynamics of individuals are essential in extracting groups in a community, and the prototypical groups extracted by our method are meaningful in characterizing the overall communication sets. © 2010 ACM.",Blogosphere; Communication dynamics; Engadget; Huffington Post; Political polls; Prototypical groups; Social communication; Social network analysis; Stock market movement,Commerce; Electric network analysis; Finance; Quality control; Social sciences computing; Surveys; Blogospheres; Engadget; Huffington Post; Political polls; Prototypical groups; Social communication; Social Network Analysis; Stock market; Communication
Engineering basic algorithms of an in-memory text search engine,2010,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80051485769&doi=10.1145%2f1877766.1877768&partnerID=40&md5=4d23052dccac2247135c5001e86b9c90,"Inverted index data structures are the key to fast text search engines. We first investigate one of the predominant operation on inverted indexes, which asks for intersecting two sorted lists of document IDs of different lengths. We explore compression and performance of different inverted list data structures. In particular, we present Lookup, a new data structure that allows intersection in expected time linear in the smaller list. Based on this result, we present the algorithmic core of a full text data base that allows fast Boolean queries, phrase queries, and document reporting using less space than the input text. The system uses a carefully choreographed combination of classical data compression techniques and inverted-index-based search data structures. Our experiments show that inverted indexes are preferable over purely suffix-array-based techniques for in-memory (English) text search engines. A similar system is now running in practice in each core of the distributed data base engine TREX of SAP. © 2010 ACM.",In-memory search engine; Inverted index; Randomization,Algorithms; Data compression; Data structures; Information retrieval; Boolean queries; Data compression techniques; Distributed database; Expected time; In-memory search engine; Inverted indices; Inverted list; Lookups; Randomization; Running-in; System use; Text data; Search engines
Improving graph-walk-based similarity with reranking: Case studies for personal information management,2010,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80051534534&doi=10.1145%2f1877766.1877770&partnerID=40&md5=afb5253d6de08a1cd86fa0cf359ec743,"Relational or semistructured data is naturally represented by a graph, where nodes denote entities and directed typed edges represent the relations between them. Such graphs are heterogeneous, describing different types of objects and links. We represent personal information as a graph that includes messages, terms, persons, dates, and other object types, and relations like sent-to and has-term. Given the graph, we apply finite random graph walks to induce a measure of entity similarity, which can be viewed as a tool for performing search in the graph. Experiments conducted using personal email collections derived from the Enron corpus and other corpora show how the different tasks of alias finding, threading, and person name disambiguation can be all addressed as search queries in this framework, where the graph-walk-based similarity metric is preferable to alternative approaches, and further improvements are achieved with learning. While researchers have suggested to tune edge weight parameters to optimize the graph walk performance per task, we apply reranking to improve the graph walk results, using features that describe high-level information such as the paths traversed in the walk. High performance, together with practical runtimes, suggest that the described framework is a useful search system in the PIM domain, as well as in other semistructured domains. © 2010 ACM.",Graph walk; Learning; PIM; Semistructured data,Information management; Software agents; Alternative approach; Edge weights; Enron corpus; Graph walk; High-level information; Learning; Name disambiguation; Personal information; Personal information management; PIM; Random graphs; Re-ranking; Runtimes; Search queries; Search system; Semi structured data; Semi-structured; Similarity metrics; Graph theory
Dependable filtering: Philosophy and realizations,2010,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80051500234&doi=10.1145%2f1877766.1877771&partnerID=40&md5=4a6569902b2be98fd2d704b120c0d7e3,"Digital content production and distribution has radically changed our business models. An unprecedented volume of supply is now on offer, whetted by the demand of millions of users from all over the world. Since users cannot be expected to browse through millions of different items to find what they might like, filtering has become a popular technique to connect supply and demand: trusted users are first identified, and their opinions are then used to create recommendations. In this domain, users' trustworthiness has been measured according to one of the following two criteria: taste similarity (i.e., ""I trust those who agree with me""), or social ties (i.e., ""I trust my friends, and the people that my friends trust""). The former criterion aims at identifying concordant users, but is subject to abuse by malicious behaviors. The latter aims at detecting well-intentioned users, but fails to capture the natural subjectivity of tastes. In this article, we propose a new definition of trusted recommenders, addressing those users that are both well-intentioned and concordant. Based on this characterisation, we propose a novel approach to information filtering that we call dependable filtering. We describe alternative algorithms realizing this approach, and demonstrate, by means of extensive performance evaluation on a variety of real large-scale datasets, the high degree of both accuracy and robustness they entail. © 2010 ACM.",Collaborative filtering; Link analysis; Profile injection; Social networks; Sybil attack,Economics; Large dataset; Social networking (online); Alternative algorithms; Business models; Digital contents; Large-scale datasets; Link analysis; Malicious behavior; Supply and demand; Sybil attack; Collaborative filtering
Efficient set intersection for inverted indexing,2010,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80051486140&doi=10.1145%2f1877766.1877767&partnerID=40&md5=e6f9724cc9385cb77833eaed94a60df1,"Conjunctive Boolean queries are a key component of modern information retrieval systems, especially when Webscale repositories are being searched. A conjunctive query q is equivalent to a |q|-way intersection over ordered sets of integers, where each set represents the documents containing one of the terms, and each integer in each set is an ordinal document identifier. As is the case with many computing applications, there is tension between the way in which the data is represented, and the ways in which it is to be manipulated. In particular, the sets representing index data for typical document collections are highly compressible, but are processed using random access techniques, meaning that methods for carrying out set intersections must be alert to issues to do with access patterns and data representation. Our purpose in this article is to explore these trade-offs, by investigating intersection techniques that make use of both uncompressed ""integer"" representations, as well as compressed arrangements. We also propose a simple hybrid method that provides both compact storage, and also faster intersection computations for conjunctive querying than is possible even with uncompressed representations. © 2010 ACM.",Bitvector; Byte-code; Compact data structures; Information retrieval; Set intersection; Set representation,Data structures; Information retrieval systems; Search engines; Bitvector; Byte-code; Compact data structures; Set intersection; Set representation; Information retrieval
An information-theoretic framework for semantic-multimedia retrieval,2010,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80051514035&doi=10.1145%2f1852102.1852105&partnerID=40&md5=1bacc3350fd0c028e2ce627ac04dbbe8,"This article is set in the context of searching text and image repositories by keyword. We develop a unified probabilistic framework for text, image, and combined text and image retrieval that is based on the detection of keywords (concepts) using automated image annotation technology. Our framework is deeply rooted in information theory and lends itself to use with other media types. We estimate a statistical model in a multimodal feature space for each possible query keyword. The key element of our framework is to identify feature space transformations that make them comparable in complexity and density. We select the optimal multimodal feature space with a minimum description length criterion from a set of candidate feature spaces that are computed with the average-mutual-information criterion for the text part and hierarchical expectation maximization for the visual part of the data. We evaluate our approach in three retrieval experiments (only text retrieval, only image retrieval, and text combined with image retrieval), verify the framework's low computational complexity, and compare with existing state-of-the-art ad-hoc models. © 2010 ACM.",Automated keyword annotation; Indexing; Multimedia; Search,Computational complexity; Feature extraction; Information theory; Semantics; Automated keyword annotation; Expectation Maximization; Feature space; Image annotation; Image repository; Key elements; Media types; Minimum description length criteria; Multimedia; Multimodal features; Probabilistic framework; Search; Statistical models; Text retrieval; Image retrieval
Clustering-based incremental web crawling,2010,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80051485415&doi=10.1145%2f1852102.1852103&partnerID=40&md5=963ac2315592fce9eeefbecb5a72b120,"When crawling resources, for example, number of machines, crawl-time, and so on, are limited, so a crawler has to decide an optimal order in which to crawl and recrawl Web pages. Ideally, crawlers should request only those Web pages that have changed since the last crawl; in practice, a crawler may not know whether a Web page has changed before downloading it. In this article, we identify features of Web pages that are correlated to their change frequency. We design a crawling algorithm that clusters Web pages based on features that correlate to their change frequencies obtained by examining past history. The crawler downloads a sample of Web pages from each cluster, and depending upon whether a significant number of these Web pages have changed in the last crawl cycle, it decides whether to recrawl the entire cluster. To evaluate the performance of our incremental crawler, we develop an evaluation framework that measures which crawling policy results in the best search results for the end-user. We run experiments on a real Web data set of about 300,000 distinct URLs distributed among 210 Web sites. The results demonstrate that the clustering-based sampling algorithm effectively clusters the pages with similar change patterns, and our clustering-based crawling algorithm outperforms existing algorithms in that it can improve the quality of the user experience for those who query the search engine. © 2010 ACM.",Clustering; Refresh policy; Sampling; Search engine; Web crawler,Information retrieval; Search engines; User interfaces; Websites; Change frequencies; Change patterns; Clustering; End users; Evaluation framework; Refresh policy; Sampling algorithm; Search results; User experience; Web crawlers; Web Crawling; Web data; Web page; Clustering algorithms
Mining near-duplicate graph for cluster-based reranking of web video search results,2010,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80051522946&doi=10.1145%2f1852102.1852108&partnerID=40&md5=2b0c219a6d5d327f68caac7aaf358650,"Recently, video search reranking has been an effective mechanism to improve the initial textbased ranking list by incorporating visual consistency among the result videos. While existing methods attempt to rerank all the individual result videos, they suffer from several drawbacks. In this article, we propose a new video reranking paradigm called cluster-based video reranking (CVR). The idea is to first construct a video near-duplicate graph representing the visual similarity relationship among videos, followed by identifying the near-duplicate clusters from the video near-duplicate graph, then ranking the obtained near-duplicate clusters based on cluster properties and intercluster links, and finally for each ranked cluster, a representative video is selected and returned. Compared to existing methods, the new CVR ranks clusters and exhibits several advantages, including superior reranking by utilizing more reliable cluster properties, fast reranking on a small number of clusters, diverse and representative results. Particularly, we formulate the near-duplicate cluster identification as a novel maximally cohesive subgraph mining problem. By leveraging the designed cluster scoring properties indicating the cluster's importance and quality, random walk is applied over the near-duplicate cluster graph to rank clusters. An extensive evaluation study proves the novelty and superiority of our proposals over existing methods. © 2010 ACM.",Cluster-based video reranking; Graph mining near-duplicate; Web search,Multimedia systems; User interfaces; Cluster property; Cluster-based; Effective mechanisms; Evaluation study; Graph mining; Number of clusters; Random Walk; Re-ranking; Subgraph mining; Video search; Visual consistency; Visual similarity; Web search; Web video; World Wide Web
A similarity measure for indefinite rankings,2010,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80051482719&doi=10.1145%2f1852102.1852106&partnerID=40&md5=af28a3892085cb5e49cea3a7363f209b,"Ranked lists are encountered in research and daily life and it is often of interest to compare these lists even when they are incomplete or have only some members in common. An example is document rankings returned for the same query by different search engines. A measure of the similarity between incomplete rankings should handle nonconjointness, weight high ranks more heavily than low, and be monotonic with increasing depth of evaluation; but no measure satisfying all these criteria currently exists. In this article, we propose a new measure having these qualities, namely rank-biased overlap (RBO). The RBO measure is based on a simple probabilistic user model. It provides monotonicity by calculating, at a given depth of evaluation, a base score that is non-decreasing with additional evaluation, and a maximum score that is nonincreasing. An extrapolated score can be calculated between these bounds if a point estimate is required. RBO has a parameter which determines the strength of the weighting to top ranks. We extend RBO to handle tied ranks and rankings of different lengths. Finally, we give examples of the use of the measure in comparing the results produced by public search engines and in assessing retrieval systems in the laboratory. © 2010 ACM.",Probabilistic models; Rank correlation; Ranking,Search engines; Daily lives; Document ranking; Incomplete rankings; Maximum score; Monotonicity; Point estimate; Probabilistic models; Rank correlation; Ranking; Retrieval systems; Similarity measure; User models; Information retrieval
PageRank without hyperlinks: Structural reranking using links induced by language models,2010,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80051496466&doi=10.1145%2f1852102.1852104&partnerID=40&md5=5f1edce8164561fe2c774fdb2564ec6f,"The ad hoc retrieval task is to find documents in a corpus that are relevant to a query. Inspired by the PageRank and HITS (hubs and authorities) algorithms for Web search, we propose a structural reranking approach to ad-hoc retrieval that applies to settings with no hyperlink information. We reorder the documents in an initially retrieved set by exploiting implicit asymmetric relationships among them. We consider generation links, which indicate that the language model induced from one document assigns high probability to the text of another. We study a number of reranking criteria based on measures of centrality in the graphs formed by generation links, and show that integrating centrality into standard language-model-based retrieval is quite effective at improving precision at top ranks; the best resultant performance is comparable, and often superior, to that of a state-of-the-art pseudo-feedback-based retrieval approach. In addition, we demonstrate the merits of our language-model-based method for inducing interdocument links by comparing it to previously suggested notions of interdocument similarities (e.g., cosines within the vector-space model). We also show that our methods for inducing centrality are substantially more effective than approaches based on document-specific characteristics, several of which are novel to this study. © 2010 ACM.",Authorities; Graph-based retrieval; High-accuracy retrieval; HITS; Hubs; Language modeling; PageRank; Social networks; Structural reranking,Computational linguistics; Hypertext systems; Natural language processing systems; State feedback; User interfaces; Vector spaces; World Wide Web; Authorities; Graph-based; High-accuracy retrieval; HITS; Hubs; Language modeling; PageRank; Re-ranking; Social Networks; Information retrieval
The task-dependent effect of tags and ratings on social media access,2010,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78649287223&doi=10.1145%2f1852102.1852107&partnerID=40&md5=5e5dd9bbdf2172f88911f34448b5114a,"Recently, online social networks have emerged that allow people to share their multimedia files, retrieve interesting content, and discover like-minded people. These systems often provide the possibility to annotate the content with tags and ratings. Using a random walk through the social annotation graph, we have combined these annotations into a retrieval model that effectively balances the personal preferences and opinions of like-minded users into a single relevance ranking for either content, tags, or people. We use this model to identify the influence of different annotation methods and system design aspects on common ranking tasks in social content systems. Our results show that a combination of rating and tagging information can improve tasks like search and recommendation. The optimal influence of both sources on the ranking is highly dependent on the retrieval task and system design. Results on content search and tag suggestion indicate that the profile created by a user's annotations can be used effectively to adapt the ranking to personal preferences. The random walk reduces sparsity problems by smoothly integrating indirectly related concepts in the relevance ranking, which is especially valuable for cold-start users or individual tagging systems like YouTube and Flickr. © 2010 ACM.",Content retrieval; Librarything; Movielens; Personalization; Random walk; Rating; Recommendation; Social media; Tagging,Electronic document exchange; Random processes; Systems analysis; User interfaces; Content retrieval; Librarything; Movielens; Personalizations; Random Walk; Recommendation; Social media; Tagging; Social networking (online)
Using topic themes for multi-document summarization,2010,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80051481326&doi=10.1145%2f1777432.1777436&partnerID=40&md5=d76bb4e62722e54cda13d35fbf5e3119,"The problem of using topic representations for multidocument summarization (MDS) has received considerable attention recently. Several topic representations have been employed for producing informative and coherent summaries. In this article, we describe five previously known topic representations and introduce two novel representations of topics based on topic themes. We present eight different methods of generating multidocument summaries and evaluate each of these methods on a large set of topics used in past DUC workshops. Our evaluation results show a significant improvement in the quality of summaries based on topic themes over MDS methods that use other alternative topic representations. © 2010 ACM.",Summarization; Topic representations; Topic themes,Evaluation results; Multi-document; Multi-document summarization; Summarization; Topic representations; Topic themes; Natural language processing systems
Probabilistic models for answer-ranking in multilingual question-answering,2010,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78649767995&doi=10.1145%2f1777432.1777439&partnerID=40&md5=75a9d089750c79c5dbf366ea27b26436,"This article presents two probabilistic models for answering ranking in the multilingual questionanswering (QA) task, which finds exact answers to a natural language question written in different languages. Although some probabilistic methods have been utilized in traditional monolingual answer-ranking, limited prior research has been conducted for answer-ranking in multilingual question-answering with formal methods. This article first describes a probabilistic model that predicts the probabilities of correctness for individual answers in an independent way. It then proposes a novel probabilistic method to jointly predict the correctness of answers by considering both the correctness of individual answers as well as their correlations. As far as we know, this is the first probabilistic framework that proposes to model the correctness and correlation of answer candidates in multilingual question-answering and provide a novel approach to design a flexible and extensible system architecture for answer selection in multilingual QA. An extensive set of experiments were conducted to show the effectiveness of the proposed probabilistic methods in English-to-Chinese and English-to-Japanese cross-lingual QA, as well as English, Chinese, and Japanese monolingual QA using TREC and NTCIR questions. © 2010 ACM.",Answer selection; Answer-merging; Answer-ranking; Probabilistic graphical model; Question-answering,Answer selection; Answer-merging; Answer-ranking; Probabilistic graphical models; Question Answering; Formal methods
STEvent: Spatio-temporal event model for social network discovery,2010,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79958068110&doi=10.1145%2f1777432.1777438&partnerID=40&md5=7e599c23c2b7ceeb3d1948caf1dad329,"Spatio-temporal data concerning the movement of individuals over space and time contains latent information on the associations among these individuals. Sources of spatio-temporal data include usage logs of mobile and Internet technologies. This article defines a spatio-temporal event by the co-occurrences among individuals that indicate potential associations among them. Each spatio-temporal event is assigned a weight based on the precision and uniqueness of the event. By aggregating the weights of events relating two individuals, we can determine the strength of association between them. We conduct extensive experimentation to investigate both the efficacy of the proposed model as well as the computational complexity of the proposed algorithms. Experimental results on three real-life spatio-temporal datasets cross-validate each other, lending greater confidence on the reliability of our proposed model. © 2010 ACM.",Data mining; Social network; Spatio-temporal databases,Algorithms; Computational complexity; Data mining; Data sets; Internet technology; Latent information; Social Networks; Space and time; Spatio-temporal; Spatio-temporal data; Spatio-temporal databases; Spatio-temporal events; Social sciences computing
Combining relations for information extraction from free text,2010,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80051527434&doi=10.1145%2f1777432.1777437&partnerID=40&md5=f3abfea48848949c131b2dbe5decb508,"Relations between entities of the same semantic type tend to be sparse in free texts. Therefore, combining relations is the key to effective information extraction (IE) on free text datasets with a small set of training samples. Previous approaches to bootstrapping for IE used different types of relations, such as dependency or co-occurrence, and faced the problems of paraphrasing and misalignment of instances. To cope with these problems, we propose a framework that integrates several types of relations. After extracting candidate entities, our framework evaluates relations between them at the phrasal, dependency, semantic frame, and discourse levels. For each of these levels, we build a classifier that outputs a score for relation instances. In order to integrate these scores, we propose three strategies: (1) integrate evaluation scores from each relation classifier; (2) incorporate the elimination of negatively labeled instances in a previous strategy; and (3) add cascading of extracted relations into strategy (2). Our framework improves the state-of-art results for supervised systems by 8%, 15%, 3%, and 5% on MUC4 (terrorism); MUC6 (management succession); ACE RDC 2003 (news, general types); and ACE RDC 2003 (news, specific types) domains respectively. © 2010 ACM.",Bootstrapping; Dependency relations; Discourse relations; Information extraction; Semantic relations,Semantics; Bootstrapping; Dependency relation; Discourse relations; Information Extraction; Semantic relations; Information analysis
Learning with click graph for query intent classification,2010,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80051482040&doi=10.1145%2f1777432.1777435&partnerID=40&md5=fa00ab301f70258f06209d3b0e5f6a13,"Topical query classification, as one step toward understanding users' search intent, is gaining increasing attention in information retrieval. Previous works on this subject primarily focused on enrichment of query features, for example, by augmenting queries with search engine results. In this work, we investigate a completely orthogonal approach-instead of improving feature representation, we aim at drastically increasing the amount of training data. To this end, we propose two semisupervised learning methods that exploit user click-through data. In one approach, we infer class memberships of unlabeled queries from those of labeled ones according to their proximities in a click graph; and then use these automatically labeled queries to train classifiers using query terms as features. In a second approach, click graph learning and query classifier training are conducted jointly with an integrated objective. Our methods are evaluated in two applications, product intent and job intent classification. In both cases, we expand the training data by over two orders of magnitude, leading to significant improvements in classification performance. An additional finding is that with a large amount of training data obtained in this fashion, a classifier based on simple query term features can outperform those using state-of-the-art, augmented features. © 2010 ACM.",Click graph; Query classification; Semisupervised learning; User intent,Search engines; Supervised learning; Classification performance; Classifier training; Click graph; Clickthrough data; Feature representation; One step; Orders of magnitude; Query classification; Query terms; Search engine results; Semi-supervised learning methods; Semisupervised learning; Training data; User intent; Information retrieval
Dynamic lightweight text compression,2010,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77955133862&doi=10.1145%2f1777432.1777433&partnerID=40&md5=a07cf26ca9fcb79ce7d1881382e4b38f,"We address the problem of adaptive compression of natural language text, considering the case where the receiver is much less powerful than the sender, as in mobile applications. Our techniques achieve compression ratios around 32% and require very little effort from the receiver. Furthermore, the receiver is not only lighter, but it can also search the compressed text with less work than that necessary to decompress it. This is a novelty in two senses: it breaks the usual compressor/decompressor symmetry typical of adaptive schemes, and it contradicts the longstanding assumption that only semistatic codes could be searched more efficiently than the uncompressed text. Our novel compression methods are preferable in several aspects over the existing adaptive and semistatic compressors for natural language texts. © 2010 ACM.",Adaptive natural language text compression; Compressed pattern matching; Real-time transmission; Searching compressed texts; Text compression,Compressed pattern matching; Natural language text; Real-time transmission; Searching compressed text; Text compressions; Pattern matching
Arnoldi versus GMRES for computing pagerank: A theoretical contribution to Google's PageRank problem,2010,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80051536979&doi=10.1145%2f1777432.1777434&partnerID=40&md5=3404b7dde1bfa0fee0e24c273668a347,"PageRank is one of the most important ranking techniques used in today's search engines. A recent very interesting research track focuses on exploiting efficient numerical methods to speed up the computation of PageRank, among which the Arnoldi-type algorithm and the GMRES algorithm are competitive candidates. In essence, the former deals with the PageRank problem from an eigenproblem, while the latter from a linear system, point of view. However, there is little known about the relations between the two approaches for PageRank. In this article, we focus on a theoretical and numerical comparison of the two approaches. Numerical experiments illustrate the effectiveness of our theoretical results. © 2010 ACM.",Arnoldi; GMRES; Google; Krylov subspace; PageRank; Web ranking,Algorithms; Linear systems; Numerical methods; Search engines; User interfaces; Arnoldi; GMRES; Google; Krylov subspace; PageRank; Web ranking; Computational efficiency
Exploiting neighborhood knowledge for single document summarization and keyphrase extraction,2010,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79551537499&doi=10.1145%2f1740592.1740596&partnerID=40&md5=1b19af3af6206caf52a6bce4a99b554e,"Document summarization and keyphrase extraction are two related tasks in the IR and NLP fields, and both of them aim at extracting condensed representations from a single text document. Existing methods for single document summarization and keyphrase extraction usually make use of only the information contained in the specified document. This article proposes using a small number of nearest neighbor documents to improve document summarization and keyphrase extraction for the specified document, under the assumption that the neighbor documents could provide additional knowledge and more clues. The specified document is expanded to a small document set by adding a few neighbor documents close to the document, and the graph-based ranking algorithm is then applied on the expanded document set to make use of both the local information in the specified document and the global information in the neighbor documents. Experimental results on the Document Understanding Conference (DUC) benchmark datasets demonstrate the effectiveness and robustness of our proposed approaches. The cross-document sentence relationships in the expanded document set are validated to be beneficial to single document summarization, and the word cooccurrence relationships in the neighbor documents are validated to be very helpful to single document keyphrase extraction. © 2010 ACM.",Document summarization; Graph-based ranking; Keyphrase extraction; Neighborhood knowledge,Information use; Additional knowledge; Benchmark datasets; Condensed representations; Document summarization; Global informations; Graph-based; Keyphrase extraction; Local information; Nearest neighbors; Neighborhood knowledge; Ranking algorithm; Single document summarization; Text document; Word co-occurrence; Text processing
Effects of position and number of relevant documents retrieved on users' evaluations of system performance,2010,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80051529994&doi=10.1145%2f1740592.1740597&partnerID=40&md5=f09f8dd55c2efeca41b64e9cc813425d,"Information retrieval research has demonstrated that system performance does not always correlate positively with user performance, and that users often assign positive evaluation scores to search systems even when they are unable to complete tasks successfully. This research investigated the relationship between objective measures of system performance and users' perceptions of that performance. In this study, subjects evaluated the performance of four search systems whose search results were manipulated systematically to produce different orderings and numbers of relevant documents. Three laboratory studies were conducted with a total of eighty-one subjects. The first two studies investigated the effect of the order of five relevant and five nonrelevant documents in a search results list containing ten results on subjects' evaluations. The third study investigated the effect of varying the number of relevant documents in a search results list containing ten results on subjects' evaluations. Results demonstrate linear relationships between subjects' evaluations and the position of relevant documents in a search results list and the total number of relevant documents retrieved. Of the two, number of relevant documents retrieved was a stronger predictor of subjects' evaluation ratings and resulted in subjects using a greater range of evaluation scores. © 2010 ACM.",Precision; Presentation of search results; Ranking; Satisfaction; Search performance; User evaluation of performance,Information retrieval; Investments; Safety devices; Precision; Ranking; Satisfaction; Search performance; Search results; User evaluations; Search engines
Exploiting query logs for cross-lingual query suggestion,2010,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78651327029&doi=10.1145%2f1740592.1740594&partnerID=40&md5=b811440c36e51278ec8256cdfb6f72c0,"Query suggestion aims to suggest relevant queries for a given query, which helps users better specify their information needs. Previous work on query suggestion has been limited to the same language. In this article, we extend it to cross-lingual query suggestion (CLQS): for a query in one language, we suggest similar or relevant queries in other languages. This is very important to the scenarios of cross-language information retrieval (CLIR) and other related cross-lingual applications. Instead of relying on existing query translation technologies for CLQS, we present an effective meanstomap the input query of one language to queries of the other language inthe query log. Important monolingual and cross-lingual information such as word translation relations and word co-occurrence statistics, and so on, are used to estimate the cross-lingual query similarity with a discriminative model. Benchmarks show that the resulting CLQS system significantly outperforms a baseline system that uses dictionary-based query translation. Besides, we evaluate CLQS with French-English and Chinese-English CLIR tasks on TREC-6 and NTCIR-4 collections, respectively. The CLIR experiments using typical retrieval models demonstrate that the CLQS-based approach has significantly higher effectiveness than several traditional query translation methods. We find that when combined with pseudo-relevance feedback, the effectiveness of CLIR using CLQS is enhanced for different pairs of languages. © 2010 ACM.",Cross-language information retrieval; Que log; Query expansion; Query suggestion; Query translation,Computational linguistics; Feedback; Information retrieval; Cross language information retrieval; Que log; Query expansion; Query suggestion; Query translations; Translation (languages)
Tuning the capacity of search engines: Load-driven routing and incremental caching to reduce and balance the load,2010,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77950595883&doi=10.1145%2f1740592.1740593&partnerID=40&md5=08e945e0381051c862807cdb741170b9,"This article introduces an architecture for a document-partitioned search engine, based on a novel approach combining collection selection and load balancing, called load-driven routing. By exploiting the query-vector document model, and the incremental caching technique, our architecture can compute very high quality results for any query, with only a fraction of the computational load used in a typical document-partitioned architecture. By trading off a small fraction of the results, our technique allows us to strongly reduce the computing pressure to a search engine back-end; we are able to retrieve more than 2/3 of the top-5 results for a given query with only 10% the computing load needed by a configuration where the query is processed by each index partition. Alternatively, we can slightly increase the load up to 25% to improve precision and get more than 80% of the top-5 results. In fact, the flexibility of our system allows a wide range of different configurations, so as to easily respond to different needs in result quality or restrictions in computing power. More important, the system configuration can be adjusted dynamically in order to fit unexpected query peaks or unpredictable failures. This article wraps up some recent works by the authors, showing the results obtained by tests conducted on 6 million documents, 2,800,000 queries and real query cost timing as measured on an actual index. © 2010 ACM.",Collection selection; Distributed IR; Incremental caching; Web search engines,Architecture; Balancing; Information retrieval; Search engines; User interfaces; Caching technique; Computational loads; Computing load; Computing power; Distributed IR; Document model; High quality; Incremental caching; Query costs; System configurations; World Wide Web
Efficient k-nearest neighbor searching in nonordered discrete data spaces,2010,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77949914323&doi=10.1145%2f1740592.1740595&partnerID=40&md5=5561025f91a11687fa2345fecdb19eb0,"Numerous techniques have been proposed in the past for supporting efficient k-nearest neighbor (k-NN) queries in continuous data spaces. Limited work has been reported in the literature for k-NN queries in a nonordered discrete data space (NDDS). Performing k-NN queries in an NDDS raises new challenges. The Hamming distance is usually used to measure the distance between two vectors (objects) in an NDDS. Due to the coarse granularity of the Hamming distance, a k-NN query in an NDDS may lead to a high degree of nondeterminism for the query result. We propose a new distance measure, called Granularity-Enhanced Hamming (GEH) distance, which effectively reduces the number of candidate solutions for a query. We have also implemented k-NN queries using multidimensional database indexing in NDDSs. Further, we use the properties of our multidimensional NDDS index to derive the probability of encountering valid neighbors within specific regions of the index. This probability is used to develop a new search ordering heuristic. Our experiments on synthetic and genomic data sets demonstrate that our index-based k-NN algorithm is efficient in finding k-NNs in both uniform and nonuniform data sets in NDDSs and that our heuristics are effective in improving the performance of such queries. © 2010 ACM.",Database; Distance measurement; Nearest neighbor; Nonordered discrete data space; Similarity search; Spatial indexing,Indexing (of information); Query languages; Vector spaces; Database; Discrete data; Nearest neighbor; Similarity search; Spatial indexing; Hamming distance
Learning author-topic models from text corpora,2010,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80051615661&doi=10.1145%2f1658377.1658381&partnerID=40&md5=ab893a63916798cd4a44d96d35a5e642,"We propose an unsupervised learning technique for extracting information about authors and topics from large text collections. We model documents as if they were generated by a two-stage stochastic process. An author is represented by a probability distribution over topics, and each topic is represented as a probability distribution over words. The probability distribution over topics in a multi-author paper is a mixture of the distributions associated with the authors. The topic-word and author-topic distributions are learned from data in an unsupervised manner using a Markov chain Monte Carlo algorithm. We apply the methodology to three large text corpora: 150, 000 abstracts from the CiteSeer digital library, 1740 papers from the Neural Information Processing Systems (NIPS) Conferences, and 121, 000 emails from the Enron corporation. We discuss in detail the interpretation of the results discovered by the system including specific topic and author models, ranking of authors by topic and topics by author, parsing of abstracts by topics and authors, and detection of unusual papers by specific authors. Experiments based on perplexity scores for test documents and precision-recall for document retrieval are used to illustrate systematic differences between the proposed author-topic model and a number of alternatives. Extensions to the model, allowing for example, generalizations of the notion of an author, are also briefly discussed. © 2010 ACM 1046-8188/2010/01-ART1 $10.00.",Author models; Gibbs sampling; Perplexity; Topic models; Unsupervised learning,Abstracting; Data processing; Digital libraries; Markov processes; Models; Stochastic models; Unsupervised learning; Author models; CiteSeer; Document Retrieval; Extracting information; Gibbs sampling; Markov chain Monte carlo algorithms; Neural information processing systems; Perplexity; Stochastic process; Text collection; Text corpora; Topic model; Two stage; Unusual papers; Probability distributions
Probabilistic static pruning of inverted files,2010,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80051661816&doi=10.1145%2f1658377.1658378&partnerID=40&md5=05f7a700801c313f92e1ac60237b9112,"Information retrieval (IR) systems typically compress their indexes in order to increase their efficiency. Static pruning is a form of lossy data compression: it removes from the index, data that is estimated to be the least important to retrieval performance, according to some criterion. Generally, pruning criteria are derived from term weighting functions, which assign weights to terms according to their contribution to a document's contents. Usually, document-term occurrences that are assigned a low weight are ruled out from the index. The main assumption is that those entries contribute little to the document content. We present a novel pruning technique that is based on a probabilistic model of IR. We employ the Probability Ranking Principle as a decision criterion over which posting list entries are to be pruned. The proposed approach requires the estimation of three probabilities, combining them in such a way that we gather all the necessary information to apply the aforementioned criterion. We evaluate our proposed pruning technique on five TREC collections and various retrieval tasks, and show that in almost every situation it outperforms the state of the art in index pruning. The main contribution of this work is proposing a pruning technique that stems directly from the same source as probabilistic retrieval models, and hence is independent of the final model used for retrieval. © 2010 ACM 1046-8188/2010/01-ART1 $10.00.",Compression; Efficiency; Inverted files; Probabilistic models; Pruning,Data compression; Information retrieval; Information retrieval systems; Decision criterions; Inverted files; Lossy data compression; Low weight; Probabilistic models; Probabilistic retrieval; Pruning; Pruning techniques; Retrieval performance; State of the art; Term weighting; TREC collection; Search engines
Statistical lattice-based spoken document retrieval,2010,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79851497439&doi=10.1145%2f1658377.1658379&partnerID=40&md5=c6ed1833694d7bdf0c786914c64467bb,"Recent research efforts on spoken document retrieval have tried to overcome the low quality of 1-best automatic speech recognition transcripts, especially in the case of conversational speech, by using statistics derived from speech lattices containing multiple transcription hypotheses as output by a speech recognizer. We present a method for lattice-based spoken document retrieval based on a statistical n-gram modeling approach to information retrieval. In this statistical lattice-based retrieval (SLBR) method, a smoothed statistical model is estimated for each document from the expected counts of words given the information in a lattice, and the relevance of each document to a query is measured as a probability under such a model.We investigate the efficacy of our method under various parameter settings of the speech recognition and lattice processing engines, using the Fisher English Corpus of conversational telephone speech. Experimental results show that our method consistently achieves better retrieval performance than using only the 1-best transcripts in statistical retrieval, outperforms a recently proposed lattice-based vector space retrieval method, and also compares favorably with a lattice-based retrieval method based on the Okapi BM25 model. © 2010 ACM 1046-8188/2010/01-ART1 $10.00.",Lattice-based spoken document retrieval; Probabilistic retrieval approach; Retrieval of conversational speech,Search engines; Speech recognition; Statistics; Vector spaces; Automatic speech recognition; Conversational telephone speech; Low qualities; N-gram modeling; Parameter setting; Probabilistic retrieval approach; Processing engine; Research efforts; Retrieval methods; Retrieval of conversational speech; Retrieval performance; Speech recognizer; Spoken document retrieval; Statistical models; Statistical retrieval; Vector space retrieval methods; Information retrieval
Semantic clustering of XML documents,2010,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78649781837&doi=10.1145%2f1658377.1658380&partnerID=40&md5=9f06f1d4b2db90e8a23525f39aee5c00,"Dealing with structure and content semantics underlying semistructured documents is challenging for any task of document management and knowledge discovery conceived for such data. In this work we address the novel problem of clustering semantically related XML documents according to their structure and content features. XML features are generated by enriching syntactic with semantic information based on a lexical knowledge base. The backbone of the proposed framework for the semantic clustering of XML documents is a data representation model that exploits the notion of tree tuple to identify semantically cohesive substructures in XML documents and represent them as transactional data. This framework is equipped with two clustering algorithms based on different paradigms, namely centroid-based partitional clustering and frequent-itemset-based hierarchical clustering. An extensive experimental evaluation was conducted on real data sets from various domains, showing the significance of our approach as a solution for the semantic clustering of XML documents. © 2010 ACM 1046-8188/2010/01-ART1 $10.00.",Similarity measures; XML document clustering; XML structure and content mining; XML transactional modeling; XML tree tuples,Clustering algorithms; Information management; Information services; Knowledge based systems; Plant extracts; Semantics; Trees (mathematics); Content semantics; Data representation models; Document management; Experimental evaluation; Hier-archical clustering; Lexical knowledge; Partitional clustering; Real data sets; Semantic clustering; Semantic information; Semi-structured documents; Similarity measure; Transactional data; XML tree tuples; XML
Building a framework for the probability ranking principle by a family of expected weighted rank,2009,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-75149184485&doi=10.1145%2f1629096.1629098&partnerID=40&md5=8066cfabda355beb5b6d5fb58e86a4ea,"A new principles framework is presented for retrieval evaluation of ranked outputs. It applies decision theory to model relevance decision preferences and shows that the Probability Ranking Principle (PRP) specifies optimal ranking. It has two new components, namely a probabilistic evaluation model and a general measure of retrieval effectiveness. Its probabilities may be interpreted as subjective or objective ones. Its performance measure is the expected weighted rank which is the weighted average rank of a retrieval list. Starting from this measure, the expected forward rank and some existing retrieval effectiveness measures (e.g., top n precision and discounted cumulative gain) are instantiated using suitable weighting schemes after making certain assumptions. The significance of these instantiations is that the ranking prescribed by PRP is shown to be optimal simultaneously for all these existing performance measures. In addition, the optimal expected weighted rank may be used to normalize the expected weighted rank of retrieval systems for (summary) performance comparison (across different topics) between systems. The framework also extends PRP and our evaluation model to handle graded relevance, thereby generalizing the discussed, existing measures (e.g., top n precision) and probabilistic retrieval models for graded relevance. © 2009 ACM.",Optimization; Probability ranking principle,Information retrieval; Probability; Evaluation models; Graded relevances; New components; Optimisations; Performance measure; Probability ranking principle; Probability rankings; Relevance decision; Retrieval effectiveness; Retrieval evaluation; Decision theory
Cyberchondria: Studies of the escalation of medical concerns in Web search,2009,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-75149180454&doi=10.1145%2f1629096.1629101&partnerID=40&md5=8a1ce90be3c7baf83c98b146f683d736,"The World Wide Web provides an abundant source of medical information. This information can assist people who are not healthcare professionals to better understand health and illness, and to provide them with feasible explanations for symptoms. However, the Web has the potential to increase the anxieties of people who have little or no medical training, especially when Web search is employed as a diagnostic procedure. We use the term cyberchondria to refer to the unfounded escalation of concerns about common symptomatology, based on the review of search results and literature on the Web. We performed a large-scale, longitudinal, log-based study of how people search for medical information online, supported by a survey of 515 individuals' health-related search experiences. We focused on the extent to which common, likely innocuous symptoms can escalate into the review of content on serious, rare conditions that are linked to the common symptoms. Our results show that Web search engines have the potential to escalate medical concerns. We show that escalation is associated with the amount and distribution of medical content viewed by users, the presence of escalatory terminology in pages visited, and a user's predisposition to escalate versus to seek more reasonable explanations for ailments. We also demonstrate the persistence of postsession anxiety following escalations and the effect that such anxieties can have on interrupting user's activities across multiple sessions. Our findings underscore the potential costs and challenges of cyberchondria and suggest actionable design implications that hold opportunity for improving the search and navigation experience for people turning to the Web to interpret common symptoms. © 2009 ACM.",Cyberchondria,Bioinformatics; Information retrieval; Search engines; Design implications; Diagnostic procedure; Health care professionals; Medical information; Medical training; Search results; Web search engines; Web searches; World Wide Web
"A distributed, service-based framework for knowledge applications with multimedia",2009,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-75149138756&doi=10.1145%2f1629096.1629100&partnerID=40&md5=98d24b8843a6dddb103710fc1727681d,"The current trend in distributed systems is towards service-based integration. This article describes an ontology-driven framework implemented to provide knowledge management for data of different modalities, with multimedia processing, annotation, and reasoning provided by remote services. The framework was developed in, and is presented in the context of, the Medical Imaging and Advanced Knowledge Technologies (MIAKT) project that sought to support the Multidisciplinary Meetings (MDMs) that take place during breast cancer screening for diagnosing the patient. However, the architecture is entirely independent of the specific application domain and can be quickly prototyped into new domains. An Enterprise server provides resource access to a client-side presentation application which, in turn, provides knowledge visualization and markup of any supported media, as defined by a domain-dependent ontology-supported language. © 2009 ACM.",Breast cancer; Descision support; Health; Ontologies; Semantic Web; Services,Knowledge management; Medical imaging; Multimedia services; Semantic Web; Semantics; Advanced knowledge technologies; Application domains; Breast cancer; Breast cancer screening; Current trends; Descision support; Distributed systems; Enterprise servers; Knowledge application; Knowledge Visualization; Multimedia processing; Ontologies semantic web; Remote services; Resource access; Service-based; Ontology
Page rank: Functional dependencies,2009,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-75149152869&doi=10.1145%2f1629096.1629097&partnerID=40&md5=be8f5166880dec0b9834a3a3b110ab31,Some of the significant issues and functional dependencies of PageRank are discussed. PageRank is a ranking technique used by existing search engines. One way to describe the idea behind PageRank is by considering a random surfer that starts from a random page and selecting the next page at every time by clicking on one of the links in the existing page. A significant part of the existing knowledge about PageRank is scattered through the research laboratories of large search engines and its analysis remained in areas of trade secrets and economic competition. The technique is formally defined formally as the stationary distribution of a stochastic process whose states are the nodes of the Web graph. The process is obtained by mixing the normalized adjacency matrix of the Web graph with a uniform process that is needed to make the mixture irreducible and aperiodic.,Damping factor; PageRank; Power method,Damping; Information retrieval; Interconnection networks; Random processes; Research laboratories; World Wide Web; Adjacency matrices; Damping factors; Functional dependency; Page ranks; PageRank; Power method; Ranking technique; Stationary distribution; Stochastic process; Trade secrets; Web graphs; Search engines
A few good topics: Experiments in topic set reduction for retrieval evaluation,2009,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-75149143907&doi=10.1145%2f1629096.1629099&partnerID=40&md5=2f8d20106db2a7ee357fc6f1f30b8841,"We consider the issue of evaluating information retrieval systems on the basis of a limited number of topics. In contrast to statistically-based work on sample sizes, we hypothesize that some topics or topic sets are better than others at predicting true system effectiveness, and that with the right choice of topics, accurate predictions can be obtained from small topics sets. Using a variety of effectiveness metrics and measures of goodness of prediction, a study of a set of TREC and NTCIR results confirms this hypothesis, and provides evidence that the value of a topic set for this purpose does generalize. © 2009 ACM.",Evaluation experiments; Search effectiveness; Test corpora; Topic selection,Information retrieval; Information retrieval systems; Information services; Accurate prediction; Effectiveness metrics; Evaluation experiments; Retrieval evaluation; Sample sizes; System effectiveness; Test corpus; Experiments
MUADDIB: A distributed recommender system supporting device adaptivity,2009,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-75149190102&doi=10.1145%2f1629096.1629102&partnerID=40&md5=d7980435184cea3ff1307d84f525e644,"Web recommender systems are Web applications capable of generating useful suggestions for visitors of Internet sites. However, in the case of large user communities and in presence of a high number of Web sites, these tasks are computationally onerous, even more if the client software runs on devices with limited resources. Moreover, the quality of the recommendations strictly depends on how the recommendation algorithm takes into account the currently used device. Some approaches proposed in the literature provide multidimensional recommendations considering, besides items and users, also the exploited device. However, these systems do not efficiently perform, since they assign to either the client or the server the arduous cost of computing recommendations. In this article, we argue that a fully distributed organization is a suitable solution to improve the efficiency of multidimensional recommender systems. In order to address these issues, we propose a novel distributed architecture, called MUADDIB, where each user's device is provided with a device assistant that autonomously retrieves information about the user's behavior. Moreover, a single profiler, associated with the user, periodically collects information coming from the different user's device assistants to construct a global user's profile. In order to generate recommendations, a recommender precomputes data provided by the profilers. This way, the site manager has only the task of suitably presenting the content of the site, while the computation of the recommendations is assigned to the other distributed components. Some experiments conducted on real data and using some well-known metrics show that the system works more effectively and efficiently than other device-based distributed recommenders. © 2009 ACM.",Adaptivity; Personalization; Recommender systems,World Wide Web; Adaptivity; Client software; Distributed architecture; Distributed components; Multidimensional recommendation; Multidimensional recommender systems; Personalizations; Recommendation algorithms; Recommender systems; Site manager; Suitable solutions; User communities; WEB application; Computational efficiency
"Clusters, language models, and ad hoc information retrieval",2009,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-75649116513&doi=10.1145%2f1508850.1508851&partnerID=40&md5=1fb3a46414fd73d6058e35e094459f8b,"The language-modeling approach to information retrieval provides an effective statistical framework for tackling various problems and often achieves impressive empirical performance. However, most previous work on language models for information retrieval focused on document-specific characteristics, and therefore did not take into account the structure of the surrounding corpus, a potentially rich source of additional information. We propose a novel algorithmic framework in which information provided by document-based language models is enhanced by the incorporation of information drawn from clusters of similar documents. Using this framework, we develop a suite of new algorithms. Even the simplest typically outperforms the standard language-modeling approach in terms of mean average precision (MAP) and recall, and our new interpolation algorithm posts statistically significant performance improvements for both metrics over all six corpora tested. An important aspect of our work is the way we model corpus structure. In contrast to most previous work on cluster-based retrieval that partitions the corpus, we demonstrate the effectiveness of a simple strategy based on a nearest-neighbors approach that produces overlapping clusters. © 2009 ACM.",Aspect models; Cluster hypothesis; Cluster-based language models; Clustering; Interpolation model; Language modeling; Smoothing,Algorithmic languages; Computational linguistics; Information services; Interpolation; Natural language processing systems; Software agents; Aspect model; Cluster hypothesis; Cluster-based; Clustering; Language model; Model languages; Information retrieval
A novel framework for efficient automated singer identification in large music databases,2009,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-75649095708&doi=10.1145%2f1508850.1508856&partnerID=40&md5=28475efdce95267e4b17f039d0e0cfda,"Over the past decade, there has been explosive growth in the availability of multimedia data, particularly image, video, and music. Because of this, content-based music retrieval has attracted attention from the multimedia database and information retrieval communities. Content-based music retrieval requires us to be able to automatically identify particular characteristics of music data. One such characteristic, useful in a range of applications, is the identification of the singer in a musical piece. Unfortunately, existing approaches to this problem suffer from either low accuracy or poor scalability. In this article, we propose a novel scheme, called Hybrid Singer Identifier (HSI), for efficient automated singer recognition. HSI uses multiple low-level features extracted from both vocal and nonvocal music segments to enhance the identification process; it achieves this via a hybrid architecture that builds profiles of individual singer characteristics based on statistical mixture models. An extensive experimental study on a large music database demonstrates the superiority of our method over state-of-the-art approaches in terms of effectiveness, efficiency, scalability, and robustness. © 2009 ACM.",Classification; EM algorithm; Evaluation; Gaussian mixture models; Music retrieval; Singer identification; Statistical modeling,Communication channels (information theory); Database systems; Information services; Mixtures; Scalability; Content-based music retrieval; EM algorithms; Experimental studies; Explosive growth; Gaussian Mixture Model; Hybrid architectures; Identification process; Low-level features; Mixture model; Multimedia data; Multimedia database; Music data; Music database; Music retrieval; Music segments; Musical pieces; State-of-the-art approach; Statistical modeling; Computer music
SEA: Segment-enrich-annotate paradigm for adapting dialog-based content for improved accessibility,2009,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-71949122659&doi=10.1145%2f1508850.1508853&partnerID=40&md5=e4d1851cf5660ed1f8318d15a382fb74,"While navigation within complex information spaces is a problem for all users, the problem is most evident with individuals who are blind who cannot simply locate, point, and click on a link in hypertext documents with a mouse. Users who are blind have to listen searching for the link in the document using only the keyboard and a screen reader program, which may be particularly inefficient in large documents with many links or deep hierarchies that are hard to navigate. Consequently, they are especially penalized when the information being searched is hidden under multiple layers of indirections. In this article, we introduce a segment-enrich-annotate (SEA) paradigm for adapting digital content with deep structures for improved accessibility. In particular, we instantiate and evaluate this paradigm through the iCare-Assistant, an assistive system for helping students who are blind in accessing Web and electronic course materials. Our evaluations, involving the participation of students who are blind, showed that the iCare-Assistant system, built based on the SEA paradigm, reduces the navigational overhead significantly and enables user who are blind access complex online course servers effectively. © 2009 ACM.",Annotation; Assistive technology for blind users; Educational discussion boards and Web sites; Segmentation; Web navigational aids,Hypertext systems; Image segmentation; Transportation; Web crawler; Annotation; Assistive system; Blind users; Complex information; Digital contents; Educational discussions; Hypertext documents; Navigational aids; E-learning
Semisupervised SVM batch mode active learning with applications to image retrieval,2009,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-68549086546&doi=10.1145%2f1508850.1508854&partnerID=40&md5=ef236844593d4c13694c3536729ea0ea,"Support vector machine (SVM) active learning is one popular and successful technique for relevance feedback in content-based image retrieval (CBIR). Despite the success, conventional SVM active learning has two main drawbacks. First, the performance of SVM is usually limited by the number of labeled examples. It often suffers a poor performance for the small-sized labeled examples, which is the case in relevance feedback. Second, conventional approaches do not take into account the redundancy among examples, and could select multiple examples that are similar (or even identical). In this work, we propose a novel scheme for explicitly addressing the drawbacks. It first learns a kernel function from a mixture of labeled and unlabeled data, and therefore alleviates the problem of small-sized training data. The kernel will then be used for a batch mode active learning method to identify the most informative and diverse examples via a min-max framework. Two novel algorithms are proposed to solve the related combinatorial optimization: the first approach approximates the problem into a quadratic program, and the second solves the combinatorial optimization approximately by a greedy algorithm that exploits the merits of submodular functions. Extensive experiments with image retrieval using both natural photo images and medical images show that the proposed algorithms are significantly more effective than the state-of-the-art approaches. A demo is available at http://msm.cais.ntu.edu.sg/LSCBIR/. © 2009 ACM.",Active learning; Batch mode active learning; Content-based image retrieval; Human-computer interaction; Semisupervised learning; Support vector machines,Algorithms; Combinatorial optimization; Content based retrieval; Feature extraction; Feedback; Gears; Human computer interaction; Information retrieval; Knowledge management; Multilayer neural networks; Active Learning; Active learning methods; Batch modes; Content-Based Image Retrieval; Conventional approach; Greedy algorithms; Kernel function; Labeled and unlabeled data; Medical images; Min-max; Novel algorithm; Photo images; Poor performance; Quadratic programs; Relevance feedback; Semi-supervised; Semi-supervised learning; State-of-the-art approach; Submodular functions; Training data; Support vector machines
Robust result merging using sample-based score estimates,2009,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-72449148083&doi=10.1145%2f1508850.1508852&partnerID=40&md5=b04009afd8b6725e368ac904c7a1ee87,"In federated information retrieval, a query is routed to multiple collections and a single answer list is constructed by combining the results. Such metasearch provides a mechanism for locating documents on the hidden Web and, by use of sampling, can proceed even when the collections are uncooperative. However, the similarity scores for documents returned from different collections are not comparable, and, in uncooperative environments, document scores are unlikely to be reported. We introduce a new merging method for uncooperative environments, in which similarity scores for the sampled documents held for each collection are used to estimate global scores for the documents returned per query. This method requires no assumptions about properties such as the retrieval models used. Using experiments on a wide range of collections, we show that in many cases our merging methods are significantly more effective than previous techniques. © 2009 ACM.",Distributed information retrieval; Result fusion; Result merging; Uncooperative collections,Content based retrieval; Information retrieval; Information services; Rapid thermal annealing; Distributed information retrieval; Global score; Hidden web; Metasearch; Result merging; Retrieval models; Similarity scores; Uncooperative environments; Merging
Bounded coordinate system indexing for real-time video clip search,2009,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-67649647346&doi=10.1145%2f1508850.1508855&partnerID=40&md5=b1cc92e589d1f3e8067aad1ec6b4ebfd,"Recently, video clips have become very popular online. The massive influx of video clips has created an urgent need for video search engines to facilitate retrieving relevant clips. Different from traditional long videos, a video clip is a short video often expressing a moment of significance. Due to the high complexity of video data, efficient video clip search from large databases turns out to be very challenging. We propose a novel video clip representation model called the Bounded Coordinate System (BCS), which is the first single representative capturing the dominating content and contentchanging trends of a video clip. It summarizes a video clip by a coordinate system, where each of its coordinate axes is identified by principal component analysis (PCA) and bounded by the range of data projections along the axis. The similarity measure of BCS considers the operations of translation, rotation, and scaling for coordinate system matching. Particularly, rotation and scaling reflect the difference of content tendencies. Compared with the quadratic time complexity of existing methods, the time complexity of measuring BCS similarity is linear. The compact video representation together with its linear similarity measure makes real-time search from video clip collections feasible. To further improve the retrieval efficiency for large video databases, a two-dimensional transformation method called Bidistance Transformation (BDT) is introduced to utilize a pair of optimal reference points with respect to bidirectional axes in BCS. Our extensive performance study on a large database of more than 30,000 video clips demonstrates that BCS achieves very high search accuracy according to human judgment. This indicates that content tendencies are important in determining the meanings of video clips and confirms that BCS can capture the inherent moment of video clip to some extent that better resembles human perception. In addition, BDT outperforms existing indexing methods greatly. Integration of the BCS model and BDT indexing can achieve real-time search from large video clip databases. © 2009 ACM.",Indexing; Query processing; Summarization; Video search,Content based retrieval; Coordinate measuring machines; Database systems; Face recognition; Gene expression; Indexing (materials working); Indexing (of information); Principal component analysis; Query processing; Rotation; Search engines; Tools; Video recording; Co-ordinate system; Coordinate axes; Data projection; Existing method; Human perception; Indexing methods; Large database; Performance study; Quadratic time; Real time videos; Reference points; Representation model; Retrieval efficiency; Search accuracy; Similarity measure; Time complexity; Video clip database; Video clip search; Video clips; Video data; Video database; Video representations; Video search; Video search engines; Video cameras
An analysis of latent semantic term self-correlation,2009,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-67649386127&doi=10.1145%2f1462198.1462200&partnerID=40&md5=00a2d871fa1408bd3ff1089f25ee765a,"Latent semantic analysis (LSA) is a generalized vector space method that uses dimension reduction to generate term correlations for use during the information retrieval process. We hypothesized that even though the dimension reduction establishes correlations between terms, the dimension reduction is causing a degradation in the correlation of a term to itself (self-correlation). In this article, we have proven that there is a direct relationship to the size of the LSA dimension reduction and the LSA self-correlation. We have also shown that by altering the LSA term self-correlations we gain a substantial increase in precision, while also reducing the computation required during the information retrieval process.",Latent semantic analysis; Term correlation,Information retrieval; Information services; Semantics; Dimension reduction; Latent semantic analysis; Latent semantics; Self-correlation; Term correlation; Vector space methods; Correlation methods
User language model for collaborative personalized search,2009,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-67649344443&doi=10.1145%2f1462198.1462203&partnerID=40&md5=bfbfc52a12fa0a7f29e07ba9b79dde30,"Traditional personalized search approaches rely solely on individual profiles to construct a user model. They are often confronted by two major problems: data sparseness and cold-start for new individuals. Data sparseness refers to the fact that most users only visit a small portion of Web pages and hence a very sparse user-term relationship matrix is generated, while cold-start for new individuals means that the system cannot conduct any personalization without previous browsing history. Recently, community-based approaches were proposed to use the group's social behaviors as a supplement to personalization. However, these approaches only consider the commonality of a group of users and still cannot satisfy the diverse information needs of different users. In this article, we present a new approach, called collaborative personalized search. It considers not only the commonality factor among users for defining group user profiles and global user profiles, but also the specialties of individuals. Then, a statistical user language model is proposed to integrate the individual model, group user model and global user model together. In this way, the probability that a user will like a Web page is calculated through a two-step smoothing mechanism. First, a global user model is used to smooth the probability of unseen terms in the individual profiles and provide aggregated behavior of global users. Then, in order to precisely describe individual interests by looking at the behaviors of similar users, users are clustered into groups and group-user models are constructed. The group-user models are integrated into an overall model through a cluster-based language model. The behaviors of the group users can be utilized to enhance the performance of personalized search. This model can alleviate the two aforementioned problems and provide a more effective personalized search than previous approaches. Large-scale experimental evaluations are conducted to show that the proposed approach substantially improves the relevance of a search over several competitive methods.",Clustering; Cold-start; Collaborative personalized search; Data Sparseness; Smoothing; User language model,Computational linguistics; Probability density function; Starting; Clustering; Cold-start; Collaborative personalized search; Data Sparseness; Smoothing; User language model; Websites
Textual analysis of stock market prediction using breaking financial news: The AZFin text system,2009,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-67649304832&doi=10.1145%2f1462198.1462204&partnerID=40&md5=800cd351b662ddf6e333a1e97805fa8f,"Our research examines a predictive machine learning approach for financial news articles analysis using several different textual representations: bag of words, noun phrases, and named entities. Through this approach, we investigated 9,211 financial news articles and 10,259,042 stock quotes covering the S&P 500 stocks during a five week period. We applied our analysis to estimate a discrete stock price twenty minutes after a news article was released. Using a support vector machine (SVM) derivative specially tailored for discrete numeric prediction and models containing different stock-specific variables, we show that the model containing both article terms and stock price at the time of article release had the best performance in closeness to the actual future stock price (MSE 0.04261), the same direction of price movement as the future price (57.1% directional accuracy) and the highest return using a simulated trading engine (2.06% return). We further investigated the different textual representations and found that a Proper Noun scheme performs better than the de facto standard of Bag of Words in all three metrics.",Prediction; Stock market; SVM,Commerce; Support vector machines; Bag of words; De facto standard; Directional accuracy; Financial news; Machine-learning; Named entities; News articles; Noun phrase; Prediction; Price movement; Proper nouns; Stock market; Stock market prediction; Stock price; Stock quotes; SVM; Textual analysis; Textual representation; Costs
Automatic metadata generation using associative networks,2009,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-67649382180&doi=10.1145%2f1462198.1462199&partnerID=40&md5=f23c749672cd6919261345f2a9e1ab18,"In spite of its tremendous value, metadata is generally sparse and incomplete, thereby hampering the effectiveness of digital information services. Many of the existing mechanisms for the automated creation of metadata rely primarily on content analysis which can be costly and inefficient. The automatic metadata generation system proposed in this article leverages resource relationships generated from existing metadata as a medium for propagation from metadata-rich to metadata-poor resources. Because of its independence from content analysis, it can be applied to a wide variety of resource media types and is shown to be computationally inexpensive. The proposed method operates through two distinct phases. Occurrence and cooccurrence algorithms first generate an associative network of repository resources leveraging existing repository metadata. Second, using the associative network as a substrate, metadata associated with metadata-rich resources is propagated to metadata-poor resources by means of a discrete-form spreading activation algorithm. This article discusses the general framework for building associative networks, an algorithm for disseminating metadata through such networks, and the results of an experiment and validation of the proposed method using a standard bibliographic dataset.",Associative networks; Metadata generation; Particle-swarms,Information services; Semantic Web; Activation algorithm; Associative network; Associative networks; Co-occurrence; Content analysis; Data sets; Media types; Metadata generation; Particle-swarms; Metadata
An adaptive threshold framework for event detection using HMM-based life profiles,2009,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-67649379006&doi=10.1145%2f1462198.1462201&partnerID=40&md5=cc940d099e21a6fcd9237a9da391cbae,"When an event occurs, it attracts attention of information sources to publish related documents along its lifespan. The task of event detection is to automatically identify events and their related documents from a document stream, which is a set of chronologically ordered documents collected from various information sources. Generally, each event has a distinct activeness development so that its status changes continuously during its lifespan. When an event is active, there are a lot of related documents from various information sources. In contrast when it is inactive, there are very few documents, but they are focused. Previous works on event detection did not consider the characteristics of the event's activeness, and used rigid thresholds for event detection. We propose a concept called life profile, modeled by a hidden Markov model, to model the activeness trends of events. In addition, a general event detection framework, LIPED, which utilizes the learned life profiles and the burst-and-diverse characteristic to adjust the event detection thresholds adaptively, can be incorporated into existing event detection methods. Based on the official TDT corpus and contest rules, the evaluation results show that existing detection methods that incorporate LIPED achieve better performance in the cost and F1 metrics, than without.",Clustering; Event detection; Hidden Markov models; Life profiles; TDT; Topic detection,Computational grammars; Object recognition; Wavelet transforms; Clustering; Event detection; Life profiles; TDT; Topic detection; Hidden Markov models
Information filtering and query indexing for an information retrieval model,2009,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-67649306757&doi=10.1145%2f1462198.1462202&partnerID=40&md5=e4d754a61f2a9edbfc4b47a8de1469a7,"In the information filtering paradigm, clients subscribe to a server with continuous queries or profiles that express their information needs. Clients can also publish documents to servers. Whenever a document is published, the continuous queries satisfying this document are found and notifications are sent to appropriate clients. This article deals with the filtering problem that needs to be solved efficiently by each server: Given a database of continuous queries db and a document d, find all queries q db that match d. We present data structures and indexing algorithms that enable us to solve the filtering problem efficiently for large databases of queries expressed in the model AWP. AWP is based on named attributes with values of type text, and its query language includes Boolean and word proximity operators.",Information filtering; Performance evaluation; Query indexing algorithms; Selective dissemination of information,Data structures; Indexing (of information); Information dissemination; Information services; Model structures; Natural language processing systems; Query languages; Servers; Signal filtering and prediction; Continuous queries; Filtering problems; Indexing algorithms; Information filtering; Information need; Information retrieval models; Large database; Performance evaluation; Proximity operator; Query indexing; Query indexing algorithms; Selective dissemination of information; Information retrieval systems
Trusting spam reporters: A reporter-based reputation system for email filtering,2008,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-66949164379&doi=10.1145%2f1416950.1416953&partnerID=40&md5=4d52dde960376cf616eec7c1edade645,"Spam is a growing problem; it interferes with valid email and burdens both email users and service providers. In this work, we propose a reactive spam-filtering system based on reporter reputation for use in conjunction with existing spam-filtering techniques. The system has a trust-maintenance component for users, based on their spam-reporting behavior. The challenge that we consider is that of maintaining a reliable system, not vulnerable to malicious users, that will provide early spam-campaign detection to reduce the costs incurred by users and systems. We report on the utility of a reputation system for spam filtering that makes use of the feedback of trustworthy users. We evaluate our proposed framework, using actual complaint feedback from a large population of users, and validate its spam-filtering performance on a collection of real email traffic over several weeks. To test the broader implication of the system, we create a model of the behavior of malicious reporters, and we simulate the system under various assumptions using a synthetic dataset. © 2008 ACM.",Reputation systems; Spam filtering; Trust,Electronic mail; Spamming; Statistical tests; Data sets; E-mail traffic; Email filtering; Filtering performance; Filtering systems; Filtering technique; Large population; Reliable systems; Reputation systems; Service provider; Spam filtering; Trust; Internet
Toward automatic facet analysis and need negotiation: Lessons from mediated search,2008,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-66949137766&doi=10.1145%2f1416950.1416956&partnerID=40&md5=146a4917d5496e6338111dd97e7336ae,"This work explores the hypothesis that interactions between a trained human search intermediary and an information seeker can inform the design of interactive IR systems. We discuss results from a controlled Wizard-of-Oz case study, set in the context of the TREC 2005 HARD track evaluation, in which a trained intermediary executed an integrated search and interaction strategy based on conceptual facet analysis and informed by need negotiation techniques common in reference interviews. Having a human and quot;in the loop and quot; yielded large improvements over fully automated systems as measured by standard ranked-retrieval metrics, demonstrating the value of mediated search. We present a detailed analysis of the intermediary's actions to gain a deeper understanding of what worked and why. One contribution is a taxonomy of clarification types informed both by empirical results and existing theories in library and information science. We discuss how these findings can guide the development of future systems. Overall, this work illustrates how studying human information-seeking processes can lead to better information retrieval applications. © 2008 ACM.",Interactive information retrieval; Reference interview,Information services; Taxonomies; Automated systems; Empirical results; Human search; Information seeking; Interaction strategy; Interactive information retrieval; Library and information science; Reference interview; Retrieval applications; Wizard of Oz; Information retrieval
Extended probabilistic HAL with close temporal association for psychiatric query document retrieval,2008,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-66949165489&doi=10.1145%2f1416950.1416954&partnerID=40&md5=41e6c7428049d73bb3d76a74f9a7900d,"Psychiatric query document retrieval can assist individuals to locate query documents relevant to their depression-related problems efficiently and effectively. By referring to relevant documents, individuals can understand how to alleviate their depression-related symptoms according to recommendations from health professionals. This work presents an extended probabilistic Hyperspace Analog to Language (epHAL) model to achieve this aim. The epHAL incorporates the close temporal associations between words in query documents to represent word cooccurrence relationships in a high-dimensional context space. The information flow mechanism further combines the query words in the epHAL space to infer related words for effective information retrieval. The language model perplexity is considered as the criterion for model optimization. Finally, the epHAL is adopted for psychiatric query document retrieval, and indicates its superiority in information retrieval over traditional approaches. © 2008 ACM.",Hyperspace Analog to Language (HAL) model; Information flow; Information retrieval; Query documents,Computational linguistics; Information retrieval; Health professionals; High-dimensional; Hyperspace Analog to Language (HAL) model; Information flow; Information flows; Language model; Model optimization; Query documents; Query words; Related word; Relevant documents; Temporal association; Word co-occurrence; Information services
CombinFormation: Mixed-initiative composition of image and text surrogates promotes information discovery,2008,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-66849083461&doi=10.1145%2f1416950.1416955&partnerID=40&md5=ee1e76b81f0fcbb510353c4fbb635d0a,"combinFormation is a mixed-initiative creativity support tool for searching, browsing, organizing, and integrating information. Images and text are connected to represent surrogates (enhanced bookmarks), optimizing the use of human cognitive facilities. Composition, an alternative to lists and spatial hypertext, is used to represent a collection of surrogates as a connected whole, using principles from art and design. This facilitates the creative process of information discovery, in which humans develop new ideas while finding and collecting information. To provoke the user to think about the large space of potentially relevant information resources, a generative agent proactively engages in collecting information resources, forming image and text surrogates, and composing them visually. The agent develops the collection and its visual representation over time, enabling the user to see ideas and relationships. To keep the human in control, we develop interactive mechanisms for authoring the composition and directing the agent. In a field study in an interdisciplinary course on The Design Process, over a hundred students alternated using combinFormation and Google+Word to collect prior work on information discovery invention assignments. The students that used combinFormation's mixed-initiative composition of image and text surrogates performed better. © 2008 ACM.",Clustering; Collections; Creative cognition; Creativity support tools; Exploratory search; Field study; Focused crawler; Information discovery; Mixed-initiative systems; Relevance feedback; Semantics; Software agents,Curricula; Feedback; Hypertext systems; Image retrieval; Information science; Semantics; Teaching; Clustering; Collections; Creative cognition; Creativity support tools; Exploratory search; Field study; Focused crawler; Information discovery; Mixed-initiative systems; Relevance feedback; Software agents
Rank-biased precision for measurement of retrieval effectiveness,2008,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-66949147248&doi=10.1145%2f1416950.1416952&partnerID=40&md5=97e7f548c4357375f09313417fa5380a,"A range of methods for measuring the effectiveness of information retrieval systems has been proposed. These are typically intended to provide a quantitative single-value summary of a document ranking relative to a query. However, many of these measures have failings. For example, recall is not well founded as a measure of satisfaction, since the user of an actual system cannot judge recall. Average precision is derived from recall, and suffers from the same problem. In addition, average precision lacks key stability properties that are needed for robust experiments. In this article, we introduce a new effectiveness metric, rank-biased precision, that avoids these problems. Rank-biased pre-cision is derived from a simple model of user behavior, is robust if answer rankings are extended to greater depths, and allows accurate quantification of experimental uncertainty, even when only partial relevance judgments are available. © 2008 ACM.",Average precision; Pooling; Precision; Recall; Relevance,Behavioral research; Information retrieval systems; Information services; Stability; Average precision; Pooling; Precision; Recall; Relevance; Information retrieval
Sound and complete relevance assessment for XML retrieval,2008,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-66949118546&doi=10.1145%2f1416950.1416951&partnerID=40&md5=b597a66f78f8ddff4bd2aa3ac1370840,"In information retrieval research, comparing retrieval approaches requires test collections consisting of documents, user requests and relevance assessments. Obtaining relevance assessments that are as sound and complete as possible is crucial for the comparison of retrieval approaches. In XML retrieval, the problem of obtaining sound and complete relevance assessments is further complicated by the structural relationships between retrieval results. A major difference between XML retrieval and flat document retrieval is that the relevance of elements (the retrievable units) is not independent of that of related elements. This has major consequences for the gathering of relevance assessments. This article describes investigations into the creation of sound and complete relevance assessments for the evaluation of content-oriented XML retrieval as carried out at INEX, the evaluation campaign for XML retrieval. The campaign, now in its seventh year, has had three substantially different approaches to gather assessments and has finally settled on a highlighting method for marking relevant passages within documents-even though the objective is to collect assessments at element level. The different methods of gathering assessments at INEX are discussed and contrasted. The highlighting method is shown to be the most reliable of the methods. © 2008 ACM.",Evaluation; INEX; Passage retrieval; Relevance assessment; XML; XML retrieval,Information services; XML; Evaluation; INEX; Passage retrieval; Relevance assessment; XML retrieval; Markup languages
Exploring memory in email refinding,2008,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-53849095712&doi=10.1145%2f1402256.1402260&partnerID=40&md5=9829759b249e26d45775b6c025acc8e2,"Human memory plays an important role in personal information management (PIM). Several scholars have noted that people refind information based on what they remember and it has been shown that people adapt their management strategies to compensate for the limitations of memory. Nevertheless, little is known about what people tend to remember about their personal information and how they use their memories to refind. The aim of this article is to increase our understanding of the role that memory plays in the process of refinding personal information. Concentrating on email re-finding, we report on a user study that investigates what attributes of email messages participants remember when trying to refind. We look at how the attributes change in different scenarios and examine the factors which impact on what is remembered. © 2008 ACM.",Email refinding; Information refinding; Memory; User study,Concentration (process); Data storage equipment; Electronic mail; Management; Management information systems; Telecommunication services; Email refinding; Information refinding; Memory; User study; Information management
"How people recall, recognize, and reuse search results",2008,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-53849128579&doi=10.1145%2f1402256.1402258&partnerID=40&md5=593440a35240f77a4a36913cb08e79e6,"When a person issues a query, that person has expectations about the search results that will be returned. These expectations can be based on the current information need, but are also influenced by how the searcher believes the search engine works, where relevant results are expected to be ranked, and any previous searches the individual has run on the topic. This paper looks in depth at how the expectations people develop about search result lists during an initial query affect their perceptions of and interactions with future repeat search result lists. Three studies are presented that give insight into how people recall, recognize, and reuse results. The first study (a study of recall) explores what people recall about previously viewed search result lists. The second study (a study of recognition) builds on the first to reveal that people often recognize a result list as one they have seen before even when it is quite different. As long as those aspects that the searcher remembers about the initial list remain the same, other aspects can change significantly. This is advantageous because, as the third study (a study of reuse) shows, when a result list appears to have changed, people have trouble re-using the previously viewed content in the list. They are less likely to find what they are looking for, less happy with the result quality, more likely to find the task hard, and more likely to take a long time searching. Although apparent consistency is important for reuse, people's inability to recognize change makes consistency without stagnation possible. New relevant results can be presented where old results have been forgotten, making both old and new content easy to find. © 2008 ACM.",Dynamic information; Personal information management; Recall; Recognition; Refinding; Reuse; Search,Search engines; Dynamic information; Personal information management; Recall; Recognition; Refinding; Reuse; Search; Information retrieval systems
"Introduction to keeping, refinding and sharing personal information",2008,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-53849145731&doi=10.1145%2f1402256.1402257&partnerID=40&md5=38182c62922e6b31672cec6245b221c4,[No abstract available],,
Meta methods for model sharing in personal information systems,2008,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-53949120873&doi=10.1145%2f1402256.1402261&partnerID=40&md5=02299ee01714ee1e522082696a315d46,"This article introduces a methodology for automatically organizing document collections into thematic categories for Personal Information Management (PIM) through collaborative sharing of machine learning models in an efficient and privacy-preserving way. Our objective is to combine multiple independently learned models from several users to construct an advanced ensemble-based decision model by taking the knowledge of multiple users into account in a decentralized manner, for example, in a peer-to-peer overlay network. High accuracy of the corresponding supervised (classification) and unsupervised (clustering) methods is achieved by restrictively leaving out uncertain documents rather than assigning them to inappropriate topics or clusters with low confidence. We introduce a formal probabilistic model for the resulting ensemble based meta methods and explain how it can be used for constructing estimators and for goal-oriented tuning. Comprehensive evaluation results on different reference data sets illustrate the viability of our approach. © 2008 ACM.",Classification; Clustering; Meta methods; Peer-to-peer; Personal information management; Restrictive methods,Artificial intelligence; Distributed computer systems; Information retrieval systems; Learning systems; Management; Management information systems; Network protocols; Sensor networks; Telecommunication services; Classification; Clustering; Meta methods; Peer-to-peer; Personal information management; Restrictive methods; Information management
Information scraps: How and why information eludes our personal information management tools,2008,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-53849086550&doi=10.1145%2f1402256.1402263&partnerID=40&md5=190f2c7984d0839602460b52447bd8c7,"In this article we investigate information scraps-personal information where content has been scribbled on Post-it notes, scrawled on the corners of sheets of paper, stuck in our pockets, sent in email messages to ourselves, and stashed in miscellaneous digital text files. Information scraps encode information ranging from ideas and sketches to notes, reminders, shipment tracking numbers, driving directions, and even poetry. Although information scraps are ubiquitous, we have much still to learn about these loose forms of information practice. Why do we keep information scraps outside of our traditional PIM applications What role do information scraps play in our overall information practice How might PIM applications be better designed to accommodate and support information scraps' creation, manipulation and retrieval We pursued these questions by studying the information scrap practices of 27 knowledge workers at five organizations. Our observations shed light on information scraps' content, form, media, and location. From this data, we elaborate on the typical information scrap lifecycle, and identify common roles that information scraps play: temporary storage, archiving, work-in-progress, reminding, and management of unusual data. These roles suggest a set of unmet design needs in current PIM tools: lightweight entry, unconstrained content, flexible use and adaptability, visibility, and mobility. © 2008 ACM.",Information scraps; Note taking; Personal information management,Intermodulation; Intermodulation measurement; Knowledge management; Management; Management information systems; Paper sheeting; Telecommunication services; Digital text; E-mail messages; Information scraps; Knowledge workers; Note taking; Personal information; Personal information management; Temporary storage; Work-in-progress; Information management
Organizing and managing personal electronic files: A mechanical engineer's perspective,2008,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-53849113702&doi=10.1145%2f1402256.1402262&partnerID=40&md5=8b6c157c4fc5b1a108f2a632300829ed,"This article deals with the organization and management of the computer files handled by mechanical engineers on their personal computers. In engineering organizations, a wide variety of electronic files (documents) are necessary to support both business processes and the activities of design and manufacture. Whilst a large number of files and hence information is formally archived, a significant amount of additional information and knowledge resides in electronic files on personal computers. The widespread use of these personal information stores means that all information is retained. However, its reuse is problematic for all but the individual as a result of the naming and organization of the files. To begin to address this issue, a study of the use and current practices for managing personal electronic files is described. The study considers the fundamental classes of files handled by engineers and analyses the organization of these files across the personal computers of 40 participants. The study involves a questionnaire and an electronic audit. The results of these qualitative and quantitative elements are used to elicit an understanding of the practices and requirements of engineers for managing personal electronic files. A potential scheme for naming and organizing personal electronic files is discussed as one possible way to satisfy these requirements. The aim of the scheme is to balance the personal nature of data storage with the need for personal records to be shared with others to support knowledge reuse in engineering organizations. Although this article is concerned with mechanical engineers, the issues dealt with are relevant to knowledge-based industries and, in particular, teams of knowledge workers. © 2008 ACM.",Directory and file naming conventions; Engineers; File sharing and file recognition and recall; Information management,Computer science; Engineering; Health risks; Information management; Knowledge based systems; Knowledge engineering; Knowledge management; Learning systems; Mechanical engineering; Societies and institutions; Business processes; Computer files; Current practices; Data storage; Directory and file naming conventions; Electronic files; Engineering organizations.; Engineers; File sharing and file recognition and recall; Knowledge reuse; Knowledge workers; Knowledge-based; Mechanical engineers; Organization and management; Personal information stores; Widespread use; Personal computers
Editorial: Reviewer merits and review control in an age of electronic manuscript management systems,2008,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-53849118169&doi=10.1145%2f1402256.1402264&partnerID=40&md5=ed8e2d5b66142a63782cdce3004c8df0,"Peer review is an important resource of scholarly communities and must be managed and nurtured carefully. Electronic manuscript management systems have begun to improve some aspects of workflow for conferences and journals but also raise issues related to reviewer roles and reputations and the control of reviews over time. Professional societies should make their policies related to reviews and reviewer histories clear to authors and reviewers, develop strategies and tools to facilitate good and timely reviews, and facilitate the training of new reviewers. © 2008 ACM.",Manuscript management systems; Peer review,Management systems; Manuscript management systems; Peer review; Peer reviews; Strategies and tools; Management
Improved search engines and navigation preference in personal information management,2008,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-53849104327&doi=10.1145%2f1402256.1402259&partnerID=40&md5=b2debc19b60c6575342237be57b99c4b,"Traditionally users access their personal files mainly by using folder navigation. We evaluate whether recent improvements in desktop search have changed this fundamental aspect of Personal Information Management (PIM). We tested this in two studies using the same questionnaire: (a) The Windows Study-a longitudinal comparison of Google Desktop and Windows XP Search Companion, and (b) The Mac Study-a large scale comparison of Mac Spotlight and Sherlock. There were few effects for improved search. First, regardless of search engine, there was a strong navigation preference: on average, users estimated that they used navigation for 56 - 68% of file retrieval events but searched for only 4 - 15% of events. Second, the effect of improving the quality of the search engine on search usage was limited and inconsistent. Third, search was used mainly as a last resort when users could not remember file location. Finally, there was no evidence that using improved desktop search engines leads people to change their filing habits to become less reliant on hierarchical file organization. We conclude by offering theoretical explanations for navigation preference, relating to differences between PIM and Internet retrieval, and suggest alternative design directions for PIM systems. © 2008 ACM.",Files retrieval; Navigation preference; Personal information management; Personal search engines; Search preference; User study,Administrative data processing; Computer software; File organization; Information retrieval; Intermodulation; Intermodulation measurement; Internet; Management; Management information systems; Navigation; Personal computers; Search engines; Telecommunication networks; Telecommunication services; Windows; World Wide Web; Files retrieval; Navigation preference; Personal information management; Personal search engines; Search preference; User study; Information management
Unified relevance models for rating prediction in collaborative filtering,2008,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-46249131793&doi=10.1145%2f1361684.1361689&partnerID=40&md5=746ab980fb09553ffc19b53fa59143ff,"Collaborative filtering aims at predicting a user's interest for a given item based on a collection of user profiles. This article views collaborative filtering as a problem highly related to information retrieval, drawing an analogy between the concepts of users and items in recommender systems and queries and documents in text retrieval. We present a probabilistic user-to-item relevance framework that introduces the concept of relevance into the related problem of collaborative filtering. Three different models are derived, namely, a user-based, an item-based, and a unified relevance model, and we estimate their rating predictions from three sources: the user's own ratings for different items, other users' ratings for the same item, and ratings from different but similar users for other but similar items. To reduce the data sparsity encountered when estimating the probability density function of the relevance variable, we apply the nonparametric (data-driven) density estimation technique known as the Parzen-window method (or kernel-based density estimation). Using a Gaussian window function, the similarity between users and/or items would, however, be based on Euclidean distance. Because the collaborative filtering literature has reported improved prediction accuracy when using cosine similarity, we generalize the Parzen-window method by introducing a projection kernel. Existing user-based and item-based approaches correspond to two simplified instantiations of our framework. User-based and item-based collaborative filterings represent only a partial view of the prediction problem, where the unified relevance model brings these partial views together under the same umbrella. Experimental results complement the theoretical insights with improved recommendation accuracy. The unified model is more robust to data sparsity because the different types of ratings are used in concert. © 2008 ACM.",Collaborative filtering; Personalization; Recommendation,Estimation; Filtration; Forecasting; Information retrieval systems; Information services; Natural language processing systems; Probability; Search engines; Signal filtering and prediction; Windows; Wireless telecommunication systems; Collaborative Filtering (CF); cosine similarity; Data driven (DD); Data sparsity; Density estimation; Different types; Euclidean distance (ED); Experimental results; Filterings; Gaussian Window; Non-parametric; Partial views; prediction accuracy; prediction problem; probability densities; Recommender systems; relevance models; Retrieval (MIR); text retrieval; Unified modeling; user profiling; Users' interests; Window methods; Probability density function
Interpreting TF-IDF term weights as making relevance decisions,2008,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-46249110298&doi=10.1145%2f1361684.1361686&partnerID=40&md5=4788f8cdc40fdd7b45ce9391279db0d5,"A novel probabilistic retrieval model is presented. It forms a basis to interpret the TF-IDF term weights as making relevance decisions. It simulates the local relevance decision-making for every location of a document, and combines all of these ""local"" relevance decisions as the ""document-wide"" relevance decision for the document. The significance of interpreting TF-IDF in this way is the potential to: (1) establish a unifying perspective about information retrieval as relevance decision-making; and (2) develop advanced TF-IDF-related term weights for future elaborate retrieval models. Our novel retrieval model is simplified to a basic ranking formula that directly corresponds to the TF-IDF term weights. In general, we show that the term-frequency factor of the ranking formula can be rendered into different term-frequency factors of existing retrieval systems. In the basic ranking formula, the remaining quantity - log p(r̄t ∈ d) is interpreted as the probability of randomly picking a nonrelevant usage (denoted by ) of term t. Mathematically, we show that this quantity can be approximated by the inverse document-frequency (IDF). Empirically, we show that this quantity is related to IDF, using four reference TREC ad hoc retrieval data collections. © 2008 ACM.",Information retrieval; Relevance decision; Term weight,Boolean functions; Data structures; Image retrieval; Information analysis; Information retrieval; Information science; Information services; Natural language processing systems; Probability; Problem solving; Search engines; Ad Hoc retrieval; data collections; Frequency factors; General (CO); Probabilistic retrieval; Retrieval (MIR); retrieval models; retrieval systems; Decision making
Incremental cluster-based retrieval using compressed cluster-skipping inverted files,2008,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-46249123385&doi=10.1145%2f1361684.1361688&partnerID=40&md5=89651fefbed0526bbc1396769b7ff0d9,"We propose a unique cluster-based retrieval (CBR) strategy using a new cluster-skipping inverted file for improving query processing efficiency. The new inverted file incorporates cluster membership and centroid information along with the usual document information into a single structure. In our incremental-CBR strategy, during query evaluation, both best(-matching) clusters and the best(-matching) documents of such clusters are computed together with a single posting-list access per query term. As we switch from term to term, the best clusters are recomputed and can dynamically change. During query-document matching, only relevant portions of the posting lists corresponding to the best clusters are considered and the rest are skipped. The proposed approach is essentially tailored for environments where inverted files are compressed, and provides substantial efficiency improvement while yielding comparable, or sometimes better, effectiveness figures. Our experiments with various collections show that the incremental-CBR strategy using a compressed cluster-skipping inverted file significantly improves CPU time efficiency, regardless of query length. The new compressed inverted file imposes an acceptable storage overhead in comparison to a typical inverted file. We also show that our approach scales well with the collection size. © 2008 ACM.",Best match; Cluster-based retrieval (CBR); Cluster-skipping inverted index structure (CS-IIS); Full search (FS); Index compression; Inverted index structure (IIS); Query processing,Bits; Data storage equipment; Query processing; Cluster based retrieval; CPU time efficiency; efficiency improvements; Inverted files; Inverted files (IF); Query evaluation; Query lengths; single structure; storage overhead; Information retrieval systems
Assessing multivariate Bernoulli models for information retrieval,2008,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-46249088392&doi=10.1145%2f1361684.1361690&partnerID=40&md5=2bc064dabd13d80df26ff4f419910c68,"Although the seminal proposal to introduce language modeling in information retrieval was based on a multivariate Bernoulli model, the predominant modeling approach is now centered on multinomial models. Language modeling for retrieval based on multivariate Bernoulli distributions is seen inefficient and believed less effective than the multinomial model. In this article, we examine the multivariate Bernoulli model with respect to its successor and examine its role in future retrieval systems. In the context of Bayesian learning, these two modeling approaches are described, contrasted, and compared both theoretically and computationally. We show that the query likelihood following a multivariate Bernoulli distribution introduces interesting retrieval features which may be useful for specific retrieval tasks such as sentence retrieval. Then, we address the efficiency aspect and show that algorithms can be designed to perform retrieval efficiently for multivariate Bernoulli models, before performing an empirical comparison to study the behaviorial aspects of the models. A series of comparisons is then conducted on a number of test collections and retrieval tasks to determine the empirical and practical differences between the different models. Our results indicate that for sentence retrieval the multivariate Bernoulli model can significantly outperform the multinomial model. However, for the other tasks the multinomial model provides consistently better performance (and in most cases significantly so). An analysis of the various retrieval characteristics reveals that the multivariate Bernoulli model tends to promote long documents whose nonquery terms are informative. While this is detrimental to the task of document retrieval (documents tend to contain considerable nonquery content), it is valuable for other tasks such as sentence retrieval, where the retrieved elements are very short and focused. © 2008 ACM.",Information retrieval; Language models; Multinomial; Multivariate Bernoulli,Computational linguistics; Information analysis; Information retrieval; Information science; Information services; Linguistics; Natural language processing systems; Operations research; Search engines; Bayesian learning (BL); Bernoulli; Bernoulli distributions; Bernoulli model; Document Retrieval; Empirical comparison; Language modeling (LM); Modeling approaches; Multinomial models; Retrieval (MIR); retrieval systems; Sentence retrieval; Test collections; Modal analysis
A basis for information retrieval in context,2008,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-46249086998&doi=10.1145%2f1361684.1361687&partnerID=40&md5=d8fc2ef7f1afa15bdcb070d9be5fbeb2,"Information retrieval (IR) models based on vector spaces have been investigated for a long time. Nevertheless, they have recently attracted much research interest. In parallel, context has been rediscovered as a crucial issue in information retrieval. This article presents a principled approach to modeling context and its role in ranking information objects using vector spaces. First, the article outlines how a basis of a vector space naturally represents context, both its properties and factors. Second, a ranking function computes the probability of context in the objects represented in a vector space, namely, the probability that a contextual factor has affected the preparation of an object. © 2008 ACM.",Personalization; Probability; Quantum mechanics; Vector-space model,Data fusion; Information analysis; Information retrieval; Information science; Information services; Probability; Search engines; (PL) properties; information objects; Long time; Ranking functions; Retrieval (MIR); Vector spaces; Vectors
Sentiment analysis in multiple languages: Feature selection for opinion classification in Web forums,2008,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-46249095180&doi=10.1145%2f1361684.1361685&partnerID=40&md5=b3bd639af6ee86ebd91fe53a59a16846,"The Internet is frequently used as a medium for exchange of information and opinions, as well as propaganda dissemination. In this study the use of sentiment analysis methodologies is proposed for classification of Web forum opinions in multiple languages. The utility of stylistic and syntactic features is evaluated for sentiment classification of English and Arabic content. Specific feature extraction components are integrated to account for the linguistic characteristics of Arabic. The entropy weighted genetic algorithm (EWGA) is also developed, which is a hybridized genetic algorithm that incorporates the information-gain heuristic for feature selection. EWGA is designed to improve performance and get a better assessment of key features. The proposed features and techniques are evaluated on a benchmark movie review dataset and U.S. and Middle Eastern Web forum postings. The experimental results using EWGA with SVM indicate high performance levels, with accuracies of over 91% on the benchmark dataset as well as the U.S. and Middle Eastern forums. Stylistic features significantly enhanced performance across all testbeds while EWGA also outperformed other feature selection methods, indicating the utility of these features and techniques for document-level classification of sentiments. © 2008 ACM.",Feature selection; Opinion mining; Sentiment analysis; Text classification,Algorithms; Boolean functions; Classification (of information); Extraction; Genetic algorithms; Heuristic algorithms; Heuristic programming; Information dissemination; Information retrieval systems; Learning systems; Linguistics; Benchmark dataset; Data sets; Enhanced performance; Exchange of information; Experimental results; Feature selection (FS); Feature selection methods; High performance; Hybridized genetic algorithm; movie review; Multiple languages; Sentiment analysis; Sentiment classification; Syntactic features; Test-beds; Web Forums; Web forums; Feature extraction
DirichletRank: Solving the zero-one gap problem of PageRank,2008,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-42049114210&doi=10.1145%2f1344411.1344416&partnerID=40&md5=9a583049a01d51e73dbe525f49d454a2,"Link-based ranking algorithms are among the most important techniques to improve web search. In particular, the PageRank algorithm has been successfully used in the Google search engine and has been attracting much attention recently. However, we find that PageRank has a zero-one gap problem which, to the best of our knowledge, has not been addressed in any previous work. This problem can be potentially exploited to spam PageRank results and make the state-of-the-art link-based antispamming techniques ineffective. The zero-one gap problem arises as a result of the current ad hoc way of computing transition probabilities in the random surfing model. We therefore propose a novel DirichletRank algorithm which calculates these probabilities using Bayesian estimation with a Dirichlet prior. DirichletRank is a variant of PageRank, but does not have the problem of zero-one gap and can be analytically shown substantially more resistant to some link spams than PageRank. Experiment results on TREC data show that DirichletRank can achieve better retrieval accuracy than PageRank due to its more reasonable allocation of transition probabilities. More importantly, experiments on the TREC dataset and another real web dataset from the Webgraph project show that, compared with the original PageRank, DirichletRank is more stable under link perturbation and is significantly more robust against both manually identified web spams and several simulated link spams. DirichletRank can be computed as efficiently as PageRank, and thus is scalable to large-scale web applications. © 2008 ACM.",DirichletRank; Link analysis; PageRank; Spamming; Zero-one gap,Perturbation techniques; Probability; Problem solving; Search engines; Spamming; Web services; DirichletRank; Link analysis; PageRank; Zero-one gap; Information systems
Writeprints: A stylometric approach to identity-level identification and similarity detection in cyberspace,2008,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-42049084142&doi=10.1145%2f1344411.1344413&partnerID=40&md5=7376afbb8100e05805ce54070ebb23c5,"One of the problems often associated with online anonymity is that it hinders social accountability, as substantiated by the high levels of cybercrime. Although identity cues are scarce in cyberspace, individuals often leave behind textual identity traces. In this study we proposed the use of stylometric analysis techniques to help identify individuals based on writing style. We incorporated a rich set of stylistic features, including lexical, syntactic, structural, content-specific, and idiosyncratic attributes. We also developed the Writeprints technique for identification and similarity detection of anonymous identities. Writeprints is a Karhunen-Loeve transforms-based technique that uses a sliding window and pattern disruption algorithm with individual author-level feature sets. The Writeprints technique and extended feature set were evaluated on a testbed encompassing four online datasets spanning different domains: email, instant messaging, feedback comments, and program code. Writeprints outperformed benchmark techniques, including SVM, Ensemble SVM, PCA, and standard Karhunen-Loeve transforms, on the identification and similarity detection tasks with accuracy as high as 94% when differentiating between 100 authors. The extended feature set also significantly outperformed a baseline set of features commonly used in previous research. Furthermore, individual-author-level feature sets generally outperformed use of a single group of attributes. © 2008 ACM.",Discourse; Online text; Style classification; Stylometry; Text mining,Classification (of information); Computer crime; Data mining; Electronic mail; Feedback; Message passing; Accountability; Online text; Style classification; Stylometry; Online systems
Towards a belief-revision-based adaptive and context-sensitive information retrieval system,2008,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-42049092041&doi=10.1145%2f1344411.1344414&partnerID=40&md5=d92149e8c4a7ed3c53ff177f074d3ee1,"In an adaptive information retrieval (IR) setting, the information seekers' beliefs about which terms are relevant or nonrelevant will naturally fluctuate. This article investigates how the theory of belief revision can be used to model adaptive IR. More specifically, belief revision logic provides a rich representation scheme to formalize retrieval contexts so as to disambiguate vague user queries. In addition, belief revision theory underpins the development of an effective mechanism to revise user profiles in accordance with information seekers' changing information needs. It is argued that information retrieval contexts can be extracted by means of the information-flow text mining method so as to realize a highly autonomous adaptive IR system. The extra bonus of a belief-based IR model is that its retrieval behavior is more predictable and explanatory. Our initial experiments show that the belief-based adaptive IR system is as effective as a classical adaptive IR system. To our best knowledge, this is the first successful implementation and evaluation of a logic-based adaptive IR model which can efficiently process large IR collections. © 2008 ACM.",Adaptive information retrieval; Belief revision; Information flow; Retrieval context; Text mining,Adaptive systems; Data mining; Information retrieval; Mathematical models; Query processing; User interfaces; Adaptive information retrieval; Belief revision; Belief revision theory; Information flow; Retrieval context; Text mining; Information systems
Classification-aware hidden-web text database selection,2008,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-42049120674&doi=10.1145%2f1344411.1344412&partnerID=40&md5=58423645663b1c627b46bbeec99945aa,"Many valuable text databases on the web have noncrawlable contents that are hidden behind search interfaces. Metasearchers are helpful tools for searching over multiple such hidden-web text databases at once through a unified query interface. An important step in the metasearching process is database selection, or determining which databases are the most relevant for a given user query. The state-of-the-art database selection techniques rely on statistical summaries of the database contents, generally including the database vocabulary and associated word frequencies. Unfortunately, hidden-web text databases typically do not export such summaries, so previous research has developed algorithms for constructing approximate content summaries from document samples extracted from the databases via querying. We present a novel focused-probing sampling algorithm that detects the topics covered in a database and adaptively extracts documents that are representative of the topic coverage of the database. Our algorithm is the first to construct content summaries that include the frequencies of the words in the database. Unfortunately, Zipf's law practically guarantees that for any relatively large database, content summaries built from moderately sized document samples will fail to cover many low-frequency words; in turn, incomplete content summaries might negatively affect the database selection process, especially for short queries with infrequent words. To enhance the sparse document samples and improve the database selection decisions, we exploit the fact that topically similar databases tend to have similar vocabularies, so samples extracted from databases with a similar topical focus can complement each other. We have developed two database selection algorithms that exploit this observation. The first algorithm proceeds hierarchically and selects the best categories for a query, and then sends the query to the appropriate databases in the chosen categories. The second algorithm uses shrinkage, a statistical technique for improving parameter estimation in the face of sparse data, to enhance the database content summaries with category-specific words. We describe how to modify existing database selection algorithms to adaptively decide (at runtime) whether shrinkage is beneficial for a query. A thorough evaluation over a variety of databases, including 315 real web databases as well as TREC data, suggests that the proposed sampling methods generate high-quality content summaries and that the database selection algorithms produce significantly more relevant database selection decisions and overall search results than existing algorithms. © 2008 ACM.",Database selection; Distributed information retrieval; Web search,Classification (of information); Query processing; Statistical methods; Text processing; Vocabulary control; Web services; Database selection; Distributed information retrieval; Web search; Database systems
Locality-based pruning methods for web search,2008,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-42049099427&doi=10.1145%2f1344411.1344415&partnerID=40&md5=69a1f11870207a871d8f999ffea38be1,"This article discusses a novel approach developed for static index pruning that takes into account the locality of occurrences of words in the text. We use this new approach to propose and experiment on simple and effective pruning methods that allow a fast construction of the pruned index. The methods proposed here are especially useful for pruning in environments where the document database changes continuously, such as large-scale web search engines. Extensive experiments are presented showing that the proposed methods can achieve high compression rates while maintaining the quality of results for the most common query types present in modern search engines, namely, conjunctive and phrase queries. In the experiments, our locality-based pruning approach allowed reducing search engine indices to 30% of their original size, with almost no reduction in precision at the top answers. Furthermore, we conclude that even an extremely simple locality-based pruning method can be competitive when compared to complex methods that do not rely on locality information. © 2008 ACM.",Indexing; Information retrieval; Pruning; Search; Search engines; Web search,Abstracting; Computational methods; Database systems; Information retrieval; Query processing; Search engines; Locality information; Pruning; Top answers; Web search; Web services
On ranking techniques for desktop search,2008,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-42049093964&doi=10.1145%2f1344411.1344417&partnerID=40&md5=64a71c29efbaec565ae89609a58b2374,"Users tend to store huge amounts of files, of various formats, on their personal computers. As a result, finding a specific, desired file within the file system is a challenging task. This article addresses the desktop search problem by considering various techniques for ranking results of a search query over the file system. First, basic ranking techniques, which are based on various file features (e.g., file name, access date, file size, etc.), are considered and their effectiveness is empirically analyzed. Next, two learning-based ranking schemes are presented, and are shown to be significantly more effective than the basic ranking methods. Finally, a novel ranking technique, based on query selectiveness, is considered for use during the cold-start period of the system. This method is also shown to be empirically effective, even though it does not involve any learning. © 2008 ACM.",Desktop search; Personal information management; Ranking,Information management; Information systems; Learning systems; Personal computers; Desktop search; Formats; Personal information management; Ranking; Search engines
A formal model of annotations of digital content,2007,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-37049000565&doi=10.1145%2f1292591.1292594&partnerID=40&md5=20928b777eee30f2b786ef9404e7760e,"This article is a study of the themes and issues concerning the annotation of digital contents, such as textual documents, images, and multimedia documents in general. These digital contents are automatically managed by different kinds of digital library management systems and more generally by different kinds of information management systems. Even though this topic has already been partially studied by other researchers, the previous research work on annotations has left many open issues. These issues concern the lack of clarity about what an annotation is, what its features are, and how it is used. These issues are mainly due to the fact that models and systems for annotations have only been developed for specific purposes. As a result, there is only a fragmentary picture of the annotation and its management, and this is tied to specific contexts of use and lacks-general validity. The aim of the article is to provide a unified and integrated picture of the annotation, ranging from defining what an annotation is to providing a formal model. The key ideas of the model are: the distinction between the meaning and the sign of the annotation, which represent the semantics and the materialization of an annotation, respectively; the clear formalization of the temporal dimension involved with annotations; and the introduction of a distributed hypertext between digital contents and annotations. Therefore, the proposed formal model captures both syntactic and semantic aspects of the annotations. Furthermore, it is built on previously existing models and may be seen as an extension of them. © 2007 ACM.",Annotation; Digital content; Digital library system; Foundations; Hypertext,Digital libraries; Formal logic; Hypertext systems; Information management; Multimedia systems; Semantics; Digital content; Digital library systems; Formal models; Digital signal processing
Error correction vs. query garbling for Arabic OCR document retrieval,2007,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-37049013904&doi=10.1145%2f1292591.1292596&partnerID=40&md5=1b4a1e342475d1cd6ee243a4fe1b884d,"Due to the existence of large numbers of legacy documents (such as old books and newspapers), improving retrieval effectiveness for OCR'ed documents continues to be an important problem. This article compares the effect of OCR error correction with and without language modeling and the effect of query garbling with weighted structured queries on the retrieval of OCR degraded Arabic documents. The results suggest that moderate error correction does not yield statistically significant improvement in retrieval effectiveness when indexing and searching using n-grams. Also, reversing error correction models to perform query garbling in conjunction with weighted structured queries yields improved retrieval effectiveness. Lastly, using very good error correction that utilizes language modeling yields the best improvement in retrieval effectiveness. © 2007 ACM.",Arabic retrieval; OCR correction; OCR retrieval,Data mining; Error correction; Indexing (of information); Natural language processing systems; Optical character recognition; Query processing; Arabic documents; Arabic retrieval; OCR correction; OCR retrieval; Information retrieval
Frequency-based identification of correct translation equivalents (FITE) obtained through transformation rules,2007,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-37049004149&doi=10.1145%2f1292591.1292593&partnerID=40&md5=a89c7b7840afe0d94b4f771be94f211b,"We devised a novel statistical technique for the identification of the translation equivalents of source words obtained by transformation rule based translation (TRT). The effectiveness of the technique called frequency-based identification of translation equivalents (FITE) was tested using biological and medical cross-lingual spelling variants and out-of-vocabulary (OOV) words in Spanish-English and Finnish-English TRT. The results showed that, depending on the source language and frequency corpus, FITE-TRT (the identification of translation equivalents from TRT's translation set by means of the FITE technique) may achieve high translation recall. In the case of the Web as the frequency corpus, translation recall was 89.2% - 91.0% for Spanish-English FITE-TRT. For both language pairs FITE-TRT achieved high translation precision: 95.0% - 98.8%. The technique also reliably identified native source language words: source words that cannot be correctly translated by TRT. Dictionary-based CLIR augmented with FITE-TRT performed substantially better than basic dictionary-based CLIR where OOV keys were kept intact. FITE-TRT with Web document frequencies was the best technique among several fuzzy translation/matching approaches tested in cross-language retrieval experiments. We also discuss the application of FITE-TRT in the automatic construction of multilingual dictionaries. © 2007 ACM.",Cross-language information retrieval; Fuzzy matching; OOV words; Transformation rules; Transliteration,Computational linguistics; Fuzzy rules; Information retrieval; Natural language processing systems; Translation (languages); Cross language information retrieval; Fuzzy matching; Transformation rules; Transliteration; Character recognition
Does a one-size recommendation system fit all? the effectiveness of collaborative filtering based recommendation systems across different domains and search modes,2007,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-37049012047&doi=10.1145%2f1292591.1292595&partnerID=40&md5=ca8a522bab047fa636a55014d1a5de93,"Collaborative filtering (CF) is a personalization technology that generates recommendations for users based on others' evaluations. CF is used by numerous e-commerce Web sites for providing personalized recommendations. Although much research has focused on refining collaborative filtering algorithms, little is known about the effects of user and domain characteristics on the accuracy of collaborative filtering systems. In this study, the effects of two factors - -product domain and users' search mode - -on the accuracy of CF are investigated. The effects of those factors are tested using data collected from two experiments in two different product domains, and from two large CF datasets, EachMovie and Book-Crossing. The study shows that the search mode of the users strongly influences the accuracy of the recommendations. CF works better when users look for specific information than when they search for general information. The accuracy drops significantly when data from different modes are mixed. The study also shows that CF is more accurate for knowledge domains than for consumer product domains. The results of this study imply that for more accurate recommendations, collaborative filtering systems should be able to identify and handle users' mode of search, even within the same domain and user group. © 2007 ACM.",Collaborative filtering; Recommendation systems,Data mining; Electronic commerce; Query processing; Signal filtering and prediction; Web services; Websites; Collaborative filtering; Recommendation systems; Information retrieval systems
Repeatable evaluation of search services in dynamic environments,2007,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-37049027673&doi=10.1145%2f1292591.1292592&partnerID=40&md5=18c2e7b5c764e659673970610a495f51,"In dynamic environments, such as the World Wide Web, a changing document collection, query population, and set of search services demands frequent repetition of search effectiveness (relevance) evaluations. Reconstructing static test collections, such as in TREC, requires considerable human effort, as large collection sizes demand judgments deep into retrieved pools. In practice it is common to perform shallow evaluations over small numbers of live engines (often pairwise, engine A vs. engine B) without system pooling. Although these evaluations are not intended to construct reusable test collections, their utility depends on conclusions generalizing to the query population as a whole. We leverage the bootstrap estimate of the reproducibility probability of hypothesis tests in determining the query sample sizes required to ensure this, finding they are much larger than those required for static collections. We propose a semiautomatic evaluation framework to reduce this effort. We validate this framework against a manual evaluation of the top ten results of ten Web search engines across 896 queries in navigational and informational tasks. Augmenting manual judgments with pseudo-relevance judgments mined from Web taxonomies reduces both the chances of missing a correct pairwise conclusion, and those of finding an errant conclusion, by approximately 50%. © 2007 ACM.",Evaluation; Web search,Approximation theory; Automation; Computer software reusability; Dynamic programming; Query processing; Static analysis; Document collection; Query population; Search services; Semiautomatic evaluation; Static tests; Search engines
YASS: Yet another suffix stripper,2007,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-35349020104&doi=10.1145%2f1281485.1281489&partnerID=40&md5=f27950936d743adfb2646490a448cd67,"Stemmers attempt to reduce a word to its stem or root form and are used widely in information retrieval tasks to increase the recall rate. Most popular stemmers encode a large number of language-specific rules built over a length of time. Such stemmers with comprehensive rules are available only for a few languages. In the absence of extensive linguistic resources for certain languages, statistical language processing tools have been successfully used to improve the performance of IR systems. In this article, we describe a clustering-based approach to discover equivalence classes of root words and their morphological variants. A set of string distance measures are defined, and the lexicon for a given text collection is clustered using the distance measures to identify these equivalence classes. The proposed approach is compared with Porter's and Lovin's stemmers on the AP and WSJ subcollections of the Tipster dataset using 200 queries. Its performance is comparable to that of Porter's and Lovin's stemmers, both in terms of average precision and the total number of relevant documents retrieved. The proposed stemming algorithm also provides consistent improvements in retrieval performance for French and Bengali, which are currently resource-poor. © 2007 ACM.",Bengali; Clustering; Corpus; French; Indian languages; Stemming; String similarity,Clustering algorithms; Computer programming languages; Distance measurement; Logic programming; Signal encoding; Bengali (Language); Corpus; French (regional language); IR systems; String similarity; Information retrieval
TOIS reviewers January 2006 through May 2007,2007,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-35348956967&doi=10.1145%2f1281485.1281486&partnerID=40&md5=34bbeace12cd29d4bd071f69db320d3c,[No abstract available],,
Adaptive hypermedia through contextualized open hypermedia structures,2007,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-35349009150&doi=10.1145%2f1281485.1281487&partnerID=40&md5=5e4c35d7c3f2388896443b745687d526,"The aim of this article is to produce an alternative view of the adaptive hypermedia (AH) domain from a contextually-aware open hypermedia (OH) perspective. We believe that a wide range of AH techniques can be supported with a small number of OH structures, which can be combined together to create more complex effects, possibly simplifying the development of new AH systems. In this work we reexamine Brusilovsky's taxonomy of AH techniques from a structural OH perspective. We also show that it is possible to identify and model common structures across the taxonomy of adaptive techniques. An agent-based adaptive hypermedia system called HA 3L is presented, which uses these OH structures to provide a straightforward implementation of a variety of adaptive hypermedia techniques. This enables us to reflect on the structural equivalence of many of the techniques, demonstrates the advantages of the OH approach, and can inform the design of future adaptive hypermedia systems. © 2007 ACM.",Adaptive hypermedia; Adaptive techniques; FOHM; Hypermedia structure; Open hypermedia,Mathematical models; Systems analysis; Adaptive hypermedia; Adaptive techniques; Hypermedia structure; Open hypermedia; Information systems
ServiceFinder: A method towards enhancing service portals,2007,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-35348964276&doi=10.1145%2f1281485.1281488&partnerID=40&md5=9e5b8d2454ea095d736c51b8cd3fcab5,"The rapid advancement of Internet technologies enables more and more educational institutes, companies, and government agencies to provide services, namely online services, through web portals. With hundreds of online services provided through a web portal, it is critical to design web portals, namely service portals, through which online services can be easily accessed by their consumers. This article addresses this critical issue from the perspective of service selection, that is, how to select a small number of service-links (i.e., hyperlinks pointing to online services) to be featured in the homepage of a service portal such that users can be directed to find the online services they seek most effectively. We propose a mathematically formulated metric to measure the effectiveness of the selected service-links in directing users to locate their desired online services and formally define the service selection problem. A solution method, ServiceFinder, is then proposed. Using real-world data obtained from the Utah State Government service portal, we show that ServiceFinder outperforms both the current practice of service selection and previous algorithms for adaptive website design. We also show that the performance of ServiceFinder is close to that of the optimal solution resulting from exhaustive search. © 2007 ACM.",Online service; Service portal; Service selection,Distance education; Internet; Online systems; Portals; Virtual reality; Online services; Service portals; Service selection; Web services
Reducing human interactions in Web directory searches,2007,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-35348966450&doi=10.1145%2f1281485.1281491&partnerID=40&md5=0a3942dbd35a4fde41f2c211252322f5,"Consider a website containing a collection of webpages with data such as in Yahoo or the Open Directory project. Each page is associated with a weight representing the frequency with which that page is accessed by users. In the tree hierarchy representation, accessing each page requires the user to travel along the path leading to it from the root. By enhancing the index tree with additional edges (hotlinks) one may reduce the access cost of the system. In other words, the hotlinks reduce the expected number of steps needed to reach a leaf page from the tree root, assuming that the user knows which hotlinks to take. The hotlink enhancement problem involves finding a set of hotlinks minimizing this cost. This article proposes the first exact algorithm for the hotlink enhancement problem. This algorithm runs in polynomial time for trees with logarithmic depth. Experiments conducted with real data show that significant improvement in the expected number of accesses per search can be achieved in websites using this algorithm. These experiments also suggest that the simple and much faster heuristic proposed previously by Czyzowicz et al. [2003] creates hotlinks that are nearly optimal in the time savings they provide to the user. The version of the hotlink enhancement problem in which the weight distribution on the leaves is unknown is discussed as well. We present a polynomial-time algorithm that is optimal for any tree for any depth. © 2007 ACM.",Algorithms; Directory tree; Hotlink; Hotlist; Hyperlink,Algorithms; Human computer interaction; Project management; Web services; Directory trees; Hotlink; Hotlist; Hyperlinks; Online searching
A novel XML music information retrieval method using graph invariants,2007,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-35348980897&doi=10.1145%2f1281485.1281490&partnerID=40&md5=e80a32cae38692a2d52c8ccc60a0215c,"The increasing diffusion of XML languages for the encoding of domain-specific multimedia information raises the need for new information retrieval models that can fully exploit structural information. An XML language specifically designed for music like MX allows queries to be made directly on the thematic material. The main advantage of such a system is that it can handle symbolic, notational, and audio objects at the same time through a multilayered structure. On the model side, common music information retrieval methods do not take into account the inner structure of melodic themes and the metric relationships between notes. In this article we deal with two main topics: a novel architecture based on a new XML language for music and a new model of melodic themes based on graph theory. This model takes advantage of particular graph invariants that can be linked to melodic themes as metadata in order to characterize all their possible modifications through specific transformations and that can be exploited in filtering algorithms. We provide a similarity function and show through an evaluation stage how it improves existing methods, particularly in the case of same-structured themes. © 2007 ACM.",Graphs; Invariants; Melodic similarity; Metadata; Music; Music information retrieval; Structural properties; XML,Computer music; Graph theory; Metadata; XML; Melodic similarity; Music information retrieval; Thematic materials; Information retrieval
On setting the hyper-parameters of term frequency normalization for information retrieval,2007,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34547452270&doi=10.1145%2f1247715.1247719&partnerID=40&md5=13bc948297eeba6c390880e2c348154b,"The setting of the term frequency normalization hyper-parameter suffers from the query dependence and collection dependence problems, which remarkably hurt the robustness of the retrieval performance. Our study in this article investigates three term frequency normalization methods, namely normalization 2, BM25's normalization and the Dirichlet Priors normalization. We tackle the query dependence problem by modifying the query term weight using a Divergence From Randomness term weighting model, and tackle the collection dependence problem by measuring the correlation of the normalized term frequency with the document length. Our research hypotheses for the two problems, as well as an automatic hyper-parameter setting methodology, are extensively validated and evaluated on four Text REtrieval Conference (TREC) collections. © 2007 ACM.",Collection-dependence; Information retrieval models; Query-dependence; Relevance feedback; Term frequency normalization; TREC experimentation,Correlation methods; Feedback control; Mathematical models; Parameter estimation; Problem solving; Query languages; Collection-dependence; Query dependence; Relevance feedback; Term frequency normalization; TREC experimentation; Information retrieval systems
Online supervised spam filter evaluation,2007,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34547480985&doi=10.1145%2f1247715.1247717&partnerID=40&md5=5c8cdbeec6fe46f7e0e892c1099562fe,"Eleven variants of six widely used open-source spam filters are tested on a chronological sequence of 49086 e-mail messages received by an individual from August 2003 through March 2004. Our approach differs from those previously reported in that the test set is large, comprises uncensored raw messages, and is presented to each filter sequentially with incremental feedback. Misclassification rates and Receiver Operating Characteristic Curve measurements are reported, with statistical confidence intervals. Quantitative results indicate that content-based filters can eliminate 98% of spam while incurring 0.1% legitimate email loss. Qualitative results indicate that the risk of loss depends on the nature of the message, and that messages likely to be lost may be those that are less critical. More generally, our methodology has been encapsulated in a free software toolkit, which may used to conduct similar experiments. © 2007 ACM.",Email; Spam; Text classification,Classification (of information); Computer aided software engineering; Content based retrieval; Electronic mail; Encapsulation; Message passing; Characteristic Curve measurements; Content-based filters; E-mail messages; Spam filters; Online systems
Discovering personally meaningful places: An interactive clustering approach,2007,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34547410460&doi=10.1145%2f1247715.1247718&partnerID=40&md5=58ed49350b7dc56d9d42aa32f7f57535,"The discovery of a person's meaningful places involves obtaining the physical locations and their labels for a person's places that matter to his daily life and routines. This problem is driven by the requirements from emerging location-aware applications, which allow a user to pose queries and obtain information in reference to places, for example, home, work or Northwest Health Club. It is a challenge to map from physical locations to personally meaningful places due to a lack of understanding of what constitutes the real users' personally meaningful places. Previous work has explored algorithms to discover personal places from location data. However, we know of no systematic empirical evaluations of these algorithms, leaving designers of location-aware applications in the dark about their choices. Our work remedies this situation. We extended a clustering algorithm to discover places. We also defined a set of essential evaluation metrics and an interactive evaluation framework. We then conducted a large-scale experiment that collected real users' location data and personally meaningful places, and illustrated the utility of our evaluation framework. Our results establish a baseline that future work can measure itself against. They also demonstrate that that our algorithm discovers places with reasonable accuracy and outperforms the well-known K-Means clustering algorithm for place discovery. Finally, we provide evidence that shapes more complex than points are required to represent the full range of people's everyday places. © 2007 ACM.",Clustering algorithms; Field studies; Location-aware applications; Place discovery; Ubiquitous computing,Data reduction; Large scale systems; Problem solving; Query languages; Ubiquitous computing; Field studies; K-Means clustering algorithms; Location-aware applications; Place discovery; Clustering algorithms
Answering XML queries by means of data summaries,2007,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34547475109&doi=10.1145%2f1247715.1247716&partnerID=40&md5=4c696c22db15a6163aec9072f2fe41dc,"XML is a rather verbose representation of semistructured data, which may require huge amounts of storage space. We propose a summarized representation of XML data, based on the concept of instance pattern, which can both provide succinct information and be directly queried. The physical representation of instance patterns exploits itemsets or association rules to summarize the content of XML datasets. Instance patterns may be used for (possibly partially) answering queries, either when fast and approximate answers are required, or when the actual dataset is not available, for example, it is currently unreachable. Experiments on large XML documents show that instance patterns allow a significant reduction in storage space, while preserving almost entirely the completeness of the query result. Furthermore, they provide fast query answers and show good scalability on the size of the dataset, thus overcoming the document size limitation of most current XQuery engines. © 2007 ACM.",Association rules; Data mining; Data summarization; Intensional answers; Itemsets; Semistructured data,Association rules; Data mining; Data structures; Information retrieval systems; Query languages; Scalability; Data summarization; Intensional answers; Itemsets; Semistructured data; XML
Temporal profiles of queries,2007,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34547472155&doi=10.1145%2f1247715.1247720&partnerID=40&md5=44865d26278bed995e0131fed61f22be,"Documents with timestamps, such as email and news, can be placed along a timeline. The timeline for a set of documents returned in response to a query gives an indication of how documents relevant to that query are distributed in time. Examining the timeline of a query result set allows us to characterize both how temporally dependent the topic is, as well as how relevant the results are likely to be. We outline characteristic patterns in query result set timelines, and show experimentally that we can automatically classify documents into these classes. We also show that properties of the query result set timeline can help predict the mean average precision of a query. These results show that meta-features associated with a query can be combined with text retrieval techniques to improve our understanding and treatment of text search on documents with timestamps. © 2007 ACM.",Ambiguity; Event detection; Language models; Precision prediction; Query classification; Temporal profiles; Time,Automatic programming; Information retrieval systems; Metadata; Pattern recognition; Query languages; Response time (computer systems); Event detection; Language models; Precision prediction; Query classification; Temporal profiles; Temporal logic
Evaluating the accuracy of implicit feedback from clicks and query reformulations in Web search,2007,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34247882698&doi=10.1145%2f1229179.1229181&partnerID=40&md5=daffc0932e92a3e1ed0574a91b4f6c9f,"This article examines the reliability of implicit feedback generated from clickthrough data and query reformulations in World Wide Web (WWW) search. Analyzing the users' decision process using eyetracking and comparing implicit feedback against manual relevance judgments, we conclude that clicks are informative but biased. While this makes the interpretation of clicks as absolute relevance judgments difficult, we show that relative preferences derived from clicks are reasonably accurate on average. We find that such relative preferences are accurate not only between results from an individual query, but across multiple sets of results within chains of query reformulations. © 2007 ACM.",Clickthrough data; Eye-tracking; Implicit feedback; Query reformulations; User studies,Data reduction; Decision making; Feedback; Search engines; User interfaces; Clickthrough data; Eye tracking; Implicit feedback; Query reformulations; User studies; Query processing
Automatic classification of Web queries using very large unlabeled query logs,2007,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34247869672&doi=10.1145%2f1229179.1229183&partnerID=40&md5=95a91c5b32ec8fc03bef707a81ae07cc,"Accurate topical classification of user queries allows for increased effectiveness and efficiency in general-purpose Web search systems. Such classification becomes critical if the system must route queries to a subset of topic-specific and resource-constrained back-end databases. Successful query classification poses a challenging problem, as Web queries are short, thus providing few features. This feature sparseness, coupled with the constantly changing distribution and vocabulary of queries, hinders traditional text classification. We attack this problem by combining multiple classifiers, including exact lookup and partial matching in databases of manually classified frequent queries, linear models trained by supervised learning, and a novel approach based on mining selectional preferences from a large unlabeled query log. Our approach classifies queries without using external sources of information, such as online Web directories or the contents of retrieved pages, making it viable for use in demanding operational environments, such as large-scale Web search services. We evaluate our approach using a large sample of queries from an operational Web search engine and show that our combined method increases recall by nearly 40% over the best single method while maintaining adequate precision. Additionally, we compare our results to those from the 2005 KDD Cup and find that we perform competitively despite our operational restrictions. This suggests it is possible to topically classify a significant portion of the query stream without requiring external sources of information, allowing for deployment in operationally restricted environments. © 2007 ACM.",,Classification (of information); Database systems; Search engines; User interfaces; Vocabulary control; World Wide Web; Feature sparseness; Partial matching; Query classification; User queries; Query processing
Soft pattern matching models for definitional question answering,2007,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34247877615&doi=10.1145%2f1229179.1229182&partnerID=40&md5=db82fc481bb962a8ed048c8efbde8e00,"We explore probabilistic lexico-syntactic pattern matching, also known as soft pattern matching, in a definitional question answering system. Most current systems use regular expression-based hard matching patterns to identify definition sentences. Such rigid surface matching often fares poorly when faced with language variations. We propose two soft matching models to address this problem: one based on bigrams and the other on the Profile Hidden Markov Model (PHMM). Both models provide a theoretically sound method to model pattern matching as a probabilistic process that generates token sequences. We demonstrate the effectiveness of the models on definition sentence retrieval for definitional question answering. We show that both models significantly outperform the state-of-the-art manually constructed hard matching patterns on recent TREC data. A critical difference between the two models is that the PHMM has a more complex topology. We experimentally show that the PHMM can handle language variations more effectively but requires more training data to converge. While we evaluate soft pattern models only on definitional question answering, we believe that both models are generic and can be extended to other areas where lexico-syntactic pattern matching can be applied. © 2007 ACM.",Definitional question answering; Soft patterns,Computer programming languages; Data reduction; Hidden Markov models; Mathematical models; Probability; Query processing; Definition sentences; Definitional question answering; Profile Hidden Markov Model (PHMM); Soft patterns; Pattern matching
An exploration of the principles underlying redundancy-based factoid question answering,2007,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34247846152&doi=10.1145%2f1229179.1229180&partnerID=40&md5=bfe8d992410ab66ce2df26e4e1863a93,"The so-called redundancy-based approach to question answering represents a successful strategy for mining answers to factoid questions such as Who shot Abraham Lincoln from the World Wide Web. Through contrastive and ablation experiments with Aranea, a system that has performed well in several TREC QA evaluations, this work examines the underlying assumptions and principles behind redundancy-based techniques. Specifically, we develop two theses: that stable characteristics of data redundancy allow factoid systems to rely on external black box components, and that despite embodying a data-driven approach, redundancy-based methods encode a substantial amount of knowledge in the form of heuristics. Overall, this work attempts to address the broader question of what really matters and to provide guidance for future researchers. © 2007 ACM.",Data redundancy; Web search,Data mining; Heuristic methods; Search engines; World Wide Web; Data redundancy; Factoid question answering; Redundancy based techniques; TREC QA evaluations; Information systems
Interest-based personalized search,2007,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33847630874&doi=10.1145%2f1198296.1198301&partnerID=40&md5=35c3e29ec20010ea5d6733a253d1dc21,"Web search engines typically provide search results without considering user interests or context. We propose a personalized search approach that can easily extend a conventional search engine on the client side. Our mapping framework automatically maps a set of known user interests onto a group of categories in the Open Directory Project (ODP) and takes advantage of manually edited data available in ODP for training text classifiers that correspond to, and therefore categorize and personalize search results according to user interests. In two sets of controlled experiments, we compare our personalized categorization system (PCAT) with a list interface system (LIST) that mimics a typical search engine and with a nonpersonalized categorization system (CAT). In both experiments, we analyze system performances on the basis of the type of task and query length. We find that PCAT is preferable to LIST for information gathering types of tasks and for searches with short queries, and PCAT outperforms CAT in both information gathering and finding types of tasks, and for searches associated with free-form queries. From the subjects' answers to a questionnaire, we find that PCAT is perceived as a system that can find relevant Web pages quicker and easier than LIST and CAT. © 2007 ACM.",Information retrieval; Open Directory; Personalized search; User interest; User interface; World Wide Web,Data acquisition; Information retrieval; Search engines; User interfaces; Websites; List interface system (LIST); Open Directory; Personalized search; User interest; Online searching
Creating and exploiting a comparable corpus in cross-language information retrieval,2007,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33847681482&doi=10.1145%2f1198296.1198300&partnerID=40&md5=e69eebab9dd9b5bafc8fa8b360d675ff,"We present a method for creating a comparable text corpus from two document collections in different languages. The collections can be very different in origin. In this study, we build a comparable corpus from articles by a Swedish news agency and a U.S. newspaper. The keys with best resolution power were extracted from the documents of one collection, the source collection, by using the relative average term frequency (RATF) value. The keys were translated into the language of the other collection, the target collection, with a dictionary-based query translation program. The translated queries were run against the target collection and an alignment pair was made if the retrieved documents matched given date and similarity score criteria. The resulting comparable collection was used as a similarity thesaurus to translate queries along with a dictionary-based translator. The combined approaches outperformed translation schemes where dictionary-based translation or corpus translation was used alone. © 2007 ACM.",Comparable corpora; Cross-language information retrieval; Query translation,Glossaries; Text processing; Thesauri; Translation (languages); Comparable corpora; Cross-language information retrieval; Query translation; Relative average term frequency (RATF) value; Information retrieval
Named entity translation matching and learning: With application for mining unseen translations,2007,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33847662524&doi=10.1145%2f1198296.1198298&partnerID=40&md5=de2d092f812bd97ddd6be4c2c8591193,"This article introduces a named entity matching model that makes use of both semantic and phonetic evidence. The matching of semantic and phonetic information is captured by a unified framework via a bipartite graph model. By considering various technical challenges of the problem, including order insensitivity and partial matching, this approach is less rigid than existing approaches and highly robust. One major component is a phonetic matching model which exploits similarity at the phoneme level. Two learning algorithms for learning the similarity information of basic phonemic matching units based on training examples are investigated. By applying the proposed named entity matching model, a mining system is developed for discovering new named entity translations from daily Web news. The system is able to discover new name translations that cannot be found in the existing bilingual dictionary. © 2007 ACM.",Learning phonetic information; Named entity translation; Text mining,Data mining; Glossaries; Learning algorithms; Semantics; Speech analysis; Web services; Bipartite graph model; Entity matching model; Order insensitivity; Translation (languages)
An empirical investigation of user term feedback in text-based targeted image search,2007,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33847676863&doi=10.1145%2f1198296.1198299&partnerID=40&md5=d362f913cfa593dabfb2552e1f8d34b5,"Text queries are natural and intuitive for users to describe their information needs. However, text-based image retrieval faces many challenges. Traditional text retrieval techniques on image descriptions have not been very successful. This is mainly due to the inconsistent textual descriptions and the discrepancies between user queries and terms in the descriptions. To investigate strategies to alleviate this vocabulary problem, this article examines the role of user term feedback in targeted image search that is based on text-based image retrieval. Term feedback refers to the feedback from a user on specific terms regarding their relevance to a target image. Previous studies have indicated the effectiveness of term feedback in interactive text retrieval. However, in our experiments on text-based image retrieval, the term feedback has not been shown to be effective. Our results indicate that, although term feedback has a positive effect by allowing users to identify more relevant terms, it also has a strong negative effect by providing more opportunities for users to specify irrelevant terms. To understand these different effects and their implications, this article further analyzes important factors that contribute to the utility of term feedback and discusses the outlook of term feedback in interactive text-based image retrieval. © 2007 ACM.",Text-based interactive image retrieval; User term feedback,Feedback; Image retrieval; Text processing; Vocabulary control; Target image; Textual descriptions; User term feedback; Vocabulary problem; Image processing
Precision recall with user modeling (PRUM): Application to structured information retrieval,2007,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33847638550&doi=10.1145%2f1198296.1198297&partnerID=40&md5=5a56432aaaea5c035c635921de871a41,"Standard Information Retrieval (IR) metrics are not well suited for new paradigms like XML or Web IR in which retrievable information units are document elements and/or sets of related documents. Part of the problem stems from the classical hypotheses on the user models: They do not take into account the structural or logical context of document elements or the possibility of navigation between units. This article proposes an explicit and formal user model that encompasses a large variety of user behaviors. Based on this model, we extend the probabilistic precision-recall metric to deal with the new IR paradigms. © 2007 ACM.",Evaluation; Information retrieval; Measure; Precision-recall; Web; XML,Probabilistic logics; User interfaces; World Wide Web; XML; IR paradigms; Precision-recall metric; Standard Information Retrieval (IR) metrics; Web IR; Information retrieval
Dynamic element retrieval in a structured environment,2006,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33845529170&doi=10.1145%2f1185877.1185880&partnerID=40&md5=715e59b22b040342e3edac54a0ead33c,"This research examines the feasibility of dynamic element retrieval in a structured environment. Structured documents and queries are represented in extended vector form, based on a modification of the basic vector space model suggested by Fox [1983]. A method for the dynamic retrieval of XML elements, which requires only a single indexing of the documents at the level of the basic indexing node, is presented. This method, which we refer to as flexible retrieval, produces a rank ordered list of retrieved elements that is equivalent to the result produced by the same retrieval against an all-element index of the collection. Flexible retrieval obviates the need for storing either an all-element index or multiple indices of the collection. © 2006 ACM.",Dynamic element retrieval; Flexible retrieval; Structured retrieval; Vector space model; XML,Information management; Knowledge representation; Mathematical models; Query languages; Vectors; XML; Documents; Dynamic element retrieval; Vector space models; Information retrieval
Preparing heterogeneous XML for full-text search,2006,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33845515739&doi=10.1145%2f1185877.1185881&partnerID=40&md5=ec9024e4bd1ebd1a231875c441ddcf4b,"XML retrieval is facing new challenges when applied to heterogeneous XML documents, where next to nothing about the document structure can be taken for granted. We have developed solutions where some of the heterogeneity issues are addressed. Our fragment selection algorithm selectively divides a heterogeneous document collection into equi-sized fragments with full-text content. If the content is considered too data-oriented, it is not accepted. The algorithm needs no information about element names. In addition, three techniques for fragment expansion are presented, all of which yield a 13 - 17% average improvement in average precision. These techniques and algorithms are among the first steps in developing document-type-independent indexing methods for the full text in heterogeneous XML collections. © 2006 ACM.",Heterogeneous documents; Indexing; XML retrieval,Algorithms; Indexing (of information); Information retrieval; Online searching; Text processing; Heterogeneous documents; Text content; XML retrieval; XML
Articulating information needs in XML query languages,2006,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33845514928&doi=10.1145%2f1185877.1185879&partnerID=40&md5=405697c22ba7d3b097af5aeea243ae1b,"Document-centric XML is a mixture of text and structure. With the increased availability of document-centric XML documents comes a need for query facilities in which both structural constraints and constraints on the content of the documents can be expressed. How does the expressiveness of languages for querying XML documents help users to express their information needs? We address this question from both an experimental and a theoretical point of view. Our experimental analysis compares a structure-ignorant with a structure-aware retrieval approach using the test suite of the INEX XML Retrieval Evaluation Initiative. Theoretically, we create two mathematical models of users' knowledge of a set of documents and define query languages which exactly fit these models. One of these languages corresponds to an XML version of fielded search, the other to the INEX query language.Our main experimental findings are: First, while structure is used in varying degrees of complexity, two-thirds of the queries can be expressed in a fielded-search-like format which does not use the hierarchical structure of the documents. Second, three-quarters of the queries use constraints on the context of the elements to be returned; these contextual constraints cannot be captured by ordinary keyword queries. Third, structure is used as a search hint, and not as a strict requirement, when judged against the underlying information need. Fourth, the use of structure in queries functions as a precision enhancing device. © 2006 ACM.",Full-text XML querying; XML retrieval; XPath,Computational complexity; Information retrieval; Knowledge representation; Mathematical models; Online searching; Query languages; Documents; XML retrieval; XML
A system for the static analysis of XPath,2006,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33845512983&doi=10.1145%2f1185877.1185882&partnerID=40&md5=6a8daa171bdae1972b30a05b1976147a,"XPath is the standard language for navigating XML documents and returning a set of matching nodes. We present a sound and complete decision procedure for containment of XPath queries, as well as other related XPath decision problems such as satisfiability, equivalence, overlap, and coverage. The considered XPath fragment covers most of the language features used in practice. Specifically, we propose a unifying logic for XML, namely, the alternation-free modal μ-calculus with converse. We show how to translate major XML concepts such as XPath and regular XML types (including DTDs) into this logic. Based on these embeddings, we show how XPath decision problems, in the presence or absence of XML types, can be solved using a decision procedure for μ-calculus satisfiability. We provide a complexity analysis of our system together with practical experiments to illustrate the efficiency of the approach for realistic scenarios. © 2006 ACM.",Containment; Equivalence; Logic; Query; XML; XPath,Computational complexity; Computational methods; Decision theory; Differentiation (calculus); Formal logic; Query languages; XML; Decision problems; Documents; Nodes; Computer programming languages
Introduction to the special issue on XML retrieval,2006,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33845540680&doi=10.1145%2f1185877.1185878&partnerID=40&md5=b16c39bbeba173f4912ef65db861361e,[No abstract available],,
eXtended cumulated gain measures for the evaluation of content-oriented XML retrieval,2006,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33845524882&doi=10.1145%2f1185877.1185883&partnerID=40&md5=c95bf209db1ba705427eaf09810be3f0,"We propose and evaluate a family of measures, the eXtended Cumulated Gain (XCG) measures, for the evaluation of content-oriented XML retrieval approaches. Our aim is to provide an evaluation framework that allows the consideration of dependency among XML document components. In particular, two aspects of dependency are considered: (1) near-misses, which are document components that are structurally related to relevant components, such as a neighboring paragraph or container section, and (2) overlap, which regards the situation wherein the same text fragment is referenced multiple times, for example, when a paragraph and its container section are both retrieved. A further consideration is that the measures should be flexible enough so that different models of user behavior may be instantiated within. Both system- and user-oriented aspects are investigated and both recall and precision-like qualities are measured. We evaluate the reliability of the proposed measures based on the INEX 2004 test collection. For example, the effects of assessment variation and topic set size on evaluation stability are investigated, and the upper and lower bounds of expected error rates are established. The evaluation demonstrates that the XCG measures are stable and reliable, and in particular, that the novel measures of effort-precision and gain-recall (ep/gr) show comparable behavior to established IR measures like precision and recall. © 2006 ACM.",Cumulated gain; Dependency; Evaluation; INEX; Metrics; Near-miss; Overlap; XML retrieval,Bit error rate; Computational methods; Data handling; Mathematical models; Text processing; XML; Documents; Paragraphs; XML retrieval; Content based retrieval
"A large scale, corpus-based approach for automatically disambiguating biomedical abbreviations",2006,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33749588452&doi=10.1145%2f1165774.1165778&partnerID=40&md5=3ff12fd8b0fa56ef6e72ae94a2066f34,"Abbreviations and acronyms are widely used in the biomedical literature and many of them represent important biomedical concepts. Because many abbreviations are ambiguous (e.g., CAT denotes both chloramphenicol acetyl transferase and computed axial tomography, depending on the context), recognizing the full form associated with each abbreviation is in most cases equivalent to identifying the meaning of the abbreviation. This, in turn, allows us to perform more accurate natural language processing, information extraction, and retrieval. In this study, we have developed supervised approaches to identifying the full forms of ambiguous abbreviations within the context they appear. We first automatically assigned multiple possible full forms for each abbreviation; we then treated the in-context full-form prediction for each specific abbreviation occurrence as a case of word-sense disambiguation. We generated automatically a dictionary of all possible full forms for each abbreviation. We applied supervised machine-learning algorithms for disambiguation. Because some of the links between abbreviations and their corresponding full forms are explicitly given in the text and can be recovered automatically, we can use these explicit links to automatically provide training data for disambiguating the abbreviations that are not linked to a full form within a text. We evaluated our methods on over 150 thousand abstracts and obtain for coverage and precision results of 82% and 92%, respectively, when performed as tenfold cross-validation, and 79% and 80%, respectively, when evaluated against an external set of abstracts in which the abbreviations are not defined. © 2006 ACM.",Data mining; Information retrieval; Machine learning; Word-sense disambiguation,Abstracting; Data mining; Information retrieval; Learning systems; Medical computing; Natural language processing systems; Text processing; Acetyl transferase; Biomedical abbreviations; Information extraction; Word sense disambiguation; Character recognition
Query enrichment for web-query classification,2006,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33749595945&doi=10.1145%2f1165774.1165776&partnerID=40&md5=de1b3f0ed967075b887e65f4adccd23c,"Web-search queries are typically short and ambiguous. To classify these queries into certain target categories is a difficult but important problem. In this article, we present a new technique called query enrichment, which takes a short query and maps it to intermediate objects. Based on the collected intermediate objects, the query is then mapped to target categories. To build the necessary mapping functions, we use an ensemble of search engines to produce an enrichment of the queries. Our technique was applied to the ACM Knowledge Discovery and Data Mining competition (ACM KDDCUP) in 2005, where we won the championship on all three evaluation metrics (precision, F1 measure, which combines precision and recall, and creativity, which is judged by the organizers) among a total of 33 teams worldwide. In this article, we show that, despite the difficulty of an abundance of ambiguous queries and lack of training data, our query-enrichment technique can solve the problem satisfactorily through a two-phase classification framework. We present a detailed description of our algorithm and experimental evaluation. Our best result for F1 and precision is 42.4% and 44.4%, respectively, which is 9.6% and 24.3% higher than those from the runner-ups, respectively. © 2006 ACM.",Ensemble learning; KDDCUP2005; Query classification; Query enrichment; Synonym-based classifier,Conformal mapping; Data mining; Object recognition; Search engines; World Wide Web; Ensemble learning; KDDCUP2005; Query classification; Query enrichment; Synonym based classifier; Query languages
CLAIRE: A modular support vector image indexing and classification system,2006,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33749641638&doi=10.1145%2f1165774.1165777&partnerID=40&md5=0c816a2459b27ff2f4d201c857880656,"Many users of image retrieval systems would prefer to express initial queries using keywords. However, manual keyword indexing is very time-consuming. Therefore, a content-based image retrieval system which can automatically assign keywords to images would be very attractive. Unfortunately, it has proved very challenging to build such systems, except where either the image domain is restricted or the keywords relate only to low-level concepts such as color. This article presents a novel image indexing and classification system, called CLAIRE (CLAssifying Images for REtrieval), composed of one image processing module and three modules of support vector machines for color, texture, and high-level concept classification for keyword assignment. The experimental prototype system described here assigns up to five keywords selected from a controlled vocabulary of 60 terms to each image. The system is trained offline by 1639 examples from the Corel stock photo library. For evaluation, five judges reviewed a sample of 800 unknown images to identify which automatically assigned keywords were actually relevant to the image. The system proved to have an 80% probability to assign at least one relevant keyword to an image. © 2006 ACM.",Content-based image retrieval; Image classification; Image indexing; Multiple classifier systems; Support vector machines,Automatic indexing; Color image processing; Content based retrieval; Probability; Query languages; Thesauri; Image classification; Image indexing; Multiple classifier systems; Support vector machines (SVM); Pattern recognition
Extraction of coherent relevant passages using hidden markov models,2006,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33749586992&doi=10.1145%2f1165774.1165775&partnerID=40&md5=c5506ee6bf397f0ba259eb03e93fd39c,"In information retrieval, retrieving relevant passages, as opposed to whole documents, not only directly benefits the end user by filtering out the irrelevant information within a long relevant document, but also improves retrieval accuracy in general. A critical problem in passage retrieval is to extract coherent relevant passages accurately from a document, which we refer to as passage extraction. While much work has been done on passage retrieval, the passage extraction problem has not been seriously studied. Most existing work tends to rely on presegmenting documents into fixed-length passages which are unlikely optimal because the length of a relevant passage is presumably highly sensitive to both the query and document. In this article, we present a new method for accurately detecting coherent relevant passages of variable lengths using hidden Markov models (HMMs). The HMM-based method naturally captures the topical boundaries between passages relevant and nonrelevant to the query. Pseudo-feedback mechanisms can be naturally incorporated into such an HMM-based framework to improve parameter estimation. We show that with appropriate parameter estimation, the HMM method outperforms a number of strong baseline methods on two datasets. We further show how the HMM method can be applied on top of any basic passage extraction method to improve passage boundaries. © 2006 ACM.",Additional Key Words and Phrases: Hidden Markov models; Passage retrieval,Feature extraction; Markov processes; Parameter estimation; Query languages; Signal filtering and prediction; Additional Key Words and Phrases; Hidden Markov models (HMM); Passage extraction; Passage retrieval; Information retrieval
Enhancing relevance feedback in image retrieval using unlabeled data,2006,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33746834100&doi=10.1145%2f1148020.1148023&partnerID=40&md5=0c616abc7c5cc6fc00915742100e6a04,"Relevance feedback is an effective scheme bridging the gap between high-level semantics and low-level features in content-based image retrieval (CBIR). In contrast to previous methods which rely on labeled images provided by the user, this article attempts to enhance the performance of relevance feedback by exploiting unlabeled images existing in the database. Concretely, this article integrates the merits of semisupervised learning and active learning into the relevance feedback process. In detail, in each round of relevance feedback two simple learners are trained from the labeled data, that is, images from user query and user feedback. Each learner then labels some unlabeled images in the database for the other learner. After retraining with the additional labeled data, the learners reclassify the images in the database and then their classifications are merged. Images judged to be positive with high confidence are returned as the retrieval result, while those judged with low confidence are put into the pool which is used in the next round of relevance feedback. Experiments show that using semisupervised learning and active learning simultaneously in CBIR is beneficial, and the proposed method achieves better performance than some existing methods. © 2006 ACM.",Active learning; Content-based image retrieval machine learning; Learning with unlabeled data; Relevance feedback; Semisupervised learning,Classification (of information); Database systems; Learning systems; Semantics; User interfaces; Active learning; Content-based image retrieval machine learning; Learning with unlabeled data; Relevance feedback; Semisupervised learning; Image retrieval
User evaluation of Físchlár-News: An automatic broadcast news delivery system,2006,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33746814898&doi=10.1145%2f1148020.1148021&partnerID=40&md5=e4ad597bc5ec9cbf5723c1a28bfb25d9,"Technological developments in content-based analysis of digital video information are undergoing much progress, with ideas for fully automatic systems now being proposed and demonstrated. Yet because we do not yet have robust operational video retrieval systems that can be deployed and used, the usual HCI practise of conducting a usage study and an informed iterative system design is thus not possible. Físchlár-News is one of the first automatic, content-based broadcast news analysis and archival systems that process broadcast news video so that users can search, browse, and play it in an easy-to-use manner with a conventional web browser. The system incorporates a number of state-of-the-art research components, some of which are not yet considered mature technology, yet it has been built to be robust enough to be deployed to users who are interested in access to daily news throughout a university campus. In this article we report and discuss a user-evaluation study conducted with 16 users, each of whom utilized the system freely for a one month period. Results from a detailed qualitative analysis are presented, looking at collected questionnaires, incident diaries, and interaction-log data. The findings suggest that our users employed the system in conjunction with their other news update methods, such as watching TV news at home and browsing online news websites at their workplace, their major concerns being up-to-dateness and coverage of the news content. They tried to accommodate the system to fit their established web browsing habits, and they found local news content and the ability to play selfcontained news stories on their desktop as major values of the system. Our study also resulted in a detailed wishlist of new features which will help in the further development of both our and others' systems. © 2006 ACM.",Content-based video retrieval; Usage analysis; User-evaluation,Computer aided design; Digital television; Human computer interaction; Information analysis; Information retrieval systems; Online systems; Personal computers; User interfaces; Web browsers; Content-based video retrieval; Usage analysis; User-evaluation; Video retrieval systems; Broadcasting
iVIBRATE: Interactive visualization-based framework for clustering large datasets,2006,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745475658&doi=10.1145%2f1148020.1148024&partnerID=40&md5=0ef3fb78da8ca7217b32d4ae674e0893,"With continued advances in communication network technology and sensing technology, there is astounding growth in the amount of data produced and made available through Cyberspace. Efficient and high-quality clustering of large datasets continues to be one of the most important problems in large-scale data analysis. A commonly used methodology for cluster analysis on large datasets is the three-phase framework of sampling/summarization, iterative cluster analysis, and disk-labeling. There are three known problems with this framework which demand effective solutions. The first problem is how to effectively define and validate irregularly shaped clusters, especially in large datasets. Automated algorithms and statistical methods are typically not effective in handling these particular clusters. The second problem is how to effectively label the entire data on disk (disk-labeling) without introducing additional errors, including the solutions for dealing with outliers, irregular clusters, and cluster boundary extension. The third obstacle is the lack of research about issues related to effectively integrating the three phases. In this article, we describe iVIBRATE - an interactive visualization-based three-phase framework for clustering large datasets. The two main components of iVIBRATE are its VISTA visual cluster-rendering subsystem which invites human interplay into the large-scale iterative clustering process through interactive visualization, and its adaptive ClusterMap labeling subsystem which offers visualization-guided disk-labeling solutions that are effective in dealing with outliers, irregular clusters, and cluster boundary extension. Another important contribution of iVIBRATE development is the identification of the special issues presented in integrating the two components and the sampling approach into a coherent framework, as well as the solutions for improving the reliability of the framework and for minimizing the amount of errors generated within the cluster analysis process. We study the effectiveness of the iVIBRATE framework through a walkthrough example dataset of a million records and we experimentally evaluate the iVIBRATE approach using both real-life and synthetic datasets. Our results show that iVIBRATE can efficiently involve the user in the clustering process and generate high-quality clustering results for large datasets. © 2006 ACM.",Algorithms; Design; Human Factors; Reliability,Algorithms; Computer aided design; Information analysis; Large scale systems; Numerical methods; Problem solving; Reliability; Clustering; Human Factor; Interactive visualization; Sensing technology; Telecommunication networks
A Maximal Figure-of-Merit (MFoM)-learning approach to robust classifier design for text categorization,2006,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33746802824&doi=10.1145%2f1148020.1148022&partnerID=40&md5=30769c64f384d13a0c22bac007f21540,"We propose a maximal figure-of-merit (MFoM)-learning approach for robust classifier design, which directly optimizes performance metrics of interest for different target classifiers. The proposed approach, embedding the decision functions of classifiers and performance metrics into an overall training objective, learns the parameters of classifiers in a decision-feedback manner to effectively take into account both positive and negative training samples, thereby reducing the required size of positive training data. It has three desirable properties: (a) it is a performance metric, oriented learning; (b) the optimized metric is consistent in both training and evaluation sets; and (c) it is more robust and less sensitive to data variation, and can handle insufficient training data scenarios. We evaluate it on a text categorization task using the Reuters-21578 dataset. Training an F 1-based binary tree classifier using MFoM, we observed significantly improved performance and enhanced robustness compared to the baseline and SVM, especially for categories with insufficient training samples. The generality for designing other metrics-based classifiers is also demonstrated by comparing precision, recall, and F 1-based classifiers. The results clearly show consistency of performance between the training and evaluation stages for each classifier, and MFoM optimizes the chosen metric. © 2006 ACM.",Decision tree; Generalized probabilistic descent method; Information retrieval; Latent semantic indexing; Maximal figure-of-merit; Text categorization,Data structures; Decision support systems; Information retrieval; Learning systems; Probabilistic logics; Semantics; Decision tree; Generalized probabilistic descent method; Latent semantic indexing; Maximal figure-of-merit; Text categorization; Classifiers
Boosting the performance of web search engines: Caching and prefetching query results by exploiting historical usage data,2006,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745196401&doi=10.1145%2f1125857.1125859&partnerID=40&md5=fd97069343befa6e595e59d4bfbe7186,"This article discusses efficiency and effectiveness issues in caching the results of queries submitted to a Web search engine (WSE). We propose SDC (Static Dynamic Cache), a new caching strategy aimed to efficiently exploit the temporal and spatial locality present in the stream of processed queries. SDC extracts from historical usage data the results of the most frequently submitted queries and stores them in a static, read-only portion of the cache. The remaining entries of the cache are dynamically managed according to a given replacement policy and are used for those queries that cannot be satisfied by the static portion. Moreover, we improve the hit ratio of SDC by using an adaptive prefetching strategy, which anticipates future requests by introducing a limited overhead over the back-end WSE. We experimentally demonstrate the superiority of SDC over purely static and dynamic policies by measuring the hit ratio achieved on three large query logs by varying the cache parameters and the replacement policy used for managing the dynamic part of the cache. Finally, we deploy and measure the throughput achieved by a concurrent version of our caching system. Our tests show how the SDC cache can be efficiently exploited by many threads that concurrently serve the queries of different users. © 2006 ACM.",Caching; Multithreading; Web search engines,Cache memory; Data reduction; Feature extraction; Query languages; Adaptive prefetching strategy; Caching; Multithreading; Query logs; Search engines
Summary in context: Searching versus browsing,2006,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745194096&doi=10.1145%2f1125857.1125861&partnerID=40&md5=5e38902b22a79903c5118b695d50a56b,"The use of text summaries in information-seeking research has focused on query-based summaries. Extracting content that resembles the query alone, however, ignores the greater context of the document. Such context may be central to the purpose and meaning of the document. We developed a generic, a query-based, and a hybrid summarizer, each with differing amounts of document context. The generic summarizer used a blend of discourse information and information obtained through traditional surface-level analysis. The query-based summarizer used only query-term information, and the hybrid summarizer used some discourse information along with query-term information. The validity of the generic summarizer was shown through an intrinsic evaluation using a well-established corpus of human-generated summaries. All three summarizers were then compared in an information-seeking experiment involving 297 subjects. Results from the information-seeking experiment showed that the generic summaries outperformed all others in the browse tasks, while the query-based and hybrid summaries outperformed the generic summary in the search tasks. Thus, the document context of generic summaries helped users browse, while such context was not helpful in search tasks. Such results are interesting given that generic summaries have not been studied in search tasks and the that majority of Internet search engines rely solely on query-based summaries. © 2006 ACM.",Browse; Generic summaries; Indicative summaries; Information seeking; Natural language processing; Search; Summarization; Text processing,Information analysis; Internet; Query languages; Text processing; Web browsers; Browse; Generic summaries; Indicative summaries; Information seeking; Natural language processing; Search; Summarization; Search engines
Detection of video sequences using compact signatures,2006,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745191834&doi=10.1145%2f1125857.1125858&partnerID=40&md5=946019e7d4155dc1def4683ab288cbac,"Digital representations are widely used for audiovisual content, enabling the creation of large on-line repositories of video, allowing access such as video on demand. However, the ease of copying and distribution of digital video makes piracy a growing concern for content owners. We investigate methods for identifying coderivative video content - that is, video clips that are derived from the same original source. By using dynamic programming to identify regions of similarity in video signatures, it is possible to efficiently and accurately identify coderivatives, even when these regions constitute only a small section of the clip being searched. We propose four new methods for producing compact video signatures, based on the way in which the video changes over time. The intuition is that such properties are likely to be preserved even when the video is badly degraded. We demonstrate that these signatures are insensitive to dramatic changes in video bitrate and resolution, two parameters that are often altered when reencoding. In the presence of mild degradations, our methods can accurately identify copies of clips that are as short as 5 s within a dataset 140 min long. These methods are much faster than previously proposed techniques; using a more compact signature, this query can be completed in a few milliseconds. © 2006 ACM.",Dynamic programming; Local alignment; Video similarity detection,Bit error rate; Dynamic programming; Electronic document identification systems; Image quality; Online systems; Audiovisual content; Local alignment; Video bitrate; Video similarity detection; Video signal processing
A space-partitioning-based indexing method for multidimensional non-ordered discrete data spaces,2006,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745200484&doi=10.1145%2f1125857.1125860&partnerID=40&md5=f3bfedf5a362eda39e5aa2e36601ec60,"There is an increasing demand for similarity searches in a multidimensional non-ordered discrete data space (NDDS) from application areas such as bioinformatics and data mining. The non-ordered and discrete nature of an NDDS raises new challenges for developing efficient indexing methods for similarity searches. In this article, we propose a new indexing technique, called the NSP-tree, to support efficient similarity searches in an NDDS. As we know, overlap causes a performance degradation for indexing methods (e.g., the R-tree) for a continuous data space. In an NDDS, this problem is even worse due to the limited number of elements available on each dimension of an NDDS. The key idea of the NSP-tree is to use a novel discrete space-partitioning (SP) scheme to ensure no overlap at each level in the tree. A number of heuristics and strategies are incorporated into the tree construction algorithms to deal with the challenges for developing an SP-based index tree for an NDDS. Our experiments demonstrate that the NSP-tree is quite promising in supporting efficient similarity searches in NDDSs. We have compared the NSP-tree with the ND-tree, a data-partitioning-based indexing technique for NDDSs that was proposed recently, and the linear scan using different NDDSs. It was found that the search performance of the NSP-tree was better than those of both methods. © 2006 ACM.",Hamming distance; Multidimensional index tree; Non-ordered discrete data spaces; Similarity search,Algorithms; Data mining; Database systems; Heuristic methods; Search engines; Trees (mathematics); Hamming distance; Multidimensional index tree; Non-ordered discrete data spaces; Similarity search; Indexing (of information)
Learning to crawl: Comparing classification schemes,2005,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-31344472207&doi=10.1145%2f1095872.1095875&partnerID=40&md5=45ebc66e31e3a0c41f697794d2a42189,"Topical crawling is a young and creative area of research that holds the promise of benefiting from several sophisticated data mining techniques. The use of classification algorithms to guide topical crawlers has been sporadically suggested in the literature. No systematic study, however, has been done on their relative merits. Using the lessons learned from our previous crawler evaluation studies, we experiment with multiple versions of different classification schemes. The crawling process is modeled as a parallel best-first search over a graph defined by the Web. The classifiers provide heuristics to the crawler thus biasing it towards certain portions of the Web graph. Our results show that Naive Bayes is a weak choice for guiding a topical crawler when compared with Support Vector Machine or Neural Network. Further, the weak performance of Naive Bayes can be partly explained by extreme skewness of posterior probabilities generated by it. We also observe that despite similar performances, different topical crawlers cover subspaces on the Web with low overlap. © 2005 ACM.",Classifiers; Focused crawlers; Machine learning; Topical crawlers,Algorithms; Data mining; Graph theory; Learning systems; Neural networks; World Wide Web; Focused crawlers; Skewness; Support Vector Machines; Topical crawlers; Classification (of information)
Evolution of web site design patterns,2005,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745195974&doi=10.1145%2f1095872.1095876&partnerID=40&md5=0691d8068ba8d0523b8d432d1310e0b6,"The Web enables broad dissemination of information and services; however, the ways in which sites are designed can either facilitate or impede users' benefit from these resources. We present a longitudinal study of web site design from 2000 to 2003. We analyze over 150 quantitative measures of interface aspects (e.g., amount of text on pages, numbers and types of links, consistency, accessibility, etc.) for 22,000 pages and over 1,500 sites that received ratings from Internet professionals. We examine characteristics of highly rated sites and provide three perspectives on the evolution of web site design patterns: (1) descriptions of design patterns during each time period; (2) changes in design patterns across the three time periods; and (3) comparisons of design patterns to those that are recommended in the relevant literature (i.e., texts by recognized experts and user studies). We illustrate how design practices conform to or deviate from recommended practices and the consequent implications. We show that the most glaring deficiency of web sites, even for sites that are highly rated, is their inadequate accessibility, in particular for browser scripts, tables, and form elements. © 2005 ACM.",Accessibility; Automated usability evaluation; Design guidelines; Empirical studies; Usability; Web site design; World Wide Web,Information dissemination; Internet; User interfaces; Web browsers; Accessibility; Automated usability evaluation; Design guidelines; Empirical studies; Usability; Web site design; World Wide Web
Taxonomy generation for text segments: A practical web-based approach,2005,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745195646&doi=10.1145%2f1095872.1095873&partnerID=40&md5=479c1e13aacd286cc2da010d6dfadc8c,"It is crucial in many information systems to organize short text segments, such as keywords in documents and queries from users, into a well-formed taxonomy. In this article, we address the problem of taxonomy generation for diverse text segments with a general and practical approach that uses the Web as an additional knowledge source. Unlike long documents, short text segments typically do not contain enough information to extract reliable features. This work investigates the possibilities of using highly ranked search-result snippets to enrich the representation of text segments. A hierarchical clustering algorithm is then designed for creating the hierarchical topic structure of text segments. Text segments with close concepts can be grouped together in a cluster, and relevant clusters linked at the same or near levels. Different from traditional clustering algorithms, which tend to produce cluster hierarchies with a very unnatural shape, the algorithm tries to produce a more natural and comprehensive tree hierarchy. Extensive experiments were conducted on different domains of text segments, including subject terms, people names, paper titles, and natural language questions. The obtained experimental results have shown the potential of the proposed approach, which provides a basis for the in-depth analysis of text segments on a larger scale and is believed able to benefit many information systems. © 2005 ACM.",Hierarchical clustering; Partitioning; Search-result snippet; Taxonomy generation; Text data mining; Text segment,Data mining; Hierarchical systems; Information science; Natural language processing systems; Query languages; World Wide Web; Hierarchical clustering; Partitioning; Search-result snippet; Taxonomy generation; Text data mining; Text segment; Text processing
Set-based vector model: An efficient approach for correlation-based ranking,2005,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745193242&doi=10.1145%2f1095872.1095874&partnerID=40&md5=5a7cd21dad6f211c6f4710fa39ebc290,"This work presents a new approach for ranking documents in the vector space model. The novelty lies in two fronts. First, patterns of term co-occurrence are taken into account and are processed efficiently. Second, term weights are generated using a data mining technique called association rules. This leads to a new ranking mechanism called the set-based vector model. The components of our model are no longer index terms but index termsets, where a termset is a set of index terms. Termsets capture the intuition that semantically related terms appear close to each other in a document. They can be efficiently obtained by limiting the computation to small passages of text. Once termsets have been computed, the ranking is calculated as a function of the termset frequency in the document and its scarcity in the document collection. Experimental results show that the set-based vector model improves average precision for all collections and query types evaluated, while keeping computational costs small. For the 2-gigabyte TREC-8 collection, the setbased vector model leads to a gain in average precision figures of 14.7% and 16.4% for disjunctive and conjunctive queries, respectively, with respect to the standard vector space model. These gains increase to 24.9% and 30.0%, respectively, when proximity information is taken into account. Query processing times are larger but, on average, still comparable to those obtained with the standard vector model (increases in processing time varied from 30% to 300%). Our results suggest that the set-based vector model provides a correlation-based ranking formula that is effective with general collections and computationally practical. © 2005 ACM.",Association rule mining; Correlation-based ranking; Data mining; Information retrieval models; Weighting index term co-occurrences,Computation theory; Data mining; Mathematical models; Set theory; Vectors; Association rule mining; Correlation-based ranking; Information retrieval models; Weighting index term co-occurrences; Correlation methods
A novel document retrieval method using the discrete wavelet transform,2005,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745186002&doi=10.1145%2f1080343.1080345&partnerID=40&md5=2d07aeedc8a8dc0b435c857d80bb6816,"Current information retrieval methods either ignore the term positions or deal with exact term positions; the former can be seen as coarse document resolution, the latter as fine document resolution. We propose a new spectral-based information retrieval method that is able to utilize many different levels of document resolution by examining the term patterns that occur in the documents. To do this, we take advantage of the multiresolution analysis properties of the wavelet transform. We show that we are able to achieve higher precision when compared to vector space and proximity retrieval methods, while producing fast query times and using a compact index. © 2005 ACM.",Daubechies; Document retrieval; Haar; Multiresolution analysis; Proximity search; Vector space methods; Wavelet transform,Data processing; Program documentation; Query languages; Spectrum analysis; Wavelet transforms; Daubechies; Document retrieval; Haar; Multiresolutional analysis; Proximity search; Vector space research; Information retrieval systems
Trustworthy 100-year digital objects: Durable encoding for when it's too late to ask,2005,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745210777&doi=10.1145%2f1080343.1080346&partnerID=40&md5=2e7938d978ab4a13fab28b82619243d3,"How can an author store digital information so that it will be reliably intelligible, even years later when he or she is no longer available to answer questions? Methods that might work are not good enough; what is preserved today should be reliably intelligible whenever someone wants it. Prior proposals fail because they generally confound saved data with irrelevant details of today's information technology - details that are difficult to define, extract, and save completely and accurately. We use a virtual machine to represent and eventually to render any data whatsoever. We focus on a case of intermediate difficulty - an executable procedure - and identify a variant for every other data type. This solution might be more elaborate than needed to render some text, image, audio, or video data. Simple data can be preserved as representations using well-known standards. We sketch practical methods for files ranging from simple structures to those containing computer programs, treating simple cases here and deferring complex cases for future work. Enough of the complete solution is known to enable practical aggressive preservation programs today. © 2005 ACM.",Encoding; Long-term digital preservation,Data structures; Database systems; Electronic communities; Encoding (symbols); Information technology; Storage allocation (computer); Data type; Encoding; Files; Long-term digital preservation; Digital storage
A market-based approach to recommender systems,2005,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-30744468979&doi=10.1145%2f1080343.1080344&partnerID=40&md5=cf2d75999391325df2c352c552127528,"Recommender systems have been widely advocated as a way of coping with the problem of information overload for knowledge workers. Given this, multiple recommendation methods have been developed. However, it has been shown that no one technique is best for all users in all situations. Thus we believe that effective recommender systems should incorporate a wide variety of such techniques and that some form of overarching framework should be put in place to coordinate the various recommendations so that only the best of them (from whatever source) are presented to the user. To this end, we show that a marketplace, in which the various recommendation methods compete to offer their recommendations to the user, can be used in this role. Specifically, this article presents the principled design of such a marketplace (including the auction protocol, the reward mechanism, and the bidding strategies of the individual recommendation agents) and evaluates the market's capability to effectively coordinate multiple methods. Through analysis and simulation, we show that our market is capable of shortlisting recommendations in decreasing order of user perceived quality and of correlating the individual agent's internal quality rating to the user's perceived quality. © 2005 ACM.",Auctions; Marketplace; Recommender systems,Computer simulation; Correlation methods; Data processing; Marketing; Quality assurance; User interfaces; Auctions; Marketplace; Quality; Recommender systems; Information technology
Evaluating implicit feedback models using searcher simulations,2005,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33646034359&doi=10.1145%2f1080343.1080347&partnerID=40&md5=bd4ffeb04f172f503fe655a3c52bf02a,"In this article we describe an evaluation of relevance feedback (RF) algorithms using searcher simulations. Since these algorithms select additional terms for query modification based on inferences made from searcher interaction, not on relevance information searchers explicitly provide (as in traditional RF), we refer to them as implicit feedback models. We introduce six different models that base their decisions on the interactions of searchers and use different approaches to rank query modification terms. The aim of this article is to determine which of these models should be used to assist searchers in the systems we develop. To evaluate these models we used searcher simulations that afforded us more control over the experimental conditions than experiments with human subjects and allowed complex interaction to be modeled without the need for costly human experimentation. The simulation-based evaluation methodology measures how well the models learn the distribution of terms across relevant documents (i.e., learn what information is relevant) and how well they improve search effectiveness (i.e., create effective search queries). Our findings show that an implicit feedback model based on Jeffrey's rule of conditioning outperformed other models under investigation. © 2005 ACM.",Evaluation; Implicit feedback; Relevance feedback; User simulations,Computer simulation; Decision theory; Evaluation; Feedback; Mathematical models; Query languages; Documents; Implicit feedback; Relevance feedback; User simulations; Information technology
Evaluating implicit measures to improve Web search,2005,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-20344382508&doi=10.1145%2f1059981.1059982&partnerID=40&md5=e850d727cc2e0a1e792ca6812ff8a292,"Of growing interest in the area of improving the search experience is the collection of implicit user behavior measures (implicit measures) as indications of user interest and user satisfaction. Rather than having to submit explicit user feedback, which can be costly in time and resources and alter the pattern of use within the search experience, some research has explored the collection of implicit measures as an efficient and useful alternative to collecting explicit measure of interest from users. This research article describes a recent study with two main objectives. The first was to test whether there is an association between explicit ratings of user satisfaction and implicit measures of user interest. The second was to understand what implicit measures were most strongly associated with user satisfaction. The domain of interest was Web search. We developed an instrumented browser to collect a variety of measures of user activity and also to ask for explicit judgments of the relevance of individual pages visited and entire search sessions. The data was collected in a workplace setting to improve the generalizability of the results. Results were analyzed using traditional methods (e.g., Bayesian modeling and decision trees) as well as a new usage behavior pattern analysis (""gene analysis""). We found that there was an association between implicit measures of user activity and the user's explicit satisfaction ratings. The best models for individual pages combined clickthrough, time spent on the search result page, and how a user exited a result or ended a search session (exit type/end action). Behavioral patterns (through the gene analysis) can also be used to predict user satisfaction for search sessions. © 2005 ACM.",Explicit feedback; Explicit ratings; Implicit measures; Prediction model; Search sessions; User interest; User satisfaction,Data acquisition; Feedback; Mathematical models; Online searching; User interfaces; World Wide Web; Explicit feedback; Explicit ratings; Implicit measures; Prediction model; Search sessions; User interest; User satisfaction; Search engines
CrimeNet explorer: A framework for criminal network knowledge discovery,2005,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-20344383501&doi=10.1145%2f1059981.1059984&partnerID=40&md5=449a64fd173fd540f8599276a79b5d02,"Knowledge about the structure and organization of criminal networks is important for both crime investigation and the development of effective strategies to prevent crimes. However, except for network visualization, criminal network analysis remains primarily a manual process. Existing tools do not provide advanced structural analysis techniques that allow extraction of network knowledge from large volumes of criminal-justice data. To help law enforcement and intelligence agencies discover criminal network knowledge efficiently and effectively, in this research we proposed a framework for automated network analysis and visualization. The framework included four stages: network creation, network partition, structural analysis, and network visualization. Based upon it, we have developed a system called CrimeNet Explorer that incorporates several advanced techniques: a concept space approach, hierarchical clustering, social network analysis methods, and multidimensional scaling. Results from controlled experiments involving student subjects demonstrated that our system could achieve higher clustering recall and precision than did untrained subjects when detecting subgroups from criminal networks. Moreover, subjects identified central members and interaction patterns between groups significantly faster with the help of structural analysis functionality than with only visualization functionality. No significant gain in effectiveness was present, however. Our domain experts also reported that they believed CrimeNet Explorer could be very useful in crime investigation. © 2005 ACM.",Clustering; Complete-link algorithm; Concept space; Crime investigation; Knowledge discovery; Law enforcement; Multidimensional scaling; Precision and recall; Social network analysis; Visualization,Computer networks; Expert systems; Knowledge engineering; Law enforcement; Social sciences; Structural analysis; Visualization; Clustering; Complete-like algorithm; Concept space; Crime investigation; Knowledge discovery; Multidimensional sealing; Precision and recall; Social network analysis; Computer crime
"Ad hoc, self-supervising peer-to-peer search networks",2005,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-20344363888&doi=10.1145%2f1059981.1059983&partnerID=40&md5=ba32ac8e7895fd723e25165e8fa437f0,"Peer-to-peer search networks are a popular and widely deployed means of searching massively distributed digital information repositories. Unfortunately, as such networks grow, peers may become overloaded processing messages from other peers. This article examines how to reduce the load on nodes in P2P networks by allowing them to self-organize into a relatively efficient network, and then self-tune to make the network even more efficient. Two local operations used by a peer are introduced: connect(), in which the peer forms an ad hoc search or index link to another peer, and break(), in which the peer breaks a link that is producing too much load. By replacing fixed rules with dynamic local decision-making, such ""self-supervising"" networks can better adjust to network conditions. Different ways to implement connect() and break() are described, and the network structures that form under different configurations are examined. Simulation results indicate that the ad hoc networks formed using the described techniques are more efficient than popular supernode topologies for several important scenarios. Results for the fault tolerance and search latency of such ad hoc networks are also presented. © 2005 ACM.",Information search and discovery; Peer-to-peer systems,Data processing; Data storage equipment; Decision making; Information retrieval; Online searching; Search engines; Ad hoc networks; Discovery; Information search; Peer-to-peer systems; Computer networks
Historical spatio-temporal aggregation,2005,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-13844299248&doi=10.1145%2f1055709.1055713&partnerID=40&md5=72febc353e48134d4d4c1b127239cdbf,"Spatio-temporal databases store information about the positions of individual objects over time. However, in many applications such as traffic supervision or mobile communication systems, only summarized data, like the number of cars in an area for a specific period, or phone-calls serviced by a cell each day, is required. Although this information can be obtained from operational databases, its computation is expensive, rendering online processing inapplicable. In this paper, we present specialized methods, which integrate spatio-temporal indexing with pre-aggregation. The methods support dynamic spatio-temporal dimensions for the efficient processing of historical aggregate queries without a priori knowledge of grouping hierarchies. The superiority of the proposed techniques over existing methods is demonstrated through a comprehensive probabilistic analysis and an extensive experimental evaluation.",Access methods; Additional Key Words and Phrases: Aggregation; Cost models,Agglomeration; Data acquisition; Data storage equipment; Hierarchical systems; Mathematical models; Mobile computing; Probabilistic logics; Warehouses; Access methods; Cost models; Online processing; Spatio-temporal aggregation; Database systems
An efficient normalized maximum likelihood algorithm for DMA sequence compression,2005,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-13844281512&doi=10.1145%2f1055709.1055711&partnerID=40&md5=e6495ad123e052bdf9b6c6825babd588,"This article presents an efficient algorithm for DNA sequence compression, which achieves the best compression ratios reported over a test set commonly used for evaluating DNA compression programs. The algorithm introduces many refinements to a compression method that combines: (1) encoding by a simple normalized maximum likelihood (NML) model for discrete regression, through reference to preceding approximate matching blocks, (2) encoding by a first order context coding and (3) representing strings in clear, to make efficient use of the redundancy sources in DNA data, under fast execution times. One of the main algorithmic features is the constraint on the matching blocks to include reasonably long contiguous matches, which not only reduces significantly the search time, but also can be used to modify the NML model to exploit the constraint for getting smaller code lengths. The algorithm handles the changing statistics of DNA data in an adaptive way and by predictively encoding the matching pointers it is successful in compressing long approximate matches. Apart from comparison with previous DNA encoding methods, we present compression results for the recently published human genome data.",Approximate sequence matching; DNA compression; Normalized maximum likelihood model,Algorithms; Data acquisition; Genes; Mathematical models; Redundancy; Regression analysis; Approximate sequence matching; DNA compression; Normalized maximum likelihood model; DNA
Introduction to genomic information retrieval,2005,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-13844281514&doi=10.1145%2f1055709.1055710&partnerID=40&md5=30dad8d8bbda692fccfa6ae88e7ef22a,[No abstract available],,
Incorporating contextual information in recommender systems using a multidimensional approach,2005,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-13844254250&doi=10.1145%2f1055709.1055714&partnerID=40&md5=9b08e28f3b425efa6c6740a8b0a80244,"The article presents a multidimensional (MD) approach to recommender systems that can provide recommendations based on additional contextual information besides the typical information on users and items used in most of the current recommender systems. This approach supports multiple dimensions, profiling information, and hierarchical aggregation of recommendations. The article also presents a multidimensional rating estimation method capable of selecting two-dimensional segments of ratings pertinent to the recommendation context and applying standard collaborative filtering or other traditional two-dimensional rating estimation techniques to these segments. A comparison of the multidimensional and two-dimensional rating estimation approaches is made, and the tradeoffs between the two are studied. Moreover, the article introduces a combined rating estimation method, which identifies the situations where the MD approach outperforms the standard two-dimensional approach and uses the MD approach in those situations and the standard two-dimensional approach elsewhere. Finally, the article presents a pilot empirical study of the combined approach, using a multidimensional movie recommender system that was developed for implementing this approach and testing its performance.",Collaborative filtering; Context-aware recommander systems; Multidimensional data models; Multidimensional recommander systems; Personalization; Rating estimation; Recommender systems,Data acquisition; Data structures; Decision making; Estimation; Filtration; Mathematical models; Real time systems; Websites; Collaborative filtering; Context-aware recommender systems; Multidimensional data models; Multidimensional recommender systems; Personalization; Rating estimation; Recommender systems; Information science
A methodology for analyzing SAGE libraries for cancer profiling,2005,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-13844271219&doi=10.1145%2f1055709.1055712&partnerID=40&md5=6457a330d72b438ce74f1f243dc8b312,"Serial Analysis of Gene Expression (SAGE) has proven to be an important alternative to microarray techniques for global profiling of mRNA populations. We have developed preprocessing methodologies to address problems in analyzing SAGE data due to noise caused by sequencing error, normalization methodologies to account for libraries sampled at different depths, and missing tag imputation methodologies to aid in the analysis of poorly sampled SAGE libraries. We have also used subspace selection using the Wilcoxon rank sum test to exclude tags that have similar expression levels regardless of source. Using these methodologies we have clustered, using the OPTICS algorithm, 88 SAGE libraries derived from cancerous and normal tissues as well as cell line material. Our results produced eight dense clusters representing ovarian cancer cell line, brain cancer cell line, brain cancer bulk tissue, prostate tissue, pancreatic cancer, breast cancer cell line, normal brain, and normal breast bulk tissue. The ovarian cancer and brain cancer cell lines clustered closely together, leading to a further investigation on possible associations between these two cancer types. We also investigated the utility of gene expression data in the classification between normal and cancerous tissues. Our results indicate that brain and breast cancer libraries have strong identities allowing robust discrimination from their normal counterparts. However, the SAGE expression data provide poor predictive accuracy in discriminating between prostate and ovarian cancers and their respective normal tissues.",Cancer profiling; Classification; Clustering; Gene expression,Biochemistry; Data acquisition; Proteins; RNA; Tissue; Tumors; Cancer profiling; Classification; Clustering; Gene expression; Genes
Fast phrase querying with combined indexes,2004,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-9144270097&doi=10.1145%2f1028099.1028102&partnerID=40&md5=e7620b9092f6743e263be6df9b000e7c,"Search engines need to evaluate queries extremely fast, a challenging task given the quantities of data being indexed. A significant proportion of the queries posed to search engines involve phrases. In this article we consider how phrase queries can be efficiently supported with low disk overheads. Our previous research has shown that phrase queries can be rapidly evaluated using nextword indexes, but these indexes are twice as large as conventional inverted files. Alternatively, special-purpose phrase indexes can be used, but it is not feasible to index all phrases. We propose combinations of nextword indexes and phrase indexes with inverted files as a solution to this problem. Our experiments show that combined use of a partial nextword, partial phrase, and conventional inverted index allows evaluation of phrase queries in a quarter the time required to evaluate such queries with an inverted file alone; the additional space overhead is only 26% of the size of the inverted file.",Nextword indexes; Phrase queries; Query evaluation; Web search,Data acquisition; Information retrieval; Query languages; Research and development management; Search engines; World Wide Web; Nextword indexes; Phrase queries; Query evaluation; Web search; Indexing (of information)
Qualitative decision making in adaptive presentation of structured information,2004,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-9144248527&doi=10.1145%2f1028099.1028100&partnerID=40&md5=3e1509c66b6b1e046ba323cc39d8965e,"We present a new approach for adaptive presentation of structured information, based on preference-based constrained optimization techniques rooted in qualitative decision-theory. In this approach, document presentation is viewed as a configuration problem whose goal is to determine the optimal presentation of a document, while taking into account the preferences of the content provider, viewer interaction with the browser, and, possibly, some layout constraints. The preferences of the content provider are represented by a CP-net, a graphical, qualitative preference model developed in Boutilier et al. [1999]. The layout constraints are represented as geometric constraints, integrated within the optimization process. We discuss the theoretical basis of our approach, as well as implemented prototype systems for Web pages and for general media-rich document presentation.",Adaptive information presentation; Preference representation; Qualitative decision theory,Artificial intelligence; Classification (of information); Human computer interaction; Information dissemination; Knowledge based systems; Optimization; Problem solving; Software prototyping; Adaptive information presentation; Hypermedia; Preference representation; Qualitative decision theory; Decision theory
Information systems interoperability: What lies beneath?,2004,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-9144233716&doi=10.1145%2f1028099.1028103&partnerID=40&md5=1d189ebde6c70ec19e24f5975bb9ef73,"Interoperability is the most critical issue facing businesses that need to access information from multiple information systems. Our objective in this research is to develop a comprehensive frame-work and methodology to facilitate semantic interoperability among distributed and heterogeneous information systems. A comprehensive framework for managing various semantic conflicts is proposed. Our proposed framework provides a unified view of the underlying representational and reasoning formalism for the semantic mediation process. This framework is then used as a basis for automating the detection and resolution of semantic conflicts among heterogeneous information sources. We define several types of semantic mediators to achieve semantic interoperability. A domain-independent ontology is used to capture various semantic conflicts. A mediation-based query processing technique is developed to provide uniform and integrated access to the multiple heterogeneous databases. A usable prototype is implemented as a proof-of-concept for this work. Finally, the usefulness of our approach is evaluated using three cases in different application domains. Various heterogeneous datasets are used during the evaluation phase. The results of the evaluation suggest that correct identification and construction of both schema and ontology-schema mapping knowledge play very important roles in achieving interoperability at both the data and schema levels.",Information integration; Mediators; Ontology; Semantic conflict resolution; Semantic heterogeneity,Artificial intelligence; Data acquisition; Data processing; Database systems; Problem solving; Semantics; Information integration; Mediators; Ontology; Remote method invocation (RMI); Semantic conflict resolution; Semantic heterogeneity; Interoperability
Analysis of lexical signatures for improving information persistence on the world wide web,2004,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-9144269133&doi=10.1145%2f1028099.1028101&partnerID=40&md5=eb3d97b4e7e0d6c3dd86fa24c62b56a7,"A lexical signature (LS) consisting of several key words from a Web document is often sufficient information for finding the document later, even if its URL has changed. We conduct a large-scale empirical study of nine methods for generating lexical signatures, including Phelps and Wilensky's original proposal (PW), seven of our own static variations, and one new dynamic method. We examine their performance on the Web over a 10-month period, and on a TREC data set, evaluating their ability to both (1) uniquely identify the original (possibly modified) document, and (2) locate other relevant documents if the original is lost. Lexical signatures chosen to minimize document frequency (DF) are good at unique identification but poor at finding relevant documents. PW works well on the relatively small TREC data set, but acts almost identically to DF on the Web, which contains billions of documents. Term-frequency-based lexical signatures (TF) are very easy to compute and often perform well, but are highly dependent on the ranking system of the search engine used. The term-frequency inverse-document-frequency- (TFIDF-) based method and hybrid methods (which combine DF with TF or TFIDF) seem to be the most promising candidates among static methods for generating effective lexical signatures. We propose a dynamic LS generator called Test & Select (TS) to mitigate LS conflict. TS outperforms all eight static methods in terms of both extracting the desired document and finding relevant information, over three different search engines. All LS methods show significant performance degradation as documents in the corpus are edited.",Broken URLs; Dead links; Digital libraries; Indexing; Information retrieval; Inverse document frequency; Lexical signatures; Robust hyperlinks; Search engines; Term frequency; TREC; World Wide Web,Data acquisition; Digital libraries; HTML; Indexing (of information); Information retrieval; Maintenance; Search engines; Servers; Broken URLs; Dead links; Indexing; Inverse document frequency; Lexical signatures; Robust hyperlinks; Term frequency; TREC; World Wide Web
Distributed content-based visual information retrieval system on peer-to-peer networks,2004,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-3843137150&doi=10.1145%2f1010614.1010619&partnerID=40&md5=703fca9d7794c49b23c59b2dbd432210,"With the recent advances of distributed computing, the limitation of information retrieval from a centralized image collection can be removed by allowing distributed image data sources to interact with each other for data storage sharing and information retrieval. In this article, we present our design and implementation of DISCOVIR: Distributed COntent-based Visual Information Retrieval system using the Peer-to-Peer (P2P) Network. We describe the system architecture and detail the interactions among various system modules. Specifically, we propose a Firework Query Model for distributed information retrieval, which aims to reduce the network traffic of query passing in the network. We carry out experiments to show the distributed image retrieval system and the Firework information retrieval algorithm. The results show that the algorithm reduces network traffic while increases searching performance.",Content-based image retrieval (CBIR); Information retrieval; Intelligent query routing; Peer clustering; Peer-to-peer (P2P) network,Algorithms; Data storage equipment; Decentralized control; Distributed computer systems; Feature extraction; Mathematical models; Query languages; Security of data; Telecommunication traffic; Throughput; Content-based image retrieval (CBIR); Intelligent query routing; Peer clustering; Peer-to-peer (P2P) networks; Information retrieval systems
Efficient mining of both positive and negative association rules,2004,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-3843055627&doi=10.1145%2f1010614.1010616&partnerID=40&md5=e99d4c1e1e61fc84571cefbccd4aca8c,"This paper presents an efficient method for mining both positive and negative association rules in databases. The method extends traditional associations to include association rules of forms A ⇒ ¬B, ¬A ⇒ B, and ¬A ⇒ ¬B, which indicate negative associations between itemsets. With a pruning strategy and an interestingness measure, our method scales to large databases. The method has been evaluated using both synthetic and real-world databases, and our experimental results demonstrate its effectiveness and efficiency.",Association rules; Negative associations,Algorithms; Constraint theory; Data reduction; Database systems; Decision making; Inventory control; Marketing; Set theory; Association rules; Automated prediction; Negative associations; Stock markets; Data mining
PocketLens: Toward a personal recommender system,2004,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-3843049042&doi=10.1145%2f1010614.1010618&partnerID=40&md5=94af765c04a4015f91ef53bf40cd428e,"Recommander systems using collaborative filtering are a popular technique for reducing information overload and finding products to purchase. One limitation of current recommenders is that they are not portable. They can only run on large computers connected to the Internet. A second limitation is that they require the user to trust the owner of the recommender with personal preference data. Personal recommenders hold the promise of delivering high quality recommendations on palmtop computers, even when disconnected from the Internet. Further, they can protect the user's privacy by storing personal information locally, or by sharing it in encrypted form. In this article we present the new PocketLens collaborative filtering algorithm along with five peer-to-peer architectures for finding neighbors. We evaluate the architectures and algorithms in a series of offline experiments. These experiments show that Pocketlens can run on connected servers, on usually connected workstations, or on occasionally connected portable devices, and produce recommendations that are as good as the best published algorithms to date.",Collaborative Filtering; Peer-to-Peer Networking; Privacy; Recommender Systems,Algorithms; Computer architecture; Computer software portability; Data privacy; Database systems; Information technology; Internet; Personal digital assistants; Security of data; Collaborative filtering; Peer-to-peer networking; Privacy; Recommender systems; Computer supported cooperative work
Relevance models to help estimate document and query parameters,2004,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-3843135035&doi=10.1145%2f1010614.1010615&partnerID=40&md5=4dc63b9f08dbc82136f9b770eaea6c63,"A central idea of Language Models is that documents (and perhaps queries) are random variables, generated by data-generating functions that are characterized by document (query) parameters. The key new idea of this paper is to model that a relevance judgment is also generated stochastically, and that its data generating function is also governed by those same document and query parameters. The result of this addition is that any available relevance judgments are easily incorporated as additional evidence about the true document and query model parameters. An additional aspect of this approach is that it also resolves the long-standing problem of document-oriented versus query-oriented probabilities. The general approach can be used with a wide variety of hypothesized distributions for documents, queries, and relevance. We test the approach on Reuters Corpus Volume 1, using one set of possible distributions. Experimental results show that the approach does succeed in incorporating relevance data to improve estimates of both document and query parameters, but on this data and for the specific distributions we hypothe-sized, performance was no better than two separate one-sided models. We conclude that the model's theoretical contribution is its integration of relevance models, document models, and query models, and that the potential for additional performance improvement over one-sided methods requires refinements.",Language models; Probabilistic models,Database systems; Heuristic methods; Information retrieval; Mathematical models; Parameter estimation; Program documentation; Random processes; Vectors; Document models; Language models; Probabilistic models; Query parameters; Query languages
Anchor text mining for translation of web queries: A transitive translation approach,2004,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-3042778919&doi=10.1145%2f984321.984324&partnerID=40&md5=84557ff544bd0cc626f36537129b58d0,"To discover translation knowledge in diverse data resources on the Web, this article proposes an effective approach to finding translation equivalents of query terms and constructing multilingual lexicons through the mining of Web anchor texts and link structures. Although Web anchor texts are wide-scoped hypertext resources, not every particular pair of languages contains sufficient anchor texts for effective extraction of translations for Web queries. For more generalized applications, the approach is designed based on a transitive translation model. The translation equivalents of a query term can be extracted via its translation in an intermediate language. To reduce interference from translation errors, the approach further integrates a competitive linking algorithm into the process of determining the most probable translation. A series of experiments has been conducted, including performance tests on term translation extraction, cross-language information retrieval, and translation suggestions for practical Web search services, respectively. The obtained experimental results have shown that the proposed approach is effective in extracting translations of unknown queries, is easy to combine with the probabilistic retrieval model to improve the cross-language retrieval performance, and is very useful when the considered language pairs lack a sufficient number of anchor texts. Based on the approach, an experimental system called LiveTrans has been developed for English-Chinese cross-language Web search.",,Data acquisition; Data mining; Data storage equipment; Database systems; Hypermedia systems; Hypertext systems; Information retrieval; Query languages; Search engines; Text processing; Data resources; Multilingual lexicons; Retrieval performance; Web queries; World Wide Web
"Streams, structures, spaces, scenarios, societies (5S): A formal model for digital libraries",2004,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-3042786480&doi=10.1145%2f984321.984325&partnerID=40&md5=19bf84c7e124350885e97ab76f9bf9f5,"Digital libraries (DLs) are complex information systems and therefore demand formal foundations lest development efforts diverge and interoperability suffers. In this article, we propose the fundamental abstractions of Streams, Structures, Spaces, Scenarios, and Societies (58), which allow us to define digital libraries rigorously and usefully. Streams are sequences of arbitrary items used to describe both static and dynamic (e.g., video) content. Structures can be viewed as labeled directed graphs, which impose organization. Spaces are sets with operations on those sets that obey certain constraints. Scenarios consist of sequences of events or actions that modify states of a computation in order to accomplish a functional requirement. Societies are sets of entities and activities and the relationships among them. Together these abstractions provide a formal foundation to define, relate, and unify concepts - among others, of digital objects, metadata, collections, and services required to formalize and elucidate ""digital libraries"". The applicability, versatility, and unifying power of the 5S model are demonstrated through its use in three distinct applications: building and interpretation of a DL taxonomy, informal and formal analysis of case studies of digital libraries (NDLTD and OAI), and utilization as a formal basis for a DL description language.",Applications; Definitions; Foundations; Taxonomy,Data storage equipment; Graph theory; Information retrieval systems; Mathematical models; Metadata; Set theory; Topology; Vectors; Complex information systems; Definitions; Formal analysis; Taxonomy; Digital libraries
A study of smoothing methods for language models applied to information retrieval,2004,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-3042824043&doi=10.1145%2f984321.984322&partnerID=40&md5=2c7b4dd72047d3192185ba570f51fb89,"Language modeling approaches to information retrieval are attractive and promising because they connect the problem of retrieval with that of language model estimation, which has been studied extensively in other application areas such as speech recognition. The basic idea of these approaches is to estimate a language model for each document, and to then rank documents by the likelihood of the query according to the estimated language model. A central issue in language model estimation is smoothing, the problem of adjusting the maximum likelihood estimator to compensate for data sparseness. In this article, we study the problem of language model smoothing and its influence on retrieval performance. We examine the sensitivity of retrieval performance to the smoothing parameters and compare several popular smoothing methods on different test collections. Experimental results show that not only is the retrieval performance generally sensitive to the smoothing parameters, but also the sensitivity pattern is affected by the query type, with performance being more sensitive to smoothing for verbose queries than for keyword queries. Verbose queries also generally require more aggressive smoothing to achieve optimal performance. This suggests that smoothing plays two different role - to make the estimated document language model more accurate and to ""explain"" the noninformative words in the query. In order to decouple these two distinct roles of smoothing, we propose a two-stage smoothing strategy, which yields better sensitivity patterns and facilitates the setting of smoothing parameters automatically. We further propose methods for estimating the smoothing parameters automatically. Evaluation on five different databases and four types of queries indicates that the two-stage smoothing method with the proposed parameter estimation methods consistently gives retrieval performance that is close to - or better than - the best results achieved using a single smoothing method and exhaustive parameter search on the test data.",Absolute discounting smoothing; Backoff smoothing; Dirichlet prior smoothing; EM algorithm; Interpolation smoothing; Jelinek-Mercer smoothing; Leave-one-out; Risk minimization; Statistical language models; Term weighting; TF-IDF weighting; Two-stage smoothing,Algorithms; Computer software; Data storage equipment; Database systems; Heuristic methods; Mathematical models; Poisson distribution; Robustness (control systems); Sensitivity analysis; Speech recognition; Statistical methods; Language models; Sensitivity patterns; Smoothing parameters; Statistical language modeling; Information retrieval
XIRQL: An XML query language based on information retrieval concepts,2004,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-3042738336&doi=10.1145%2f984321.984326&partnerID=40&md5=311f42d961e7dc13a40469f459fbaa14,"XIRQL (""circle"") is an XML query language that incorporates imprecision and vagueness for both structural and content-oriented query conditions. The corresponding uncertainty is handled by a consistent probabilistic model. The core features of XIRQL are (1) document ranking based on index term weighting, (2) specificity-oriented search for retrieving the most relevant parts of documents, (3) datatypes with vague predicates for dealing with specific types of content and (4) structural vagueness for vague interpretation of structural query conditions. A XIRQL database may contain several classes of documents, where all documents in a class conform to the same DTD; links between documents also are supported. XIRQL queries are translated into a path algebra, which can be processed by our HyREX retrieval engine.",Path algebra; Probabilistic retrieval; Ranked retrieval; Vague predicates; XML; XQuery,Algebra; Data storage equipment; Database systems; Hypermedia systems; Interfaces (computer); Mathematical models; Probability; Query languages; Search engines; XML; Path algebra; Probabilistic retrieval; Ranked retrieval; Vague predicates; XQuery; Information retrieval
Multidocument summarization: An added value to clustering in interactive retrieval,2004,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-3042785005&doi=10.1145%2f984321.984323&partnerID=40&md5=389c03209809cd501561245dcd70c4b2,"A more and more generalized problem in effective information access is the presence in the same corpus of multiple documents that contain similar information. Generally, users may be interested in locating, for a topic addressed by a group of similar documents, one or several particular aspects. This kind of task, called instance or aspectual retrieval, has been explored in several TREC Interactive Tracks. In this article, we propose in addition to the classification capacity of clustering techniques, the possibility of offering a indicative extract about the contents of several sources by means of naultidocument summarization techniques. Two kinds of summaries are provided. The first one covers the similarities of each cluster of documents retrieved. The second one shows the particularities of each document with respect to the common topic in the cluster. The document multitopic structure has been used in order to determine similarities and differences of topics in the cluster of documents. The system is independent of document domain and genre. An evaluation of the proposed system with users proves significant improvements in effectiveness. The results of previous experiments that have compared clustering algorithms are also reported.",Multidocument summarization; Topic segmentation,Algorithms; Database systems; Information retrieval systems; Interfaces (computer); Probability; Problem solving; Query languages; Search engines; Clustering algorithms; Clustering techniques; Multidocument summarization; Topic segmentation; Information retrieval
Applying associative retrieval techniques to alleviate the sparsity problem in collaborative filtering,2004,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-3042819722&doi=10.1145%2f963770.963775&partnerID=40&md5=35b2b11a59ff42fa14ac7c03abba4099,"Recommender systems are being widely applied in many application settings to suggest products, services, and information items to potential consumers, Collaborative filtering, the most successful recommendation approach, makes recommendations based on past transactions and feedback from consumers sharing similar interests. A major problem limiting the usefulness of collaborative filtering is the sparsity problem, which refers to a situation in which transactional or feedback data is sparse and insufficient to identify similarities in consumer interests. In this article, we propose to deal with this sparsity problem by applying an associative retrieval framework and related spreading activation algorithms to explore transitive associations among consumers through their past transactions and feedback. Such transitive associations are a valuable source of information to help infer consumer interests and can be explored to deal with the sparsity problem. To evaluate the effectiveness of our approach, we have conducted an experimental study using a data set from an online bookstore. We experimented with three spreading activation algorithms including a constrained Leaky Capacitor algorithm, a branch-and-bound serial symbolic search algorithm, and a Hopfield net parallel relaxation search algorithm. These algorithms were compared with several collaborative filtering approaches that do not consider the transitive associations: a simple graph search approach, two variations of the user-based approach, and an item-based approach. Our experimental results indicate that spreading activation-based approaches significantly out-performed the other collaborative filtering methods as measured by recommendation precision, recall, the F-measure, and the rank score. We also observed the over-activation effect of the spreading activation approach, that is, incorporating transitive associations with past transactional data that is not sparse may ""dilute"" the data used to infer user preferences and lead to degradation in recommendation performance.",Associative retrieval; Collaborative filtering; Recommender system; Sparsity problem; Spreading activation,Customer satisfaction; Data acquisition; Electronic commerce; Feedback; Graph theory; Information analysis; Marketing; Matrix algebra; Online systems; User interfaces; Associate retreival; Collaborative filtering; Recommender systems; Sparsity problems; Information retrieval
Introduction to recommender systems: Algorithms and evaluation,2004,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-3042701774&doi=10.1145%2f963770.963771&partnerID=40&md5=09e7ffb1ca150933561a9831597aa198,[No abstract available],,
Item-based top-N recommendation algorithms,2004,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-3042821101&doi=10.1145%2f963770.963776&partnerID=40&md5=dc38dc8b47e784d885286ed54d7eff7f,"The explosive growth of the world-wide-web and the emergence of e-commeroe has led to the development of recommender systems - a personalized information filtering technology used to identify a set of items that will be of interest to a certain user. User-based collaborative altering is the most successful technology for building recommender systems to date and is extensively used in many commercial recommender systems. Unfortunately, the computational complexity of these methods grows linearly with the number of customers, which in typical commercial applications can be several millions. To address these scalability concerns model-based recommendation techniques have been developed. These techniques analyze the user-item matrix to discover relations between the different items and use these relations to compute the list of recommendations. In this article, we present one such class of model-based recommendation algorithms that first determines the similarities between the various items and then uses them to identify the set of items to be recommended. The key steps in this class of algorithms are (i) the method used to compute the similarity between the items, and (ii) the method used to combine these similarities in order to compute the similarity between a basket of items and a candidate recommender item. Our experimental evaluation on eight real datasets shows that these item-based algorithms are up to two orders of magnitude faster than the traditional user-neighborhood based recommender systems and provide recommendations with comparable or better quality.",E-commerce; Predicting user behavior; World wide web,Algorithms; Computational complexity; Cosine transforms; Customer satisfaction; Electronic commerce; Electronic mail; Information technology; Mathematical models; Matrix algebra; Collaborative filtering; Datasets; Predicting user behavior; Recommender systems; World Wide Web
Latent semantic models for collaborative filtering,2004,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-3042742744&doi=10.1145%2f963770.963774&partnerID=40&md5=3a377ad8912eec4a375c7cd68d2760bd,"Collaborative filtering aims at learning predictive models of user preferences, interests or behavior from community data, that is, a database of available user preferences. In this article, we describe a new family of model-based algorithms designed for this task. These algorithms rely on a statistical modelling technique that introduces latent class variables in a mixture model setting to discover user communities and prototypical interest profiles. We investigate several variations to deal with discrete and continuous response variables as well as with different objective functions. The main advantages of this technique over standard memory-based methods are higher accuracy, constant time prediction, and an explicit and compact model representation. The latter can also be used to mine for user communitites. The experimental evaluation shows that substantial improvements in accucracy over existing methods and published results can be obtained.",Collaborative filtering; Latent semantic analysis; Machine learning; Mixture models; Recommender systems,Algorithms; Database systems; Functions; Lagrange multipliers; Learning systems; Mathematical models; Matrix algebra; Mixtures; Signal filtering and prediction; Statistical methods; User interfaces; Collaborative filtering; Latent semantic analysis; Mixture models; Recommender systems; Semantics
Evaluating collaborative filtering recommender systems,2004,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-3042697346&doi=10.1145%2f963770.963772&partnerID=40&md5=3affd2a019ec52854130e9344b268268,"Recommender systems have been evaluated in many, often incomparable, ways. In this article, we review the key decisions in evaluating collaborative filtering recommender systems: the user tasks being evaluated, the types of analysis and datasets being used, the ways in which prediction quality is measured, the evaluation of prediction attributes other than quality, and the user-based evaluation of the system as a whole. In addition to reviewing the evaluation strategies used by prior researchers, we present empirical results from the analysis of various accuracy metrics on one content domain where all the tested metrics collapsed roughly into three equivalence classes. Metrics within each equivalency class were strongly correlated, while metrics from different equivalency classes were uncorrelated.",Collaborative filtering; Evaluation; Metrics; Recommender systems,Algorithms; Correlation methods; Data reduction; Decision making; Decision support systems; Equivalence classes; Metric system; Problem solving; Signal processing; Standardization; Standards; Collaborative filtering; Datasets; Ratings density; Recommender systems; Signal filtering and prediction
Trustworthy 100-year digital objects: Evidence after every witness is dead,2004,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-3843079515&doi=10.1145%2f1010614.1010617&partnerID=40&md5=90bf8aa1e02ea00e2e8727d946eb5cb8,"In ancient times, wax seals impressed with signet rings were affixed to documents as evidence of their authenticity. A digital counterpart is a message authentication code fixed firmly to each important document. If a digital object is sealed together with its own audit trail, each user can examine this evidence to decide whether to trust the content - no matter how distant this user is in time, space, and social affiliation from the document's source. We propose an architecture and design that accomplish this: encapsulation of digital object content with metadata describing its origins, cryptographic sealing, webs of trust for public keys rooted in a forest of respected institutions, and a certain way of managing information identifiers. These means will satisfy emerging needs in civilian and military record management, including medical patient records, regulatory records for aircraft and pharmaceuticals, business records for financial audit, legislative and legal briefs, and scholarly works. This is true for any kind of digital object, independent of its purposes and of most data type and representation details, and provides every kind of user - information authors and editors, librarians and collection managers, and information consumers - with autonomy for implied tasks. Our prototype will conform to applicable standards, will be interoperable over most computing bases, and will be compatible with existing digital library software. The proposed architecture integrates software that is mostly available and widely accepted.",Legal Aspects; Reliability; Security,Cryptography; Digital libraries; Information technology; Interoperability; Metadata; Program documentation; Software engineering; World Wide Web; Data types; Digital information; Digital interchange; Digital objects; Digital signal processing
Ontological user profiling in recommender systems,2004,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-3042825514&doi=10.1145%2f963770.963773&partnerID=40&md5=503d0c7a59884025ff5e1a61329e140c,"We explore a novel ontological approach to user profiling within recommender systems, working on the problem of recommending on-line academic research papers. Our two experimental systems, Quickstep and Foxtrot, create user profiles from unobtrusively monitored behaviour and relevance feedback, representing the profiles in terms of a research paper topic ontology. A novel profile visualization approach is taken to acquire profile feedback. Research papers are classified using ontological classes and collaborative recommendation algorithms used to recommend papers seen by similar people on their current topics of interest. Two small-scale experiments, with 24 subjects over 3 months, and a large-scale experiment, with 260 subjects over an academic year, are conducted to evaluate different aspects of our approach. Ontological inference is shown to improve user profiling, external ontological knowledge used to successfully bootstrap a recommender system and profile visualization employed to improve profiling accuracy. The overall performance of our ontological recommender systems are also presented and favourably compared to other systems in the literature.",Agent; Machine learning; Ontology; Personalization; Recommender systems; User modelling; User profiling,Algorithms; Feedback; Industrial research; Inspection; Knowledge acquisition; Learning systems; Signal filtering and prediction; User interfaces; Experimental systems; Ontological user profiling; Recommender systems; User profiling; World Wide Web
A semisupervised learning method to merge search engine results,2003,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-2442515614&doi=10.1145%2f944012.944017&partnerID=40&md5=1ab869650ba5f99ec3268148fb0d1285,"The proliferation of searchable text databases on local area networks and the Internet causes the problem of finding information that may be distributed among many disjoint text databases (distributed information retrieval). How to merge the results returned by selected databases is an important subproblem of the distributed information retrieval task. Previous research assumed that either resource providers cooperate to provide normalizing statistics or search clients down-load all retrieved documents and compute normalized scores without cooperation from resource providers. This article presents a semisupervised learning solution to the result merging problem. The key contribution is the observation that information used to create resource descriptions for resource selection can also be used to create a centralized sample database to guide the normalization of document scores returned by different databases. At retrieval time, the query is sent to the selected databases, which return database-specific document scores, and to a centralized sample database, which returns database-independent document scores. Documents that have both a database-specific score and a database-independent score serve as training data for learning to normalize the scores of other documents. An extensive set of experiments demonstrates that this method is more effective than the well-known CORI result-merging algorithm under a variety of conditions.",Distributed information retrieval; Resource ranking; Resource selection; Results merging; Semisupervised learning method; Server selection,Algorithms; Computer software; Database systems; Decision making; Information retrieval; Internet; Servers; Distributed information retrieval; Resource ranking; Resource selection; Server selection; Search engines
MEGA - The maximizing expected generalization algorithm for learning complex query concepts,2003,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-2442512048&doi=10.1145%2f944012.944014&partnerID=40&md5=46d2da4e002893297681030ca457d837,"Specifying exact query concepts has become increasingly challenging to end-users. This is because many query concepts (e.g., those for looking up a multimedia object) can be hard to articulate, and articulation can be subjective. In this study, we propose a query-concept learner that learns query criteria through an intelligent sampling process. Our concept learner aims to fulfill two primary design objectives: (1) it has to be expressive in order to model most practical query concepts and (2) it must learn a concept quickly and with a small number of labeled data since online users tend to be too impatient to provide much feedback. To fulfill the first goal, we model query concepts in k-CNF, which can express almost all practical query concepts. To fulfill the second design goal, we propose our maximizing expected generalization algorithm (MEGA), which converges to target concepts quickly by its two complementary steps: sample selection and concept refinement. We also propose a divide-and-conquer method that divides the concept-learning task into G subtasks to achieve speedup. We notice that a task must be divided carefully, or search accuracy may suffer. Through analysis and mining results, we observe that organizing image features in a multiresolution manner, and minimizing intragroup feature correlation, can speed up query-concept learning substantially while maintaining high search accuracy. Through examples, analysis, experiments, and a prototype implementation, we show that MEGA converges to query concepts significantly faster than traditional methods.",Active learning; Data mining; Query concept; Relevance feedback,Data mining; Database systems; Feedback; Image retrieval; Multimedia systems; Active learning; Query concept; Relevance feedback; Information science
Comparing the performance of collection selection algorithms,2003,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-2442500342&doi=10.1145%2f944012.944016&partnerID=40&md5=37fe3a03376e06242d6c0341cf000e43,"The proliferation of online information resources increases the importance of effective and efficient information retrieval in a multicollection environment. Multicollection searching is cast in three parts: collection selection (also referred to as database selection), query processing and results merging. In this work, we focus our attention on the evaluation of the first step, collection selection. In this article, we present a detailed discussion of the methodology that we used to evaluate and compare collection selection approaches, covering both test environments and evaluation measures. We compare the CORI, CW and gGlOSS collection selection approaches using six test environments utilizing three document testbeds. We note similar trends in performance among the collection selection approaches, but the CORI approach consistently outperforms the other approaches, suggesting that effective collection selection can be achieved using limited information about each collection. The contributions of this work are both the assembled evaluation methodology as well as the application of that methodology to compare collection selection approaches in a standardized environment.",Collection selection; Database selection; Distributed information retrieval; Distributed text retrieval; Metasearch engine; Resource discovery; Resource ranking; Resource selection; Server ranking; Server selection; Text retrieval,Algorithms; Database systems; Online searching; Search engines; Servers; Collection selection; Database selection; Metasearch engines; Resource ranking; Server selection; Information retrieval
Measuring praise and criticism: Inference of semantic orientation from association,2003,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-2442507763&doi=10.1145%2f944012.944013&partnerID=40&md5=8dcb3edca2c22d8f4f3d272dbd0a0cba,"The evaluative character of a word is called its semantic orientation. Positive semantic orientation indicates praise (e.g., ""honest"", ""intrepid"") and negative semantic orientation indicates criticism (e.g., ""disturbing"", ""superfluous""). Semantic orientation varies in both direction (positive or negative) and degree (mild to strong). An automated system for measuring semantic orientation would have application in text classification, text filtering, tracking opinions in online discussions, analysis of survey responses, and automated chat systems (chatbots). This article introduces a method for inferring the semantic orientation of a word from its statistical association with a set of positive and negative paradigm words. Two instances of this approach are evaluated, based on two different statistical measures of word association: pointwise mutual information (PMI) and latent semantic analysis (LSA). The method is experimentally tested with 3,596 words (including adjectives, adverbs, nouns, and verbs) that have been manually labeled positive (1,614 words) and negative (1,982 words). The method attains an accuracy of 82.8% on the full test set, but the accuracy rises above 95% when the algorithm is allowed to abstain from classifying mild words.",Latent semantic analysis; Mutual information; Semantic association; Semantic orientation; Text classification; Text mining; Unsupervised learning; Web mining,Algorithms; Information retrieval; Linguistics; Statistical methods; World Wide Web; Semantic association; Semantic orientation; Text mining; Web mining; Semantics
"Coverage, relevance, and ranking: The impact of query operators on Web search engine results",2003,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-2442457056&doi=10.1145%2f944012.944015&partnerID=40&md5=1a0b0e45234ca103b05a92843b10f7ad,"Research has reported that about 10% of Web searchers utilize advanced query operators, with the other 90% using extremely simple queries. It is often assumed that the use of query operators, such as Boolean operators and phrase searching, improves the effectiveness of Web searching. We test this assumption by examining the effects of query operators on the performance of three major Web search engines. We selected one hundred queries from the transaction log of a Web search service. Each of these original queries contained query operators such as AND, OR, MUST APPEAR (+), or PHRASE ("" ""). We then removed the operators from these one hundred advanced queries. We submitted both the original and modified queries to three major Web search engines; a total of 600 queries were submitted and 5,748 documents evaluated. We compared the results from the original queries with the operators to the results from the modified queries without the operators. We examined the results for changes in coverage, relative precision, and ranking of relevant documents. The use of most query operators had no significant effect on coverage, relative precision, or ranking, although the effect varied depending on the search engine. We discuss implications for the effectiveness of searching techniques as currently taught, for future information retrieval system design, and for future research.",Boolean operators; Coverage; Query operators; Ranking; Relative precision; Search engines; Web results,Boolean algebra; Database systems; Digital libraries; Information retrieval; World Wide Web; Boolean operators; Coverage; Query operators; Relative precision; Web results; Search engines
Query-independent evidence in home page finding,2003,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-2442488148&doi=10.1145%2f858476.858479&partnerID=40&md5=dd67c55ea3e65d9244ccdaf900143773,"Hyperlink recommendation evidence, that is, evidence based on the structure of a web's link graph, is widely exploited by commercial Web search systems. However there is little published work to support its popularity. Another form of query-independent evidence, URL-type, has been shown to be beneficial on a home page finding task. We compared the usefulness of these types of evidence on the home page finding task, combined with both content and anchor text baselines. Our experiments made use of five query sets spanning three corpora - one enterprise crawl, and the WT10g and VLC2 Web test collections. We found that, in optimal conditions, all of the query-independent methods studied (in-degree, URL-type, and two variants of PageRank) offered a better than random improvement on a content-only baseline. However, only URL-type offered a better than random improvement on an anchor text baseline. In realistic settings, for either baseline, only URL-type offered consistent gains. In combination with URL-type the anchor text baseline was more useful for finding popular home pages, but URL-type with content was more useful for finding randomly selected home pages. We conclude that a general home page finding system should combine evidence from document content, anchor text, and URL-type classification.",Citation and link analysis; Connectivity; Web information retrieval,Data storage equipment; Graph theory; Information retrieval; Query languages; Search engines; Websites; Citation and link analysis; Connectivity; Document contents; Web information retrieval; Online searching
Logical and physical design issues for smart card databases,2003,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-2442517723&doi=10.1145%2f858476.858478&partnerID=40&md5=b668fe958bd7cdcd04e05c33be5628f7,"The design of very small databases for smart cards and for portable embedded systems is deeply constrained by the peculiar features of the physical medium. We propose a joint approach to the logical and physical database design phases and evaluate several data structures with respect to the performance, power consumption, and endurance parameters of read/program operations on the Flash-BEPROM storage medium.",Access methods; Data structures; Design methodology; Flash memory; Personal information systems; Smart card,Data processing; Data storage equipment; Data structures; Database systems; Flash memory; Information management; Access methods; Data consistency; Design methodology; Personal information systems; Smart cards
The use of dynamic contexts to improve casual Internet searching,2003,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-2442489730&doi=10.1145%2f858476.858477&partnerID=40&md5=5085f9e58996e9746a9a443a3035c036,"Research has shown that most users' online information searches are suboptimal. Query optimization based on a relevance feedback or genetic algorithm using dynamic query contexts can help casual users search the Internet. These algorithms can draw on implicit user feedback based on the surrounding links and text in a search engine result set to expand user queries with a variable number of keywords in two manners. Positive expansion adds terms to a user's keywords with a Boolean ""and,"" negative expansion adds terms to the user's keywords with a Boolean ""not."" Each algorithm was examined for three user groups, high, middle, and low achievers, who were classified according to their overall performance. The interactions of users with different levels of expertise with different expansion types or algorithms were evaluated. The genetic algorithm with negative expansion tripled recall and doubled precision for low achievers, but high achievers displayed an opposed trend and seemed to be hindered in this condition. The effect of other conditions was less substantial.",Automatic query expansion; Genetic algorithm; Implicit user feedback; Information retrieval; Internet; Personalization; Relevance feedback,Data storage equipment; Genetic algorithms; Information retrieval; Internet; Query languages; Search engines; Automatic query expansion; Implicit user feedback; Personalization; Relevance feedback; Online searching
Region proximity in metric spaces and its use for approximate similarity search,2003,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-2442438150&doi=10.1145%2f763693.763696&partnerID=40&md5=981c22ce79865a5e344f5aa647b6c3aa,"Similarity search structures for metric data typically bound object partitions by ball regions. Since regions can overlap, a relevant issue is to estimate the proximity of regions in order to predict the number of objects in the regions' intersection. This paper analyzes the problem using a probabilistic approach and provides a solution that effectively computes the proximity through realistic heuristics that only require small amounts of auxiliary data. An extensive simulation to validate the technique is provided. An application is developed to demonstrate how the proximity measure can be successfully applied to the approximate similarity search. Search speedup is achieved by ignoring data regions whose proximity to the query region is smaller than a user-defined threshold. This idea is implemented in a metric tree environment for the similarity range and ""nearest neighbors"" queries. Several measures of efficiency and effectiveness are applied to evaluate proposed approximate search algorithms on real-life data sets. An analytical model is developed to relate proximity parameters and the quality of search. Improvements of two orders of magnitude are achieved for moderately approximated search results. We demonstrate that the precision of proximity measures can significantly influence the quality of approximated algorithms.",Approximate similarity search; Approximation algorithms; Metric data; Metric trees; Performance evaluation,Algorithms; Approximation theory; Data storage equipment; Feature extraction; Information retrieval; Query languages; Search engines; Approximate similarity search; Approximation algorithms; Metric data; Metric trees; Performance evaluation; Database systems
A hierarchical access control model for video database systems,2003,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-2442563871&doi=10.1145%2f763693.763695&partnerID=40&md5=43460efaced29efbca6a9dca3a77d916,"Content-based video database access control is becoming very important, but it depends on the progresses of the following related research issues: (a) efficient video analysis for supporting semantic visual concept representation; (b) effective video database indexing structure; (c) the development of suitable video database models; and (d) the development of access control models tailored to the characteristics of video data. In this paper, we propose a novel approach to support multilevel access control in video databases. Our access control technique combines a video database indexing mechanism with a hierarchical organization of visual concepts (i.e., video database indexing units), so that different classes of users can access different video elements or even the same video element with different quality levels according to their permissions. These video elements, which, in our access control mechanism, are used for specifying the authorization objects, can be a semantic cluster, a subcluster, a video scene, a video shot, a video frame, or even a salient object (i.e., region of interest). In the paper, we first introduce our techniques for obtaining these multilevel video access units. We also propose a hierarchical video database indexing technique to support our multilevel video access control mechanism. Then, we present an innovative access control model which is able to support flexible multilevel access control to video elements. Moreover, the application of our multilevel video database modeling, representation, and indexing for MPEG-7 is discussed.",Access control; Indexing schemes; Video database models,Database systems; Hierarchical systems; Indexing (of information); Information analysis; Process control; Semantics; Societies and institutions; Access control; Indexing schemes; Video contents; Video database models; Content based retrieval
Early user - System interaction for database selection in massive domain-specific online environments,2003,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-2442616315&doi=10.1145%2f635484.635488&partnerID=40&md5=014f8010ce7e4cb20458c485cb2860f5,"The continued growth of very large data environments such as Westlaw and Dialog, in addition to the World Wide Web, increases the importance of effective and efficient database selection and searching. Current research focuses largely on completely autonomous and automatic selection, searching, and results merging in distributed environments. This fully automatic approach has significant deficiencies, including reliance upon thresholds below which databases with relevant documents are not searched (compromised recall). It also merges documents, often from disparate data sources that users may have discarded before their source selection task proceeded (diluted precision). We examine the impact that early user interaction can have on the process of database selection. After analyzing thousands of real user queries, we show that precision can be significantly increased when queries are categorized by the users themselves, then handled effectively by the system. Such query categorization strategies may eliminate limitations of fully automated query processing approaches. Our system harnesses the WIN search engine, a sibling to INQUERY, run against one or more authority sources when search is required. We compare our approach to one that does not recognize or utilize distinct features associated with user queries. We show that by avoiding a one-size-fits-all approach that restricts the role users can play in information discovery, database selection effectiveness can be appreciably improved.",Database selection; Metadata for retrieval; Structuring information to aid search and navigation; User interaction,Database systems; Human computer interaction; Information retrieval systems; Metadata; Online systems; Search engines; Database selection; Metadata for retrieval; Structuring information to aid search and navigation; User interaction; World Wide Web
QProber: A system for automatic classification of hidden-Web databases,2003,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0344127987&doi=10.1145%2f635484.635485&partnerID=40&md5=283c5c24c5b56be9ce0a680e7effa08b,"The contents of many valuable Web-accessible databases are only available through search interfaces and are hence invisible to traditional Web ""crawlers."" Recently, commercial Web sites have started to manually organize Web-accessible databases into Yahoo!-like hierarchical classification schemes. Here we introduce QProber, a modular system that automates this classification process by using a small number of query probes, generated by document classifiers. QProber can use a variety of types of classifiers to generate the probes. To classify a database, QProber does not retrieve or inspect any documents or pages from the database, but rather just exploits the number of matches that each query probe generates at the database in question. We have conducted an extensive experimental evaluation of QProber over collections of real documents, experimenting with different types of document classifiers and retrieval models. We have also tested our system with over one hundred Web-accessible databases. Our experiments show that our system has low overhead and achieves high classification accuracy across a variety of databases.",Database classification; Hidden Web; Web databases,Algorithms; Automation; Codes (symbols); Database systems; Hierarchical systems; Information retrieval; Mathematical models; Query languages; Database classification; Hidden Web; QProber; Web databases; Websites
Local versus global link information in the Web,2003,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0142125446&doi=10.1145%2f635484.635486&partnerID=40&md5=f25484b113b76745d494b51c80a559ab,"Information derived from the cross-references among the documents in a hyperlinked environment, usually referred to as link information, is considered important since it can be used to effectively improve document retrieval. Depending on the retrieval strategy, link information can be local or global. Local link information is derived from the set of documents returned as answers to the current user query. Global link information is derived from all the documents in the collection. In this work, we investigate how the use of local link information compares to the use of global link information. For the comparison, we run a series of experiments using a large document collection extracted from the Web, For our reference collection, the results indicate that the use of local link information improves precision by 74%. When global link information is used, precision improves by 35%. However, when only the first 10 documents in the ranking are considered, the average gain in precision obtained with the use of global link information is higher than the gain obtained with the use of local link information. This is an interesting result since it provides insight and justification for the use of global link information in major Web search engines, where users are mostly interested in the first 10 answers. Furthermore, global information can be computed in the background, which allows speeding up query processing.",Belief networks; Link analysis; Local and global information; World Wide Web,Algorithms; Information management; Information retrieval systems; Mathematical models; Query languages; Search engines; Telecommunication links; Belief networks; Link analysis; Local and global information; Query processing; World Wide Web
Exploiting hierarchical domain structure to compute similarity,2003,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-1842650939&doi=10.1145%2f635484.635487&partnerID=40&md5=072ea905716b4a2d7ac4cedc69051c93,"The notion of similarity between objects finds use in many contexts, for example, in search engines, collaborative filtering, and clustering. Objects being compared often are modeled as sets, with their similarity traditionally determined based on set intersection. Intersection-based measures do not accurately capture similarity in certain domains, such as when the data is sparse or when there are known relationships between items within sets. We propose new measures that exploit a hierarchical domain structure in order to produce more intuitive similarity scores. We extend our similarity measures to provide appropriate results in the presence of multisets (also handled unsatisfactorily by traditional measures), for example, to correctly compute the similarity between customers who buy several instances of the same product (say milk), or who buy several products in the same category (say dairy products). We also provide an experimental comparison of our measures against traditional similarity measures, and report on a user study that evaluated how well our measures match human intuition.",Collaborative filtering; Data mining; Hierarchy; Similarity measures,Algorithms; Computational methods; Data mining; Hierarchical systems; Human computer interaction; Information retrieval systems; Mathematical models; Collaborative filtering; Domain structures; Hierarchy; Similarity measures; Search engines
Performance issues and error analysis in an open-domain question answering system,2003,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-2442589044&doi=10.1145%2f763693.763694&partnerID=40&md5=ac7609571104103031cae0757dd1235c,"This paper presents an in-depth analysis of a state-of-the-art Question Answering system. Several scenarios are examined: (1) the performance of each module in a serial baseline system, (2) the impact of feedbacks and the insertion of a logic prover, and (3) the impact of various retrieval strategies and lexical resources. The main conclusion is that the overall performance depends on the depth of natural language processing resources and the tools used for answer finding.",Natural language applications; Performance analysis; Question answering; Text retrieval,Algorithms; Error analysis; Information retrieval; Systems analysis; Text processing; Natural language applications; Performance analysis; Question answering; Text retrieval; Natural language processing systems
Probabilistic Models of Information Retrieval Based on Measuring the Divergence from Randomness,2002,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33750388702&doi=10.1145%2f582415.582416&partnerID=40&md5=36590106c61b0dd745d18f2ec4a41bf8,"We introduce and create a framework for deriving probabilistic models of Information Retrieval. The models are nonparametric models of IR obtained in the language model approach. We derive term-weighting models by measuring the divergence of the actual term distribution from that obtained under a random process. Among the random processes we study the binomial distribution and Bose-Einstein statistics.We define two types of term frequency normalization for tuning term weights in the document-query matching process. The first normalization assumes that documents have the same length and measures the information gain with the observed term once it has been accepted as a good descriptor of the observed document. The second normalization is related to the document length and to other statistics. These two normalization methods are applied to the basic models in succession to obtain weighting formulae. Results show that our framework produces different nonparametric models forming baseline alternatives to the standard tf-idf model. © 2002, ACM. All rights reserved.",Aftereffect model; Algorithms; binomial law; BM25; Bose-Einstein statistics; document length normalization; eliteness; Experimentation; idf; information retrieval; Laplace; Poisson; probabilistic models; randomness; succession law; term frequency normalization; term weighting,Laplace transforms; Mathematical models; Polynomials; Probability; Query languages; Random processes; Aftereffect model; Binomial law; Bose-Einstein statistics; Document length normalization; Probabilistic models; Succession law; Term frequency normalization; Term weighting; Information retrieval
Cumulated gain-based evaluation of IR techniques,2002,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-1842637192&doi=10.1145%2f582415.582418&partnerID=40&md5=2476472ef70fc99e78e543e83e6173f7,"Modem large retrieval environments tend to overwhelm their users by their large output. Since all documents are not of equal relevance to their users, highly relevant documents should be identified and ranked first for presentation. In order to develop IR techniques in this direction, it is necessary to develop evaluation approaches and methods that credit IR methods for their ability to retrieve highly relevant documents. This can be done by extending traditional evaluation methods, that is, recall and precision based on binary relevance judgments, to graded relevance judgments. Alternatively, novel measures based on graded relevance judgments may be developed. This article proposes several novel measures that compute the cumulative gain the user obtains by examining the retrieval result up to a given ranked position. The first one accumulates the relevance scores of retrieved documents along the ranked result list. The second one is similar but applies a discount factor to the relevance scores in order to devaluate late-retrieved documents. The third one computes the relative-to-the-ideal performance of IR techniques, based on the cumulative gain they are able to yield. These novel measures are defined and discussed and their use is demonstrated in a case study using TREC data: sample system run results for 20 queries in TREC-7. As a relevance base we used novel graded relevance judgments on a four-point scale. The test results indicate that the proposed measures credit IR methods for their ability to retrieve highly relevant documents and allow testing of statistical significance of effectiveness differences. The graphs based on the measures also provide insight into the performance IR techniques and allow interpretation, for example, from the user point of view. © 2002 ACM.",Cumulated gain; Graded relevance judgments,Binary codes; Modems; Precision engineering; Statistical methods; User interfaces; Binary relevance judgments; Cumulated gains; Graded relevance judgments; Relevant documents; Information retrieval systems
A semantic network-based design methodology for XML documents,2002,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-1442343516&doi=10.1145%2f582415.582417&partnerID=40&md5=de6c1a6cc70f9c1c1b08e75c6020afca,"The eXtensible Markup Language (XML) is fast emerging as the dominant standard for describing and interchanging data among various systems and databases on the Internet. It offers the Document Type Definition (DTD) as a formalism for defining the syntax and structure of XML documents. The XML Schema definition language, as a replacement for the DTD, provides more rich facilities for defining and constraining the content of XML documents. However, it does not concentrate on the semantics that underlies these documents, representing a logical data model rather than a conceptual model. To enable efficient business application development in large-scale electronic commerce environments, it is necessary to describe and model real-world data semantics and their complex interrelationships. In this article, we describe a design methodology for XML documents. The aim is to enforce XML conceptual modeling power and bridge the gap between software development and XML document structures. The proposed methodology is comprised of two design levels: the semantic level and the schema level. The first level is based on a semantic network, which provides semantic modeling of XML through four major components: a set of atomic and complex nodes, representing real-world objects; a set of directed edges, representing semantic relationships between the objects; a set of labels denoting different types of semantic relationships, including aggregation, generalization, association, and of-property relationships; and finally a set of constraints defined over nodes and edges to constrain semantic relationships and object domains. The other level of the proposed methodology is concerned with detailed XML schema design, including element /attribute declarations and simple / complex type definitions. The mapping between the two design levels is proposed to transform the XML semantic model into the XML Schema, based on which XML documents can be systematically created, managed, and validated. © 2002 ACM.",Design; Languages,Codes (symbols); Computer aided design; Constraint theory; Mathematical models; Semantics; Software engineering; Conceptual modeling power; Document Type Definition (DTD); Semantic modeling; Semantic networks; XML
Improving retrieval feedback with multiple term-ranking function combination,2002,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-8744311647&doi=10.1145%2f568727.568728&partnerID=40&md5=2ec0a566fb34b8b66289af813b78bd47,"In this article we consider methods for automatic query expansion from top retrieved documents (i.e., retrieval feedback) that make use of various functions for scoring expansion terms within Rocchio's classical reweighting scheme. An analytical comparison shows that the retrieval performance of methods based on distinct term-scoring functions is comparable on the whole query set but differs considerably on single queries, consistent with the fact that the ordered sets of expansion terms suggested for each query by the different functions are largely uncorrelated. Motivated by those findings, we argue that the results of multiple functions can be merged, by analogy with ensembling classifiers, and present a simple combination technique based on the rank values of the suggested terms. The combined retrieval feedback method is effective not only with respect to unexpanded queries but also to any individual method, with notable improvements on the system's precision. Furthermore, the combined method is robust with respect to variation of experimental parameters and it is beneficial even when the same information needs are expressed with shorter queries. © 2002 ACM.",Automatic query expansion; Information retrieval; Method combination; Retrieval feedback; Short queries,Autonomous agents; Classifiers; Feedback control; Functions; Query languages; Robustness (control systems); Automatic query expansion; Retrieval feedback; Short queries; Unexpanded queries; Information retrieval
Peer-to-peer data trading to preserve information,2002,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0010387472&doi=10.1145%2f506309.506310&partnerID=40&md5=361eaeb185bd5bc8f033cee01ce43268,"Data archiving systems rely on replication to preserve information. This paper discusses how a network of autonomous archiving sites can trade data to achieve the most reliable replication. A series of binary trades among sites produces a peer-to-peer archiving network. Two trading algorithms are examined, one based on trading collections (even if they are different sizes) and another based on trading equal sized blocks of space (which can then store collections). The concept of deeds is introduced; deeds track the blocks of space owned by one site at another. Policies for tuning these algorithms to provide the highest reliability, for example by changing the order in which sites are contacted and offered trades, are discussed. Finally, simulation results are presented that reveal which policies are best. The experiments indicate that a digital archive can achieve the best reliability by trading blocks of space (deeds), and that following certain policies will allow that site to maximize its reliability. © 2002 ACM.",Data replication; Digital archiving; Digital library; Fault tolerance; Resource negotiation,Algorithms; Autonomous agents; Block codes; Computer simulation; Information retrieval; Websites; Data replication; Digital archiving systems; Digital library; Resource negotiation; Distributed computer systems
A general-purpose compression scheme for large collections,2002,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33645851469&doi=10.1145%2f568727.568730&partnerID=40&md5=41dfed62755d5c923b7d64a23bd5f57b,"Compression of large collections can lead to improvements in retrieval times by offsetting the CPU decompression costs with the cost of seeking and retrieving data from disk. We propose a semistatic phrase-based approach called XRAY that builds a model offline using sample training data extracted from a collection, and then compresses the entire collection online in a single pass. The particular benefits of XRAY are that it can be used in applications where individual records or documents must be decompressed, and that decompression is fast. The XRAY scheme also allows new data to be added to a collection without modifying the semistatic model. Moreover, XRAY can be used to compress general-purpose data such as genomic, scientific, image, and geographic collections without prior knowledge of the structure of the data. We show that XRAY is effective on both text and general-purpose collections. In general, XRAY is more effective than the popular GZIP and COMPRESS schemes, while being marginally less effective than BZIP2. We also show that XRAY is efficient: of the popular schemes we tested, it is typically only slower than GZIP in decompression. Moreover, the query evaluation costs of retrieval of documents from a large collection with our search engine is improved by more than 30% when XRAY is incorporated compared to an uncompressed approach. We use simple techniques for obtaining the training data from the collection to be compressed and show that with just over 4% of data the entire collection can be effectively compressed. We also propose four schemes for phrase-match selection during the single pass compression of the collection. We conclude that with these novel approaches XRAY is a fast and effective scheme for compression and decompression of large general-purpose collections. © 2002 ACM.",Phrase-based compression; Random access; Sampling,Data acquisition; Data structures; General purpose computers; Mathematical models; Nonbibliographic retrieval systems; Query languages; Search engines; COMPRESS schemes; Phrase based compressions; Query evaluation; Single pass compression; Data compression
"Burst tries: A fast, efficient data structure for string keys",2002,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0038564328&doi=10.1145%2f506309.506312&partnerID=40&md5=7ce7206458dcb19f2c554f5ca9616576,"Many applications depend on efficient management of large sets of distinct strings in memory. For example, during index construction for text databases a record is held for each distinct word in the text, containing the word itself and information such as counters. We propose a new data structure, the burst trie, that has significant advantages over existing options for such applications: it uses about the same memory as a binary search tree; it is as fast as a trie; and, while not as fast as a hash table, a burst trie maintains the strings in sorted or near-sorted order. In this paper we describe burst tries and explore the parameters that govern their performance. We experimentally determine good choices of parameters, and compare burst tries to other structures used for the same task, with a variety of data sets. These experiments show that the burst trie is particularly effective for the skewed frequency distributions common in text collections, and dramatically outperforms all other data structures for the task of managing strings while maintaining sort order. © 2002 ACM.",Binary trees; Splay trees; String data structures; Text databases; Tries; Vocabulary accumulation,Binary codes; Indexing (of information); Information management; Parameter estimation; Storage allocation (computer); Trees (mathematics); Splay trees; String data structures; Text databases; Vocabulary accumulation; Data structures
Theory of keyblock-based image retrieval,2002,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-3042783132&doi=10.1145%2f506309.506313&partnerID=40&md5=5ad53339fa9e3e5450d798b277808757,"The success of text-based retrieval motivates us to investigate analogous techniques which can support the querying and browsing of image data. However, images differ significantly from text both syntactically and semantically in their mode of representing and expressing information. Thus, the generalization of information retrieval from the text domain to the image domain is non-trivial. This paper presents a framework for information retrieval in the image domain which supports content-based querying and browsing of images. A critical first step to establishing such a framework is to construct a codebook of ""keywords"" for images which is analogous to the dictionary for text documents. We refer to such ""keywords"" in the image domain as ""keyblocks."" In this paper, we first present various approaches to generating a codebook containing keyblocks at different resolutions. Then we present a keyblock-based approach to content-based image retrieval. In this approach, each image is encoded as a set of one-dimensional index codes linked to the keyblocks in the codebook, analogous to considering a text document as a linear list of keywords. Generalizing upon text-based information retrieval methods, we then offer various techniques for image-based information retrieval. By comparing the performance of this approach with conventional techniques using color and texture features, we demonstrate the effectiveness of the keyblock-based approach to content-based image retrieval. © 2002 ACM.",Clustering; Codebook; Content-based image retrieval; Keyblock,Block codes; Content based retrieval; Indexing (of information); Information analysis; Query languages; Syntactics; Content based image retrieval; Keyblocks; Text based information; Text documents; Image retrieval
Collection statistics for fast duplicate document detection,2002,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0013206133&doi=10.1145%2f506309.506311&partnerID=40&md5=89905234da4c8e6da3eede9da3524830,"We present a new algorithm for duplicate document detection that uses collection statistics. We compare our approach with the state-of-the-art approach using multiple collections. These collections include a 30 MB 18,577 web document collection developed by Excite@Home and three NIST collections. The first NIST collection consists of 100 MB 18,232 LA-Times documents, which is roughly similar in the number of documents to the Excite@Home collection. The other two collections are both 2 GB and are the 247,491-web document collection and the TREC disks 4 and 5-528,023 document collection. We show that our approach called I-Match, scales in terms of the number of documents and works well for documents of all sizes. We compared our solution to the state of the art and found that in addition to improved accuracy of detection, our approach executed in roughly one-fifth the time. © 2002 ACM.",,Data recording; Information technology; Magnetic disk storage; Statistics; World Wide Web; Collection statistics; Data collection; Web documents; Information retrieval systems
An intelligent approach to handling imperfect information in concept-based natural language queries,2002,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-24644489002&doi=10.1145%2f568727.568729&partnerID=40&md5=ca82501908eb44452caf014cd09b87e9,"Missing information, imprecision, inconsistency, vagueness, uncertainty, and ignorance abound in information systems. Such imperfection is a fact of life in database systems. Although these problems are widely studied in relational database systems, this is not the case in conceptual query systems. And yet, concept-based query languages have been proposed and some are already commercial products. It is therefore imperative to study these problems in concept-based query languages, with a view to prescribing formal approaches to dealing with the problems. In this article, we have done just that for a concept-based natural language query system that we developed. A methodology for handling and resolving each type of imperfection is developed. The proposed approaches are automated as much as possible, with the user mainly serving an assistive function. © 2002 ACM.",Anaphoric query; Concept-based query; Conceptual query language; Elliptical query; Imperfect queries; Incomplete information; Inconsistency; Inexplicit query; Missing information; Natural language interface; Natural language query; Semantically mismatched query,Automation; Database systems; Formal languages; Intelligent agents; Problem solving; Query languages; Concept based queries; Conceptual query languages; Elliptical queries; Imperfect queries; Incomplete information; Inexplicit queries; Missing information; Natural language interfaces; Natural language queries; Semantically mismatched queries; Information retrieval
When experts agree: Using non-affiliated experts to rank popular topics,2002,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0344029640&doi=10.1145%2f503104.503107&partnerID=40&md5=c0570d42b28a7d11d36988d24200c2bb,"In response to a query, a search engine returns a ranked list of documents. If the query is about a popular topic (i.e., it matches many documents), then the returned list is usually too long to view fully. Studies show that users usually look at only the top 10 to 20 results. However, we can exploit the fact that the best targets for popular topics are usually linked to by enthusiasts in the same domain. In this paper, we propose a novel ranking scheme for popular topics that places the most authoritative pages on the query topic at the top of the ranking. Our algorithm operates on a special index of ""expert documents."" These are a subset of the pages on the WWW identified as directories of links to non-affiliated sources on specific topics. Results are ranked based on the match between the query and relevant descriptive text for hyperlinks on expert pages pointing to a given result page. We present a prototype search engine that implements our ranking scheme and discuss its performance. With a relatively small (2.5 million page) expert index, our algorithm was able to perform comparably on popular queries with the best of the mainstream search engines. © 2002 ACM.",Authorities; Connectivity; Host affiliation; Link analysis; Ranking; Topic experts; WWW search,Algorithms; Information retrieval systems; Query languages; Search engines; Software prototyping; Telecommunication links; Host affiliation; Link analysis; Topic experts; World Wide Web search; Expert systems
PicASHOW: Pictorial authority search by hyperlinks on the Web,2002,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-10444225728&doi=10.1145%2f503104.503105&partnerID=40&md5=3d37f10d15404d9666b2c11ec45eb73a,"We describe PicASHOW, a fully automated WWW image retrieval system that is based on several link-structure analyzing algorithms. Our basic premise is that a page p displays (or links to) an image when the author of p considers the image to be of value to the viewers of the page. We thus extend some well known link-based WWW page retrieval schemes to the context of image retrieval. PicASHOWs analysis of the link structure enables it to retrieve relevant images even when those are stored in files with meaningless names. The same analysis also allows it to identify image containers and image hubs. We define these as Web pages that are rich in relevant images, or from which many images are readily accessible. PicASHOW requires no image analysis whatsoever and no creation of taxonomies for preclassification of the Web's images. It can be implemented by standard WWW search engines with reasonable overhead, in terms of both computations and storage, and with no change to user query formats. It can thus be used to easily add image retrieving capabilities to standard search engines. Our results demonstrate that PicASHOW, while relying almost exclusively on link analysis, compares well with dedicated WWW image retrieval systems. We conclude that link analysis, a proven effective technique for Web page search, can improve the performance of Web image retrieval, as well as extend its definition to include the retrieval of image hubs and containers. © 2002 ACM.",Hubs and authorities; Image hubs; Image retrieval; Link structure analysis,Algorithms; Automation; Codes (symbols); Data structures; Image analysis; Search engines; Telecommunication links; World Wide Web; Image hubs; Link analysis; Link structure analysis; Image retrieval
Placing search in context: The concept revisited,2002,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0344029639&doi=10.1145%2f503104.503110&partnerID=40&md5=86065c56e57227c14ea8bd608a5e9b8c,"Keyword-based search engines are in widespread use today as a popular means for Web-based information retrieval. Although such systems seem deceptively simple, a considerable amount of skill is required in order to satisfy non-trivial information needs. This paper presents a new conceptual paradigm for performing search in context, that largely automates the search process, providing even non-professional users with highly relevant results. This paradigm is implemented in practice in the IntelliZap system, where search is initiated from a text query marked by the user in a document she views, and is guided by the text surrounding the marked query in that document (""the context""). The context-driven information retrieval process involves semantic keyword extraction and clustering to automatically generate new, augmented queries. The latter are submitted to a host of general and domain-specific search engines. Search results are then semantically reranked, using context. Experimental results testify that using context to guide search, effectively offers even inexperienced users an advanced search tool on the Web. © 2002 ACM.",Context; Invisible web; Search; Semantic processing; Statistical natural language processing,Automation; Context sensitive grammars; Information retrieval; Query languages; Semantics; User interfaces; Invisible webs; Keyword based search engines; Semantic processing; Statistical natural language processing; Search engines
Query clustering using user logs,2002,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0038546532&doi=10.1145%2f503104.503108&partnerID=40&md5=e5f3e844de91068634753a12bb3250e0,"Query clustering is a process used to discover frequently asked questions or most popular topics on a search engine. This process is crucial for search engines based on question-answering. Because of the short lengths of queries, approaches based on keywords are not suitable for query clustering. This paper describes a new query clustering method that makes use of user logs which allow us to identify the documents the users have selected for a query. The similarity between two queries may be deduced from the common documents the users selected for them. Our experiments show that a combination of both keywords and user logs is better than using either method alone. © 2002 ACM.",Query clustering; Search engine; User log; Web data mining,Identification (control systems); Information retrieval systems; Information technology; Security systems; User interfaces; Query clustering; User logs; Web data mining; Query languages
Knowledge encapsulation for focused search from pervasive devices,2002,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0042895714&doi=10.1145%2f503104.503106&partnerID=40&md5=9e04b4b952eeb35669aa2cb12c948864,"Mobile knowledge seekers often need access to information on the Web during a meeting or on the road, while away from their desktop. A common practice today is to use pervasive devices such as Personal Digital Assistants or mobile phones. However, these devices have inherent constraints (e.g., slow communication, form factor) which often make information discovery tasks impractical. In this paper, we present a new focused-search approach specifically oriented for the mode of work and the constraints dictated by pervasive devices. It combines focused search within specific topics with encapsulation of topic-specific information in a persistent repository. One key characteristic of these persistent repositories is that their footprint is small enough to fit on local devices, and yet they are rich enough to support many information discovery tasks in disconnected mode. More specifically, we suggest a representation for topic-specific information based on ""knowledgeagent bases"" that comprise all the information necessary to access information about a topic (under the form of key concepts and key Web pages) and assist in the full search process from query formulation assistance to result scanning on the device itself. The key contribution of our work is the coupling of focused search with encapsulated knowledge representation making information discovery from pervasive devices practical as well as efficient. We describe our model in detail and demonstrate its aspects through sample scenarios. © 2002 ACM.",Disconnected search; Focused searches; Knowledge agents; Pervasive devices,Constraint theory; Encapsulation; Information analysis; Personal digital assistants; Search engines; World Wide Web; Disconnected searches; Focused searches; Knowledge agents; Pervasive devices; Knowledge based systems
Efficient Web browsing on handheld devices using page and form summarization,2002,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0242674476&doi=10.1145%2f503104.503109&partnerID=40&md5=ede2f7f7e90c8032b60e4fce00baf164,"We present a design and implementation for displaying and manipulating HTML pages on small handheld devices such as personal digital assistants (PDAs), or cellular phones. We introduce methods for summarizing parts of Web pages and HTML forms. Each Web page is broken into text units that can each be hidden, partially displayed, made fully visible, or summarized. A variety of methods are introduced that summarize the text units. In addition, HTML forms are also summarized by displaying just the text labels that prompt the use for input. We tested the relative performance of the summarization methods by asking human subjects to accomplish single-page information search tasks. We found that the combination of keywords and single-sentence summaries provides significant improvements in access times and number of required pen actions, as compared to other schemes. Our experiments also show that our algorithms can identify the appropriate labels for forms in 95% of the cases, allowing effective form support for small screens. © 2002 ACM.",Forms; Handheld computers; Mobile computing; PDA; Personal digital assistant; Summarization; Ubiquitous computing; WAP; Wireless computing; WML,Algorithms; Computer aided design; Hand held computers; HTML; Personal digital assistants; Security systems; Websites; Cellular phones; Single page information search; Ubiquitous computing; Web browsers
Query-based sampling of text databases,2002,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0002104204&doi=10.1145%2f382979.383040&partnerID=40&md5=c1052df550780b3c58c85d4212ffeff8,"The proliferation of searchable text databases on corporate networks and the Internet causes a database selection problem for many people. Algorithms such as gGlOSS and CORI can automatically select which text databases to search for a given information need, but only if given a set of resource descriptions that accurately represent the contents of each database. The existing techniques for acquiring resource descriptions have significant limitations when used in wide-area networks controlled by many parties. This paper presents query-based sampling, a new technique for acquiring accurate resource descriptions. Query-based sampling does not require the cooperation of resource providers, nor does it require that resource providers use a particular search engine or representation technique. An extensive set of experimental results demonstrates that accurate resource descriptions are created, that computation and communication costs are reasonable, and that the resource descriptions do in fact enable accurate automatic database selection.",Algorithms; Design; Distributed information retrieval; Experimentation; Query-based sampling; Resource ranking; Resource selection; Server selection,
Complete answer aggregates for tree-like databases: A novel approach to combine querying and navigation,2002,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0008789639&doi=10.1145%2f382979.383042&partnerID=40&md5=39bd82d18cbec973fe51af092ff5a152,"The use of markup languages like SGML, HTML, or XML for encoding the structure of documents or linguistic data has lead to many databases where entries are adequately described as trees. In this context querying formalisms are interesting that offer the possibility to refer both to textual content and logical structure. We consider models where the structure specified in a query is not only used as a filter, but also for selecting and presenting different parts of the data. If answers are formalized as mappings from query nodes to the database, a simple enumeration of all mappings in the answer set will often suffer from the effect that many answers have common subparts. From a theoretical point of view this may lead to an exponential time complexity of the computation and presentation of all answers. Concentrating on the language of so-called tree queries - a variant and extension of Kilpeläinen's Tree Matching formalism - we introduce the notion of a ""complete answer aggregate"" for a given query. This new data structure offers a compact view of the set of all answers and supports active exploration of the answer space. Since complete answer aggregates use a powerful structure-sharing mechanism their maximal size is of order script O sign(d · h · q) where d and q respectively denote the size of the database and the query, and h is the maximal depth of a path of the database. An algorithm is given that computes a complete answer aggregate for a given tree query in time script O sign(d · log(d) · h · q). For the sublanguage of so-called rigid tree queries, as well as for so-called ""nonrecursive"" databases, an improved bound of script O sign(d · log(d) · q) is obtained. The algorithm is based on a specific index structure that supports practical efficiency.",Algorithms; Answer presentation; Design; Information retrieval; Logic; Query languages; Semistructured data; SGML; Structured documents; Tree databases; Tree matching; XML,
SALSA: The stochastic approach for link-structure analysis,2002,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0001313149&doi=10.1145%2f382979.383041&partnerID=40&md5=8dfdc059ae1fc1cb03943e509e205427,"Today, when searching for information on the WWW, one usually performs a query through a term-based search engine. These engines return, as the query's result, a list of Web pages whose contents matches the query. For broad-topic queries, such searches often result in a huge set of retrieved documents, many of which are irrelevant to the user. However, much information is contained in the link-structure of the WWW. Information such as which pages are linked to others can be used to augment search algorithms. In this context, Jon Kleinberg introduced the notion of two distinct types of Web pages: hubs and authorities. Kleinberg argued that hubs and authorities exhibit a mutually reinforcing relationship: a good hub will point to many authorities, and a good authority will be pointed at by many hubs. In light of this, he devised an algorithm aimed at finding authoritative pages. We present SALSA, a new stochastic approach for link-structure analysis, which examines random walks on graphs derived from the link-structure. We show that both SALSA and Kleinberg's Mutual Reinforcement approach employ the same metaalgorithm. We then prove that SALSA is equivalent to a weighted in-degree analysis of the link-structure of WWW subgraphs, making it computationally more efficient than the Mutual Reinforcement approach. We compare the results of applying SALSA to the results derived through Kleinberg's approach. These comparisons reveal a topological phenomenon called the TKC Effect which, in certain cases, prevents the Mutual Reinforcement approach from identifying meaningful authorities.",Algorithms; Experimentation; Hubs and authorities; Link-structure analysis; Random walks; SALSA; Theory; TKC Effect,
Genre taxonomy: A knowledge repository of communicative actions,2001,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0040076216&doi=10.1145%2f502795.502798&partnerID=40&md5=1274a4f9fb549e2ae8360f6634611246,"We propose a genre taxonomy as a knowledge repository of communicative structures or ""typified actions"" enacted by organizational members. The genre taxonomy is intended to help people make sense of diverse types of communicative actions and provide ideas for improving work processes that coordinate the communication of information. It engages several features to achieve this objective. First, the genre taxonomy represents the elements of both genres and genre systems as embedded in a social context reflecting the communicative questions why, what, who, when, where, and how (5W1H). In other words, the genre taxonomy represents the purpose, content, participants, timing, location, and form of communicative action. Second, the genre taxonomy distinguishes between widely recognized genres such as a report and specific genres such as a particular company's technical report, because the difference sheds light on the context of genre use. Third, the genre taxonomy represents use and evolution of a genre over time to help people understand how a genre is used and changed by a community over time. Fourth, the genre taxonomy represents aspects of information coordination via genres, thus providing ideas for improving work processes using genres. We have constructed a prototype of such a genre taxonomy using the Process Handbook, a process knowledge repository developed at MIT. We have included both widely recognized genres such as the memo and specific genres such as those used in the Process Handbook itself. We suggest that this genre taxonomy may be useful in the innovation of new document templates or methods for communication because it helps to clarify different possible uses of similar genres and explicates how genres play a coordination role among people and between people and their tasks. © 2001 ACM.",H.1.2 [Models and Principles]: User/Machine Systems - human in formation processing; J.1 [Administrative Data Processing]: business; J.4 [Social and Behavioral Sciences]: sociology,
Computing graphical queries over XML data,2001,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0010404515&doi=10.1145%2f502795.502797&partnerID=40&md5=04a4ff5115e5599dae7c22778d9e7cdf,"The rapid evolution of XML from a mere data exchange format to a universal syntax for encoding domain-specific information raises the need for new query languages specifically conceived to address the characteristics of XML. Such languages should be able not only to extract information from XML documents, but also to apply powerful transformation and restructuring operators, based on a well-defined semantics. Moreover, XML queries should be natural to write and understand, as nontechnical persons also are expected to access the large XML information bases supporting their businesses. This article describes XML-GL, a graphical query language for XML data. XML-GL's uniqueness is in the definition of a graph-based syntax to express a wide variety of XML queries, ranging from simple selections to expressive data transformations involving grouping, aggregation, and arithmetic calculations. XML-GL has an operational semantics based on the notion of graph matching, which serves as a guideline both for the implementation of native processors, and for the adoption of XML-GL as a front-end to any of the XML query languages that are presently under discussion as the standard paradigm for querying XML data. © 2001 ACM.",Document restructuring; Graphical query languages; II.2.3 [Information Systems]: Languages - XML documents; II.3.3 [Information Systems]: Information Storage and Retrieval; Languages; Semantics,
Approximate spatio-temporal retrieval,2001,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0038895731&doi=10.1145%2f366836.366874&partnerID=40&md5=0c45c7c6a7929c813efd19e75e5d1008,"This paper proposes a framework for the handling of spatio-temporal queries with inexact matches, using the concept of relation similarity. We initially describe a binary string encoding for 1D relations that permits the automatic derivation of similarity measures. We then extend this model to various granularity levels and many dimensions, and show that reasoning on spatio-temporal structure is significantly facilitated in the new framework. Finally, we provide algorithms and optimization methods for four types of queries: (i) object retrieval based on some spatio-temporal relations with respect to a reference object, (ii) spatial joins, i.e., retrieval of object pairs that satisfy some input relation, (iii) structural queries, which retrieve configurations matching a particular spatio-temporal structure, and (iv) special cases of motion queries. Considering the current large availability of multidimensional data and the increasing need for flexible query-answering mechanisms, our techniques can be used as the core of spatio-temporal query processors. © 2001 ACM.",H.2.2 [Database Management]: Physical Design - Access methods; H.2.4 [Database Management]: Systems - Multimedia databases; H.2.8 [Database Management]: Database applications - Spatial databases and GIS,
A statechart-based model for hypermedia applications,2001,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0039488530&doi=10.1145%2f366836.366869&partnerID=40&md5=6370b9b4c6aca377982048709f3125c4,"This paper presents a formal definition for HMBS (Hypermedia Model Based on Statecharts). HMBS uses the structure and execution semantics of statecharts to specify both the structural organization and the browsing semantics of hypermedia applications. Statecharts are an extension of finite-state machines and the model is thus a generalization of hypergraph-based hypertext models. Some of the most important features of HMBS are its ability to model hierarchy and synchronization of information; provision of mechanisms for specifying access structures, navigational contexts, access control, multiple tailored versions, and hierarchical views. Analysis of the underlying statechart machine allows verification of page reachability, valid paths, and other properties, thus providing mechanisms to support authors in the development of structured applications. © 2001 ACM.",F.1.1 [Computation by Abstract Devices]: Models of Computation - Relations among models; I.7.2 [Document and Text Processing]: Document Preparation - Hypertext/hypermedia,
WebQuilt: A proxy-based approach to remote web usability testing,2001,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0012878521&doi=10.1145%2f502115.502118&partnerID=40&md5=2b8da9d7e436a77a830ed0b6c2953748,"WebQuilt is a web logging and visualization system that helps web design teams run usability tests (both local and remote) and analyze the collected data. Logging is done through a proxy, overcoming many of the problems with server-side and client-side logging. Captured usage traces can be aggregated and visualized in a zooming interface that shows the web pages people viewed. The visualization also shows the most common paths taken through the web site for a given task, as well as the optimal path for that task, as designated by the designer. This paper discusses the architecture of WebQuilt and describes how it can be extended for new kinds of analyses and visualizations. © 2001 ACM.",H.1.2 [Models and Principles]: User/Machine Systems - Human factors; H.3.5 [Information Storage and Retrieval]: Online Information Services - Web-based services; H.5.2 [Information Interfaces and Presentation]: User Interfaces - Evaluation/methodology,
Building a distributed full-text index for the web,2001,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0038895148&doi=10.1145%2f502115.502116&partnerID=40&md5=5a715a5772a81dcae8144fa8657874df,"We identify crucial design issues in building a distributed inverted index for a large collection of Web pages. We introduce a novel pipelining technique for structuring the core index-building system that substantially reduces the index construction time. We also propose a storage scheme for creating and managing inverted files using an embedded database system. We suggest and compare different strategies for collecting global statistics from distributed inverted indexes. Finally, we present performance results from experiments on a testbed distributed Web indexing system that we have implemented. © 2001 ACM.",Design; Distributed indexing; H.3.1 [Information Storage and Retrieval]: Content Analysis and Indexing - Indexing methods; H.3.4 [Information Storage and Retrieval]: Systems and Software - Distributed systems; Inverted files; Performance; Text retrieval,
A highly scalable and effective method for metasearch,2001,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0038895143&doi=10.1145%2f502115.502120&partnerID=40&md5=caa0a75ddd1373f9681a588cd277dbb7,"A metasearch engine is a system that supports unified access to multiple local search engines. Database selection is one of the main challenges in building a large-scale metasearch engine. The problem is to efficiently and accurately determine a small number of potentially useful local search engines to invoke for each user query. In order to enable accurate selection, metadata that reflect the contents of each search engine need to be collected and used. This article proposes a highly scalable and accurate database selection method. This method has several novel features. First, the metadata for representing the contents of all search engines are organized into a single integrated representative. Such a representative yields both computational efficiency and storage efficiency. Second, the new selection method is based on a theory for ranking search engines optimally. Experimental results indicate that this new method is very effective. An operational prototype system has been built based on the proposed approach. © 2001 ACM.",C.2.4 [Computer-Communication Networks]: Distributed Systems - Distributed databases; H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval - Search process; Selection process,
Application of aboutness to functional benchmarking in information retrieval,2001,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0012348259&doi=10.1145%2f502795.502796&partnerID=40&md5=58333f8e1da99649c181ed1f382e6a19,"Experimental approaches are widely employed to benchmark the performance of an information retrieval (IR) system. Measurements in terms of recall and precision are computed as performance indicators. Although they are good at assessing the retrieval effectiveness of an IR system, they fail to explore deeper aspects such as its underlying functionality and explain why the system shows such performance. Recently, inductive (i.e., theoretical) evaluation of IR systems has been proposed to circumvent the controversies of the experimental methods. Several studies have adopted the inductive approach, but they mostly focus on theoretical modeling of IR properties by using some metalogic. In this article, we propose to use inductive evaluation for functional benchmarking of IR models as a complement of the traditional experiment-based performance benchmarking. We define a functional benchmark suite in two stages: the evaluation criteria based on the notion of ""aboutness,"" and the formal evaluation methodology using the criteria. The proposed benchmark has been successfully applied to evaluate various well-known classical and logic-based IR models. The functional benchmarking results allow us to compare and analyze the functionality of the different IR models. © 2001 ACM.",H.1.1 [Models and Principles]: Systems and Information Theory; H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval - retrieval models; Search process; Selection process,
Scaling question answering to the web,2001,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0040673186&doi=10.1145%2f502115.502117&partnerID=40&md5=06d3603517c766277e2eb306605ff0f1,"The wealth of information on the web makes it an attractive resource for seeking quick answers to simple, factual questions such as ""who was the first American in space?"" or ""what is the second tallest mountain in the world?"" Yet today's most advanced web search services (e.g., Google and AskJeeves) make it surprisingly tedious to locate answers to such questions. In this paper, we extend question-answering techniques, first studied in the information retrieval literature, to the web and experimentally evaluate their performance. First we introduce MULDER, which we believe to be the first general-purpose, fully-automated question-answering system available on the web. Second, we describe MULDER'S architecture, which relies on multiple search-engine queries, natural-language parsing, and a novel voting procedure to yield reliable answers coupled with high recall. Finally, we compare MULDER'S performance to that of Google and AskJeeves on questions drawn from the TREC-8 question answering track. We find that MULDER'S recall is more than a factor of three higher than that of AskJeeves. In addition, we find that Google requires 6.6 times as much user effort to achieve the same level of recall as MULDER. © 2001 ACM.",Algorithms; Answer extraction; Answer selection; Design; Experimentation; H.4.m [Information Systems Applications]: Miscellaneous; Human Factors; Languages; Natural language processing; Performance; Query formulation; Search engines,
On the design of a learning crawler for topical resource discovery,2001,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0040079611&doi=10.1145%2f502115.502119&partnerID=40&md5=0594f78db80af376293e98d94b523b67,"In recent years, the World Wide Web has shown enormous growth in size. Vast repositories of information are available on practically every possible topic. In such cases, it is valuable to perform topical resource discovery effectively. Consequently, several new ideas have been proposed in recent years; among them a key technique is focused crawling which is able to crawl particular topical portions of the World Wide Web quickly, without having to explore all web pages. In this paper, we propose the novel concept of intelligent crawling which actually learns characteristics of the linkage structure of the World Wide Web while performing the crawling. Specifically, the intelligent crawler uses the inlinking web page content, candidate URL structure, or other behaviors of the inlinking web pages or siblings in order to estimate the probability that a candidate is useful for a given crawl. This is a much more general framework than the focused crawling technique which is based on a pre-defined understanding of the topical structure of the web. The techniques discussed in this paper are applicable for crawling web pages which satisfy arbitrary user-defined predicates such as topical queries, keyword queries, or any combinations of the above. Unlike focused crawling, it is not necessary to provide representative topical examples, since the crawler can learn its way into the appropriate topic. We refer to this technique as intelligent crawling because of its adaptive nature in adjusting to the web page linkage structure. We discuss how to intelligently select features which are most useful for a given crawl. The learning crawler is capable of reusing the knowledge gained in a given crawl in order to provide more efficient crawling for closely related predicates. © 2001 ACM.",Algorithms; Crawling; Experimentation; H.2.8 [Database Management]: Database Applications - Data Mining; World Wide Web,
An information-theoretic approach to automatic query expansion,2001,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0002272569&doi=10.1145%2f366836.366860&partnerID=40&md5=57738e30c1e3af67e288c5a757c9c1bd,"Techniques for automatic query expansion from top retrieved documents have shown promise for improving retrieval effectiveness on large collections; however, they often rely on an empirical ground, and there is a shortage of cross-system comparisons. Using ideas from Information Theory, we present a computationally simple and theoretically justified method for assigning scores to candidate expansion terms. Such scores are used to select and weight expansion terms within Rocchio's framework for query reweighting. We compare ranking with information-theoretic query expansion versus ranking with other query expansion techniques, showing that the former achieves better retrieval effectiveness on several performance measures. We also discuss the effect on retrieval effectiveness of the main parameters involved in automatic query expansion, such as data sparseness, query difficulty, number of selected documents, and number of selected terms, pointing out interesting relationships. © 2001 ACM.",Algorithms; Design; H.3.1 [Information Storage and Retrieval]: Content Analysis and Indexing - Indexing methods; H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval - Retrieval models; Query formulation; Relevance feedback,
Beneath the surface of organizational processes: A social representation framework for business process redesign,2000,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0012692974&doi=10.1145%2f358108.358111&partnerID=40&md5=d8dd113d72f0bee2860bfb54cb507954,"This paper raises the question, ""What is an effective representation framework for organizational process redesign?"" By combining our knowledge of existing process models with data from a field study, the paper develops criteria for an effective process representation. Using these criteria and the case study, the paper integrates the process redesign and information system literatures to develop a representation framework that captures a process' social context. The paper argues that this social context framework, which represents people's motivations, social relationships, and social constraints, gives redesigners a richer sense of the process and allows process redesigners to simultaneously change social and logistic systems. The paper demonstrates the framework and some of its benefits and limitations.",Business process redesign; Organizational change; Process representation,
Evaluating the performance of distributed architectures for information retrieval using a variety of workloads,2000,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0003214715&doi=10.1145%2f333135.333136&partnerID=40&md5=3cf50ef826ca6379d47e482c1ab79b59,"The information explosion across the Internet and elsewhere offers access to an increasing number of document collections. In order for users to effectively access these collections, information retrieval (IR) systems must provide coordinated, concurrent, and distributed access. In this article, we explore how to achieve scalable performance in a distributed system for collection sizes ranging from 1GB to 128GB. We implement a fully functional distributed IR system based on a multithreaded version of the Inquery unified IR system. To explore the design space more fully, we also implement and validate a flexible simulation model. We measure performance as a function of system parameters such as client command rate, number of document collections, terms per query, query term frequency, number of answers returned, and command mixture. Our results show that it is important to model both query and document commands because the heterogeneity of commands significantly impacts performance. Based on our results, we recommend simple changes to the prototype and evaluate the changes using the simulator. Because of the significant resource demands of information retrieval, it is not difficult to generate workloads that overwhelm system resources regardless of the architecture. However under some realistic workloads, we demonstrate system organizations for which response time gracefully degrades as the workload increases and performance scales with the number of processors. This scalable architecture includes a surprisingly small number of brokers through which a large number of clients and servers communicate.",,
Chimera: Hypermedia for heterogeneous software development environments,2000,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0003303121&doi=10.1145%2f352595.352596&partnerID=40&md5=b13edf4f4da19b568b0c77cee426bca4,"Emerging software development environments are characterized by heterogeneity: they are composed of diverse object stores, user interfaces, and tools. This paper presents an approach for providing hypermedia services in this heterogeneous setting. Central notions of the approach include the following: anchors are established with respect to interactive views of objects rather than the objects themselves; composable, n-ary links can be established between anchors on different views of objects which may be stored in distinct object bases; viewers may be implemented in different programming languages; and, hypermedia services are provided to multiple, concurrently active, viewers. The paper describes the approach, supporting architecture, and lessons learned. Related work in the areas of supporting heterogeneity and hypermedia data modeling is discussed. The system has been employed in a variety of contexts including research, development, and education.",D.2.2 [Software Engineering]: Tools and Techniques; Design; H.5.1 [Multimedia Information Systems]; Heterogeneous hypermedia; Hypermedia system architectures; I.7.2 [Document Preparation]: Hypertext/Hypermedia; Link servers; Open hypermedia system,
Improving the effectiveness of information retrieval with local context analysis,2000,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0003029479&doi=10.1145%2f333135.333138&partnerID=40&md5=05f9281725e22b39ee65ac826e0236f8,"Techniques for automatic query expansion have been extensively studied in information retrieval research as a means of addressing the word mismatch between queries and documents. These techniques can be categorized as either global or local. While global techniques rely on analysis of a whole collection to discover word relationships, local techniques emphasize analysis of the top-ranked documents retrieved for a query. While local techniques have shown to be more effective than global techniques in general, existing local techniques are not robust and can seriously hurt retrieval when few of the retrieved documents are relevant. We propose a new technique, called local context analysis, which selects expansion terms based on cooccurrence with the query terms within the top-ranked documents. Experiments on a number of collections, both English and non-English, show that local context analysis offers more effective and consistent retrieval results.",H.3.1 [Information Storage and Retrieval]: Content Analysis and Indexing-Indexing methods; H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval-Query formulation; Linguistic processing; Relevance; Search process; Thesauruses,
The maximum entropy approach and probabilistic IR models,2000,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0010551427&doi=10.1145%2f352595.352597&partnerID=40&md5=def7e578197f0c691aa313fdf524c05b,"This paper takes a fresh look at modeling approaches to information retrieval that have been the basis of much of the probabilistically motivated IR research over the last 20 years. We shall adopt a subjectivist Bayesian view of probabilities and argue that classical work on probabilistic retrieval is best understood from this perspective. The main focus of the paper will be the ranking formulas corresponding to the Binary Independence Model (BIM), presented originally by Roberston and Sparck Jones [1977] and the Combination Match Model (CMM), developed shortly thereafter by Croft and Harper [1979]. We will show how these same ranking formulas can result from a probabilistic methodology commonly known as Maximum Entropy (MAXENT).",Binary independence model; Combination match; H.3.3 [Information Storage and Retrieval]; Idf weighting; Information Search and Retrieval - Retrieval models; Linked dependence; Probability ranking principle; Theory,
Beyond intratransaction association analysis: Mining multidimensional intertransaction association rules,2000,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0013024811&doi=10.1145%2f358108.358114&partnerID=40&md5=9538c82111d31bade40885396ab899b8,"In this paper, we extend the scope of mining association rules from traditional single-dimensional intratransaction associations, to multidimensional intertransaction associations. Intratransaction associations are the associations among items within the same transaction, where the notion of the transaction could be the items bought by the same customer, the events happened on the same day, and so on. However, an intertransaction association describes the association relationships among different transactions, such as ""if (company) A's stock goes up on day 1, B's stock will go down on day 2, but go up on day 4."" In this case, whether we treat company or day as the unit of transaction, the associated items belong to different transactions. Moreover, such an intertransaction association can be extended to associate multiple contextual properties in the same rule, so that multidimensional intertransaction associations can be defined and discovered. A two-dimensional intertransaction association rule example is ""After McDonald and Burger King open branches, KFC will open a branch two months later and one mile away,"" which involves two dimensions: time and space. Mining intertransaction associations poses more challenges on efficient processing than mining intratransaction associations. Interestingly, intratransaction association can be treated as a special case of intertransaction association from both a conceptual and algorithmic point of view. In this study, we introduce the notion of multidimensional intertransaction association rules, study their measurements-support and confidence- and develop algorithms for mining intertransaction associations by extension of Apriori. We overview our experience using the algorithms on both real-life and synthetic data sets. Further extensions of multidimensional intertransaction association rules and potential applications are also discussed.",Data mining; Intra/intertransaction association rules; Multidimensional context,
Shortest-substring retrieval and ranking,2000,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0002977272&doi=10.1145%2f333135.333137&partnerID=40&md5=1b2444ca0215e7e12ce14b86762981c6,"We present a model for arbitrary passage retrieval using Boolean queries. The model is applied to the task of ranking documents, or other structural elements, in the order of their expected relevance. Features such as phrase matching, truncation, and stemming integrate naturally into the model. Properties of Boolean algebra are obeyed, and the exact-match semantics of Boolean retrieval are preserved. Simple inverted-list file structures provide an efficient implementation. Retrieval effectiveness is comparable to that of standard ranking techniques. Since global statistics are not used, the method is of particular value in distributed environments. Since ranking is based on arbitrary passages, the structural elements to be ranked may be specified at query time and do not need to be restricted to predefined elements.",Algorithms; Boolean retrieval model; H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval; H.3.4 [Information Storage and Retrieval]: Systems and Software - Performance evaluation (efficiency and effectiveness); Passage; Performance,
Fast and flexible word searching on compressed text,2000,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0001009631&doi=10.1145%2f348751.348754&partnerID=40&md5=68657030eb758b58c6e93a0aa8ed86b4,"We present a fast compression and decompression technique for natural language texts. The novelties are that (1) decompression of arbitrary portions of the text can be done very efficiently, (2) exact search for words and phrases can be done on the compressed text directly, using any known sequential pattern-matching algorithm, and (3) word-based approximate and extended search can also be done efficiently without any decoding. The compression scheme uses a semistatic word-based model and a Huffman code where the coding alphabet is byte-oriented rather than bit-oriented. We compress typical English texts to about 30% of their original size, against 40% and 35% for Compress and Gzip, respectively. Compression time is close to that of Compress and approximately half the time of Gzip, and decompression time is lower than that of Gzip and one third of that of Compress. We present three algorithms to search the compressed text. They allow a large number of variations over the basic word and phrase search capability, such as sets of characters, arbitrary regular expressions, and approximate matching. Separators and stopwords can be discarded at search time without significantly increasing the cost. When searching for simple words, the experiments show that running our algorithms on a compressed text is twice as fast as running the best existing software on the uncompressed version of the same text. When searching complex or approximate patterns, our algorithms are up to 8 times faster than the search on uncompressed text. We also discuss the impact of our technique in inverted files pointing to logical blocks and argue for the possibility of keeping the text compressed all the time, decompressing only for displaying purposes.",Algorithms; Compressed pattern matching; Design; E.4 [Data]: Coding and Information Theory - Data compaction and compression; H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval - Search process; Natural language text compression,
Model-driven development of web applications: The autoweb system,2000,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0001161301&doi=10.1145%2f358108.358110&partnerID=40&md5=7e41f992b649e807b24b693b153d7ed6,"This paper describes a methodology for the development of WWW applications and a tool environment specifically tailored for the methodology. The methodology and the development environment are based upon models and techniques already used in the hypermedia, information systems, and software engineering fields, adapted and blended in an original mix. The foundation of the proposal is the conceptual design of WWW applications, using HDM-lite, a notation for the specification of structure, navigation, and presentation semantics. The conceptual schema is then translated into a ""traditional"" database schema, which describes both the organization of the content and the desired navigation and presentation features. The WWW pages can therefore be dynamically generated from the database content, following the navigation requests of the user. A CASE environment, called Autoweb System, offers a set of software tools, which assist the design and the execution of a WWW application, in all its different aspects. Real-life experiences of the use of the methodology and of the Autoweb System in both the industrial and academic context are reported.",Application; Development; HTML; Intranet; Modeling; WWW,
Extending document management systems with user-specific active properties,2000,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0000332818&doi=10.1145%2f348751.348758&partnerID=40&md5=8f2cadfb36efc8d8a5f8a7cb00206519,"Document properties are a compelling infrastructure on which to develop document management applications. A property-based approach avoids many of the problems of traditional hierarchical storage mechanisms, reflects document organizations meaningful to user tasks, provides a means to integrate the perspectives of multiple individuals and groups, and does this all within a uniform interaction framework. Document properties can reflect not only categorizations of documents and document use, but also expressions of desired system activity, such as sharing criteria, replication management, and versioning. Augmenting property-based document management systems with active properties that carry executable code enables the provision of document-based services on a property infrastructure. The combination of document properties as a uniform mechanism for document management, and active properties as a way of delivering document services, represents a new paradigm for document management infrastructures. The Placeless Documents system is an experimental prototype developed to explore this new paradigm. It is based on the seamless integration of user-specific, active properties. We present the fundamental design approach, explore the challenges and opportunities it presents, and show how our architecture deals with them.",C.2.4 [Computer-Communication Networks]: Distributed Systems - Distributed databases; D.4.3 [Operating Systems]: File Systems Management - Distributed file systems; E.5 [Data]: Files - Organization / structure; H.3.2 [Information Storage and Retrieval],
Efficient content-based indexing of large image databases,2000,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0000742555&doi=10.1145%2f348751.348762&partnerID=40&md5=87c14ee23bb303871d418c4557a91c53,"Large image databases have emerged in various applications in recent years. A prime requisite of these databases is the means by which their contents can be indexed and retrieved. A multilevel signature file called the Two Signature Multi-Level Signature File (2SMLSF) is introduced as an efficient access structure for large image databases. The 2SMLSF encodes image information into binary signatures and creates a tree structure that can be efficiently searched to satisfy a user's query. Two types of signatures are generated. Type I signatures are used at all tree levels except the leaf level and are based only on the domain objects included in the image. Type II signatures, on the other hand, are stored at the leaf level and are based on the included domain objects and their spatial relationships. The 2SMLSF was compared analytically to existing signature file techniques. The 2SMLSF significantly reduces the storage requirements; the index structure can answer more queries; and the 2SMLSF performance significantly improves over current techniques. Both storage reduction and performance improvement increase with the number of objects per image and the number of images in the database. For an example large image databases, a storage reduction of 78% may be achieved while the performance improvement may reach 98%.",Algorithms; Content analysis and indexing; Document managing; Documentation; H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval - Query formulation; Image databases; Index generation; Multimedia databases; Query formulation,
Data integration using similarity joins and a word-based information representation language,2000,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0000666461&doi=10.1145%2f352595.352598&partnerID=40&md5=c21166412194764c2e3a9492d8c26994,"The integration of distributed, heterogeneous databases, such as those available on the World Wide Web, poses many problems. Here we consider the problem of integrating data from sources that lack common object identifiers. A solution to this problem is proposed for databases that contain informal, natural-language ""names"" for objects; most Web-based databases satisfy this requirement, since they usually present their information to the end-user through a veneer of text. We describe WHIRL, a ""soft"" database management system which supports ""similarity joins,"" based on certain robust, general-purpose similarity metrics for text. This enables fragments of text (e.g., informal names of objects) to be used as keys. WHIRL includes textual objects as a built-in type, similarity reasoning as a built-in predicate, and answers every query with a list of answer substitutions that are ranked according to an overall score. Experiments show that WHIRL is much faster than naive inference methods, even for short queries, and efficient on typical queries to real-world databases with tens of thousands of tuples. Inferences made by WHIRL are also surprisingly accurate, equaling the accuracy of hand-coded normalization routines on one benchmark problem, and outperforming exact matching with a plausible global domain on a second.",H.2.3 [Information Systems]: Database Management - data manipulation languages; H.2.5 [Information Systems]: Database Management - heterogeneous databases; Query languages,
The equalizing impact of a group support system on status differentials,1999,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0010108189&doi=10.1145%2f297117.297126&partnerID=40&md5=85b4199eee7edee876197998e45c0a64,"This study investigates the impact of the electronic communication capability of a group support system (GSS) on status differentials in small groups. A laboratory experiment was used to answer the research questions. Three support levels were studied: manual, face-to-face GSS, and dispersed GSS. Two task types were examined: intellective and preference. Five dependent variables reflecting different aspects of status differentials were measured: status influence, sustained influence, residual disagreement, perceived influence, and decision confidence. The results show that manual groups had higher status influence, sustained influence, and decision confidence, but lower residual disagreement than face-to-face GSS and dispersed GSS groups. Preference task groups also produced higher status influence and sustained influence, but lower residual disagreement compared to intellective task groups. In addition, manual groups working on the preference task reported higher perceived influence than face-to-face GSS and dispersed GSS groups working on the same task. These findings suggest that when groups are engaged in activities for which status differentials are undesirable, a GSS can be used in both face-to-face and dispersed settings to dampen status differentials. Moreover, when a task amplifies status differentials, the use of a GSS tends to produce correspondingly stronger dampening effects.",Electronic communication; Group support systems; Management; Status differentials; Task type; Theory,
The impact on retrieval effectiveness of skewed frequency distributions,1999,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0039698761&doi=10.1145%2f326440.326447&partnerID=40&md5=16c2a76696607690c1903ee054926af6,"We present an analysis of word senses that provides a fresh insight into the impact of word ambiguity on retrieval effectiveness with potential broader implications for other processes of information retrieval. Using a methodology of forming artificially ambiguous words, known as pseudowords, and through reference to other researchers' work, the analysis illustrates that the distribution of the frequency of occurrence of the senses of a word plays a strong role in ambiguity's impact on effectiveness. Further investigation shows that this analysis may also be applicable to other processes of retrieval, such as Cross Language Information Retrieval, query expansion, retrieval of OCR'ed texts, and stemming. The analysis appears to provide a means of explaining, at least in part, reasons for the processes' impact (or lack of it) on effectiveness.",H.3.1 [Information Storage and Retrieval]: Content Analysis and Indexing - Linguistic processing; I.2.7 [Artificial Intelligence]: Natural Language Processing - Text analysis; I.6.4 [Simulation and Modeling]: Model Validation and Analysis,
PIC matrices: A computationally tractable class of probabilistic query operators,1999,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0009984026&doi=10.1145%2f326440.326444&partnerID=40&md5=131a8517a7ec021560bdbd366b494001,"The inference network model of information retrieval allows for a probabilistic interpretation of query operators. In particular, Boolean query operators are conveniently modeled as link matrices of the Bayesian Network. Prior work has shown, however, that these operators do not perform as well as the the pnorm operators used for modeling query operators in the context of the vector space model. This motivates the search for alternative probabilistic formulations for these operators. The design of such alternatives must contend with the issue of computational tractability, since the evaluation of an arbitrary operator requires exponential time. We define a flexible class of link matrices that are natural candidates for the implementation of query operators and an O(n2) algorithm (n = the number of parent nodes) for the computation of probabilities involving link matrices of this class. We present experimental results indicating that Boolean operators implemented in terms of link matrices from this class perform as well as pnorm operators in the context of the INQUERY inference network.",Algorithms; Bayesian Networks; Boolean queries; Computational complexity; H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval - Query formulation; Inference networks; Link matrices; Performance; Piecewise linear functions; Pnorm; Theory,
Predicate rewriting for translating Boolean queries in a heterogeneous information system,1999,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0040213195&doi=10.1145%2f297117.297120&partnerID=40&md5=ffb8e9d68f0fb4012f3bdda1d202cf43,"Searching over heterogeneous information sources is difficult in part because of the nonuniform query languages. Our approach is to allow users to compose Boolean queries in one rich front-end language. For each user query and target source, we transform the user query into a subsuming query that can be supported by the source but that may return extra documents. The results are then processed by a filter query to yield the correct final results. In this article we introduce the architecture and associated mechanism for query translation. In particular, we discuss techniques for rewriting predicates in Boolean queries into native subsuming forms, which is a basis of translating complex queries. In addition, we present experimental results for evaluating the cost of postfiltering. We also discuss the drawbacks of this approach and cases when it may not be effective. We have implemented prototype versions of these mechanisms and demonstrated them on heterogeneous Boolean systems.",Algorithms; Boolean queries; Content-based retrieval; Experimentation; Filtering; Languages; Measurement; Predicate rewriting; Query subsumption; Query translation,
A flexible authorization mechanism for relational data management systems,1999,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0040884892&doi=10.1145%2f306686.306687&partnerID=40&md5=1f5741e77d3f297232ed85ade5769c62,"In this article, we present an authorization model that can be used to express a number of discretionary access control policies for relational data management systems. The model permits both positive and negative authorizations and supports exceptions at the same time. The model is flexible in that the users can specify, for each authorization they grant, whether the authorization can allow for exceptions or whether it must be strongly obeyed. It provides authorization management for groups with exceptions at any level of the group hierarchy, and temporary suspension of authorizations. The model supports ownership together with decentralized administration of authorizations. Administrative privileges can also be restricted so that owners retain control over their tables.","Access control mechanism; Access control policy; Authorization; D.4.6 [Operating Systems]: Security and Protection -access controls; Data management system; H.2.0 [Database Management]: General - security, integrity, and protection; Security; Theory",
Integrating geometrical and linguistic analysis for email signature block parsing,1999,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0013282545&doi=10.1145%2f326440.326442&partnerID=40&md5=978c248c3e3ace5646120c939d78c70c,"The signature block is a common structured component found in email messages. Accurate identification and analysis of signature blocks is important in many multimedia messaging and information retrieval applications such as email text-to-speech rendering, automatic construction of personal address databases, and interactive message retrieval. It is also a very challenging task, because signature blocks often appear in complex two-dimensional layouts which are guided only by loose conventions. Traditional text analysis methods designed to deal with sequential text cannot handle two-dimensional structures, while the highly unconstrained nature of signature blocks makes the application of two-dimensional grammars very difficult. In this article, we describe an algorithm for signature block analysis which combines two-dimensional structural segmentation with one-dimensional grammatical constraints. The information obtained from both layout and linguistic analysis is integrated in the form of weighted finite-state transducers. The algorithm is currently implemented as a component in a preprocessing system for email text-to-speech rendering.",Algorithms; Email signature block; Finite-state transducer; H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval - Selection process; H.4.3 [Information Systems Applications]: Communications Applications - Electronic mail,
Harp: A distributed query system for legacy public libraries and structured databases,1999,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0040218513&doi=10.1145%2f314516.314521&partnerID=40&md5=df29d7bc80598f03c86eccd15277e732,"The main purpose of a digital library is to facilitate users easy access to enormous amount of globally networked information. Typically, this information includes preexisting public library catalog data, digitized document collections, and other databases. In this article, we describe the distributed query system of a digital library prototype system known as HARP. In the HARP project, we have designed and implemented a distributed query processor and its query front-end to support integrated queries to preexisting public library catalogs and structured databases. This article describes our experiences in the design of an extended Sequel (SQL) query language known as HarpSQL. It also presents the design and implementation of the distributed query system. Our experience in distributed query processor and user interface design and development will be highlighted. We believe that our prototyping effort will provide useful lessons to the development of a complete digital library infrastructure.",Digital libraries; Internet databases; Interoperable databases,
Efficient passage ranking for document databases,1999,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0000764262&doi=10.1145%2f326440.326445&partnerID=40&md5=2b60204b157c4c4af4b7b399235cde3a,"Queries to text collections are resolved by ranking the documents in the collection and returning the highest-scoring documents to the user. An alternative retrieval method is to rank passages, that is, short fragments of documents, a strategy that can improve effectiveness and identify relevant material in documents that are too large for users to consider as a whole. However, ranking of passages can considerably increase retrieval costs. In this article we explore alternative query evaluation techniques, and develop new techniques for evaluating queries on passages. We show experimentally that, appropriately implemented, effective passage retrieval is practical in limited memory on a desktop machine. Compared to passage ranking with adaptations of current document ranking algorithms, our new ""DO-TOS"" passage-ranking algorithm requires only a fraction of the resources, at the cost of a small loss of effectiveness.",Algorithms; E.5 [Data]: Files; H.2.2 [Database Management]: Physical Design; H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval; Inverted files; Passage retrieval; Performance; Query evaluation; Text databases; Text retrieval,
A corpus analysis approach for automatic query expansion and its extension to multiple databases,1999,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0001558224&doi=10.1145%2f314516.314519&partnerID=40&md5=215f13f2e721a0bae6374133194a41e5,"Searching online text collections can be both rewarding and frustrating. While valuable information can be found, typically many irrelevant documents are also retrieved, while many relevant ones are missed. Terminology mismatches between the user's query and document contents are a main cause of retrieval failures. Expanding a user's query with related words can improve search performance, but finding and using related words is an open problem. This research uses corpus analysis techniques to automatically discover similar words directly from the contents of the databases which are not tagged with part-of-speech labels. Using these similarities, user queries are automatically expanded, resulting in conceptual retrieval rather than requiring exact word matches between queries and documents. We are able to achieve a 7.6% improvement for TREC 5 queries and up to a 28.5% improvement on the narrow-domain Cystic Fibrosis collection. This work has been extended to multidatabase collections where each subdatabase has a collection-specific similarity matrix associated with it. If the best matrix is selected, substantial search improvements are possible. Various techniques to select the appropriate matrix for a particular query are analyzed, and a 4.8% improvement in the results is validated.",Query expansion,
Context interchange: New features and formalisms for the intelligent integration of information,1999,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0000876805&doi=10.1145%2f314516.314520&partnerID=40&md5=f3dd42b549bc74917f8806c13a4fa820,"The Context Interchange strategy presents a novel perspective for mediated data access in which semantic conflicts among heterogeneous systems are not identified a priori, but are detected and reconciled by a context mediator through comparison of contexts axioms corresponding to the systems engaged in data exchange. In this article, we show that queries formulated on shared views, export schema, and shared ""ontologies"" can be mediated in the same way using the Context Interchange framework. The proposed framework provides a logic-based object-oriented formalism for representing and reasoning about data semantics in disparate systems, and has been validated in a prototype implementation providing mediated data access to both traditional and web-based information sources.",Abductive reasoning; Information integration; Mediators; Semantic heterogeneity; Semantic interoperability,
Interface and data architecture for query preview in networked information systems,1999,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0000570664&doi=10.1145%2f314516.314522&partnerID=40&md5=7a52d48a0130ca5709041e5b1eaa8753,"There are numerous problems associated with formulating queries on networked information systems. These include increased data volume and complexity, accompanied by slow network access. This article proposes a new approach to a network query user interfaces that consists of two phases: query preview and query refinement. This new approach is based on the concepts of dynamic queries and query previews, which guides users in rapidly and dynamically eliminating undesired records, reducing the data volume to a manageable size, and refining queries locally before submission over a network. Examples of two applications are given: a Restaurant Finder and a prototype for NASA's Earth Observing Systems Data Information Systems (EOSDIS). Data architecture is discussed, and user feedback is presented.",Direct manipulation; Dynamic query; EOSDIS; Graphical user interface; Query preview; Query refinement; Science data,
A decision-theoretic approach to database selection in networked IR,1999,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0000650740&doi=10.1145%2f314516.314517&partnerID=40&md5=8e2da3f7a79a7e75c17ed2c36e390b29,"In networked IR, a client submits a query to a broker, which is in contact with a large number of databases. In order to yield a maximum number of documents at minimum cost, the broker has to make estimates about the retrieval cost of each database, and then decide for each database whether or not to use it for the current query, and if, how many documents to retrieve from it. For this purpose, we develop a general decision-theoretic model and discuss different cost structures. Besides cost for retrieving relevant versus nonrelevant documents, we consider the following parameters for each database: expected retrieval quality, expected number of relevant documents in the database, and cost factors for query processing and document delivery. For computing the overall optimum, a divide-and-conquer algorithm is given. If there are several brokers knowing different databases, a preselection of brokers can only be performed heuristically, but the computation of the optimum can be done similarly to the single-broker case. In addition, we derive a formula which estimates the number of relevant documents in a database based on dictionary information.",Networked retrieval; Probabilistic retrieval; Probability ranking principle; Resource discovery,
Methods for information server selection,1999,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0001888008&doi=10.1145%2f297117.297123&partnerID=40&md5=e912271f301b40a0eb2220f255c346b3,"The problem of using a broker to select a subset of available information servers in order to achieve a good trade-off between document retrieval effectiveness and cost is addressed. Server selection methods which are capable of operating in the absence of global information, and where servers have no knowledge of brokers, are investigated. A novel method using Lightweight Probe queries (LWP method) is compared with several methods based on data from past query processing, while Random and Optimal server rankings serve as controls. Methods are evaluated, using TREC data and relevance judgments, by computing ratios, both empirical and ideal, of recall and early precision for the subset versus the complete set of available servers. Estimates are also made of the best-possible performance of each of the methods. LWP and Topic Similarity methods achieved best results, each being capable of retrieving about 60% of the relevant documents for only one-third of the cost of querying all servers. Subject to the applicable cost model, the LWP method is likely to be preferred because it is suited to dynamic environments. The good results obtained with a simple automatic LWP implementation were replicated using different data and a larger set of query topics.",Design; Experimentation; Information servers; Lightweight Probe queries; Network servers; Performance; Server ranking; Server selection; Text retrieval,
Context-sensitive learning methods for text categorization,1999,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0001345686&doi=10.1145%2f306686.306688&partnerID=40&md5=e546735926f9220cc32aa2dba85070c2,"Two recently implemented machine-learning algorithms, RIPPER and sleeping-experts for phrases, are evaluated on a number of large text categorization problems. These algorithms both construct classifiers that allow the ""context"" of a word w to affect how (or even whether) the presence or absence of w will contribute to a classification. However, RIPPER and sleeping-experts differ radically in many other respects: differences include different notions as to what constitutes a context, different ways of combining contexts to construct a classifier, different methods to search for a combination of contexts, and different criteria as to what contexts should be included in such a combination. In spite of these differences, both RIPPER and sleeping-experts perform extremely well across a wide variety of categorization problems, generally outperforming previously applied learning methods. We view this result as a confirmation of the usefulness of classifiers that represent contextual information.",Algorithms; Experimentation; H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval; I.2.6 [Artificial Intelligence]: Learning - concept learning; parameter learning; I.5.4 [Pattern Recognition]: Applications - text processing,
A robust framework for content-based retrieval by spatial similarity in image databases,1999,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0000901068&doi=10.1145%2f306686.306689&partnerID=40&md5=27ea498309b1bf2c784984a903e2adcc,"A framework for retrieving images by spatial similarity (FRISS) in image databases is presented. In this framework, a robust retrieval by spatial similarity (RSS) algorithm is defined as one that incorporates both directional and topological spatial constraints, retrieves similar images, and recognizes images even after they undergo translation, scaling, rotation (both perfect and multiple), or any arbitrary combination of transformations. The FRISS framework is discussed and used as a base for comparing various existing RSS algorithms. Analysis shows that none of them satisfies all the FRISS specifications. An algorithm, SIMDTC, is then presented. SIMDTC introduces the concept of a rotation correction angle (RCA) to align objects in one image spatially closer to matching objects in another image for more accurate similarity assessment. Similarity between two images is a function of the number of common objects between them and the closeness of directional and topological spatial relationships between object pairs in both images. The SIMDTC retrieval is invariant under translation, scaling, and perfect rotation, and the algorithm is able to rank multiple rotation variants. The algorithm was tested using synthetic images and the TESSA image database. Analysis shows the robustness of the SIMDTC algorithm over current algorithms.",Algorithms; Content-based retrieval; Design; Documentation; H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval - retrieval models; query formulation; Image databases; Multimedia databases; Query formulation; Retrieval models,
Incremental formalization with the Hyper-Object Substrate,1999,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0000505605&doi=10.1145%2f306686.306690&partnerID=40&md5=bcf12f0e68a26424c0f128440b337755,"Computers require formally represented information to perform computations that support users; yet users who have needed such support have often proved to be unable or unwilling to formalize it. To address this problem, this article introduces an approach called incremental formalization, in which, first, users express information informally and then the system aids them in formalizing it. Incremental formalization requires a system architecture that (1) integrates formal and informal representations and (2) supports progressive formalization of information. The system should have both tools to capture naturally available informal information and techniques to suggest possible formalizations of this information. The Hyper-Object Substrate (HOS) was developed to satisfy these requirements. HOS has been applied to a number of problem domains, including network design, archeological site analysis, and neuroscience education. Users have been successful in adding informal information and then later formalizing it incrementally with the aid of the system. Our experience with HOS has reaffirmed the need for information spaces to evolve during use and has identified additional considerations in the design and instantiation of systems enabling and supporting incremental formalization.",Design; Formalization; H.5.2 [Information Interfaces and Presentation]: User Interfaces; H.5.4 [Information Interfaces and Presentation]: Hypertext/Hypermedia; Human Factors; I.2.4 [Artificial Intelligence]: Knowledge Representation Formalisms and Methods,
Augmenting organizational memory: A field study of Answer Garden,1998,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0032108037&doi=10.1145%2f290159.290160&partnerID=40&md5=cccf8b4c5e93be0c3afdc32adeb12e1c,"A growing concern for organizations and groups has been to augment their knowledge and expertise. One such augmentation is to provide an organizational memory, some record of the organization's knowledge. However, relatively little is known about how computer systems might enhance organizational, group, or community memory. This article presents Answer Garden, a system for growing organizational memory. The article describes the system and its underlying implementation. It then presents findings from a field study of Answer Garden. The article discusses the usage data and qualitative evaluations from the field study, and then draws a set of lessons for next-generation organizational memory systems. © 1998 ACM.",C.2.4 [Computer-Communication Networks]: Distributed Systems - distributed applications; H.1.2 [Models and Principles]: User/Machine Systems; H.3 [Information Systems]: Information Storage and Retrieval,Computer supported cooperative work; Distributed computer systems; Information retrieval; Multimedia systems; User interfaces; Answer garden system; Collective memory; Community memory; Organizational memory; Data storage equipment
Self-spatial join selectivity estimation using fractal concepts,1998,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0032035979&doi=10.1145%2f279339.279342&partnerID=40&md5=b0bce6d175521db85fde2a708ed2109a,"The problem of selectivity estimation for queries of nontraditional databases is still an open issue. In this article, we examine the problem of selectivity estimation for some types of spatial queries in databases containing real data. We have shown earlier [Faloutsos and Kamel 1994] that real point sets typically have a nonuniform distribution, violating consistently the uniformity and independence assumptions. Moreover, we demonstrated that the theory of fractals can help to describe real point sets. In this article we show how the concept of fractal dimension, i.e., (noninteger) dimension, can lead to the solution for the selectivity estimation problem in spatial databases. Among the infinite family of fractal dimensions, we consider here the Hausdorff fractal dimension D0 and the ""Correlation"" fractal dimension D2. Specifically, we show that (a) the average number of neighbors for a given point set follows a power law, with D2 as exponent, and (b) the average number of nonempty range queries follows a power law with E - D0 as exponent (E is the dimension of the embedding space). We present the formulas to estimate the selectivity for ""biased"" range queries, for self-spatial joins, and for the average number of nonempty range queries. The result of some experiments on real and synthetic point sets are shown. Our formulas achieve very low relative errors, typically about 10%, versus 40%-100% of the formulas that are based on the uniformity and independence assumptions. © 1998 ACM.",Algorithms; Fractal dimension; H.2.4 [Database Management]: Systems-query processing; H.2.8 [Database Management]: Database Applications-spatial databases and GIS; Range query; Selectivity estimation; Spatial join; Theory,Algorithms; Computer selection and evaluation; Database systems; Fractal dimension; Selectivity estimation; Fractals
A study of probability kinematics in information retrieval,1998,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0032108219&doi=10.1145%2f290159.290161&partnerID=40&md5=f9decc782c676c3a8bf0e1a62bc3e802,"We analyze the kinematics of probabilistic term weights at retrieval time for different Information Retrieval models. We present four models based on different notions of probabilistic retrieval. Two of these models are based on classical probability theory and can be considered as prototypes of models long in use in Information Retrieval, like the Vector Space Model and the Probabilistic Model. The two other models are based on a logical technique of evaluating the probability of a conditional called imaging; one is a generalization of the other. We analyze the transfer of probabilities occurring in the term space at retrieval time for these four models, compare their retrieval performance using classical test collections, and discuss the results. We believe that our results provide useful suggestions on how to improve existing probabilistic models of Information Retrieval by taking into consideration term-term similarity. © 1998 ACM.",F.1.1 [Computation by Abstract Devices]: Models of Computation; H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval; Logical imaging; Performance; Probabilistic modeling; Probabilistic retrieval,Computational methods; Kinematics; Mathematical models; Probability; Vectors; Probabilistic retrieval; Vector space model; Information retrieval
The knowledge in multiple human relevance judgments,1998,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0032035833&doi=10.1145%2f279339.279340&partnerID=40&md5=0dbb6883e877d8bd4fc78ca36061cd72,"We show first that the pooling of multiple human judgments of relevance provides a predictor of relevance that is superior to that obtained from a single human's relevance judgments. A learning algorithm applied to a set of relevance judgments obtained from a single human would be expected to perform on new material at a level somewhat below that human. However, we examine two learning methods which when trained on the superior source of pooled human relevance judgments are able to perform at the level of a single human on new material. All performance comparisons are based on an independent human judge. Both algorithms function by producing term weights - one by a log odds calculation and the other by producing a least-squares fit to human relevance ratings. Some characteristics of the algorithms are examined. © 1998 ACM.",H.1.2 [Models and Principles]: User/Machine System-human information processing; H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval-selection process; I.2.6 [Artificial Intelligence]: Learning-knowledge acquisition,Algorithms; Information retrieval; Least squares approximations; Inverse document frequency weights; Information technology
A semidiscrete matrix decomposition for latent semantic indexing in information retrieval,1998,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0032183760&doi=10.1145%2f291128.291131&partnerID=40&md5=8c983138c9596f4f4881fa3d1b228508,"The vast amount of textual information available today is useless unless it can be effectively and efficiently searched. The goal in information retrieval is to find documents that are relevant to a given user query. We can represent a document collection by a matrix whose (i, j) entry is nonzero only if the ith term appears in the jth document; thus each document corresponds to a column vector. The query is also represented as a column vector whose ith term is nonzero only if the ith term appears in the query. We score each document for relevancy by taking its inner product with the query. The highest-scoring documents are considered the most relevant. Unfortunately, this method does not necessarily retrieve all relevant documents because it is based on literal term matching. Latent semantic indexing (LSI) replaces the document matrix with an approximation generated by the truncated singular-value decomposition (SVD). This method has been shown to overcome many difficulties associated with literal term matching. In this article we propose replacing the SVD with the semidiscrete decomposition (SDD). We will describe the SDD approximation, show how to compute it, and compare the SDD-based LSI method to the SVD-based LSI method. We will show that SDD-based LSI does as well as SVD-based LSI in terms of document retrieval while requiring only one-twentieth the storage and one-half the time to compute each query. We will also show how to update the SDD approximation when documents are added or deleted from the document collection. © 1998 ACM.",G.1.2 [Numerical Analysis]: Approxiamtion; H.3.3 [Information Systems]: Information Search and Retrieval,Algorithms; Approximation theory; Computational linguistics; Indexing (of information); Matrix algebra; Latent semantic indexing (LSI); Semidiscrete decomposition (SDD); Information retrieval
A hypermedia version control framework,1998,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0032036127&doi=10.1145%2f279339.279341&partnerID=40&md5=fd9d20b5ce407557492fdcf006e6a54b,"The areas of application of hypermedia technology, combined with the capabilities that hypermedia provides for manipulating structure, create an environment in which version control is very important. A hypermedia version control framework has been designed to specifically address the version control problem in open hypermedia environments. One of the primary distinctions of the framework is the partitioning of hypermedia version control functionality into intrinsic and application-specific categories. The version control framework has been used as a model for the design of version control services for a hyperbase management system that provides complete version support for both data and structural entities. In addition to serving as a version control model for open hypermedia environments, the framework offers a clarifying and unifying context in which to examine the issues of version control in hypermedia. © 1998 ACM.",D.2.7 [Software Engineering]: Distribution and Maintenance-version control; H.1.1 [Models and Principles]: Systems and Information Theory-general systems theory; H.2.1 [Database Management]: Logical design-data models,Computer operating systems; Database systems; Management; Hipermedia; Hyperbase management systems; Management information systems
Metric details for natural-language spatial relations,1998,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0032182940&doi=10.1145%2f291128.291129&partnerID=40&md5=3bc0589212155d8e6470a19cbd7566b3,"Spatial relations often are desired answers that a geographic information system (GIS) should generate in response to a user's query. Current GISs provide only rudimentary support for processing and interpreting natural-language-like spatial relations, because their models and representations are primarily quantitative, while natural-language spatial relations are usually dominated by qualitative properties. Studies of the use of spatial relations in natural language showed that topology accounts for a significant portion of the geometric properties. This article develops a formal model that captures metric details for the description of natural-language spatial relations. The metric details are expressed as refinements of the categories identified by the 9-intersection, a model for topological spatial relations, and provide a more precise measure than does topology alone as to whether a geometric configuration matches with a spatial term or not. Similarly, these measures help in identifying the spatial term that describes a particular configuration. Two groups of metric details are derived: splitting ratios as the normalized values of lengths and areas of intersections; and closeness measures as the normalized distances between disjoint object parts. The resulting model of topological and metric properties was calibrated for 64 spatial terms in English, providing values for the best fit as well as value ranges for the significant parameters of each term. Three examples demonstrate how the framework and its calibrated values are used to determine the best spatial term for a relationship between two geometric objects. © 1998 ACM.",H.2.3 [Database Management]: Languages - query languages; H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval - query formulation; Search process; Selection process,Database systems; Expert systems; Information retrieval; Mathematical models; Natural language processing systems; Query languages; Topology; Closeness measures; Metric details; Splitting ratios; Geographic information systems
Arithmetic coding revisited,1998,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0032108353&doi=10.1145%2f290159.290162&partnerID=40&md5=269007050664b31cb410baae2f5172fc,"Over the last decade, arithmetic coding has emerged as an important compression tool. It is now the method of choice for adaptive coding on multisymbol alphabets because of its speed, low storage requirements, and effectiveness of compression. This article describes a new implementation of arithmetic coding that incorporates several improvements over a widely used earlier version by Witten, Neal, and Cleary, which has become a de facto standard. These improvements include fewer multiplicative operations, greatly extended range of alphabet sizes and symbol probabilities, and the use of low-precision arithmetic, permitting implementation by fast shift/add operations. We also describe a modular structure that separates the coding, modeling, and probability estimation components of a compression system. To motivate the improved coder, we consider the needs of a word-based text compression program. We report a range of experimental results using this and other models. Complete source code is available. © 1998 ACM.",Algorithms; Approximate coding; Arithmetic coding; E.1 [Data]: Data Structures; E.4 [Data]: Coding and Information Theory - data compaction and compression; Performance; Text compression; Word-based model,Codes (symbols); Data compression; Data structures; Digital arithmetic; Mathematical models; Probability; Arithmetic coding; Text compression; Word based model; Encoding (symbols)
Structured hypertext with domain semantics,1998,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0032183759&doi=10.1145%2f291128.291132&partnerID=40&md5=0200ff4397dd04e1e9c88f8ab801c2b4,"One important facet of current hypertext research involves using knowledge-based techniques to develop and maintain document structures. A semantic net is one such technique. However, most semantic-net-based hypertext systems leave the linking consistency of the net to individual users. Users without guidance may accidentally introduce structural and relational inconsistencies in the semantic nets. The relational inconsistency hinders the creation of domain information models. The structural inconsistency leads to unstable documents, especially when a document is composed by computation with traversal algorithms. This work tackles the above problems by integrating logical structure and domain semantics into a semantic net. A semantic-net-based structured-hypertext model has been formalized. The model preserves structural and relational consistency after changes to the semantic net. The hypertext system (RICH) based on this model has been implemented and tested. The RICH system can define and enforce a set of rules to maintain the integrity of the semantic net and provide particular support for creating multihierarchies with the reuse of existing contents and structures. Users have found such flexible but enforceable semantics to be helpful. © 1998 ACM.",E.1 [Data]: Data Structure - graphs; H.2.1 [Database Management]: Logical design - data models; H.3.4 [Information Storage and Retrieval]: Systems and Software; H.5 [Information Systems]: Information Interfaces and Presentation,Computational linguistics; Data storage equipment; Data structures; Graph theory; Information retrieval; Knowledge based systems; Relational database systems; Domain semantics; Semantic nets; Structured hypertext; Hypertext systems
Collaborative conceptual schema design: A process model and prototype system,1998,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0032183858&doi=10.1145%2f291128.291130&partnerID=40&md5=7e736fef055c7ef3e106e3d1a6ecb9c9,"Recent years have seen an increased interest in providing support for collaborative activities among groups of users participating in various information systems design tasks such as, requirements determination and process modeling. However, little attention has been paid to the collaborative conceptual database design process. In this article, we develop a model of the collaborative conceptual schema development process and describe the design and implementation of a graphical multiuser conceptual schema design tool that is based on the model. The system we describe allows a group of users to work collaboratively on the creation of database schemas in a synchronous (same-time) mode (either in a face-to-face or distributed setting). Extensive modeling support is provided to assist users in creating semantically correct conceptual schemas. The system also provides users with several graphical facilities such as, a large drawing workspace with the ability to scroll or ""jump"" to any portion of this workspace, zooming capabilities, and the ability to move object(s) to any portion of the workspace. The unique component of the system, however, is its built-in support for collaborative schema design. The system supports a relaxed WYSIWIS environment, i.e., each user can control the graphical layout of the same set of schema objects. The system ensures that changes/additions made by any user are immediately reflected at other user workstations and that all users' schemas are consistent. Any conflicts that may compromise the integrity of the shared schema are flagged and resolved by the system. The results from a preliminary experiment suggest that the use of our system in a collaborative mode improved information sharing among users, minimized conflicts, and led to a more comprehensive schema definition. © 1998 ACM.",Collaboration; Conceptual modeling; Database design; Design; Graphical CASE tools; Groupware; H.2.1 [Logical Design]; H.4 [Information Systems]; K.6.3 [Software Development]; Management; Semantic modeling,Computational linguistics; Computer aided software engineering; Computer graphics; Computer supported cooperative work; Groupware; Collaborative conceptual schema design; Distributed database systems
Electronic mail as a coalition-building information technology,1998,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031599421&doi=10.1145%2f267954.267958&partnerID=40&md5=f8c93a3d45c267126cced8eb558561e9,"One of the most intriguing lines of research within the literature on diffusion of information technologies (IT) is the study of the power and politics of this process. The major objective of this article is to build on the work of Kling and Markus on power and IT, by extending their perspective to email. To demonstrate how email can be used for political purposes within an organizational context, a case study is presented. The case study describes a series of events which took place in a university. In the case, email was used by a group of employees to stage a rebellion against the university president. The discussion demonstrates that email features make it amenable to a range of political uses. The article is concluded with a discussion of the implications from this case to email research and practice. © 1998 ACM.",Abuse; Coalition building; Email; Human Factors; K.4.1; MIS; Politics,Information dissemination; Information science; Information technology; Coalition building information technology; Electronic mail
Hyperdocuments as automata: Verification of trace-based browsing properties by model checking,1998,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031599361&doi=10.1145%2f267954.267955&partnerID=40&md5=fe5f1d9aa0ac2bc2b503081c715b2527,"We present a view of hyperdocuments in which each document encodes its own browsing semantics in its links. This requires a mental shift in how a hyperdocument is thought of abstractly. Instead of treating the links of a document as defining a static directed graph, they are thought of as defining an abstract program, termed the links-automaton of the document. A branching temporal logic notation, termed HTL*, is introduced for specifying properties a document should exhibit during browsing. An automated program verification technique called model checking is used to verify that browsing specifications in a subset of HTL* are met by the behavior defined in the links-automaton. We illustrate the generality of these techniques by applying them first to several Trellis documents and then to a Hyperties document. © 1998 ACM.","D.2.2 [Software Engineering]: Tools and Techniques - Petri nets; D.2.4 [Software Engineering]: Program Verification - assertion, checkers",Computation theory; Encoding (symbols); Graph theory; Browsing semantics; Hyperdocuments; Hypermedia; Model checking; Automata theory
Evaluation of an algorithm for finding a match of a distorted texture pattern in a large image database,1998,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031599184&doi=10.1145%2f267954.267956&partnerID=40&md5=1e91c8cbecf6dd20c1e775920ba73702,"Evaluation of an algorithm for finding a match for a random texture pattern in a large image database is presented. The algorithm was designed assuming that the random pattern may be subject to misregistration relative to its representation in the database and assuming that it may have missing parts. The potential applications involve authentication of legal documents, bank notes, or credit cards, where thin fibers are embedded randomly into the document medium during medium fabrication. The algorithm achieves image matching by a three-step hierarchical procedure, which starts by matching parts of fiber patterns while solving the misregistration problem and ends up by matching complete fiber patterns. Performance of the algorithm is studied both theoretically and experimentally. Theoretical analysis includes the study of the probability that two documents have the same pattern, and the probability of the algorithm establishing a wrong match, as well as the algorithm's performance in terms of processing time. Experiments involving over 250,000 trials using databases of synthetic documents, containing up to 100,000 documents, were used to confirm theoretical predictions. In addition, experiments involving a database containing real images were conducted in order to confirm that the algorithm has potential in real applications. © 1998 ACM.",Grayscale manipulation; H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval; I.2.4 [Artificial Intelligence]: Knowledge Representation Formalisms and Methods; I.4.3 [Image Processing]: Enhancement - filtering,Algorithms; Database systems; Image processing; Image database; Image matching; Identification (control systems)
Corpus-based stemming using cooccurrence of word variants,1998,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031599183&doi=10.1145%2f267954.267957&partnerID=40&md5=b6df16fafcaf4605ae70a0357fb06fea,"Stemming is used in many information retrieval (IR) systems to reduce variant word forms to common roots. It is one of the simplest applications of natural-language processing to IR and is one of the most effective in terms of user acceptance and consistency, though small retrieval improvements. Current stemming techniques do not, however, reflect the language use in specific corpora, and this can lead to occasional serious retrieval failures. We propose a technique for using corpus-based word variant cooccurrence statistics to modify or create a stemmer. The experimental results generated using English newspaper and legal text and Spanish text demonstrate the viability of this technique and its advantages relative to conventional approaches that only employ morphological rules. © 1998 ACM.",Algorithms; H.3.1 [Information Storage and Retrieval]: Content Analysis and Indexing - indexing methods; H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval - query formulation; Linguistic processing; Search process,Algorithms; Database systems; Failure analysis; Cooccurrence; Corpus analysis; Stemming; Information retrieval
"A multilevel approach to intelligent information filtering: Model, system, and evaluation",1997,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031248276&doi=10.1145%2f263479.263481&partnerID=40&md5=598301b82c6ed68d3d523bcf43ff17e0,"In information-filtering environments, uncertainties associated with changing interests of the user and the dynamic document stream must be handled efficiently. In this article, a filtering model is proposed that decomposes the overall task into subsystem functionalities and highlights the need for multiple adaptation techniques to cope with uncertainties. A filtering system, SIFTER, has been implemented based on the model, using established techniques in information retrieval and artificial intelligence. These techniques include document representation by a vector-space model, document classification by unsupervised learning, and user modeling by reinforcement learning. The system can filter information based on content and a user's specific interests. The user's interests are automatically learned with only limited user intervention in the form of optional relevance feedback for documents. We also describe experimental studies conducted with SIFTER to filter computer and information science documents collected from the Internet and commercial database services. The experimental results demonstrate that the system performs very well in filtering documents in a realistic problem setting. © 1997 ACM.",H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval-clustering; I.2.6 [Artificial Intelligence]: Learning; I.7.3 [Text Processing]: Index Generation; Selection process,Artificial intelligence; Computer simulation; Data processing; Database systems; Learning systems; Intelligent information filtering; Reinforcement learning; Unsupervised learning; Information retrieval systems
Proximal nodes: A model to query document databases by content and structure,1997,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031248710&doi=10.1145%2f263479.263482&partnerID=40&md5=568cb48ddf569a3110a33e8738cdf269,"A model to query document databases by both their content and structure is presented. The goal is to obtain a query language that is expressive in practice while being efficiently implementable, features not present at the same time in previous work. The key ideas of the model are a set-oriented query language based on operations on nearby structure elements of one or more hierarchies, together with content and structural indexing and bottom-up evaluation. The model is evaluated in regard to expressiveness and efficiency, showing that it provides a good trade-off between both goals. Finally, it is shown how to include in the model other media different from text. © 1997 ACM.",H.1.2 [Information Systems]: User/Machine Systems-human information processing; H.2.1 [Database Management]: Logical Design-data models; H.2.2 [Database Management]: Physical Design-access methods; H.2.3 [Database Management]: Languages-query languages,Algorithms; Computer programming languages; Data processing; Data structures; Human engineering; Man machine systems; Performance; Hierarchical documents; Structured text; Text algebras; Query languages
On automated message processing in electronic commerce and work support systems: Speech act theory and expressive felicity,1997,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031245238&doi=10.1145%2f263479.263480&partnerID=40&md5=4454e4f86d249928b88d3ddcc10f25cc,"Electronic messaging, whether in an office environment or for electronic commerce, is normally carried out in natural language, even when supported by information systems. For a variety of reasons, it would be useful if electronic messaging systems could have semantic access to, that is, access to the meanings and contents of, the messages they process. Given that natural language understanding is not a practicable alternative, there remain three approaches to delivering systems with semantic access: electronic data interchange (EDI), tagged messages, and the development of a formal language for business communication (FLBC). We favor the latter approach. In this article we compare and contrast these three approaches, present a theoretical basis for an FLBC (using speech act theory), and describe a prototype implementation. © 1997 ACM.",Electronic commerce; Formal language for business communication; I.2.4 [Artificial Intelligence]: Knowledge Representation Formalisms and Methods-representation languages; Languages; Speech act theory; Theory,Artificial intelligence; Formal languages; Knowledge representation; Software prototyping; Speech processing; Electronic commerce; Formal language for business communication; Speech act theory; Data communication systems
Data structures for efficient broker implementation,1997,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031177232&doi=10.1145%2f256163.256165&partnerID=40&md5=8fbc2a69613c7f316766fbfc59525186,"With the profusion of text databases on the Internet, it is becoming increasingly hard to find the most useful databases for a given query. To attack this problem, several existing and proposed systems employ brokers to direct user queries, using a local database of summary information about the available databases. This summary information must effectively distinguish relevant databases and must be compact while allowing efficient access. We offer evidence that one broker, GlOSS, can be effective at locating databases of interest even in a system of hundreds of databases and can examine the performance of accessing the GlOSS summaries for two promising storage methods: the grid file and partitioned hashing. We show that both methods can be tuned to provide good performance for a particular workload (within a broad range of workloads), and we discuss the tradeoffs between the two data structures. As a side effect of our work, we show that grid files are more broadly applicable than previously thought; in particular, we show that by varying the policies used to construct the grid file we can provide good performance for a wide range of workloads even when storing highly skewed data. © 1997 ACM.",H.2.2 [Database Management]: Physical design - access methods; H.3.1 [Information Storage and Retrieval]: Content Analysis and Indexing - indexing methods; H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval - search process,Data storage equipment; Data structures; Information retrieval; Query languages; User interfaces; Grid files; Internet; Partitioned hashing; Text databases; Distributed database systems
Recursive hashing functions for n-grams,1997,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031177375&doi=10.1145%2f256163.256168&partnerID=40&md5=2549370cd679c3242d5b60ba0ad63d2d,"Many indexing, retrieval, and comparison methods are based on counting or cataloguing n-grams in streams of symbols. The fastest method of implementing such operations is through the use of hash tables. Rapid hashing of consecutive n-grams is best done using a recursive hash function, in which the hash value of the current n-gram is derived from the hash value of its predecessor. This article generalizes recursive hash functions found in the literature and proposes new methods offering superior performance. Experimental results demonstrate substantial speed improvement over conventional approaches, while retaining near-ideal hash value distribution. © 1997 ACM.",E.2 [Data]: Data Storage Representations - hash-table representations; G.2.1 [Discrete Mathematics]: Combinatorics - recurrences and difference equations; G.3 [Mathematics of Computing]: Probability And Statistics - probabilistic algorithms,Computational complexity; Data structures; Indexing (of information); Information retrieval; Recursive hashing functions; Recursive functions
A text compression scheme that allows fast searching directly in the compressed file,1997,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031118558&doi=10.1145%2f248625.248639&partnerID=40&md5=7702fb8f778cdc8a5ebbbe0c9a6af460,"A new text compression scheme is presented in this article. The main purpose of this scheme is to speed up string matching by searching the compressed file directly. The scheme requires no modification of the string-matching algorithm, which is used as a black box; any string-matching procedure can be used. Instead, the pattern is modified; only the outcome of the matching of the modified pattern against the compressed file is decompressed. Since the compressed file is smaller than the original file, the search is faster both in terms of I/O time and processing time than a search in the original file. For typical text files, we achieve about 30% reduction of space and slightly less of search time. A 30% space saving is not competitive with good text compression schemes, and thus should not be used where space is the predominant concern. The intended applications of this scheme are files that are searched often, such as catalogs, bibliographic files, and address books. Such files are typically not compressed, but with this scheme they can remain compressed indefinitely, saving space while allowing faster search at the same time. A particular application to an information retrieval system that we developed is also discussed. © 1997 ACM.",E.4 [Data]: Coding and Information Theory - data compaction and compression; F.2.2 [Analysis of Algorithms and Problem Complexity]: Nonnumerical Algorithms and Problems - pattern matching,Algorithms; Information retrieval systems; Pattern recognition; String matching algorithms; Data compression
Hyperform: A hypermedia system development environment,1997,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030825212&doi=10.1145%2f239041.239043&partnerID=40&md5=15fac692e3e88a27bfe8f70987aeed51,"Development of hypermedia systems is a complex matter. The current trend toward open, extensible, and distributed multiuser hypermedia systems adds additional complexity to the development process. As a means of reducing this complexity, there has been an increasing interest in hyperbase management systems that allow hypermedia system developers to abstract from the intricacies and complexity of the hyperbase layer and fully attend to application and user interface issues. Design, development, and deployment experiences of a dynamic, open, and distributed multiuser hypermedia system development environment called Hyperform is presented. Hyperform is based on the concepts of extensibility, tailorability, and rapid prototyping of hypermedia system services. Open, extensible hyperbase management systems permit hypermedia system developers to tailor hypermedia functionality for specific applications and to serve as a platform for research. The Hyperform development environment is comprised of multiple instances of four component types: (1) a hyperbase management system server, (2) a tool integrator, (3) editors, and (4) participating tools. Hyperform has been deployed in Unix environments, and experiments have shown that Hyperform greatly reduces the effort required to provide customized hyperbase management system support for distributed multiuser hypermedia systems. © 1997 ACM.",H.1.1 [models and principles]: Systems and information theory - General systems theory; H.2.1 [database management]: Logical design - Data models; H.2.4 [database management]: Systems - Distributed systems,Computational complexity; Computer architecture; Data structures; Database systems; Object oriented programming; Rapid prototyping; System theory; Advanced hypermedia system architecture; Extensible hyperbase management system; Hyperform; Object oriented extension language; Information retrieval systems
The effect of accessing nonmatching documents on relevance feedback,1997,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031121625&doi=10.1145%2f248625.248650&partnerID=40&md5=697b45452ce47e54b8e619b9e3ae8182,"Traditional information retrieval (IR) systems only allow users access to documents that match their current query, and therefore, users can only give relevance feedback on matching documents (or those with a matching strength greater than a set threshold). This article shows that, in systems that allow access to nonmatching documents (e.g., hybrid hypertext and information retrieval systems), the strength of the effect of giving relevance feedback varies between matching and nonmatching documents. For positive feedback the results shown here are encouraging, as they can be justified by an intuitive view of the process. However, for negative feedback the results show behavior that cannot easily be justified and that varies greatly depending on the model of feedback used. © 1997 ACM.",Experimentation; Free-text information retrieval; H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval - search process; Hypertext; Negative feedback; Probabilistic model; Relevance feedback; Theory; Vector space model,Feedback; Mathematical models; Probability; Vectors; Free text information retrieval; Information retrieval systems
Access control for large collections,1997,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031118725&doi=10.1145%2f248625.248652&partnerID=40&md5=127f6d412dd4e72d1e6e8633ab7bb7fe,"Efforts to place vast information resources at the fingertips of each individual in large user populations must be balanced by commensurate attention to information protection. For centralized operational systems in controlled environments, external administrative controls may suffice. For distributed systems with less-structured tasks, more-diversified information, and a heterogeneous user set, the computing system must administer enterprise-chosen access control policies. One kind of resource is a digital library that emulates massive collections of paper and other physical media for clerical, engineering, and cultural applications. This article considers the security requirements for such libraries and proposes an access control method that mimics organizational practice by combining a subject tree with ad hoc role granting that controls privileges for many operations independently, that treats (all but one) privileged roles (e.g., auditor, security officer) like every other individual authorization, and that binds access control information to objects indirectly for scaling, flexibility, and reflexive protection. We sketch a realization and show that it will perform well, generalizes many deployed proposed access control policies, and permits individual data centers to implement other models economically and without disruption. © 1997 ACM.",C.2.4 [Computer-Communication Networks]: Distributed Systems - distributed applications; D.2.0 [Software Engineering]: General - protection mechanisms; D.4.6 [Operating Systems]: Security and Protection; Distributed databases,Distributed computer systems; Distributed database systems; Security of data; Access control; Digital library; Information retrieval systems
Experiences with selecting search engines using metasearch,1997,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031177323&doi=10.1145%2f256163.256164&partnerID=40&md5=40882de1c799785288b93615926a56f4,"Search engines are among the most useful and high-profile resources on the Internet. The problem of finding information on the Internet has been replaced with the problem of knowing where search engines are, what they are designed to retrieve, and how to use them. This article describes and evaluates SavvySearch, a metasearch engine designed to intelligently select and interface with multiple remote search engines. The primary metasearch issue examined is the importance of carefully selecting and ranking remote search engines for user queries. We studied the efficacy of SavvySearch's incrementally acquired metaindex approach to selecting search engines by analyzing the effect of time and experience on performance. We also compared the metaindex approach to the simpler categorical approach and showed how much experience is required to surpass the simple scheme. © 1997 ACM.",Algorithms; Experimentation; H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval; H.3.4 [Information Storage and Retrieval]: Systems and Software; Information retrieval; Machine learning; Search engine; WWW,Inference engines; Interfaces (computer); Learning algorithms; Learning systems; Query languages; Wide area networks; Internet; Search engines; Software package SavvySearch; Information retrieval systems
Modeling word occurrences for the compression of concordances,1997,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031176241&doi=10.1145%2f256163.256166&partnerID=40&md5=1f6365f8f18dce98d643f7dc1555d644,"An earlier paper developed a procedure for compressing concordances, assuming that all elements occurred independently. The models introduced in that paper are extended here to take the possibility of clustering into account. The concordance is conceptualized as a set of bitmaps, in which the bit locations represent documents, and the one-bits represent the occurrence of given terms. Hidden Markov Models (HMMs) are used to describe the clustering of the one-bits. However, for computational reasons, the HMM is approximated by traditional Markov models. A set of criteria is developed to constrain the allowable set of n-state models, and a full inventory is given for n ≤ 4. Graph-theoretic reduction and complementation operations are defined among the various models and are used to provide a structure relating the models studied. Finally, the new methods were tested on the concordances of the English Bible and of two of the world's largest full-text retrieval system: the Trésor de la Langue Française and the Responsa Project. © 1997 ACM.",E.2 [Data]: Data Storage Representations - composite structures; E.4 [Data]: Coding and Information Theory - data compaction and compression; F.1.2 [Computation by Abstract Devices]: Modes of Computation - probabilistic computation; Markov models,Approximation theory; Classification (of information); Computational methods; Data compression; Data storage equipment; Data structures; Graph theory; Markov processes; Mathematical models; Full text retrieval systems; Hidden Markov models (HMM); Information retrieval systems
Making a digital library: The contents of the CORE Project,1997,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031125202&doi=10.1145%2f248625.248627&partnerID=40&md5=57ec6e73bcc58699ae7721fb5cb19cee,"The CORE (Chemical Online Retrieval Experiment) project is a library of primary journal articles in chemistry. Any library has an inside and an outside; in this article we describe the inside of the library and the methods for building the system and accumulating the database. A later article will describe the outside (user experiences). Among electronic-library projects, the CORE project is unusual in that it has both ASCII derived from typesetting and image data for all its pages, and among experimental electronic-library projects, it is unusually large. We describe here (a) the processes of scanning and analyzing about 400,000 pages of primary journal material, (b) the conversion of a similar amount of textual database material, (c) the linking of these two data sources, and (d) the indexing of the text material. © 1997 ACM.",H.3.2 [Information Storage and Retrieval]: Information Storage; H.3.6 [Information Storage and Retrieval]: Library Automation - large text archieves; H.5.2 [Information Interfaces and Presentation]: User Interfaces - interaction,Database systems; Indexing (of information); User interfaces; Chemical online retrieval experiment (CORE) project; Information retrieval systems
Customizing information capture and access,1997,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030784164&doi=10.1145%2f239041.239048&partnerID=40&md5=d8bc73f9f1bd294a479bd9c508e74f17,"This article presents a customizable architecture for software agents that capture and access information in large, heterogeneous, distributed electronic repositories. The key idea is to exploit underlying structure at various levels of granularity to build high-level indices with task-specific interpretations. Information agents construct such indices and are configured as a network of reusable modules called structure detectors and segmenters. We illustrate our architecture with the design and implementation of smart information filters in two contexts: retrieving stock market data from Internet newsgroups and retrieving technical reports from Internet FTP sites. © 1997 ACM.",Design; Documentation; Experimentation; H.3.3 [information storage and retrieval]: Information search and retrieval - Selection process; H.3.4 [information storage and retrieval]: Systems and software - Current awareness systems; Information networks,Computer architecture; Computer networks; Computer software; Information retrieval systems; Information theory; Information gathering; Software agents; Table recognition; Data acquisition
A probabilistic relational algebra for the integration of information retrieval and database systems,1997,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030836948&doi=10.1145%2f239041.239045&partnerID=40&md5=4bc4ac9c83e1d97e8c3660a27e0624b0,"We present a probabilistic relational algebra (PRA) which is a generalization of standard relational algebra. In PRA, tuples are assigned probabilistic weights giving the probability that a tuple belongs to a relation. Based on intensional semantics, the tuple weights of the result of a PRA expression always conform to the underlying probabilistic model. We also show for which expressions extensional semantics yields the same results. Furthermore, we discuss complexity issues and indicate possibilities for optimization. With regard to databases, the approach allows for representing imprecise attribute values, whereas for information retrieval, probabilistic document indexing and probabilistic search term weighting can be modeled. We introduce the concept of vague predicates which yield probabilistic weights instead of Boolean values, thus allowing for queries with vague selection conditions. With these features, PRA implements uncertainty and vagueness in combination with the relational model. © 1997 ACM.",H.2.1 [database management]: Logical design - Data models; H.3.3 [information storage and retrieval]: Information search and retrieval - Retrieval models; Hypertext retrieval; Imprecise data; Logical retrieval model; Probabilistic retrieval; Theory,Computational complexity; Computational linguistics; Computer simulation; Data structures; Indexing (of information); Information retrieval; Optimization; Probability; Query languages; Hypertext retrieval; Imprecise data; Logical retrieval model; Probabilistic relational algebra; Probabilistic retrieval; Relational data model; Uncertain data; Vague predicates; Relational database systems
An Extension of Ukkonen's Enhanced Dynamic Programming ASM Algorithm,1996,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-1542428871&doi=10.1145%2f214174.214183&partnerID=40&md5=109b688c0715b06099864cad1df40c96,"We describe an improvement on Ukkonen's Enhanced Dynamic Programming (EHD) approximate string-matching algorithm for unit-penalty four-edit comparisons. The new algorithm has an asymptotic complexity similar to that of Ukkonen's but is significantly faster due to a decrease in the number of array cell calculations. A 42% speedup was achieved in an application involving name comparisons. Even greater improvements are possible when comparing longer and more dissimilar strings. Although the speed of the algorithm under consideration is comparable to other fast ASM algorithms, it has greater effectiveness in text-processing applications because it supports all four basic Damerau-type editing operations.",F.2.2 [Analysis of Algorithms and Problem Complexity]:Nonnumerical Algorithms; H.3.1 [Information Storage and Retrieval]: Content Analysis and Indexing; H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval,
Using Local Optimality Criteria for Efficient Information Retrieval with Redundant Information Filters,1996,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030129189&doi=10.1145%2f226163.226165&partnerID=40&md5=268b59a7829288589c679fdccc0a1d4c,"We consider information retrieval when the data - for instance, multimedia - is computationally expensive to fetch. Our approach uses ""information filters"" to considerably narrow the universe of possibilities before retrieval. We are especially interested in redundant information filters that save time over more general but more costly filters. Efficient retrieval requires that decisions must be made about the necessity, order, and concurrent processing of proposed filters (an ""execution plan""). We develop simple polynomial-time local criteria for optimal execution plans and show that most forms of concurrency are suboptimal with information filters. Although the general problem of finding an optimal execution plan is likely to be exponential in the number of filters, we show experimentally that our local optimality criteria, used in a polynomial-time algorithm, nearly always find the global optimum with 15 filters or less, a sufficient number of filters for most applications. Our methods require no special hardware and avoid the high processor idleness that is characteristic of massive-parallelism solutions to this problem. We apply our ideas to an important application, information retrieval of captioned data using natural-language understanding, a problem for which the natural-language processing can be the bottleneck if not implemented well.",Boolean algebra; Conjunction; Filters; H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval - Search process; Natural language; Optimization; Performance; Queries,Algorithms; Boolean algebra; Concurrency control; Information retrieval systems; Natural language processing systems; Optimization; Performance; Query languages; Conjunction; Redundant information filters; Information retrieval
Natural-Language Retrieval of Images Based on Descriptive Captions,1996,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030194177&doi=10.1145%2f230538.230539&partnerID=40&md5=83d9c5df22113419e1e1796595fb498e,"We describe a prototype intelligent information retrieval system that uses natural-language understanding to efficiently locate captioned data. Multimedia data generally require captions to explain their features and significance. Such descriptive captions often rely on long nominal compounds (strings of consecutive nouns) which create problems of disambiguating word sense. In our system, captions and user queries are parsed and interpreted to produce a logical form, using a detailed theory of the meaning of nominal compounds. A fine-grain match can then compare the logical form of the query to the logical forms for each caption. To improve system efficiency, we first perform a coarse-grain match with index files, using nouns and verbs extracted from the query. Our experiments with randomly selected queries and captions from an existing image library show an increase of 30% in precision and 50% in recall over the keyphrase approach currently used. Our processing times have a median of seven seconds as compared to eight minutes for the existing system, and our system is much easier to use.",H.3.1 [Information Storage and Retrieval]: Content Analysis and Indexing-indexing methods; H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval - query formulation; Linguistic processing; Search process; Selection process,Algorithms; Computational linguistics; Database systems; Formal logic; Image processing; Knowledge based systems; Knowledge representation; Natural language processing systems; Query languages; Coarse grain match; Descriptive captions; Fine grain match; Intelligent information retrieval system; Multimedia; Information retrieval systems
Computerized Performance Monitors as Multidimensional Systems: Derivation and Application,1996,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030126560&doi=10.1145%2f226163.226167&partnerID=40&md5=eb33cd4b2461f149ee60e7835b3f778c,"An increasing number of companies are introducing computer technology into more aspects of work. Effective use of information systems to support office and service work can improve staff productivity, broaden a company's market, or dramatically change its business. It can also increase the extent to which work is computer mediated and thus within the reach of software known as Computerized Performance Monitoring and Control Systems (CPMCSs). Virtually all research has studied CPMCSs as unidimensional systems. Employees are described as ""monitored"" or ""unmonitored"" or as subject to ""high,"" ""moderate,"" or ""low"" levels of monitoring. Research that does not clearly distinguish among possible monitor design cannot explain how designs may differ in effect. Nor can it suggest how to design better monitors. A multidimensional view of CPMCSs describes monitor designs in terms of object of measurements, tasks measured, recipient of data, reporting period, and message content. This view is derived from literature in control systems, organizational behavior, and management information systems. The multidimensional view can then be incorporated into causal models to explain contradictory results of earlier CPMCS research.",Computerized performance evaluation; Customer service; H.4.2 [Information Systems Applications]: Types of Systems - Employee monitoring; K.4.3 [Computers and Society]: Organizational Impacts - Employee productivity; Management; Theory; Work measurement,Computer applications; Control systems; Management information systems; Monitoring; Personnel rating; Productivity; Systems analysis; Computerized performance evaluation; Computerized performance monitoring and control systems; Computerized work monitoring; Work monitoring system design; Computer software
Document Ranking on Weight-Partitioned Signature Files,1996,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030128352&doi=10.1145%2f226163.226164&partnerID=40&md5=e249cf82d2b94dc717cffa2895ec2f39,"A signature file organization, called the weight-partitioned signature file, for supporting document ranking is proposed. It employs multiple signature files, each of which corresponds to one term frequency, to represent terms with different term frequencies. Words with the same term frequency in a document are grouped together and hashed into the signature file corresponding to that term frequency. This eliminates the need to record the term frequency explicitly for each word. We investigate the effect of false drops on retrieval effectiveness if they are not eliminated in the search process. We have shown that false drops introduce insignificant degradation on precision and recall when the false-drop probability is below a certain threshold. This is an important result since false-drop elimination could become the bottleneck in systems using fast signature file search techniques. We perform an analytical study on the performance of the weight-partitioned signature file under different search strategies and configurations. An optimal formula is obtained to determine for a fixed total storage overhead the storage to be allocated to each partition in order to minimize the effect of false drops on document ranks. Experiments were performed using a document collection to support the analytical results.",Design; Experimentation; H.2.2 [Database Management]: Physical Design - Access methods; H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval - Retrieval models; H.3.6 [Information Storage and Retrieval]: Library Automation,Computer simulation; Encoding (symbols); File organization; Information retrieval; Performance; Probability; Storage allocation (computer); Access method; Document ranking; Document retrieval; Superimposed coding; Text retrieval; Weight partitioned signature files; Information retrieval systems
TROLL - A Language for Object-Oriented Specification of Information Systems,1996,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030127113&doi=10.1145%2f226163.226166&partnerID=40&md5=b134caf3944852394d7774def83a5365,"TROLL is a language particularly suited for the early stages of information system development, when the universe of discourse must be described. In TROLL the descriptions of the static and dynamic aspects of entities are integrated into object descriptions. Sublanguages for data terms, for first-order and temporal assertions, and for processes, are used to describe respectively the static properties, the behavior, and the evolution over time of objects. TROLL organizes system design through object-orientation and the support of abstractions such as classification, specialization, roles, and aggregation. Language features for state interactions and dependencies among components support the composition of the system from smaller modules, as does the facility of defining interfaces on top of object descriptions.",D.2.1. [Software Engineering]: Requirements/specification - Languages; D.3.2 [Programming Languages]: Language Classifications - TROLL; D.3.3. [Programming Languages]: Language Constructs and Features; H.1.0 [Models and Principles]: General,Computer hardware description languages; Data processing; Management information systems; Software engineering; Systems analysis; Language classifications; Language constructs and features; Object oriented specification; Computer programming languages
Self-Indexing Inverted Files for Fast Text Retrieval,1996,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030260359&doi=10.1145%2f237496.237497&partnerID=40&md5=cba6955288872444e3ae8b852c81841b,"Query-processing costs on large text databases are dominated by the need to retrieve and scan the inverted list of each query term. Retrieval time for inverted lists can be greatly reduced by the use of compression, but this adds to the CPU time required. Here we show that the CPU component of query response time for conjunctive Boolean queries and for informal ranked queries can be similarly reduced, at little cost in terms of storage, by the inclusion of an internal index in each compressed inverted list. This method has been applied in a retrieval system for a collection of nearly two million short documents. Our experimental results show that the self-indexing strategy adds less than 20% to the size of the compressed inverted file, which itself occupies less than 10% of the indexed text, yet can reduce processing time for Boolean queries of 5-10 terms to under one fifth of the previous cost. Similarly, ranked queries of 40-50 terms can be evaluated in as little as 25% of the previous time, with little or no loss of retrieval effectiveness.",E.4 [Data]: Coding and Information Theory - data compaction and compression; H.3.1 [Information Storage and Retrieval]: Content Analysis and Indexing - indexing methods; H.3.2 [Information Storage and Retrieval]: Information Storage - file organization,Data compression; Data storage equipment; File organization; Information retrieval; Information retrieval systems; Query languages; Boolean queries; Full text retrieval; Index compression; Inverted file; Query processing; Self indexing; Indexing (of information)
The Model-Assisted Global Query System for Multiple Databases in Distributed Enterprises,1996,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030261973&doi=10.1145%2f237496.237499&partnerID=40&md5=c94bf7e9850f4ede267edaf7ca8bcdd2,"Today's enterprises typically employ multiple information systems, which are independently developed, locally administered, and different in logical or physical designs. Therefore, a fundamental challenge in enterprise information management is the sharing of information for enterprise users across organizational boundaries; this requires a global query system capable of providing on-line intelligent assistance to users. Conventional technologies, such as schema-based query languages and hard-coded schema integration, are not sufficient to solve this problem. This article develops a new approach, a ""model-assisted global query system,"" that utilizes an on-line repository of enterprise metadata - the Metadatabase - to facilitate global query formulation and processing with certain desirable properties such as adaptiveness and open-systems architecture. A definitional model characterizing the various classes and roles of the required metadata as knowledge for the system is presented. The significance of possessing this knowledge (via a Metadatabase) toward improving the global query capabilities available previously is analyzed. On this basis, a direct method using model traversal and a query language using global model constructs are developed along with other new methods required for this approach. It is then tested through a prototype system in a computer-integrated manufacturing (CIM) setting.",H.2.3 [Database Management]: Languages - query languages; H.2.4 [Database Management]: Systems - distributed systems; H.2.7 [Database Management]: Database Administration - data dictionary / directory; Query processing,Data storage equipment; Distributed database systems; Information retrieval; Logic design; Mathematical models; Online systems; User interfaces; Enterprise information management; Global query system; Hard coded schema integration; Metadatabases; Model traversal; Multiple information systems; Online intelligent assistance; Query languages
Extending Object-Oriented Systems with Roles,1996,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030190171&doi=10.1145%2f230538.230540&partnerID=40&md5=f345191fac75d3b4b3648a463b7cf35c,"In many class-based object-oriented systems the association between an instance and a class is exclusive and permanent. Therefore these systems have serious difficulties in representing objects taking on different roles over time. Such objects must be reclassified any time they evolve (e.g., if a person becomes a student and later an employee). Class hierarchies must be planned carefully and may grow exponentially if entities may take on several independent roles. The problem is even more severe for object-oriented databases than for common object-oriented programming. Databases store objects over longer periods, during which the represented entities evolve. This article shows how class-based object-oriented systems can be extended to handle evolving objects well. Class hierarchies are complemented by role hierarchies, whose nodes represent role types an object classified in the root may take on. At any point in time, an entity is represented by an instance of the root and an instance of every role type whose role it currently plays. In a natural way, the approach extends traditional object-oriented concepts, such as classification, object identity, specialization, inheritance, and polymorphism in a natural way. The practicability of the approach is demonstrated by an implementation in Smalltalk. Smalltalk was chosen because it is widely known, which is not true for any particular class-based object-oriented database programming language. Roles can be provided in Smalltalk by adding a few classes. There is no need to modify the semantics of Smalltalk itself. Role hierarchies are mapped transparently onto ordinary classes. The presented implementation can easily be ported to object-oriented database programming languages based on Smalltalk, such as Gemstone's OPAL.",D.1.5 [Programming Techniques]: Object-Oriented Programming; D.2.10 [Software Engineering]: Design - methodologies; D.3.3 [Programming Languages]: Language Constructs and Features; H.2 [Database Management]: General; Representation,Computational linguistics; Computer programming languages; Data structures; Object oriented programming; Software engineering; Class hierarchies; Object oriented databases; Role hierarchies; Semantics; Smalltalk programming language; Database systems
Information System Behavior Specification by High-Level Petri Nets,1996,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030260733&doi=10.1145%2f237496.237498&partnerID=40&md5=1562b48bfab31af8dcdc3aa8d5529a09,"The specification of an information system should include a description of structural system aspects as well as a description of the system behavior. In this article, we show how this can be achieved by high-level Petri nets - namely, the so-called NR/T-nets (Nested-Relation/ Transition Nets). In NR/T-nets, the structural part is modeled by nested relations, and the behavioral part is modeled by a novel Petri net formalism. Each place of a net represents a nested relation scheme, and the marking of each place is given as a nested relation of the respective type. Insert and delete operations in a nested relational database (NF2-database) are expressed by transitions in a net. These operations may operate not only on whole tuples of a given relation, but also on ""subtuples"" of existing tuples. The arcs of a net are inscribed with so-called Filter Tables, which allow (together with an optional logical expression as transition inscription) conditions to be formulated on the specified (sub-) tuples. The occurrence rule for NR/T-net transitions is defined by the operations union, intersection, and ""negative"" in lattices of nested relations. The structure of an NR/T-net, together with the occurrence rule, defines classes of possible information system procedures, i.e., sequences of (possibly concurrent) operations in an information system.",Design; H.1.1 [models and principles]: Systems and Information theory - general systems theory; H.2.1 [Database Management]: Logical Design - data models; H.2.3 [Database Management]: Languages - data manipulation languages (DML); Languages; Management,Computer aided logic design; Computer hardware description languages; Data structures; Petri nets; Query languages; Behavior specification; Complex objects; Conceptual design; Data manipulation languages; Nested relations; Transition nets; Information retrieval systems
Bias in Computer Systems,1996,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030196306&doi=10.1145%2f230538.230561&partnerID=40&md5=7759e06c07c18aae348ee040f6b4cc15,"From an analysis of actual cases, three categories of bias in computer systems have been developed: preexisting, technical, and emergent. Preexisting bias has its roots in social institutions, practices, and attitudes. Technical bias arises from technical constraints or considerations. Emergent bias arises in a context of use. Although others have pointed to bias in particular computer systems and have noted the general problem, we know of no comparable work that examines this phenomenon comprehensively and which offers a framework for understanding and remedying it. We conclude by suggesting that freedom from bias should be counted among the select set of criteria - including reliability, accuracy, and efficiency -according to which the quality of systems in use in society should be judged.",Bias; Computer ethics; Computers and society; D.2.0 [Software]: Software Engineering; Design; Design methods; Ethics; H.1.2 [Information Systems]: User/Machine Systems; Human Factors; Human values; K.4.0 [Computers and Society]: General; Standards,Man machine systems; Philosophical aspects; Reliability; Social aspects; Social sciences computing; Societies and institutions; Software engineering; Standards; Systems analysis; Computer ethics; Human values; Social computing; Social impact; Computer systems
A General Explanation Component for Conceptual Modeling in CASE Environments,1996,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030193153&doi=10.1145%2f230538.230560&partnerID=40&md5=bb667fffbe83c7872a27934c6c51b2c7,"In information systems engineering, conceptual models are constructed to assess existing information systems and work out requirements for new ones. As these models serve as a means for communication between customers and developers, it is paramount that both parties understand the models, as well as that the models form a proper basis for the subsequent design and implementation of the systems. New CASE environments are now experimenting with formal modeling languages and various techniques for validating conceptual models, though it seems difficult to come up with a technique that handles the linguistic barriers between the parties involved in a satisfactory manner. In this article, we discuss the theoretical basis of an explanation component implemented for the PPP CASE environment. This component integrates other validation techniques and provides a very flexible natural-language interface to complex model information. It describes properties of the modeling language and the conceptual models in terms familiar to users, and the explanations can be combined with graphical model views. When models are executed, it can justify requested inputs and explain computed outputs by relating trace information to properties of the models.",D.2.2 [Software Engineering]: Tools and Techniques - Computer-aided software engineering (CASE); Design; Documentation; I.2.7 [Artificial Intelligence]: Natural Language Processing; I.6.4 [Simulation and Modeling]: Model Validation and Analysis,Artificial intelligence; Computational linguistics; Computer graphics; Computer simulation; Computer simulation languages; Database systems; Formal languages; Natural language processing systems; Program documentation; Conceptual modeling; Information systems engineering; Validation techniques; Computer aided software engineering
A Visual Retrieval Environment for Hypermedia Information Systems,1996,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0029732906&doi=10.1145%2f214174.214175&partnerID=40&md5=0163431a8b2ec255d120ff09f7f5175b,"We present a graph-based object model that may be used as a uniform framework for direct manipulation of multimedia information. After an introduction motivating the need for abstraction and structuring mechanisms in hypermedia systems, we introduce the data model and the notion of perspective, a form of data abstraction that acts as a user interface to the system, providing control over the visibility of the objects and their properties. A perspective is defined to include an intension and an extension. The intension is defined in terms of a pattern, a subgraph of the schema graph, and the extension is the set of pattern-matching instances. Perspectives, as well as database schema and instances, are graph structures that can be manipulated in various ways. The resulting uniform approach is well suited to a visual interface. A visual interface for complex information systems provides high semantic power, thus exploiting the semantic expressibility of the underlying data model, while maintaining ease of interaction with the system. In this way, we reach the goal of decreasing cognitive load on the user, with the additional advantage of always maintaining the same interaction style. We present a visual retrieval environment that effectively combines filtering, browsing, and navigation to provide an integrated view of the retrieval problem. Design and implementation issues are outlined for MORE (Multimedia Object Retrieval Environment), a prototype system relying on the proposed model. The focus is on the main user interface functionalities, and actual interaction sessions are presented including schema creation, information loading, and information retrieval.",H.2.1 [Database Management]: Logical Design - data models; H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval - query formulation; selection process,Abstracting; Computer simulation; Data structures; Database systems; Graphical user interfaces; Information services; Information technology; Interactive computer graphics; Systems analysis; Visualization; Browsing; Hypermedia information systems; Hypertext; Information filtering; Multimedia; Multimedia object retrieval environment; Pattern matching; Schema graph; Subgraph; Visual retrieval environment; Information retrieval systems
Evaluation of Model-Based Retrieval Effectiveness with OCR Text,1996,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0002849652&doi=10.1145%2f214174.214180&partnerID=40&md5=74a8e74e78e726860ef66658c7c852f2,"We give a comprehensive report on our experiments with retrieval from OCR-generated text using systems based on standard models of retrieval. More specifically, we show that average precision and recall is not affected by OCR errors across systems for several collections. The collections used in these experiments include both actual OCR-generated text and standard information retrieval collections corrupted through the simulation of OCR errors. Both the actual and simulation experiments include full-text and abstract-length documents. We also demonstrate that the ranking and feedback methods associated with these models are generally not robust enough to deal with OCR errors. It is further shown that the OCR errors and garbage strings generated from the mistranslation of graphic objects increase the size of the index by a wide margin. We not only point out problems that can arise from applying OCR text within an information retrieval environment, we also suggest solutions to overcome some of these problems.",H.3.1 [Information Storage and Retrieval]: Content Analysis and Indexing - indexing methods; H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval - retrieval models; search process,
Sequential Patterns in Information Systems Development: An Application of a Social Process Model,1996,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0029732314&doi=10.1145%2f214174.214178&partnerID=40&md5=8781ef7d2baf9eab407e35ada1496d4d,"We trace the process of developing and implementing a materials management system in one company over a 15-year period. Using a process research model developed by Newman and Robey, we identify 44 events in the process and define them as either encounters or episodes. Encounters are concentrated events, such as meetings and anouncements, that separate episodes, which are events of longer duration. By examining the sequence of events over the 15 years of the case, we identify a pattern of repeated failure, followed by success. Our discussion centers on the value of detecting and displaying such patterns and the need for theoretical interpretation of recurring sequences of events. Five alternative theoretical perspectives, originally proposed by Kling, are used to interpret the sequential patterns identified by the model. We conclude that the form of the process model allows researchers who operate from different perspectives to enrich their understanding of the process of system development.",Human Factors; K.6.1 [Management of Computing and Information Systems]: Project and People Management; Management; Social processes; System implementation,Administrative data processing; Computer systems; Data structures; Database systems; Information retrieval systems; Systems analysis; Materials management system; Process research model; Sequential patterns; Social process model; System implementation; Management information systems
Evaluating hypermedia and learning: methods and results from the Perseus project,1994,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0028131120&partnerID=40&md5=9d4280e622b0780f644640dfedfa4125,"The Perseus Project has developed a hypermedia corpus of materials related to the ancient Greek world. The materials include a variety of texts and images, and tools for using these materials and navigating the system. Results from a three-year evaluation of Perseus use in a variety of college settings are described. The evaluation assessed both this particular system and the application of the technological genre to information management and to learning.",,Information science; Learning systems; Logic design; Navigation systems; Computer milieux; Human computer interaction; Human information processing; Hypermedia; Machine systems; Human engineering
MLI: A Multi-level Inference Mechanism for User Attributes in Social Networks,2022,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85158160635&doi=10.1145%2f3545797&partnerID=40&md5=4f3c53241575acbb540e94e9c790b894,"In the social network, each user has attributes for self-description called user attributes, which are semantically hierarchical. Attribute inference has become an essential way for social platforms to realize user classifications and targeted recommendations. Most existing approaches mainly focus on the flat inference problem neglecting the semantic hierarchy of user attributes, which will cause serious inconsistency in multi-level tasks. In this article, we propose a multi-level model MLI, where information propagation part collects attribute information by mining the global graph structure, and the attribute correction part realizes the mutual correction between different levels of attributes. Further, we put forward the concept of generalized semantic tree, a way of representing the hierarchical structure of user attributes, whose nodes are allowed to have multiple parent nodes unlike the regular tree. Both regular and generalized semantic trees are commonly used in practice, and can be handled by our model. Besides, by making the inference start from sub-networks with sufficient attribute information, we design a ""Ripple""algorithm to improve the efficiency and effectiveness of our model. For evaluation purposes, we conduct extensive verification experiments on DBLP datasets. The experimental results show the superior effect of MLI, compared with the state-of-the-art methods.  © 2022 Association for Computing Machinery.",Attribute inference; hierarchical inference; social network,Graphic methods; Information dissemination; Semantics; Social networking (online); Attribute inference; Attribute information; Hierarchical inference; Inference mechanism; Inference problem; Multilevels; Semantic hierarchies; Semantic tree; Social network; User classification; Inference engines
Capture Salient Historical Information: A Fast and Accurate Non-autoregressive Model for Multi-turn Spoken Language Understanding,2022,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143432685&doi=10.1145%2f3545800&partnerID=40&md5=bf6c108820a37bfee3d2f1f662c20642,"Spoken Language Understanding (SLU), a core component of the task-oriented dialogue system, expects a shorter inference facing the impatience of human users. Existing work increases inference speed by designing non-autoregressive models for single-turn SLU tasks but fails to apply to multi-turn SLU in confronting the dialogue history. The intuitive idea is to concatenate all historical utterances and utilize the non-autoregressive models directly. However, this approach seriously misses the salient historical information and suffers from the uncoordinated-slot problems. To overcome those shortcomings, we propose a novel model for multi-turn SLU named Salient History Attention with Layer-Refined Transformer (SHA-LRT), which comprises a SHA module, a Layer-Refined Mechanism (LRM), and a Slot Label Generation (SLG) task. SHA captures salient historical information for the current dialogue from both historical utterances and results via a well-designed history-attention mechanism. LRM predicts preliminary SLU results from Transformer's middle states and utilizes them to guide the final prediction, and SLG obtains the sequential dependency information for the non-autoregressive encoder. Experiments on public datasets indicate that our model significantly improves multi-turn SLU performance (17.5% on Overall) with accelerating (nearly 15 times) the inference process over the state-of-the-art baseline as well as effective on the single-turn SLU tasks.  © 2022 Association for Computing Machinery.",Multi-task learning; spoken interfaces; task-oriented dialogue system,Speech processing; Speech recognition; Autoregressive modelling; Core components; Dialogue systems; Historical information; Multi-turn; Multitask learning; Spoken interface; Spoken language understanding; Task-oriented; Task-oriented dialog system; Learning systems
Sequence-aware Knowledge Distillation for a Lightweight Event Representation,2022,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85158076372&doi=10.1145%2f3545798&partnerID=40&md5=314e2b979e5a4022039d6aa8a43a1822,"Event representation targets to model the event-reasoning process as a machine-readable format. Previous studies on event representation mostly concentrate on a sole modeling perspective and have not well investigated the scenario-level knowledge, which can cause information loss. To cope with this dilemma, we propose a unified fine-tuning architecture-based approach (UniFA-S) that integrates all levels of trainings, including the scenario-level knowledge. However, another challenge for existing models is the ever-increasing computation overheads, restricting the deployment ability on limited resources devices. Hence, in this article, we aim to compress the cumbersome model UniFA-S into a lighter and easy-to-deploy one without much performance damage. To this end, we propose a sequence-aware knowledge distillation model (SaKD) that employs a dynamic self-distillation on the decouple-compress-couple framework for compressing UniFA-S, which cannot only realize the model compression, but also retain the integrity of individual components. We also design two fitting strategies to address the less-supervised issue at the distillation stage. Comprehensive experiments on representation-and-inference ability-based tasks validate the effectiveness of SaKD. Compared to UniFA-S, SaKD realizes a more portable event representation model at the cost of only 1.0% performance drop in terms of accuracy or Spearman's correlation, which is far less than other knowledge distillation models.  © 2022 Association for Computing Machinery.",event prediction; Event representation; event similarity; knowledge distillation,Architecture-based; Event prediction; Event representations; Event similarity; Fine tuning; Information loss; Knowledge distillation; Machine-readable format; Performance; Reasoning process; Distillation
A Generic Federated Recommendation Framework via Fake Marks and Secret Sharing,2022,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85158166441&doi=10.1145%2f3548456&partnerID=40&md5=36bacfea0101e440d7f9a4dc8d1a0e79,"With the implementation of privacy protection laws such as GDPR, it is increasingly difficult for organizations to legally collect users' data. However, a typical machine learning-based recommendation algorithm requires the data to learn users' preferences. Some recent works thus turn to develop federated learning-based recommendation algorithms, but most of them either cannot protect the users' privacy well, or sacrifice the model accuracy. In this article, we propose a lossless and generic federated recommendation framework via fake marks and secret sharing (FMSS). Our FMSS can not only protect the two types of users' privacy, i.e., rating values and rating behaviors, without sacrificing the recommendation performance, but can also be applied to most recommendation algorithms for rating prediction, item ranking, and sequential recommendation. Specifically, we extend existing fake items to fake marks, and combine it with secret sharing to perturb the data uploaded by the clients to a server. We then apply our FMSS to six representative recommendation algorithms, i.e., MF-MPC and NeuMF for rating prediction, eALS and VAE-CF for item ranking, and Fossil and GRU4Rec for sequential recommendation. The experimental results demonstrate that our FMSS is a lossless and generic framework, which is able to federate a series of different recommendation algorithms in a lossless and privacy-aware manner.  © 2022 Association for Computing Machinery.",fake marks; Federated recommendation; item ranking; rating prediction; secret sharing; sequential recommendation,Fake detection; Learning algorithms; Machine learning; Recommender systems; Fake mark; Federated recommendation; Item rankings; Lossless; Privacy protection; Rating prediction; Recommendation algorithms; Secret-sharing; Sequential recommendation; User privacy; Forecasting
Time-aware Path Reasoning on Knowledge Graph for Recommendation,2022,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159287326&doi=10.1145%2f3531267&partnerID=40&md5=4cc13eaaea728863eba2868269aeb097,"Reasoning on knowledge graph (KG) has been studied for explainable recommendation due to its ability of providing explicit explanations. However, current KG-based explainable recommendation methods unfortunately ignore the temporal information (such as purchase time, recommend time, etc.), which may result in unsuitable explanations. In this work, we propose a novel Time-aware Path reasoning for Recommendation (TPRec for short) method, which leverages the potential of temporal information to offer better recommendation with plausible explanations. First, we present an efficient time-aware interaction relation extraction component to construct collaborative knowledge graph with time-aware interactions (TCKG for short), and then we introduce a novel time-aware path reasoning method for recommendation. We conduct extensive experiments on three real-world datasets. The results demonstrate that the proposed TPRec could successfully employ TCKG to achieve substantial gains and improve the quality of explainable recommendation.  © 2022 Association for Computing Machinery.",Explainable recommendation; reinforcement learning; temporal knowledge graphs,Graph theory; Knowledge graph; 'current; Efficient time; Explainable recommendation; Graph-based; Knowledge graphs; Recommendation methods; Reinforcement learnings; Temporal information; Temporal knowledge; Temporal knowledge graph; Reinforcement learning
A Multi-Objective Optimization Framework for Multi-Stakeholder Fairness-Aware Recommendation,2022,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159177799&doi=10.1145%2f3564285&partnerID=40&md5=8f0ae22c5188d315b8b06adeed103c6c,"Nowadays, most online services are hosted on multi-stakeholder marketplaces, where consumers and producers may have different objectives. Conventional recommendation systems, however, mainly focus on maximizing consumers' satisfaction by recommending the most relevant items to each individual. This may result in unfair exposure of items, thus jeopardizing producer benefits. Additionally, they do not care whether consumers from diverse demographic groups are equally satisfied. To address these limitations, we propose a multi-objective optimization framework for fairness-aware recommendation, Multi-FR, that adaptively balances accuracy and fairness for various stakeholders with Pareto optimality guarantee. We first propose four fairness constraints on consumers and producers. In order to train the whole framework in an end-to-end way, we utilize the smooth rank and stochastic ranking policy to make these fairness criteria differentiable and friendly to back-propagation. Then, we adopt the multiple gradient descent algorithm to generate a Pareto set of solutions, from which the most appropriate one is selected by the Least Misery Strategy. The experimental results demonstrate that Multi-FR largely improves recommendation fairness on multiple stakeholders over the state-of-the-art approaches while maintaining almost the same recommendation accuracy. The training efficiency study confirms our model's ability to simultaneously optimize different fairness constraints for many stakeholders efficiently.  © 2022 Association for Computing Machinery.",Fairness-aware recommendation; multi-objective optimization; multi-stakeholder; Pareto optimal,Backpropagation; Gradient methods; Pareto principle; Stochastic systems; Consumer satisfactions; Demographic groups; Fairness constraints; Fairness-aware recommendation; Multi-objectives optimization; Multi-stakeholder; On-line service; Optimization framework; Pareto-optimal; Pareto-optimality; Multiobjective optimization
Generalized Funnelling: Ensemble Learning and Heterogeneous Document Embeddings for Cross-Lingual Text Classification,2022,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159341116&doi=10.1145%2f3544104&partnerID=40&md5=e0b06c9897c6f941c04fcfe3ab0f9369,"Funnelling (Fun) is a recently proposed method for cross-lingual text classification (CLTC) based on a two-tier learning ensemble for heterogeneous transfer learning (HTL). In this ensemble method, 1st-tier classifiers, each working on a different and language-dependent feature space, return a vector of calibrated posterior probabilities (with one dimension for each class) for each document, and the final classification decision is taken by a meta-classifier that uses this vector as its input. The meta-classifier can thus exploit class-class correlations, and this (among other things) gives Fun an edge over CLTC systems in which these correlations cannot be brought to bear. In this article, we describe Generalized Funnelling (gFun), a generalization of Fun consisting of an HTL architecture in which 1st-tier components can be arbitrary view-generating functions, i.e., language-dependent functions that each produce a language-independent representation (""view"") of the (monolingual) document. We describe an instance of gFun in which the meta-classifier receives as input a vector of calibrated posterior probabilities (as in Fun) aggregated to other embedded representations that embody other types of correlations, such as word-class correlations (as encoded by Word-Class Embeddings), word-word correlations (as encoded by Multilingual Unsupervised or Supervised Embeddings), and word-context correlations (as encoded by multilingual BERT). We show that this instance of gFun substantially improves over Fun and over state-of-the-art baselines by reporting experimental results obtained on two large, standard datasets for multilingual multilabel text classification. Our code that implements gFun is publicly available.  © 2022 Association for Computing Machinery.",cross-lingual text classification; ensemble learning; heterogeneous transfer learning; Transfer learning; word embeddings,Classification (of information); Information retrieval systems; Large dataset; Text processing; Transfer learning; Vector spaces; Cross-lingual; Cross-lingual text classification; Embeddings; Ensemble learning; Heterogeneous transfer learning; Meta-classifiers; Text classification; Transfer learning; Word embedding; Embeddings
Adversarial Auto-encoder Domain Adaptation for Cold-start Recommendation with Positive and Negative Hypergraphs,2022,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159284804&doi=10.1145%2f3544105&partnerID=40&md5=737427dc9950d79056b71ece01c3df1e,"This article presents a novel model named Adversarial Auto-encoder Domain Adaptation to handle the recommendation problem under cold-start settings. Specifically, we divide the hypergraph into two hypergraphs, i.e., a positive hypergraph and a negative one. Below, we adopt the cold-start user recommendation for illustration. After achieving positive and negative hypergraphs, we apply hypergraph auto-encoders to them to obtain positive and negative embeddings of warm users and items. Additionally, we employ a multi-layer perceptron to get warm and cold-start user embeddings called regular embeddings. Subsequently, for warm users, we assign positive and negative pseudo-labels to their positive and negative embeddings, respectively, and treat their positive and regular embeddings as the source and target domain data, respectively. Then, we develop a matching discriminator to jointly minimize the classification loss of the positive and negative warm user embeddings and the distribution gap between the positive and regular warm user embeddings. In this way, warm users' positive and regular embeddings are connected. Since the positive hypergraph maintains the relations between positive warm user and item embeddings, and the regular warm and cold-start user embeddings follow a similar distribution, the regular cold-start user embedding and positive item embedding are bridged to discover their relationship. The proposed model can be easily extended to handle the cold-start item recommendation by changing inputs. We perform extensive experiments on real-world datasets for both cold-start user and cold-start item recommendations. Promising results in terms of precision, recall, normalized discounted cumulative gain, and hit rate verify the effectiveness of the proposed method.  © 2022 Association for Computing Machinery.",adversarial learning; auto-encoder; cold-start recommendation; domain adaptation; hypergraph; Recommendation systems,Data mining; Graph theory; Recommender systems; Signal encoding; Adversarial learning; Auto encoders; Cold-start; Cold-start Recommendations; Domain adaptation; Embeddings; Hyper graph; Regular embedding; User recommendations; Warm start; Embeddings
A Relative Information Gain-based Query Performance Prediction Framework with Generated Query Variants,2022,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85148762796&doi=10.1145%2f3545112&partnerID=40&md5=a65a8825f91270942a7ec569613e54ba,"Query performance prediction (QPP) methods, which aim to predict the performance of a query, often rely on evidences in the form of different characteristic patterns in the distribution of Retrieval Status Values (RSVs). However, for neural IR models, it is usually observed that the RSVs are often less reliable for QPP because they are bounded within short intervals, different from the situation for statistical models. To address this limitation, we propose a model-agnostic QPP framework that gathers additional evidences by leveraging information from the characteristic patterns of RSV distributions computed over a set of automatically generated query variants, relative to that of the current query. Specifically, the idea behind our proposed method - Weighted Relative Information Gain (WRIG), is that a substantial relative decrease or increase in the standard deviation of the RSVs of the query variants is likely to be a relative indicator of how easy or difficult the original query is. To cater for the absence of human-annotated query variants in real-world scenarios, we further propose an automatic query variant generation method. This can produce variants in a controlled manner by substituting terms from the original query with new ones sampled from a weighted distribution, constructed either via a relevance model or with the help of an embedded representation of query terms. Our experiments on the TREC-Robust, ClueWeb09B, and MS MARCO datasets show that WRIG, by the use of this relative changes in QPP estimate, leads to significantly better results than a state-of-the-art baseline method that leverages information from (manually created) query variants by the application of additive smoothing [64]. The results also show that our approach can improve the QPP effectiveness of neural retrieval approaches in particular.  © 2022 Association for Computing Machinery.",neural model retrieval scores; Query performance prediction; query variant generation,Information retrieval; Query processing; Information gain; IR models; Neural model retrieval score; Neural modelling; Performance; Prediction methods; Query variant generation; Query-performance predictions; Relative information; Short-interval; Forecasting
"A Survey on Cross-domain Recommendation: Taxonomies, Methods, and Future Directions",2022,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85158131058&doi=10.1145%2f3548455&partnerID=40&md5=093d77b2c124057123e5bc5bf8f48812,"Traditional recommendation systems are faced with two long-standing obstacles, namely data sparsity and cold-start problems, which promote the emergence and development of Cross-Domain Recommendation (CDR). The core idea of CDR is to leverage information collected from other domains to alleviate the two problems in one domain. Since the early 2010s, many efforts have been engaged for cross-domain recommendation. Recently, with the development of deep learning and neural networks, a large number of methods have emerged. However, there is a limited number of systematic surveys on CDR, especially regarding the latest proposed methods as well as the recommendation scenarios and recommendation tasks they address. In this survey article, we first proposed a two-level taxonomy of cross-domain recommendation that classifies different recommendation scenarios and recommendation tasks. We then introduce and summarize existing cross-domain recommendation approaches under different recommendation scenarios in a structured manner. We also organize datasets commonly used. We conclude this survey by providing several potential research directions about this field.  © 2022 Association for Computing Machinery.",Cross-domain recommendation; datasets; deep learning; machine learning; survey,Clock and data recovery circuits (CDR circuits); Deep learning; Learning systems; Cold start problems; Cross-domain recommendations; Data sparsity; Dataset; Deep learning; Learning network; Machine-learning; Neural-networks; Number of methods; Potential researches; Taxonomies
A Revisiting Study of Appropriate Offline Evaluation for Top-N Recommendation Algorithms,2022,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85158132817&doi=10.1145%2f3545796&partnerID=40&md5=b8a63fb0104c7d20c10d97bd7c4ef8b4,"In recommender systems, top-N recommendation is an important task with implicit feedback data. Although the recent success of deep learning largely pushes forward the research on top-N recommendation, there are increasing concerns on appropriate evaluation of recommendation algorithms. It therefore is important to study how recommendation algorithms can be reliably evaluated and thoroughly verified. This work presents a large-scale, systematic study on six important factors from three aspects for evaluating recommender systems. We carefully select 12 top-N recommendation algorithms and eight recommendation datasets. Our experiments are carefully designed and extensively conducted with these algorithms and datasets. In particular, all the experiments in our work are implemented based on an open sourced recommendation library, Recbole [139], which ensures the reproducibility and reliability of our results. Based on the large-scale experiments and detailed analysis, we derive several key findings on the experimental settings for evaluating recommender systems. Our findings show that some settings can lead to substantial or significant differences in performance ranking of the compared algorithms. In response to recent evaluation concerns, we also provide several suggested settings that are specially important for performance comparison.  © 2022 Association for Computing Machinery.",evaluation; experimental setup; Top-N recommendation,Deep learning; Evaluation; Experimental setup; Implicit feedback; Large-scales; Offline evaluation; Push forwards; Recommendation algorithms; Reproducibilities; Systematic study; Top-N recommendation; Recommender systems
Are Neural Ranking Models Robust?,2022,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138620788&doi=10.1145%2f3534928&partnerID=40&md5=f1889317ae7d0b1d45104af2f6c0febd,"Recently, we have witnessed the bloom of neural ranking models in the information retrieval (IR) field. So far, much effort has been devoted to developing effective neural ranking models that can generalize well on new data. There has been less attention paid to the robustness perspective. Unlike the effectiveness, which is about the average performance of a system under normal purpose, robustness cares more about the system performance in the worst case or under malicious operations instead. When a new technique enters into the real-world application, it is critical to know not only how it works in average, but also how would it behave in abnormal situations. So, we raise the question in this work: Are neural ranking models robust To answer this question, first, we need to clarify what we refer to when we talk about the robustness of ranking models in IR. We show that robustness is actually a multi-dimensional concept and there are three ways to define it in IR: (1) the performance variance under the independent and identically distributed (I.I.D.) setting; (2) the out-of-distribution (OOD) generalizability; and (3) the defensive ability against adversarial operations. The latter two definitions can be further specified into two different perspectives, respectively, leading to five robustness tasks in total. Based on this taxonomy, we build corresponding benchmark datasets, design empirical experiments, and systematically analyze the robustness of several representative neural ranking models against traditional probabilistic ranking models and learning-to-rank (LTR) models. The empirical results show that there is no simple answer to our question. While neural ranking models are less robust against other IR models in most cases, some of them can still win two out of five tasks. This is the first comprehensive study on the robustness of neural ranking models. We believe the way we study the robustness as well as our findings would be beneficial to the IR community. We will also release all the data and codes to facilitate the future research in this direction.  © 2022 Association for Computing Machinery.",Ranking Models; Robustness; Systematic Analysis,Benchmark datasets; Empirical experiments; Multi dimensional; Performance; Probabilistic ranking; Ranking model; Real-world; Robustness; Systematic analysis; Systems performance
An Adaptive Graph Pre-training Framework for Localized Collaborative Filtering,2022,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85158147178&doi=10.1145%2f3555372&partnerID=40&md5=10517a1573995207af8614b47d37f5e2,"Graph neural networks (GNNs) have been widely applied in the recommendation tasks and have achieved very appealing performance. However, most GNN-based recommendation methods suffer from the problem of data sparsity in practice. Meanwhile, pre-training techniques have achieved great success in mitigating data sparsity in various domains such as natural language processing (NLP) and computer vision (CV). Thus, graph pre-training has the great potential to alleviate data sparsity in GNN-based recommendations. However, pre-training GNNs for recommendations faces unique challenges. For example, user-item interaction graphs in different recommendation tasks have distinct sets of users and items, and they often present different properties. Therefore, the successful mechanisms commonly used in NLP and CV to transfer knowledge from pre-training tasks to downstream tasks such as sharing learned embeddings or feature extractors are not directly applicable to existing GNN-based recommendations models. To tackle these challenges, we delicately design an adaptive graph pre-training framework for localized collaborative filtering (ADAPT). It does not require transferring user/item embeddings, and is able to capture both the common knowledge across different graphs and the uniqueness for each graph simultaneously. Extensive experimental results have demonstrated the effectiveness and superiority of ADAPT.  © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Graph neural networks; model pre-training; recommendation systems,Collaborative filtering; Embeddings; Knowledge graph; Natural language processing systems; Recommender systems; Data sparsity; Embeddings; Graph neural networks; Language processing; Localised; Model pre-training; Natural languages; Network-based; Pre-training; Training framework; Graph neural networks
"On the Robustness of Aspect-based Sentiment Analysis: Rethinking Model, Data, and Training",2022,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147543501&doi=10.1145%2f3564281&partnerID=40&md5=e7c667c7e914298a411b91f09f1af1a2,"Aspect-based sentiment analysis (ABSA) aims at automatically inferring the specific sentiment polarities toward certain aspects of products or services behind the social media texts or reviews, which has been a fundamental application to the real-world society. Since the early 2010s, ABSA has achieved extraordinarily high accuracy with various deep neural models. However, existing ABSA models with strong in-house performances may fail to generalize to some challenging cases where the contexts are variable, i.e., low robustness to real-world environments. In this study, we propose to enhance the ABSA robustness by systematically rethinking the bottlenecks from all possible angles, including model, data, and training. First, we strengthen the current best-robust syntax-aware models by further incorporating the rich external syntactic dependencies and the labels with aspect simultaneously with a universal-syntax graph convolutional network. In the corpus perspective, we propose to automatically induce high-quality synthetic training data with various types, allowing models to learn sufficient inductive bias for better robustness. Last, we based on the rich pseudo data perform adversarial training to enhance the resistance to the context perturbation and meanwhile employ contrastive learning to reinforce the representations of instances with contrastive sentiments. Extensive robustness evaluations are conducted. The results demonstrate that our enhanced syntax-aware model achieves better robustness performances than all the state-of-the-art baselines. By additionally incorporating our synthetic corpus, the robust testing results are pushed with around 10% accuracy, which are then further improved by installing the advanced training strategies. In-depth analyses are presented for revealing the factors influencing the ABSA robustness.  © 2022 Association for Computing Machinery.",adversarial training; contrastive learning; Data mining; robust study; sentiment analysis; social media; syntactic structure,Data mining; Learning systems; Social networking (online); Syntactics; Adversarial training; Contrastive learning; Model training; Modeling data; Performance; Real-world; Robust study; Sentiment analysis; Social media; Syntactic structure; Sentiment analysis
Ranking Models for the Temporal Dimension of Text,2022,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159160089&doi=10.1145%2f3565481&partnerID=40&md5=3a28e1833b91f4c2247441a867fab8aa,"Temporal features of text have been shown to improve clustering and organization of documents, text classification, visualization, and ranking. Temporal ranking models consider the temporal expressions found in text (e.g., ""in 2021""or ""last year"") as time units, rather than as keywords, to define a temporal relevance and improve ranking. This article introduces a new class of ranking models called Temporal Metric Space Models (TMSM), based on a new domain for representing temporal information found in documents and queries, where each temporal expression is represented as a time interval. Furthermore, we introduce a new frequency-based baseline called Temporal BM25 (TBM25). We evaluate the effectiveness of each proposed metric against a purely textual baseline, as well as several variations of the metrics themselves, where we change the aggregate function, the time granularity and the combination weight. Our extensive experiments on five test collections show statistically significant improvements of TMSM and TBM25 over state-of-the-art temporal ranking models. Combining the temporal similarity scores with the text similarity scores always improves the results, when the combination weight is between 2% and 6% for the temporal scores. This is true also for test collections where only 5% of queries contain explicit temporal expressions.  © 2022 Association for Computing Machinery.",Temporal information retrieval; Temporal Metric Space; temporal ranking; texto-temporal relevance; time similarity; timexes,Information retrieval; Information retrieval systems; Knowledge management; Set theory; Text processing; Topology; Metric spaces; Ranking model; Temporal expressions; Temporal information retrievals; Temporal metric space; Temporal ranking; Temporal relevance; Texto-temporal relevance; Time similarity; Timexes; Classification (of information)
Learning to Ask: Conversational Product Search via Representation Learning,2022,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142741737&doi=10.1145%2f3555371&partnerID=40&md5=d794d608df80e2730851e733a2e857e4,"Online shopping platforms, such as Amazon and AliExpress, are increasingly prevalent in society, helping customers purchase products conveniently. With recent progress in natural language processing, researchers and practitioners shift their focus from traditional product search to conversational product search. Conversational product search enables user-machine conversations and through them collects explicit user feedback that allows to actively clarify the users' product preferences. Therefore, prospective research on an intelligent shopping assistant via conversations is indispensable. Existing publications on conversational product search either model conversations independently from users, queries, and products or lead to a vocabulary mismatch. In this work, we propose a new conversational product search model, ConvPS, to assist users in locating desirable items. The model is first trained to jointly learn the semantic representations of user, query, item, and conversation via a unified generative framework. After learning these representations, they are integrated to retrieve the target items in the latent semantic space. Meanwhile, we propose a set of greedy and explore-exploit strategies to learn to ask the user a sequence of high-performance questions for conversations. Our proposed ConvPS model can naturally integrate the representation learning of the user, query, item, and conversation into a unified generative framework, which provides a promising avenue for constructing accurate and robust conversational product search systems that are flexible and adaptive. Experimental results demonstrate that our ConvPS model significantly outperforms state-of-the-art baselines.  © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Conversational product search; learning to ask; representation learning,Learning systems; Natural language processing systems; Sales; Semantics; Conversational product search; Language processing; Learn+; Learning to ask; Natural languages; Online shopping; Recent progress; Representation learning; Traditional products; User query; Query processing
Collaborative Graph Learning for Session-based Recommendation,2022,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130268511&doi=10.1145%2f3490479&partnerID=40&md5=28661332c4b9ff0b32f5531df1056e9c,"Session-based recommendation (SBR), which mainly relies on a user's limited interactions with items to generate recommendations, is a widely investigated task. Existing methods often apply RNNs or GNNs to model user's sequential behavior or transition relationship between items to capture her current preference. For training such models, the supervision signals are merely generated from the sequential interactions inside a session, neglecting the correlations of different sessions, which we argue can provide additional supervisions for learning the item representations. Moreover, previous methods mainly adopt the cross-entropy loss for training, where the user's ground truth preference distribution towards items is regarded as a one-hot vector of the target item, easily making the network over-confident and leading to a serious overfitting problem. Thus, in this article, we propose a Collaborative Graph Learning (CGL) approach for session-based recommendation. CGL first applies the Gated Graph Neural Networks (GGNNs) to learn item embeddings and then is trained by considering both the main supervision as well as the self-supervision signals simultaneously. The main supervisions are produced by the sequential order while the self-supervisions are derived from the global graph constructed by all sessions. In addition, to prevent overfitting, we propose a Target-aware Label Confusion (TLC) learning method in the main supervised component. Extensive experiments are conducted on three publicly available datasets, i.e., Retailrocket, Diginetica, and Gowalla. The experimental results show that CGL can outperform the state-of-the-art baselines in terms of Recall and MRR.  © 2022 Association for Computing Machinery.",collaborative learning; graph neural networks; label confusion learning; self-supervised learning; Session-based recommendation,Behavioral research; Data mining; Learning systems; 'current; Collaborative learning; Cross entropy; Entropy loss; Graph neural networks; Ground truth; Label confusion learning; Self-supervised learning; Sequential interactions; Session-based recommendation; Graph neural networks
Simulating and Modeling the Risk of Conversational Search,2022,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130255431&doi=10.1145%2f3507357&partnerID=40&md5=3c4b9f6c2eccb4b12d70b1ac2b3ed0d2,"In conversational search, agents can interact with users by asking clarifying questions to increase their chance of finding better results. Many recent works and shared tasks in both natural language processing and information retrieval communities have focused on identifying the need to ask clarifying questions and methodologies of generating them. These works assume that asking a clarifying question is a safe alternative to retrieving results. As existing conversational search models are far from perfect, it is possible and common that they could retrieve/generate bad clarifying questions. Asking too many clarifying questions can also drain a user's patience when the user prefers searching efficiency over correctness. Hence, these models can backfire and harm a user's search experience due to these risks from asking clarifying questions. In this work, we propose a simulation framework to simulate the risk of asking questions in conversational search and further revise a risk-aware conversational search model to control the risk. We show the model's robustness and effectiveness through extensive experiments on three conversational datasets - MSDialog, Ubuntu Dialog Corpus, and Opendialkg - in which we compare it with multiple baselines. We show that the risk-control module can work with two different re-ranker models and outperform all of the baselines in most of our experiments.  © 2022 Association for Computing Machinery.",Conversational search; reinforcement learning; risk control,Reinforcement learning; User profile; Control module; Conversational search; Language informations; Model robustness; Risk aware; Risks controls; Search agents; Search models; Searching efficiency; Simulation framework; Natural language processing systems
Learning Text-image Joint Embedding for Efficient Cross-modal Retrieval with Deep Feature Engineering,2022,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129489567&doi=10.1145%2f3490519&partnerID=40&md5=cc95d083da39a47e07f0c30bcab2e0f8,"This article introduces a two-phase deep feature engineering framework for efficient learning of semantics enhanced joint embedding, which clearly separates the deep feature engineering in data preprocessing from training the text-image joint embedding model. We use the Recipe1M dataset for the technical description and empirical validation. In preprocessing, we perform deep feature engineering by combining deep feature engineering with semantic context features derived from raw text-image input data. We leverage LSTM to identify key terms, deep NLP models from the BERT family, TextRank, or TF-IDF to produce ranking scores for key terms before generating the vector representation for each key term by using Word2vec. We leverage Wide ResNet50 and Word2vec to extract and encode the image category semantics of food images to help semantic alignment of the learned recipe and image embeddings in the joint latent space. In joint embedding learning, we perform deep feature engineering by optimizing the batch-hard triplet loss function with soft-margin and double negative sampling, taking into account also the category-based alignment loss and discriminator-based alignment loss. Extensive experiments demonstrate that our SEJE approach with deep feature engineering significantly outperforms the state-of-the-art approaches.  © 2021 Association for Computing Machinery.",Cross-modal retrieval; deep feature engineering; multi-modal learning,Embeddings; Engineering education; Image enhancement; Long short-term memory; Semantics; Cross-modal; Cross-modal retrieval; Deep feature engineering; Embeddings; Feature engineerings; Image joints; Learning text; Multi-modal learning; Text images; Two phase; Alignment
Dynamic Graph Reasoning for Conversational Open-Domain Question Answering,2022,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130253015&doi=10.1145%2f3498557&partnerID=40&md5=73e68a42f048f9b402f07e974f6b5062,"In recent years, conversational agents have provided a natural and convenient access to useful information in people's daily life, along with a broad and new research topic, conversational question answering (QA). On the shoulders of conversational QA, we study the conversational open-domain QA problem, where users' information needs are presented in a conversation and exact answers are required to extract from the Web. Despite its significance and value, building an effective conversational open-domain QA system is non-trivial due to the following challenges: (1) precisely understand conversational questions based on the conversation context; (2) extract exact answers by capturing the answer dependency and transition flow in a conversation; and (3) deeply integrate question understanding and answer extraction. To address the aforementioned issues, we propose an end-to-end Dynamic Graph Reasoning approach to Conversational open-domain QA (DGRCoQA for short). DGRCoQA comprises three components, i.e., a dynamic question interpreter (DQI), a graph reasoning enhanced retriever (GRR), and a typical Reader, where the first one is developed to understand and formulate conversational questions while the other two are responsible to extract an exact answer from the Web. In particular, DQI understands conversational questions by utilizing the QA context, sourcing from predicted answers returned by the Reader, to dynamically attend to the most relevant information in the conversation context. Afterwards, GRR attempts to capture the answer flow and select the most possible passage that contains the answer by reasoning answer paths over a dynamically constructed context graph. Finally, the Reader, a reading comprehension model, predicts a text span from the selected passage as the answer. DGRCoQA demonstrates its strength in the extensive experiments conducted on a benchmark dataset. It significantly outperforms the existing methods and achieves the state-of-the-art performance.  © 2022 Association for Computing Machinery.",Conversational question answering; open-domain question answering,Conversational agents; Conversational question answering; Daily lives; Dynamic graph; Non-trivial; Open domain question answering; Question Answering; Question answering systems; Research topics; User information need; Flow graphs
Combining Graph Convolutional Neural Networks and Label Propagation,2022,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130236115&doi=10.1145%2f3490478&partnerID=40&md5=12a7b18125d84c0aa956bd7cec054ba6,"Label Propagation Algorithm (LPA) and Graph Convolutional Neural Networks (GCN) are both message passing algorithms on graphs. Both solve the task of node classification, but LPA propagates node label information across the edges of the graph, while GCN propagates and transforms node feature information. However, while conceptually similar, theoretical relationship between LPA and GCN has not yet been systematically investigated. Moreover, it is unclear how LPA and GCN can be combined under a unified framework to improve the performance. Here we study the relationship between LPA and GCN in terms of feature/label influence, in which we characterize how much the initial feature/label of one node influences the final feature/label of another node in GCN/LPA. Based on our theoretical analysis, we propose an end-to-end model that combines GCN and LPA. In our unified model, edge weights are learnable, and the LPA serves as regularization to assist the GCN in learning proper edge weights that lead to improved performance. Our model can also be seen as learning the weights of edges based on node labels, which is more direct and efficient than existing feature-based attention models or topology-based diffusion models. In a number of experiments for semi-supervised node classification and knowledge-graph-aware recommendation, our model shows superiority over state-of-the-art baselines.  © 2021 Association for Computing Machinery.",Graph convolutional neural networks; label propagation algorithm; semi-supervised learning,Backpropagation; Classification (of information); Convolution; Convolutional neural networks; Knowledge graph; Message passing; Supervised learning; Topology; Edge weights; Feature information; Label information; Label propagation; Label propagation algorithm; Message-passing algorithm; Performance; Propagation algorithm; Propagation graph; Unified framework; Graph neural networks
The yes Footprint of Factorization Models and Their Applications in Collaborative Filtering,2022,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130301369&doi=10.1145%2f3490475&partnerID=40&md5=9d9cbc4e65c412fe21e811135abc162e,"Factorization models have been successfully applied to the recommendation problems and have significant impact to both academia and industries in the field of Collaborative Filtering (CF). However, the intermediate data generated in factorization models' decision making process (or training process, footprint) have been overlooked even though they may provide rich information to further improve recommendations. In this article, we introduce the concept of Convergence Pattern, which records how ratings are learned step-by-step in factorization models in the field of CF. We show that the concept of Convergence Patternexists in both the model perspective (e.g., classical Matrix Factorization (MF) and deep-learning factorization) and the training (learning) perspective (e.g., stochastic gradient descent (SGD), alternating least squares (ALS), and Markov Chain Monte Carlo (MCMC)). By utilizing the Convergence Pattern, we propose a prediction model to estimate the prediction reliability of missing ratings and then improve the quality of recommendations. Two applications have been investigated: (1) how to evaluate the reliability of predicted missing ratings and thus recommend those ratings with high reliability. (2) How to explore the estimated reliability to adjust the predicted ratings to further improve the predication accuracy. Extensive experiments have been conducted on several benchmark datasets on three recommendation tasks: decision-aware recommendation, rating predicted, and Top-N recommendation. The experiment results have verified the effectiveness of the proposed methods in various aspects.  © 2021 Association for Computing Machinery.",decision making process; decision-aware recommendation; recommendation self-rectification; Recommendation system,Decision making; Deep learning; Gradient methods; Knowledge management; Markov processes; Matrix factorization; Reliability; Stochastic models; Stochastic systems; Alternating least squares; Decision-aware recommendation; Decision-making process; Factorization model; Markov chain Monte Carlo; Matrix factorizations; Modeling decision makings; Recommendation self-rectification; Stochastic gradient descent; Training process; Collaborative filtering
Toward Personalized Answer Generation in E-Commerce via Multi-perspective Preference Modeling,2022,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124780221&doi=10.1145%2f3507782&partnerID=40&md5=292f39ab3ac4a7ddd55eebee9f3f0bd0,"Recently, Product Question Answering (PQA) on E-Commerce platforms has attracted increasing attention as it can act as an intelligent online shopping assistant and improve the customer shopping experience. Its key function, automatic answer generation for product-related questions, has been studied by aiming to generate content-preserving while question-related answers. However, an important characteristic of PQA, i.e., personalization, is neglected by existing methods. It is insufficient to provide the same ""completely summarized""answer to all customers, since many customers are more willing to see personalized answers with customized information only for themselves, by taking into consideration their own preferences toward product aspects or information needs. To tackle this challenge, we propose a novel Personalized Answer GEneration method with multi-perspective preference modeling, which explores historical user-generated contents to model user preference for generating personalized answers in PQA. Specifically, we first retrieve question-related user history as external knowledge to model knowledge-level user preference. Then, we leverage the Gaussian Softmax distribution model to capture latent aspect-level user preference. Finally, we develop a persona-aware pointer network to generate personalized answers in terms of both content and style by utilizing personal user preference and dynamic user vocabulary. Experimental results on real-world E-Commerce QA datasets demonstrate that the proposed method outperforms existing methods by generating informative and customized answers and show that answer generation in E-Commerce can benefit from personalization.  © 2022 Association for Computing Machinery.",Answer generation; E-Commerce; personalization; product question answering,Sales; User profile; Answer generation; Commerce platforms; Customized information; Multi-perspective; Online shopping; Personalizations; Preference models; Product question answering; Question Answering; User's preferences; Electronic commerce
"Understanding the ""Pathway"" Towards a Searcher's Learning Objective",2022,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130288143&doi=10.1145%2f3495222&partnerID=40&md5=6280bc86b2fc5cea4e903da93db0d926,"Search systems are often used to support learning-oriented goals. This trend has given rise to the ""search-as-learning""movement, which proposes that search systems should be designed to support learning. To this end, an important research question is: How does a searcher's type of learning objective (LO) influence their trajectory (or pathway) toward that objective? We report on a lab study (N = 36) in which participants gathered information to meet a specific type of LO. To characterize LOs and pathways, we leveraged Anderson and Krathwohl's (A&K's) taxonomy [3]. A&K's taxonomy situates LOs at the intersection of two orthogonal dimensions: (1) cognitive process (CP) (remember, understand, apply, analyze, evaluate, and create) and (2) knowledge type (factual, conceptual, procedural, and metacognitive knowledge). Participants completed learning-oriented search tasks that varied along three CPs (apply, evaluate, and create) and three knowledge types (factual, conceptual, and procedural knowledge). A pathway is defined as a sequence of learning instances (e.g., subgoals) that were also each classified into cells from A&K's taxonomy. Our study used a think-aloud protocol, and pathways were generated through a qualitative analysis of participants' think-aloud comments and recorded screen activities. We investigate three research questions. First, in RQ1, we study the impact of the LO on pathway characteristics (e.g., pathway length). Second, in RQ2, we study the impact of the LO on the types of A&K cells traversed along the pathway. Third, in RQ3, we study common and uncommon transitions between A&K cells along pathways conditioned on the knowledge type of the objective. We discuss implications of our results for designing search systems to support learning.  © 2022 Association for Computing Machinery.",learning objectives; learning pathways; Search-as-learning,Learning systems; Taxonomies; Andersons; Conceptual knowledge; Factual knowledge; Knowledge types; Learning objectives; Learning pathway; Research questions; Search system; Search-as-learning; Support learning; Safety devices
Relevance Assessments for Web Search Evaluation: Should We Randomise or Prioritise the Pooled Documents?,2022,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130219843&doi=10.1145%2f3494833&partnerID=40&md5=058a46ab9b8f4909335a1a9c667190cc,"In the context of depth-k pooling for constructing web search test collections, we compare two approaches to ordering pooled documents for relevance assessors: The prioritisation strategy (PRI) used widely at NTCIR, and the simple randomisation strategy (RND). In order to address research questions regarding PRI and RND, we have constructed and released the WWW3E8 dataset, which contains eight independent relevance labels for 32,375 topic-document pairs, i.e., a total of 259,000 labels. Four of the eight relevance labels were obtained from PRI-based pools; the other four were obtained from RND-based pools. Using WWW3E8, we compare PRI and RND in terms of inter-assessor agreement, system ranking agreement, and robustness to new systems that did not contribute to the pools. We also utilise an assessor activity log we obtained as a byproduct of WWW3E8 to compare the two strategies in terms of assessment efficiency. Our main findings are: (a) The presentation order has no substantial impact on assessment efficiency; (b) While the presentation order substantially affects which documents are judged (highly) relevant, the difference between the inter-assessor agreement under the PRI condition and that under the RND condition is of no practical significance; (c) Different system rankings under the PRI condition are substantially more similar to one another than those under the RND condition; and (d) PRI-based relevance assessment files (qrels) are substantially and statistically significantly more robust to new systems than RND-based ones. Finding (d) suggests that PRI helps the assessors identify relevant documents that affect the evaluation of many existing systems, including those that did not contribute to the pools. Hence, if researchers need to evaluate their current IR systems using legacy IR test collections, we recommend the use of those constructed using the PRI approach unless they have a good reason to believe that their systems retrieve relevant documents that are vastly different from the pooled documents. While this robustness of PRI may also mean that the PRI-based pools are biased against future systems that retrieve highly novel relevant documents, one should note that there is no evidence that RND is any better in this respect.  © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Information retrieval; pooling; relevance assessments; test collections; web search,Efficiency; Information retrieval; Lakes; Legacy systems; Condition; Inter-assessor agreements; Pooling; Prioritization; Randomisation; Relevance assessments; Relevant documents; System rankings; Test Collection; Web searches; Websites
Reinforced Neighborhood Selection Guided Multi-Relational Graph Neural Networks,2022,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130298782&doi=10.1145%2f3490181&partnerID=40&md5=c9d13160bb17b83da3dd8eff05c25f70,"Graph Neural Networks (GNNs) have been widely used for the representation learning of various structured graph data, typically through message passing among nodes by aggregating their neighborhood information via different operations. While promising, most existing GNNs oversimplify the complexity and diversity of the edges in the graph and thus are inefficient to cope with ubiquitous heterogeneous graphs, which are typically in the form of multi-relational graph representations. In this article, we propose RioGNN, a novel Reinforced, recursive, and flexible neighborhood selection guided multi-relational Graph Neural Network architecture, to navigate complexity of neural network structures whilst maintaining relation-dependent representations. We first construct a multi-relational graph, according to the practical task, to reflect the heterogeneity of nodes, edges, attributes, and labels. To avoid the embedding over-assimilation among different types of nodes, we employ a label-aware neural similarity measure to ascertain the most similar neighbors based on node attributes. A reinforced relation-aware neighbor selection mechanism is developed to choose the most similar neighbors of a targeting node within a relation before aggregating all neighborhood information from different relations to obtain the eventual node embedding. Particularly, to improve the efficiency of neighbor selecting, we propose a new recursive and scalable reinforcement learning framework with estimable depth and width for different scales of multi-relational graphs. RioGNN can learn more discriminative node embedding with enhanced explainability due to the recognition of individual importance of each relation via the filtering threshold mechanism. Comprehensive experiments on real-world graph data and practical tasks demonstrate the advancements of effectiveness, efficiency, and the model explainability, as opposed to other comparative GNN models.  © 2021 Association for Computing Machinery.",Graph neural network; multi-relational graph; node embedding; recursive optimization; reinforcement learning,Complex networks; Efficiency; Embeddings; Graph theory; Message passing; Network architecture; Reinforcement learning; Embeddings; Graph data; Graph neural networks; Multi-relational graph; Neighborhood information; Neighbourhood; Node embedding; Recursive optimization; Relational graph; Similar neighbors; Graph neural networks
MiDTD: A Simple and Effective Distillation Framework for Distantly Supervised Relation Extraction,2022,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130285708&doi=10.1145%2f3503917&partnerID=40&md5=b472be10f864d15fcbabf07944581606,"Relation extraction (RE), an important information extraction task, faced the great challenge brought by limited annotation data. To this end, distant supervision was proposed to automatically label RE data, and thus largely increased the number of annotated instances. Unfortunately, lots of noise relation annotations brought by automatic labeling become a new obstacle. Some recent studies have shown that the teacher-student framework of knowledge distillation can alleviate the interference of noise relation annotations via label softening. Nevertheless, we find that they still suffer from two problems: propagation of inaccurate dark knowledge and constraint of a unified distillation temperature. In this article, we propose a simple and effective Multi-instance Dynamic Temperature Distillation (MiDTD) framework, which is model-agnostic and mainly involves two modules: multi-instance target fusion (MiTF) and dynamic temperature regulation (DTR). MiTF combines the teacher's predictions for multiple sentences with the same entity pair to amend the inaccurate dark knowledge in each student's target. DTR allocates alterable distillation temperatures to different training instances to enable the softness of most student's targets to be regulated to a moderate range. In experiments, we construct three concrete MiDTD instantiations with BERT, PCNN, and BiLSTM-based RE models, and the distilled students significantly outperform their teachers and the state-of-the-art (SOTA) methods.  © 2022 Association for Computing Machinery.",distant supervision; knowledge distillation; label softening; multi-instance learning; Natural language processing; neural network; NLP,Backpropagation; Data mining; Natural language processing systems; Neural networks; Students; Distant supervision; Distillation temperature; Knowledge distillation; Label softening; Multi-instance learning; Neural-networks; Relation extraction; Simple++; Target fusions; Teachers'; Distillation
Scaling High-Quality Pairwise Link-Based Similarity Retrieval on Billion-Edge Graphs,2022,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129887477&doi=10.1145%2f3495209&partnerID=40&md5=45bccbe5889924087e1993ea98bdc955,"SimRank is an attractive link-based similarity measure used in fertile fields of Web search and sociometry. However, the existing deterministic method by Kusumoto et al. [24] for retrieving SimRank does not always produce high-quality similarity results, as it fails to accurately obtain diagonal correction matrix D. Moreover, SimRank has a ""connectivity trait""problem: increasing the number of paths between a pair of nodes would decrease its similarity score. The best-known remedy, SimRank++ [1], cannot completely fix this problem, since its score would still be zero if there are no common in-neighbors between two nodes.In this article, we study fast high-quality link-based similarity search on billion-scale graphs. (1) We first devise a ""varied-D""method to accurately compute SimRank in linear memory. We also aggregate duplicate computations, which reduces the time of [24] from quadratic to linear in the number of iterations. (2) We propose a novel ""cosine-based""SimRank model to circumvent the ""connectivity trait""problem. (3) To substantially speed up the partial-pairs ""cosine-based""SimRank search on large graphs, we devise an efficient dimensionality reduction algorithm, PSR#, with guaranteed accuracy. (4) We give mathematical insights to the semantic difference between SimRank and its variant, and correct an argument in [24] that ""if D is replaced by a scaled identity matrix (1-Æ"")I, their top-K rankings will not be affected much"". (5) We propose a novel method that can accurately convert from Li et al. SimRank ∼{S} to Jeh and Widom's SimRank S. (6) We propose GSR#, a generalisation of our ""cosine-based""SimRank model, to quantify pairwise similarities across two distinct graphs, unlike SimRank that would assess nodes across two graphs as completely dissimilar. Extensive experiments on various datasets demonstrate the superiority of our proposed approaches in terms of high search quality, computational efficiency, accuracy, and scalability on billion-edge graphs.  © 2022 Association for Computing Machinery.",link analysis; scalable algorithms; Similarity search,Computational efficiency; Matrix algebra; Semantics; Fertile fields; High quality; Link analysis; Link-based; Scalable algorithms; Scalings; Similarity measure; Similarity retrieval; Similarity search; Simrank; Graphic methods
Graph Co-Attentive Session-based Recommendation,2022,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130261451&doi=10.1145%2f3486711&partnerID=40&md5=5eee676d9f8ccf7f64b66bc6a4d82124,"Session-based recommendation aims to generate recommendations merely based on the ongoing session, which is a challenging task. Previous methods mainly focus on modeling the sequential signals or the transition relations between items in the current session using RNNs or GNNs to identify user's intent for recommendation. Such models generally ignore the dynamic connections between the local and global item transition patterns, although the global information is taken into consideration by exploiting the global-level pair-wise item transitions. Moreover, existing methods that mainly adopt the cross-entropy loss with softmax generally face a serious over-fitting problem, harming the recommendation accuracy. Thus, in this article, we propose a Graph Co-Attentive Recommendation Machine (GCARM) for session-based recommendation. In detail, we first design a Graph Co-Attention Network (GCAT) to consider the dynamic correlations between the local and global neighbors of each node during the information propagation. Then, the item-level dynamic connections between the output of the local and global graphs are modeled to generate the final item representations. After that, we produce the prediction scores and design a Max Cross-Entropy (MCE) loss to prevent over-fitting. Extensive experiments are conducted on three benchmark datasets, i.e., Diginetica, Gowalla, and Yoochoose. The experimental results show that GCARM can achieve the state-of-the-art performance in terms of Recall and MRR, especially on boosting the ranking of the target item.  © 2021 Association for Computing Machinery.",graph co-attention networks; max cross-entropy; Session-based recommendation,Entropy; 'current; Cross entropy; Entropy loss; Global informations; Graph co-attention network; Max cross-entropy; Over fitting problem; Session-based recommendation; Transition patterns; Transition relations; Information dissemination
Complex-valued Neural Network-based Quantum Language Models,2022,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130242036&doi=10.1145%2f3505138&partnerID=40&md5=ef07c38464c98452973fae0fd3d413d7,"Language modeling is essential in Natural Language Processing and Information Retrieval related tasks. After the statistical language models, Quantum Language Model (QLM) has been proposed to unify both single words and compound terms in the same probability space without extending term space exponentially. Although QLM achieved good performance in ad hoc retrieval, it still has two major limitations: (1) QLM cannot make use of supervised information, mainly due to the iterative and non-differentiable estimation of the density matrix, which represents both queries and documents in QLM. (2) QLM assumes the exchangeability of words or word dependencies, neglecting the order or position information of words.This article aims to generalize QLM and make it applicable to more complicated matching tasks (e.g., Question Answering) beyond ad hoc retrieval. We propose a complex-valued neural network-based QLM solution called C-NNQLM to employ an end-to-end approach to build and train density matrices in a light-weight and differentiable manner, and it can therefore make use of external well-trained word vectors and supervised labels. Furthermore, C-NNQLM adopts complex-valued word vectors whose phase vectors can directly encode the order (or position) information of words. Note that complex numbers are also essential in the quantum theory. We show that the real-valued NNQLM (R-NNQLM) is a special case of C-NNQLM.The experimental results on the QA task show that both R-NNQLM and C-NNQLM achieve much better performance than the vanilla QLM, and C-NNQLM's performance is on par with state-of-the-art neural network models. We also evaluate the proposed C-NNQLM on text classification and document retrieval tasks. The results on most datasets show that the C-NNQLM can outperform R-NNQLM, which demonstrates the usefulness of the complex representation for words and sentences in C-NNQLM.  © 2022 Association for Computing Machinery.",language model; neural network; Quantum theory; question answering,Classification (of information); Complex networks; Computational linguistics; Information retrieval systems; Matrix algebra; Modeling languages; Natural language processing systems; Text processing; Ad Hoc retrieval; Complex-valued neural networks; Density matrix; Language model; Network-based; Neural-networks; Performance; Position information; Question Answering; Word vectors; Quantum theory
"""What Can I Cook with these Ingredients?"" - Understanding Cooking-Related Information Needs in Conversational Search",2022,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128598953&doi=10.1145%2f3498330&partnerID=40&md5=d565ccada4d1a7d5c27c327fb33d6011,"As conversational search becomes more pervasive, it becomes increasingly important to understand the users' underlying information needs when they converse with such systems in diverse domains. We conduct an in situ study to understand information needs arising in a home cooking context as well as how they are verbally communicated to an assistant. A human experimenter plays this role in our study. Based on the transcriptions of utterances, we derive a detailed hierarchical taxonomy of diverse information needs occurring in this context, which require different levels of assistance to be solved. The taxonomy shows that needs can be communicated through different linguistic means and require different amounts of context to be understood. In a second contribution, we perform classification experiments to determine the feasibility of predicting the type of information need a user has during a dialogue using the turn provided. For this multi-label classification problem, we achieve average F1 measures of 40% using BERT-based models. We demonstrate with examples which types of needs are difficult to predict and show why, concluding that models need to include more context information in order to improve both information need classification and assistance to make such systems usable.  © 2022 Association for Computing Machinery.",conversational agents; Conversational search; information needs,Search engines; Context information; Conversational agents; Conversational search; Diverse domains; Hierarchical taxonomy; In-Situ Study; Taxonomies
Leveraging Narrative to Generate Movie Script,2022,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130215618&doi=10.1145%2f3507356&partnerID=40&md5=152d5f4910923d45546cc98f9a7f903a,"Generating a text based on a predefined guideline is an interesting but challenging problem. A series of studies have been carried out in recent years. In dialogue systems, researchers have explored driving a dialogue based on a plan, while in story generation, a storyline has also been proved to be useful. In this article, we address a new task - generating movie scripts based on a predefined narrative. As an early exploration, we study this problem in a ""retrieval-based""setting. We propose a model (ScriptWriter-CPre) to select the best response (i.e., next script line) among the candidates that fit the context (i.e., previous script lines) as well as the given narrative. Our model can keep track of what in the narrative has been said and what is to be said. Besides, it can also predict which part of the narrative should be paid more attention to when selecting the next line of script. In our study, we find the narrative plays a different role than the context. Therefore, different mechanisms are designed for deal with them. Due to the unavailability of data for this new application, we construct a new large-scale data collection GraphMovie from a movie website where end-users can upload their narratives freely when watching a movie. This new dataset is made available publicly to facilitate other studies in text generation under the guideline. Experimental results on the dataset show that our proposed approach based on narratives significantly outperforms the baselines that simply use the narrative as a kind of context.  © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Movie Script Generation; Narrative-guided Text Generation; Retrieval-based Method,Speech processing; Best response; Dialogue systems; Keep track of; Movie script generation; Narrative-guided text generation; Retrieval-based method; Script generation; Story generations; Storylines; Text generations; Motion pictures
Scalable Representation Learning for Dynamic Heterogeneous Information Networks via Metagraphs,2022,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130210857&doi=10.1145%2f3485189&partnerID=40&md5=915d81b8e42fcd5fdd63eed43e34e16b,"Content representation is a fundamental task in information retrieval. Representation learning is aimed at capturing features of an information object in a low-dimensional space. Most research on representation learning for heterogeneous information networks (HINs) focuses on static HINs. In practice, however, networks are dynamic and subject to constant change. In this article, we propose a novel and scalable representation learning model, M-DHIN, to explore the evolution of a dynamic HIN. We regard a dynamic HIN as a series of snapshots with different time stamps. We first use a static embedding method to learn the initial embeddings of a dynamic HIN at the first time stamp. We describe the features of the initial HIN via metagraphs, which retains more structural and semantic information than traditional path-oriented static models. We also adopt a complex embedding scheme to better distinguish between symmetric and asymmetric metagraphs. Unlike traditional models that process an entire network at each time stamp, we build a so-called change dataset that only includes nodes involved in a triadic closure or opening process, as well as newly added or deleted nodes. Then, we utilize the above metagraph-based mechanism to train on the change dataset. As a result of this setup, M-DHIN is scalable to large dynamic HINs since it only needs to model the entire HIN once while only the changed parts need to be processed over time. Existing dynamic embedding models only express the existing snapshots and cannot predict the future network structure. To equip M-DHIN with this ability, we introduce an LSTM-based deep autoencoder model that processes the evolution of the graph via an LSTM encoder and outputs the predicted graph. Finally, we evaluate the proposed model, M-DHIN, on real-life datasets and demonstrate that it significantly and consistently outperforms state-of-the-art models.  © 2022 Association for Computing Machinery.",Dynamic heterogeneous information network; metagraph; network representation learning,Embeddings; Long short-term memory; Semantics; Content representation; Dynamic heterogeneous information network; Embeddings; Heterogeneous information; Information networks; Information object; Meta-graph; Network representation; Network representation learning; Time-stamp; Information services
A Systematic Analysis on the Impact of Contextual Information on Point-of-Interest Recommendation,2022,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130252893&doi=10.1145%2f3508478&partnerID=40&md5=92245e8ba7d5891e8cc85d0d7955c5fd,"As the popularity of Location-based Social Networks increases, designing accurate models for Point-of-Interest (POI) recommendation receives more attention. POI recommendation is often performed by incorporating contextual information into previously designed recommendation algorithms. Some of the major contextual information that has been considered in POI recommendation are the location attributes (i.e., exact coordinates of a location, category, and check-in time), the user attributes (i.e., comments, reviews, tips, and check-in made to the locations), and other information, such as the distance of the POI from user's main activity location and the social tie between users. The right selection of such factors can significantly impact the performance of the POI recommendation. However, previous research does not consider the impact of the combination of these different factors. In this article, we propose different contextual models and analyze the fusion of different major contextual information in POI recommendation. The major contributions of this article are as follows: (i) providing an extensive survey of context-aware location recommendation; (ii) quantifying and analyzing the impact of different contextual information (e.g., social, temporal, spatial, and categorical) in the POI recommendation on available baselines and two new linear and non-linear models, which can incorporate all the major contextual information into a single recommendation model; and (iii) evaluating the considered models using two well-known real-world datasets. Our results indicate that while modeling geographical and temporal influences can improve recommendation quality, fusing all other contextual information into a recommendation model is not always the best strategy.  © 2022 Association for Computing Machinery.",context-aware recommendation; Contextual information; location-based social networks; point-of-interest recommendation,Location; Social networking (online); User profile; Accurate modeling; Activity locations; Check-in; Context-aware recommendations; Contextual information; Location-based social networks; Point-of-interest recommendation; Recommendation algorithms; Social ties; Systematic analysis; Recommender systems
Semantic Models for the First-Stage Retrieval: A Comprehensive Review,2022,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129510238&doi=10.1145%2f3486250&partnerID=40&md5=93ecaf934c5dbd808250370296a99721,"Multi-stage ranking pipelines have been a practical solution in modern search systems, where the first-stage retrieval is to return a subset of candidate documents and latter stages attempt to re-rank those candidates. Unlike re-ranking stages going through quick technique shifts over the past decades, the first-stage retrieval has long been dominated by classical term-based models. Unfortunately, these models suffer from the vocabulary mismatch problem, which may block re-ranking stages from relevant documents at the very beginning. Therefore, it has been a long-term desire to build semantic models for the first-stage retrieval that can achieve high recall efficiently. Recently, we have witnessed an explosive growth of research interests on the first-stage semantic retrieval models. We believe it is the right time to survey current status, learn from existing methods, and gain some insights for future development. In this article, we describe the current landscape of the first-stage retrieval models under a unified framework to clarify the connection between classical term-based retrieval methods, early semantic retrieval methods, and neural semantic retrieval methods. Moreover, we identify some open challenges and envision some future directions, with the hope of inspiring more research on these important yet less investigated topics.  © 2022 Association for Computing Machinery.",information retrieval; Semantic retrieval models; survey,Information retrieval; Semantic Web; Semantics; Mismatch problems; Multi-stages; Practical solutions; Re-ranking; Retrieval methods; Retrieval models; Search system; Semantic modelling; Semantic retrieval; Semantic retrieval model; Surveys
Feature-Level Attentive ICF for Recommendation,2022,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130217343&doi=10.1145%2f3490477&partnerID=40&md5=45b8e4e103a16327211fcce11d36fa6c,"Item-based collaborative filtering (ICF) enjoys the advantages of high recommendation accuracy and ease in online penalization and thus is favored by the industrial recommender systems. ICF recommends items to a target user based on their similarities to the previously interacted items of the user. Great progresses have been achieved for ICF in recent years by applying advanced machine learning techniques (e.g., deep neural networks) to learn the item similarity from data. The early methods simply treat all the historical items equally and recently proposed methods attempt to distinguish the different importance of historical items when recommending a target item. Despite the progress, we argue that those ICF models neglect the diverse intents of users on adopting items (e.g., watching a movie because of the director, leading actors, or the visual effects). As a result, they fail to estimate the item similarity on a finer-grained level to predict the user's preference to an item, resulting in sub-optimal recommendation. In this work, we propose a general feature-level attention method for ICF models. The key of our method is to distinguish the importance of different factors when computing the item similarity for a prediction. To demonstrate the effectiveness of our method, we design a light attention neural network to integrate both item-level and feature-level attention for neural ICF models. It is model-agnostic and easy-to-implement. We apply it to two baseline ICF models and evaluate its effectiveness on six public datasets. Extensive experiments show the feature-level attention enhanced models consistently outperform their counterparts, demonstrating the potential of differentiating user intents on the feature-level for ICF recommendation models.  © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Attention; collaborative filtering; diverse preference; item-based recommendation,Deep neural networks; Knowledge management; Online systems; Recommender systems; Attention; Diverse preference; Feature level; Filtering models; Item similarity; Item-based; Item-based collaborative filtering; Item-based recommendation; Penalisation; Recommendation accuracy; Collaborative filtering
Grounded Task Prioritization with Context-Aware Sequential Ranking,2022,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130221726&doi=10.1145%2f3486861&partnerID=40&md5=9c42865460a0c79b5c679c464532d9a3,"People rely on task management applications and digital assistants to capture and track their tasks, and help with executing them. The burden of organizing and scheduling time for tasks continues to reside with users of these systems, despite the high cognitive load associated with these activities. Users stand to benefit greatly from a task management system capable of prioritizing their pending tasks, thus saving them time and effort. In this article, we make three main contributions. First, we propose the problem of task prioritization, formulating it as a ranking over a user's pending tasks given a history of previous interactions with a task management system. Second, we perform an extensive analysis on the large-scale anonymized, de-identified logs of a popular task management application, deriving a dataset of grounded, real-world tasks from which to learn and evaluate our proposed system. We also identify patterns in how people record tasks as complete, which vary consistently with the nature of the task. Third, we propose a novel contextual deep learning solution capable of performing personalized task prioritization. In a battery of tests, we show that this approach outperforms several operational baselines and other sequential ranking models from previous work. Our findings have implications for understanding the ways people prioritize and manage tasks with digital tools, and in the design of support for users of task management applications.  © 2021 Association for Computing Machinery.",Contextual recommendation; Sequential ranking; Task prioritization; Tasks,Cognitive systems; Deep learning; Large dataset; Context-Aware; Contextual recommendations; Digital assistants; Management applications; Prioritization; Sequential ranking; Task; Task management; Task management system; Task prioritization; Digital devices
A Re-classification of Information Seeking Tasks and Their Computational Solutions,2022,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130225377&doi=10.1145%2f3497875&partnerID=40&md5=e6948f5159013b37ee10a45270307cf9,"This article presents a re-classification of information seeking (IS) tasks, concepts, and algorithms. The proposed taxonomy provides new dimensions to look into information seeking tasks and methods. The new dimensions include number of search iterations, search goal types, and procedures to reach these goals. Differences along these dimensions for the information seeking tasks call for suitable computational solutions. The article then reviews machine learning solutions that match each new category. The article ends with a review of evaluation campaigns for IS systems.  © 2022 Association for Computing Machinery.",dynamic search; Information seeking; interactive search; reinforcement learning,Classification (of information); Information systems; Reinforcement learning; Computational solutions; Dynamic search; Information seeking; Interactive search; New dimensions; Information use
Jointly Predicting Future Content in Multiple Social Media Sites Based on Multi-task Learning,2022,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130282064&doi=10.1145%2f3495530&partnerID=40&md5=5e3600ab8a20a51a5f8fdd8084a79797,"User-generated contents (UGC) in social media are the direct expression of users' interests, preferences, and opinions. User behavior prediction based on UGC has increasingly been investigated in recent years. Compared to learning a person's behavioral patterns in each social media site separately, jointly predicting user behavior in multiple social media sites and complementing each other (cross-site user behavior prediction) can be more accurate. However, cross-site user behavior prediction based on UGC is a challenging task due to the difficulty of cross-site data sampling, the complexity of UGC modeling, and uncertainty of knowledge sharing among different sites. For these problems, we propose a Cross-Site Multi-Task (CSMT) learning method to jointly predict user behavior in multiple social media sites. CSMT mainly derives from the hierarchical attention network and multi-task learning. Using this method, the UGC in each social media site can obtain fine-grained representations in terms of words, topics, posts, hashtags, and time slices as well as the relevances among them, and prediction tasks in different social media sites can be jointly implemented and complement each other. By utilizing two cross-site datasets sampled from Weibo, Douban, Facebook, and Twitter, we validate our method's superiority on several classification metrics compared with existing related methods.  © 2022 Association for Computing Machinery.",behavioral analytics; hierarchical attention network; multi-task; Social media; user-generated contents,Behavioral research; Classification (of information); Forecasting; Learning systems; Uncertainty analysis; User profile; Behavior prediction; Behavioral analytic; Hierarchical attention network; Multi tasks; Multitask learning; Prediction-based; Social media; User behaviors; User-generated; User-generated content; Social networking (online)
Personalized and Explainable Employee Training Course Recommendations: A Bayesian Variational Approach,2022,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129211319&doi=10.1145%2f3490476&partnerID=40&md5=d9b68b5a6b03145fdb3ef78b48f8ddd0,"As a major component of strategic talent management, learning and development (L&D) aims at improving the individual and organization performances through planning tailored training for employees to increase and improve their skills and knowledge. While many companies have developed the learning management systems (LMSs) for facilitating the online training of employees, a long-standing important issue is how to achieve personalized training recommendations with the consideration of their needs for future career development. To this end, in this article, we present a focused study on the explainable personalized online course recommender system for enhancing employee training and development. Specifically, we first propose a novel end-to-end hierarchical framework, namely Demand-aware Collaborative Bayesian Variational Network (DCBVN), to jointly model both the employees' current competencies and their career development preferences in an explainable way. In DCBVN, we first extract the latent interpretable representations of the employees' competencies from their skill profiles with autoencoding variational inference based topic modeling. Then, we develop an effective demand recognition mechanism for learning the personal demands of career development for employees. In particular, all the above processes are integrated into a unified Bayesian inference view for obtaining both accurate and explainable recommendations. Furthermore, for handling the employees with sparse or missing skill profiles, we develop an improved version of DCBVN, called the Demand-aware Collaborative Competency Attentive Network (DCCAN) framework, by considering the connectivity among employees. In DCCAN, we first build two employee competency graphs from learning and working aspects. Then, we design a graph-attentive network and a multi-head integration mechanism to infer one's competency information from her neighborhood employees. Finally, we can generate explainable recommendation results based on the competency representations. Extensive experimental results on real-world data clearly demonstrate the effectiveness and the interpretability of both of our frameworks, as well as their robustness on sparse and cold-start scenarios.  © 2021 Association for Computing Machinery.",Employee training course recommendation; intelligent education; recommender system,Bayesian networks; Curricula; E-learning; Employment; Human resource management; Inference engines; Learning systems; Online systems; Personnel training; Bayesian; Career development; Employee training; Employee training course recommendation; Individual performance; Intelligent educations; Talent development; Talent management; Training course; Variational approaches; Recommender systems
STARec: Adaptive Learning with Spatiotemporal and Activity Influence for POI Recommendation,2022,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130246681&doi=10.1145%2f3485631&partnerID=40&md5=d763c50056f82a4393e806edd692942e,"POI recommendation has become an essential means to help people discover attractive places. Intuitively, activities have an important impact on users' decision-making, because users select POIs to attend corresponding activities. However, many existing studies ignore the social motivation of user behaviors and regard all check-ins as influenced only by individual user interests. As a result, they cannot model user preferences accurately, which degrades recommendation effectiveness. In this article, from the perspective of activities, this study proposes a probabilistic generative model called STARec. Specifically, based on the social effect of activities, STARec defines users' social preferences as distinct from their individual interests and combines these with individual user activity interests to effectively depict user preferences. Moreover, the inconsistency between users' social preferences and their decisions is modeled. An activity frequency feature is introduced to acquire accurate user social preferences because of close correlation between these and the key impact factor of corresponding check-ins. An alias sampling-based training method was used to accelerate training. Extensive experiments were conducted on two real-world datasets. Experimental results demonstrated that the proposed STARec model achieves superior performance in terms of high recommendation accuracy, robustness to data sparsity, effectiveness in handling cold-start problems, efficiency, and interpretability.  © 2021 Association for Computing Machinery.",frequency feature of activities; Gaussian LDA; inconsistency; LightLDA; Personalized POI recommendation; probabilistic generative model; social effect of activities; social motivation of user behaviors; spatiotemporal activity influence,Behavioral research; Decision making; Motivation; Frequency feature of activity; Frequency features; Gaussian LDA; Gaussians; Generative model; Inconsistency; Lightlda; Personalized POI recommendation; Probabilistic generative model; Probabilistics; Social effect of activity; Social motivation of user behavior; Social motivations; Spatiotemporal activity; Spatiotemporal activity influence; User behaviors; Economic and social effects
Personalizing Medication Recommendation with a Graph-Based Approach,2022,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127642878&doi=10.1145%2f3488668&partnerID=40&md5=d18f3c8078014e29193ea29b558a1bde,"The broad adoption of electronic health records (EHRs) has led to vast amounts of data being accumulated on a patient's history, diagnosis, prescriptions, and lab tests. Advances in recommender technologies have the potential to utilize this information to help doctors personalize the prescribed medications. However, existing medication recommendation systems have yet to make use of all these information sources in a seamless manner, and they do not provide a justification on why a particular medication is recommended. In this work, we design a two-stage personalized medication recommender system called PREMIER that incorporates information from the EHR. We utilize the various weights in the system to compute the contributions from the information sources for the recommended medications. Our system models the drug interaction from an external drug database and the drug co-occurrence from the EHR as graphs. Experiment results on MIMIC-III and a proprietary outpatient dataset show that PREMIER outperforms state-of-The-Art medication recommendation systems while achieving the best tradeoff between accuracy and drug-drug interaction. Case studies demonstrate that the justifications provided by PREMIER are appropriate and aligned to clinical practices.  © 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM.",attention-based recommender system; Electronic health records,Drug interactions; Graphic methods; Records management; AS graph; Attention-based recommende system; Co-occurrence; Drug-drug interactions; Graph-based; Information sources; Patient history; Personalized medications; State of the art; System models; Recommender systems
Dynamic Structural Role Node Embedding for User Modeling in Evolving Networks,2022,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127674063&doi=10.1145%2f3472955&partnerID=40&md5=f63f4b0d649d438d8910bbbce5343240,"Complex user behavior, especially in settings such as social media, can be organized as time-evolving networks. Through network embedding, we can extract general-purpose vector representations of these dynamic networks which allow us to analyze them without extensive feature engineering. Prior work has shown how to generate network embeddings while preserving the structural role proximity of nodes. These methods, however, cannot capture the temporal evolution of the structural identity of the nodes in dynamic networks. Other works, on the other hand, have focused on learning microscopic dynamic embeddings. Though these methods can learn node representations over dynamic networks, these representations capture the local context of nodes and do not learn the structural roles of nodes. In this article, we propose a novel method for learning structural node embeddings in discrete-Time dynamic networks. Our method, called HR2vec, tracks historical topology information in dynamic networks to learn dynamic structural role embeddings. Through experiments on synthetic and real-world temporal datasets, we show that our method outperforms other well-known methods in tasks where structural equivalence and historical information both play important roles. HR2vec can be used to model dynamic user behavior in any networked setting where users can be represented as nodes. Additionally, we propose a novel method (called network fingerprinting) that uses HR2vec embeddings for modeling whole (or partial) time-evolving networks. We showcase our network fingerprinting method on synthetic and real-world networks. Specifically, we demonstrate how our method can be used for detecting foreign-backed information operations on Twitter.  © 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM.",dynamic network; Network embedding; node embedding; structural role; temporal network; time-evolving network; user profiling,Behavioral research; Social networking (online); User profile; Dynamic network; Embeddings; Evolving networks; Learn+; Network embedding; Node embedding; Structural role; Temporal networks; Time-evolving network; User's profiling; Network embeddings
I Know What You Need: Investigating Document Retrieval Effectiveness with Partial Session Contexts,2022,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127706899&doi=10.1145%2f3488667&partnerID=40&md5=44f45697a908c02db324bd669a74ec2b,"Reducing user effort in finding relevant information is one of the key objectives of search systems. Existing approaches have been shown to effectively exploit the context from the current search session of users for automatically suggesting queries to reduce their search efforts. However, these approaches do not accomplish the end goal of a search system-that of retrieving a set of potentially relevant documents for the evolving information need during a search session. This article takes the problem of query prediction one step further by investigating the problem of contextual recommendation within a search session. More specifically, given the partial context information of a session in the form of a small number of queries, we investigate how a search system can effectively predict the documents that a user would have been presented with had he continued the search session by submitting subsequent queries. To address the problem, we propose a model of contextual recommendation that seeks to capture the underlying semantics of information need transitions of a current user's search context. This model leverages information from a number of past interactions of other users with similar interactions from an existing search log. To identify similar interactions, as a novel contribution, we propose an embedding approach that jointly learns representations of both individual query terms and also those of queries (in their entirety) from a search log data by leveraging session-level containment relationships. Our experiments conducted on a large query log, namely the AOL, demonstrate that using a joint embedding of queries and their terms within our proposed framework of document retrieval outperforms a number of text-only and sequence modeling based baselines.  © 2021 Association for Computing Machinery.",Proactive IR; query log processing; representation learning; task semantics modeling,Embeddings; Information retrieval; Safety devices; Search engines; User profile; 'current; Document Retrieval; Proactive IR; Query log processing; Query logs; Representation learning; Search sessions; Search system; Semantic modelling; Task semantic modeling; Semantics
Are Topics Interesting or Not? An LDA-based Topic-graph Probabilistic Model for Web Search Personalization,2022,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127644380&doi=10.1145%2f3476106&partnerID=40&md5=1fc2dba4679f85cc87ef559f84b11c20,"In this article, we propose a Latent Dirichlet Allocation-(LDA) based topic-graph probabilistic personalization model for Web search. This model represents a user graph in a latent topic graph and simultaneously estimates the probabilities that the user is interested in the topics, as well as the probabilities that the user is not interested in the topics. For a given query issued by the user, the webpages that have higher relevancy to the interested topics are promoted, and the webpages more relevant to the non-interesting topics are penalized. In particular, we simulate a user's search intent by building two profiles: A positive user profile for the probabilities of the user is interested in the topics and a corresponding negative user profile for the probabilities of being not interested in the the topics. The profiles are estimated based on the user's search logs. A clicked webpage is assumed to include interesting topics. A skipped (viewed but not clicked) webpage is assumed to cover some non-interesting topics to the user. Such estimations are performed in the latent topic space generated by LDA. Moreover, a new approach is proposed to estimate the correlation between a given query and the user's search history so as to determine how much personalization should be considered for the query. We compare our proposed models with several strong baselines including state-of-The-Art personalization approaches. Experiments conducted on a large-scale real user search log collection illustrate the effectiveness of the proposed models.  © 2021 Association for Computing Machinery.",Latent Dirichlet Allocation (LDA); Personalization; probabilistic model; topic-graph; Web search,Information retrieval; Probability; Statistics; User profile; Latent Dirichlet allocation; Personalizations; Probabilistic models; Probabilistics; Search logs; Topic graph; User's profiles; Web searches; Web-page; Websites
Embedding Hierarchical Structures for Venue Category Representation,2022,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127642695&doi=10.1145%2f3478285&partnerID=40&md5=efee390f7c4c77f1c009cbbf0838b332,"Venue categories used in location-based social networks often exhibit a hierarchical structure, together with the category sequences derived from users' check-ins. The two data modalities provide a wealth of information for us to capture the semantic relationships between those categories. To understand the venue semantics, existing methods usually embed venue categories into low-dimensional spaces by modeling the linear context (i.e., the positional neighbors of the given category) in check-in sequences. However, the hierarchical structure of venue categories, which inherently encodes the relationships between categories, is largely untapped. In this article, we propose a venue Category Embedding Model named Hier-CEM, which generates a latent representation for each venue category by embedding the Hierarchical structure of categories and utilizing multiple types of context. Specifically, we investigate two kinds of hierarchical context based on any given venue category hierarchy and show how to model them together with the linear context collaboratively. We apply Hier-CEM to three tasks on two real check-in datasets collected from Foursquare. Experimental results show that Hier-CEM is better at capturing both semantic and sequential information inherent in venues than state-of-The-Art embedding methods.  © 2021 Association for Computing Machinery.",check-in data; hierarchical category structure; multiple context types; Venue category representation,Embeddings; Semantics; User profile; Check-in; Check-in data; Embeddings; Hierarchical category structure; Hierarchical structures; Location-based social networks; Multiple context type; Multiple contexts; Venue category representation; Wealth of information; Computational electromagnetics
Modeling Global and Local Interactions for Online Conversation Recommendation,2022,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126153342&doi=10.1145%2f3473970&partnerID=40&md5=8761ff7bb3941b15c33dcdc891589474,"The popularity of social media platforms results in a huge volume of online conversations produced every day. To help users better engage in online conversations, this article presents a novel framework to automatically recommend conversations to users based on what they said and how they behaved in their chatting histories. While prior work mostly focuses on post-level recommendation, we aim to explore conversation context and model the interaction patterns therein. Furthermore, to characterize personal interests from interleaving user interactions, we learn (1) global interactions, represented by topic and discourse word clusters to reflect users' content and pragmatic preferences, and (2) local interactions, encoding replying relations and chronological order of conversation turns to characterize users' prior behavior. Built on collaborative filtering, our model captures global interactions via discovering word distributions to represent users' topical interests and discourse behaviors, while local interactions are explored with graph-structured networks exploiting both reply structure and temporal features. Extensive experiments on three datasets from Twitter and Reddit show that our model coupling global and local interactions significantly outperforms the state-of-The-Art model. Further analyses show that our model is able to capture meaningful features from global and local interactions, which results in its superior performance in conversation recommendation.  © 2021 Copyright held by the owner/author(s).",Conversation recommendation; graph neural networks; probabilistic graphical models,Collaborative filtering; Graph neural networks; Graphic methods; User profile; Conversation recommendation; Global interaction; Graph neural networks; Interaction pattern; Interleavings; Learn+; Local interactions; Probabilistic graphical models; Social media platforms; User interaction; Social networking (online)
Graph Neural Collaborative Topic Model for Citation Recommendation,2022,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127643788&doi=10.1145%2f3473973&partnerID=40&md5=8d2255f16c892c3b7dd6da0a5a8061d9,"Due to the overload of published scientific articles, citation recommendation has long been a critical research problem for automatically recommending the most relevant citations of given articles. Relational topic models (RTMs) have shown promise on citation prediction via joint modeling of document contents and citations. However, existing RTMs can only capture pairwise or direct (first-order) citation relationships among documents. The indirect (high-order) citation links have been explored in graph neural network-based methods, but these methods suffer from the well-known explainability problem. In this article, we propose a model called Graph Neural Collaborative Topic Model that takes advantage of both relational topic models and graph neural networks to capture high-order citation relationships and to have higher explainability due to the latent topic semantic structure. Experiments on three real-world citation datasets show that our model outperforms several competitive baseline methods on citation recommendation. In addition, we show that our approach can learn better topics than the existing approaches. The recommendation results can be well explained by the underlying topics.  © 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM.",collaborative filtering; Explainable citation recommendation; graph neural networks; relational topic models,Data mining; Graph neural networks; Semantics; Critical researches; Explainable citation recommendation; Graph neural networks; High-order; Higher-order; Joint models; Relational topic model; Research problems; Scientific articles; Topic Modeling; Collaborative filtering
"Introduction to the Special Section on Graph Technologies for User Modeling and Recommendation, Part 2",2022,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127668838&doi=10.1145%2f3490180&partnerID=40&md5=60d9ce31d6d62d031e1aedfbc8e4a37a,"As a powerful data structure that represents the relationships among data objects, graph-structure data is ubiquitous in real-world applications. Learning on graph-structure data has become a hot spot in machine learning and data mining. Since most data in user-oriented services can be naturally organized as graphs, graph technologies have attracted increasing attention from IR community and achieved immense success, especially in two major research topics user modeling and recommendation. © 2022 Association for Computing Machinery. All rights reserved.",bias; fairness; Graph; knowledge graph; meta-learning; sequential recommendation; social recommendation,Data mining; Graphic methods; Bias; Data objects; Fairness; Knowledge graphs; Metalearning; Sequential recommendation; Social recommendation; Special sections; User Modelling; User recommendations; Knowledge graph
LkeRec: Toward Lightweight End-To-End Joint Representation Learning for Building Accurate and Effective Recommendation,2022,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127709855&doi=10.1145%2f3486673&partnerID=40&md5=cef819ab08e07da94e07de99a35692bd,"Explicit and implicit knowledge about users and items have been used to describe complex and heterogeneous side information for recommender systems (RSs). Many existing methods use knowledge graph embedding (KGE) to learn the representation of a user-item knowledge graph (KG) in low-dimensional space. In this article, we propose a lightweight end-To-end joint learning framework for fusing the tasks of KGE and RSs at the model level. Our method proposes a lightweight KG embedding method by using bidirectional bijection relation-Type modeling to enable scalability for large graphs while using self-Adaptive negative sampling to optimize negative sample generating. Our method further generates the integrated views for users and items based on relation-Types to explicitly model users' preferences and items' features, respectively. Finally, we add virtual ""recommendation""relations between the integrated views of users and items to model the preferences of users on items, seamlessly integrating RS with user-item KG over a unified graph. Experimental results on multiple datasets and benchmarks show that our method can achieve a better accuracy of recommendation compared with existing state-of-The-Art methods. Complexity and runtime analysis suggests that our method can gain a lower time and space complexity than most of existing methods and improve scalability.  © 2021 Association for Computing Machinery.",End-To-end joint learning; knowledge graph embedding; recommendation; scalability; self-Attention,Graph embeddings; Knowledge graph; User profile; End to end; End-to-end joint learning; Explicit knowledge; Graph embeddings; Implicit knowledge; Joint learning; Knowledge graph embedding; Knowledge graphs; Recommendation; Self-attention; Scalability
A Comparison between Term-Independence Retrieval Models for Ad Hoc Retrieval,2022,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127622824&doi=10.1145%2f3483612&partnerID=40&md5=e00458b4045791c1ad9e8fdc73430b82,"In Information Retrieval, numerous retrieval models or document ranking functions have been developed in the quest for better retrieval effectiveness. Apart from some formal retrieval models formulated on a theoretical basis, various recent works have applied heuristic constraints to guide the derivation of document ranking functions. While many recent methods are shown to improve over established and successful models, comparison among these new methods under a common environment is often missing. To address this issue, we perform an extensive and up-To-date comparison of leading term-independence retrieval models implemented in our own retrieval system. Our study focuses on the following questions: (RQ1) Is there a retrieval model that consistently outperforms all other models across multiple collections; (RQ2) What are the important features of an effective document ranking function? Our retrieval experiments performed on several TREC test collections of a wide range of sizes (up to the terabyte-sized Clueweb09 Category B) enable us to answer these research questions. This work also serves as a reproducibility study for leading retrieval models. While our experiments show that no single retrieval model outperforms all others across all tested collections, some recent retrieval models, such as MATF and MVD, consistently perform better than the common baselines.  © 2021 Association for Computing Machinery.",comparison; evaluation; Information retrieval; multiple hypotheses testing; retrieval model,Search engines; Ad Hoc retrieval; Common environment; Comparison; Document ranking; Evaluation; Models comparisons; Multiple hypotheses testing; Ranking functions; Retrieval effectiveness; Retrieval models; Information retrieval
BotSpot++: A Hierarchical Deep Ensemble Model for Bots Install Fraud Detection in Mobile Advertising,2022,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127694567&doi=10.1145%2f3476107&partnerID=40&md5=9b4424f6d64edc366f98258629044751,"Mobile advertising has undoubtedly become one of the fastest-growing industries in the world. The influx of capital attracts increasing fraudsters to defraud money from advertisers. Fraudsters can leverage many techniques, where bots install fraud is the most difficult to detect due to its ability to emulate normal users by implementing sophisticated behavioral patterns to evade from detection rules defined by human experts. Therefore, we proposed BotSpot1 for bots install fraud detection previously. However, there are some drawbacks in BotSpot, such as the sparsity of the devices' neighbors, weak interactive information of leaf nodes, and noisy labels. In this work, we propose BotSpot++ to improve these drawbacks: (1) for the sparsity of the devices' neighbors, we propose to construct a super device node to enrich the graph structure and information flow utilizing domain knowledge and a clustering algorithm; (2) for the weak interactive information, we propose to incorporate a self-Attention mechanism to enhance the interaction of various leaf nodes; and (3) for the noisy labels, we apply a label smoothing mechanism to alleviate it. Comprehensive experimental results show that BotSpot++ yields the best performance compared with six state-of-The-Art baselines. Furthermore, we deploy our model to the advertising platform of Mobvista,2 a leading global mobile advertising company. The online experiments also demonstrate the effectiveness of our proposed method.  © 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM.",app install fraud; bots fraud detection; graph neural network; Mobile ad fraud,Botnet; Clustering algorithms; Data mining; Domain Knowledge; Flow graphs; Graph neural networks; Marketing; App install fraud; Bot fraud detection; Ensemble models; Fraud detection; Fraudsters; Graph neural networks; Interactive informations; Mobile ad fraud; Mobile advertizing; Noisy labels; Crime
EFraudCom: An E-commerce Fraud Detection System via Competitive Graph Neural Networks,2022,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125815546&doi=10.1145%2f3474379&partnerID=40&md5=a360e7673af64e2b903850159f9f36d5,"With the development of e-commerce, fraud behaviors have been becoming one of the biggest threats to the e-commerce business. Fraud behaviors seriously damage the ranking system of e-commerce platforms and adversely influence the shopping experience of users. It is of great practical value to detect fraud behaviors on e-commerce platforms. However, the task is non-Trivial, since the adversarial action taken by fraudsters. Existing fraud detection systems used in the e-commerce industry easily suffer from performance decay and can not adapt to the upgrade of fraud patterns, as they take already known fraud behaviors as supervision information to detect other suspicious behaviors.In this article, we propose a competitive graph neural networks (CGNN)-based fraud detection system (eFraudCom) to detect fraud behaviors at one of the largest e-commerce platforms, ""Taobao""1. In the eFraudCom system, (1) the competitive graph neural networks (CGNN) as the core part of eFraudCom can classify behaviors of users directly by modeling the distributions of normal and fraud behaviors separately; (2) some normal behaviors will be utilized as weak supervision information to guide the CGNN to build the profile for normal behaviors that are more stable than fraud behaviors. The algorithm dependency on fraud behaviors will be eliminated, which enables eFraudCom to detect fraud behaviors in presence of the new fraud patterns; (3) the mutual information regularization term can maximize the separability between normal and fraud behaviors to further improve CGNN. eFraudCom is implemented into a prototype system and the performance of the system is evaluated by extensive experiments. The experiments on two Taobao and two public datasets demonstrate that the proposed deep framework CGNN is superior to other baselines in detecting fraud behaviors. A case study on Taobao datasets verifies that CGNN is still robust when the fraud patterns have been upgraded.  © 2022 Association for Computing Machinery.",fraud detection system; graph neural networks; Online e-commerce platforms,Classification (of information); Electronic commerce; Graph neural networks; User profile; Commerce platforms; E- commerces; E-commerce business; Fraud detection system; Fraudsters; Graph neural networks; Non-trivial; Normal behavior; Online e-commerce platform; Ranking system; Crime
Dual Gated Graph Attention Networks with Dynamic Iterative Training for Cross-Lingual Entity Alignment,2022,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127727323&doi=10.1145%2f3471165&partnerID=40&md5=228b88208e6fe2ddfba38d4c27e91148,"Cross-lingual entity alignment has attracted considerable attention in recent years. Past studies using conventional approaches to match entities share the common problem of missing important structural information beyond entities in the modeling process. This allows graph neural network models to step in. Most existing graph neural network approaches model individual knowledge graphs (KGs) separately with a small amount of pre-Aligned entities served as anchors to connect different KG embedding spaces. However, this characteristic can cause several major problems, including performance restraint due to the insufficiency of available seed alignments and ignorance of pre-Aligned links that are useful in contextual information in-between nodes. In this article, we propose DuGa-DIT, a dual gated graph attention network with dynamic iterative training, to address these problems in a unified model. The DuGa-DIT model captures neighborhood and cross-KG alignment features by using intra-KG attention and cross-KG attention layers. With the dynamic iterative process, we can dynamically update the cross-KG attention score matrices, which enables our model to capture more cross-KG information. We conduct extensive experiments on two benchmark datasets and a case study in cross-lingual personalized search. Our experimental results demonstrate that DuGa-DIT outperforms state-of-The-Art methods.  © 2021 Association for Computing Machinery.",cross graph attention; entity alignment; iterative; Knowledge graph,Alignment; Graph neural networks; Iterative methods; Conventional approach; Cross graph attention; Cross-lingual; Entity alignment; Graph neural networks; Iterative; Knowledge graphs; Modeling process; Pre-aligned; Structural information; Knowledge graph
Multimodal Web Page Segmentation Using Self-organized Multi-objective Clustering,2022,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127666695&doi=10.1145%2f3480966&partnerID=40&md5=f37b738195966c29833fd384eb69d2dd,"Web page segmentation (WPS) aims to break a web page into different segments with coherent intra-and inter-semantics. By evidencing the morpho-dispositional semantics of a web page, WPS has traditionally been used to demarcate informative from non-informative content, but it has also evidenced its key role within the context of non-linear access to web information for visually impaired people. For that purpose, a great deal of ad hoc solutions have been proposed that rely on visual, logical, and/or text cues. However, such methodologies highly depend on manually tuned heuristics and are parameter-dependent. To overcome these drawbacks, principled frameworks have been proposed that provide the theoretical bases to achieve optimal solutions. However, existing methodologies only combine few discriminant features and do not define strategies to automatically select the optimal number of segments. In this article, we present a multi-objective clustering technique called MCS that relies on-means, in which (1) visual, logical, and text cues are all combined in a early fusion manner and (2) an evolutionary process automatically discovers the optimal number of clusters (segments) as well as the correct positioning of seeds. As such, our proposal is parameter-free, combines many different modalities, does not depend on manually tuned heuristics, and can be run on any web page without any constraint. An exhaustive evaluation over two different tasks, where (1) the number of segments must be discovered or (2) the number of clusters is fixed with respect to the task at hand, shows that MCS drastically improves over most competitive and up-To-date algorithms for a wide variety of external and internal validation indices. In particular, results clearly evidence the impact of the visual and logical modalities towards segmentation performance.  © 2022 Association for Computing Machinery.",evolutionary computation; multi-objective optimization; multimodal early fusion; self-organizing maps; Web page segmentation,Calculations; Conformal mapping; Genetic algorithms; Heuristic methods; Self organizing maps; Semantics; Websites; Early fusion; Evolutionary computation; Multi-modal; Multi-objective clustering; Multi-objectives optimization; Multimodal early fusion; Number of clusters; Optimal number; Web page segmentation; Web-page; Multiobjective optimization
An Unsupervised Aspect-Aware Recommendation Model with Explanation Text Generation,2022,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125029488&doi=10.1145%2f3483611&partnerID=40&md5=e1d2abb2a46a2f828484febaccb64b13,"Review based recommendation utilizes both users' rating records and the associated reviews for recommendation. Recently, with the rapid demand for explanations of recommendation results, reviews are used to train the encoder-decoder models for explanation text generation. As most of the reviews are general text without detailed evaluation, some researchers leveraged auxiliary information of users or items to enrich the generated explanation text. Nevertheless, the auxiliary data is not available in most scenarios and may suffer from data privacy problems. In this article, we argue that the reviews contain abundant semantic information to express the users' feelings for various aspects of items, while these information are not fully explored in current explanation text generation task. To this end, we study how to generate more fine-grained explanation text in review based recommendation without any auxiliary data. Though the idea is simple, it is non-Trivial since the aspect is hidden and unlabeled. Besides, it is also very challenging to inject aspect information for generating explanation text with noisy review input. To solve these challenges, we first leverage an advanced unsupervised neural aspect extraction model to learn the aspect-Aware representation of each review sentence. Thus, users and items can be represented in the aspect space based on their historical associated reviews. After that, we detail how to better predict ratings and generate explanation text with the user and item representations in the aspect space. We further dynamically assign review sentences which contain larger proportion of aspect words with larger weights to control the text generation process, and jointly optimize rating prediction accuracy and explanation text generation quality with a multi-Task learning framework. Finally, extensive experimental results on three real-world datasets demonstrate the superiority of our proposed model for both recommendation accuracy and explainability.  © 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM.",aspect-Aware recommendation; Explainable recommendation; multi-Task learning; review-based recommendation; unsupervised aspect extraction,Data privacy; Learning systems; Quality control; Semantics; Am generals; Aspect-aware recommendation; Auxiliary data; Encoder-decoder; Explainable recommendation; Multitask learning; Review-based recommendation; Text generations; Unsupervised aspect extraction; User rating; Extraction
Truncated Models for Probabilistic Weighted Retrieval,2022,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127717628&doi=10.1145%2f3476837&partnerID=40&md5=ff3c19a2d963baa42bf803155d49ca7f,"Existing probabilistic retrieval models do not restrict the domain of the random variables that they deal with. In this article, we show that the upper bound of the normalized term frequency (tf) from the relevant documents is much smaller than the upper bound of the normalized tf from the whole collection. As a result, the existing models suffer from two major problems: (i) the domain mismatch causes data modeling error, (ii) since the outliers have very large magnitude and the retrieval models follow tf hypothesis, the combination of these two factors tends to overestimate the relevance score. In an attempt to address these problems, we propose novel weighted probabilistic models based on truncated distributions. We evaluate our models on a set of large document collections. Significant performance improvement over six existing probabilistic models is demonstrated.  © 2021 Association for Computing Machinery.",Probabilistic model; ranking; truncated distributions; web search,Probability distributions; Probabilistic models; Probabilistic retrieval; Probabilistics; Ranking; Relevant documents; Retrieval models; Term Frequency; Truncated distributions; Upper Bound; Web searches; Information retrieval
GraphHINGE: Learning Interaction Models of Structured Neighborhood on Heterogeneous Information Network,2022,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127696083&doi=10.1145%2f3472956&partnerID=40&md5=75fa111d94f953d0ae0e6f3ed3abff19,"Heterogeneous information network (HIN) has been widely used to characterize entities of various types and their complex relations. Recent attempts either rely on explicit path reachability to leverage path-based semantic relatedness or graph neighborhood to learn heterogeneous network representations before predictions. These weakly coupled manners overlook the rich interactions among neighbor nodes, which introduces an early summarization issue. In this article, we propose GraphHINGE (Heterogeneous INteract and aggreGatE), which captures and aggregates the interactive patterns between each pair of nodes through their structured neighborhoods. Specifically, we first introduce Neighborhood-based Interaction (NI) module to model the interactive patterns under the same metapaths, and then extend it to Cross Neighborhood-based Interaction (CNI) module to deal with different metapaths. Next, in order to address the complexity issue on large-scale networks, we formulate the interaction modules via a convolutional framework and learn the parameters efficiently with fast Fourier transform. Furthermore, we design a novel neighborhood-based selection (NS) mechanism, a sampling strategy, to filter high-order neighborhood information based on their low-order performance. The extensive experiments on six different types of heterogeneous graphs demonstrate the performance gains by comparing with state-of-The-Arts in both click-Through rate prediction and top-N recommendation tasks.  © 2022 Association for Computing Machinery.",heterogeneous information network; neighborhood-based interaction; Recommender system,Aggregates; Complex networks; Fast Fourier transforms; Graph theory; Heterogeneous networks; Semantics; World Wide Web; Heterogeneous information; Heterogeneous information network; Information networks; Interaction modeling; Interaction modules; Learn+; Learning interactions; Modeling of structured; Neighborhood-based interaction; Neighbourhood; Information services
Social Context-Aware Person Search in Videos via Multi-modal Cues,2022,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127654481&doi=10.1145%2f3480967&partnerID=40&md5=34795cd108cacc447182e31a02e56271,"Person search has long been treated as a crucial and challenging task to support deeper insight in personalized summarization and personality discovery. Traditional methods, e.g., person re-identification and face recognition techniques, which profile video characters based on visual information, are often limited by relatively fixed poses or small variation of viewpoints and suffer from more realistic scenes with high motion complexity (e.g., movies). At the same time, long videos such as movies often have logical story lines and are composed of continuously developmental plots. In this situation, different persons usually meet on a specific occasion, in which informative social cues are performed. We notice that these social cues could semantically profile their personality and benefit person search task in two aspects. First, persons with certain relationships usually co-occur in short intervals; in case one of them is easier to be identified, the social relation cues extracted from their co-occurrences could further benefit the identification for the harder ones. Second, social relations could reveal the association between certain scenes and characters (e.g., classmate relationship may only exist among students), which could narrow down candidates into certain persons with a specific relationship. In this way, high-level social relation cues could improve the effectiveness of person search. Along this line, in this article, we propose a social context-Aware framework, which fuses visual and social contexts to profile persons in more semantic perspectives and better deal with person search task in complex scenarios. Specifically, we first segment videos into several independent scene units and abstract out social contexts within these scene units. Then, we construct inner-personal links through a graph formulation operation for each scene unit, in which both visual cues and relation cues are considered. Finally, we perform a relation-Aware label propagation to identify characters' occurrences, combining low-level semantic cues (i.e., visual cues) and high-level semantic cues (i.e., relation cues) to further enhance the accuracy. Experiments on real-world datasets validate that our solution outperforms several competitive baselines.  © 2021 Association for Computing Machinery.",graph modeling; label propagation; neural network; Person search; social relation; user profile,Abstracting; Backpropagation; Complex networks; Face recognition; Graph theory; Semantics; User profile; Context-Aware; Graph model; Label propagation; Neural-networks; Person search; Search tasks; Social context; Social cues; Social relations; User's profiles; Neural networks
A Game Theory Approach for Estimating Reliability of Crowdsourced Relevance Assessments,2022,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127706905&doi=10.1145%2f3480965&partnerID=40&md5=86134b35d9dcc30f33b4e15092db09e1,"In this article, we propose an approach to improve quality in crowdsourcing (CS) tasks using Task Completion Time (TCT) as a source of information about the reliability of workers in a game-Theoretical competitive scenario. Our approach is based on the hypothesis that some workers are more risk-inclined and tend to gamble with their use of time when put to compete with other workers. This hypothesis is supported by our previous simulation study. We test our approach with 35 topics from experiments on the TREC-8 collection being assessed as relevant or non-relevant by crowdsourced workers both in a competitive (referred to as ""Game"") and non-competitive (referred to as ""Base"") scenario. We find that competition changes the distributions of TCT, making them sensitive to the quality (i.e., wrong or right) and outcome (i.e., relevant or non-relevant) of the assessments. We also test an optimal function of TCT as weights in a weighted majority voting scheme. From probabilistic considerations, we derive a theoretical upper bound for the weighted majority performance of cohorts of 2, 3, 4, and 5 workers, which we use as a criterion to evaluate the performance of our weighting scheme. We find our approach achieves a remarkable performance, significantly closing the gap between the accuracy of the obtained relevance judgements and the upper bound. Since our approach takes advantage of TCT, which is an available quantity in any CS tasks, we believe it is cost-effective and, therefore, can be applied for quality assurance in crowdsourcing for micro-Tasks.  © 2021 Association for Computing Machinery.",crowdsourcing; Game theory; relevance assessment,Cost effectiveness; Game theory; Reliability theory; Optimal function; Performance; Probabilistics; Relevance assessments; Simulation studies; Sources of informations; Task completion time; Upper Bound; Voting schemes; Workers'; Crowdsourcing
Clarifying Ambiguous Keywords with Personal Word Embeddings for Personalized Search,2022,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127629630&doi=10.1145%2f3470564&partnerID=40&md5=a8559b8559d1caea724a3581ef52d79f,"Researchers attempt to solve the personalized search problem from an alternative perspective of clarifying the user’s intention of the current query. They propose a personalized search model with personal word embeddings for each individual user that mainly contain the word meanings that the user already knows and can reflect the user interests. To learn great personal word embeddings, they design a pre-training model that captures both the textual information of the query log and the information about user interests contained in the click-through data represented as a graph structure. With personal word embeddings, we obtain the personalized word and context-aware representations of the query and documents. Furthermore, they also employ the current session as the short-term search context to dynamically disambiguate the current query. They use a matching model to calculate the matching score between the personalized query and document representations for ranking. Experimental results on two large-scale query logs show that the designed model significantly outperforms state-of-the-art personalization models.",personal word embedding; Personalized search; session graph; user interest,'current; Embeddings; Personal word embedding; Personalized search; Query logs; Search models; Search problem; Session graph; Users' interests; Word meaning; Information retrieval
Hyperspherical Variational Co-embedding for Attributed Networks,2022,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127655614&doi=10.1145%2f3478284&partnerID=40&md5=274462119713d992835cbceb11874d33,"Network-based information has been widely explored and exploited in the information retrieval literature. Attributed networks, consisting of nodes, edges as well as attributes describing properties of nodes, are a basic type of network-based data, and are especially useful for many applications. Examples include user profiling in social networks and item recommendation in user-item purchase networks. Learning useful and expressive representations of entities in attributed networks can provide more effective building blocks to down-stream network-based tasks such as link prediction and attribute inference. Practically, input features of attributed networks are normalized as unit directional vectors. However, most network embedding techniques ignore the spherical nature of inputs and focus on learning representations in a Gaussian or Euclidean space, which, we hypothesize, might lead to less effective representations. To obtain more effective representations of attributed networks, we investigate the problem of mapping an attributed network with unit normalized directional features into a non-Gaussian and non-Euclidean space. Specifically, we propose a hyperspherical variational co-embedding for attributed networks (HCAN), which is based on generalized variational auto-encoders for heterogeneous data with multiple types of entities. HCAN jointly learns latent embeddings for both nodes and attributes in a unified hyperspherical space such that the affinities between nodes and attributes can be captured effectively. We argue that this is a crucial feature in many real-world applications of attributed networks. Previous Gaussian network embedding algorithms break the assumption of uninformative prior, which leads to unstable results and poor performance. In contrast, HCAN embeds nodes and attributes as von Mises-Fisher distributions, and allows one to capture the uncertainty of the inferred representations. Experimental results on eight datasets show that HCAN yields better performance in a number of applications compared with nine state-of-The-Art baselines.  © 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM.",generalized variational auto-encoders; hyperspherical representation; Network embedding,Data mining; Gaussian distribution; Geometry; Network coding; User profile; Auto encoders; Building blockes; Embeddings; Generalized variational auto-encoder; Hyperspherical; Hyperspherical representation; Network embedding; Network-based; Property; User's profiling; Network embeddings
Component-based Analysis of Dynamic Search Performance,2022,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127702388&doi=10.1145%2f3483237&partnerID=40&md5=330cd6ca37798c0d7de78de91dfe5ab2,"In many search scenarios, such as exploratory, comparative, or survey-oriented search, users interact with dynamic search systems to satisfy multi-Aspect information needs. These systems utilize different dynamic approaches that exploit various user feedback granularity types. Although studies have provided insights about the role of many components of these systems, they used black-box and isolated experimental setups. Therefore, the effects of these components or their interactions are still not well understood. We address this by following a methodology based on Analysis of Variance (ANOVA). We built a Grid Of Points that consists of systems based on different ways to instantiate three components: initial rankers, dynamic rerankers, and user feedback granularity. Using evaluation scores based on the TREC Dynamic Domain collections, we built several ANOVA models to estimate the effects. We found that (i) although all components significantly affect search effectiveness, the initial ranker has the largest effective size, (ii) the effect sizes of these components vary based on the length of the search session and the used effectiveness metric, and (iii) initial rankers and dynamic rerankers have more prominent effects than user feedback granularity. To improve effectiveness, we recommend improving the quality of initial rankers and dynamic rerankers. This does not require eliciting detailed user feedback, which might be expensive or invasive.  © 2021 Association for Computing Machinery.",ANOVA; component-Analysis; evaluation; interactive search; Subtopic retrieval,Analysis of variance (ANOVA); Information retrieval; Analyse of variance; Component analysis; Component based analysis; Dynamic search; Evaluation; Interactive search; Search performance; Search system; Subtopic retrievals; User feedback; Search engines
SPEX: A Generic Framework for Enhancing Neural Social Recommendation,2022,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124087343&doi=10.1145%2f3473338&partnerID=40&md5=d44a3632a8151fb764f4f55495152cc1,"Social Recommender Systems (SRS) have attracted considerable attention since its accompanying service, social networks, helps increase user satisfaction and provides auxiliary information to improve recommendations. However, most existing SRS focus on social influence and ignore another essential social phenomenon, i.e., social homophily. Social homophily, which is the premise of social influence, indicates that people tend to build social relations with similar people and form influence propagation paths. In this article, we propose a generic framework Social PathExplorer (SPEX) to enhance neural SRS. SPEX treats the neural recommendation model as a black box and improves the quality of recommendations by modeling the social recommendation task, the formation of social homophily, and their mutual effect in the manner of multi-task learning. We design a Graph Neural Network based component for influence propagation path prediction to help SPEX capture the rich information conveyed by the formation of social homophily. We further propose an uncertainty based task balancing method to set appropriate task weights for the recommendation task and the path prediction task during the joint optimization. Extensive experiments have validated that SPEX can be easily plugged into various state-of-the-art neural recommendation models and help improve their performance. The source code of our work is available at: https://github.com/XMUDM/SPEX.  © 2021 Association for Computing Machinery.",multi-task learning; social homophily; social influence; Social recommender,Backpropagation; Economic and social effects; Graph neural networks; Auxiliary information; Generic frameworks; Homophily; Path prediction; Propagation paths; Social homophily; Social influence; Social recommende; Social recommender systems; Users' satisfactions; Recommender systems
Beyond Relevance Ranking: A General Graph Matching Framework for Utility-Oriented Learning to Rank,2022,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124090918&doi=10.1145%2f3464303&partnerID=40&md5=2d66a81c6371f68490c98d1d1d2ee78d,"Learning to rank from logged user feedback, such as clicks or purchases, is a central component of many real-world information systems. Different from human-annotated relevance labels, the user feedback is always noisy and biased. Many existing learning to rank methods infer the underlying relevance of query-item pairs based on different assumptions of examination, and still optimize a relevance based objective. Such methods rely heavily on the correct estimation of examination, which is often difficult to achieve in practice. In this work, we propose a general framework U-rank+ for learning to rank with logged user feedback from the perspective of graph matching. We systematically analyze the biases in user feedback, including examination bias and selection bias. Then, we take both biases into consideration for unbiased utility estimation that directly based on user feedback, instead of relevance. In order to maximize the estimated utility in an efficient manner, we design two different solvers based on Sinkhorn and LambdaLoss for U-rank+. The former is based on a standard graph matching algorithm, and the latter is inspired by the traditional method of learning to rank. Both of the algorithms have good theoretical properties to optimize the unbiased utility objective while the latter is proved to be empirically more effective and efficient in practice. Our framework U-rank+ can deal with a general utility function and can be used in a widespread of applications including web search, recommendation, and online advertising. Semi-synthetic experiments on three benchmark learning to rank datasets demonstrate the effectiveness of U-rank+. Furthermore, our proposed framework has been deployed on two different scenarios of a mainstream App store, where the online A/B testing shows that U-rank+ achieves an average improvement of 19.2% on click-through rate and 20.8% improvement on conversion rate in recommendation scenario, and 5.12% on platform revenue in online advertising scenario over the production baselines.  © 2021 Association for Computing Machinery.",examination bias; graph matching; implicit feedback; Learning to rank; position bias; selection bias; utility maximization,Data mining; Learning systems; Examination bias; General graph; Graph matchings; Implicit feedback; Online advertizing; Position bias; Relevance ranking; Selection bias; User feedback; Utility maximizations; Marketing
"Personalized, Sequential, Attentive, Metric-Aware Product Search",2022,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124094833&doi=10.1145%2f3473337&partnerID=40&md5=3dc05c92c69ceeec2a38cc63f42c281c,"The task of personalized product search aims at retrieving a ranked list of products given a user's input query and his/her purchase history. To address this task, we propose the PSAM model, a Personalized, Sequential, Attentive and Metric-aware (PSAM) model, that learns the semantic representations of three different categories of entities, i.e., users, queries, and products, based on user sequential purchase historical data and the corresponding sequential queries. Specifically, a query-based attentive LSTM (QA-LSTM) model and an attention mechanism are designed to infer users dynamic embeddings, which is able to capture their short-term and long-term preferences. To obtain more fine-grained embeddings of the three categories of entities, a metric-aware objective is deployed in our model to force the inferred embeddings subject to the triangle inequality, which is a more realistic distance measurement for product search. Experiments conducted on four benchmark datasets show that our PSAM model significantly outperforms the state-of-the-art product search baselines in terms of effectiveness by up to 50.9% improvement under NDCG@20. Our visualization experiments further illustrate that the learned product embeddings are able to distinguish different types of products.  © 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM.",LSTM; metric learning; neural networks; personalized web search; Product search,Information retrieval; Long short-term memory; Semantics; Embeddings; Learn+; LSTM; Metric learning; Neural-networks; Personalized products; Personalized web searches; Product search; Semantic representation; User input; Embeddings
The Network Visibility Problem,2022,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124101295&doi=10.1145%2f3460475&partnerID=40&md5=6af8cbc3bb7196ccd06bb896763c0369,"Social media is an attention economy where broadcasters are constantly competing for attention in their followers' feeds. Broadcasters are likely to elicit greater attention from their followers if their posts remain visible at the top of their followers' feeds for a longer period of time. However, this depends on the rate at which their followers receive information in their feeds, which in turn depends on the broadcasters they follow. Motivated by this observation and recent calls for fairness of exposure in social networks, in this article, we look at the task of recommending links from the perspective of visibility optimization. Given a set of candidate links provided by a link recommendation algorithm, our goal is to find a subset of those links that would provide the highest visibility to a set of broadcasters. To this end, we first show that this problem reduces to maximizing a nonsubmodular nondecreasing set function under matroid constraints. Then, we show that the set function satisfies a notion of approximate submodularity that allows the standard greedy algorithm to enjoy theoretical guarantees. Experiments on both synthetic and real data gathered from Twitter show that the greedy algorithm is able to consistently outperform several competitive baselines.  © 2021 Copyright held by the owner/author(s).",fairness of exposure; Link recommendation; network optimization; submodularity; visibility optimization,Set theory; Social networking (online); Fairness of exposure; Greedy algorithms; Link recommendation; Network optimization; Optimisations; Recommendation algorithms; Set function; Social media; Submodularity; Visibility optimization; Visibility
Exploiting Group Information for Personalized Recommendation with Graph Neural Networks,2022,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124099160&doi=10.1145%2f3464764&partnerID=40&md5=df2717304f5f5146f21f8a366b9a0082,"Personalized recommendation has become more and more important for users to quickly find relevant items. The key issue of the recommender system is how to model user preferences. Previous work mostly employed user historical data to learn users' preferences, but faced with the data sparsity problem. The prevalence of online social networks promotes increasing online discussion groups, and users in the same group often have similar interests and preferences. Therefore, it is necessary to integrate group information for personalized recommendation. The existing work on group-information-enhanced recommender systems mainly relies on the item information related to the group, which is not expressive enough to capture the complicated preference dependency relationships between group users and the target user. In this article, we solve the problem with the graph neural networks. Specifically, the relationship between users and items, the item preferences of groups, and the groups that users participate in are constructed as bipartite graphs, respectively, and the user preferences for items are learned end to end through the graph neural network. The experimental results on the Last.fm and Douban Movie datasets show that considering group preferences can improve the recommendation performance and demonstrate the superiority on sparse users compared  © 2021 Association for Computing Machinery.",graph neural network; group preferences; Personalized recommendation,Recommender systems; Social networking (online); Data sparsity problems; Discussion Groups; Graph neural networks; Group preference; Historical data; Key Issues; Learn+; Online discussions; Personalized recommendation; User's preferences; Graph neural networks
Direction-Aware User Recommendation Based on Asymmetric Network Embedding,2022,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124107292&doi=10.1145%2f3466754&partnerID=40&md5=889285c316074ac571ac26f85dc26533,"User recommendation aims at recommending users with potential interests in the social network. Previous works have mainly focused on the undirected social networks with symmetric relationship such as friendship, whereas recent advances have been made on the asymmetric relationship such as the following and followed by relationship. Among the few existing direction-aware user recommendation methods, the random walk strategy has been widely adopted to extract the asymmetric proximity between users. However, according to our analysis on real-world directed social networks, we argue that the asymmetric proximity captured by existing random walk based methods are insufficient due to the inbalance in-degree and out-degree of nodes.To tackle this challenge, we propose InfoWalk, a novel informative walk strategy to efficiently capture the asymmetric proximity solely based on random walks. By transferring the direction information into the weights of each step, InfoWalk is able to overcome the limitation of edges while simultaneously maintain both the direction and proximity. Based on the asymmetric proximity captured by InfoWalk, we further propose the qualitative (DNE-L) and quantitative (DNE-T) directed network embedding methods, capable of preserving the two properties in the embedding space. Extensive experiments conducted on six real-world benchmark datasets demonstrate the superiority of the proposed DNE model over several state-of-the-art approaches in various tasks.  © 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM.",graph neural networks; random walk; User recommendation,Data mining; Graph neural networks; Random processes; Asymmetric networks; Graph neural networks; In-Degree; Network embedding; Random Walk; Random walk strategies; Real-world; Recommendation methods; Symmetrics; User recommendations; Network embeddings
Profiling Users for Question Answering Communities via Flow-Based Constrained Co-Embedding Model,2022,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124098005&doi=10.1145%2f3470565&partnerID=40&md5=ecedf071dbffb2cbe73aecd57a7582fc,"In this article, we study the task of user profiling in question answering communities (QACs). Previous user profiling algorithms suffer from a number of defects: they regard users and words as atomic units, leading to the mismatch between them; they are designed for other applications but not for QACs; and some semantic profiling algorithms do not co-embed users and words, leading to making the affinity measurement between them difficult. To improve the profiling performance, we propose a neural Flow-based Constrained Co-embedding Model, abbreviated as FCCM. FCCM jointly co-embeds the vector representations of both users and words in QACs such that the affinities between them can be semantically measured. Specifically, FCCM extends the standard variational auto-encoder model to enforce the inferred embeddings of users and words subject to the voting constraint, i.e., given a question and the users who answer this question in the community, representations of the users whose answers receive more votes are closer to the representations of the words associated with these answers, compared with representations of whose receiving fewer votes. In addition, FCCM integrates normalizing flow into the variational auto-encoder framework to avoid the assumption that the distributions of the embeddings are Gaussian, making the inferred embeddings fit the real distributions of the data better. Experimental results on a Chinese Zhihu question answering dataset demonstrate the effectiveness of our proposed FCCM model for the task of user profiling in QACs.  © 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Co-embedding; question answering communities; user profiling; variational auto-encoder,Semantics; Signal encoding; User profile; Affinity measurement; Atomic units; Auto encoders; Co-embedding; Embeddings; Flow based; Performance; Question answering communities; User's profiling; Variational auto-encoder; Embeddings
Learning a Hierarchical Intent Model for Next-Item Recommendation,2022,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124068013&doi=10.1145%2f3473972&partnerID=40&md5=17b3163263116156ad7932f8168b3cb4,"A session-based recommender system (SBRS) captures users' evolving behaviors and recommends the next item by profiling users in terms of items in a session. User intent and user preference are two factors affecting his (her) decisions. Specifically, the former narrows the selection scope to some item types, while the latter helps to compare items of the same type. Most SBRSs assume one arbitrary user intent dominates a session when making a recommendation. However, this oversimplifies the reality that a session may involve multiple types of items conforming to different intents. In current SBRSs, items conforming to different user intents have cross-interference in profiling users for whom only one user intent is considered. Explicitly identifying and differentiating items conforming to various user intents can address this issue and model rich contextual information of a session. To this end, we design a framework modeling user intent and preference explicitly, which empowers the two factors to play their distinctive roles. Accordingly, we propose a key-array memory network (KA-MemNN) with a hierarchical intent tree to model coarse-to-fine user intents. The two-layer weighting unit (TLWU) in KA-MemNN detects user intents and generates intent-specific user profiles. Furthermore, the hierarchical semantic component (HSC) integrates multiple sets of intent-specific user profiles along with different user intent distributions to model a multi-intent user profile. The experimental results on real-world datasets demonstrate the superiority of KA-MemNN over selected state-of-the-art methods.  © 2021 Association for Computing Machinery.",attention mechanism; intent modeling; memory network; representation learning; User modeling,Learning systems; Semantics; 'current; Attention mechanisms; Contextual information; Cross interference; Intent models; Memory network; Representation learning; User Modelling; User's preferences; User's profiles; User profile
Learning from Substitutable and Complementary Relations for Graph-based Sequential Product Recommendation,2022,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124071771&doi=10.1145%2f3464302&partnerID=40&md5=4ac6dfb860ed58541c73c305993c5fba,"Sequential product recommendation, aiming at predicting the products that a target user will interact with soon, has become a hotspot topic. Most of the sequential recommendation models focus on learning from users' interacted product sequences in a purely data-driven manner. However, they largely overlook the knowledgeable substitutable and complementary relations between products. To address this issue, we propose a novel Substitutable and Complementary Graph-based Sequential Product Recommendation model, namely, SCG-SPRe. The innovations of SCG-SPRe lie in its two main modules: (1) The module of interactive graph neural networks jointly encodes the high-order product correlations in the substitutable graph and the complementary graph into two types of relation-specific product representations. (2) The module of kernel-enhanced transformer networks adaptively fuses multiple temporal kernels to characterize the unique temporal patterns between a candidate product to be recommended and any interacted product in a target behavior sequence. Thanks to the seamless integration of the two modules, SCG-SPRe obtains candidate-dependent user representations for different candidate products to compute the corresponding ranking scores. We conduct extensive experiments on three public datasets, demonstrating SCG-SPRe is superior to competitive sequential recommendation baselines and validating the benefits of explicitly modeling the product-product relations.  © 2021 Association for Computing Machinery.",attention mechanism; graph neural networks; Sequential recommendation; substitutable and complementary relations,Graphic methods; Recommender systems; User profile; Attention mechanisms; Complementary relation; Data driven; Graph neural networks; Graph-based; Hotspot topics; Product recommendation; Product sequence; Sequential recommendation; Substitutable and complementary relation; Graph neural networks
Graph Technologies for User Modeling and Recommendation: Introduction to the Special Issue - Part 1,2022,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124091192&doi=10.1145%2f3477596&partnerID=40&md5=27cdc192d930f76b1fdb0796bea7adf0,"Recent years have seen a surge of research on graph techniques due to their powerful abilities in modeling the relationships among data objects. Learning on graph-structured data has gradually become a central theme in machine learning and data mining. The special issue of ACM Transactions on Information Systems covers a wide range of topics related to graph technologies, including three articles on sequential recommendation, five articles on social recommendation, two articles on the fairness issue on recommendation and learning to rank, one article on network visibility, one article on recommendation in legal scenarios, and one article on user profiling in question answering communities. The articles discussed cover a broad range of topics related to graph technologies for user modeling and recommendation, including but not limited to collaborative filtering, sequential recommendation, social recommendation, bias and fairness, and other issues.",bias; fairness; Graph; knowledge graph; meta-learning; sequential recommendation; social recommendation,Collaborative filtering; Data mining; Learning systems; User profile; Bias; Data objects; Fairness; Graph technique; Knowledge graphs; Metalearning; Sequential recommendation; Social recommendation; User Modelling; User recommendations; Knowledge graph
Learning to Learn a Cold-start Sequential Recommender,2022,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124108238&doi=10.1145%2f3466753&partnerID=40&md5=ceb1f377bd64bd65d25954cd4e641391,"The cold-start recommendation is an urgent problem in contemporary online applications. It aims to provide users whose behaviors are literally sparse with as accurate recommendations as possible. Many data-driven algorithms, such as the widely used matrix factorization, underperform because of data sparseness. This work adopts the idea of meta-learning to solve the user's cold-start recommendation problem. We propose a meta-learning-based cold-start sequential recommendation framework called metaCSR, including three main components: Diffusion Representer for learning better user/item embedding through information diffusion on the interaction graph; Sequential Recommender for capturing temporal dependencies of behavior sequences; and Meta Learner for extracting and propagating transferable knowledge of prior users and learning a good initialization for new users. metaCSR holds the ability to learn the common patterns from regular users' behaviors and optimize the initialization so that the model can quickly adapt to new users after one or a few gradient updates to achieve optimal performance. The extensive quantitative experiments on three widely used datasets show the remarkable performance of metaCSR in dealing with the user cold-start problem. Meanwhile, a series of qualitative analysis demonstrates that the proposed metaCSR has good generalization.  © 2022 Association for Computing Machinery.",Cold-start recommendation; graph representation; meta-learning; sequential recommendation,Cold-start; Cold-start Recommendations; Data-driven algorithm; Graph representation; Learning to learn; Matrix factorizations; Metalearning; On-line applications; Sequential recommendation; Urgent problems; Factorization
Bilateral Filtering Graph Convolutional Network for Multi-relational Social Recommendation in the Power-law Networks,2022,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122163587&doi=10.1145%2f3469799&partnerID=40&md5=f9e568de65f535d7bb07eb887b78f63d,"In recent years, advances in Graph Convolutional Networks (GCNs) have given new insights into the development of social recommendation. However, many existing GCN-based social recommendation methods often directly apply GCN to capture user-item and user-user interactions, which probably have two main limitations: (a) Due to the power-law property of the degree distribution, the vanilla GCN with static normalized adjacency matrix has limitations in learning node representations, especially for the long-tail nodes; (b) multi-typed social relationships between users that are ubiquitous in the real world are rarely considered. In this article, we propose a novel Bilateral Filtering Heterogeneous Attention Network (BFHAN), which improves long-tail node representations and leverages multi-typed social relationships between user nodes. First, we propose a novel graph convolutional filter for the user-item bipartite network and extend it to the user-user homogeneous network. Further, we theoretically analyze the correlation between the convergence values of different graph convolutional filters and node degrees after stacking multiple layers. Second, we model multi-relational social interactions between users as the multiplex network and further propose a multiplex attention network to capture distinctive inter-layer influences for user representations. Last but not least, the experimental results demonstrate that our proposed method outperforms several state-of-the-art GCN-based methods for social recommendation tasks.  © 2021 Association for Computing Machinery.",heterogeneous graph neural network; multi-typed social relationships; power-law networks; Social recommendation,Convolution; Convolutional neural networks; Factorization; Nonlinear filtering; Social aspects; Bilateral filtering; Convolutional networks; Graph neural networks; Heterogeneous graph; Heterogeneous graph neural network; Multi-typed social relationship; Network-based; Power-law networks; Social recommendation; Social relationships; Graph neural networks
Efficient Multi-modal Hashing with Online Query Adaption for Multimedia Retrieval,2022,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123199153&doi=10.1145%2f3477180&partnerID=40&md5=471d4784fc7134bb070a285550cfe59f,"Multi-modal hashing supports efficient multimedia retrieval well. However, existing methods still suffer from two problems: (1) Fixed multi-modal fusion. They collaborate the multi-modal features with fixed weights for hash learning, which cannot adaptively capture the variations of online streaming multimedia contents. (2) Binary optimization challenge. To generate binary hash codes, existing methods adopt either two-step relaxed optimization that causes significant quantization errors or direct discrete optimization that consumes considerable computation and storage cost. To address these problems, we first propose a Supervised Multi-modal Hashing with Online Query-adaption method. A self-weighted fusion strategy is designed to adaptively preserve the multi-modal features into hash codes by exploiting their complementarity. Besides, the hash codes are efficiently learned with the supervision of pair-wise semantic labels to enhance their discriminative capability while avoiding the challenging symmetric similarity matrix factorization. Further, we propose an efficient Unsupervised Multi-modal Hashing with Online Query-adaption method with an adaptive multi-modal quantization strategy. The hash codes are directly learned without the reliance on the specific objective formulations. Finally, in both methods, we design a parameter-free online hashing module to adaptively capture query variations at the online retrieval stage. Experiments validate the superiority of our proposed methods.  © 2021 Association for Computing Machinery.",adaptive multi-modal quantization; asymmetric semantic supervision; complementary; Multi-modal hashing; online query adaption; prototypes,Factorization; Hash functions; Optimization; Adaptive multi-modal quantization; Asymmetric semantic supervision; Complementary; Multi-modal; Multi-modal hashing; Multimedia Retrieval; Online query adaption; Prototype; Quantisation; Semantics
LegalGNN: Legal Information Enhanced Graph Neural Network for Recommendation,2022,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122142209&doi=10.1145%2f3469887&partnerID=40&md5=75d6880345aa88500b3936ea473c607d,"Recommendation in legal scenario (Legal-Rec) is a specialized recommendation task that aims to provide potential helpful legal documents for users. While there are mainly three differences compared with traditional recommendation: (1) Both the structural connections and textual contents of legal information are important in the Legal-Rec scenario, which means feature fusion is very important here. (2) Legal-Rec users prefer the newest legal cases (the latest legal interpretation and legal practice), which leads to a severe new-item problem. (3) Different from users in other scenarios, most Legal-Rec users are expert and domain-related users. They often concentrate on several topics and have more stable information needs. So it is important to accurately model user interests here. To the best of our knowledge, existing recommendation work cannot handle these challenges simultaneously.To address these challenges, we propose a legal information enhanced graph neural network-based recommendation framework (LegalGNN). First, a unified legal content and structure representation model is designed for feature fusion, where the Heterogeneous Legal Information Network (HLIN) is constructed to connect the structural features (e.g., knowledge graph) and contextual features (e.g., the content of legal documents) for training. Second, to model user interests, we incorporate the queries users issued in legal systems into the HLIN and link them with both retrieved documents and inquired users. This extra information is not only helpful for estimating user preferences, but also valuable for cold users/items (with less interaction history) in this scenario. Third, a graph neural network with relational attention mechanism is applied to make use of high-order connections in HLIN for Legal-Rec. Experimental results on a real-world legal dataset verify that LegalGNN outperforms several state-of-the-art methods significantly. As far as we know, LegalGNN is the first graph neural model for legal recommendation.  © 2021 Association for Computing Machinery.",graph neural network; heterogeneous environments; heterogeneous information network; Legal information recommendation,Graph neural networks; Information services; Recommender systems; Features fusions; Graph neural networks; Heterogeneous environments; Heterogeneous information; Heterogeneous information network; Information networks; Information recommendation; Legal documents; Legal information; Legal information recommendation; Authentication
Multi-Graph Heterogeneous Interaction Fusion for Social Recommendation,2022,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122143293&doi=10.1145%2f3466641&partnerID=40&md5=69b84b8ec824376936b79c16fd280a3b,"With the rapid development of online social recommendation system, substantial methods have been proposed. Unlike traditional recommendation system, social recommendation performs by integrating social relationship features, where there are two major challenges, i.e., early summarization and data sparsity. Thus far, they have not been solved effectively. In this article, we propose a novel social recommendation approach, namely Multi-Graph Heterogeneous Interaction Fusion (MG-HIF), to solve these two problems. Our basic idea is to fuse heterogeneous interaction features from multi-graphs, i.e., user-item bipartite graph and social relation network, to improve the vertex representation learning. A meta-path cross-fusion model is proposed to fuse multi-hop heterogeneous interaction features via discrete cross-correlations. Based on that, a social relation GAN is developed to explore latent friendships of each user. We further fuse representations from two graphs by a novel multi-graph information fusion strategy with attention mechanism. To the best of our knowledge, this is the first work to combine meta-path with social relation representation. To evaluate the performance of MG-HIF, we compare MG-HIF with seven states of the art over four benchmark datasets. The experimental results show that MG-HIF achieves better performance.  © 2021 Association for Computing Machinery.",graph gan; heterogeneous interaction fusion; meta-path; multi-graph; Social recommendation,Benchmarking; Recommender systems; Graph gan; Heterogeneous interaction fusion; Heterogeneous interactions; Interaction features; Meta-path; Multi-graph; Performance; Social recommendation; Social relations; Social relationships; Graph theory
Does More Context Help? Effects of Context Window and Application Source on Retrieval Performance,2022,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123900630&doi=10.1145%2f3474055&partnerID=40&md5=258b4b3540878863e3f89c03cadc8a29,"We study the effect of contextual information obtained from a user's digital trace on Web search performance. Contextual information is modeled using Dirichlet-Hawkes processes (DHP) and used in augmenting Web search queries. The context is captured by monitoring all naturally occurring user behavior using continuous 24/7 recordings of the screen and associating the context with the queries issued by the users. We report a field study in which 13 participants installed a screen recording and digital activity monitoring system on their laptops for 14 days, resulting in data on all Web search queries and the associated context data. A query augmentation (QAug) model was built to expand the original query with semantically related terms. The effects of context window and source were determined by training context models with temporally varying context windows and varying application sources. The context models were then utilized to re-rank the QAug model. We evaluate the context models by using the Web document rankings of the original query as a control condition compared against various experimental conditions: (1) a search context condition in which the context was sourced from search history; (2) a non-search context condition in which the context was sourced from all interactions excluding search history; (3) a comprehensive context condition in which the context was sourced from both search and non-search histories; and (4) an application-specific condition in which the context was sourced from interaction histories captured on a specific application type. Our results indicated that incorporating more contextual information significantly improved Web search rankings as measured by the positions of the documents on which users clicked in the search result pages. The effects and importance of different context windows and application sources, along with different query types are analyzed, and their impact on Web search performance is discussed.  © 2021 Association for Computing Machinery.",application source; context window; contextual information; digital user behavior; query augmentation; Web search,Behavioral research; Search engines; Websites; Application source; Condition; Context models; Context window; Contextual information; Digital user behavior; Query augmentations; Search history; User behaviors; Web searches; Information retrieval
A Graph-Based Approach for Mitigating Multi-Sided Exposure Bias in Recommender Systems,2022,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124074685&doi=10.1145%2f3470948&partnerID=40&md5=3d32fcac4441ed8805851cc4249c5166,"Fairness is a critical system-level objective in recommender systems that has been the subject of extensive recent research. A specific form of fairness is supplier exposure fairness, where the objective is to ensure equitable coverage of items across all suppliers in recommendations provided to users. This is especially important in multistakeholder recommendation scenarios where it may be important to optimize utilities not just for the end user but also for other stakeholders such as item sellers or producers who desire a fair representation of their items. This type of supplier fairness is sometimes accomplished by attempting to increase aggregate diversity to mitigate popularity bias and to improve the coverage of long-tail items in recommendations. In this article, we introduce FairMatch, a general graph-based algorithm that works as a post-processing approach after recommendation generation to improve exposure fairness for items and suppliers. The algorithm iteratively adds high-quality items that have low visibility or items from suppliers with low exposure to the users' final recommendation lists. A comprehensive set of experiments on two datasets and comparison with state-of-the-art baselines show that FairMatch, although it significantly improves exposure fairness and aggregate diversity, maintains an acceptable level of relevance of the recommendations.  © 2021 Association for Computing Machinery.",aggregate diversity; exposure fairness; long-tail; popularity bias; Recommender systems,Aggregates; Graphic methods; Iterative methods; Aggregate diversity; Critical systems; End-users; Exposure fairness; Graph-based; Long tail; Multi-stakeholder; Popularity bias; Recent researches; System levels; Recommender systems
Sequential-Knowledge-Aware Next POI Recommendation: A Meta-Learning Approach,2022,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124092927&doi=10.1145%2f3460198&partnerID=40&md5=aeb25dd4663e5bb94a4b6ca108d4e391,"Accurately recommending the next point of interest (POI) has become a fundamental problem with the rapid growth of location-based social networks. However, sparse, imbalanced check-in data and diverse user check-in patterns pose severe challenges for POI recommendation tasks. Knowledge-aware models are known to be primary in leveraging these problems. However, as most knowledge graphs are constructed statically, sequential information is yet integrated. In this work, we propose a meta-learned sequential-knowledge-aware recommender (Meta-SKR), which utilizes sequential, spatio-temporal, and social knowledge to recommend the next POI for a location-based social network user. The framework mainly contains four modules. First, in the graph construction module, a novel type of knowledge graph - the sequential knowledge graph, which is sensitive to the check-in order of POIs - is built to model users' check-in patterns. To deal with the problem of data sparsity, a meta-learning module based on latent embedding optimization is then introduced to generate user-conditioned parameters of the subsequent sequential-knowledge-aware embedding module, where representation vectors of entities (nodes) and relations (edges) are learned. In this embedding module, gated recurrent units are adapted to distill intra- and inter-sequential knowledge graph information. We also design a novel knowledge-aware attention mechanism to capture information surrounding a given node. Finally, POI recommendation is provided by inferring potential links of knowledge graphs in the prediction module. Evaluations on three real-world check-in datasets show that Meta-SKR can achieve high recommendation accuracy even with sparse data.  © 2021 Association for Computing Machinery.",knowledge graphs; meta-learning; POI recommendation,Graph embeddings; Knowledge management; Social networking (online); Check-in; Embeddings; Knowledge graphs; Location-based social networks; Meta-learning approach; Metalearning; Point of interest recommendation; Rapid growth; Sequential information; Spatio-temporal; Knowledge graph
HyperSoRec: Exploiting Hyperbolic User and Item Representations with Multiple Aspects for Social-aware Recommendation,2022,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124104357&doi=10.1145%2f3463913&partnerID=40&md5=9e2191478c204bd70a35309afa56be85,"Social recommendation has achieved great success in many domains including e-commerce and location-based social networks. Existing methods usually explore the user-item interactions or user-user connections to predict users' preference behaviors. However, they usually learn both user and item representations in Euclidean space, which has large limitations for exploring the latent hierarchical property in the data. In this article, we study a novel problem of hyperbolic social recommendation, where we aim to learn the compact but strong representations for both users and items. Meanwhile, this work also addresses two critical domain-issues, which are under-explored. First, users often make trade-offs with multiple underlying aspect factors to make decisions during their interactions with items. Second, users generally build connections with others in terms of different aspects, which produces different influences with aspects in social network. To this end, we propose a novel graph neural network (GNN) framework with multiple aspect learning, namely, HyperSoRec. Specifically, we first embed all users, items, and aspects into hyperbolic space with superior representations to ensure their hierarchical properties. Then, we adapt a GNN with novel multi-aspect message-passing-receiving mechanism to capture different influences among users. Next, to characterize the multi-aspect interactions of users on items, we propose an adaptive hyperbolic metric learning method by introducing learnable interactive relations among different aspects. Finally, we utilize the hyperbolic translational distance to measure the plausibility in each user-item pair for recommendation. Experimental results on two public datasets clearly demonstrate that our HyperSoRec not only achieves significant improvement for recommendation performance but also shows better representation ability in hyperbolic space with strong robustness and reliability.  © 2021 Association for Computing Machinery.",Hyperbolic social recommendation; multi-aspect item interaction; multi-aspect user influence,Economic and social effects; Learning systems; Message passing; Social networking (online); Graph neural networks; Hyperbolic social recommendation; Hyperbolic spaces; Learn+; Multi aspects; Multi-aspect item interaction; Multi-aspect user influence; Property; User influences; Graph neural networks
Exploiting Positional Information for Session-Based Recommendation,2022,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124097136&doi=10.1145%2f3473339&partnerID=40&md5=b205306098b041cea82882aa504c9781,"For present e-commerce platforms, it is important to accurately predict users' preference for a timely next-item recommendation. To achieve this goal, session-based recommender systems are developed, which are based on a sequence of the most recent user-item interactions to avoid the influence raised from outdated historical records. Although a session can usually reflect a user's current preference, a local shift of the user's intention within the session may still exist. Specifically, the interactions that take place in the early positions within a session generally indicate the user's initial intention, while later interactions are more likely to represent the latest intention. Such positional information has been rarely considered in existing methods, which restricts their ability to capture the significance of interactions at different positions. To thoroughly exploit the positional information within a session, a theoretical framework is developed in this paper to provide an in-depth analysis of the positional information. We formally define the properties of forward-awareness and backward-awareness to evaluate the ability of positional encoding schemes in capturing the initial and the latest intention. According to our analysis, existing positional encoding schemes are generally forward-aware only, which can hardly represent the dynamics of the intention in a session. To enhance the positional encoding scheme for the session-based recommendation, a dual positional encoding (DPE) is proposed to account for both forward-awareness and backward-awareness. Based on DPE, we propose a novel Positional Recommender (PosRec) model with a well-designed Position-aware Gated Graph Neural Network module to fully exploit the positional information for session-based recommendation tasks. Extensive experiments are conducted on two e-commerce benchmark datasets, Yoochoose and Diginetica and the experimental results show the superiority of the PosRec by comparing it with the state-of-the-art session-based recommender models.  © 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM.",graph neural network; positional encoding; Session-based recommendation,Data mining; Electronic commerce; Encoding (symbols); Graph neural networks; Network coding; 'current; Commerce platforms; E- commerces; Encoding schemes; Graph neural networks; Historical records; Positional encoding; Positional information; Session-based recommendation; User's preferences; Recommender systems
Fast Filtering of Search Results Sorted by Attribute,2022,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124087978&doi=10.1145%2f3477982&partnerID=40&md5=0ec42dd5bb8fad5155bddad738771ed6,"Modern search services often provide multiple options to rank the search results, e.g., sort ""by relevance"", ""by price""or ""by discount""in e-commerce. While the traditional rank by relevance effectively places the relevant results in the top positions of the results list, the rank by attribute could place many marginally relevant results in the head of the results list leading to poor user experience. In the past, this issue has been addressed by investigating the relevance-aware filtering problem, which asks to select the subset of results maximizing the relevance of the attribute-sorted list. Recently, an exact algorithm has been proposed to solve this problem optimally. However, the high computational cost of the algorithm makes it impractical for the Web search scenario, which is characterized by huge lists of results and strict time constraints. For this reason, the problem is often solved using efficient yet inaccurate heuristic algorithms. In this article, we first prove the performance bounds of the existing heuristics. We then propose two efficient and effective algorithms to solve the relevance-aware filtering problem. First, we propose OPT-Filtering, a novel exact algorithm that is faster than the existing state-of-the-art optimal algorithm. Second, we propose an approximate and even more efficient algorithm, -Filtering, which, given an allowed approximation error , finds a (1-)-optimal filtering, i.e., the relevance of its solution is at least (1-) times the optimum. We conduct a comprehensive evaluation of the two proposed algorithms against state-of-the-art competitors on two real-world public datasets. Experimental results show that OPT-Filtering achieves a significant speedup of up to two orders of magnitude with respect to the existing optimal solution, while -Filtering further improves this result by trading effectiveness for efficiency. In particular, experiments show that -Filtering can achieve quasi-optimal solutions while being faster than all state-of-the-art competitors in most of the tested configurations.  © 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM.",approximation algorithms; efficiency-effectiveness trade-offs; filtering algorithms; Relevance-aware filtering,Approximation algorithms; Commerce; Economic and social effects; Filtration; Heuristic algorithms; Optimal systems; Optimization; Signal filtering and prediction; E- commerces; Efficiency-effectiveness trade-offs; Exact algorithms; Filtering algorithm; Filtering problems; Optimal solutions; Relevance-aware filtering; Search services; State of the art; Trade off; Efficiency
CHA: Categorical Hierarchy-based Attention for Next POI Recommendation,2022,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123934541&doi=10.1145%2f3464300&partnerID=40&md5=6febe8f586c9b1c1fc5598dd114ed808,"Next Point-of-interest (POI) recommendation is a key task in improving location-related customer experiences and business operations, but yet remains challenging due to the substantial diversity of human activities and the sparsity of the check-in records available. To address these challenges, we proposed to explore the category hierarchy knowledge graph of POIs via an attention mechanism to learn the robust representations of POIs even when there is insufficient data. We also proposed a spatial-Temporal decay LSTM and a Discrete Fourier Series-based periodic attention to better facilitate the capturing of the personalized behavior pattern. Extensive experiments on two commonly adopted real-world location-based social networks (LBSNs) datasets proved that the inclusion of the aforementioned modules helps to boost the performance of next and next new POI recommendation tasks significantly. Specifically, our model in general outperforms other state-of-The-Art methods by a large margin. © 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM.",categorical hierarchy-based attention; next new POI recommendation; Next POI recommendation,Knowledge graph; Long short-term memory; Attention mechanisms; Business operation; Categorical hierarchy; Categorical hierarchy-based attention; Check-in; Customer experience; Human activities; Knowledge graphs; Next new point-of-interest recommendation; Next point-of-interest recommendation; Fourier series
User Profiling Based on Nonlinguistic Audio Data,2022,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123956727&doi=10.1145%2f3474826&partnerID=40&md5=ed793a31ca55b10b1f5fd2d56aaafca0,"User profiling refers to inferring people's attributes of interest (AoIs) like gender and occupation, which enables various applications ranging from personalized services to collective analyses. Massive nonlinguistic audio data brings a novel opportunity for user profiling due to the prevalence of studying spontaneous face-To-face communication. Nonlinguistic audio is coarse-grained audio data without linguistic content. It is collected due to privacy concerns in private situations like doctor-patient dialogues. The opportunity facilitates optimized organizational management and personalized healthcare, especially for chronic diseases. In this article, we are the first to build a user profiling system to infer gender and personality based on nonlinguistic audio. Instead of linguistic or acoustic features that are unable to extract, we focus on conversational features that could reflect AoIs. We firstly develop an adaptive voice activity detection algorithm that could address individual differences in voice and false-positive voice activities caused by people nearby. Secondly, we propose a gender-Assisted multi-Task learning method to combat dynamics in human behavior by integrating gender differences and the correlation of personality traits. According to the experimental evaluation of 100 people in 273 meetings, we achieved 0.759 and 0.652 in F1-score for gender identification and personality recognition, respectively. © 2021 Association for Computing Machinery.",gender identification; multi-Task learning; nonlinguistic audio; personality recognition; User profiling,Behavioral research; Linguistics; Speech recognition; User profile; Audio data; Coarse-grained; Face-to-face communications; Gender identification; Multitask learning; Nonlinguistic audio; Personality recognition; Personalized service; Privacy concerns; User's profiling; Learning systems
Question Tagging via Graph-guided Ranking,2022,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123256870&doi=10.1145%2f3468270&partnerID=40&md5=44681d7bd8a2b59cf173787e6949d8b4,"With the increasing prevalence of portable devices and the popularity of community Question Answering (cQA) sites, users can seamlessly post and answer many questions. To effectively organize the information for precise recommendation and easy searching, these platforms require users to select topics for their raised questions. However, due to the limited experience, certain users fail to select appropriate topics for their questions. Thereby, automatic question tagging becomes an urgent and vital problem for the cQA sites, yet it is non-Trivial due to the following challenges. On the one hand, vast and meaningful topics are available yet not utilized in the cQA sites; how to model and tag them to relevant questions is a highly challenging problem. On the other hand, related topics in the cQA sites may be organized into a directed acyclic graph. In light of this, how to exploit relations among topics to enhance their representations is critical. To settle these challenges, we devise a graph-guided topic ranking model to tag questions in the cQA sites appropriately. In particular, we first design a topic information fusion module to learn the topic representation by jointly considering the name and description of the topic. Afterwards, regarding the special structure of topics, we propose an information propagation module to enhance the topic representation. As the comprehension of questions plays a vital role in question tagging, we design a multi-level context-modeling-based question encoder to obtain the enhanced question representation. Moreover, we introduce an interaction module to extract topic-Aware question information and capture the interactive information between questions and topics. Finally, we utilize the interactive information to estimate the ranking scores for topics. Extensive experiments on three Chinese cQA datasets have demonstrated that our proposed model outperforms several state-of-The-Art competitors. © 2021 Association for Computing Machinery.",community question answering; Graph-guided topic ranking; question tagging,Information dissemination; Social networking (online); User profile; Community question answering; First designs; Fusion modules; Graph-guided topic ranking; Interactive informations; Non-trivial; Portable device; Question tagging; Ranking model; Topic rankings; Directed graphs
Knowledge Preserving and Distribution Alignment for Heterogeneous Domain Adaptation,2022,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123613926&doi=10.1145%2f3469856&partnerID=40&md5=b3bff525da12c06e40bd9423d75382ed,"Domain adaptation aims at improving the performance of learning tasks in a target domain by leveraging the knowledge extracted from a source domain. To this end, one can perform knowledge transfer between these two domains. However, this problem becomes extremely challenging when the data of these two domains are characterized by different types of features, i.e., the feature spaces of the source and target domains are different, which is referred to as heterogeneous domain adaptation (HDA). To solve this problem, we propose a novel model called Knowledge Preserving and Distribution Alignment (KPDA), which learns an augmented target space by jointly minimizing information loss and maximizing domain distribution alignment. Specifically, we seek to discover a latent space, where the knowledge is preserved by exploiting the Laplacian graph terms and reconstruction regularizations. Moreover, we adopt the Maximum Mean Discrepancy to align the distributions of the source and target domains in the latent space. Mathematically, KPDA is formulated as a minimization problem with orthogonal constraints, which involves two projection variables. Then, we develop an algorithm based on the Gauss-Seidel iteration scheme and split the problem into two subproblems, which are solved by searching algorithms based on the Barzilai-Borwein (BB) stepsize. Promising results demonstrate the effectiveness of the proposed method. © 2021 Association for Computing Machinery.",distribution alignment; domain adaptation; Heterogeneous domain adaptation; local structure; reconstruction; transfer learning,Alignment; Iterative methods; Distribution alignment; Domain adaptation; Heterogeneous domain adaptation; Heterogeneous domains; Local structure; Reconstruction; Target domain; Transfer learning; Two domains; Knowledge management
Interpretable Aspect-Aware Capsule Network for Peer Review Based Citation Count Prediction,2022,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123933833&doi=10.1145%2f3466640&partnerID=40&md5=2a8f5c935d1da14ca4b050a60c515b3b,"Citation count prediction is an important task for estimating the future impact of research papers. Most of the existing works utilize the information extracted from the paper itself. In this article, we focus on how to utilize another kind of useful data signal (i.e., peer review text) to improve both the performance and interpretability of the prediction models.Specially, we propose a novel aspect-Aware capsule network for citation count prediction based on review text. It contains two major capsule layers, namely the feature capsule layer and the aspect capsule layer, with two different routing approaches, respectively. Feature capsules encode the local semantics from review sentences as the input of aspect capsule layer, whereas aspect capsules aim to capture high-level semantic features that will be served as final representations for prediction. Besides the predictive capacity, we also enhance the model interpretability with two strategies. First, we use the topic distribution of the review text to guide the learning of aspect capsules so that each aspect capsule can represent a specific aspect in the review. Then, we use the learned aspect capsules to generate readable text for explaining the predicted citation count. Extensive experiments on two real-world datasets have demonstrated the effectiveness of the proposed model in both performance and interpretability. © 2021 Association for Computing Machinery.",capsule network; Citation count prediction; peer review,Semantics; Capsule network; Citation count prediction; Data signals; Interpretability; Peer review; Performance; Prediction modelling; Prediction-based; Research papers; Routing approach; Forecasting
CATS: Customizable Abstractive Topic-based Summarization,2022,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123936548&doi=10.1145%2f3464299&partnerID=40&md5=f586107fdab14983933d3b73012d4a5c,"Neural sequence-To-sequence models are the state-of-The-Art approach used in abstractive summarization of textual documents, useful for producing condensed versions of source text narratives without being restricted to using only words from the original text. Despite the advances in abstractive summarization, custom generation of summaries (e.g., towards a user's preference) remains unexplored. In this article, we present CATS, an abstractive neural summarization model that summarizes content in a sequence-To-sequence fashion while also introducing a new mechanism to control the underlying latent topic distribution of the produced summaries. We empirically illustrate the efficacy of our model in producing customized summaries and present findings that facilitate the design of such systems. We use the well-known CNN/DailyMail dataset to evaluate our model. Furthermore, we present a transfer-learning method and demonstrate the effectiveness of our approach in a low resource setting, i.e., abstractive summarization of meetings minutes, where combining the main available meetings' transcripts datasets, AMI and International Computer Science Institute(ICSI), results in merely a few hundred training documents. © 2021 Association for Computing Machinery.",abstractive summarization; Sequence-To-sequence neural models; topical customization,Abstracting; Abstractive summarization; Customisation; Customizable; Neural modelling; Sequence models; Sequence-to-sequence neural model; Source text; State-of-the-art approach; Textual documents; Topical customization; Learning systems
Unstructured Text Enhanced Open-Domain Dialogue System: A Systematic Survey,2022,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123935894&doi=10.1145%2f3464377&partnerID=40&md5=67171f8394d6869a80fad87de4bf0ea8,"Incorporating external knowledge into dialogue generation has been proven to benefit the performance of an open-domain Dialogue System (DS), such as generating informative or stylized responses, controlling conversation topics. In this article, we study the open-domain DS that uses unstructured text as external knowledge sources (Unstructured Text Enhanced Dialogue System (UTEDS)). The existence of unstructured text entails distinctions between UTEDS and traditional data-driven DS and we aim at analyzing these differences. We first give the definition of the UTEDS related concepts, then summarize the recently released datasets and models. We categorize UTEDS into Retrieval and Generative models and introduce them from the perspective of model components. The retrieval models consist of Fusion, Matching, and Ranking modules, while the generative models comprise Dialogue and Knowledge Encoding, Knowledge Selection (KS), and Response Generation modules. We further summarize the evaluation methods utilized in UTEDS and analyze the current models' performance. At last, we discuss the future development trends of UTEDS, hoping to inspire new research in this field. © 2021 Association for Computing Machinery.",knowledge grounded; knowledge selection; open-domain dialogue; Unstructured text,Open systems; Dialogue generations; Dialogue systems; External knowledge; Generative model; Knowledge grounded; Knowledge selection; Open-domain dialog; Performance; Retrieval models; Unstructured texts; Speech processing
Knowledge-Guided Disentangled Representation Learning for Recommender Systems,2022,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123910744&doi=10.1145%2f3464304&partnerID=40&md5=66555981df4f98181a3a5edb40aef4a0,"In recommender systems, it is essential to understand the underlying factors that affect user-item interaction. Recently, several studies have utilized disentangled representation learning to discover such hidden factors from user-item interaction data, which shows promising results. However, without any external guidance signal, the learned disentangled representations lack clear meanings, and are easy to suffer from the data sparsity issue. In light of these challenges, we study how to leverage knowledge graph (KG) to guide the disentangled representation learning in recommender systems. The purpose for incorporating KG is twofold, making the disentangled representations interpretable and resolving data sparsity issue. However, it is not straightforward to incorporate KG for improving disentangled representations, because KG has very different data characteristics compared with user-item interactions. We propose a novel Knowledge-guided Disentangled Representations approach (KDR) to utilizing KG to guide the disentangled representation learning in recommender systems. The basic idea, is to first learn more interpretable disentangled dimensions (explicit disentangled representations) based on structural KG, and then align implicit disentangled representations learned from user-item interaction with the explicit disentangled representations. We design a novel alignment strategy based on mutual information maximization. It enables the KG information to guide the implicit disentangled representation learning, and such learned disentangled representations will correspond to semantic information derived from KG. Finally, the fused disentangled representations are optimized to improve the recommendation performance. Extensive experiments on three real-world datasets demonstrate the effectiveness of the proposed model in terms of both performance and interpretability. © 2021 Association for Computing Machinery.",disentangled representation; Knowledge graph; recommender system; representation learning,Knowledge graph; Knowledge management; Learning systems; Semantics; Data characteristics; Data sparsity; Disentangled representation; Graph information; Knowledge graphs; Learn+; Mutual information maximization; Representation learning; Structural knowledge; Underlying factors; Recommender systems
Joint Representation Learning with Relation-Enhanced Topic Models for Intelligent Job Interview Assessment,2022,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123959539&doi=10.1145%2f3469654&partnerID=40&md5=6be607f6ebbbbfe36a27e62dbabbc6ea,"The job interview is considered as one of the most essential tasks in talent recruitment, which forms a bridge between candidates and employers in fitting the right person for the right job. While substantial efforts have been made on improving the job interview process, it is inevitable to have biased or inconsistent interview assessment due to the subjective nature of the traditional interview process. To this end, in this article, we propose three novel approaches to intelligent job interview by learning the large-scale real-world interview data. Specifically, we first develop a preliminary model, named Joint Learning Model on Interview Assessment (JLMIA), to mine the relationship among job description, candidate resume, and interview assessment. Then, we further design an enhanced model, named Neural-JLMIA, to improve the representative capability by applying neural variance inference. Last, we propose to refine JLMIA with Refined-JLMIA (R-JLMIA) by modeling individual characteristics for each collection, i.e., disentangling the core competences from resume and capturing the evolution of the semantic topics over different interview rounds. As a result, our approaches can effectively learn the representative perspectives of different job interview processes from the successful job interview records in history. In addition, we exploit our approaches for two real-world applications, i.e., person-job fit and skill recommendation for interview assessment. Extensive experiments conducted on real-world data clearly validate the effectiveness of our models, which can lead to substantially less bias in job interviews and provide an interpretable understanding of job interview assessment. © 2021 Association for Computing Machinery.",Interview assessment; latent variable model; neural topic model; representation disentanglement; sequential data,Employment; Learning systems; Semantics; Interview assessment; Job interviews; Joint learning; Latent variable modeling; Learning models; Neural topic model; Real-world; Representation disentanglement; Sequential data; Topic Modeling; Job analysis
On the Study of Transformers for Query Suggestion,2022,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123915257&doi=10.1145%2f3470562&partnerID=40&md5=fbd7f4e9a65c6fca3a969596f8af4918,"When conducting a search task, users may find it difficult to articulate their need, even more so when the task is complex. To help them complete their search, search engine usually provide query suggestions. A good query suggestion system requires to model user behavior during the search session. In this article, we study multiple Transformer architectures applied to the query suggestion task and compare them with recurrent neural network (RNN)-based models. We experiment Transformer models with different tokenizers, with different Encoders (large pretrained models or fully trained ones), and with two kinds of architectures (flat or hierarchic). We study the performance and the behaviors of these various models, and observe that Transformer-based models outperform RNN-based ones. We show that while the hierarchical architectures exhibit very good performances for query suggestion, the flat models are more suitable for complex and long search tasks. Finally, we investigate the flat models behavior and demonstrate that they indeed learn to recover the hierarchy of a search session. © 2021 Association for Computing Machinery.",bart; bert; hierachical model; query prediction; query suggestion; transformers; User modeling,Behavioral research; Complex networks; Network architecture; Search engines; Bart; Bert; Hierachical model; Performance; Query prediction; Query suggestion; Search sessions; Search tasks; Transformer; User Modelling; Recurrent neural networks
A Review on Question Generation from Natural Language Text,2022,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123911965&doi=10.1145%2f3468889&partnerID=40&md5=798fbcc6fbe7f19ada6178603ac81fc7,"Question generation is an important yet challenging problem in Artificial Intelligence (AI), which aims to generate natural and relevant questions from various input formats, e.g., natural language text, structure database, knowledge base, and image. In this article, we focus on question generation from natural language text, which has received tremendous interest in recent years due to the widespread applications such as data augmentation for question answering systems. During the past decades, many different question generation models have been proposed, from traditional rule-based methods to advanced neural network-based methods. Since there have been a large variety of research works proposed, we believe it is the right time to summarize the current status, learn from existing methodologies, and gain some insights for future development. In contrast to existing reviews, in this survey, we try to provide a more comprehensive taxonomy of question generation tasks from three different perspectives, i.e., the types of the input context text, the target answer, and the generated question. We take a deep look into existing models from different dimensions to analyze their underlying ideas, major design principles, and training strategies We compare these models through benchmark tasks to obtain an empirical understanding of the existing techniques. Moreover, we discuss what is missing in the current literature and what are the promising and desired future directions. © 2021 Association for Computing Machinery.",natural language generation; Question generation; survey,Knowledge based systems; Natural language processing systems; Data augmentation; Input format; Natural language generation; Natural languages texts; Network-based; Neural-networks; Question answering systems; Question generation; Rule-based method; Text structure; Surveys
Hierarchical Hyperedge Embedding-Based Representation Learning for Group Recommendation,2022,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123909143&doi=10.1145%2f3457949&partnerID=40&md5=7b1b63c28527969a3f522775ad5c596a,"Group recommendation aims to recommend items to a group of users. In this work, we study group recommendation in a particular scenario, namely occasional group recommendation, where groups are formed ad hoc and users may just constitute a group for the first time-that is, the historical group-item interaction records are highly limited. Most state-of-The-Art works have addressed the challenge by aggregating group members' personal preferences to learn the group representation. However, the representation learning for a group is most complex beyond the aggregation or fusion of group member representation, as the personal preferences and group preferences may be in different spaces and even orthogonal. In addition, the learned user representation is not accurate due to the sparsity of users' interaction data. Moreover, the group similarity in terms of common group members has been overlooked, which, however, has the great potential to improve the group representation learning. In this work, we focus on addressing the aforementioned challenges in the group representation learning task, and devise a hierarchical hyperedge embedding-based group recommender, namely HyperGroup. Specifically, we propose to leverage the user-user interactions to alleviate the sparsity issue of user-item interactions, and design a graph neural network-based representation learning network to enhance the learning of individuals' preferences from their friends' preferences, which provides a solid foundation for learning groups' preferences. To exploit the group similarity (i.e., overlapping relationships among groups) to learn a more accurate group representation from highly limited group-item interactions, we connect all groups as a network of overlapping sets (a.k.a. hypergraph), and treat the task of group preference learning as embedding hyperedges (i.e., user sets/groups) in a hypergraph, where an inductive hyperedge embedding method is proposed. To further enhance the group-level preference modeling, we develop a joint training strategy to learn both user-item and group-item interactions in the same process. We conduct extensive experiments on two real-world datasets, and the experimental results demonstrate the superiority of our proposed HyperGroup in comparison to the state-of-The-Art baselines. © 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Group recommendation; hyperedge embedding; representation learning,Data mining; Graph neural networks; User profile; Embeddings; Group members; Group recommendations; Group representation; Hyperedge embedding; Hyperedges; Learn+; Personal preferences; Representation learning; State of the art; Embeddings
Collaborative Reflection-Augmented Autoencoder Network for Recommender Systems,2022,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123927255&doi=10.1145%2f3467023&partnerID=40&md5=b592d04b1400a0bea9fa5109309a8dcf,"As the deep learning techniques have expanded to real-world recommendation tasks, many deep neural network based Collaborative Filtering (CF) models have been developed to project user-item interactions into latent feature space, based on various neural architectures, such as multi-layer perceptron, autoencoder, and graph neural networks. However, the majority of existing collaborative filtering systems are not well designed to handle missing data. Particularly, in order to inject the negative signals in the training phase, these solutions largely rely on negative sampling from unobserved user-item interactions and simply treating them as negative instances, which brings the recommendation performance degradation. To address the issues, we develop a Collaborative Reflection-Augmented Autoencoder Network (CRANet), that is capable of exploring transferable knowledge from observed and unobserved user-item interactions. The network architecture of CRANet is formed of an integrative structure with a reflective receptor network and an information fusion autoencoder module, which endows our recommendation framework with the ability of encoding implicit user's pairwise preference on both interacted and non-interacted items. Additionally, a parametric regularization-based tied-weight scheme is designed to perform robust joint training of the two-stage CRANetmodel. We finally experimentally validate CRANeton four diverse benchmark datasets corresponding to two recommendation tasks, to show that debiasing the negative signals of user-item interactions improves the performance as compared to various state-of-The-Art recommendation techniques. Our source code is available at https://github.com/akaxlh/CRANet. © 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Autoencoder; Collaborative Filtering; Recommender Systems,Benchmarking; Collaborative filtering; Deep neural networks; Graph neural networks; Multilayer neural networks; Network architecture; Auto encoders; Feature space; Filtering models; Learning techniques; Multi-layer graphs; Multilayer perceptrons neural networks (MLPs); Network-based; Neural architectures; Real-world; Space-based; Recommender systems
What and How long: Prediction of Mobile App Engagement,2022,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123914584&doi=10.1145%2f3464301&partnerID=40&md5=b91533940f2d38d1656048ca4c5e02b5,"User engagement is crucial to the long-Term success of a mobile app. Several metrics, such as dwell time, have been used for measuring user engagement. However, how to effectively predict user engagement in the context of mobile apps is still an open research question. For example, do the mobile usage contexts (e.g., time of day) in which users access mobile apps impact their dwell time? Answers to such questions could help mobile operating system and publishers to optimize advertising and service placement. In this article, we first conduct an empirical study for assessing how user characteristics, temporal features, and the short/long-Term contexts contribute to gains in predicting users' app dwell time on the population level. The comprehensive analysis is conducted on large app usage logs collected through a mobile advertising company. The dataset covers more than 12K anonymous users and 1.3 million log events. Based on the analysis, we further investigate a novel mobile app engagement prediction problem-can we predict simultaneously what app the user will use next and how long he/she will stay on that app? We propose several strategies for this joint prediction problem and demonstrate that our model can improve the performance significantly when compared with the state-of-The-Art baselines. Our work can help mobile system developers in designing a better and more engagement-Aware mobile app user experience. © 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM.",app engagement prediction; app usage; behavior modeling; demographics; dwell time; Mobile apps; next app prediction; user engagement; user modeling,Marketing; User profile; App engagement prediction; App predictions; App usage; Behaviour models; Dwell time; Mobile app; Next app prediction; Prediction problem; User engagement; User Modelling; Forecasting
The Simpson s Paradox in the Offline Evaluation of Recommendation Systems,2022,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123948331&doi=10.1145%2f3458509&partnerID=40&md5=28093bfa8022cc59277a3121d9fc1ef8,"Recommendation systems are often evaluated based on user's interactions that were collected from an existing, already deployed recommendation system. In this situation, users only provide feedback on the exposed items and they may not leave feedback on other items since they have not been exposed to them by the deployed system. As a result, the collected feedback dataset that is used to evaluate a new model is influenced by the deployed system, as a form of closed loop feedback. In this article, we show that the typical offline evaluation of recommender systems suffers from the so-called Simpson's paradox. Simpson's paradox is the name given to a phenomenon observed when a significant trend appears in several different sub-populations of observational data but disappears or is even reversed when these sub-populations are combined together. Our in-depth experiments based on stratified sampling reveal that a very small minority of items that are frequently exposed by the deployed system plays a confounding factor in the offline evaluation of recommendation systems. In addition, we propose a novel evaluation methodology that takes into account the confounder, i.e., the deployed system's characteristics. Using the relative comparison of many recommendation models as in the typical offline evaluation of recommender systems, and based on the Kendall rank correlation coefficient, we show that our proposed evaluation methodology exhibits statistically significant improvements of 14% and 40% on the examined open loop datasets (Yahoo! and Coat), respectively, in reflecting the true ranking of systems with an open loop (randomised) evaluation in comparison to the standard evaluation. © 2021 Association for Computing Machinery.",experimental design; Offline evaluation; popularity bias; selection bias; Simpson's paradox,Feedback; Population statistics; Deployed systems; Evaluation methodologies; Experimental design; Offline evaluation; Open-loop; Popularity bias; Selection bias; Simpson; Simpson paradox; Sub-populations; Recommender systems
Anytime Ranking on Document-Ordered Indexes,2022,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118687529&doi=10.1145%2f3467890&partnerID=40&md5=1250cd3d1721428fe91c3cab98fc3d8f,"Inverted indexes continue to be a mainstay of text search engines, allowing efficient querying of large document collections. While there are a number of possible organizations, document-ordered indexes are the most common, since they are amenable to various query types, support index updates, and allow for efficient dynamic pruning operations. One disadvantage with document-ordered indexes is that high-scoring documents can be distributed across the document identifier space, meaning that index traversal algorithms that terminate early might put search effectiveness at risk. The alternative is impact-ordered indexes, which primarily support top-disjunctions but also allow for anytime query processing, where the search can be terminated at any time, with search quality improving as processing latency increases. Anytime query processing can be used to effectively reduce high-percentile tail latency that is essential for operational scenarios in which a service level agreement (SLA) imposes response time requirements. In this work, we show how document-ordered indexes can be organized such that they can be queried in an anytime fashion, enabling strict latency control with effective early termination. Our experiments show that processing document-ordered topical segments selected by a simple score estimator outperforms existing anytime algorithms, and allows query runtimes to be accurately limited to comply with SLA requirements. © 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM.",dynamic pruning; query processing; Tail latency; web search,Information retrieval; Search engines; Document collection; Document identifiers; Dynamic pruning; Index update; Inverted indices; Query types; Servicelevel agreement (SLA); Tail latency; Text search; Web searches; Query processing
Contextualized Knowledge-Aware Attentive Neural Network: Enhancing Answer Selection with Knowledge,2022,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122121615&doi=10.1145%2f3457533&partnerID=40&md5=bdc946f899ff3ee822d9fb5c3dc42a65,"Answer selection, which is involved in many natural language processing applications, such as dialog systems and question answering (QA), is an important yet challenging task in practice, since conventional methods typically suffer from the issues of ignoring diverse real-world background knowledge. In this article, we extensively investigate approaches to enhancing the answer selection model with external knowledge from knowledge graph (KG). First, we present a context-knowledge interaction learning framework, Knowledge-Aware Neural Network, which learns the QA sentence representations by considering a tight interaction with the external knowledge from KG and the textual information. Then, we develop two kinds of knowledge-Aware attention mechanism to summarize both the context-based and knowledge-based interactions between questions and answers. To handle the diversity and complexity of KG information, we further propose a Contextualized Knowledge-Aware Attentive Neural Network, which improves the knowledge representation learning with structure information via a customized Graph Convolutional Network and comprehensively learns context-based and knowledge-based sentence representation via the multi-view knowledge-Aware attention mechanism. We evaluate our method on four widely used benchmark QA datasets, including WikiQA, TREC QA, InsuranceQA, and Yahoo QA. Results verify the benefits of incorporating external knowledge from KG and show the robust superiority and extensive applicability of our method. © 2021 Association for Computing Machinery.",Answer selection; attention mechanism; graph convolutional network; knowledge graph,Convolution; Natural language processing systems; Answer selection; Attention mechanisms; Contextualized knowledge; Convolutional networks; External knowledge; Graph convolutional network; Knowledge graphs; Learn+; Neural-networks; Question Answering; Knowledge graph
Topic Difficulty: Collection and Query Formulation Effects,2022,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115868363&doi=10.1145%2f3470563&partnerID=40&md5=f5994a82fc089b49d2a0a667ee4809bb,"Several recent studies have explored the interaction effects between topics, systems, corpora, and components when measuring retrieval effectiveness. However, all of these previous studies assume that a topic or information need is represented by a single query. In reality, users routinely reformulate queries to satisfy an information need. In recent years, there has been renewed interest in the notion of ""query variations""which are essentially multiple user formulations for an information need. Like many retrieval models, some queries are highly effective while others are not. This is often an artifact of the collection being searched which might be more or less sensitive to word choice. Users rarely have perfect knowledge about the underlying collection, and so finding queries that work is often a trial-And-error process. In this work, we explore the fundamental problem of system interaction effects between collections, ranking models, and queries. To answer this important question, we formalize the analysis using ANalysis Of VAriance (ANOVA) models to measure multiple components effects across collections and topics by nesting multiple query variations within each topic. Our findings show that query formulations have a comparable effect size of the topic factor itself, which is known to be the factor with the greatest effect size in prior ANOVA studies. Both topic and formulation have a substantially larger effect size than any other factor, including the ranking algorithms and, surprisingly, even query expansion. This finding reinforces the importance of further research in understanding the role of query rewriting in IR related tasks. © 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM.",effect size; query formulation; retrieval effectiveness; Topic difficulty,Query processing; Effect size; Interaction effect; Multiple user; Query formulation; Retrieval effectiveness; Retrieval models; System interactions; Topic difficulty; Trial-and-error process; Word choices; Analysis of variance (ANOVA)
A Graph Theoretic Approach for Multi-Objective Budget Constrained Capsule Wardrobe Recommendation,2022,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121923812&doi=10.1145%2f3457182&partnerID=40&md5=176a897e095a02da883409933fac6335,"Traditionally, capsule wardrobes are manually designed by expert fashionistas through their creativity and technical prowess. The goal is to curate minimal fashion items that can be assembled into several compatible and versatile outfits. It is usually a cost and time intensive process, and hence lacks scalability. Although there are a few approaches that attempt to automate the process, they tend to ignore the price of items or shopping budget. In this article, we formulate this task as a multi-objective budget constrained capsule wardrobe recommendation (MOBCCWR) problem. It is modeled as a bipartite graph having two disjoint vertex sets corresponding to top-wear and bottom-wear items, respectively. An edge represents compatibility between the corresponding item pairs. The objective is to find a 1-neighbor subset of fashion items as a capsule wardrobe that jointly maximize compatibility and versatility scores by considering corresponding user-specified preference weight coefficients and an overall shopping budget as a means of achieving personalization. We study the complexity class of MOBCCWR, show that it is NP-Complete, and propose a greedy algorithm for finding a near-optimal solution in real time. We also analyze the time complexity and approximation bound for our algorithm. Experimental results show the effectiveness of the proposed approach on both real and synthetic datasets. © 2021 Association for Computing Machinery.",bipartite graph; budget; Capsule wardrobe; compatibility; e-commerce; fashion; outfit; recommendation; versatility,Approximation algorithms; Budget control; Wear of materials; Bipartite graphs; Budget; Capsule wardrobe; Compatibility; E- commerces; Fashion; Multi objective; Outfit; Recommendation; Versatility; Graph theory
Multi-interest Diversification for End-To-end Sequential Recommendation,2022,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118829983&doi=10.1145%2f3475768&partnerID=40&md5=d46e0063606889153998aa2043ea21c9,"Sequential recommenders capture dynamic aspects of users' interests by modeling sequential behavior. Previous studies on sequential recommendations mostly aim to identify users' main recent interests to optimize the recommendation accuracy; they often neglect the fact that users display multiple interests over extended periods of time, which could be used to improve the diversity of lists of recommended items. Existing work related to diversified recommendation typically assumes that users' preferences are static and depend on post-processing the candidate list of recommended items. However, those conditions are not suitable when applied to sequential recommendations. We tackle sequential recommendation as a list generation process and propose a unified approach to take accuracy as well as diversity into consideration, called multi-interest, diversified, sequential recommendation. Particularly, an implicit interest mining module is first used to mine users' multiple interests, which are reflected in users' sequential behavior. Then an interest-Aware, diversity promoting decoder is designed to produce recommendations that cover those interests. For training, we introduce an interest-Aware, diversity promoting loss function that can supervise the model to learn to recommend accurate as well as diversified items. We conduct comprehensive experiments on four public datasets and the results show that our proposal outperforms state-of-The-Art methods regarding diversity while producing comparable or better accuracy for sequential recommendation. © 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM.",diversified recommendation; Sequential recommendation,Diversified recommendation; Dynamic aspects; End to end; Multi interests; Post-processing; Recommendation accuracy; Sequential recommendation; User's preferences; Users' interests; Work-related; User profile
Integrating Collaboration and Leadership in Conversational Group Recommender Systems,2021,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118780161&doi=10.1145%2f3462759&partnerID=40&md5=a6a65f305251794bc2a3f056673cb029,"Recent observational studies highlight the importance of considering the interactions between users in the group recommendation process, but to date their integration has been marginal. In this article, we propose a collaborative model based on the social interactions that take place in a web-based conversational group recommender system. The collaborative model allows the group recommender to implicitly infer the different roles within the group, namely, collaborative and leader user(s). Moreover, it serves as the basis of several novel collaboration-based consensus strategies that integrate both individual and social interactions in the group recommendation process. A live-user evaluation confirms that our approach accurately identifies the collaborative and leader users in a group and produces more effective recommendations.  © 2021 Association for Computing Machinery.",collaboration; Group recommendation; interactions; leadership; live-user evaluation,Collaboration; Collaborative modeling; Group recommendations; Group recommender systems; Interaction; Leadership; Live-user evaluation; Observational study; Social interactions; User evaluations; Recommender systems
Meta-Information in Conversational Search,2021,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118801617&doi=10.1145%2f3468868&partnerID=40&md5=c777a6787f47a13dbd48e5412962c294,"The exchange of meta-information has always formed part of information behavior. In this article, we show that this rule also extends to conversational search. Information about the user's information need, their preferences, and the quality of search results are only some of the most salient examples of meta-information that are exchanged as a matter of course in a search conversation. To understand the importance of meta-information for conversational search, we revisit its definition and survey how meta-information has been taken into account in the past in information retrieval. Meta-information has gone by many names, about which a concise overview is provided. An in-depth analysis of the role of meta-information in search and conversation theories reveals that they provide significant support for the importance of meta-information in conversational search. We further identify conversational search datasets are suitable for a deeper inspection with regard to meta-information, namely, Spoken Conversational Search and Microsoft Information-Seeking Conversations. A quantitative data analysis demonstrates the practical significance of meta-information in information-seeking conversations, whereas a qualitative analysis shows the effects of exchanging different types. Finally, we discuss practical applications and challenges of meta-information in conversational search, including a case study of VERSE, an existing search system for the visually impaired.  © 2021 Association for Computing Machinery.",Conversational search; information retrieval; information seeking; meta-information,Information use; Search engines; Case-studies; Conversational search; Deep inspection; In-depth analysis; Information behaviours; Information seeking; Meta information; MicroSoft; Qualitative analysis; User information need; Information retrieval
Target-guided Emotion-aware Chat Machine,2021,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118806220&doi=10.1145%2f3456414&partnerID=40&md5=e6c6ced181030c381786cb461a5c6b58,"The consistency of a response to a given post at the semantic level and emotional level is essential for a dialogue system to deliver humanlike interactions. However, this challenge is not well addressed in the literature, since most of the approaches neglect the emotional information conveyed by a post while generating responses. This article addresses this problem and proposes a unified end-to-end neural architecture, which is capable of simultaneously encoding the semantics and the emotions in a post and leveraging target information to generate more intelligent responses with appropriately expressed emotions. Extensive experiments on real-world data demonstrate that the proposed method outperforms the state-of-the-art methods in terms of both content coherence and emotion appropriateness.  © 2021 Association for Computing Machinery.",Dialogue generation; emotional chatbot; emotional conversation,Semantics; Chatbots; Dialogue generations; Dialogue systems; Emotional chatbot; Emotional conversation; Emotional information; End to end; Neural architectures; Semantic levels; Target information; Speech processing
MyrrorBot: A Digital Assistant Based on Holistic User Models for Personalized Access to Online Services,2021,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118783296&doi=10.1145%2f3447679&partnerID=40&md5=b625257ca973076f45fd8bd9663600cb,"In this article, we present MyrrorBot, a personal digital assistant implementing a natural language interface that allows the users to: (i) access online services, such as music, video, news, andfood recommendations, in a personalized way, by exploiting a strategy for implicit user modeling called holistic user profiling; (ii) query their own user models, to inspect the features encoded in their profiles and to increase their awareness of the personalization process.Basically, the system allows the users to formulate natural language requests related to their information needs. Such needs are roughly classified in two groups: quantified self-related needs (e.g., Did I sleep enough? Am I extrovert?) and personalized access to online services (e.g., Play a song I like). The intent recognition strategy implemented in the platform automatically identifies the intent expressed by the user and forwards the request to specific services and modules that generate an appropriate answer that fulfills the query.In the experimental evaluation, we evaluated both qualitative (users' acceptance of the system, usability) as well as quantitative (time required to complete basic tasks, effectiveness of the personalization strategy) aspects of the system, and the results showed that MyrrorBot can improve the way people access online services and applications. This leads to a more effective interaction and paves the way for further development of our system.  © 2021 Association for Computing Machinery.",Chatbots; personal digital assistants; personalization; recommender systems; user models,Modeling languages; Natural language processing systems; Petroleum reservoir evaluation; Query processing; Chatbots; Classifieds; Digital assistants; Music video; Natural language interfaces; Natural languages; On-line service; Personalizations; User Modelling; User's profiling; Online systems
Theories of Conversation for Conversational IR,2021,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118771061&doi=10.1145%2f3439869&partnerID=40&md5=c2ffcbf8615c4a5d2262642be67578f0,"Conversational information retrieval is a relatively new and fast-developing research area, but conversation itself has been well studied for decades. Researchers have analysed linguistic phenomena such as structure and semantics but also paralinguistic features such as tone, body language, and even the physiological states of interlocutors. We tend to treat computers as social agents - especially if they have some humanlike features in their design - and so work from human-to-human conversation is highly relevant to how we think about the design of human-to-computer applications. In this article, we summarise some salient past work, focusing on social norms; structures; and affect, prosody, and style. We examine social communication theories briefly as a review to see what we have learned about how humans interact with each other and how that might pertain to agents and robots. We also discuss some implications for research and design of conversational IR systems.  © 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Conversational information retrieval; embodied conversational agent,Information theory; Semantics; Body language; Conversational information retrieval; Embodied conversational agent; Human-to-human conversation; Linguistic phenomena; Paralinguistic; Physiological state; Research areas; Social agents; Social norm; Information retrieval
Dialogue History Matters! Personalized Response Selection in Multi-Turn Retrieval-Based Chatbots,2021,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118797710&doi=10.1145%2f3453183&partnerID=40&md5=02452a13ddcec87edab94892c2e0d4b0,"Existing multi-turn context-response matching methods mainly concentrate on obtaining multi-level and multi-dimension representations and better interactions between context utterances and response. However, in real-place conversation scenarios, whether a response candidate is suitable not only counts on the given dialogue context but also other backgrounds, e.g., wording habits, user-specific dialogue history content. To fill the gap between these up-to-date methods and the real-world applications, we incorporate user-specific dialogue history into the response selection and propose a personalized hybrid matching network (PHMN). Our contributions are two-fold: (1) our model extracts personalized wording behaviors from user-specific dialogue history as extra matching information; (2) we perform hybrid representation learning on context-response utterances and explicitly incorporate a customized attention mechanism to extract vital information from context-response interactions so as to improve the accuracy of matching. We evaluate our model on two large datasets with user identification, i.e., personalized Ubuntu dialogue Corpus (P-Ubuntu) and personalized Weibo dataset (P-Weibo). Experimental results confirm that our method significantly outperforms several strong models by combining personalized attention, wording behaviors, and hybrid representation learning.  © 2021 Association for Computing Machinery.",dialogue history modeling; hybrid representation learning; Open-domain dialogue system; personalized ranking; retrieval-based chatbot; semantic matching,Behavioral research; Large dataset; Learning systems; Linux; Social networking (online); Speech processing; Chatbots; Dialog history modeling; Dialogue systems; Hybrid representation learning; Hybrid representations; Open-domain dialog system; Personalized ranking; Response selection; Retrieval-based chatbot; Semantic matching; Semantics
Why or Why Not? The Effect of Justification Styles on Chatbot Recommendations,2021,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118791458&doi=10.1145%2f3441715&partnerID=40&md5=3fa55daf8109f28328cdfde752f9aba4,"Chatbots or conversational recommenders have gained increasing popularity as a new paradigm for Recommender Systems (RS). Prior work on RS showed that providing explanations can improve transparency and trust, which are critical for the adoption of RS. Their interactive and engaging nature makes conversational recommenders a natural platform to not only provide recommendations but also justify the recommendations through explanations. The recent surge of interest inexplainable AI enables diverse styles of justification, and also invites questions on how styles of justification impact user perception. In this article, we explore the effect of ""why""justifications and ""why not""justifications on users' perceptions of explainability and trust. We developed and tested a movie-recommendation chatbot that provides users with different types of justifications for the recommended items. Our online experiment (n = 310) demonstrates that the ""why""justifications (but not the ""why not""justifications) have a significant impact on users' perception of the conversational recommender. Particularly, ""why""justifications increase users' perception of system transparency, which impacts perceived control, trusting beliefs and in turn influences users' willingness to depend on the system's advice. Finally, we discuss the design implications for decision-assisting chatbots.  © 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM.",chatbots; Conversational agent; explanation; human computer interaction; trust; user interface; user study,Human computer interaction; Recommender systems; Transparency; Chatbots; Conversational agents; Design implications; Explanation; Movie recommendations; On-line experiments; Trust; Trusting beliefs; User perceptions; User study; User interfaces
Multi-Response Awareness for Retrieval-Based Conversations: Respond with Diversity via Dynamic Representation Learning,2021,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118797959&doi=10.1145%2f3470450&partnerID=40&md5=cefb56a707a710773c17ec59ab5095b1,"Conversational systems now attract great attention due to their promising potential and commercial values. To build a conversational system with moderate intelligence is challenging and requires big (conversational) data, as well as interdisciplinary techniques. Thanks to the prosperity of the Web, the massive data available greatly facilitate data-driven methods such as deep learning for human-computer conversational systems. In general, retrieval-based conversational systems apply various matching schema between query utterances and responses, but the classic retrieval paradigm suffers from prominent weakness for conversations: the system finds similar responses given a particular query. For real human-to-human conversations, on the contrary, responses can be greatly different yet all are possibly appropriate. The observation reveals the diversity phenomenon in conversations. In this article, we ascribe the lack of conversational diversity to the reason that the query utterances are statically modeled regardless of candidate responses through traditional methods. To this end, we propose a dynamic representation learning strategy that models the query utterances and different response candidates in an interactive way. To be more specific, we propose a Respond-with-Diversity model augmented by the memory module interacting with both the query utterances and multiple candidate responses. Hence, we obtain dynamic representations for the input queries conditioned on different response candidates. We frame the model as an end-to-end learnable neural network. In the experiments, we demonstrate the effectiveness of the proposed model by achieving a good appropriateness score and much better diversity in retrieval-based conversations between humans and computers.  © 2021 Association for Computing Machinery.",Conversational system; dynamic memories; respond with diversity,Information retrieval; Conversational systems; Data-driven methods; Dynamic memory; Dynamic representation; Human-to-human conversation; Learning strategy; Massive data; Matchings; Multiresponse; Respond with diversity; Deep learning
A Large-scale Analysis of Mixed Initiative in Information-Seeking Dialogues for Conversational Search,2021,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118779770&doi=10.1145%2f3466796&partnerID=40&md5=c0ad4fecad2a9f6770389d94536c2bc4,"Conversational search is a relatively young area of research that aims at automating an information-seeking dialogue. In this article, we help to position it with respect to other research areas within conversational artificial intelligence (AI) by analysing the structural properties of an information-seeking dialogue. To this end, we perform a large-scale dialogue analysis of more than 150K transcripts from 16 publicly available dialogue datasets. These datasets were collected to inform different dialogue-based tasks including conversational search. We extract different patterns of mixed initiative from these dialogue transcripts and use them to compare dialogues of different types. Moreover, we contrast the patterns found in information-seeking dialogues that are being used for research purposes with the patterns found in virtual reference interviews that were conducted by professional librarians. The insights we provide (1) establish close relations between conversational search and other conversational AI tasks and (2) uncover limitations of existing conversational datasets to inform future data collection tasks.  © 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM.",conversational search; Information-seeking dialogue; mixed initiative,Information retrieval; Information use; Conversational search; Data collection; Dialogue analysis; Information-seeking dialogues; Large-scale analysis; Large-scales; Mixed-initiative; Research areas; Research purpose; Virtual reference; Large dataset
Meta-evaluation of Conversational Search Evaluation Metrics,2021,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118773398&doi=10.1145%2f3445029&partnerID=40&md5=85ea22f50597e57c5eccc1564e2b63fa,"Conversational search systems, such as Google assistant and Microsoft Cortana, enable users to interact with search systems in multiple rounds through natural language dialogues. Evaluating such systems is very challenging, given that any natural language responses could be generated, and users commonly interact for multiple semantically coherent rounds to accomplish a search task. Although prior studies proposed many evaluation metrics, the extent of how those measures effectively capture user preference remain to be investigated. In this article, we systematically meta-evaluate a variety of conversational search metrics. We specifically study three perspectives on those metrics: (1) reliability: the ability to detect ""actual""performance differences as opposed to those observed by chance; (2) fidelity: the ability to agree with ultimate user preference; and (3) intuitiveness: the ability to capture any property deemed important: adequacy, informativeness, and fluency in the context of conversational search. By conducting experiments on two test collections, we find that the performance of different metrics vary significantly across different scenarios, whereas consistent with prior studies, existing metrics only achieve weak correlation with ultimate user preference and satisfaction. METEOR is, comparatively speaking, the best existing single-turn metric considering all three perspectives. We also demonstrate that adapted session-based evaluation metrics can be used to measure multi-turn conversational search, achieving moderate concordance with user satisfaction. To our knowledge, our work establishes the most comprehensive meta-evaluation for conversational search to date.  © 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Conversational search; discriminative power; meta-evaluation; metric,Conversational search; Discriminative power; Evaluation metrics; Google+; Meta-evaluation; Metric; Performance; Search system; User's preferences; Users' satisfactions; Safety devices
Response Ranking with Multi-types of Deep Interactive Representations in Retrieval-based Dialogues,2021,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118797132&doi=10.1145%2f3462207&partnerID=40&md5=a468e9527b33780917691d6c7924622c,"Building an intelligent dialogue system with the ability to select a proper response according to a multi-turn context is challenging in three aspects: (1) the meaning of a context-response pair is built upon language units from multiple granularities (e.g., words, phrases, and sub-sentences, etc.); (2) local (e.g., a small window around a word) and long-range (e.g., words across the context and the response) dependencies may exist in dialogue data; and (3) the relationship between the context and the response candidate lies in multiple relevant semantic clues or relatively implicit semantic clues in some real cases. However, existing approaches usually encode the dialogue with mono-type representation and the interaction processes between the context and the response candidate are executed in a rather shallow manner, which may lead to an inadequate understanding of dialogue content and hinder the recognition of the semantic relevance between the context and response. To tackle these challenges, we propose a representation[K]-interaction[L]-matching framework that explores multiple types of deep interactive representations to build context-response matching models for response selection. Particularly, we construct different types of representations for utterance-response pairs and deepen them via alternate encoding and interaction. By this means, the model can handle the relation of neighboring elements, phrasal pattern, and long-range dependencies during the representation and make a more accurate prediction through multiple layers of interactions between the context-response pair. Experiment results on three public benchmarks indicate that the proposed model significantly outperforms previous conventional context-response matching models and achieve slightly better results than the BERT model for multi-turn response selection in retrieval-based dialogue systems.  © 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM.",context-response matching; response selection; Retrieval-based dialogue systems,Encoding (symbols); Semantics; Context-response matching; Dialogue systems; Implicit semantics; Intelligent dialogue systems; Matching models; Matchings; Multi-turn; Real case; Response selection; Retrieval-based dialog system; Speech processing
Conversational Search and Recommendation: Introduction to the Special Issue,2021,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117847604&doi=10.1145%2f3465272&partnerID=40&md5=b7d07b8e872e5f8ab3a8fb7dbba9f9ef,"An introduction to the special issue on conversational search and recommendation is presented in this article. While conversational search and recommendation has roots in early Information Retrieval (IR) research, the recent advances in automatic voice recognition and conversational agents have created increasing interest in this area. In recent years, the IR and related communities have witnessed a number of major contributions to the field of conversational search and recommendation. They include but are not limited to conversational search conceptualization. The growing body of work in this area has been supplemented by an increasing number of recent seminars.",conversational question answering; conversational recommendation; Conversational search; interactive information retrieval;,Speech recognition; Conversational agents; Conversational question answering; Conversational recommendations; Conversational search; Information retrieval research; Interactive information retrieval; Interactive information retrieval;; Question Answering; Information retrieval
From users' intentions to IF-THEN Rules in the internet of things,2021,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118795857&doi=10.1145%2f3447264&partnerID=40&md5=b9ae6cc029724e65f4087bd4ce90748d,"In the Internet of Things era, users are willing to personalize the joint behavior of their connected entities, i.e., smart devices and online service, by means of trigger-action rules such as ""IF the entrance Nest security camera detects a movement, THEN blink the Philips Hue lamp in the kitchen.""Unfortunately, the spread of new supported technologies makes the number of possible combinations between triggers and actions continuously growing, thus motivating the need of assisting users in discovering new rules and functionality, e.g., through recommendation techniques. To this end, we present , a semantic Conversational Search and Recommendation (CSR) system able to suggest pertinent IF-THEN rules that can be easily deployed in different contexts starting from an abstract user's need. By exploiting a conversational agent, the user can communicate her current personalization intention by specifying a set of functionality at a high level, e.g., to decrease the temperature of a room when she left it. Stemming from this input, implements a semantic recommendation process that takes into account (a) the current user's intention, (b) the connected entities owned by the user, and (c) the user's long-term preferences revealed by her profile. If not satisfied with the suggestions, then the user can converse with the system to provide further feedback, i.e., a short-term preference, thus allowing to provide refined recommendations that better align with the original intention. We evaluate by running different offline experiments with simulated users and real-world data. First, we test the recommendation process in different configurations, and we show that recommendation accuracy and similarity with target items increase as the interaction between the algorithm and the user proceeds. Then, we compare with other similar baseline recommender systems. Results are promising and demonstrate the effectiveness of in recommending IF-THEN rules that satisfy the current personalization intention of the user.  © 2021 Association for Computing Machinery.",abstraction; conversational recommender system; functionality; Internet of Things; semantic web; Trigger-action programming,Recommender systems; Semantic Web; 'current; Abstraction; Conversational recommender systems; Functionality; Joint behavior; Personalizations; Semantic-Web; Smart devices; Trigger-action programming; User's intentions; Internet of things
Seamlessly Unifying Attributes and Items: Conversational Recommendation for Cold-start Users,2021,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108536399&doi=10.1145%2f3446427&partnerID=40&md5=ab25ac8f5f6acc3020ca0c42f16a48d3,"Static recommendation methods like collaborative filtering suffer from the inherent limitation of performing real-time personalization for cold-start users. Online recommendation, e.g., multi-armed bandit approach, addresses this limitation by interactively exploring user preference online and pursuing the exploration-exploitation (EE) trade-off. However, existing bandit-based methods model recommendation actions homogeneously. Specifically, they only consider the items as the arms, being incapable of handling the item attributes, which naturally provide interpretable information of user's current demands and can effectively filter out undesired items. In this work, we consider the conversational recommendation for cold-start users, where a system can both ask the attributes from and recommend items to a user interactively. This important scenario was studied in a recent work [54]. However, it employs a hand-crafted function to decide when to ask attributes or make recommendations. Such separate modeling of attributes and items makes the effectiveness of the system highly rely on the choice of the hand-crafted function, thus introducing fragility to the system. To address this limitation, we seamlessly unify attributes and items in the same arm space and achieve their EE trade-offs automatically using the framework of Thompson Sampling. Our Conversational Thompson Sampling (ConTS) model holistically solves all questions in conversational recommendation by choosing the arm with the maximal reward to play. Extensive experiments on three benchmark datasets show that ConTS outperforms the state-of-the-art methods Conversational UCB (ConUCB) [54] and Estimation - Action - Reflection model [27] in both metrics of success rate and average number of conversation turns.  © 2021 Association for Computing Machinery.",Conversational recommendation; dialogue system; interactive recommendation; recommender system,Collaborative filtering; Economic and social effects; Speech processing; Cold-start; Conversational recommendations; Dialogue systems; Exploration/exploitation; Inherent limitations; Interactive recommendation; Real- time; Recommendation methods; Thompson samplings; Trade off; Recommender systems
Multi-Stage Conversational Passage Retrieval: An Approach to Fusing Term Importance Estimation and Neural Query Rewriting,2021,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106192840&doi=10.1145%2f3446426&partnerID=40&md5=e73eedbe295d5478ad6119f8d6741457,"Conversational search plays a vital role in conversational information seeking. As queries in information seeking dialogues are ambiguous for traditional ad hoc information retrieval (IR) systems due to the coreference and omission resolution problems inherent in natural language dialogue, resolving these ambiguities is crucial. In this article, we tackle conversational passage retrieval, an important component of conversational search, by addressing query ambiguities with query reformulation integrated into a multi-stage ad hoc IR system. Specifically, we propose two conversational query reformulation (CQR) methods: (1) term importance estimation and (2) neural query rewriting. For the former, we expand conversational queries using important terms extracted from the conversational context with frequency-based signals. For the latter, we reformulate conversational queries into natural, stand-alone, human-understandable queries with a pretrained sequence-to-sequence model. Detailed analyses of the two CQR methods are provided quantitatively and qualitatively, explaining their advantages, disadvantages, and distinct behaviors. Moreover, to leverage the strengths of both CQR methods, we propose combining their output with reciprocal rank fusion, yielding state-of-the-art retrieval effectiveness, 30% improvement in terms of NDCG@3 compared to the best submission of Text REtrieval Conference (TREC) Conversational Assistant Track (CAsT) 2019.  © 2021 Association for Computing Machinery.",Conversational search; multi-stage retrieval; query variations,Information retrieval; Information use; Natural language processing systems; Conversational search; Information seeking; Information-retrieval systems; Multi-stage retrieval; Multi-stages; Passage retrieval; Query reformulation; Query rewritings; Query variation; Term importance; Search engines
Conversations with Search Engines: SERP-based Conversational Response Generation,2021,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111232645&doi=10.1145%2f3432726&partnerID=40&md5=145ee10498c61ffd1838d9c494dc742b,"In this article, we address the problem of answering complex information needs by conducting conversations with search engines, in the sense that users can express their queries in natural language and directly receive the information they need from a short system response in a conversational manner. Recently, there have been some attempts towards a similar goal, e.g., studies on Conversational Agents (CAs) and Conversational Search (CS). However, they either do not address complex information needs in search scenarios or they are limited to the development of conceptual frameworks and/or laboratory-based user studies.We pursue two goals in this article: (1)the creation of a suitable dataset, the Search as a Conversation (SaaC) dataset, for the development of pipelines for conversations with search engines, and(2)the development of a state-of-the-art pipeline for conversations with search engines, Conversations with Search Engines (CaSE), using this dataset. SaaC is built based on a multi-turn conversational search dataset, where we further employ workers from a crowdsourcing platform to summarize each relevant passage into a short, conversational response. CaSE enhances the state-of-the-art by introducing a supporting token identification module and a prior-aware pointer generator, which enables us to generate more accurate responses.We carry out experiments to show that CaSE is able to outperform strong baselines. We also conduct extensive analyses on the SaaC dataset to show where there is room for further improvement beyond CaSE. Finally, we release the SaaC dataset and the code for CaSE and all models used for comparison to facilitate future research on this topic.  © 2021 Association for Computing Machinery.",Conversational modeling; dataset; neural model; search engine,Pipelines; Complex information; Conceptual frameworks; Conversational agents; Conversational model; Dataset; Natural languages; Neural modelling; Response generation; State of the art; System response; Search engines
How Am I Doing?: Evaluating Conversational Search Systems Offline,2021,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115407070&doi=10.1145%2f3451160&partnerID=40&md5=3ab552adeec079a8f0f6e6fcd4234c57,"As conversational agents like Siri and Alexa gain in popularity and use, conversation is becoming a more and more important mode of interaction for search. Conversational search shares some features with traditional search, but differs in some important respects: conversational search systems are less likely to return ranked lists of results (a SERP), more likely to involve iterated interactions, and more likely to feature longer, well-formed user queries in the form of natural language questions. Because of these differences, traditional methods for search evaluation (such as the Cranfield paradigm) do not translate easily to conversational search. In this work, we propose a framework for offline evaluation of conversational search, which includes a methodology for creating test collections with relevance judgments, an evaluation measure based on a user interaction model, and an approach to collecting user interaction data to train the model. The framework is based on the idea of ""subtopics"", often used to model novelty and diversity in search and recommendation, and the user model is similar to the geometric browsing model introduced by RBP and used in ERR. As far as we know, this is the first work to combine these ideas into a comprehensive framework for offline evaluation of conversational search.  © 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM.",conversational search; evaluation; Information retrieval; test collections,Information retrieval; Search engines; Conversational agents; Conversational search; Cranfield; Evaluation; Natural language questions; Offline; Offline evaluation; Search system; Test Collection; User query; Safety devices
Editorial Message from the New Editor-in-Chief,2021,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119157057&doi=10.1145%2f3447945&partnerID=40&md5=c2904201b68dcfa4cb475c789a1eb156,[No abstract available],,
Context-aware Target Apps Selection and Recommendation for Enhancing Personal Mobile Assistants,2021,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117047033&doi=10.1145%2f3447678&partnerID=40&md5=8b1509de786c2ee6125916418b2c990c,"Users install many apps on their smartphones, raising issues related to information overload for users and resource management for devices. Moreover, the recent increase in the use of personal assistants has made mobile devices even more pervasive in users' lives. This article addresses two research problems that are vital for developing effective personal mobile assistants: target apps selection and recommendation. The former is the key component of a unified mobile search system: a system that addresses the users' information needs for all the apps installed on their devices with a unified mode of access. The latter, instead, predicts the next apps that the users would want to launch. Here we focus on context-aware models to leverage the rich contextual information available to mobile devices. We design an in situ study to collect thousands of mobile queries enriched with mobile sensor data (now publicly available for research purposes). With the aid of this dataset, we study the user behavior in the context of these tasks and propose a family of context-aware neural models that take into account the sequential, temporal, and personal behavior of users. We study several state-of-the-art models and show that the proposed models significantly outperform the baselines. © 2021 Association for Computing Machinery.",app recommendation; Mobile information retrieval; mobile usage understanding; neural networks; query analysis; user behavior analysis,Behavioral research; Information management; Information retrieval; Mobile telecommunication systems; App recommendation; Context-Aware; Mobile information; Mobile information retrieval; Mobile usage; Mobile usage understanding; Neural-networks; Query analysis; Smart phones; User behavior analysis; Search engines
Reinforcement Learning based Collective Entity Alignment with Adaptive Features,2021,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101471633&doi=10.1145%2f3446428&partnerID=40&md5=c01f05c8a3540f984b42196e1dbd0727,"Entity alignment (EA) is the task of identifying the entities that refer to the same real-world object but are located in different knowledge graphs (KGs). For entities to be aligned, existing EA solutions treat them separately and generate alignment results as ranked lists of entities on the other side. Nevertheless, this decision-making paradigm fails to take into account the interdependence among entities. Although some recent efforts mitigate this issue by imposing the 1-to-1 constraint on the alignment process, they still cannot adequately model the underlying interdependence and the results tend to be sub-optimal.To fill in this gap, in this work, we delve into the dynamics of the decision-making process, and offer a reinforcement learning (RL)-based model to align entities collectively. Under the RL framework, we devise the coherence and exclusiveness constraints to characterize the interdependence and restrict collective alignment. Additionally, to generate more precise inputs to the RL framework, we employ representative features to capture different aspects of the similarity between entities in heterogeneous KGs, which are integrated by an adaptive feature fusion strategy. Our proposal is evaluated on both cross-lingual and mono-lingual EA benchmarks and compared against state-of-the-art solutions. The empirical results verify its effectiveness and superiority. © 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM.",adaptive feature fusion; Entity alignment; reinforcement learning,Alignment; Decision making; Knowledge graph; Adaptive feature fusion; Adaptive features; Decision-making process; Decisions makings; Entity alignment; Features fusions; Heterogeneous Knowledge; Knowledge graphs; Real-world objects; Reinforcement learnings; Reinforcement learning
Exploiting Real-time Search Engine Queries for Earthquake Detection: A Summary of Results,2021,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119092058&doi=10.1145%2f3453842&partnerID=40&md5=4bc0e02f89e0da074e655938260297d0,"Online search engine has been widely regarded as the most convenient approach for information acquisition. Indeed, the intensive information-seeking behaviors of search engine users make it possible to exploit search engine queries as effective ""crowd sensors""for event monitoring. While some researchers have investigated the feasibility of using search engine queries for coarse-grained event analysis, the capability of search engine queries for real-time event detection has been largely neglected. To this end, in this article, we introduce a large-scale and systematic study on exploiting real-time search engine queries for outbreak event detection, with a focus on earthquake rapid reporting. In particular, we propose a realistic system of real-time earthquake detection through monitoring millions of queries related to earthquakes from a dominant online search engine in China. Specifically, we first investigate a large set of queries for selecting the representative queries that are highly correlated with the outbreak of earthquakes. Then, based on the real-time streams of selected queries, we design a novel machine learning-enhanced two-stage burst detection approach for detecting earthquake events. Meanwhile, the location of an earthquake epicenter can be accurately estimated based on the spatial-temporal distribution of search engine queries. Finally, through the extensive comparison with earthquake catalogs from China Earthquake Networks Center, 2015, the detection precision of our system can achieve 87.9%, and the accuracy of location estimation (province level) is 95.7%. In particular, 50% of successfully detected results can be found within 62 s after earthquake, and 50% of successful locations can be found within 25.5 km of seismic epicenter. Our system also found more than 23.3% extra earthquakes that were felt by people but not publicly released, 12.1% earthquake-like special outbreaks, and meanwhile, revealed many interesting findings, such as the typical query patterns of earthquake rumor and regular memorial events. Based on these results, our system can timely feed back information to the search engine users according to various cases and accelerate the information release of felt earthquakes. © 2021 Association for Computing Machinery.",crowd sensors; earthquake rapid reporting; real-time event detection; Search engine queries,Behavioral research; Earthquakes; Felt; Information retrieval; Location; Crowd sensor; Earthquake detection; Earthquake rapid reporting; Events detection; Information acquisitions; Real- time; Real-time event detection; Real-time search engines; Search engine query; Time event; Search engines
Inductive Contextual Relation Learning for Personalization,2021,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119096804&doi=10.1145%2f3450353&partnerID=40&md5=f80a331b095400addc9dc54f44ccfb7c,"Web personalization, e.g., recommendation or relevance search, tailoring a service/product to accommodate specific online users, is becoming increasingly important. Inductive personalization aims to infer the relations between existing entities and unseen new ones, e.g., searching relevant authors for new papers or recommending new items to users. This problem, however, is challenging since most of recent studies focus on transductive problem for existing entities. In addition, despite some inductive learning approaches have been introduced recently, their performance is sub-optimal due to relatively simple and inflexible architectures for aggregating entity's content. To this end, we propose the inductive contextual personalization (ICP) framework through contextual relation learning. Specifically, we first formulate the pairwise relations between entities with a ranking optimization scheme that employs neural aggregator to fuse entity's heterogeneous contents. Next, we introduce a node embedding term to capture entity's contextual relations, as a smoothness constraint over the prior ranking objective. Finally, the gradient descent procedure with adaptive negative sampling is employed to learn the model parameters. The learned model is capable of inferring the relations between existing entities and inductive ones. Thorough experiments demonstrate that ICP outperforms numerous baseline methods for two different applications, i.e., relevant author search and new item recommendation. © 2021 Association for Computing Machinery.",content-based recommendation; node embedding; Personalization; relation learning,Gradient methods; Information retrieval; Knowledge management; Optimization; World Wide Web; Content-based recommendation; Embeddings; Node embedding; Online users; Performance; Personalizations; Relation learning; Service products; Simple++; Web personalization; Embeddings
Assessing Top-k Preferences,2021,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111638876&doi=10.1145%2f3451161&partnerID=40&md5=123a3ce87c1b53f619b83e7267f1aa8d,"Assessors make preference judgments faster and more consistently than graded judgments. Preference judgments can also recognize distinctions between items that appear equivalent under graded judgments. Unfortunately, preference judgments can require more than linear effort to fully order a pool of items, and evaluation measures for preference judgments are not as well established as those for graded judgments, such as NDCG. In this article, we explore the assessment process for partial preference judgments, with the aim of identifying and ordering the top items in the pool, rather than fully ordering the entire pool. To measure the performance of a ranker, we compare its output to this preferred ordering by applying a rank similarity measure. We demonstrate the practical feasibility of this approach by crowdsourcing partial preferences for the TREC 2019 Conversational Assistance Track, replacing NDCG with a new measure named compatibility. This new measure has its most striking impact when comparing modern neural rankers, where it is able to recognize significant improvements in quality that would otherwise be missed by NDCG. © 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Preference judgments; question answering,Petroleum reservoir evaluation; Assessment process; Evaluation measures; Partial preferences; Performance; Preference judgment; Question Answering; Rank similarities; Similarity measure; Lakes
CoSam: An Efficient Collaborative Adaptive Sampler for Recommendation,2021,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117854455&doi=10.1145%2f3450289&partnerID=40&md5=5683d115afac4f9c7332975c0504b6a3,"Sampling strategies have been widely applied in many recommendation systems to accelerate model learning from implicit feedback data. A typical strategy is to draw negative instances with uniform distribution, which, however, will severely affect a model's convergence, stability, and even recommendation accuracy. A promising solution for this problem is to over-sample the ""difficult""(a.k.a. informative) instances that contribute more on training. But this will increase the risk of biasing the model and leading to non-optimal results. Moreover, existing samplers are either heuristic, which require domain knowledge and often fail to capture real ""difficult""instances, or rely on a sampler model that suffers from low efficiency. To deal with these problems, we propose CoSam, an efficient and effective collaborative sampling method that consists of (1) a collaborative sampler model that explicitly leverages user-item interaction information in sampling probability and exhibits good properties of normalization, adaption, interaction information awareness, and sampling efficiency, and (2) an integrated sampler-recommender framework, leveraging the sampler model in prediction to offset the bias caused by uneven sampling. Correspondingly, we derive a fast reinforced training algorithm of our framework to boost the sampler performance and sampler-recommender collaboration. Extensive experiments on four real-world datasets demonstrate the superiority of the proposed collaborative sampler model and integrated sampler-recommender framework. © 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM.",adaption; efficiency; recommendation; Sampling,Domain Knowledge; Recommender systems; Adaption; Implicit feedback; Interaction information; Model convergence; Model learning; Negative instances; Recommendation; Recommendation accuracy; Sampling strategies; Uniform distribution; Efficiency
A Hybrid Framework for Session Context Modeling,2021,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119095623&doi=10.1145%2f3448127&partnerID=40&md5=3b45c4aa15f7584cb60c48ca90531ffa,"Understanding user intent is essential for various retrieval tasks. By leveraging contextual information within sessions, e.g., query history and user click behaviors, search systems can capture user intent more accurately and thus perform better. However, most existing systems only consider intra-session contexts and may suffer from the problem of lacking contextual information, because short search sessions account for a large proportion in practical scenarios. We believe that in these scenarios, considering more contexts, e.g., cross-session dependencies, may help alleviate the problem and contribute to better performance. Therefore, we propose a novel Hybrid framework for Session Context Modeling (HSCM), which realizes session-level multi-task learning based on the self-attention mechanism. To alleviate the problem of lacking contextual information within current sessions, HSCM exploits the cross-session contexts by sampling user interactions under similar search intents in the historical sessions and further aggregating them into the local contexts. Besides, application of the self-attention mechanism rather than RNN-based frameworks in modeling session-level sequences also helps (1) better capture interactions within sessions, (2) represent the session contexts in parallelization. Experimental results on two practical search datasets show that HSCM not only outperforms strong baseline solutions such as HiNT, CARS, and BERTserini in document ranking, but also performs significantly better than most existing query suggestion methods. According to the results in an additional experiment, we have also found that HSCM is superior to most ranking models in click prediction. © 2021 Association for Computing Machinery.",Document ranking; query suggestion; self-attention mechanism,Behavioral research; Information retrieval; Learning systems; Attention mechanisms; Context models; Contextual information; Document ranking; Existing systems; Hybrid framework; Query suggestion; Search system; Self-attention mechanism; Session level; Search engines
Modeling Multiple Coexisting Category-Level Intentions for Next Item Recommendation,2021,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119099944&doi=10.1145%2f3441642&partnerID=40&md5=41a8377c630c00b6d31d705c0c39efe5,"Purchase intentions have a great impact on future purchases and thus can be exploited for making recommendations. However, purchase intentions are typically complex and may change from time to time. Through empirical study with two e-commerce datasets, we observe that behaviors of multiple types can indicate user intentions and a user may have multiple coexisting category-level intentions that evolve over time. In this article, we propose a novel Intention-Aware Recommender System (IARS) which consists of four components for mining such complex intentions from user behaviors of multiple types. In the first component, we utilize several Recurrent Neural Networks (RNNs) and an attention layer to model diverse user intentions simultaneously and design two kinds of Multi-behavior GRU (MGRU) cells to deal with heterogeneous behaviors. To reveal user intentions, we carefully design three tasks that share representations from MGRUs. The next-item recommendation is the main task and leverages attention to select user intentions according to candidate items. The remaining two (item prediction and sequence comparison) are auxiliary tasks and can reveal user intentions. Extensive experiments on the two real-world datasets demonstrate the effectiveness of our models compared with several state-of-the-art recommendation methods in terms of hit ratio and NDCG. © 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM.",recommender system; Recurrent neural networks,Behavioral research; Complex networks; Electronic commerce; Multilayer neural networks; Purchasing; Recurrent neural networks; Sales; E- commerces; Empirical studies; Main tasks; Purchase intention; Real-world datasets; Recommendation methods; Sequence comparisons; State of the art; User behaviors; User's intentions; Recommender systems
Popularity Bias in False-positive Metrics for Recommender Systems Evaluation,2021,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119101717&doi=10.1145%2f3452740&partnerID=40&md5=ae3bd2b2a8de87fad86be650341b282c,"We investigate the impact of popularity bias in false-positive metrics in the offline evaluation of recommender systems. Unlike their true-positive complements, false-positive metrics reward systems that minimize recommendations disliked by users. Our analysis is, to the best of our knowledge, the first to show that false-positive metrics tend to penalise popular items, the opposite behavior of true-positive metrics - causing a disagreement trend between both types of metrics in the presence of popularity biases. We present a theoretical analysis of the metrics that identifies the reason that the metrics disagree and determines rare situations where the metrics might agree - the key to the situation lies in the relationship between popularity and relevance distributions, in terms of their agreement and steepness - two fundamental concepts we formalize. We then examine three well-known datasets using multiple popular true- and false-positive metrics on 16 recommendation algorithms. Specific datasets are chosen to allow us to estimate both biased and unbiased metric values. The results of the empirical study confirm and illustrate our analytical findings. With the conditions of the disagreement of the two types of metrics established, we then determine under which circumstances true-positive or false-positive metrics should be used by researchers of offline evaluation in recommender systems.1 © 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM.",evaluation; false positives; metric; non-random missing data; popularity bias; Recommender systems,Evaluation; False positive; Metric; Non-random missing data; Offline evaluation; Popularity bias; Random missing data; Reward systems; System evaluation; True positive; Recommender systems
DGeye: Probabilistic Risk Perception and Prediction for Urban Dangerous Goods Management,2021,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119094408&doi=10.1145%2f3448256&partnerID=40&md5=401492f01f6f542f6e059b6845c3a2d0,"Recent years have witnessed the emergence of worldwide megalopolises and the accompanying public safety events, making urban safety a top priority in modern urban management. Among various threats, dangerous goods such as gas and hazardous chemicals transported through cities have bred repeated tragedies and become the deadly ""bomb""we sleep with every day. While tremendous research efforts have been devoted to dealing with dangerous goods transportation (DGT) issues, further study is still in great need to quantify this problem and explore its intrinsic dynamics from a big data perspective. In this article, we present a novel system called DGeye, to feature a fusion between DGT trajectory data and residential population data for dangers perception and prediction. Specifically, DGeye first develops a probabilistic graphical model-based approach to mine spatio-temporally adjacent risk patterns from population-aware risk trajectories. Then, DGeye builds the novel causality network among risk patterns for risk pain-point identification, risk source attribution, and online risky state prediction. Experiments on both Beijing and Tianjin cities demonstrate the effectiveness of DGeye in real-life DGT risk management. As a case in point, our report powered by DGeye successfully drove the government to lay down gas pipelines for the famous Guijie food street in Beijing. © 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM.",dangerous goods transportation; risk causal network; risk management; risk pattern; Urban safety,Forecasting; Freight transportation; Population statistics; Risk perception; Safety engineering; Urban transportation; Causal network; Dangerous goods; Dangerous goods transportations; Probabilistic risk; Public safety; Risk causal network; Risk pattern; Risks management; Urban management; Urban safety; Risk management
RLPS: A Reinforcement Learning Based Framework for Personalized Search,2021,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119092654&doi=10.1145%2f3446617&partnerID=40&md5=f10465f7f1cdbf2de3a3ae2c96fd2e1f,"Personalized search is a promising way to improve search qualities by taking user interests into consideration. Recently, machine learning and deep learning techniques have been successfully applied to search result personalization. Most existing models simply regard the personal search history as a static set of user behaviors and learn fixed ranking strategies based on all the recorded data. Though improvements have been achieved, the essence that the search process is a sequence of interactions between the search engine and user is ignored. The user's interests may dynamically change during the search process, therefore, it would be more helpful if a personalized search model could track the whole interaction process and adjust its ranking strategy continuously. In this article, we adapt reinforcement learning to personalized search and propose a framework, referred to as RLPS. It utilizes a Markov Decision Process (MDP) to track sequential interactions between the user and search engine, and continuously update the underlying personalized ranking model with the user's real-time feedback to learn the user's dynamic interests. Within this framework, we implement two models: the listwise RLPS-L and the hierarchical RLPS-H. RLPS-L interacts with users and trains the ranking model with document lists, while RLPS-H improves model training by designing a layered structure and introducing document pairs. In addition, we also design a feedback-aware personalized ranking component to capture the user's feedback, which impacts the user interest profile for the next query. Significant improvements over existing personalized search models are observed in the experiments on the public AOL search log and a commercial log. © 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Markov decision process (MDP); Personalized search; reinforcement learning,Behavioral research; Deep learning; Information retrieval; Markov processes; Reinforcement learning; User profile; Learn+; Markov decision process; Markov Decision Processes; Personalized search; Ranking model; Ranking strategy; Reinforcement learnings; Search models; Search process; Users' interests; Search engines
HGAT: Heterogeneous Graph Attention Networks for Semi-supervised Short Text Classification,2021,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112840639&doi=10.1145%2f3450352&partnerID=40&md5=7e4149fd870456fd46e60e4d9925f6ae,"Short text classification has been widely explored in news tagging to provide more efficient search strategies and more effective search results for information retrieval. However, most existing studies, concentrating on long text classification, deliver unsatisfactory performance on short texts due to the sparsity issue and the insufficiency of labeled data. In this article, we propose a novel heterogeneous graph neural network-based method for semi-supervised short text classification, leveraging full advantage of limited labeled data and large unlabeled data through information propagation along the graph. Specifically, we first present a flexible heterogeneous information network (HIN) framework for modeling short texts, which can integrate any type of additional information and meanwhile capture their relations to address the semantic sparsity. Then, we propose Heterogeneous Graph Attention networks (HGAT) to embed the HIN for short text classification based on a dual-level attention mechanism, including node-level and type-level attentions. To efficiently classify new coming texts that do not previously exist in the HIN, we extend our model HGAT for inductive learning, avoiding re-training the model on the evolving HIN. Extensive experiments on single-/multi-label classification demonstrates that our proposed model HGAT significantly outperforms state-of-the-art methods across the benchmark datasets under both transductive and inductive learning. © 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM.",graph neural networks; heterogeneous information network; inductive learning; semi-supervised learning; Short texts,Backpropagation; Classification (of information); Graph neural networks; Semantics; Supervised learning; Text processing; Graph neural networks; Heterogeneous graph; Heterogeneous information; Heterogeneous information network; Information networks; Labeled data; Search strategies; Semi-supervised; Short text classifications; Short texts; Information services
Spoken conversational context improves query auto-completion in web search,2021,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111093697&doi=10.1145%2f3447875&partnerID=40&md5=9fe98331a8fc735058cd82042b2eaf79,"Web searches often originate from conversations in which people engage before they perform a search. Therefore, conversations can be a valuable source of context with which to support the search process. We investigate whether spoken input from conversations can be used as a context to improve query auto-completion. We model the temporal dynamics of the spoken conversational context preceding queries and use these models to re-rank the query auto-completion suggestions. Data were collected from a controlled experiment and comprised conversations among 12 participant pairs conversing about movies or traveling. Search query logs during the conversations were recorded and temporally associated with the conversations. We compared the effects of spoken conversational input in four conditions: a control condition without contextualization; an experimental condition with the model using search query logs; an experimental condition with the model using spoken conversational input; and an experimental condition with the model using both search query logs and spoken conversational input. We show the advantage of combining the spoken conversational context with the Web-search context for improved retrieval performance. Our results suggest that spoken conversations provide a rich context for supporting information searches beyond current user-modeling approaches. © 2021 Association for Computing Machinery.",Background speech; QAC; Query auto-completion; Speech input; Voice,Learning to rank; Websites; Auto completion; Contextualization; Controlled experiment; Experimental conditions; Information search; Retrieval performance; Spoken conversation; Temporal dynamics; Information retrieval
A Critical Reassessment of the Saerens-Latinne-Decaestecker Algorithm for Posterior Probability Adjustment,2021,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103538557&doi=10.1145%2f3433164&partnerID=40&md5=623425f228aa2d373cfb1bed8cd4a120,"We critically re-examine the Saerens-Latinne-Decaestecker (SLD) algorithm, a well-known method for estimating class prior probabilities (""priors"") and adjusting posterior probabilities (""posteriors"") in scenarios characterized by distribution shift, i.e., difference in the distribution of the priors between the training and the unlabelled documents. Given a machine learned classifier and a set of unlabelled documents for which the classifier has returned posterior probabilities and estimates of the prior probabilities, SLD updates them both in an iterative, mutually recursive way, with the goal of making both more accurate; this is of key importance in downstream tasks such as single-label multiclass classification and cost-sensitive text classification. Since its publication, SLD has become the standard algorithm for improving the quality of the posteriors in the presence of distribution shift, and SLD is still considered a top contender when we need to estimate the priors (a task that has become known as ""quantification""). However, its real effectiveness in improving the quality of the posteriors has been questioned. We here present the results of systematic experiments conducted on a large, publicly available dataset, across multiple amounts of distribution shift and multiple learners. Our experiments show that SLD improves the quality of the posterior probabilities and of the estimates of the prior probabilities, but only when the number of classes in the classification scheme is very small and the classifier is calibrated. As the number of classes grows, or as we use non-calibrated classifiers, SLD converges more slowly (and often does not converge at all), performance degrades rapidly, and the impact of SLD on the quality of the prior estimates and of the posteriors becomes negative rather than positive. © 2020 ACM.",dataset shift; distribution shift; posterior probabilities; prior probabilities; probabilistic classifiers; Text classification,Classification (of information); Information retrieval systems; Iterative methods; Large dataset; Text processing; Turing machines; Classification scheme; Multi-class classification; Number of class; Posterior probability; Prior probability; Standard algorithms; Systematic experiment; Text classification; Probability distributions
Effects of Personalized and Aggregate Top-N Recommendation Lists on User Preference Ratings,2021,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103570029&doi=10.1145%2f3430028&partnerID=40&md5=fdd7e9d2faa89a77ab7550563b98c257,"Prior research has shown a robust effect of personalized product recommendations on user preference judgments for items. Specifically, the display of system-predicted preference ratings as item recommendations has been shown in multiple studies to bias users' preference ratings after item consumption in the direction of the predicted rating. Top-N lists represent another common approach for presenting item recommendations in recommender systems. Through three controlled laboratory experiments, we show that top-N lists do not induce a discernible bias in user preference judgments. This result is robust, holding for both lists of personalized item recommendations and lists of items that are top-rated based on averages of aggregate user ratings. Adding numerical ratings to the list items does generate a bias, consistent with earlier studies. Thus, in contexts where preference biases are of concern to an online retailer or platform, top-N lists, without numerical predicted ratings, would be a promising format for displaying item recommendations. © 2021 ACM.",decision biases; personalization; Recommender systems; top-N recommendations; user preferences,Computer networks; Information systems; Controlled laboratories; In contexts; Numerical rating; Personalized products; User rating; Aggregates
"Retrieval Evaluation Measures that Agree with Users' SERP Preferences: Traditional, Preference-based, and Diversity Measures",2021,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103579615&doi=10.1145%2f3431813&partnerID=40&md5=0e02c8b18ff7e9fa488ef526b3b89763,"We examine the ""goodness""of ranked retrieval evaluation measures in terms of how well they align with users' Search Engine Result Page (SERP) preferences for web search. The SERP preferences cover 1,127 topic-SERP-SERP triplets extracted from the NTCIR-9 INTENT task, reflecting the views of 15 different assessors. Each assessor made two SERP preference judgements for each triplet: one in terms of relevance and the other in terms of diversity. For each evaluation measure, we compute the Agreement Rate (AR) of each triplet: The proportion of assessors that agree with the measure's SERP preference. We then compare the mean ARs of the measures as well as those of best/median/worst assessors using Tukey HSD tests. Our first experiment compares traditional ranked retrieval measures based on the SERP relevance preferences: we find that normalised Discounted Cumulative Gain (nDCG) and intentwise Rank-biased Utility (iRBU) perform best in that they are the only measures that are statistically indistinguishable from our best assessor; nDCG also statistically significantly outperforms our median assessor. Our second experiment utilises 119,646 document preferences that we collected for a subset of the above topic-SERP-SERP triplets (containing 894 triplets) to compare preference-based evaluation measures as well as traditional ones. Again, we evaluate them based on the SERP relevance preferences. The results suggest that measures such as wpref5 are the most promising among the preference-based measures considered, although they underperform the best traditional measures such as nDCG on average. Our third experiment compares diversified search measures based on the SERP diversity preferences as well as the SERP relevance preferences, and it shows that D-measures are clearly the most reliable: in particular, D-nDCG and D-RBP statistically significantly outperform the median assessor and all intent-Aware measures; they also outperform the recently proposed RBU on average. Also, in terms of agreement with SERP diversity preferences, D-nDCG statistically significantly outperforms RBU. Hence, if IR researchers want to use evaluation measures that align well with users' SERP preferences, then we recommend nDCG and iRBU for traditional search, and D-measures such as D-nDCG for diversified search. As for document preference-based measures that we have examined, we do not have a strong reason to recommended them over traditional measures like nDCG, since they align slightly less well with users' SERP preferences despite their quadratic assessment cost. © 2020 ACM.",Document preferences; evaluation measures; preference assessments; search engine result pages; search result diversification; SERP preferences,Information retrieval; Search engines; Diversity measure; Evaluation measures; Preference-based; Ranked retrieval; Retrieval evaluation; Search engine results; Web searches; Petroleum reservoir evaluation
Weighting Passages Enhances Accuracy,2021,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103542050&doi=10.1145%2f3428687&partnerID=40&md5=c269b80afa8f037497f1c61cb73513c0,"We observe that in curated documents the distribution of the occurrences of salient terms, e.g., terms with a high Inverse Document Frequency, is not uniform, and such terms are primarily concentrated towards the beginning and the end of the document. Exploiting this observation, we propose a novel version of the classical BM25 weighting model, called BM25 Passage (BM25P), which scores query results by computing a linear combination of term statistics in the different portions of the document. We study a multiplicity of partitioning schemes of document content into passages and compute the collection-dependent weights associated with them on the basis of the distribution of occurrences of salient terms in documents. Moreover, we tune BM25P hyperparameters and investigate their impact on ad hoc document retrieval through fully reproducible experiments conducted using four publicly available datasets. Our findings demonstrate that our BM25P weighting model markedly and consistently outperforms BM25 in terms of effectiveness by up to 17.44% in NDCG@5 and 85% in NDCG@1, and up to 21% in MRR. © 2020 ACM.",BM25P; evaluation; Passage retrieval; salient terms; weighting models,Information systems; Document contents; Document Retrieval; Hyperparameters; Inverse Document Frequency; Linear combinations; Query results; Weighting model; Computer networks
VM-NSP: Vertical Negative Sequential Pattern Mining with Loose Negative Element Constraints,2021,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103588836&doi=10.1145%2f3440874&partnerID=40&md5=e6ec438023b1b5d505fe25e9cdb2791a,"Negative sequential patterns (NSPs) capture more informative and actionable knowledge than classic positive sequential patterns (PSPs) due to the involvement of both occurring and nonoccurring behaviors and events, which can contribute to many relevant applications. However, NSP mining is nontrivial, as it involves fundamental challenges requiring distinct theoretical foundations and is not directly addressable by PSP mining. In the very limited research reported on NSP mining, a negative element constraint (NEC) is incorporated to only consider the NSPs composed of specific forms of elements (containing either positive or negative items), which results in many valuable NSPs being missed. Here, we loosen the NEC (called loose negative element constraint (LNEC)) to include partial negative elements containing both positive and negative items, which enables the discovery of more flexible patterns but incorporates significant new learning challenges, such as representing and mining complete NSPs. Accordingly, we formalize the LNEC-based NSP mining problem and propose a novel vertical NSP mining framework, VM-NSP, to efficiently mine the complete set of NSPs by a vertical representation (VR) of each sequence. An efficient bitmap-based vertical NSP mining algorithm, bM-NSP, introduces a bitmap hash table-based VR and a prefix-based negative sequential candidate generation strategy to optimize the discovery performance. VM-NSP and its implementation bM-NSP form the first VR-based approach for complete NSP mining with LNEC. Theoretical analyses and experiments confirm the performance superiority of bM-NSP on synthetic and real-life datasets w.r.t. diverse data factors, which substantially expands existing NSP mining methods toward flexible NSP discovery. © 2021 ACM.",behavior informatics; negative sequence analysis; negative sequential pattern mining; nonoccurring behavior analytics; Sequence analysis; vertical representation,Computer networks; Information systems; Candidate generation; Flexible patterns; Mining algorithms; Mining problems; Real life datasets; Sequential patterns; Sequential-pattern mining; Theoretical foundations; Data mining
Toward Dynamic User Intention: Temporal Evolutionary Effects of Item Relations in Sequential Recommendation,2021,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103599448&doi=10.1145%2f3432244&partnerID=40&md5=d44b88762faa04be6694cf93d30b019e,"User intention is an important factor to be considered for recommender systems, which always changes dynamically in different contexts. Recent studies (represented by sequential recommendation) begin to focus on predicting what users want beyond what users like, which are better at capturing user intention and have attracted a surge of interest. However, user intention modeling is non-Trivial, because it is generally influenced by various factors, among which item relations and their temporal evolutionary effects are of great importance. For example, consumption of a cellphone will have varying impacts on the demands for its relational items: For complements, the demands are likely to be promoted in the short term; while for substitutes, the long-Term effect may take advantage, because users do not need another cellphone immediately. Moreover, the temporal evolutions of different relational effects vary across different domains, which makes it challenging to adaptively take them into consideration. As a result, most existing studies only loosely incorporate item relations by encoding their semantics into embeddings, neglecting fine-grained time-Aware effects. In this work, we propose Knowledge-Aware Dynamic Attention (KDA) to take both relational effects and their temporal evolutions into consideration. Specifically, to model dynamic impacts of historical relational interactions on user intention, we aggregate the history sequence into relation-specific embeddings, where the attention weight consists of two parts. First, we measure the relational intensity between historical items and the target item to model the absolute degree of influence in terms of each relation. Second, to model how the relational effects drift with time, we innovatively introduce Fourier transform with learnable frequency-domain embeddings to estimate temporal decay functions of different relations adaptively. Subsequently, the self-Attention mechanism is leveraged to derive the final representation of the whole history sequence, which reflects the dynamic user intention and will be applied to generate the recommendation list. Extensive experiments in three real-world datasets indicate the proposed KDA model significantly outperforms the state-of-The-Art methods on the Top-K recommendation task. Moreover, the proposed Fourier-based method opens up a new avenue to adaptively integrate temporal dynamics into general neural models. © 2020 ACM.",attention mechanism; dynamic user intention; Fourier transform; knowledge enhanced model; Sequential recommendation; time-Aware model,Frequency domain analysis; Semantics; Telephone sets; Attention mechanisms; Different domains; Real-world datasets; State-of-the-art methods; Temporal dynamics; Temporal evolution; Top-K recommendations; User intention model; Embeddings
Learning to Respond with Your Favorite Stickers,2021,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103550374&doi=10.1145%2f3429980&partnerID=40&md5=258dd1c674e43f0c7d2eb9b6dde7758c,"Stickers with vivid and engaging expressions are becoming increasingly popular in online messaging apps, and some works are dedicated to automatically select sticker response by matching the stickers image with previous utterances. However, existing methods usually focus on measuring the matching degree between the dialog context and sticker image, which ignores the user preference of using stickers. Hence, in this article, we propose to recommend an appropriate sticker to user based on multi-Turn dialog context and sticker using history of user. Two main challenges are confronted in this task. One is to model the sticker preference of user based on the previous sticker selection history. Another challenge is to jointly fuse the user preference and the matching between dialog context and candidate sticker into final prediction making. To tackle these challenges, we propose a Preference Enhanced Sticker Response Selector (PESRS) model. Specifically, PESRS first employs a convolutional-based sticker image encoder and a self-Attention-based multi-Turn dialog encoder to obtain the representation of stickers and utterances. Next, deep interaction network is proposed to conduct deep matching between the sticker and each utterance. Then, we model the user preference by using the recently selected stickers as input and use a key-value memory network to store the preference representation. PESRS then learns the short-Term and long-Term dependency between all interaction results by a fusion network and dynamically fuses the user preference representation into the final sticker selection prediction. Extensive experiments conducted on a large-scale real-world dialog dataset show that our model achieves the state-of-The-Art performance for all commonly used metrics. Experiments also verify the effectiveness of each component of PESRS. © 2021 ACM.",multi-Turn dialog; Sticker selection; user modeling,Signal encoding; Image encoders; Interaction networks; Key values; Long-term dependencies; Matching degree; Memory network; Preference representation; State-of-the-art performance; Large dataset
Microtask Detection,2021,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103596211&doi=10.1145%2f3432290&partnerID=40&md5=f7b770b9d3a145b32bf5a30dffa1253e,"Information systems, such as task management applications and digital assistants, can help people keep track of tasks of different types and different time durations, ranging from a few minutes to days or weeks. Helping people better manage their tasks and their time are core capabilities of assistive technologies, situated within a broader context of supporting more effective information access and use. Throughout the course of a day, there are typically many short time periods of downtime (e.g., five minutes or less) available to individuals. Microtasks are simple tasks that can be tackled in such short amounts of time. Identifying microtasks in task lists could help people utilize these periods of low activity to make progress on their task backlog. We define actionable tasks as self-contained tasks that need to be completed or acted on. However, not all to-do tasks are actionable. Many task lists are collections of miscellaneous items that can be completed at any time (e.g., books to read, movies to watch), notes (e.g., names, addresses), or the individual items are constituents in a list that is itself a task (e.g., a grocery list). In this article, we introduce the novel challenge of microtask detection, and we present machine-learned models for automatically determining which tasks are actionable and which of these actionable tasks are microtasks. Experiments show that our models can accurately identify actionable tasks, accurately detect actionable microtasks, and that we can combine these models to generate a solution that scales microtask detection to all tasks. We discuss our findings in detail, along with their limitations. These findings have implications for the design of systems to help people make the most of their time. © 2021 ACM.",information access; information systems; microtask detection; Tasks,Information use; Assistive technology; Core capabilities; Digital assistants; Information access; Keep track of; Task management; Time duration; Time-periods; Information management
Meaningful Answer Generation of E-Commerce Question-Answering,2021,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103548054&doi=10.1145%2f3432689&partnerID=40&md5=90b93fa35119480c9e658d2a6fae2269,"In e-commerce portals, generating answers for product-related questions has become a crucial task. In this article, we focus on the task of product-Aware answer generation, which learns to generate an accurate and complete answer from large-scale unlabeled e-commerce reviews and product attributes. However, safe answer problems (i.e., neural models tend to generate meaningless and universal answers) pose significant challenges to text generation tasks, and e-commerce question-Answering task is no exception. To generate more meaningful answers, in this article, we propose a novel generative neural model, called the Meaningful Product Answer Generator (MPAG), which alleviates the safe answer problem by taking product reviews, product attributes, and a prototype answer into consideration. Product reviews and product attributes are used to provide meaningful content, while the prototype answer can yield a more diverse answer pattern. To this end, we propose a novel answer generator with a review reasoning module and a prototype answer reader. Our key idea is to obtain the correct question-Aware information from a large-scale collection of reviews and learn how to write a coherent and meaningful answer from an existing prototype answer. To be more specific, we propose a read-And-write memory consisting of selective writing units to conduct reasoning among these reviews. We then employ a prototype reader consisting of comprehensive matching to extract the answer skeleton from the prototype answer. Finally, we propose an answer editor to generate the final answer by taking the question and the above parts as input. Conducted on a real-world dataset collected from an e-commerce platform, extensive experimental results show that our model achieves state-of-The-Art performance in terms of both automatic metrics and human evaluations. Human evaluation also demonstrates that our model can consistently generate specific and proper answers. © 2021 ACM.",e-commerce; product-Aware answer generation; Question-Answering,Computer networks; Information systems; Automatic metrics; Human evaluation; Product attributes; Product reviews; Question Answering; Question Answering Task; State-of-the-art performance; Text generations; Electronic commerce
Unbiased Learning to Rank,2021,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103576858&doi=10.1145%2f3439861&partnerID=40&md5=9b28c8288621c9c7f036c5d871f65560,"How to obtain an unbiased ranking model by learning to rank with biased user feedback is an important research question for IR. Existing work on unbiased learning to rank (ULTR) can be broadly categorized into two groups-the studies on unbiased learning algorithms with logged data, namely, the offline unbiased learning, and the studies on unbiased parameters estimation with real-Time user interactions, namely, the online learning to rank. While their definitions of unbiasness are different, these two types of ULTR algorithms share the same goal-to find the best models that rank documents based on their intrinsic relevance or utility. However, most studies on offline and online unbiased learning to rank are carried in parallel without detailed comparisons on their background theories and empirical performance. In this article, we formalize the task of unbiased learning to rank and show that existing algorithms for offline unbiased learning and online learning to rank are just the two sides of the same coin. We evaluate eight state-of-The-Art ULTR algorithms and find that many of them can be used in both offline settings and online environments with or without minor modifications. Further, we analyze how different offline and online learning paradigms would affect the theoretical foundation and empirical effectiveness of each algorithm on both synthetic and real search data. Our findings provide important insights and guidelines for choosing and deploying ULTR algorithms in practice. © 2021 ACM.",Learning to rank; online learning; unbiased learning,E-learning; Learning to rank; Background theory; Empirical performance; Online environments; Parameters estimation; Research questions; State of the art; Theoretical foundations; User interaction; Learning algorithms
Multilingual Review-Aware Deep Recommender System via Aspect-based Sentiment Analysis,2021,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103553078&doi=10.1145%2f3432049&partnerID=40&md5=48c8cc9f28c7b4de985f36f8f57ce4e8,"With the dramatic expansion of international markets, consumers write reviews in different languages, which poses a new challenge for Recommender Systems (RSs) dealing with this increasing amount of multilingual information. Recent studies that leverage deep-learning techniques for review-Aware RSs have demonstrated their effectiveness in modelling fine-grained user-item interactions through the aspects of reviews. However, most of these models can neither take full advantage of the contextual information from multilingual reviews nor discriminate the inherent ambiguity of words originated from the user's different tendency in writing. To this end, we propose a novel Multilingual Review-Aware Deep Recommendation Model (MrRec) for rating prediction tasks. MrRec mainly consists of two parts: (1) Multilingual aspect-based sentiment analysis module (MABSA), which aims to jointly extract aligned aspects and their associated sentiments in different languages simultaneously with only requiring overall review ratings. (2) Multilingual recommendation module that learns aspect importances of both the user and item with considering different contributions of multiple languages and estimates aspect utility via a dual interactive attention mechanism integrated with aspect-specific sentiments from MABSA. Finally, overall ratings can be inferred by a prediction layer adopting the aspect utility value and aspect importance as inputs. Extensive experimental results on nine real-world datasets demonstrate the superior performance and interpretability of our model. © 2021 ACM.",co-Attention; deep learning; multilingual aspect-based sentiment analysis; neural attention; Recommender systems,Deep learning; International trade; Learning systems; Sentiment analysis; Attention mechanisms; Contextual information; International markets; Interpretability; Learning techniques; Multiple languages; Prediction tasks; Real-world datasets; Recommender systems
A Troubling Analysis of Reproducibility and Progress in Recommender Systems Research,2021,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102164097&doi=10.1145%2f3434185&partnerID=40&md5=1e0c2633f986142623dd2f39297c308d,"The design of algorithms that generate personalized ranked item lists is a central topic of research in the field of recommender systems. In the past few years, in particular, approaches based on deep learning (neural) techniques have become dominant in the literature. For all of them, substantial progress over the state-of-The-Art is claimed. However, indications exist of certain problems in today's research practice, e.g., with respect to the choice and optimization of the baselines used for comparison, raising questions about the published claims. To obtain a better understanding of the actual progress, we have compared recent results in the area of neural recommendation approaches based on collaborative filtering against a consistent set of existing simple baselines. The worrying outcome of the analysis of these recent works-all were published at prestigious scientific conferences between 2015 and 2018-is that 11 of the 12 reproducible neural approaches can be outperformed by conceptually simple methods, e.g., based on the nearest-neighbor heuristic or linear models. None of the computationally complex neural methods was actually consistently better than already existing learning-based techniques, e.g., using matrix factorization or linear models. In our analysis, we discuss common issues in today's research practice, which, despite the many papers that are published on the topic, have apparently led the field to a certain level of stagnation. © 2021 ACM.",deep learning; evaluation; reproducibility; Recommender systems,Collaborative filtering; Deep learning; Factorization; Recommender systems; Design of algorithms; Matrix factorizations; Nearest neighbors; Reproducibilities; SIMPLE method; State of the art; Heuristic methods
Toward Comprehensive User and Item Representations via Three-tier Attention Network,2021,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119094481&doi=10.1145%2f3446341&partnerID=40&md5=ba4a3c0e56fcb8b368caadca0e74f760,"Product reviews can provide rich information about the opinions users have of products. However, it is nontrivial to effectively infer user preference and item characteristics from reviews due to the complicated semantic understanding. Existing methods usually learn features for users and items from reviews in single static fashions and cannot fully capture user preference and item features. In this article, we propose a neural review-based recommendation approach that aims to learn comprehensive representations of users/items under a three-tier attention framework. We design a review encoder to learn review features from words via a word-level attention, an aspect encoder to learn aspect features via a review-level attention, and a user/item encoder to learn the final representations of users/items via an aspect-level attention. In word- and review-level attentions, we adopt the context-aware mechanism to indicate importance of words and reviews dynamically instead of static attention weights. In addition, the attentions in the word and review levels are of multiple paradigms to learn multiple features effectively, which could indicate the diversity of user/item features. Furthermore, we propose a personalized aspect-level attention module in user/item encoder to learn the final comprehensive features. Extensive experiments are conducted and the results in rating prediction validate the effectiveness of our method. © 2021 Association for Computing Machinery.",attention; context-aware; multi-aspect; personalized; Recommender system,Semantics; Attention; Context-Aware; Learn+; Multi aspects; Multiple features; Personalized; Product reviews; Semantics understanding; User's preferences; Word level; Signal encoding
Interactive Sequential Basket Recommendation by Learning Basket Couplings and Positive/Negative Feedback,2021,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119092369&doi=10.1145%2f3444368&partnerID=40&md5=f1368dc47d0976ee15bf1197fc1b65f3,"Sequential recommendation, such as next-basket recommender systems (NBRS), which model users' sequential behaviors and the relevant context/session, has recently attracted much attention from the research community. Existing session-based NBRS involve session representation and inter-basket relations but ignore their hybrid couplings with the intra-basket items, often producing irrelevant or similar items in the next basket. In addition, they do not predict next-baskets (more than one next basket recommended). Interactive recommendation further involves user feedback on the recommended basket. The existing work on next-item recommendation involves positive feedback on selected items but ignores negative feedback on unselected ones. Here, we introduce a new setting - interactive sequential basket recommendation, which iteratively predicts next baskets by learning the intra-/inter-basket couplings between items and both positive and negative user feedback on recommended baskets. A hierarchical attentive encoder-decoder model (HAEM) continuously recommends next baskets one after another during sequential interactions with users after analyzing the item relations both within a basket and between adjacent sequential baskets (i.e., intra-/inter-basket couplings) and incorporating the user selection and unselection (i.e., positive/negative) feedback on the recommended baskets to refine NBRS. HAEM comprises a basket encoder and a sequence decoder to model intra-/inter-basket couplings and a prediction decoder to sequentially predict next-baskets by interactive feedback-based refinement. Empirical analysis shows that HAEM significantly outperforms the state-of-the-art baselines for NBRS and session-based recommenders for accurate and novel recommendation. We also show the effect of continuously refining sequential basket recommendation by including unselection feedback during interactive recommendation. © 2021 Association for Computing Machinery.",attention model; basket coupling; factorization machine; interactive sequential basket recommendation; negative feedback; next-basket recommender system; positive feedback; recurrent neural network; Sequential recommendation,Couplings; Decoding; Feedback; Forecasting; Recommender systems; Signal encoding; Attention model; Basket coupling; Encoder-decoder; Factorization machines; Interactive sequential basket recommendation; Next-basket recommende system; Positive/negative; Research communities; Sequential recommendation; User feedback; Recurrent neural networks
Faceted Search with Object Ranking and Answer Size Constraints,2020,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097331893&doi=10.1145%2f3425603&partnerID=40&md5=618936de001493b92c8ee58e8a5ed32c,"Faceted Search is a widely used interaction scheme in digital libraries, e-commerce, and recently also in Linked Data. Surprisingly, object ranking in the context of Faceted Search is not well studied in the literature. In this article, we propose an extension of the model with two parameters that enable specifying the desired answer size and the granularity of the sought object ranking. These parameters allow tackling the problem of too big or too small answers and can specify how refined the sought ranking should be. Then, we provide an algorithm that takes as input these parameters and by considering the hard-constraints (filters), the soft-constraints (preferences), as well as the statistical properties of the dataset (through various frequency-based ranking schemes), produces an object ranking that satisfies these parameters, in a transparent way for the user. Then, we present extensive simulation-based evaluation results that provide evidence that the proposed model also improves the answers and reduces the user's cost. Finally, we propose GUI extensions that are required and present an implementation of the model.  © 2020 ACM.",approximate results; automatic ranking; Faceted Search; simulation-based evaluation,Computer networks; Information systems; Evaluation results; Extensive simulations; Hard constraints; Interaction schemes; Object rankings; Size constraint; Soft constraint; Statistical properties; Digital libraries
Pretrained Embeddings for Stance Detection with Hierarchical Capsule Network on Social Media,2020,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097337873&doi=10.1145%2f3412362&partnerID=40&md5=da68ed02b0cc4a9420b36ac6d580be66,"Stance detection on social media aims to identify the stance of social media users toward a topic or claim, which can provide powerful information for various downstream tasks. Many existing stance detection approaches neglect to model the deep semantic representation information in tweets and do not explore aggregating the hierarchical features among words, thus degrading performance. To address these issues, this article proposes a novel deep learning approach Pretrained Embeddings for Stance Detection with Hierarchical Capsule Network (PE-HCN) without complicated preprocessing. Specifically, PE-HCN first adopts a pretrained language model and then uses a related textual entailment task for fine-tuning to obtain the deep textual representations of tweets. The PE-HCN approach extends the dynamic routing scheme to cope with these deep textual representations by utilizing primary capsules for routing the information among words in each tweet and applying secondary capsules to transmit the aggregated features to each category capsule accordingly. Moreover, to improve the confidences of the category capsules, we design an adaptive feedback mechanism to dynamically strengthen the routing signals. Through experiments on three benchmark datasets, compared with the state-of-the-art baselines, the extensive results exhibit that PE-HCN achieves competitive improvements of up to 6.32%, 2.09%, and 1.8%, respectively.  © 2020 ACM.",capsule network; Language model; stance detection,Embeddings; Semantics; Social networking (online); Adaptive feedback; Benchmark datasets; Detection approach; Hierarchical features; Learning approach; Semantic representation; Textual entailment; Textual representation; Deep learning
Graph-based Regularization on Embedding Layers for Recommendation,2020,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097330310&doi=10.1145%2f3414067&partnerID=40&md5=0fd4be5cd71277523f5b9d2647861b9a,"Neural networks have been extensively used in recommender systems. Embedding layers are not only necessary but also crucial for neural models in recommendation as a typical discrete task. In this article, we argue that the widely used l2 regularization for normal neural layers (e.g., fully connected layers) is not ideal for embedding layers from the perspective of regularization theory in Reproducing Kernel Hilbert Space. More specifically, the l2 regularization corresponds to the inner product and the distance in the Euclidean space where correlations between discrete objects (e.g., items) are not well captured. Inspired by this observation, we propose a graph-based regularization approach to serve as a counterpart of the l2 regularization for embedding layers. The proposed regularization incurs almost no extra computational overhead especially when being trained with mini-batches. We also discuss its relationships to other approaches (namely, data augmentation, graph convolution, and joint learning) theoretically. We conducted extensive experiments on five publicly available datasets from various domains with two state-of-the-art recommendation models. Results show that given a kNN (k-nearest neighbor) graph constructed directly from training data without external information, the proposed approach significantly outperforms the l2 regularization on all the datasets and achieves more notable improvements for long-tail users and items.  © 2020 ACM.",Embedding; graph-based regularization; neural recommender system,Computation theory; Embeddings; Nearest neighbor search; Recommender systems; Computational overheads; Data augmentation; Discrete objects; External informations; Graph-based regularizations; K-nearest neighbors; Regularization theory; Reproducing Kernel Hilbert spaces; Graphic methods
End-to-End Neural Matching for Semantic Location Prediction of Tweets,2020,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097354699&doi=10.1145%2f3415149&partnerID=40&md5=d41253edea64ed4448d7a956721bd5b0,"The impressive increasing availability of social media posts has given rise to considerable research challenges. This article is concerned with the problem of semantic location prediction of geotagged tweets. The underlying task is to associate to a social media post, the focal spatial object, if any (e.g., Place Of Interest POI), it topically focuses on. Although relevant for a number of applications such as POI recommendation, this problem has not so far received the attention it deserves. In previous work, the problem has mainly been tackled by means of language models that rely on costly probability estimation of word relevance across spatial regions. We propose the Spatially-aware Geotext Matching (SGM) model, which relies on a neural network learning framework. The model combines exact word-word-local interaction matching signals with semantic global tweet-POI interaction matching signals. The local interactions are built over kernel spatial word distributions that allow revealing spatially driven word pair similarity patterns. The global interactions consider the strength of the interaction between the tweet and the POI from both the spatial and semantic perspectives. Experimental results on two real-world datasets demonstrate the effectiveness of our proposed SGM model compared to state-of-the-art baselines including language models and traditional neural interaction-based models.  © 2020 ACM.",neural text matching; point of interest; Semantic location prediction; tweet,Computational linguistics; Social networking (online); Global interaction; Local interactions; Neural interactions; Neural network learning; Probability estimation; Real-world datasets; Research challenges; Similarity patterns; Semantics
An Attention-based Deep Relevance Model for Few-shot Document Filtering,2020,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097341155&doi=10.1145%2f3419972&partnerID=40&md5=e48984d3bd93a9d666fe2791f47bc88a,"With the large quantity of textual information produced on the Internet, a critical necessity is to filter out the irrelevant information and organize the rest into categories of interest (e.g., an emerging event). However, supervised-learning document filtering methods heavily rely on a large number of labeled documents for model training. Manually identifying plenty of positive examples for each category is expensive and time-consuming. Also, it is unrealistic to cover all the categories from an evolving text source that covers diverse kinds of events, user opinions, and daily life activities. In this article, we propose a novel attention-based deep relevance model for few-shot document filtering (named ADRM), inspired by the relevance feedback methodology proposed for ad hoc retrieval. ADRM calculates the relevance score between a document and a category by taking a set of seed words and a few seed documents relevant to the category. It constructs the category-specific conceptual representation of the document based on the corresponding seed words and seed documents. Specifically, to filter irrelevant yet noisy information in the seed documents, ADRM employs two types of attention mechanisms (namely whole-match attention and max-match attention) and generates category-specific representations for them. Then ADRM is devised to extract the relevance signals by modeling the hidden feature interactions in the word embedding space. The relevance signals are extracted through a gated convolutional process, a self-attention layer, and a relevance aggregation layer. Extensive experiments on three real-world datasets show that ADRM consistently outperforms the existing technical alternatives, including the conventional classification and retrieval baselines, and the state-of-the-art deep relevance ranking models for few-shot document filtering. We also perform an ablation study to demonstrate that each component in ADRM is effective for enhancing filtering performance. Further analysis shows that ADRM is robust under varying parameter settings.  © 2020 ACM.",deep learning; document filtering; Few-shot learning,Classification (of information); Information retrieval; Learning to rank; Signal processing; Attention mechanisms; Category specifics; Daily life activities; Feature interactions; Filtering performance; Real-world datasets; Technical alternatives; Textual information; Information retrieval systems
PONE: A Novel Automatic Evaluation Metric for Open-domain Generative Dialogue Systems,2020,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097352565&doi=10.1145%2f3423168&partnerID=40&md5=360d622d205e7a7ba84682a78629a2f1,"Open-domain generative dialogue systems have attracted considerable attention over the past few years. Currently, how to automatically evaluate them is still a big challenge. As far as we know, there are three kinds of automatic evaluations for open-domain generative dialogue systems: (1) Word-overlap-based metrics; (2) Embedding-based metrics; (3) Learning-based metrics. Due to the lack of systematic comparison, it is not clear which kind of metrics is more effective. In this article, we first measure systematically all kinds of metrics to check which kind is best. Extensive experiments demonstrate that learning-based metrics are the most effective evaluation metrics for open-domain generative dialogue systems. Moreover, we observe that nearly all learning-based metrics depend on the negative sampling mechanism, which obtains extremely imbalanced and low-quality samples to train a score model. To address this issue, we propose a novel learning-based metric that significantly improves the correlation with human judgments by using augmented POsitive samples and valuable NEgative samples, called PONE. Extensive experiments demonstrate that PONE significantly outperforms the state-of-the-art learning-based evaluation method. Besides, we have publicly released the codes of our proposed metric and state-of-the-art baselines.1  © 2020 ACM.",automatic evaluation; generative dialogue systems; Open-domain,Speech processing; Automatic evaluation; Dialogue systems; Evaluation metrics; Human judgments; Low qualities; Negative samples; Sampling mechanisms; State of the art; Learning systems
Neural Feature-aware Recommendation with Signed Hypergraph Convolutional Network,2020,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097341722&doi=10.1145%2f3423322&partnerID=40&md5=79669eabc4371d1347689638becc03fc,"Understanding user preference is of key importance for an effective recommender system. For comprehensive user profiling, many efforts have been devoted to extract user feature-level preference from the review information. Despite effectiveness, existing methods mostly assume linear relationships among the users, items, and features, and the collaborative information is usually utilized in an implicit and insufficient manner, which limits the recommender capacity in modeling users' diverse preferences. For bridging this gap, in this article, we propose to formulate user feature-level preferences by a neural signed hypergraph and carefully design the information propagation paths for diffusing collaborative filtering signals in a more effective manner. By taking the advantages of the neural model's powerful expressiveness, the complex relationship patterns among users, items, and features are sufficiently discovered and well utilized. By infusing graph structure information into the embedding process, the collaborative information is harnessed in a more explicit and effective way. We conduct comprehensive experiments on real-world datasets to demonstrate the superiorities of our model.  © 2020 ACM.",collaborative filtering; feature-based recommendation; graph convolutional network; hypergraph neural network; Recommendation system,Collaborative filtering; Graph structures; Information dissemination; Signal processing; Collaborative information; Complex relationships; Convolutional networks; Embedding process; Information propagation; Linear relationships; Real-world datasets; Structure information; Convolutional neural networks
How the Accuracy and Confidence of Sensitivity Classification Affects Digital Sensitivity Review,2020,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097335761&doi=10.1145%2f3417334&partnerID=40&md5=964a24482cb0a767f34bcda204115c98,"Government documents must be manually reviewed to identify any sensitive information, e.g., confidential information, before being publicly archived. However, human-only sensitivity review is not practical for born-digital documents due to, for example, the volume of documents that are to be reviewed. In this work, we conduct a user study to evaluate the effectiveness of sensitivity classification for assisting human sensitivity reviewers. We evaluate how the accuracy and confidence levels of sensitivity classification affects the number of documents that are correctly judged as being sensitive (reviewer accuracy) and the time that it takes to sensitivity review a document (reviewing speed). In our within-subject study, the participants review government documents to identify real sensitivities while being assisted by three sensitivity classification treatments, namely None (no classification predictions), Medium (sensitivity predictions from a simulated classifier with a balanced accuracy (BAC) of 0.7), and Perfect (sensitivity predictions from a classifier with an accuracy of 1.0). Our results show that sensitivity classification leads to significant improvements (ANOVA, p < 0.05) in reviewer accuracy in terms of BAC (+37.9% Medium, +60.0% Perfect) and also in terms of F2 (+40.8% Medium, +44.9% Perfect). Moreover, we show that assisting reviewers with sensitivity classification predictions leads to significantly increased (ANOVA, p < 0.05) mean reviewing speeds (+72.2% Medium, +61.6% Perfect). We find that reviewers do not agree with the classifier significantly more as the classifier's confidence increases. However, reviewing speed is significantly increased when the reviewers agree with the classifier (ANOVA, p < 0.05). Our in-depth analysis shows that when the reviewers are not assisted with sensitivity predictions, mean reviewing speeds are 40.5% slower for sensitive judgements compared to not-sensitive judgements. However, when the reviewers are assisted with sensitivity predictions, the difference in reviewing speeds between sensitive and not-sensitive judgements is reduced by 10%, from 40.5% to 30.8%. We also find that, for sensitive judgements, sensitivity classification predictions significantly increase mean reviewing speeds by 37.7% when the reviewers agree with the classifier's predictions (t-test, p < 0.05). Overall, our findings demonstrate that sensitivity classification is a viable technology for assisting human reviewers with the sensitivity review of digital documents.  © 2020 ACM.",document classification; freedom of information; Technology-assisted review; user study,Analysis of variance (ANOVA); Forecasting; Speed; Classification prediction; Confidence levels; Confidential information; Government documents; Human sensitivity; Sensitive informations; Sensitivity classification; Sensitivity prediction; Information retrieval systems
Bots in Social and Interaction Networks,2020,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097346225&doi=10.1145%2f3419369&partnerID=40&md5=c62af64066a55fd93d0bf42a69d969de,"The rise of bots and their influence on social networks is a hot topic that has aroused the interest of many researchers. Despite the efforts to detect social bots, it is still difficult to distinguish them from legitimate users. Here, we propose a simple yet effective semi-supervised method that allows distinguishing between bots and legitimate users with high accuracy. The method learns a joint representation of social connections and interactions between users by leveraging graph-based representation learning. Then, on the proximity graph derived from user embeddings, a sample of bots is used as seeds for a label propagation algorithm. We demonstrate that when the label propagation is done according to pairwise account proximity, our method achieves F1 = 0.93, whereas other state-of-the-art techniques achieve F1 ≤ 0.87. By applying our method to a large dataset of retweets, we uncover the presence of different clusters of bots in the network of Twitter interactions. Interestingly, such clusters feature different degrees of integration with legitimate users. By analyzing the interactions produced by the different clusters of bots, our results suggest that a significant group of users was systematically exposed to content produced by bots and to interactions with bots, indicating the presence of a selective exposure phenomenon.  © 2020 ACM.",disinformation; label propagation; selective exposure; semi-supervised bot detection; Social bots; user embeddings,Graph algorithms; Graphic methods; Large dataset; Semi-supervised learning; Graph-based representations; Interaction networks; Label propagation; Legitimate users; Proximity graphs; Semi-supervised method; Social connection; State-of-the-art techniques; Botnet
Deep Learning for Sequential Recommendation,2020,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097349588&doi=10.1145%2f3426723&partnerID=40&md5=bd4f575f0f0cd8b61068c9fa47981f49,"In the field of sequential recommendation, deep learning - (DL) based methods have received a lot of attention in the past few years and surpassed traditional models such as Markov chain-based and factorization-based ones. However, there is little systematic study on DL-based methods, especially regarding how to design an effective DL model for sequential recommendation. In this view, this survey focuses on DL-based sequential recommender systems by taking the aforementioned issues into consideration. Specifically, we illustrate the concept of sequential recommendation, propose a categorization of existing algorithms in terms of three types of behavioral sequences, summarize the key factors affecting the performance of DL-based models, and conduct corresponding evaluations to showcase and demonstrate the effects of these factors. We conclude this survey by systematically outlining future directions and challenges in this field.  © 2020 ACM.",deep learning; evaluations; influential factors; Sequential recommendation; session-based recommendation; survey,Markov chains; Surveys; Systematic study; Traditional models; Deep learning
When to Stop Reviewing in Technology-Assisted Reviews,2020,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85093981909&doi=10.1145%2f3411755&partnerID=40&md5=4887b515f51f259f77832dfa30cef6a1,"Technology-Assisted Reviews (TAR) aim to expedite document reviewing (e.g., medical articles or legal documents) by iteratively incorporating machine learning algorithms and human feedback on document relevance. Continuous Active Learning (CAL) algorithms have demonstrated superior performance compared to other methods in efficiently identifying relevant documents. One of the key challenges for CAL algorithms is deciding when to stop displaying documents to reviewers. Existing work either lacks transparency-it provides an ad-hoc stopping point, without indicating how many relevant documents are still not found, or lacks efficiency by paying an extra cost to estimate the total number of relevant documents in the collection prior to the actual review. In this article, we handle the problem of deciding the stopping point of TAR under the continuous active learning framework by jointly training a ranking model to rank documents, and by conducting a ""greedy""sampling to estimate the total number of relevant documents in the collection. We prove the unbiasedness of the proposed estimators under a with-replacement sampling design, while experimental results demonstrate that the proposed approach, similar to CAL, effectively retrieves relevant documents; but it also provides a transparent, accurate, and effective stopping point. © 2020 ACM.",active sampling; Total recall; unbiased estimator,Iterative methods; Learning systems; Learning to rank; Tar; Active Learning; Legal documents; Ranking model; Relevant documents; Sampling design; Learning algorithms
Learning Unsupervised Knowledge-Enhanced Representations to Reduce the Semantic Gap in Information Retrieval,2020,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85093979566&doi=10.1145%2f3417996&partnerID=40&md5=87b100f34a9bc88bc05d3d253aba5a1e,"The semantic mismatch between query and document terms-i.e., the semantic gap-is a long-standing problem in Information Retrieval (IR). Two main linguistic features related to the semantic gap that can be exploited to improve retrieval are synonymy and polysemy. Recent works integrate knowledge from curated external resources into the learning process of neural language models to reduce the effect of the semantic gap. However, these knowledge-enhanced language models have been used in IR mostly for re-ranking and not directly for document retrieval. We propose the Semantic-Aware Neural Framework for IR (SAFIR), an unsupervised knowledge-enhanced neural framework explicitly tailored for IR. SAFIR jointly learns word, concept, and document representations from scratch. The learned representations encode both polysemy and synonymy to address the semantic gap. SAFIR can be employed in any domain where external knowledge resources are available. We investigate its application in the medical domain where the semantic gap is prominent and there are many specialized and manually curated knowledge resources. The evaluation on shared test collections for medical literature retrieval shows the effectiveness of SAFIR in terms of retrieving and ranking relevant documents most affected by the semantic gap. © 2020 ACM.",Knowledge-enhanced retrieval; medical literature; representation learning; semantic gap,Computational linguistics; Knowledge representation; Learning systems; Semantics; Document Representation; Document Retrieval; External knowledge; External resources; Knowledge resource; Linguistic features; Medical literatures; Semantic mismatches; Information retrieval
A Survey on Heterogeneous One-class Collaborative Filtering,2020,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85093977817&doi=10.1145%2f3402521&partnerID=40&md5=2e67586b7473efc0bee9c13f0e92e186,"Recommender systems play an important role in providing personalized services for users in the context of information overload. Generally, users' feedback toward items often contain the most significant information reflecting their preferences, which enables accurate personalized recommendation. In real applications, users' feedback are usually heterogeneous (rather than homogeneous) such as purchases and examinations in e-commerce, which reflects users' preferences in different degrees. Effective modeling of such heterogeneous one-class feedback is challenging compared with that of homogeneous feedback of ratings. As a response, heterogeneous one-class collaborative filtering (HOCCF) is proposed, which often converts the heterogeneous feedback into two parts (i.e., target feedback and auxiliary feedback), aiming to care more about the target feedback (e.g., purchases) with the assistance of the auxiliary feedback (e.g., examinations). In this survey, we provide an overview of the representative HOCCF methods from the perspective of factorization-based methods, transfer learning-based methods, and deep learning-based methods. First, we review the factorization-based methods according to different strategies. Second, we describe the transfer learning-based methods with different knowledge sharing manners. Third, we discuss the deep learning-based methods according to the neural architectures. Moreover, we include some important example applications, describe the empirical studies, and discuss some promising future directions. © 2020 ACM.",deep learning; Heterogeneous one-class collaborative filtering; matrix factorization; transfer learning,Collaborative filtering; Deep learning; Electronic commerce; Factorization; Surveys; Transfer learning; Empirical studies; Information overloads; Knowledge-sharing; Learning-based methods; Neural architectures; Personalized recommendation; Personalized service; Real applications; Learning systems
Pairwise Link Prediction Model for out of Vocabulary Knowledge Base Entities,2020,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85093977632&doi=10.1145%2f3406116&partnerID=40&md5=ac005090762cebf3c3549fc57759e583,"Real-world knowledge bases such as DBPedia, Yago, and Freebase contain sparse linkage connectivity, which poses a severe challenge to link prediction between entities. To cope with such data scarcity issues, recent models have focused on learning interactions between entity pairs by means of relations that exist between them. However promising, some relations are associated with very few tail entities or head entities, resulting in poor estimation of the relation interaction between entities. In this article, we break the sole dependency of modeling relation interactions between entity pairs by associating a triple with pairwise embeddings, i.e., distributed vector representations for pairs of word-based entities and relation of a triple. We capture the interactions that exist between pairwise embeddings by means of a Pairwise Factorization Model that employs a factorization machine with relation attention. This approach allows parameters for related interactions to be estimated efficiently, ensuring that the pairwise embeddings are discriminative, providing strong supervisory signals for the decoding task of link prediction. The Pairwise Factorization Model we propose exploits a neural bag-of-words model as the encoder, which effectively encodes word-based entities into distributed vector representations for the decoder. The proposed model is simple and enjoys efficiency and capability, showing superior link prediction performance over state-of-The-Art complex models on benchmark datasets DBPedia50K and FB15K-237. © 2020 ACM.",graph convolutional networks; Knowledge bases; representation learning,Benchmarking; Decoding; Embeddings; Factorization; Forecasting; Information retrieval; Knowledge based systems; Bag-of-words models; Benchmark datasets; Factorization machines; Factorization model; Learning interactions; Modeling relations; State of the art; Vector representations; Predictive analytics
Block-Aware Item Similarity Models for Top-N Recommendation,2020,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85093945561&doi=10.1145%2f3411754&partnerID=40&md5=88f0cd0d25c2ceedb90226c8ee9108f2,"Top-N recommendations have been studied extensively. Promising results have been achieved by recent item-based collaborative filtering (ICF) methods. The key to ICF lies in the estimation of item similarities. Observing the block-diagonal structure of the item similarities in practice, we propose a block-diagonal regularization (BDR) over item similarities for ICF. The intuitions behind BDR are as follows: (1) with BDR, item clustering is embedded into the learning of ICF methods; (2) BDR induces sparsity of item similarities, which guarantees recommendation efficiency; and (3) BDR captures in-block transitivity to overcome rating sparsity. By regularizing the item similarity matrix of item similarity models with BDR, we obtain a block-Aware item similarity model. Our experimental evaluations on a large number of datasets show that the block-diagonal structure is crucial to the performance of top-N recommendation. © 2020 ACM.",Item collaborative filtering; item similarity model; top-N recommendation,Large dataset; Block diagonal; Experimental evaluation; Item similarity; Item-based collaborative filtering; Recommendation efficiency; Collaborative filtering
CRSAL: Conversational recommender systems with adversarial learning,2020,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85093981606&doi=10.1145%2f3394592&partnerID=40&md5=5ff39bbff05e6ce4134b6c08946bfb55,"Recommender systems have been attracting much attention from both academia and industry because of their ability to capture user interests and generate personalized item recommendations. As the life pace in contemporary society speeds up, traditional recommender systems are inevitably limited by their disconnected interaction styles and low adaptivity to users' evolving demands. Consequently, conversational recommender systems emerge as a prospective research area, where an intelligent dialogue agent is integrated with a recommender system. Conversational recommender systems possess the ability to accurately understand end-users' intent or request and generate human-like dialogue responses when performing recommendations. However, existing conversational recommender systems only allow the systems to ask users for more preference information, while users' further questions and concerns about the recommended items (e.g., enquiring the location of a recommended restaurant) can hardly be addressed. Though the recent task-oriented dialogue systems allow for two-way communications, they are not easy to train because of their high dependence on human guidance in terms of user intent recognition and system response generation. Hence, to enable two-way human-machine communications and tackle the challenges brought by manually crafted rules, we propose Conversational Recommender System with Adversarial Learning (CRSAL), a novel end-To-end system to tackle the task of conversational recommendation. In CRSAL, we innovatively design a fully statistical dialogue state tracker coupled with a neural policy agent to precisely capture each user's intent from limited dialogue data and generate conversational recommendation actions. We further develop an adversarial Actor-Critic reinforcement learning approach to adaptively refine the quality of generated system actions, thus ensuring coherent human-like dialogue responses. Extensive experiments on two benchmark datasets fully demonstrate the superiority of CRSAL on conversational recommendation tasks. © 2020 ACM.",adversarial learning; Conversational recommender systems; deep neural networks; dialogue systems,Learning systems; Reinforcement learning; Speech processing; Speech recognition; Actor-critic reinforcement learning; Adversarial learning; Conversational recommendations; Conversational recommender systems; Human-machine communication; Intelligent dialogue; Preference information; Two way communications; Recommender systems
Enhancing Employer Brand Evaluation with Collaborative Topic Regression Models,2020,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85093971149&doi=10.1145%2f3392734&partnerID=40&md5=9f2864e3b121b45acabd6b2df0e65097,"Employer Brand Evaluation (EBE) is to understand an employer's unique characteristics to identify competitive edges. Traditional approaches rely heavily on employers' financial information, including financial reports and filings submitted to the Securities and Exchange Commission (SEC), which may not be readily available for private companies. Fortunately, online recruitment services provide a variety of employers' information from their employees' online ratings and comments, which enables EBE from an employee's perspective. To this end, in this article, we propose a method named Company Profiling-based Collaborative Topic Regression (CPCTR) to collaboratively model both textual (i.e., reviews) and numerical information (i.e., salaries and ratings) for learning latent structural patterns of employer brands. With identified patterns, we can effectively conduct both qualitative opinion analysis and quantitative salary benchmarking. Moreover, a Gaussian processes-based extension, GPCTR, is proposed to capture the complex correlation among heterogeneous information. Extensive experiments are conducted on three real-world datasets to validate the effectiveness and generalizability of our methods in real-life applications. The results clearly show that our methods outperform state-of-The-Art baselines and enable a comprehensive understanding of EBE. © 2020 ACM.",collaborative topic regression; Employer brand evaluation; Gaussian processes; salary benchmarking,Regression analysis; Wages; Complex correlation; Financial information; Heterogeneous information; Numerical information; Real-life applications; Real-world datasets; Securities and Exchange Commissions; Traditional approaches; Numerical methods
Large-Alphabet Semi-Static Entropy Coding Via Asymmetric Numeral Systems,2020,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85093975705&doi=10.1145%2f3397175&partnerID=40&md5=ffb1e81ff2441015685a850132f275dd,"An entropy coder takes as input a sequence of symbol identifiers over some specified alphabet and represents that sequence as a bitstring using as few bits as possible, typically assuming that the elements of the sequence are independent of each other. Previous entropy coding methods include the well-known Huffman and arithmetic approaches. Here we examine the newer asymmetric numeral systems (ANS) technique for entropy coding and develop mechanisms that allow it to be efficiently used when the size of the source alphabet is large-thousands or millions of symbols. In particular, we examine different ways in which probability distributions over large alphabets can be approximated and in doing so infer techniques that allow the ANS mechanism to be extended to support large-Alphabet entropy coding. As well as providing a full description of ANS, we also present detailed experiments using several different types of input, including data streams arising as typical output from the modeling stages of text compression software, and compare theproposed ANS variants with Huffman and arithmetic coding baselines, measuring both compression effectiveness and also encoding and decoding throughput. We demonstrate that in applications in which semi-static compression is appropriate, ANS-based coders can provide an excellent balance between compression effectiveness and speed, even when the alphabet is large. © 2020 ACM.",arithmetic code; Asymmetric numeral systems; Burrows-Wheeler transform; compression; entropy coder; Huffman code,Computer hardware description languages; Data streams; Entropy; Arithmetic Coding; Encoding and decoding; Entropy coders; Entropy coding; Large alphabets; Source alphabet; Static compression; Text compressions; Probability distributions
Explaining Text Matching on Neural Natural Language Inference,2020,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85093941498&doi=10.1145%2f3418052&partnerID=40&md5=4b6b78ece1c65459f79fa0e3409e0793,"Natural language inference (NLI) is the task of detecting the existence of entailment or contradiction in a given sentence pair. Although NLI techniques could help numerous information retrieval tasks, most solutions for NLI are neural approaches whose lack of interpretability prohibits both straightforward integration and diagnosis for further improvement. We target the task of generating token-level explanations for NLI from a neural model. Many existing approaches for token-level explanation are either computationally costly or require additional annotations for training. In this article, we first introduce a novel method for training an explanation generator that does not require additional human labels. Instead, the explanation generator is trained with the objective of predicting how the model's classification output will change when parts of the inputs are modified. Second, we propose to build an explanation generator in a multi-Task learning setting along with the original NLI task so the explanation generator can utilize the model's internal behavior. The experiment results suggest that the proposed explanation generator outperforms numerous strong baselines. In addition, our method does not require excessive additional computation at prediction time, which renders it an order of magnitude faster than the best-performing baseline. © 2020 ACM.",interpretable machine learning; Natural language inference; neural network explanation; rationale,Multi-task learning; Interpretability; Natural languages; Neural modeling; Prediction time; Text-matching; Learning systems
Fine-Grained Privacy Detection with Graph-Regularized Hierarchical Attentive Representation Learning,2020,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85093948067&doi=10.1145%2f3406109&partnerID=40&md5=d50f987474b4deed9c01a57898cace4e,"Due to the complex and dynamic environment of social media, user generated contents (UGCs) may inadvertently leak users' personal aspects, such as the personal attributes, relationships and even the health condition, and thus place users at high privacy risks. Limited research efforts, thus far, have been dedicated to the privacy detection from users' unstructured data (i.e., UGCs). Moreover, existing efforts mainly focus on applying conventional machine learning techniques directly to traditional hand-crafted privacy-oriented features, ignoring the powerful representing capability of the advanced neural networks. In light of this, in this article, we present a fine-grained privacy detection network (GrHA) equipped with graph-regularized hierarchical attentive representation learning. In particular, the proposed GrHA explores the semantic correlations among personal aspects with graph convolutional networks to enhance the regularization for the UGC representation learning, and, hence, fulfil effective fine-grained privacy detection. Extensive experiments on a real-world dataset demonstrate the superiority of the proposed model over state-of-The-Art competitors in terms of eight standard metrics. As a byproduct, we have released the codes and involved parameters to facilitate the research community. © 2020 ACM.",Fine-grained privacy detection; graph convolutional networks; hierarchical attention mechanism,Convolutional neural networks; Health risks; Privacy by design; Semantics; Conventional machines; Convolutional networks; Detection networks; Dynamic environments; Personal attributes; Research communities; Unstructured data; User-generated content; Learning systems
MergeDTS,2020,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092716898&doi=10.1145%2f3411753&partnerID=40&md5=b5b2c18c016eef37c1928dd14709cdf2,"Online ranker evaluation is one of the key challenges in information retrieval. Although the preferences of rankers can be inferred by interleaving methods, the problem of how to effectively choose the ranker pair that generates the interleaved list without degrading the user experience too much is still challenging. On the one hand, if two rankers have not been compared enough, the inferred preference can be noisy and inaccurate. On the other hand, if two rankers are compared too many times, the interleaving process inevitably hurts the user experience too much. This dilemma is known as the exploration versus exploitation tradeoff. It is captured by the K-Armed dueling bandit problem, which is a variant of the K-Armed bandit problem, where the feedback comes in the form of pairwise preferences. Today's deployed search systems can evaluate a large number of rankers concurrently, and scaling effectively in the presence of numerous rankers is a critical aspect of K-Armed dueling bandit problems. In this article, we focus on solving the large-scale online ranker evaluation problem under the so-called Condorcet assumption, where there exists an optimal ranker that is preferred to all other rankers. We propose Merge Double Thompson Sampling (MergeDTS), which first utilizes a divide-And-conquer strategy that localizes the comparisons carried out by the algorithm to small batches of rankers, and then employs Thompson Sampling to reduce the comparisons between suboptimal rankers inside these small batches. The effectiveness (regret) and efficiency (time complexity) of MergeDTS are extensively evaluated using examples from the domain of online evaluation for web search. Our main finding is that for large-scale Condorcet ranker evaluation problems, MergeDTS outperforms the state-of-The-Art dueling bandit algorithms. © 2020 ACM.",dueling bandits; implicit feedback; Online evaluation; preference learning,Probability; Divide and conquer; Evaluation problems; Interleaving methods; K-armed bandit problem; On-line evaluation; State of the art; Thompson samplings; Time complexity; User experience
FNED: A Deep Network for Fake News Early Detection on Social Media,2020,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088296412&doi=10.1145%2f3386253&partnerID=40&md5=42119e99a60369088a9bed6b29cae208,"The fast spreading of fake news stories on social media can cause inestimable social harm. Developing effective methods to detect them early is of paramount importance. A major challenge of fake news early detection is fully utilizing the limited data observed at the early stage of news propagation and then learning useful patterns from it for identifying fake news. In this article, we propose a novel deep neural network to detect fake news early. It has three novel components: (1) a status-sensitive crowd response feature extractor that extracts both text features and user features from combinations of users' text response and their corresponding user profiles, (2) a position-aware attention mechanism that highlights important user responses at specific ranking positions, and (3) a multi-region mean-pooling mechanism to perform feature aggregation based on multiple window sizes. Experimental results on two real-world datasets demonstrate that our proposed model can detect fake news with greater than 90% accuracy within 5 minutes after it starts to spread and before it is retweeted 50 times, which is significantly faster than state-of-the-art baselines. Most importantly, our approach requires only 10% labeled fake news samples to achieve this effectiveness under PU-Learning settings.  © 2020 ACM.",deep learning; Fake news detection; social media,Backpropagation; Deep learning; Social networking (online); Attention mechanisms; Feature aggregation; Multiple windows; Novel component; Real-world datasets; Response features; State of the art; Useful patterns; Deep neural networks
Robust Unsupervised Cross-modal Hashing for Multimedia Retrieval,2020,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088299364&doi=10.1145%2f3389547&partnerID=40&md5=7c472e973f06a9c16f38bf2387c9497a,"With the quick development of social websites, there are more opportunities to have different media types (such as text, image, video, etc.) describing the same topic from large-scale heterogeneous data sources. To efficiently identify the inter-media correlations for multimedia retrieval, unsupervised cross-modal hashing (UCMH) has gained increased interest due to the significant reduction in computation and storage. However, most UCMH methods assume that the data from different modalities are well paired. As a result, existing UCMH methods may not achieve satisfactory performance when partially paired data are given only. In this article, we propose a new-type of UCMH method called robust unsupervised cross-modal hashing (RUCMH). The major contribution lies in jointly learning modal-specific hash function, exploring the correlations among modalities with partial or even without any pairwise correspondence, and preserving the information of original features as much as possible. The learning process can be modeled via a joint minimization problem, and the corresponding optimization algorithm is presented. A series of experiments is conducted on four real-world datasets (Wiki, MIRFlickr, NUS-WIDE, and MS-COCO). The results demonstrate that RUCMH can significantly outperform the state-of-the-art unsupervised cross-modal hashing methods, especially for the partially paired case, which validates the effectiveness of RUCMH.  © 2020 ACM.",cross-modal hashing; Multimedia retrieval; partially paired data; unsupervised learning,Digital storage; Hash functions; Learning algorithms; Hashing method; Heterogeneous data sources; Learning process; Minimization problems; Multimedia Retrieval; Optimization algorithms; Real-world datasets; State of the art; Learning systems
Safe Exploration for Optimizing Contextual Bandits,2020,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088292625&doi=10.1145%2f3385670&partnerID=40&md5=ab1b7abd3c7f4e33d5f13003e840ebbc,"Contextual bandit problems are a natural fit for many information retrieval tasks, such as learning to rank, text classification, recommendation, and so on. However, existing learning methods for contextual bandit problems have one of two drawbacks: They either do not explore the space of all possible document rankings (i.e., actions) and, thus, may miss the optimal ranking, or they present suboptimal rankings to a user and, thus, may harm the user experience. We introduce a new learning method for contextual bandit problems, Safe Exploration Algorithm (SEA), which overcomes the above drawbacks. SEA starts by using a baseline (or production) ranking system (i.e., policy), which does not harm the user experience and, thus, is safe to execute but has suboptimal performance and, thus, needs to be improved. Then SEA uses counterfactual learning to learn a new policy based on the behavior of the baseline policy. SEA also uses high-confidence off-policy evaluation to estimate the performance of the newly learned policy. Once the performance of the newly learned policy is at least as good as the performance of the baseline policy, SEA starts using the new policy to execute new actions, allowing it to actively explore favorable regions of the action space. This way, SEA never performs worse than the baseline policy and, thus, does not harm the user experience, while still exploring the action space and, thus, being able to find an optimal policy. Our experiments using text classification and document retrieval confirm the above by comparing SEA (and a boundless variant called BSEA) to online and offline learning methods for contextual bandit problems.  © 2020 ACM.",Counterfactual learning; Exploration; Learning to rank,Classification (of information); Information retrieval; Information retrieval systems; Learning systems; Probability; Text processing; User experience; Contextual bandits; Document Retrieval; Exploration algorithms; Learning to learn; Off-line learning; Policy evaluation; Sub-optimal performance; Text classification; Learning to rank
Dual-factor Generation Model for Conversation,2020,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088323287&doi=10.1145%2f3394052&partnerID=40&md5=63399fbee459c53bd4361d1f85b3fdcc,"The conversation task is usually formulated as a conditional generation problem, i.e., to generate a natural and meaningful response given the input utterance. Generally speaking, this formulation is apparently based on an oversimplified assumption that the response is solely dependent on the input utterance. It ignores the subjective factor of the responder, e.g., his/her emotion or knowledge state, which is a major factor that affects the response in practice. Without explicitly differentiating such subjective factor behind the response, existing generation models can only learn the general shape of conversations, leading to the blandness problem of the response. Moreover, there is no intervention mechanism within the existing generation process, since the response is fully decided by the input utterance. In this work, we propose to view the conversation task as a dual-factor generation problem, including an objective factor denoting the input utterance and a subjective factor denoting the responder state. We extend the existing neural sequence-to-sequence (Seq2Seq) model to accommodate the responder state modeling. We introduce two types of responder state, i.e., discrete and continuous state, to model emotion state and topic preference state, respectively. We show that with our dual-factor generation model, we can not only better fit the conversation data, but also actively control the generation of the response with respect to sentiment or topic specificity.  © 2020 ACM.",Conversation; dual-factor generation; responder state modeling,Information systems; Conditional generation; Continuous state; Generation process; Knowledge state; Major factors; Modeling emotions; State modeling; Topic preferences; Computer networks
Challenges in Building Intelligent Open-domain Dialog Systems,2020,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087158672&doi=10.1145%2f3383123&partnerID=40&md5=025f7ce6791d601f136210ea780fe3d4,"There is a resurgent interest in developing intelligent open-domain dialog systems due to the availability of large amounts of conversational data and the recent progress on neural approaches to conversational AI [33]. Unlike traditional task-oriented bots, an open-domain dialog system aims to establish long-term connections with users by satisfying the human need for communication, affection, and social belonging. This article reviews the recent work on neural approaches that are devoted to addressing three challenges in developing such systems: Semantics, consistency, and interactiveness. Semantics requires a dialog system to not only understand the content of the dialog but also identify users' emotional and social needs during the conversation. Consistency requires the system to demonstrate a consistent personality to win users' trust and gain their long-term confidence. Interactiveness refers to the system's ability to generate interpersonal responses to achieve particular social goals such as entertainment and conforming. The studies we select to present in this survey are based on our unique views and are by no means complete. Nevertheless, we hope that the discussion will inspire new research in developing more intelligent open-domain dialog systems.  © 2020 ACM.",chatbot; conversation generation; conversational AI; Dialog system; response generation; social bot,Computer networks; Information systems; Dialog systems; Human needs; In-buildings; Large amounts; Recent progress; Social goals; Social needs; Task-oriented; Semantics
Towards Question-based High-recall Information Retrieval,2020,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088311172&doi=10.1145%2f3388640&partnerID=40&md5=c544239509203acb953724afaade4a38,"While continuous active learning algorithms have proven effective in finding most of the relevant documents in a collection, the cost for locating the last few remains high for applications such as Technology-assisted Reviews (TAR). To locate these last few but significant documents efficiently, Zou et al. [2018] have proposed a novel interactive algorithm. The algorithm is based on constructing questions about the presence or absence of entities in the missing relevant documents. The hypothesis made is that entities play a central role in documents carrying key information and that the users are able to answer questions about the presence or absence of an entity in the missing relevance documents. Based on this, a Sequential Bayesian Search-based approach that selects the optimal sequence of questions to ask was devised. In this work, we extend Zou et al. [2018] by (a) investigating the noise tolerance of the proposed algorithm; (b) proposing an alternative objective function to optimize, which accounts for user ""erroneous""answers; (c) proposing a method that sequentially decides the best point to stop asking questions to the user; and (d) conducting a small user study to validate some of the assumptions made by Zou et al. [2018]. Furthermore, all experiments are extended to demonstrate the effectiveness of the proposed algorithms not only in the phase of abstract appraisal (i.e., finding the abstracts of potentially relevant documents in a collection) but also finding the documents to be included in the review (i.e., finding the subset of those relevant abstracts for which the article remains relevant). The experimental results demonstrate that the proposed algorithms can greatly improve performance, requiring reviewing fewer irrelevant documents to find the last relevant ones compared to state-of-the-art methods, even in the case of noisy answers. Further, they show that our algorithm learns to stop asking questions at the right time. Last, we conduct a small user study involving an expert reviewer. The user study validates some of the assumptions made in this work regarding the user's willingness to answer the system questions and the extent of it, as well as the ability of the user to answer these questions.  © 2020 ACM.",asking questions; interactive search; SBSTAR; SBSTAR<sub>ext</sub>; Technology-assisted reviews,Abstracting; Active-learning algorithm; Improve performance; Interactive algorithms; Noise tolerance; Objective functions; Optimal sequence; Relevant documents; State-of-the-art methods; Learning algorithms
Nonuniform Hyper-Network Embedding with Dual Mechanism,2020,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088317534&doi=10.1145%2f3388924&partnerID=40&md5=96d0b5c010387a659e8fd28e1d12f29a,"Network embedding which aims to learn the low-dimensional representations for vertices in networks has been extensively studied in recent years. Although there are various models designed for networks with different properties and different structures for different tasks, most of them are only applied to normal networks which only contain pairwise relationships between vertices. In many realistic cases, relationships among objects are not pairwise and such relationships can be better modeled by a hyper-network in which each edge can connect an uncertain number of vertices. In this article, we focus on two properties of hyper-networks: Nonuniform and dual property. In order to make full use of these two properties, we firstly propose a flexible model called Hyper2vec to learn the embeddings of hyper-networks by applying a biased second order random walk strategy to hyper-networks in the framework of Skip-gram. Then, we combine the features of hyperedges by considering the dual hyper-networks to build a further model called NHNE based on 1D convolutional neural networks, and train a tuplewise similarity function for the nonuniform relationships in hyper-networks. Extensive experiments demonstrate the significant effectiveness of our methods for hyper-network embedding.  © 2020 ACM.",dual network; hyper-network; link prediction; Network embedding,Embeddings; Different structure; Dual mechanisms; Flexible model; Low-dimensional representation; Network embedding; Random walk strategies; Second orders; Similarity functions; Convolutional neural networks
Keeping the Data Lake in Form,2020,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088311136&doi=10.1145%2f3388870&partnerID=40&md5=452f6a3dd4b6d01607529911b9063151,"Data lakes (DLs) are large repositories of raw datasets from disparate sources. As more datasets are ingested into a DL, there is an increasing need for efficient techniques to profile them and to detect the relationships among their schemata, commonly known as holistic schema matching. Schema matching detects similarity between the information stored in the datasets to support information discovery and retrieval. Currently, this is computationally expensive with the volume of state-of-the-art DLs. To handle this challenge, we propose a novel early-pruning approach to improve efficiency, where we collect different types of content metadata and schema metadata about the datasets, and then use this metadata in early-pruning steps to pre-filter the schema matching comparisons. This involves computing proximities between datasets based on their metadata, discovering their relationships based on overall proximities and proposing similar dataset pairs for schema matching. We improve the effectiveness of this task by introducing a supervised mining approach for effectively detecting similar datasets that are proposed for further schema matching. We conduct extensive experiments on a real-world DL that proves the success of our approach in effectively detecting similar datasets for schema matching, with recall rates of more than 85% and efficiency improvements above 70%. We empirically show the computational cost saving in space and time by applying our approach in comparison to instance-based schema matching techniques.  © 2020 ACM.",content metadata management; data governance; Data lake; dataset similarity mining; early pruning; holistic schema matching,Efficiency; Filtration; Lakes; Metadata; Computational costs; Early pruning; Efficiency improvement; Holistic schema matching; Information discovery; Schema matching; Space and time; State of the art; Large dataset
Using an Inverted Index Synopsis for Query Latency and Performance Prediction,2020,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088295539&doi=10.1145%2f3389795&partnerID=40&md5=b38b1612497d6e45c5eb1ac46e2312c6,"Predicting the query latency by a search engine has important benefits, for instance, in allowing the search engine to adjust its configuration to address long-running queries without unnecessarily sacrificing its effectiveness. However, for the dynamic pruning techniques that underlie many commercial search engines, achieving accurate predictions of query latencies is difficult. We propose the use of index synopses-which are stochastic samples of the full index-for attaining accurate timing predictions. Indeed, we experiment using the TREC ClueWeb09 collection, and a large set of real user queries, and find that using small index synopses it is possible to very accurately estimate properties of the larger index, including sizes of posting list unions and intersections. Thereafter, we demonstrate that index synopses facilitate two key use cases: First, for query efficiency prediction, we show that predicting the query latencies on the full index and classifying long-running queries can be accurately achieved using index synopses; second, for query performance prediction, we show that the effectiveness of queries can be estimated more accurately using a synopsis index post-retrieval predictor than a pre-retrieval predictor. Overall, our experiments demonstrate the value of such a stochastic sample of a larger index at predicting the properties of the larger index.  © 2020 ACM.",dynamic pruning; Inverted index synopsis; query efficiency prediction; query performance prediction,Search engines; Stochastic systems; Accurate prediction; Accurate timing; Dynamic pruning; Inverted indices; Performance prediction; Query efficiency; Query latency; Query performance prediction; Forecasting
Exploiting Cross-session Information for Session-based Recommendation with Graph Neural Networks,2020,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088287935&doi=10.1145%2f3382764&partnerID=40&md5=1bf45cc69cac58357ce487cd3f57ad28,"Different from the traditional recommender system, the session-based recommender system introduces the concept of the session, i.e., a sequence of interactions between a user and multiple items within a period, to preserve the user's recent interest. The existing work on the session-based recommender system mainly relies on mining sequential patterns within individual sessions, which are not expressive enough to capture more complicated dependency relationships among items. In addition, it does not consider the cross-session information due to the anonymity of the session data, where the linkage between different sessions is prevented. In this article, we solve these problems with the graph neural networks technique. First, each session is represented as a graph rather than a linear sequence structure, based on which a novel Full Graph Neural Network (FGNN) is proposed to learn complicated item dependency. To exploit and incorporate cross-session information in the individual session's representation learning, we further construct a Broadly Connected Session (BCS) graph to link different sessions and a novel Mask-Readout function to improve session embedding based on the BCS graph. Extensive experiments have been conducted on two e-commerce benchmark datasets, i.e., Yoochoose and Diginetica, and the experimental results demonstrate the superiority of our proposal through comparisons with state-of-the-art session-based recommender models.  © 2020 ACM.",graph neural networks; Recommender system; session-based recommendation,Electronic commerce; Recommender systems; Benchmark datasets; Dependency relationship; Graph neural networks; Linear sequence; Mining sequential patterns; Multiple items; State of the art; Neural networks
OutdoorSent: Sentiment Analysis of Urban Outdoor Images by Using Semantic and Deep Features,2020,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086300705&doi=10.1145%2f3385186&partnerID=40&md5=52aa8336e6e457eeba9b0509eb880bed,"Opinion mining in outdoor images posted by users during different activities can provide valuable information to better understand urban areas. In this regard, we propose a framework to classify the sentiment of outdoor images shared by users on social networks. We compare the performance of state-of-the-art ConvNet architectures and one specifically designed for sentiment analysis. We also evaluate how the merging of deep features and semantic information derived from the scene attributes can improve classification and cross-dataset generalization performance. The evaluation explores a novel dataset-namely, OutdoorSent- A nd other publicly available datasets. We observe that the incorporation of knowledge about semantic attributes improves the accuracy of all ConvNet architectures studied. Besides, we found that exploring only images related to the context of the study-outdoor, in our case-is recommended, i.e., indoor images were not significantly helpful. Furthermore, we demonstrated the applicability of our results in the United States city of Chicago, Illinois, showing that they can help to improve the knowledge of subjective characteristics of different areas of the city. For instance, particular areas of the city tend to concentrate more images of a specific class of sentiment, which are also correlated with median income, opening up opportunities in different fields.  © 2020 ACM.",deep learning; image processing; information retrieval; location-based social networks; Sentiment analysis,Convolutional neural networks; Data mining; Network architecture; Semantics; Sentiment analysis; Generalization performance; Opinion mining; Outdoor images; Semantic attribute; Semantic information; Specific class; State of the art; Urban outdoor; Classification (of information)
Emotion Dynamics of Public Opinions on Twitter,2020,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082384720&doi=10.1145%2f3379340&partnerID=40&md5=2148cc5f34873ac96880efa81e0fcf3c,"Recently, social media has been considered the fastest medium for information broadcasting and sharing. Considering the wide range of applications such as viralmarketing, political campaigns, social advertisement, and so on, influencing characteristics of users or tweets have attracted several researchers. It is observed from various studies that influential messages or users create a high impact on a social ecosystem. In this study, we assume that public opinion on a social issue on Twitter carries a certain degree of emotion, and there is an emotion flow underneath the Twitter network. In this article, we investigate social dynamics of emotion present in users' opinions and attempt to understand (i) changing characteristics of users' emotions toward a social issue over time, (ii) influence of public emotions on individuals' emotions, (iii) cause of changing opinion by social factors, and so on. We study users' emotion dynamics over a collection of 17.65M tweets with 69.36K users and observe 63% of the users are likely to change their emotional state against the topic into their subsequent tweets. Tweets were coming from the member community shows higher influencing capability than the other community sources. It is also observed that retweets influence users more than hashtags, mentions, and replies. © 2020 Association for Computing Machinery.",Emotion transition; influence measure; opinion discussion; social agreement; social dynamics,Social aspects; Social networking (online); Emotion transition; influence measure; opinion discussion; Social agreement; Social dynamics; Dynamics
Learning Semantic Representations from Directed Social Links to Tag Microblog Users at Scale,2020,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082388035&doi=10.1145%2f3377550&partnerID=40&md5=14123ec86740fce47177b8e0b1e5efa5,"This article presents a network embedding approach to automatically generate tags for microblog users. Instead of using text data, we aim to annotate microblog users with meaningful tags by leveraging rich social link data. To utilize directed social links, we use two kinds of node representations for modeling user interest in terms of their followers and followees, respectively. To alleviate the sparsity problem, we propose a novel method based on two transformation functions for capturing implicit interest similarity. Different from previous works on capturing high-order proximity, our model is able to directly characterize the effect of the context user on the proximity of node pairs. Another novelty of our model is that the importance scores of users learned from the classic PageRank algorithm are utilized to set the link weights. By using such weights, our model is more capable of disentangling the interest similarity evidence of a link. We jointly consider the above factors when designing the final objective function. We construct a very large evaluation set consisting of 2.6M users, 0.5M tags, and 0.8B following links. To our knowledge, it is the largest reported dataset for microblog user tagging in the literature. Extensive experiments on this dataset demonstrate the effectiveness of the proposed approach. We implement this approach with several optimization techniques, which makes our model easy to scale to very large social networks. Ubiquitous social links provide important data resources to understand user interests. Our work provides an effective and efficient solution to annotate user interests solely using the link data, which has important practical value in industry. To illustrate the use of our models, we implement a demonstration system for visualizing, navigating, and searching microblog users. © 2020 Association for Computing Machinery.",Microblog user tagging; Network embedding; Social importance,Embeddings; Implicit interests; Learning semantics; Micro-blog; Network embedding; Optimization techniques; PageRank algorithm; Social importance; Transformation functions; Semantics
An Enhanced Neural Network Approach to Person-Job Fit in Talent Recruitment,2020,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082387972&doi=10.1145%2f3376927&partnerID=40&md5=05db21462e97616f58b2d7ae8b5651d7,"The widespread use of online recruitment services has led to an information explosion in the job market. As a result, recruiters have to seek intelligent ways for Person-Job Fit, which is the bridge for adapting the right candidates to the right positions. Existing studies on Person-Job Fit usually focus on measuring the matching degree between talent qualification and job requirements mainly based on the manual inspection of human resource experts, which could be easily misguided by the subjective, incomplete, and inefficient nature of human judgment. To that end, in this article, we propose a novel end-to-end Topic-based Abilityaware Person-Job Fit Neural Network (TAPJFNN) framework, which has a goal of reducing the dependence on manual labor and can provide better interpretability about the fitting results. The key idea is to exploit the rich information available in abundant historical job application data. Specifically, we propose a word-level semantic representation for both job requirements and job seekers' experiences based on Recurrent Neural Network (RNN). Along this line, two hierarchical topic-based ability-aware attention strategies are designed to measure the different importance of job requirements for semantic representation, as well as measure the different contribution of each job experience to a specific ability requirement. In addition, we design a refinement strategy for Person-Job Fit prediction based on historical recruitment records. Furthermore, we introduce how to exploit our TAPJFNN framework for enabling two specific applications in talent recruitment: talent sourcing and job recommendation. Particularly, in the application of job recommendation, a novel training mechanism is designed for addressing the challenge of biased negative labels. Finally, extensive experiments on a large-scale real-world dataset clearly validate the effectiveness and interpretability of the TAPJFNN and its variants compared with several baselines. © 2020 Association for Computing Machinery.",neural network; person-job fit; Recruitment analysis,Large dataset; Neural networks; Semantics; Information explosion; Manual inspection; Online recruitment; person-job fit; Recruitment analysis; Recurrent neural network (RNN); Refinement strategy; Semantic representation; Recurrent neural networks
Large-Scale Question Tagging via Joint Question-Topic Embedding Learning,2020,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082390688&doi=10.1145%2f3380954&partnerID=40&md5=1054eea480b03e84105282f465dfd5e4,"Recent years have witnessed a flourishing of community-driven question answering (cQA), like Yahoo! Answers and AnswerBag, where people can seek precise information. After 2010, some novel cQA systems, including Quora and Zhihu, gained momentum. Besides interactions, the latter enables users to label the questions with topic tags that highlight the key points conveyed in the questions. In this article, we shed light on automatically annotating a newly posted question with topic tags that are predefined and preorganized into a directed acyclic graph. To accomplish this task, we present an end-to-end deep interactive embedding model to jointly learn the embeddings of questions and topics by projecting them into the same space for a similarity measure. In particular, we first learn the embeddings of questions and topic tags by two deep parallel models. Thereinto, we regularize the embeddings of topic tags via fully exploring their hierarchical structures, which is able to alleviate the problem of imbalanced topic distribution. Thereafter, we interact each question embedding with the topic tag matrix, i.e., all the topic tag embeddings. Following that, a sigmoid cross-entropy loss is appended to reward the positive question-topic pairs and penalize the negative ones. To justify our model, we have conducted extensive experiments on an unprecedented largescale social QA dataset obtained from Zhihu.com, and the experimental results demonstrate that our model achieves superior performance to several state-of-the-art baselines. © 2020 Association for Computing Machinery.",CQA; embedding learning; Question tagging; topic hierarchy,Directed graphs; Community-driven question answering; Directed acyclic graph (DAG); embedding learning; Hierarchical structures; Question tagging; Similarity measure; Topic distributions; Topic hierarchy; Embeddings
Learning or Forgetting? A Dynamic Approach for Tracking the Knowledge Proficiency of Students,2020,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082382138&doi=10.1145%2f3379507&partnerID=40&md5=a442f958aa2cd9b19b72898beb31864d,"The rapid development of the technologies for online learning provides students with extensive resources for self-learning and brings new opportunities for data-driven research on educational management. An important issue of online learning is to diagnose the knowledge proficiency (i.e., the mastery level of a certain knowledge concept) of each student. Considering that it is a common case that students inevitably learn and forget knowledge from time to time, it is necessary to track the change of their knowledge proficiency during the learning process. Existing approaches either relied on static scenarios or ignored the interpretability of diagnosis results. To address these problems, in this article, we present a focused study on diagnosing the knowledge proficiency of students, where the goal is to track and explain their evolutions simultaneously. Specifically, we first devise an explanatory probabilistic matrix factorization model, Knowledge Proficiency Tracing (KPT), by leveraging educational priors. KPT model first associates each exercise with a knowledge vector in which each element represents a specific knowledge conceptwith the help of Q-matrix. Correspondingly, at each time, each student can be represented as a proficiency vector in the same knowledge space. Then, our KPT model jointly applies two classical educational theories (i.e., learning curve and forgetting curve) to capture the change of students' proficiency level on concepts over time. Furthermore, for improving the predictive performance, we develop an improved version of KPT, named Exercise-correlated Knowledge Proficiency Tracing (EKPT), by considering the connectivity among exercises with the same knowledge concepts. Finally, we apply our KPT and EKPT models to three important diagnostic tasks, including knowledge estimation, score prediction, and diagnosis result visualization. Extensive experiments on four real-world datasets demonstrate that both of our models could track the knowledge proficiency of students effectively and interpretatively. © 2020 Association for Computing Machinery.",Diagnosis; educational theories; knowledge proficiency levels,Diagnosis; E-learning; Factorization; Knowledge management; Learning systems; Research and development management; Vector spaces; Educational management; Educational theory; Extensive resources; Predictive performance; Probabilistic matrix factorizations; Proficiency level; Real-world datasets; Specific knowledge; Students
Local Variational Feature-Based Similarity Models for Recommending Top-N New Items,2020,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082399189&doi=10.1145%2f3372154&partnerID=40&md5=71a489653a0ca2b851139b5114986096,"The top-N recommendation problem has been studied extensively. Item-based collaborative filtering recommendation algorithms show promising results for the problem. They predict a user's preferences by estimating similarities between a target and user-rated items. Top-N recommendation remains a challenging task in scenarios where there is a lack of preference history for new items. Feature-based Similarity Models (FSMs) address this particular problem by extending item-based collaborative filtering by estimating similarity functions of item features. The quality of the estimated similarity function determines the accuracy of the recommendation. However, existing FSMs only estimate global similarity functions; i.e., they estimate using preference information across all users. Moreover, the estimated similarity functions are linear; hence, they may fail to capture the complex structure underlying item features. In this article, we propose to improve FSMs by estimating local similarity functions, where each function is estimated for a subset of like-minded users. To capture global preference patterns, we extend the global similarity function from linear to nonlinear, based on the effectiveness of variational autoencoders. We propose a Bayesian generativemodel, called the Local Variational Feature-based Similarity Model, to encapsulate local and global similarity functions. We present a variational Expectation Minimization algorithm for efficient approximate inference. Extensive experiments on a large number of real-world datasets demonstrate the effectiveness of our proposed model. © 2020 Association for Computing Machinery.",deep generative model; item cold-start; item feature; Top-N recommendation,Inference engines; Large dataset; Cold start; Expectation-minimization algorithm; Feature-based similarities; Generative model; item feature; Item-based collaborative filtering; Preference information; Top-N recommendation; Collaborative filtering
Improving implicit recommender systems with auxiliary data,2020,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079399348&doi=10.1145%2f3372338&partnerID=40&md5=aa7997ac49f9f3090b183b6c223cae28,"Most existing recommender systems leverage the primary feedback only, despite the fact that users also generate a large amount of auxiliary feedback. These feedback usually indicate different user preferences when comparing to the primary feedback directly used to optimize the system performance. For example, in E-commerce sites, view data is easily accessible, which provides a valuable yet weaker signal than the primary feedback of purchase. In this work, we improve implicit feedback-based recommender systems (dubbed Implicit Recommender Systems) by integrating auxiliary view data into matrix factorization (MF). To exploit different preference levels, we propose both pointwise and pairwise models in terms of how to leverage users' viewing behaviors. The latter model learns the pairwise ranking relations among purchased, viewed, and non-viewed interactions, being more effective and flexible than the former pointwise MF method. However, such a pairwise formulation poses a computational efficiency problem in learning the model. To address this problem, we design a new learning algorithm based on the element-wise Alternating Least Squares (eALS) learner. Notably, our designed algorithm can efficiently learn model parameters from the whole user-item matrix (including all missing data), with a rather low time complexity that is dependent on the observed data only. Extensive experiments on two real-world datasets demonstrate that our method outperforms several state-of-the-art MF methods by 6.43%~6.75%. Our implementation is available at https://github.com/dingjingtao/Auxiliary_enhanced_ALS. © 2020 Association for Computing Machinery. All rights reserved.",Auxiliary feedback; EALS; Implicit feedback; Item recommendation; Matrix factorization,Computational efficiency; Electronic commerce; Factorization; Learning algorithms; Recommender systems; Alternating least squares; E-commerce sites; EALS; Implicit feedback; Item recommendation; Matrix factorizations; Model parameters; Real-world datasets; Matrix algebra
A deep learning architecture for psychometric natural language processing,2020,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079433738&doi=10.1145%2f3365211&partnerID=40&md5=0b3b51b8565213e022c2a50f71f413d4,"Psychometric measures reflecting people's knowledge, ability, attitudes, and personality traits are critical for many real-world applications, such as e-commerce, health care, and cybersecurity. However, traditional methods cannot collect and measure rich psychometric dimensions in a timely and unobtrusive manner. Consequently, despite their importance, psychometric dimensions have received limited attention from the natural language processing and information retrieval communities. In this article, we propose a deep learning architecture, PyNDA, to extract psychometric dimensions from user-generated texts. PyNDA contains a novel representation embedding, a demographic embedding, a structural equation model (SEM) encoder, and a multitask learning mechanism designed to work in unison to address the unique challenges associated with extracting rich, sophisticated, and user-centric psychometric dimensions. Our experiments on three real-world datasets encompassing 11 psychometric dimensions, including trust, anxiety, and literacy, show that PyNDA markedly outperforms traditional feature-based classifiers as well as the state-of-the-art deep learning architectures. Ablation analysis reveals that each component of PyNDA significantly contributes to its overall performance. Collectively, the results demonstrate the efficacy of the proposed architecture for facilitating rich psychometric analysis. Our results have important implications for user-centric information extraction and retrieval systems looking to measure and incorporate psychometric dimensions. © 2020 Copyright held by the owner/author(s).",Deep learning; Natural language processing; Psychometric measures; Text classification,Architecture; Behavioral research; Classification (of information); Embeddings; Information retrieval; Learning algorithms; Learning systems; Natural language processing systems; Search engines; Text processing; Learning architectures; NAtural language processing; Proposed architectures; Psychometric analysis; Psychometric measures; Real-world datasets; Structural equation modeling; Text classification; Deep learning
A multi-label classification method using a hierarchical and transparent representation for paper-reviewer recommendation,2020,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079402520&doi=10.1145%2f3361719&partnerID=40&md5=493bf3160306256d4edeb51dfe7b8609,"The paper-reviewer recommendation task is of significant academic importance for conference chairs and journal editors. It aims to recommend appropriate experts in a discipline to comment on the quality of papers of others in that discipline. How to effectively and accurately recommend reviewers for the submitted papers is a meaningful and still tough task. Generally, the relationship between a paper and a reviewer often depends on the semantic expressions of them. Creating a more expressive representation can make the peer-review process more robust and less arbitrary. So the representations of a paper and a reviewer are very important for the paper-reviewer recommendation. Actually, a reviewer or a paper often belongs to multiple research fields, which increases difficulty in paper-reviewer recommendation. In this article, we propose a Multi-Label Classification method using a HIErarchical and transPArent Representation named Hiepar-MLC. First, we introduce HIErarchical and transPArent Representation (Hiepar) to express the semantic information of the reviewer and the paper. Hiepar is learned from a two-level bidirectional gated recurrent unit based network applying the attention mechanism. It is capable of capturing the two-level hierarchical information (word-sentence-document) and highlighting the elements in reviewers or papers to support the labels. This word-sentence-document information mirrors the hierarchical structure of a reviewer or a paper and captures the exact semantics of them. Then we transform the paper-reviewer recommendation problem into a multi-level classification issue, whose multiple research labels exactly guide the learning process. It is flexible in that we can select any multi-label classification method to solve the paper-reviewer recommendation problem. Further, we propose a simple multi-label-based reviewer assignment (MLBRA) strategy to select the appropriate reviewers. It is interesting in that we also explore the paper-reviewer recommendation in the coarse-grain granularity. Extensive experiments on the real-world dataset consisting of the papers in the ACM Digital Library show that Hiepar-MLC achieves better label prediction performance than the existing representation alternatives. In addition, with the MLBRA strategy, we show the effectiveness and the feasibility of our transformation from paper-reviewer recommendation to multi-label classification. © 2020 Association for Computing Machinery.",ACM Digital Library; Hierarchical; Multi-label classification; Paper-reviewer recommendation; Transparent,Digital libraries; Semantics; Attention mechanisms; Hierarchical; Hierarchical information; Hierarchical structures; Multi label classification; Multi-level classifications; Semantic information; Transparent; Classification (of information)
Efficient neural matrix factorization without sampling for recommendation,2020,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078275737&doi=10.1145%2f3373807&partnerID=40&md5=0df13e9078923af43637b385aa56025a,"Recommendation systems play a vital role to keep users engaged with personalized contents in modern online platforms. Recently, deep learning has revolutionized many research fields and there is a surge of interest in applying it for recommendation. However, existing studies have largely focused on exploring complex deeplearning architectures for recommendation task, while typically applying the negative sampling strategy for model learning. Despite effectiveness, we argue that these methods suffer from two important limitations: (1) the methods with complex network structures have a substantial number of parameters, and require expensive computations even with a sampling-based learning strategy; (2) the negative sampling strategy is not robust,making sampling-based methods difficult to achieve the optimal performance in practical applications. In this work, we propose to learn neural recommendation models from the whole training data without sampling. However, such a non-sampling strategy poses strong challenges to learning efficiency. To address this, we derive three new optimization methods through rigorous mathematical reasoning, which can efficiently learn model parameters from the whole data (including all missing data) with a rather low time complexity. Moreover, based on a simple Neural Matrix Factorization architecture, we present a general framework named ENMF, short for Efficient Neural Matrix Factorization. Extensive experiments on three real-world public datasets indicate that the proposed ENMF framework consistently and significantly outperforms the state-of-the-art methods on the Top-K recommendation task. Remarkably, ENMF also shows significant advantages in training efficiency, which makes it more applicable to real-world large-scale systems. © 2020 Association for Computing Machinery.",Efficient learning; Implicit feedback; Matrix factorization; Neural networks; Recommendation system,Complex networks; Deep learning; Efficiency; Factorization; Importance sampling; Large scale systems; Network architecture; Neural networks; Online systems; Recommender systems; Sampling; Efficient learning; Implicit feedback; Mathematical reasoning; Matrix factorizations; Personalized content; Sampling-based method; State-of-the-art methods; Top-K recommendations; Matrix algebra
A Price-per-attention auction scheme using mouse cursor information,2020,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079410769&doi=10.1145%2f3374210&partnerID=40&md5=58817b9f1c5fe380169c87fca14d9262,"Payments in online ad auctions are typically derived from click-through rates, so that advertisers do not pay for ineffective ads. But advertisers often care about more than just clicks. That is, for example, if they aim to raise brand awareness or visibility. There is thus an opportunity to devise a more effective ad pricing paradigm, in which ads are paid only if they are actually noticed. This article contributes a novel auction format based on a pay-per-attention (PPA) scheme. We show that the PPA auction inherits the desirable properties (strategy-proofness and efficiency) as its pay-per-impression and pay-per-click counterparts, and that it also compares favourably in terms of revenues. To make the PPA format feasible, we also contribute a scalable diagnostic technology to predict user attention to ads in sponsored search using raw mouse cursor coordinates only, regardless of the page content and structure. We use the user attention predictions in numerical simulations to evaluate the PPA auction scheme. Our results show that, in relevant economic settings, the PPA revenues would be strictly higher than the existing auction payment schemes. © 2020 Copyright held by the owner/author(s).",Auctions; Direct displays; Mouse cursor analysis; Online advertising; Sponsored search; User attention,Commerce; Auctions; Mouse cursor; Online advertising; Sponsored searches; User attention; Mammals
Jointly learning representations of nodes and attributes for attributed networks,2020,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079431925&doi=10.1145%2f3377850&partnerID=40&md5=c7b175779de63e4ce032a487194f591f,"Previous embedding methods for attributed networks aim at learning low-dimensional vector representations only for nodes but not for both nodes and attributes, resulting in the fact that node embeddings cannot be directly used to recover the correlations between nodes and attributes. However, capturing such correlations by embeddings is of great importance for many real-world applications, such as attribute inference and user profiling. Moreover, in real-world scenarios, many attributed networks evolve over time, with their nodes, links, and attributes changing from time to time. In this article, we study the problem of jointly learning low-dimensional representations of both nodes and attributes for static and dynamic attributed networks. To address this problem, we propose a Co-embedding model for Static Attributed Networks (CSAN), which jointly learns low-dimensional representations of both attributes and nodes in the same semantic space such that their affinities can be effectively captured andmeasured, and a Co-embedding model for Dynamic Attributed Networks (CDAN) to dynamically track low-dimensional representations of nodes and attributes over time. To obtain effective embeddings, both our co-embedding models, CSAN and CDAN, embed each node and attribute with means and variances of Gaussian distributions via variational auto-encoders. Our CDAN model formulates the dynamic changes of a dynamic attributed network by aggregating perturbation features from the nodes' local neighborhoods as well as attributes' associations such that the evolving patterns of the given network can be tracked. Experimental results on real-world networks demonstrate that our proposed embedding models outperform state-of-the-art non-dynamic and dynamic embedding models. © 2020 Copyright held by the owner/author(s).",Attributed network; Dynamic embedding; Network embedding; Variational auto-encoder,Embeddings; Semantics; Signal encoding; Auto encoders; Dynamic embedding; Evolving patterns; Local neighborhoods; Low-dimensional representation; Network embedding; Real-world networks; Real-world scenario; Learning systems
Enhancing personalized recommendation by implicit preference communities modeling,2019,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077747085&doi=10.1145%2f3352592&partnerID=40&md5=c24a832e3b88f5ccbc9def8e337b7ee1,"Recommender systems aim to capture user preferences and provide accurate recommendations to users accordingly. For each user, there usually exist others with similar preferences, and a collection of users may also have similar preferences with each other, thus forming a community. However, such communities may not necessarily be explicitly given, and the users inside the same communities may not know each other; they are formally defined and named Implicit Preference Communities (IPCs) in this article. By enriching user preferences with the information of other users in the communities, the performance of recommender systems can also be enhanced. Historical explicit ratings are a good resource to construct the IPCs of users but is usually sparse. Meanwhile, user preferences are easily affected by their social connections, which can be jointly used for IPC modeling with the ratings. However, this imposes two challenges for model design. First, the rating and social domains are heterogeneous; thus, it is challenging to coordinate social information and rating behaviors for a same learning task. Therefore, transfer learning is a good strategy for IPC modeling. Second, the communities are not explicitly labeled, and existing supervised learning approaches do not fit the requirement of IPC modeling. As co-clustering is an effective unsupervised learning approach for discovering block structures in high-dimensional data, it is a cornerstone for discovering the structure of IPCs. In this article, we propose a recommendation model with Implicit Preference Communities from user ratings and social connections. To tackle the unsupervised learning limitation, we design a Bayesian probabilistic graphical model to capture the IPC structure for recommendation. Meanwhile, following the spirit of transfer learning, both rating behaviors and social connections are introduced into the model by parameter sharing. Moreover, Gibbs sampling-based algorithms are proposed for parameter inferences of the models. Furthermore, to meet the need for online scenarios when the data arrive sequentially as a stream, a novel online sampling-based parameter inference algorithm for recommendation is proposed. To the best of our knowledge, this is the first attempt to propose and formally define the concept of IPC. © 2019 Association for Computing Machinery.",Bayesian inference; Implicit preference community; Online learning; Recommendation; Transfer learning,Bayesian networks; Clustering algorithms; Inference engines; Machine learning; Parameter estimation; Quality of service; Recommender systems; Unsupervised learning; Bayesian inference; Implicit preference community; Online learning; Recommendation; Transfer learning; Structural design
The effects of task complexity on the use of different types of information in a search assistance tool,2019,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077751089&doi=10.1145%2f3371707&partnerID=40&md5=c77d71d6316219ba2600fa360523566b,"In interactive information retrieval, an important research question is: How do task characteristics influence users’ needs and behaviors? We report on a laboratory study (N = 32) that investigated the effects of task complexity on the types of information used by participants while searching. Participants completed tasks of four complexity levels and had access to four different types of information provided through a search-assistance tool referred to as the InfoBoxes (IB). The IB tool presented the following types of task-related information (info-types) on different tabs: (1) facts, (2) concepts, (3) opinions, and (4) insights. Facts (and opinions) were defined as objective (and subjective) statements relevant to the task. Concepts were defined as important ideas, principles, or entities related to the task. Insights were defined as tips or advice about the task. The study investigated six research questions that considered the effects of task complexity on: (RQ1) participants’ pre-/post-task perceptions about useful info-types; (RQ2) use of different info-types during the task; (RQ3) motivations for engaging with the IB; (RQ4) gains from using it; (RQ5) the search stage participants were in while engaging with the IB; and (RQ6) motivations for sometimes avoiding the IB. Our results suggest that task complexity influenced all six types of outcomes. We discuss implications of our results for designing search assistance tools and systems that favor certain types of content based on task characteristics. © 2019 Association for Computing Machinery.",Cognitive task complexity; Search assistance; Search behaviors,Computer networks; Information systems; Cognitive task; Complexity levels; Interactive information retrieval; Laboratory studies; Research questions; Search assistance; Search behavior; Task characteristics; Motivation
Next-item recommendation via collaborative filtering with bidirectional item similarity,2019,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077753604&doi=10.1145%2f3366172&partnerID=40&md5=d535fb0a6cc3ebf0948c82120379496e,"Exploiting temporal effect has empirically been recognized as a promising way to improve recommendation performance in recent years. In real-world applications, one-class data in the form of (user, item, timestamp) are usually more accessible and abundant than numerical ratings. In this article, we focus on exploiting such one-class data in order to provide personalized next-item recommendation services. Specifically, we base our work on the framework of time-aware item-based collaborative filtering and propose a simple yet effective similarity measurement called bidirectional item similarity (BIS) that is able to capture sequential patterns even from noisy data. Furthermore, we extend BIS via some factorization techniques and obtain an adaptive version, i.e., adaptive BIS (ABIS), in order to better fit the behavioral data. We also design a compound weighting function that leverages the complementarity between two well-known time-aware weighting functions. With the proposed similarity measurements and weighting function, we obtain two novel collaborative filtering methods that are able to achieve significantly better performance than the state-of-the-art methods, showcasing their effectiveness for next-item recommendation. © 2019 Association for Computing Machinery.",Bidirectional item similarity; Collaborative filtering; Matrix factorization; Next-item recommendation,Factorization; Collaborative filtering methods; Factorization techniques; Item similarity; Item-based collaborative filtering; Matrix factorizations; Next-item recommendation; Recommendation performance; State-of-the-art methods; Collaborative filtering
Emotional conversation generation based on a Bayesian deep neural network,2019,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077738395&doi=10.1145%2f3368960&partnerID=40&md5=e84bcd5ec75b7b4322b5fd977232f89e,"The field of conversation generation using neural networks has attracted increasing attention from researchers for several years. However, traditional neural language models tend to generate a generic reply with poor semantic logic and no emotion. This article proposes an emotional conversation generation model based on a Bayesian deep neural network that can generate replies with rich emotions, clear themes, and diverse sentences. The topic and emotional keywords of the replies are pregenerated by introducing commonsense knowledge in the model. The reply is divided into multiple clauses, and then a multidimensional generator based on the transformer mechanism proposed in this article is used to iteratively generate clauses from two dimensions: sentence granularity and sentence structure. Subjective and objective experiments prove that compared with existing models, the proposed model effectively improves the semantic logic and emotional accuracy of replies. This model also significantly enhances the diversity of replies, largely overcoming the shortcomings of traditional models that generate safe replies. © 2019 Association for Computing Machinery.",Affective computing; Bayesian neural network; Deep learning; Emotional conversation generation; Natural language processing,Computation theory; Computer circuits; Deep learning; Natural language processing systems; Neural networks; Semantics; Affective Computing; Bayesian neural networks; Commonsense knowledge; Emotional conversation generation; Model-based OPC; NAtural language processing; Sentence structures; Traditional models; Deep neural networks
Investigating searchers’ mental models to inform search explanations,2019,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077742332&doi=10.1145%2f3371390&partnerID=40&md5=8167a88d95c48c38bd2dfff89080caaf,"Modern web search engines use many signals to select and rank results in response to queries. However, searchers’ mental models of search are relatively unsophisticated, hindering their ability to use search engines efficiently and effectively. Annotating results with more in-depth explanations could help, but search engine providers need to know what to explain. To this end, we report on a study of searchers’ mental models of web selection and ranking, with more than 400 respondents to an online survey and 11 face-to-face interviews. Participants volunteered a range of factors and showed good understanding of important concepts such as popularity, wording, and personalization. However, they showed little understanding of recency or diversity and incorrect ideas of payment for ranking. Where there are already explanatory annotations on the results page—such as “ad” markers and keyword highlighting—participants were familiar with ranking concepts. This suggests that further explanatory annotations may be useful. © 2019 Copyright held by the owner/author(s).",Explanation; Mental models; Ranking; Web search,Information retrieval; Search engines; Surveys; Websites; Explanation; Face-to-face interview; Mental model; Online surveys; Personalizations; Ranking; Web searches; Cognitive systems
Constructing click model for mobile search with viewport time,2019,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075725306&doi=10.1145%2f3360486&partnerID=40&md5=47d4b835308c11959c1ed6f4ce13cb60,"A series of click models has been proposed to extract accurate and unbiased relevance feedback from valuable yet noisy click-through data in search logs. Previous works have shown that users search behavior in mobile and desktop scenarios are rather different in many aspects, therefore, the click models designed for desktop search may not be effective in the mobile context. To address this problem, we propose two novel click models for mobile search: (1) Mobile Click Model (MCM), which models click necessity bias and examination satisfaction bias; (2) Viewport Time ClickModel (VTCM), which further extendsMCM by utilizing the viewport time. Extensive experiments on large-scale real mobile search logs show that: (1) MCM and VTCM outperform existing models in predicting users' clicks and estimating result relevance; (2) MCM and VTCM can extract richer information, such as the click necessity of search results and the probability of user satisfaction, from mobile click logs; (3) By modeling the viewport time distributions of heterogeneous results, VTCM can bring a significant improvement over MCM in click prediction and relevance estimation tasks. Our proposed click models can help better understand user behavior patterns in mobile search and improve the ranking performance of mobile search engines. © 2019 ACM.",Click model; Mobile search; Viewport time; Web search,Probability distributions; Search engines; Mobile search; Ranking performance; Relevance estimations; Relevance feedback; Time distribution; User behavior patterns; Viewport time; Web searches; Behavioral research
Offline versus online representation learning of documents using external knowledge,2019,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075714830&doi=10.1145%2f3349527&partnerID=40&md5=f42dd4a95209490a9975a9bc0a500d49,"An intensive recent research work investigated the combined use of hand-curated knowledge resources and corpus-driven resources to learn effective text representations. The overall learning process could be run by online revising the learning objective or by offline refining an original learned representation. The differentiated impact of each of the learning approaches on the quality of the learned representations has not been studied so far in the literature. This article focuses on the design of comparable offline vs. online knowledgeenhanced document representation learning models and the comparison of their effectiveness using a set of standard IR and NLP downstream tasks. The results of quantitative and qualitative analyses show that (1) offline vs. online learning approaches have dissimilar result trends regarding the task as well as the dataset distribution counts with regard to domain application; (2) while considering external knowledge resources is undoubtedly beneficial, the way used to express relational constraints could affect semantic inference effectiveness. The findings of this work present opportunities for the design of future representation learning models, but also for providing insights about the evaluation of such models. © 2019 ACM.",Information retrieval; Knowledge resources; Natural language processing; Representation learning,Information retrieval; Learning systems; Natural language processing systems; Semantics; Document Representation; Knowledge resource; Learning objectives; NAtural language processing; Online representation; Quantitative and qualitative analysis; Relational constraint; Representation learning; E-learning
Question answering in knowledge bases: A verification assisted model with iterative training,2019,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075765532&doi=10.1145%2f3345557&partnerID=40&md5=ef2f9139d96c9f27c138bc978c7499ba,"Question answering over knowledge bases aims to take full advantage of the information in knowledge bases with the ultimate purpose of returning answers to questions. To access the substantial knowledge within the KB, many model architectures are hindered by the bottleneck of accurately predicting relations that connect subject entities in questions to object entities in the knowledge base. To break the bottleneck, this article presents a novel model architecture, APVA, which includes a verification mechanism to check the correctness of predicted relations. Specifically, APVA takes advantage of KB-based information to improve relation prediction but verifies the correctness of the predicted relation by means of simple negative sampling in a logistic regression framework. The APVA architecture offers a natural way to integrate an iterative training procedure, which we call turbo training. Accordingly, we introduce APVA-TURBO to perform question answering over knowledge bases. We demonstrate extensive experiments to show that APVA-TURBO outperforms existing approaches on question answering. © 2019 ACM.",Knowledge base; Question answering,Knowledge based systems; Knowledge base; Knowledge basis; Logistic regressions; Model architecture; Question Answering; Training procedures; Architecture
Boosting search performance using query variations,2019,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075737319&doi=10.1145%2f3345001&partnerID=40&md5=74e9ec5edfacc6abf080848ff1969d24,"Rank fusion is a powerful technique that allows multiple sources of information to be combined into a single result set. Query variations covering the same information need represent one way in which different sources of information might arise. However, when implemented in the obvious manner, fusion over query variations is not cost-effective, at odds with the usual web-search requirement for strict per-query efficiency guarantees. In this work, we propose a novel solution to query fusion by splitting the computation into two parts: One phase that is carried out offline, to generate pre-computed centroid answers for queries addressing broadly similar information needs, and then a second online phase that uses the corresponding topic centroid to compute a result page for each query. To achieve this, we make use of score-based fusion algorithms whose costs can be amortized via the pre-processing step and that can then be efficiently combined during subsequent per-query re-ranking operations. Experimental results using the ClueWeb12B collection and the UQV100 query variations demonstrate that centroid-based approaches allow improved retrieval effectiveness at little or no loss in query throughput or latency and within reasonable pre-processing requirements.We additionally show that queries that do not match any of the pre-computed clusters can be accurately identified and efficiently processed in our proposed ranking pipeline. © 2019 ACM.",Dynamic pruning; Effectiveness; Efficiency; Experimentation; Rank fusion,Cost effectiveness; Efficiency; Dynamic pruning; Effectiveness; Experimentation; Fusion algorithms; Pre-processing step; Query efficiency; Retrieval effectiveness; Sources of informations; Pipeline processing systems
Relevance feedback: The whole is inferior to the sum of its parts,2019,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075719297&doi=10.1145%2f3360487&partnerID=40&md5=2bd7062804a8f849412ada7d50d26d50,"Document retrieval methods that utilize relevance feedback often induce a single query model from the set of feedback documents, specifically, the relevant documents.We empirically show that for a few state-of-theart query-model induction methods, retrieval performance can be significantly improved by constructing the query model from a subset of the relevant documents rather than from all of them. Motivated by this finding, we propose a new approach for relevance-feedback-based retrieval. The approach, derived from the risk minimization framework, is based on utilizing multiple query models induced from all subsets of the given relevant documents. Empirical evaluation shows that the approach posts performance that is statistically significantly better than that of applying the standard practice of utilizing a single query model induced from the relevant documents. While the average relative improvements are small to moderate, the robustness of the approach is substantially higher than that of a variety of reference comparison methods that address various challenges in using relevance feedback. © 2019 ACM.",Ad hoc retrieval; Relevance feedback,Feedback; Query processing; Ad Hoc retrieval; Comparison methods; Empirical evaluations; Feedback documents; Relevance feedback; Relevant documents; Retrieval performance; Standard practices; Information retrieval
Modeling embedding dimension correlations via convolutional neural collaborative filtering,2019,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075694872&doi=10.1145%2f3357154&partnerID=40&md5=dee84b743fb806a314865e6fcaff143e,"As the core of recommender systems, collaborative filtering (CF) models the affinity between a user and an item from historical user-item interactions, such as clicks, purchases, and so on. Benefiting from the strong representation power, neural networks have recently revolutionized the recommendation research, setting up a new standard for CF. However, existing neural recommender models do not explicitly consider the correlations among embedding dimensions, making them less effective in modeling the interaction function between users and items. In this work, we emphasize on modeling the correlations among embedding dimensions in neural networks to pursue higher effectiveness for CF. We propose a novel and general neural collaborative filtering framework-namely, ConvNCF, which is featured with two designs: (1) applying outer product on user embedding and item embedding to explicitly model the pairwise correlations between embedding dimensions, and (2) employing convolutional neural network above the outer product to learn the high-order correlations among embedding dimensions. To justify our proposal, we present three instantiations of ConvNCF by using different inputs to represent a user and conduct experiments on two real-world datasets. Extensive results verify the utility of modeling embedding dimension correlations with ConvNCF, which outperforms several competitive CF methods. © 2019 ACM.",Convolutional neural network; Embedding dimension correlation; Neural collaborative filtering; Recommender system,Convolution; Embeddings; Neural networks; Recommender systems; Convolutional neural network; Embedding dimensions; High order correlation; Interaction functions; Model embedding; Pairwise correlation; Real-world datasets; Representation power; Collaborative filtering
Next and next new POI recommendation via latent behavior pattern inference,2019,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075683849&doi=10.1145%2f3354187&partnerID=40&md5=b9cc3d54a0a1e124bd5ab288da8c2b28,"Next and next new point-of-interest (POI) recommendation are essential instruments in promoting customer experiences and business operations related to locations. However, due to the sparsity of the check-in records, they still remain insufficiently studied. In this article, we propose to utilize personalized latent behavior patterns learned from contextual features, e.g., time of day, day of week, and location category, to improve the effectiveness of the recommendations. Two variations of models are developed, including GPDM, which learns a fixed pattern distribution for all users; and PPDM, which learns personalized pattern distribution for each user. In both models, a soft-max function is applied to integrate the personalized Markov chain with the latent patterns, and a sequential Bayesian Personalized Ranking (S-BPR) is applied as the optimization criterion. Then, Expectation Maximization (EM) is in charge of finding optimized model parameters. Extensive experiments on three large-scale commonly adopted real-world LBSN data sets prove that the inclusion of location category and latent patterns helps to boost the performance of POI recommendations. Specifically, ourmodels in general significantly outperform other state-of-the-artmethods for both next and next new POI recommendation tasks. Moreover, our models are capable of making accurate recommendations regardless of the short/long duration or distance. © 2019 ACM.",Latent behavior patterns; Next new POI recommendation; Next POI recommendation,Markov processes; Maximum principle; Business operation; Contextual feature; Customer experience; Expectation Maximization; Latent behaviors; Next new POI recommendation; Next POI recommendation; Optimization criteria; Location
Adaptive local low-rank matrix approximation for recommendation,2019,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075547845&doi=10.1145%2f3360488&partnerID=40&md5=f93e8e5eb598198196781fb0cac1d21f,"Low-rank matrix approximation (LRMA) has attracted more and more attention in the community of recommendation. Even though LRMA-based recommendation methods (including Global LRMA and Local LRMA) obtain promising results, they suffer from the complicated structure of the large-scale and sparse rating matrix, especially when the underlying system includes a large set of items with various types and a huge amount of users with diverse interests. Thus, they have to predefine the important parameters, such as the rank of the rating matrix and the number of submatrices. Moreover, most existing Local LRMA methods are usually designed in a two-phase separated framework and do not consider the missing mechanisms of rating matrix. In this article, a non-parametric unified Bayesian graphical model is proposed for Adaptive Local low-rankMatrix Approximation (ALoMA). ALoMA has ability to simultaneously identify rating submatrices, determine the optimal rank for each submatrix, and learn the submatrix-specific user/item latent factors. Meanwhile, the missing mechanism is adopted to characterize the whole rating matrix. These four parts are seamlessly integrated and enhance each other in a unified framework. Specifically, the user-item rating matrix is adaptively divided into proper number of submatrices in ALoMA by exploiting the Chinese Restaurant Process. For each submatrix, by considering both global/local structure information and missing mechanisms, the latent user/item factors are identified in an optimal latent space by adopting automatic relevance determination technique. We theoretically analyze the model's generalization error bounds and give an approximation guarantee. Furthermore, an efficient Gibbs sampling-based algorithm is designed to infer the proposed model. A series of experiments have been conducted on six real-world datasets (Epinions, Douban, Dianping, Yelp, Movielens (10M), and Netflix). The results demonstrate that ALoMA outperforms the state-of-The-Art LRMA-based methods and can easily provide interpretable recommendation results. © 2019 Association for Computing Machinery. All rights reserved.",Clustering; Probabilistic graphical model; Recommendation system,Bayesian networks; Error analysis; Graphic methods; Recommender systems; Automatic relevance determination; Bayesian graphical models; Clustering; Complicated structures; Generalization error bounds; Low-rank matrix approximations; Probabilistic graphical models; Recommendation methods; Matrix algebra
Explainable product search with a dynamic relation embedding model,2019,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074155853&doi=10.1145%2f3361738&partnerID=40&md5=0d2f2338a9c69806badc81b7033ba6d7,"Product search is one of the most popular methods for customers to discover products online. Most existing studies on product search focus on developing effective retrieval models that rank items by their likelihood to be purchased. However, they ignore the problem that there is a gap between how systems and customers perceive the relevance of items. Without explanations, users may not understand why product search engines retrieve certain items for them, which consequentially leads to imperfect user experience and suboptimal system performance in practice. In this work, we tackle this problem by constructing explainable retrieval models for product search. Specifically, we propose to model the “search and purchase” behavior as a dynamic relation between users and items, and create a dynamic knowledge graph based on both the multi-relational product data and the context of the search session. Ranking is conducted based on the relationship between users and items in the latent space, and explanations are generated with logic inferences and entity soft matching on the knowledge graph. Empirical experiments show that our model, which we refer to as the Dynamic Relation Embedding Model (DREM), significantly outperforms the state-of-the-art baselines and has the ability to produce reasonable explanations for search results. © 2019 Association for Computing Machinery.",Explainable model; Knowledge graph; Product search; Relation embedding,Embeddings; Graphic methods; Search engines; Dynamic relation; Empirical experiments; Knowledge graphs; Logic inferences; Product search; Relation embedding; Retrieval models; State of the art; Sales
Bore: Adapting to reader consumption behavior instability for news recommendation,2019,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074145780&partnerID=40&md5=b3806970800d8bb72a5c08dd7de9bd46,"News recommendation has become an essential way to help readers discover interesting stories. While a growing line of research has focused on modeling reading preferences for news recommendation, they neglect the instability of reader consumption behaviors, i.e., consumption behaviors of readers may be influenced by other factors in addition to user interests, which degrades the recommendation effectiveness of existing methods. In this article, we propose a probabilistic generative model, BoRe, where user interests and crowd effects are used to adapt to the instability of reader consumption behaviors, and reading sequences are utilized to adapt user interests evolving over time. Further, the extreme sparsity problem in the domain of news severely hinders accurately modeling user interests and reading sequences, which discounts BoRe's ability to adapt to the instability. Accordingly, we leverage domain-specific features to model user interests in the situation of extreme sparsity. Meanwhile, we consider groups of users instead of individuals to capture reading sequences. Besides, we study how to reduce the computation to allow online application. Extensive experiments have been conducted to evaluate the effectiveness and efficiency of BoRe on real-world datasets. The experimental results show the superiority of BoRe, compared with the state-of-the-art competing methods. © 2019 Association for Computing Machinery.",Domain-specific feature; Instability; News recommendation; Probabilistic generative model; Reader consumption behavior,Plasma stability; Domain specific; Effectiveness and efficiencies; Generative model; News recommendation; On-line applications; Reader consumption behavior; Real-world datasets; Sparsity problems; Stability
Geographic diversification of recommended POIs in frequently visited areas,2019,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074170101&doi=10.1145%2f3362505&partnerID=40&md5=f49fd57879c05bbd2542bd3000ce26ce,"In the personalized Point-Of-Interest (POI) (or venue) recommendation, the diversity of recommended POIs is an important aspect. Diversity is especially important when POIs are recommended in the target users' frequently visited areas, because users are likely to revisit such areas. In addition to the (POI) category diversity that is a popular diversification objective in recommendation domains, diversification of recommended POI locations is an interesting subject itself. Despite its importance, existing POI recommender studies generally focus on and evaluate prediction accuracy. In this article, geographical diversification (geo-diversification), a novel diversification concept that aims to increase recommendation coverage for a target users' geographic areas of interest, is introduced, from which a method that improves geo-diversity as an addition to existing state-of-the-art POI recommenders is proposed. In experiments with the datasets from two real Location Based Social Networks (LSBNs), we first analyze the performance of four state-of-the-art POI recommenders from various evaluation perspectives including category diversity and geo-diversity that have not been examined previously. The proposed method consistently improves geo-diversity (CPR(geo)@20) by 5 to 12% when combined with four state-of-the-art POI recommenders with negligible prediction accuracy (Recall@20) loss and provides 6 to 18% geo-diversity improvement with tolerable prediction accuracy loss (up to 2.4%). © 2019 Association for Computing Machinery.",Diversity; Geographical diversity; LBSN; POI; POI recommendation; Recommendation,Computer networks; Information systems; Diversity; Geographical diversity; LBSN; POI recommendation; Recommendation; Forecasting
"Understanding assimilation-contrast effects in online rating systems: Modelling, debiasing, and applications",2019,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074143418&doi=10.1145%2f3362651&partnerID=40&md5=fb722d4d3f7afb662f43312ba2a81aa3,"“Unbiasedness,” which is an important property to ensure that users' ratings indeed reflect their true evaluations of products, is vital both in shaping consumer purchase decisions and providing reliable recommendations in online rating systems. Recent experimental studies showed that distortions from historical ratings would ruin the unbiasedness of subsequent ratings. How to “discover” historical distortions in each single rating (or at the micro-level), and perform the “debiasing operations” are our main objective. Using 42M real customer ratings, we first show that users either “assimilate” or “contrast” to historical ratings under different scenarios, which can be further explained by a well-known psychological argument: the “Assimilate-Contrast” theory. This motivates us to propose the Historical Influence Aware Latent Factor Model (HIALF), the “first” model for real rating systems to capture and mitigate historical distortions in each single rating. HIALF allows us to study the influence patterns of historical ratings from a modelling perspective, which perfectly matches the assimilation and contrast effects observed in experiments. Moreover, HIALF achieves significant improvements in predicting subsequent ratings and characterizing relationships in ratings. It also contributes to better recommendations, wiser consumer purchase decisions, and deeper understanding of historical distortions in both honest rating and misbehaving rating settings. © 2019 Association for Computing Machinery.",Modelling and debiasing historical ratings' influence; Recommender systems,Purchasing; Recommender systems; Sales; Consumer purchase; Contrast effects; De-biasing; Influence patterns; Latent factor models; Micro level; Online rating systems; Rating system; Online systems
Joint neural collaborative filtering for recommender systems,2019,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071277096&doi=10.1145%2f3343117&partnerID=40&md5=20d319bacffce982b7f28e2c46e6663c,"We propose a Joint Neural Collaborative Filtering (J-NCF) method for recommender systems. The J-NCF model applies a joint neural network that couples deep feature learning and deep interaction modeling with a rating matrix. Deep feature learning extracts feature representations of users and items with a deep learning architecture based on a user-item rating matrix. Deep interaction modeling captures non-linear user-item interactions with a deep neural network using the feature representations generated by the deep feature learning process as input. J-NCF enables the deep feature learning and deep interaction modeling processes to optimize each other through joint training, which leads to improved recommendation performance. In addition, we design a new loss function for optimization that takes both implicit and explicit feedback, point-wise and pair-wise loss into account. Experiments on several real-world datasets show significant improvements of J-NCF over state-of-the-art methods, with improvements of up to 8.24% on the MovieLens 100K dataset, 10.81% on the MovieLens 1M dataset, and 10.21% on the Amazon Movies dataset in terms of HR@10. NDCG@10 improvements are 12.42%, 14.24%, and 15.06%, respectively. We also conduct experiments to evaluate the scalability and sensitivity of J-NCF. Our experiments show that the J-NCF model has a competitive recommendation performance with inactive users and different degrees of data sparsity when compared to state-of-the-art baselines. © 2019 Copyright held by the owner/author(s).",Collaborative filtering; Neural recommendation,Deep neural networks; Machine learning; Matrix algebra; Recommender systems; Deep feature learning; Feature representation; Interaction model; Learning architectures; Neural recommendation; Real-world datasets; Recommendation performance; State-of-the-art methods; Collaborative filtering
Search result reranking with visual and structure information sources,2019,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068844180&doi=10.1145%2f3329188&partnerID=40&md5=8177a25cfe0a6c74a2bd8a245df8e885,"Relevance estimation is among the most important tasks in the ranking of search results. Current methodologies mainly concentrate on text matching, link analysis, and user behavior models. However, users judge the relevance of search results directly from Search Engine Result Pages (SERPs), which provide valuable signals for reranking. In this article, we propose two different approaches to aggregate the visual, structure, as well as textual information sources of search results in relevance estimation. The first one is a late-fusion framework named Joint Relevance Estimation model (JRE). JRE estimates the relevance independently from screenshots, textual contents, and HTML source codes of search results and jointly makes the final decision through an inter-modality attention mechanism. The second one is an early-fusion framework named Tree-based Deep Neural Network (TreeNN), which embeds the texts and images into the HTML parse tree through a recursive process. To evaluate the performance of the proposed models, we construct a large-scale practical Search Result Relevance (SRR) dataset that consists of multiple information sources and relevance labels of over 60,000 search results. Experimental results show that the proposed two models achieve better performance than state-of-the-art ranking solutions as well as the original rankings of commercial search engines. © 2019 Association for Computing Machinery.",Information retrieval; Multimodal; Ranking; Relevance,Behavioral research; Deep neural networks; HTML; Information retrieval; Large dataset; Search engines; Attention mechanisms; Information sources; Multi-modal; Ranking; Relevance; Relevance estimations; Search engine results; Structure information; Natural language processing systems
Memory-augmented dialogue management for task-oriented dialogue systems,2019,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068847705&doi=10.1145%2f3317612&partnerID=40&md5=e6db062e44e462c7ea1f8eda7cb9eeda,"Dialogue management (DM) is responsible for predicting the next action of a dialogue system according to the current dialogue state and thus plays a central role in task-oriented dialogue systems. Since DM requires having access not only to local utterances but also to the global semantics of the entire dialogue session, modeling the long-range history information is a critical issue. To this end, we propose MAD, a novel memory-augmented dialogue management model that employs a memory controller and two additional memory structures (i.e., a slot-value memory and an external memory). The slot-value memory tracks the dialogue state by memorizing and updating the values of semantic slots (i.e., cuisine, price, and location), and the external memory augments the representation of hidden states of traditional recurrent neural networks by storing more context information. To update the dialogue state efficiently, we also propose slot-level attention on user utterances to extract specific semantic information for each slot. Experiments show that our model can obtain state-of-the-art performance and outperforms existing baselines. © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Attention; Dialogue management; Dialogue state; Memory network; Neural network,Neural networks; Semantics; Speech processing; Attention; Context information; Dialogue management; Dialogue state; History informations; Memory network; Specific semantics; State-of-the-art performance; Recurrent neural networks
Funnelling: A new ensemble method for heterogeneous transfer learning and its application to cross-lingual text classification,2019,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068863736&doi=10.1145%2f3326065&partnerID=40&md5=2d953322dab3b2c0921e2c702d4cc52a,"Cross-lingual Text Classification (CLC) consists of automatically classifying, according to a common set C of classes, documents each written in one of a set of languages L, and doing so more accurately than when “naïvely” classifying each document via its corresponding language-specific classifier. To obtain an increase in the classification accuracy for a given language, the system thus needs to also leverage the training examples written in the other languages. We tackle “multilabel” CLC via funnelling, a new ensemble learning method that we propose here. Funnelling consists of generating a two-tier classification system where all documents, irrespective of language, are classified by the same (second-tier) classifier. For this classifier, all documents are represented in a common, language-independent feature space consisting of the posterior probabilities generated by first-tier, language-dependent classifiers. This allows the classification of all test documents, of any language, to benefit from the information present in all training documents, of any language. We present substantial experiments, run on publicly available multilingual text collections, in which funnelling is shown to significantly outperform a number of state-of-the-art baselines. All code and datasets (in vector form) are made publicly available. © 2019 Copyright held by the owner/author(s).",Crosslingual text classification; Funnelling; Heterogeneous transfer learning; Transfer learning,C (programming language); Information retrieval systems; Learning systems; Text processing; Classification accuracy; Classification system; Funnelling; Language independents; Multilingual texts; Posterior probability; Text classification; Transfer learning; Classification (of information)
"The effects of working memory, perceptual speed, and inhibition in aggregated search",2019,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068824706&doi=10.1145%2f3322128&partnerID=40&md5=43b2089ea22a3db9d8353e794ce46342,"Prior work has studied how different characteristics of individual users (e.g., personality traits and cognitive abilities) can impact search behaviors and outcomes. We report on a laboratory study (N = 32) that investigated the effects of three different cognitive abilities (perceptual speed, working memory, and inhibition) in the context of aggregated search. Aggregated search systems combine results from multiple heterogeneous sources (or verticals) in a unified presentation. Participants in our study interacted with two different aggregated search interfaces (a within-subjects design) that differed based on the extent to which the layout distinguished between results originating from different verticals. The interleaved interface merged results from different verticals in a fairly unconstrained fashion. Conversely, the blocked interface displayed results from the same vertical as a group, displayed each group of vertical results in the same region on the SERP for every query, and used a border around each group of vertical results to help distinguish among results from different sources. We investigated three research questions (RQ1–RQ3). Specifically, we investigated the effects of the interface condition and each cognitive ability on three types of outcomes: (RQ1) participants’ levels of workload, (RQ2) participants’ levels of user engagement, and (RQ3) participants’ search behaviors. Our results found different main and interaction effects. Perceptual speed and inhibition did not significantly affect participants’ workload and user engagement but significantly affected their search behaviors. Specifically, with the interleaved interface, participants with lower perceptual speed had more difficulty finding relevant results on the SERP, and participants with lower inhibitory attention control searched at a slower pace. Working memory did not have a strong effect on participants’ behaviors but had several significant effects on the levels of workload and user engagement reported by participants. Specifically, participants with lower working memory reported higher levels of workload and lower levels of user engagement. We discuss implications of our results for designing aggregated search interfaces that are well suited for users with different cognitive abilities. © 2019 Association for Computing Machinery.",Aggregated search; Cognitive abilities; Inhibition; Perceptual speed; Search behaviors; User engagement; Working memory; Workload,Enzyme inhibition; Speed; Aggregated search; Cognitive ability; Search behavior; User engagement; Working memory; Workload; Behavioral research
Using collection shards to study retrieval performance effect sizes,2019,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065794671&doi=10.1145%2f3310364&partnerID=40&md5=9f0dfa7e9dbfb0e5f7ad890b48b380b4,"Despite the bulk of research studying how to more accurately compare the performance of IR systems, less attention is devoted to better understanding the different factors that play a role in such performance and how they interact. This is the case of shards, i.e., partitioning a document collection into sub-parts, which are used for many different purposes, ranging from efficiency to selective search or making test collection evaluation more accurate. In all these cases, there is empirical knowledge supporting the importance of shards, but we lack actual models that allow us to measure the impact of shards on system performance and how they interact with topics and systems. We use the general linear mixed model framework and present a model that encompasses the experimental factors of system, topic, shard, and their interaction effects. This detailed model allows us to more accurately estimate differences between the effect of various factors. We study shards created by a range of methods used in prior work and better explain observations noted in prior work in a principled setting and offer new insights. Notably, we discover that the topic*shard interaction effect, in particular, is a large effect almost globally across all datasets, an observation that, to our knowledge, has not been measured before. © 2019 Association for Computing Machinery.",ANOVA; Effectiveness model; GLMM; Shard effect,Analysis of variance (ANOVA); Computer networks; Information systems; Document collection; Effectiveness models; Empirical knowledge; Experimental factors; GLMM; Linear mixed models; Retrieval performance; Shard effect; Large dataset
Does diversity affect user satisfaction in image search,2019,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065766681&doi=10.1145%2f3320118&partnerID=40&md5=8188371e74be57b4940adda9ba33eb4c,"Diversity has been taken into consideration by existing Web image search engines in ranking search results. However, there is no thorough investigation of how diversity affects user satisfaction in image search. In this article, we address the following questions: (1) How do different factors, such as content and visual presentations, affect users' perception of diversity? (2) How does search result diversity affect user satisfaction with different search intents? To answer those questions, we conduct a set of laboratory user studies to collect users' perceived diversity annotations and search satisfaction. We find that the existence of nearly duplicated image results has the largest impact on users' perceived diversity, followed by the similarity in content and visual presentations. Besides these findings, we also investigate the relationship between diversity and satisfaction in image search. Specifically, we find that users' preference for diversity varies across different search intents. When users want to collect information or save images for further usage (the Locate search tasks), more diversified result lists lead to higher satisfaction levels. The insights may help commercial image search engines to design better result ranking strategies and evaluation metrics. © 2019 Association for Computing Machinery.",Image diversity; Image search; User satisfaction,Computer networks; Information systems; Evaluation metrics; Image diversity; Image search; Image search engine; Ranking strategy; User satisfaction; Users' perception; Visual presentation; Search engines
Attentive aspect modeling for review-aware recommendation,2019,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065789334&doi=10.1145%2f3309546&partnerID=40&md5=37182fb2e7c21713e2431d8b22553769,"In recent years, many studies extract aspects from user reviews and integrate them with ratings for improving the recommendation performance. The common aspects mentioned in a user's reviews and a product's reviews indicate indirect connections between the user and product. However, these aspect-based methods suffer from two problems. First, the common aspects are usually very sparse, which is caused by the sparsity of user-product interactions and the diversity of individual users' vocabularies. Second, a user's interests on aspects could be different with respect to different products, which are usually assumed to be static in existing methods. In this article, we propose an Attentive Aspect-based Recommendation Model (AARM) to tackle these challenges. For the first problem, to enrich the aspect connections between user and product, besides common aspects, AARM also models the interactions between synonymous and similar aspects. For the second problem, a neural attention network which simultaneously considers user, product, and aspect information is constructed to capture a user's attention toward aspects when examining different products. Extensive quantitative and qualitative experiments show that AARM can effectively alleviate the two aforementioned problems and significantly outperforms several state-of-the-art recommendation methods on the top-N recommendation task. © 2019 Copyright held by the owner/author(s).",Aspects; Attention mechanism; Neural network; Top-N recommendation,Information systems; Neural networks; Aspects; Attention mechanisms; Product interaction; Qualitative experiments; Recommendation methods; Recommendation performance; State of the art; Top-N recommendation; Computer networks
On annotation methodologies for image search evaluation,2019,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065804657&doi=10.1145%2f3309994&partnerID=40&md5=b6670ac8a7bf657237adcd7d232195ae,"Image search engines differ significantly from general web search engines in the way of presenting search results. The difference leads to different interaction and examination behavior patterns, and therefore requires changes in evaluation methodologies. However, evaluation of image search still utilizes the methods for general web search. In particular, offline metrics are calculated based on coarse-fine topical relevance judgments with the assumption that users examine results in a sequential manner. In this article, we investigate annotation methods via crowdsourcing for image search evaluation based on a lab-based user study. Using user satisfaction as the golden standard, we make several interesting findings. First, instead of item-based annotation, annotating relevance in a row-based way is more efficient without hurting performance. Second, besides topical relevance, image quality plays a crucial role when evaluating the image search results, and the importance of image quality changes with search intent. Third, compared to traditional four-level scales, the fine-grain annotation method outperforms significantly. To our best knowledge, our work is the first to systematically study how diverse factors in data annotation impact image search evaluation. Our results suggest different strategies for exploiting the crowdsourcing to get data annotated under different conditions. © 2019 Association for Computing Machinery.",Crowdsourcing annotation; Image search; Offline evaluation; User satisfaction,Crowdsourcing; Image annotation; Image quality; Information retrieval; Websites; Annotation methods; Evaluation methodologies; Image search; Image search engine; Offline evaluation; Relevance judgment; Sequential manners; User satisfaction; Search engines
Deep item-based collaborative filtering for top-N recommendation,2019,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065769291&doi=10.1145%2f3314578&partnerID=40&md5=30795d93f8ad1bd26b9cda4cf6161255,"Item-based Collaborative Filtering (ICF) has been widely adopted in recommender systems in industry, owing to its strength in user interest modeling and ease in online personalization. By constructing a user's profile with the items that the user has consumed, ICF recommends items that are similar to the user's profile. With the prevalence of machine learning in recent years, significant processes have been made for ICF by learning item similarity (or representation) from data. Nevertheless, we argue that most existing works have only considered linear and shallow relationships between items, which are insufficient to capture the complicated decision-making process of users. In this article, we propose a more expressive ICF solution by accounting for the nonlinear and higher-order relationships among items. Going beyond modeling only the second-order interaction (e.g., similarity) between two items, we additionally consider the interaction among all interacted item pairs by using nonlinear neural networks. By doing this, we can effectively model the higher-order relationship among items, capturing more complicated effects in user decision-making. For example, it can differentiate which historical itemsets in a user's profile are more important in affecting the user to make a purchase decision on an item. We treat this solution as a deep variant of ICF, thus term it as DeepICF. To justify our proposal, we perform empirical studies on two public datasets from MovieLens and Pinterest. Extensive experiments verify the highly positive effect of higher-order item interaction modeling with nonlinear neural networks. Moreover, we demonstrate that by more fine-grained second-order interaction modeling with attention network, the performance of our DeepICF method can be further improved. © 2019 Association for Computing Machinery.",Collaborative filtering; Deep learning; Implicit feedback; Item-based CF; Neural networks,Decision making; Deep learning; Neural networks; Online systems; Decision making process; Implicit feedback; Item-based CF; Item-based collaborative filtering; Nonlinear neural networks; Purchase decision; Second-order interaction; User interest model; Collaborative filtering
Personalised reranking of paper recommendations using paper content and user behavior,2019,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063355029&doi=10.1145%2f3312528&partnerID=40&md5=47e5cd446d48d6a06a1b14e6544bd773,"Academic search engines have been widely used to access academic papers, where users' information needs are explicitly represented as search queries. Some modern recommender systems have taken one step further by predicting users' information needs without the presence of an explicit query. In this article, we examine an academic paper recommender that sends out paper recommendations in email newsletters, based on the users' browsing history on the academic search engine. Specifically, we look at users who regularly browse papers on the search engine, and we sign up for the recommendation newsletters for the first time. We address the task of reranking the recommendation candidates that are generated by a production system for such users. We face the challenge that the users on whom we focus have not interacted with the recommender system before, which is a common scenario that every recommender system encounters when new users sign up. We propose an approach to reranking candidate recommendations that utilizes both paper content and user behavior. The approach is designed to suit the characteristics unique to our academic recommendation setting. For instance, content similarity measures can be used to find the closest match between candidate recommendations and the papers previously browsed by the user. To this end, we use a knowledge graph derived from paper metadata to compare entity similarities (papers, authors, and journals) in the embedding space. Since the users on whom we focus have no prior interactions with the recommender system, we propose a model to learn a mapping from users' browsed articles to user clicks on the recommendations. We combine both content and behavior into a hybrid reranking model that outperforms the production baseline significantly, providing a relative 13% increase in Mean Average Precision and 28% in Precision@1. Moreover, we provide a detailed analysis of the model components, highlighting where the performance boost comes from. The obtained insights reveal useful components for the reranking process and can be generalized to other academic recommendation settings as well, such as the utility of graph embedding similarity. Also, recent papers browsed by users provide stronger evidence for recommendation than historical ones. © 2019 Association for Computing Machinery.",Academic search; Paper recommendation; Reranking,Behavioral research; Embeddings; Paper; Search engines; Academic search; Content similarity; E-mail newsletters; Entity similarities; Model components; Paper recommendations; Production system; Re-ranking; Recommender systems
Exploring high-order user preference on the knowledge graph for recommender systems,2019,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063323331&doi=10.1145%2f3312738&partnerID=40&md5=01f3c76235fdb8918b0f66ec309fe5f3,"To address the sparsity and cold-start problem of collaborative filtering, researchers usually make use of side information, such as social networks or item attributes, to improve the performance of recommendation. In this article, we consider the knowledge graph (KG) as the source of side information. To address the limitations of existing embedding-based and path-based methods for KG-aware recommendation, we propose RippleNet, an end-to-end framework that naturally incorporates the KG into recommender systems. RippleNet has two versions: (1) The outward propagation version, which is analogous to the actual ripples on water, stimulates the propagation of user preferences over the set of knowledge entities by automatically and iteratively extending a user's potential interests along links in the KG. The multiple “ripples” activated by a user's historically clicked items are thus superposed to form the preference distribution of the user with respect to a candidate item. (2) The inward aggregation version aggregates and incorporates the neighborhood information biasedly when computing the representation of a given entity. The neighborhood can be extended to multiple hops away to model high-order proximity and capture users' long-distance interests. In addition, we intuitively demonstrate how a KG assists with recommender systems in RippleNet, and we also find that RippleNet provides a new perspective of explainability for the recommended results in terms of the KG. Through extensive experiments on real-world datasets, we demonstrate that both versions of RippleNet achieve substantial gains in a variety of scenarios, including movie, book, and news recommendations, over several state-of-the-art baselines. © 2019 Association for Computing Machinery.",Inward aggregation; Knowledge graph; Outward propagation; Recommender systems,Collaborative filtering; Iterative methods; Quality of service; Cold start problems; Knowledge graphs; Multiple hops; Neighborhood information; News recommendation; Real-world datasets; Side information; State of the art; Recommender systems
Risk-sensitive learning to rank with evolutionary multi-objective feature selection,2019,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062373723&doi=10.1145%2f3300196&partnerID=40&md5=f1b46bd1f8ffe5f0d7e1519899baf475,"Learning to Rank (L2R) is one of the main research lines in Information Retrieval. Risk-sensitive L2R is a subarea of L2R that tries to learn models that are good on average while at the same time reducing the risk of performing poorly in a few but important queries (e.g., medical or legal queries). One way of reducing risk in learned models is by selecting and removing noisy, redundant features, or features that promote some queries to the detriment of others. This is exacerbated by learning methods that usually maximize an average metric (e.g., mean average precision (MAP) or Normalized Discounted Cumulative Gain (NDCG)). However, historically, feature selection (FS) methods have focused only on effectiveness and feature reduction as the main objectives. Accordingly, in this work, we propose to evaluate FS for L2R with an additional objective in mind, namely risk-sensitiveness. We present novel single and multi-objective criteria to optimize feature reduction, effectiveness, and risk-sensitiveness, all at the same time. We also introduce a new methodology to explore the search space, suggesting effective and efficient extensions of a well-known Evolutionary Algorithm (SPEA2) for FS applied to L2R. Our experiments show that explicitly including risk as an objective criterion is crucial to achieving a more effective and risk-sensitive performance. We also provide a thorough analysis of our methodology and experimental results. © 2019 Association for Computing Machinery.",Feature selection; Learning to rank; Risk-sensitiveness,Computer networks; Information systems; Evolutionary Multi-objectives; Feature reduction; Learning methods; Learning to rank; Multi objective; Objective criteria; Redundant features; Search spaces; Feature extraction
Temporal relational ranking for stock prediction,2019,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062842940&doi=10.1145%2f3309547&partnerID=40&md5=9a644cd54771c28c60e4cc0b65bf6ea2,"Stock prediction aims to predict the future trends of a stock in order to help investors make good investment decisions. Traditional solutions for stock prediction are based on time-series models. With the recent success of deep neural networks in modeling sequential data, deep learning has become a promising choice for stock prediction. However, most existing deep learning solutions are not optimized toward the target of investment, i.e., selecting the best stock with the highest expected revenue. Specifically, they typically formulate stock prediction as a classification (to predict stock trends) or a regression problem (to predict stock prices). More importantly, they largely treat the stocks as independent of each other. The valuable signal in the rich relations between stocks (or companies), such as two stocks are in the same sector and two companies have a supplier-customer relation, is not considered. In this work, we contribute a new deep learning solution, named Relational Stock Ranking (RSR), for stock prediction. Our RSR method advances existing solutions in two major aspects: (1) tailoring the deep learning models for stock ranking, and (2) capturing the stock relations in a time-sensitive manner. The key novelty of our work is the proposal of a new component in neural network modeling, named Temporal Graph Convolution, which jointly models the temporal evolution and relation network of stocks. To validate our method, we perform back-testing on the historical data of two stock markets, NYSE and NASDAQ. Extensive experiments demonstrate the superiority of our RSR method. It outperforms state-of-the-art stock prediction solutions achieving an average return ratio of 98% and 71% on NYSE and NASDAQ, respectively. © 2019 Association for Computing Machinery.",Graph-based learning; Learning to rank; Stock prediction,Deep neural networks; Economics; Electronic trading; Financial markets; Graphic methods; Investments; Public relations; Customer relations; Graph-based learning; Investment decisions; Learning to rank; Neural network model; Regression problem; Stock predictions; Temporal evolution; Forecasting
Learning from multi-annotator data: A noise-aware classification framework,2019,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062361785&doi=10.1145%2f3309543&partnerID=40&md5=db50cbc09ba99c3ff16e735dbe330122,"In the field of sentiment analysis and emotion detection in social media, or other tasks such as text classification involving supervised learning, researchers rely more heavily on large and accurate labelled training datasets. However, obtaining large-scale labelled datasets is time-consuming and high-quality labelled datasets are expensive and scarce. To deal with these problems, online crowdsourcing systems provide us an efficient way to accelerate the process of collecting training data via distributing the enormous tasks to various annotators to help create large amounts of labelled data at an affordable cost. Nowadays, these crowdsourcing platforms are heavily needed in dealing with social media text, since the social network platforms (e.g., Twitter) generate huge amounts of data in textual form everyday. However, people from different social and knowledge backgrounds have different views on various texts, which may lead to noisy labels. The existing noisy label aggregation/refinement algorithms mostly focus on aggregating labels from noisy annotations, which would not guarantee their effectiveness on the subsequent classification/ranking tasks. In this article, we propose a noise-aware classification framework that integrates the steps of noisy label aggregation and classification. The aggregated noisy crowd labels are fed into a classifier for training, while the predicted labels are employed as feedback for adjusting the parameters at the label aggregating stage. The classification framework is suitable for directly running on crowdsourcing datasets and applies to various kinds of classification algorithms. The feedback strategy makes it possible for us to find optimal parameters instead of using known data for parameter selection. Simulation experiments demonstrate that our method provide significant label aggregation performance for both binary and multiple classification tasks under various noisy environments. Experimenting on real-world data validates the feasibility of our framework in real noise data and helps us verify the reasonableness of the simulated experiment settings. © 2019 Association for Computing Machinery.",Crowdsourcing; Emotion detection; Sentiment analysis; Social media,Crowdsourcing; Data mining; Large dataset; Online systems; Sentiment analysis; Social networking (online); Classification algorithm; Classification framework; Crowdsourcing platforms; Emotion detection; Feedback strategies; Multiple Classification; Simulated experiments; Social media; Classification (of information)
Gaussian processes for rumour stance classification in social media,2019,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062350006&doi=10.1145%2f3295823&partnerID=40&md5=268c6486371e548feb3c8939607ce67c,"Social media tend to be rife with rumours while new reports are released piecemeal during breaking news. Interestingly, one can mine multiple reactions expressed by social media users in those situations, exploring their stance towards rumours, ultimately enabling the flagging of highly disputed rumours as being potentially false. In this work, we set out to develop an automated, supervised classifier that uses multi-task learning to classify the stance expressed in each individual tweet in a conversation around a rumour as either supporting, denying or questioning the rumour. Using a Gaussian Process classifier, and exploring its effectiveness on two datasets with very different characteristics and varying distributions of stances, we show that our approach consistently outperforms competitive baseline classifiers. Our classifier is especially effective in estimating the distribution of different types of stance associated with a given rumour, which we set forth as a desired characteristic for a rumour-tracking system that will show both ordinary users of Twitter and professional news practitioners how others orient to the disputed veracity of a rumour, with the final aim of establishing its actual truth value. © 2019 Copyright held by the owner/author(s).",Breaking news; Machine learning; Rumours; Social media; Stance classification; Veracity classification,Gaussian distribution; Gaussian noise (electronic); Learning systems; Social networking (online); Breaking news; Gaussian Processes; Multitask learning; Rumours; Social media; Supervised classifiers; Tracking system; Truth values; Classification (of information)
Handling massive n-gram datasets efficiently,2019,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062334164&doi=10.1145%2f3302913&partnerID=40&md5=3cd3796e423500242ab5f72a4d9d506c,"Two fundamental problems concern the handling of large n-gram language models: indexing, that is, compressing the n-grams and associated satellite values without compromising their retrieval speed, and estimation, that is, computing the probability distribution of the n-grams extracted from a large textual source. Performing these two tasks efficiently is vital for several applications in the fields of Information Retrieval, Natural Language Processing, and Machine Learning, such as auto-completion in search engines and machine translation. Regarding the problem of indexing, we describe compressed, exact, and lossless data structures that simultaneously achieve high space reductions and no time degradation with respect to the state-of-the-art solutions and related software packages. In particular, we present a compressed trie data structure in which each word of an n-gram following a context of fixed length k, that is, its preceding k words, is encoded as an integer whose value is proportional to the number of words that follow such context. Since the number of words following a given context is typically very small in natural languages, we lower the space of representation to compression levels that were never achieved before, allowing the indexing of billions of strings. Despite the significant savings in space, our technique introduces a negligible penalty at query time. Specifically, the most space-efficient competitors in the literature, which are both quantized and lossy, do not take less than our trie data structure and are up to 5 times slower. Conversely, our trie is as fast as the fastest competitor but also retains an advantage of up to 65% in absolute space. Regarding the problem of estimation, we present a novel algorithm for estimating modified Kneser-Ney language models that have emerged as the de-facto choice for language modeling in both academia and industry thanks to their relatively low perplexity performance. Estimating such models from large textual sources poses the challenge of devising algorithms that make a parsimonious use of the disk. The state-of-the-art algorithm uses three sorting steps in external memory: we show an improved construction that requires only one sorting step by exploiting the properties of the extracted n-gram strings. With an extensive experimental analysis performed on billions of n-grams, we show an average improvement of 4.5 times on the total runtime of the previous approach. © 2019 Copyright held by the owner/author(s).",Algorithm engineering; Efficiency; Scalability,Data structures; Efficiency; Indexing (of information); Learning algorithms; Learning systems; Modeling languages; Natural language processing systems; Probability distributions; Scalability; Search engines; Sorting; Algorithm engineering; Experimental analysis; Machine translations; NAtural language processing; Natural languages; Space reductions; State-of-the-art algorithms; Trie data structures; Computational linguistics
A multi-view-based collective entity linking method,2019,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062329874&doi=10.1145%2f3300197&partnerID=40&md5=b181d3cc72d673a2438926fb1b9e2ae9,"Facing lots of name mentions appearing on the web, entity linking is essential for many information processing applications. To improve linking accuracy, the relations between entities are usually considered in the linking process. This kind of method is called collective entity linking and can obtain high-quality results. There are two kinds of information helpful to reveal the relations between entities, i.e., contextual information and structural information of entities. Most traditional collective entity linking methods consider them separately. In fact, these two kinds of information represent entities from specific and diverse views and can enhance each other, respectively. Besides, if we look into each view closely, it can be separated into sub-views that are more meaningful. For this reason, this article proposes a multi-view-based collective entity linking algorithm, which combines several views of entities into an objective function for entity linking. The importance of each view can be valued and the linking results can be obtained along with resolving this objective function. Experimental results demonstrate that our linking algorithm can acquire higher accuracy than many state-of-the-art entity linking methods. Besides, since we simplify the entity's structure and change the entity linking to a sub-matrix searching problem, our algorithm also obtains high efficiency. © 2019 Association for Computing Machinery.",Contextual information; Gradient-descent; Multi-view-based entity linking; Structural information; Weighing process,Information systems; Contextual information; Gradient descent; Linking algorithms; Multi-views; Objective functions; Processing applications; State of the art; Structural information; Computer networks
Top-N recommendation with multi-channel positive feedback using factorization machines,2019,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062328543&doi=10.1145%2f3291756&partnerID=40&md5=4c3a775715e8c127c22c6e6e0dfc3879,"User interactions can be considered to constitute different feedback channels, for example, view, click, like or follow, that provide implicit information on users' preferences. Each implicit feedback channel typically carries a unary, positive-only signal that can be exploited by collaborative filtering models to generate lists of personalized recommendations. This article investigates how a learning-to-rank recommender system can best take advantage of implicit feedback signals from multiple channels. We focus on Factorization Machines (FMs) with Bayesian Personalized Ranking (BPR), a pairwise learning-to-rank method, that allows us to experiment with different forms of exploitation. We perform extensive experiments on three datasets with multiple types of feedback to arrive at a series of insights. We compare conventional, direct integration of feedback types with our proposed method, which exploits multiple feedback channels during the sampling process of training. We refer to our method as multi-channel sampling. Our results show that multi-channel sampling outperforms conventional integration, and that sampling with the relative “level” of feedback is always superior to a level-blind sampling approach. We evaluate our method experimentally on three datasets in different domains and observe that with our multi-channel sampler the accuracy of recommendations can be improved considerably compared to the state-of-the-art models. Further experiments reveal that the appropriate sampling method depends on particular properties of datasets such as popularity skewness. © 2019 Association for Computing Machinery.",Factorizaion machines; Implicit feedback; Learning-to-rank; Multi-channel feedback,Collaborative filtering; Direct integration; Factorization machines; Implicit feedback; Implicit informations; Learning to rank; Multi channel; Multi-channel samplings; Personalized recommendation; Factorization
SMaph: A piggyback approach for entity-linking in web queries,2019,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061257131&doi=10.1145%2f3284102&partnerID=40&md5=f41638b6f31898d7f58f79d073e853a6,"We study the problem of linking the terms of a web-search query to a semantic representation given by the set of entities (a.k.a. concepts) mentioned in it. We introduce SMAPH, a system that performs this task using the information coming from a web search engine, an approach we call “piggybacking.” We employ search engines to alleviate the noise and irregularities that characterize the language of queries. Snippets returned as search results also provide a context for the query that makes it easier to disambiguate the meaning of the query. From the search results, SMAPH builds a set of candidate entities with high coverage. This set is filtered by linking back the candidate entities to the terms occurring in the input query, ensuring high precision. A greedy disambiguation algorithm performs this filtering; it maximizes the coherence of the solution by iteratively discovering the pertinent entities mentioned in the query. We propose three versions of SMAPH that outperform state-of-the-art solutions on the known benchmarks and on the GERDAQ dataset, a novel dataset that we have built specifically for this problem via crowd-sourcing and that we make publicly available. © 2018 is held by the owner/author(s). Publication rights licensed to ACM.",Entity-linking; ERD; Piggyback; Query annotation,Information retrieval; Iterative methods; Semantics; Websites; Entity-linking; High-precision; Piggyback; Query annotation; Semantic representation; State of the art; Web search queries; Search engines
Spatiotemporal representation learning for translation-based POI recommendation,2019,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061293869&doi=10.1145%2f3295499&partnerID=40&md5=8bbd6e4725ba88188954fd2a68dce95d,"The increasing proliferation of location-based social networks brings about a huge volume of user check-in data, which facilitates the recommendation of points of interest (POIs). Time and location are the two most important contextual factors in the user's decision-making for choosing a POI to visit. In this article, we focus on the spatiotemporal context-aware POI recommendation, which considers the joint effect of time and location for POI recommendation. Inspired by the recent advances in knowledge graph embedding, we propose a spatiotemporal context-aware and translation-based recommender framework (STA) to model the third-order relationship among users, POIs, and spatiotemporal contexts for large-scale POI recommendation. Specifically, we embed both users and POIs into a “transition space” where spatiotemporal contexts (i.e., a <time, location> pair) are modeled as translation vectors operating on users and POIs. We further develop a series of strategies to exploit various correlation information to address the data sparsity and cold-start issues for new spatiotemporal contexts, new users, and new POIs. We conduct extensive experiments on two real-world datasets. The experimental results demonstrate that our STA framework achieves the superior performance in terms of high recommendation accuracy, robustness to data sparsity, and effectiveness in handling the cold-start problem. © 2019 Association for Computing Machinery.",Contextual modeling; Location-based social networks; POI recommendation; Spatiotemporal aware,Decision making; Vector spaces; Cold start problems; Contextual modeling; Location-based social networks; POI recommendation; Points of interest; Real-world datasets; Recommendation accuracy; Spatiotemporal aware; Location
A context-aware user-item representation learning for item recommendation,2019,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061247620&doi=10.1145%2f3298988&partnerID=40&md5=f0e7ede3711e7d3565b0d63460588bcc,"Both reviews and user-item interactions (i.e., rating scores) have been widely adopted for user rating prediction. However, these existing techniques mainly extract the latent representations for users and items in an independent and static manner. That is, a single static feature vector is derived to encode user preference without considering the particular characteristics of each candidate item. We argue that this static encoding scheme is incapable of fully capturing users' preferences, because users usually exhibit different preferences when interacting with different items. In this article, we propose a novel context-aware user-item representation learning model for rating prediction, named CARL. CARL derives a joint representation for a given user-item pair based on their individual latent features and latent feature interactions. Then, CARL adopts Factorization Machines to further model higher order feature interactions on the basis of the user-item pair for rating prediction. Specifically, two separate learning components are devised in CARL to exploit review data and interaction data, respectively: review-based feature learning and interaction-based feature learning. In the review-based learning component, with convolution operations and attention mechanism, the pair-based relevant features for the given user-item pair are extracted by jointly considering their corresponding reviews. However, these features are only reivew-driven and may not be comprehensive. Hence, an interaction-based learning component further extracts complementary features from interaction data alone, also on the basis of user-item pairs. The final rating score is then derived with a dynamic linear fusion mechanism. Experiments on seven real-world datasets show that CARL achieves significantly better rating prediction accuracy than existing state-of-the-art alternatives. Also, with the attention mechanism, we show that the pair-based relevant information (i.e., context-aware information) in reviews can be highlighted to interpret the rating prediction for different user-item pairs. © 2019 Association for Computing Machinery.",Neural networks; Rating prediction; Recommendation systems,Encoding (symbols); Machine learning; Neural networks; Recommender systems; Attention mechanisms; Complementary features; Context-aware informations; Factorization machines; Feature interactions; Prediction accuracy; Real-world datasets; Relevant features; Forecasting
ELSA: A multilingual document summarization algorithm based on frequent itemsets and latent semantic analysis,2019,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061279783&doi=10.1145%2f3298987&partnerID=40&md5=b388c1484e38f3a04eb22d46a9d8a9f7,"Sentence-based summarization aims at extracting concise summaries of collections of textual documents. Summaries consist of a worthwhile subset of document sentences. The most effective multilingual strategies rely on Latent Semantic Analysis (LSA) and on frequent itemset mining, respectively. LSA-based summarizers pick the document sentences that cover the most important concepts. Concepts are modeled as combinations of single-document terms and are derived from a term-by-sentence matrix by exploiting Singular Value Decomposition (SVD). Itemset-based summarizers pick the sentences that contain the largest number of frequent itemsets, which represent combinations of frequently co-occurring terms. The main drawbacks of existing approaches are (i) the inability of LSA to consider the correlation between combinations of multiple-document terms and the underlying concepts, (ii) the inherent redundancy of frequent itemsets because similar itemsets may be related to the same concept, and (iii) the inability of itemset-based summarizers to correlate itemsets with the underlying document concepts. To overcome the issues of both of the abovementioned algorithms, we propose a new summarization approach that exploits frequent itemsets to describe all of the latent concepts covered by the documents under analysis and LSA to reduce the potentially redundant set of itemsets to a compact set of uncorrelated concepts. The summarizer selects the sentences that cover the latent concepts with minimal redundancy. We tested the summarization algorithm on both multilingual and English-language benchmark document collections. The proposed approach performed significantly better than both itemset- and LSA-based summarizers, and better than most of the other state-of-the-art approaches. © 2019 Association for Computing Machinery.",Frequent weighted itemset mining; Multilingual summarization; Text mining,Data mining; Redundancy; Semantics; Singular value decomposition; Frequent itemset mining; Inherent redundancy; Itemset mining; Latent Semantic Analysis; Multilingual documents; Multilingual summarization; State-of-the-art approach; Text mining; Natural language processing systems
Seed-guided topic model for document filtering and classification,2019,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061266905&doi=10.1145%2f3238250&partnerID=40&md5=a749a55fb89c5b5e896d119df7ad064b,"One important necessity is to filter out the irrelevant information and organize the relevant information into meaningful categories. However, developing text classifiers often requires a large number of labeled documents as training examples. Manually labeling documents is costly and time-consuming. More importantly, it becomes unrealistic to know all the categories covered by the documents beforehand. Recently, a few methods have been proposed to label documents by using a small set of relevant keywords for each category, known as dataless text classification. In this article, we propose a seed-guided topic model for the dataless text filtering and classification (named DFC). Given a collection of unlabeled documents, and for each specified category a small set of seed words that are relevant to the semantic meaning of the category, DFC filters out the irrelevant documents and classifies the relevant documents into the corresponding categories through topic influence. DFC models two kinds of topics: category-topics and general-topics. Also, there are two kinds of category-topics: relevant-topics and irrelevant-topics. Each relevant-topic is associated with one specific category, representing its semantic meaning. The irrelevant-topics represent the semantics of the unknown categories covered by the document collection. And the general-topics capture the global semantic information. DFC assumes that each document is associated with a single category-topic and a mixture of general-topics. A novelty of the model is that DFC learns the topics by exploiting the explicit word co-occurrence patterns between the seed words and regular words (i.e., non-seed words) in the document collection. A document is then filtered, or classified, based on its posterior category-topic assignment. Experiments on two widely used datasets show that DFC consistently outperforms the state-of-the-art dataless text classifiers for both classification with filtering and classification without filtering. In many tasks, DFC can also achieve comparable or even better classification accuracy than the state-of-the-art supervised learning solutions. Our experimental results further show that DFC is insensitive to the tuning parameters. Moreover, we conduct a thorough study about the impact of seed words for existing dataless text classification techniques. The results reveal that it is not using more seed words but the document coverage of the seed words for the corresponding category that affects the dataless classification performance. © 2018 ACM",,Classification (of information); Classifiers; Semantics; Text processing; Classification accuracy; Classification performance; Document collection; Document filtering; Relevant documents; Semantic information; Text classification; Unlabeled documents; Information retrieval systems
A deep Bayesian tensor-based system for video recommendation,2019,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061251803&doi=10.1145%2f3233773&partnerID=40&md5=a9d9d615b4029d52f828f7202adec948,"With the availability of abundant online multi-relational video information, recommender systems that can effectively exploit these sorts of data and suggest creatively interesting items will become increasingly important. Recent research illustrates that tensor models offer effective approaches for complex multi-relational data learning and missing element completion. So far, most tensor-based user clustering models have focused on the accuracy of recommendation. Given the dynamic nature of online media, recommendation in this setting is more challenging as it is difficult to capture the users' dynamic topic distributions in sparse data settings as well as to identify unseen items as candidates of recommendation. Targeting at constructing a recommender system that can encourage more creativity, a deep Bayesian probabilistic tensor framework for tag and item recommendation is proposed. During the score ranking processes, a metric called Bayesian surprise is incorporated to increase the creativity of the recommended candidates. The new algorithm, called Deep Canonical PARAFAC Factorization (DCPF), is evaluated on both synthetic and large-scale real-world problems. An empirical study for video recommendation demonstrates the superiority of the proposed model and indicates that it can better capture the latent patterns of interactions and generates interesting recommendations based on creative tag combinations. © 2018 ACM",Bayesian methods; Computational creativity; Tensor decomposition,Bayesian networks; Online systems; Tensors; Bayesian methods; Computational creativities; Effective approaches; Empirical studies; Real-world problem; Tensor decomposition; Topic distributions; Video information; Recommender systems
Binary sketches for secondary filtering,2019,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061239953&doi=10.1145%2f3231936&partnerID=40&md5=8cc5708075b1f0a80b0c90015e4942c8,"This article addresses the problem of matching the most similar data objects to a given query object. We adopt a generic model of similarity that involves the domain of objects and metric distance functions only. We examine the case of a large dataset in a complex data space, which makes this problem inherently difficult. Many indexing and searching approaches have been proposed, but they have often failed to efficiently prune complex search spaces and access large portions of the dataset when evaluating queries. We propose an approach to enhancing the existing search techniques to significantly reduce the number of accessed data objects while preserving the quality of the search results. In particular, we extend each data object with its sketch, a short binary string in Hamming space. These sketches approximate the similarity relationships in the original search space, and we use them to filter out non-relevant objects not pruned by the original search technique. We provide a probabilistic model to tune the parameters of the sketch-based filtering separately for each query object. Experiments conducted with different similarity search techniques and real-life datasets demonstrate that the secondary filtering can speed-up similarity search several times. © 2018 ACM",Binary sketch; Filter and refine; Hamming space; Similarity search,Information analysis; Binary sketch; Complex search spaces; Generic modeling; Hamming space; Metric distances; Probabilistic modeling; Real life datasets; Similarity search; Large dataset
Shallow and deep syntactic/semantic structures for passage reranking in question-answering systems,2019,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061246074&doi=10.1145%2f3233772&partnerID=40&md5=9bc8face218e0ac9dd03ca691e072e55,"In this article, we extensively study the use of syntactic and semantic structures obtained with shallow and full syntactic parsers for answer passage reranking. We propose several dependency and constituent-based structures, also enriched with Linked Open Data (LD) knowledge to represent pairs of questions and answer passages. We encode such tree structures in learning-to-rank (L2R) algorithms using tree kernels, which can project them in tree substructure spaces, where each dimension represents a powerful syntactic/semantic feature. Additionally, since we define links between question and passage structures, our tree kernel spaces also include relational structural features. We carried out an extensive comparative experimentation of our models for automatic answer selection benchmarks on different TREC QA corpora as well as the newer Wikipedia-based dataset, namely WikiQA, which has been widely used to test sentence rerankers. The results consistently demonstrate that our structural semantic models achieve the state of the art in passage reranking. In particular, we derived the following important findings: (i) relational syntactic structures are essential to achieve superior results; (ii) models trained with dependency trees can outperform those trained with shallow trees, e.g., in case of sentence reranking; (iii) external knowledge automatically generated with focus and question classifiers is very effective; and (iv) the semantic information derived by LD and incorporated in syntactic structures can be used to replace the knowledge provided by the above-mentioned classifiers. This is a remarkable advantage as it enables our models to increase coverage and portability over new domains. © 2018 ACM",Kernel methods; Learning to rank; Linked data; Question answering; Structural kernels,Classification (of information); Forestry; Linked data; Natural language processing systems; Open Data; Semantics; Statistical tests; Trees (mathematics); Automatically generated; Kernel methods; Learning to rank; Question Answering; Question answering systems; Semantic information; Structural kernels; Structural semantics; Syntactics
Adversarial distillation for efficient recommendation with external knowledge,2019,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061283194&doi=10.1145%2f3281659&partnerID=40&md5=75ba3f0093d662beb719b0417b5ca24b,"Integrating external knowledge into the recommendation system has attracted increasing attention in both industry and academic communities. Recent methods mostly take the power of neural network for effective knowledge representation to improve the recommendation performance. However, the heavy deep architectures in existing models are usually incorporated in an embedded manner, which may greatly increase the model complexity and lower the runtime efficiency. To simultaneously take the power of deep learning for external knowledge modeling as well as maintaining the model efficiency at test time, we reformulate the problem of recommendation with external knowledge into a generalized distillation framework. The general idea is to free the complex deep architecture into a separate model, which is only used in the training phrase, while abandoned at test time. In particular, in the training phrase, the external knowledge is processed by a comprehensive teacher model to produce valuable information to teach a simple and efficient student model. Once the framework is learned, the teacher model is abandoned, and only the succinct yet enhanced student model is used to make fast predictions at test time. In this article, we specify the external knowledge as user review, and to leverage it in an effective manner, we further extend the traditional generalized distillation framework by designing a Selective Distillation Network (SDNet) with adversarial adaption and orthogonality constraint strategies to make it more robust to noise information. Extensive experiments verify that our model can not only improve the performance of rating prediction, but also can significantly reduce time consumption when making predictions as compared with several state-of-the-art methods. © 2018 Association for Computing Machinery.",Adversarial training; Distillation network; External knowledge; Personalization; Recommendation system,Complex networks; Deep learning; Distillation; Efficiency; Forecasting; Knowledge management; Knowledge representation; Network architecture; Personnel training; Academic community; Deep architectures; External knowledge; Orthogonality constraints; Personalizations; Recommendation performance; Run-time efficiency; State-of-the-art methods; Recommender systems
Jointly minimizing the expected costs of review for responsiveness and privilege in e-discovery,2019,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061260572&doi=10.1145%2f3268928&partnerID=40&md5=7635a1d5104c120991cced8c9ed4011f,"Discovery is an important aspect of the civil litigation process in the United States of America, in which all parties to a lawsuit are permitted to request relevant evidence from other parties. With the rapid growth of digital content, the emerging need for “e-discovery” has created a strong demand for techniques that can be used to review massive collections both for “responsiveness” (i.e., relevance) to the request and for “privilege” (i.e., presence of legally protected content that the party performing the review may have a right to withhold). In this process, the party performing the review may incur costs of two types, namely, annotation costs (deriving from the fact that human reviewers need to be paid for their work) and misclassification costs (deriving from the fact that failing to correctly determine the responsiveness or privilege of a document may adversely affect the interests of the parties in various ways). Relying exclusively on automatic classification would minimize annotation costs but could result in substantial misclassification costs, while relying exclusively on manual classification could generate the opposite consequences. This article proposes a risk minimization framework (called MINECORE, for “minimizing the expected costs of review”) that seeks to strike an optimal balance between these two extreme stands. In MINECORE (a) the documents are first automatically classified for both responsiveness and privilege, and then (b) some of the automatically classified documents are annotated by human reviewers for responsiveness (typically by junior reviewers) and/or, in cascade, for privilege (typically by senior reviewers), with the overall goal of minimizing the expected cost (i.e., the risk) of the entire process. Risk minimization is achieved by optimizing, for both responsiveness and privilege, the choice of which documents to manually review. We present a simulation study in which classes from a standard text classification test collection (RCV1-v2) are used as surrogates for responsiveness and privilege. The results indicate that MINECORE can yield substantially lower total cost than any of a set of strong baselines. © 2018 Association for Computing Machinery.",E-discovery; Semi-automated text classification; Technology-assisted review; Utility theory,Classification (of information); Laws and legislation; Automatic classification; Classified documents; E discoveries; Manual classification; Misclassification costs; Text classification; United States of America; Utility theory; Text processing
Brotli: A general-purpose data compressor,2019,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061227512&doi=10.1145%2f3231935&partnerID=40&md5=aeec1ff463631feae9e314a32a08b147,"Brotli is an open source general-purpose data compressor introduced by Google in late 2013 and now adopted in most known browsers and Web servers. It is publicly available on GitHub and its data format was submitted as RFC 7932 in July 2016. Brotli is based on the Lempel-Ziv compression scheme and planned as a generic replacement of Gzip and ZLib. The main goal in its design was to compress data on the Internet, which meant optimizing the resources used at decoding time, while achieving maximal compression density. This article is intended to provide the first thorough, systematic description of the Brotli format as well as a detailed computational and experimental analysis of the main algorithmic blocks underlying the current encoder implementation, together with a comparison against compressors of different families constituting the state-of-the-art either in practice or in theory. This treatment will allow us to raise a set of new algorithmic and software engineering problems that deserve further attention from the scientific community. © 2018 Association for Computing Machinery.",Data compression; Experiments; Lempel-Ziv parsing; NP-completeness; Shortest paths; Treaps,Computation theory; Data compression; Experiments; Open source software; Encoder implementation; Experimental analysis; Lempel-ziv compression schemes; Lempel-Ziv parsing; Np-completeness; Scientific community; Shortest path; Treaps; Compressors
Transfer to rank for heterogeneous one-class collaborative filtering,2019,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061297402&doi=10.1145%2f3243652&partnerID=40&md5=b2d933306555457edc633fd74d940f7e,"Heterogeneous one-class collaborative filtering is an emerging and important problem in recommender systems, where two different types of one-class feedback, i.e., purchases and browses, are available as input data. The associated challenges include ambiguity of browses, scarcity of purchases, and heterogeneity arising from different feedback. In this article, we propose to model purchases and browses from a new perspective, i.e., users' roles of mixer, browser and purchaser. Specifically, we design a novel transfer learning solution termed role-based transfer to rank (RoToR), which contains two variants, i.e., integrative RoToR and sequential RoToR. In integrative RoToR, we leverage browses into the preference learning task of purchases, in which we take each user as a sophisticated customer (i.e., mixer) that is able to take different types of feedback into consideration. In sequential RoToR, we aim to simplify the integrative one by decomposing it into two dependent phases according to a typical shopping process. Furthermore, we instantiate both variants using different preference learning paradigms such as pointwise preference learning and pairwise preference learning. Finally, we conduct extensive empirical studies with various baseline methods on three large public datasets and find that our RoToR can perform significantly more accurate than the state-of-the-art methods. © 2019 Association for Computing Machinery.",Heterogeneous one-class collaborative filtering; One-class feedback; Role-based recommendation; Transfer to rank,Large dataset; Mixers (machinery); Sales; Baseline methods; Empirical studies; Input datas; Preference learning; Role-based; State-of-the-art methods; Transfer learning; Transfer to rank; Collaborative filtering
MMalfM: Explainable recommendation by leveraging reviews and images,2019,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060056583&doi=10.1145%2f3291060&partnerID=40&md5=1ba13e6d901d2e50cc154bf75ae32887,"Personalized rating prediction is an important research problem in recommender systems. Although the latent factor model (e.g., matrix factorization) achieves good accuracy in rating prediction, it suffers from many problems including cold-start, non-transparency, and suboptimal results for individual user-item pairs. In this article, we exploit textual reviews and item images together with ratings to tackle these limitations. Specifically, we first apply a proposed multi-modal aspect-aware topic model (MATM) on text reviews and item images to model users' preferences and items' features from different aspects, and also estimate the aspect importance of a user toward an item. Then, the aspect importance is integrated into a novel aspect-aware latent factor model (ALFM), which learns user's and item's latent factors based on ratings. In particular, ALFM introduces a weight matrix to associate those latent factors with the same set of aspects in MATM, such that the latent factors could be used to estimate aspect ratings. Finally, the overall rating is computed via a linear combination of the aspect ratings, which are weighted by the corresponding aspect importance. To this end, our model could alleviate the data sparsity problem and gain good interpretability for recommendation. Besides, every aspect rating is weighted by its aspect importance, which is dependent on the targeted user's preferences and the targeted item's features. Therefore, it is expected that the proposed method can model a user's preferences on an item more accurately for each user-item pair. Comprehensive experimental studies have been conducted on the Yelp 2017 Challenge dataset and Amazon product datasets. Results show that (1) our method achieves significant improvement compared to strong baseline methods, especially for users with only few ratings; (2) item visual features can improve the prediction performance-the effects of item image features on improving the prediction results depend on the importance of the visual features for the items; and (3) our model can explicitly interpret the predicted results in great detail. © 2019 Association for Computing Machinery.",Aspect; Explainable recommendation; Latent factor model; Multi-modal; Rating prediction,Factorization; Forecasting; Matrix algebra; Aspect; Data sparsity problems; Explainable recommendation; Latent factor models; Linear combinations; Matrix factorizations; Multi-modal; Prediction performance; Image enhancement
Understanding faceted search from data science and human factor perspectives,2019,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060058888&doi=10.1145%2f3284101&partnerID=40&md5=4759c5e4a311d4e8e6fab28e4af2e47e,"Faceted search has become a common feature on most search interfaces in e-commerce websites, digital libraries, government's open information portals, and so on. Beyond the existing studies on developing algorithms for faceted search and empirical studies on facet usage, this study investigated user real-time interactions with facets over the course of a search from both data science and human factor perspectives. It adopted a Random Forest (RF) model to successfully predict facet use using search dynamic variables. In addition, the RF model provided a ranking of variables by their predictive power, which suggests that the search process follows rhythmic flow of a sequence within which facet addition is mostly influenced by its immediately preceding action. In the follow-up user study, we found that participants used facets at critical points from the beginning to end of search sessions. Participants used facets for distinctive reasons at different stages. They also used facets implicitly without applying the facets to their search. Most participants liked the faceted search, although a few participants were concerned about the choice overload introduced by facets. The results of this research can be used to understand information seekers and propose or refine a set of practical design guidelines for faceted search. © 2019 Copyright held by the owner/author(s).",Faceted search; Predictive analytics; Random forest; Server logs; User study,Decision trees; Human engineering; Predictive analytics; E-commerce websites; Empirical studies; Faceted search; Information portals; Random forests; Real time interactions; Server logs; User study; Digital libraries
Attentive long short-term preference modeling for personalized product search,2019,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060061038&doi=10.1145%2f3295822&partnerID=40&md5=923fbc570c3a1239dfa35d401e7dfa55,"E-commerce users may expect different products even for the same query, due to their diverse personal preferences. It is well known that there are two types of preferences: long-term ones and short-term ones. The former refers to users' inherent purchasing bias and evolves slowly. By contrast, the latter reflects users' purchasing inclination in a relatively short period. They both affect users' current purchasing intentions. However, few research efforts have been dedicated to jointly model them for the personalized product search. To this end, we propose a novel Attentive Long Short-Term Preference model, dubbed as ALSTP, for personalized product search. Our model adopts the neural networks approach to learn and integrate the long- and short-term user preferences with the current query for the personalized product search. In particular, two attention networks are designed to distinguish which factors in the short-term as well as long-term user preferences are more relevant to the current query. This unique design enables our model to capture users' current search intentions more accurately. Our work is the first to apply attention mechanisms to integrate both long- and short-term user preferences with the given query for the personalized search. Extensive experiments over four Amazon product datasets show that our model significantly outperforms several state-of-the-art product search methods in terms of different evaluation metrics. © 2019 Association for Computing Machinery.",Attention mechanism; Long short-term preference; Personalized product search,Computer networks; Information systems; Attention mechanisms; Evaluation metrics; Personalized products; Personalized search; Preference modeling; Search intentions; Short term; State of the art; Sales
Fine-grained geolocation of tweets in temporal proximity,2019,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060043964&doi=10.1145%2f3291059&partnerID=40&md5=9516313404c87f3c1923614195c63320,"In fine-grained tweet geolocation, tweets are linked to the specific venues (e.g., restaurants, shops) from which they were posted. This explicitly recovers the venue context that is essential for applications such as location-based advertising or user profiling. For this geolocation task, we focus on geolocating tweets that are contained in tweet sequences. In a tweet sequence, tweets are posted from some latent venue(s) by the same user and within a short time interval. This scenario arises from two observations: (1) It is quite common that users post multiple tweets in a short time and (2) most tweets are not geocoded. To more accurately geolocate a tweet, we propose a model that performs query expansion on the tweet (query) using two novel approaches. The first approach temporal query expansion considers users' staying behavior around venues. The second approach visitation query expansion leverages on user revisiting the same or similar venues in the past. We combine both query expansion approaches via a novel fusion framework and overlay them on a Hidden Markov Model to account for sequential information. In our comprehensive experiments across multiple datasets and metrics, we show our proposed model to be more robust and accurate than other baselines. © 2019 Association for Computing Machinery.",Staying behavior; Temporal proximity; Tweet geolocation,Hidden Markov models; Geolocations; Location-based advertising; Multiple data sets; Sequential information; Short time intervals; Staying behavior; Temporal proximity; Temporal queries; Expansion
Product-based neural networks for user response prediction over multi-field categorical data,2018,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056454292&doi=10.1145%2f3233770&partnerID=40&md5=8b2080a522f2dfa3171ecbfe42ebd3ed,"User response prediction is a crucial component for personalized information retrieval and fltering scenarios, such as recommender system and web search. The data in user response prediction is mostly in a multi-feld categorical format and transformed into sparse representations via one-hot encoding. Due to the sparsity problems in representation and optimization, most research focuses on feature engineering and shallow modeling. Recently, deep neural networks have attracted research attention on such a problem for their high capacity and end-to-end training scheme. In this article, we study user response prediction in the scenario of click prediction. We frst analyze a coupled gradient issue in latent vector-based models and propose kernel product to learn feld-aware feature interactions. Then, we discuss an insensitive gradient issue in DNN-based models and propose Product-based Neural Network, which adopts a feature extractor to explore feature interactions. Generalizing the kernel product to a net-in-net architecture, we further propose Productnetwork in Network (PIN), which can generalize previous models. Extensive experiments on four industrial datasets and one contest dataset demonstrate that our models consistently outperform eight baselines on both area under curve and log loss. Besides, PIN makes great click-through rate improvement (relatively 34.67%) in online A/B test. © 2018 ACM.",Deep learning; Product-based neural network; Recommender system,Deep learning; Deep neural networks; Forecasting; Recommender systems; State assignment; Click-through rate; Feature engineerings; Feature extractor; Feature interactions; Personalized information retrieval; Response prediction; Sparse representation; Sparsity problems; Search engines
Learning to adaptively rank document retrieval system configurations,2018,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056486729&doi=10.1145%2f3231937&partnerID=40&md5=6f123d6711d3a06d3f8dfeb77bf0db2a,"Modern Information Retrieval (IR) systems have become more and more complex, involving a large number of parameters. For example, a system may choose from a set of possible retrieval models (BM25, language model, etc.), or various query expansion parameters, whose values greatly influence the overall retrieval effectiveness. Traditionally, these parameters are set at a system level based on training queries, and the same parameters are then used for diflerent queries. We observe that it may not be easy to set all these parameters separately, since they can be dependent. In addition, a global setting for all queries may not best ft all individual queries with different characteristics. The parameters should be set according to these characteristics. In this article, we propose a novel approach to tackle this problem by dealing with the entire system confgurations (i.e., a set of parameters representing an IR system behaviour) instead of selecting a single parameter at a time. The selection of the best confguration is cast as a problem of ranking different possible confgurations given a query. We apply learning-to-rank approaches for this task. We exploit both the query features and the system confguration features in the learning-to-rank method so that the selection of confguration is query dependent. The experiments we conducted on four TREC ad hoc collections show that this approach can signifcantly outperform the traditional method to tune system confguration globally (i.e., grid search) and leads to higher effectiveness than the top performing systems of the TREC tracks. We also perform an ablation analysis on the impact of different features on the model learning capability and show that query expansion features are among the most important for adaptive systems. © 2018 ACM.",Adaptive information retrieval; Data analytics; Information retrieval; Information systems; Learning to rank; Query features; Retrieval system parameters,Information retrieval; Information systems; Adaptive information retrieval; Data analytics; Learning to rank; Query features; Retrieval systems; Search engines
Efficient learning-based recommendation algorithms for top-N tasks and top-N workers in large-scale crowdsourcing systems,2018,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056463832&doi=10.1145%2f3231934&partnerID=40&md5=2935e263995463ba81313a328610084c,"The task and worker recommendation problems in crowdsourcing systems have brought up unique characteristics that are not present in traditional recommendation scenarios, i.e., the huge flow of tasks with short lifespans, the importance of workers' capabilities, and the quality of the completed tasks. These unique features make traditional recommendation approaches no longer satisfactory for task and worker recommendation in crowdsourcing systems. In this article, we propose a two-tier data representation scheme (defning a worker-category suitability score and a worker-task attractiveness score) to support personalized task and worker recommendations. We also extend two optimization methods, namely least mean square error and Bayesian personalized rank, to better ft the characteristics of task/worker recommendation in crowdsourcing systems. We then integrate the proposed representation scheme and the extended optimization methods along with the two adapted popular learning models, i.e., matrix factorization and kNN, and result in two lines of top-N recommendation algorithms for crowdsourcing systems: (1) Top-N-Tasks recommendation algorithms for discovering the top-N most suitable tasks for a given worker and (2) Top-N-Workers recommendation algorithms for identifying the top-N best workers for a task requester. An extensive experimental study is conducted that validates the effectiveness and efciency of a broad spectrum of algorithms, accompanied by our analysis and the insights gained. © 2018 ACM.",Crowd computing; Crowdsourcing; Machine learning; Ranking algorithms; Task recommendation,Crowdsourcing; Factorization; Learning systems; Least squares approximations; Mean square error; Crowd computing; Data representations; Least mean square error; Matrix factorizations; Ranking algorithm; Recommendation algorithms; Representation schemes; Task recommendation; Learning algorithms
Personalized context-aware point of interest recommendation,2018,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050921794&doi=10.1145%2f3231933&partnerID=40&md5=ee25174f6d5bbe981fd78dc9de78914b,"Personalized recommendation of Points of Interest (POIs) plays a key role in satisfying users on Location-Based Social Networks (LBSNs). In this article, we propose a probabilistic model to find the mapping between user-annotated tags and locations' taste keywords. Furthermore, we introduce a dataset on locations' contextual appropriateness and demonstrate its usefulness in predicting the contextual relevance of locations. We investigate four approaches to use our proposed mapping for addressing the data sparsity problem: one model to reduce the dimensionality of location taste keywords and three models to predict user tags for a new location. Moreover, we present different scores calculated from multiple LBSNs and show how we incorporate new information from the mapping into a POI recommendation approach. Then, the computed scores are integrated using learning to rank techniques. The experiments on two TREC datasets show the effectiveness of our approach, beating state-of-the-art methods. © 2018 ACM 1046-8188/2018/10-ART45 $15.00",Content-based recommendation; Contextual suggestion; Location-based social networks; Point of interest recommendation; User modeling,Mapping; Social networking (online); Content-based recommendation; Contextual suggestion; Location-based social networks; Point of interest; User Modeling; Location
Interactive intent modeling for exploratory search,2018,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058345181&doi=10.1145%2f3231593&partnerID=40&md5=ceb0d343fc65d7f0827912fe70a76d9e,"Exploratory search requires the system to assist the user in comprehending the information space and expressing evolving search intents for iterative exploration and retrieval of information. We introduce interactive intent modeling, a technique that models a user's evolving search intents and visualizes them as keywords for interaction. The user can provide feedback on the keywords, from which the system learns and visualizes an improved intent estimate and retrieves information. We report experiments comparing variants of a system implementing interactive intent modeling to a control system. Data comprising search logs, interaction logs, essay answers, and questionnaires indicate significant improvements in task performance, information retrieval performance over the session, information comprehension performance, and user experience. The improvements in retrieval effectiveness can be attributed to the intent modeling and the effect on users' task performance, breadth of information comprehension, and user experience are shown to be dependent on a richer visualization. Our results demonstrate the utility of combining interactive modeling of search intentions with interactive visualization of the models that can benefit both directing the exploratory search process and making sense of the information space. Our findings can help design personalized systems that support exploratory information seeking and discovery of novel information. © 2018 Association for Computing Machinery. All rights reserved.",Proactive search; User intent modeling,Iterative methods; Surveys; Visualization; Comprehension performance; Information retrieval performance; Information seeking; Intent models; Interactive modeling; Interactive visualizations; Proactive search; Retrieval effectiveness; Search engines
From qestion to text: Qestion-oriented feature atention for answer selection,2018,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056450758&doi=10.1145%2f3233771&partnerID=40&md5=ce050b10416beec49a4d2f44c0eb7ae3,"Understanding unstructured texts is an essential skill for human beings as it enables knowledge acquisition. Although understanding unstructured texts is easy for we human beings with good education, it is a great challenge for machines. Recently, with the rapid development of artifcial intelligence techniques, researchers put efforts to teach machines to understand texts and justify the educated machines by letting them solve the questions upon the given unstructured texts, inspired by the reading comprehension test as we humans do. However, feature effectiveness with respect to different questions signifcantly hinders the performance of answer selection, because different questions may focus on various aspects of the given text and answer candidates. To solve this problem, we propose a question-oriented feature attention (QFA) mechanism, which learns to weight different engineering features according to the given question, so that important features with respect to the specifc question is emphasized accordingly. Experiments on MCTest dataset have wellvalidated the effectiveness of the proposed method. Additionally, the proposed QFA is applicable to various IR tasks, such as question answering and answer selection. We have verifed the applicability on a crawled community-based question-answering dataset. © 2018 ACM.",Answer selection; Attention method; Question answering,Computer networks; Information systems; Answer selection; Attention method; Community-based question answering; Important features; Oriented features; Question Answering; Reading comprehension tests; Unstructured texts; Feature extraction
How does domain expertise affect users' search interaction and outcome in exploratory search?,2018,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054038002&doi=10.1145%2f3223045&partnerID=40&md5=7c5c05bbbd39b8a669eb6582b0f594d5,"People often conduct exploratory search to explore unfamiliar information space and learn new knowledge. While supporting the highly dynamic and interactive exploratory search is still challenging for the search system, we want to investigate which factors can make the exploratory search successful and satisfying from the user's perspective. Previous research suggests that domain experts have different search strategies and are more successful in finding domain-specific information, but how the domain expertise level will influence users' interaction and search outcomes in exploratory search, especially in different knowledge domains, is still unclear. In this work, via a carefully designed user study that involves 30 participants, we investigate the influence of domain expertise levels on the interaction and outcome of exploratory search in three different domains: environment, medicine, and politics. We record participants' search behaviors, including their explicit feedback and eye fixation sequences, in a laboratory setting. With this dataset, we identify both domain-independent and domain-dependent effects on user behaviors and search outcomes. Our results extend existing research on the effect of domain expertise in search and suggest different strategies for exploiting domain expertise to support exploratory search in different knowledge domains. © 2018 ACM.",Domain expertise; Exploratory search; User behavior analysis,Expert systems; Different domains; Domain expertise; Domain independents; Domain-specific information; Explicit feedback; Exploratory search; Information spaces; User behavior analysis; Behavioral research
Neural vector spaces for unsupervised information retrieval,2018,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054039864&doi=10.1145%2f3196826&partnerID=40&md5=4036a94d009c30bb6657e2ef8902608a,"We propose the Neural Vector Space Model (NVSM), a method that learns representations of documents in an unsupervised manner for news article retrieval. In the NVSM paradigm, we learn low-dimensional representations of words and documents from scratch using gradient descent and rank documents according to their similarity with query representations that are composed from word representations. We show that NVSM performs better at document ranking than existing latent semantic vector space methods. The addition of NVSM to a mixture of lexical language models and a state-of-the-art baseline vector space model yields a statistically significant increase in retrieval effectiveness. Consequently, NVSM adds a complementary relevance signal. Next to semantic matching, we find that NVSM performs well in cases where lexical matching is needed. NVSM learns a notion of term specificity directly from the document collection without feature engineering. We also show that NVSM learns regularities related to Luhn significance. Finally, we give advice on how to deploy NVSM in situations where model selection (e.g., cross-validation) is infeasible. We find that an unsupervised ensemble of multiple models trained with different hyperparameter values performs better than a single cross-validated model. Therefore, NVSM can safely be used for ranking documents without supervised relevance judgments. © 2018 ACM.",Ad-hoc retrieval; Document retrieval; Latent vector spaces; Representation learning; Semantic matching,Information retrieval; Search engines; Semantics; Vectors; Ad Hoc retrieval; Document Retrieval; Latent vectors; Representation learning; Semantic matching; Vector spaces
Joint modeling of participant influence and latent topics for recommendation in event-based social networks,2018,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046550493&doi=10.1145%2f3183712&partnerID=40&md5=8575a190526001789765b1c4e997f3a8,"Event-based social networks (EBSNs) are becoming popular in recent years. Users can publish a planned event on an EBSN website, calling for other users to participate in the event. When a user is making a decision on whether to participate in an event in EBSNs, one aspect for consideration is existing participants defined as users who have agreed to join this event. Existing participants of the event may affect the decision of the user, to which we refer as participant influence. However, participant influence is not well studied by previous works. In this article, we propose an event recommendation model that considers participant influence, and exploits the influence of existing participants on the decisions of new participants based on Poisson factorization. The effect of participant influence is associated with the target event, the host group of the event, and the location of the event. Furthermore, our proposed model can extract latent event topics from event text descriptions, and characterize events, groups, and locations by distributions of event topics. Associations between latent event topics and participant influence are exploited for improving event recommendation. Besides making event recommendation, the proposed model is able to reveal the semantic properties of the participant influence between two users semantically. We have conducted extensive experiments on some datasets extracted from a real-world EBSN. Our proposed model achieves superior event recommendation performance over several state-of-the-art models. The results demonstrate that the consideration of participant influence can improve event recommendation. © 2018 ACM.",Event-based social networks; Poisson factorization,Factorization; Semantics; Social networking (online); Event-based; Host group; Joint modeling; Real-world; Recommendation performance; Semantic properties; State of the art; Data mining
GeoMF++: Scalable location recommendation via joint geographical modeling and matrix factorization,2018,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046544781&doi=10.1145%2f3182166&partnerID=40&md5=a0e20674645e570f70d5749d6f584f90,"Location recommendation is an important means to help people discover attractive locations. However, extreme sparsity of user-location matrices leads to a severe challenge, so it is necessary to take implicit feedback characteristics of user mobility data into account and leverage the location's spatial information. To this end, based on previously developed GeoMF, we propose a scalable and flexible framework, dubbed GeoMF++, for joint geographical modeling and implicit feedback-based matrix factorization. We then develop an efficient optimization algorithm for parameter learning, which scales linearly with data size and the total number of neighbor grids of all locations. GeoMF++ can be well explained from two perspectives. First, it subsumes two-dimensional kernel density estimation so that it captures spatial clustering phenomenon in user mobility data; Second, it is strongly connected with widely used neighbor additive models, graph Laplacian regularized models, and collectivematrix factorization. Finally, we extensively evaluate GeoMF++ on two large-scale LBSN datasets. The experimental results show that GeoMF++ consistently outperforms the state-of-the-art and other competing baselines on both datasets in terms of NDCG and Recall. Besides, the efficiency studies show that GeoMF++ is much more scalable with the increase of data size and the dimension of latent space. © 2018 ACM.",Geographical modeling; LBSNs; Location recommendation,Factorization; Location; Flexible framework; Geographical models; Kernel Density Estimation; LBSNs; Matrix factorizations; Optimization algorithms; Spatial informations; Strongly connected; Matrix algebra
"Factors influencing users' information requests: Medium, target, and extra-topical dimension",2018,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054019807&doi=10.1145%2f3209624&partnerID=40&md5=0a06565700afc460e5902f81a942aa70,"We report on a crowdsourced study that investigated how two factors influence the way people formulate information requests. Our first factor, medium, considers whether the request is produced using text or voice. Our second factor, target, considers whether the request is intended for a search engine or a human intermediary (i.e., someone who will search on the user's behalf). In particular, we study how these two factors influence the way people formulate requests in situations where the information need has a specific type of extra-topical dimension (i.e., a type of constraint that is independent from the information need's topic). We focus on six extra-topical dimensions: (1) domain knowledge, (2) viewpoint, (3) experiential, (4) venue location, (5) source location, and (6) temporal. The extra-topical dimension was manipulated by giving participants carefully constructed search tasks. We analyzed a large number of information requests produced by study participants, and address three research questions. We study the effects of our two factors (medium and target) on (RQ1) participants' perceptions about their own information requests, (RQ2) the different characteristics of their information requests (e.g., natural language structure, retrieval performance), and (RQ3) participants' strategies for requesting information when the search task has a specific type of extra-topical dimension. Our results found that both factors influenced participants' perceptions about their own information requests, the characteristics of participants' requests, and the strategies adopted by participants to request information matching the extra-topical dimension. Our results have implications for future research on methods that can harness (rather than ignore) extra-topical query terms to retrieve relevant information. © 2018 ACM.",Information requests; Query formulation; Relevance criteria; Search performance; Spoken search,Computer networks; Information systems; Information request; Query formulation; Relevance criteria; Search performance; Spoken search; Search engines
Further insights on drawing sound conclusions from noisy judgments,2018,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047160226&doi=10.1145%2f3186195&partnerID=40&md5=8a2fe6b92daa1b787fc11a4848c398b1,"The effectiveness of a search engine is typically evaluated using hand-labeled datasets, where the labels indicate the relevance of documents to queries. Often the number of labels needed is too large to be created by the best annotators, and so less expensive labels (e.g., from crowdsourcing) are used. This introduces errors in the labels, and thus errors in standard effectiveness metrics (such as P@k and DCG). These errors must be taken into consideration when using the metrics. Previous work has approached assessor error by taking aggregates over multiple inexpensive assessors. We take a different approach and introduce equations and algorithms that can adjust the metrics to the values they would have had if there were no annotation errors. This is especially important when two search engines are compared on their metrics. We give examples where one engine appeared to be statistically significantly better than the other, but the effect disappeared after the metrics were corrected for annotation error. In other words, the evidence supporting a statistical difference was illusory and caused by a failure to account for annotation error. © 2018 ACM.",Precision; Standard error; Statistical significance,Search engines; Annotation errors; Effectiveness metrics; Labeled datasets; Precision; Standard errors; Statistical differences; Statistical significance; Errors
Swipe and tell: Using implicit feedback to predict user engagement on tablets,2018,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054010849&doi=10.1145%2f3185153&partnerID=40&md5=28f6cb0add2e940107bd63f6a08513ab,"When content consumers explicitly judge content positively, we consider them to be engaged. Unfortunately, explicit user evaluations are difficult to collect, as they require user effort. Therefore, we propose to use device interactions as implicit feedback to detect engagement. We assess the usefulness of swipe interactions on tablets for predicting engagement and make the comparison with using traditional features based on time spent. We gathered two unique datasets of more than 250,000 swipes, 100,000 unique article visits, and over 35,000 explicitly judged news articles by modifying two commonly used tablet apps of two newspapers. We tracked all device interactions of 407 experiment participants during one month of habitual news reading. We employed a behavioral metric as a proxy for engagement, because our analysis needed to be scalable to many users, and scanning behavior required us to allow users to indicate engagement quickly. We point out the importance of taking into account content ordering, report the most predictive features, zoom in on briefly read content and on the most frequently read articles. Our findings demonstrate that fine-grained tablet interactions are useful indicators of engagement for newsreaders on tablets. The best features successfully combine both time-based aspects and swipe interactions. © 2018 ACM.",Briefly read content; Content ordering; Dwell time; Frequently read content; Implicit feedback; Newspaper; Online news; Tablets; Touch interactions; User engagement,Computer networks; Information systems; Briefly read content; Content ordering; Dwell time; Frequently read content; Implicit feedback; Newspaper; Online news; Tablets; Touch interaction; User engagement; Newsprint
Sentence relations for extractive summarization with deep neural networks,2018,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047163458&doi=10.1145%2f3200864&partnerID=40&md5=4d7ee89900fcdce278d15fbd549eca46,"Sentence regression is a type of extractive summarization that achieves state-of-the-art performance and is commonly used in practical systems. The most challenging task within the sentence regression framework is to identify discriminative features to represent each sentence. In this article, we study the use of sentence relations, e.g., Contextual Sentence Relations (CSR), Title Sentence Relations (TSR), and Query Sentence Relations (QSR), so as to improve the performance of sentence regression. CSR, TSR, and QSR refer to the relations between a main body sentence and its local context, its document title, and a given query, respectively. We propose a deep neural network model, Sentence Relation-based Summarization (SRSum), that consists of five sub-models, PriorSum, CSRSum, TSRSum, QSRSum, and SFSum. PriorSum encodes the latent semantic meaning of a sentence using a bi-gram convolutional neural network. SFSum encodes the surface information of a sentence, e.g., sentence length, sentence position, and so on. CSRSum, TSRSum, and QSRSum are three sentence relation sub-models correspondingtoCSR, TSR, and QSR, respectively. CSRSum evaluates the ability of each sentence to summarize its local contexts. Specifically, CSRSum applies a CSR-based word-level and sentence-level attention mechanism to simulate the context-aware reading of a human reader, where words and sentences that have anaphoric relations or local summarization abilities are easily remembered and paid attention to. TSRSum evaluates the semantic closeness of each sentence with respect to its title, which usually reflects the main ideas of a document. TSRSum applies a TSR-based attention mechanism to simulate people's reading ability with the main idea (title) in mind. QSRSum evaluates the relevance of each sentence with given queries for the query-focused summarization. QSRSum applies a QSR-based attention mechanism to simulate the attentive reading of a human reader with some queries in mind. The mechanism can recognize which parts of the given queries are more likely answered by a sentence under consideration. Finally as a whole, SRSum automatically learns useful latent features by jointly learning representations of query sentences, content sentences, and title sentences as well as their relations. We conduct extensive experiments on six benchmark datasets, including generic multi-document summarization and query-focused multi-document summarization. On both tasks, SRSum achieves comparable or superior performance compared with state-of-the-art approaches in terms of multiple ROUGE metrics. © 2018 ACM.",Attentive pooling; Extractive summarization; Neural network; Sentence relations,Encoding (symbols); Neural networks; Regression analysis; Semantics; Attentive pooling; Convolutional neural network; Discriminative features; Extractive summarizations; Multi-document summarization; Sentence relations; State-of-the-art approach; State-of-the-art performance; Deep neural networks
Location-aware influence maximization over dynamic social streams,2018,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054014034&doi=10.1145%2f3230871&partnerID=40&md5=037fdaf91f3fb9d69e5aa3851191525a,"Influence maximization (IM), which selects a set of k seed users (a.k.a., a seed set) to maximize the influence spread over a social network, is a fundamental problem in a wide range of applications. However, most existing IM algorithms are static and location-unaware. They fail to provide high-quality seed sets efficiently when the social network evolves rapidly and IM queries are location-aware. In this article, we first define two IM queries, namely Stream Influence Maximization (SIM) and Location-aware SIM (LSIM), to track influential users over social streams. Technically, SIM adopts the sliding window model and maintains a seed set with the maximum influence value collectively over the most recent social actions. LSIM further considers social actions are associated with geo-tags and identifies a seed set that maximizes the influence value in a query region over a location-aware social stream. Then, we propose the Sparse Influential Checkpoints (SIC) framework for efficient SIM query processing. SIC maintains a sequence of influential checkpoints over the sliding window and each checkpoint maintains a partial solution for SIM in an append-only substream of social actions. Theoretically, SIC keeps a logarithmic number of checkpoints w.r.t. the size of the sliding window and always returns an approximate solution from one of the checkpoint for the SIM query at any time. Furthermore, we propose the Location-based SIC (LSIC) framework and its improved version LSIC+, both of which process LSIM queries by integrating the SIC framework with a Quadtree spatial index. LSIC can provide approximate solutions for both ad hoc and continuous LSIM queries in real time, while LSIC+ further improves the solution quality of LSIC. Experimental results on real-world datasets demonstrate the effectiveness and efficiency of the proposed frameworks against the state-of-the-art IM algorithms. © 2018 ACM.",Data stream; Influence maximization; Region query; Social network; Spatial index; Submodular optimization,Communication channels (information theory); Social networking (online); Data stream; Influence maximizations; Region query; Spatial indexes; Submodular optimizations; Location
The characteristics of voice search: Comparing spoken with typed-in mobile web search queries,2018,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046537372&doi=10.1145%2f3182163&partnerID=40&md5=f9e16c33a796375f5f115443e4f9dce6,"The growing popularity of mobile search and the advancement in voice recognition technologies have opened the door for web search users to speak their queries rather than type them. While this kind of voice search is still in its infancy, it is gradually becoming more widespread. In this article, we report a comprehensive voice search query log analysis of a commercial web search engine's mobile application. We compare voice and text search by various aspects, with special focus on the semantic and syntactic characteristics of the queries. Our analysis suggests that voice queries focus more on audio-visual content and question answering and less on social networking and adult domains. In addition, voice queries are more commonly submitted on the go. We also conduct an empirical evaluation showing that the language of voice queries is closer to natural language than the language of text queries. Our analysis points out further differences between voice and text search. We discuss the implications of these differences for the design of future voice-enabled web search tools. © 2018 ACM.",Conversational search; Mobile search; Query log analysis; Spoken search; Voice queries; Voice search,Mobile telecommunication systems; Natural language processing systems; Search engines; Semantics; Websites; Conversational search; Mobile search; Query log analysis; Spoken search; Voice searches; Information retrieval
CO2: Inferring personal interests from raw footprints by connecting the offline world with the online world,2018,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046547678&doi=10.1145%2f3182164&partnerID=40&md5=2ff6f196cfe8fbbfbab337e5a6444256,"User-generated trajectories (UGTs), such as travel records from bus companies, capture rich information of human mobility in the offline world. However, some interesting applications of these raw footprints have not been exploited well due to the lack of textual information to infer the subject's personal interests. Although there is rich semantic information contained in the spatial-and temporal-aware user-generated contents (STUGC) published in the online world, such as Twitter, less effort has been made to utilize this information to facilitate the interest discovery process. In this article, we design an effective probabilistic framework named CO2 to connect the offline world with the online world in order to discover users' interests directly from their raw footprints in UGT. CO2 first infers trip intentions by utilizing the semantic information in STUGC and then discovers user interests by aggregating the intentions. To evaluate the effectiveness of CO2, we use two large-scale real-world datasets as a case study and further conduct a questionnaire survey to show the superior performance of CO2. © 2018 ACM.",Personal interests; Raw footprints; Trip intention; User-generated contents; User-generated trajectories,Carbon dioxide; Surveys; Personal interests; Raw footprints; Trip intention; User-generated; User-generated content; Semantics
An integrated signature-based framework for efficient visual similarity detection and measurement in video shots,2018,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054025328&doi=10.1145%2f3190784&partnerID=40&md5=80c00cbf73d42b051582c16be3631594,"This article presents a framework for speedy video matching and retrieval through detection and measurement of visual similarity. The framework's efficiency stems from its power to encode a given shot content into a compact fixed-length signature that helps in robust real-time matching. Separate scene and motion signatures are developed and fused together to fully represent and match respective video shots. Scene information is captured through the Statistical Dominant Color Profile (SDCP), while motion information is captured through a graph-based signature called the Dominant Color Graph Profile (DCGP). The SDCP is a fixed-length compact signature that statistically encodes the colors' spatiotemporal patterns across video frames. The DCGP is a fixed-length signature that records and tracks the gray levels across subsampled video frames, where the graph structural properties are used to extract the signature values. Finally, the overall video signature is generated by fusing the individual scene and motion signatures. The signature-based aspect of the proposed framework is the key to its high matching speed (>2000 fps) compared to current techniques that rely on exhaustive processing. To maximize the benefit of the framework, compressed-domain videos are utilized as a case study following their wide availability. However, the framework avoids full video decompression and operates on tiny frames rather than full-size decompressed frames. Experiments on various standard and challenging dataset groups show the framework's robust performance in terms of both retrieval and computational performance. © 2018 ACM.",Compressed video; Graph; MPEG; Signature; Video retrieval; Video similarity,Encoding (symbols); Graphic methods; Image compression; Compressed video; Graph; MPEG; Signature; Video retrieval; Video similarity; Color
Unsupervised learning of parsimonious general-purpose embeddings for user and location modeling,2018,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046545141&doi=10.1145%2f3182165&partnerID=40&md5=df84249d397f829392328b4cabc2a539,"Many social network applications depend on robust representations of spatio-temporal data. In this work, we present an embedding model based on feed-forward neural networks which transforms social media checkins into dense feature vectors encoding geographic, temporal, and functional aspects for modeling places, neighborhoods, and users. We employ the embedding model in a variety of applications including location recommendation, urban functional zone study, and crime prediction. For location recommendation, we propose a Spatio-Temporal Embedding Similarity algorithm (STES) based on the embedding model. In a range of experiments on real life data collected from Foursquare, we demonstrate our model's effectiveness at characterizing places and people and its applicability in aforementioned problem domains. Finally, we select eightmajor cities around the globe and verify the robustness and generality of our model by porting pre-trained models from one city to another, thereby alleviating the need for costly local training. © 2018 ACM.",Check-in embedding; Crime prediction; Personalized location recommendation; Social networks; Urban functional zone study,Crime; Social networking (online); Check-in; Functional aspects; Functional zones; Location modeling; Network applications; Personalized location recommendation; Similarity algorithm; Spatio-temporal data; Location
"Location extraction from social media: Geoparsing, location disambiguation, and geotagging",2018,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054003757&doi=10.1145%2f3202662&partnerID=40&md5=c959df74e1bd1a7427898f4e53483652,"Location extraction, also called ""toponym extraction,"" is a field covering geoparsing, extracting spatial representations from location mentions in text, and geotagging, assigning spatial coordinates to content items. This article evaluates five ""best-of-class"" location extraction algorithms. We develop a geoparsing algorithm using an OpenStreetMap database, and a geotagging algorithm using a language model constructed from social media tags and multiple gazetteers. Third-party work evaluated includes a DBpedia-based entity recognition and disambiguation approach, a named entity recognition and Geonames gazetteer approach, and a Google Geocoder API approach. We perform two quantitative benchmark evaluations, one geoparsing tweets and one geotagging Flickr posts, to compare all approaches. We also perform a qualitative evaluation recalling top N location mentions from tweets during major news events. The OpenStreetMap approach was best (F1 0.90+) for geoparsing English, and the language model approach was best (F1 0.66) for Turkish. The language model was best (F1@1km 0.49) for the geotagging evaluation. The map database was best (R@20 0.60+) in the qualitative evaluation. We report on strengths, weaknesses, and a detailed failure analysis for the approaches and suggest concrete areas for further research. © 2018 ACM.",Benchmark; Disambiguation; Geocoding; Geoparsing; Geotagging; Information extraction; Location; Location extraction; Social media; Toponym; Toponym extraction,Benchmarking; Computational linguistics; Geographic information systems; Information retrieval; Maps; Social networking (online); Disambiguation; Geo coding; Geo-tagging; Geoparsing; Social media; Toponym; Location
Automatically learning topics and difficulty levels of problems in online judge systems,2018,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046532364&doi=10.1145%2f3158670&partnerID=40&md5=f2b5b08f3933830d9a398a30d0feb56d,"Online Judge (OJ) systems have been widely used inmany areas, including programming,mathematical problems solving, and job interviews. Unlike other online learning systems, such as Massive Open Online Course, most OJ systems are designed for self-directed learning without the intervention of teachers. Also, in most OJ systems, problems are simply listed in volumes and there is no clear organization of them by topics or difficulty levels. As such, problems in the same volume are mixed in terms of topics or difficulty levels. By analyzing large-scale users' learning traces, we observe that there are two major learning modes (or patterns). Users either practice problems in a sequential manner from the same volume regardless of their topics or they attempt problems about the same topic, which may spread across multiple volumes. Our observation is consistent with the findings in classic educational psychology. Based on our observation, we propose a novel two-mode Markov topic model to automatically detect the topics of online problems by jointly characterizing the two learning modes. For further predicting the difficulty level of online problems, we propose a competition-based expertise model using the learned topic information. Extensive experiments on three large OJ datasets have demonstrated the effectiveness of our approach in three different tasks, including skill topic extraction, expertise competition prediction and problem recommendation. © 2017 ACM.",Expertise learning; Online judge systems; Topic models,Data mining; Online systems; Problem solving; Teaching; Educational psychology; Expertise learning; Massive open online course; Mathematical problems; On-line learning systems; Online judges; Self-directed learning; Topic model; E-learning
QuoteRec: Toward quote recommendation for writing,2018,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046553730&doi=10.1145%2f3183370&partnerID=40&md5=3f35e543d4d8b1e70fc917576d349162,"Quote is a language phenomenon of transcribing the statement of someone else, such as a proverb and a famous saying. An appropriate usage of quote usually equips the expression with more elegance and credibility. However, there are times when we are eager to stress our idea by citing a quote, while nothing relevant comes to mind. Therefore, it is exciting to have a recommender system which provides quote recommendations while we are writing. This article extends previous study of quote recommendation, the task that recommends the appropriate quote according to the context (i.e., the content occurring before and after the quote). In this article, a quote recommender system called QuoteRec is presented to tackle the task. We investigate two models to learn the vector representations of quotes and contexts, and then rank the candidate quotes based on the representations. The firstmodel learns the quote representation according to the contexts of a quote. The second model is an extension of the neural network model in previous study, which learns the representation of a quote by concerning both its content and contexts. Experimental results demonstrate the effectiveness of the two models in learning the semantic representations of quotes, and the neural network model achieves state-of-the-art results on the quote recommendation task. © 2018 ACM.",Deep learning; Document recommendation; LSTM; Quote recommendation,Deep learning; Recommender systems; Semantics; Document recommendation; LSTM; Neural network model; Quote recommendation; Semantic representation; State of the art; Vector representations; Long short-term memory
Explicit diversification of event aspects for temporal summarization,2018,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042482787&doi=10.1145%2f3158671&partnerID=40&md5=b3040fdefc46601acc6471f235d7f065,"During major events, such as emergencies and disasters, a large volume of information is reported on newswire and social media platforms. Temporal summarization (TS) approaches are used to automatically produce concise overviews of such events by extracting text snippets from related articles over time. Current TS approaches rely on a combination of event relevance and textual novelty for snippet selection. However, for events that span multiple days, textual novelty is often a poor criterion for selecting snippets, since many snippets are textually unique but are semantically redundant or non-informative. In this article, we propose a framework for the diversification of snippets using explicit event aspects, building on recent works in search result diversification. In particular, we first propose two techniques to identify explicit aspects that a user might want to see covered in a summary for different types of event. We then extend a state-of-the-art explicit diversification framework to maximize the coverage of these aspects when selecting summary snippets for unseen events. Through experimentation over the TREC TS 2013, 2014, and 2015 datasets, we show that explicit diversification for temporal summarization significantly outperforms classical novelty-based diversification, as the use of explicit event aspects reduces the amount of redundant and off-topic snippets returned, while also increasing summary timeliness. © 2018 ACM.",Explicit diversification; Temporal summarization; XQuAD,Information systems; Explicit diversification; Large volumes; Major events; Social media platforms; State of the art; Temporal summarization; Text snippets; XQuAD; Computer networks
Exploiting user and venue characteristics for fine-Grained tweet geolocation,2018,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042465276&doi=10.1145%2f3156667&partnerID=40&md5=da49e2380dcd5729554e35a9484b1e44,"Which venue is a tweet posted from? We call this a fine-grained geolocation problem. Given an observed tweet, the task is to infer its discrete posting venue, e.g., a specific restaurant. This recovers the venue context and differs from prior work, which geolocats tweets to location coordinates or cities/neighborhoods. First, we conduct empirical analysis to uncover venue and user characteristics for improving geolocation. For venues, we observe spatial homophily, in which venues near each other have more similar tweet content (i.e., text representations) compared to venues further apart. For users, we observe that they are spatially focused and more likely to visit venues near their previous visits. We also find that a substantial proportion of users post one or more geocoded tweet(s), thus providing their location history data. We then propose geolocation models that exploit spatial homophily and spatial focus characteristics plus posting time information. Our models rank candidate venues of test tweets such that the actual posting venue is ranked high. To better tune model parameters, we introduce a learning-to-rank framework. Our best model significantly outperforms state-of-the-art baselines. Furthermore, we show that tweets without any location-indicative words can be geolocated meaningfully as well. © 2018 ACM.",Learning to rank; Spatial focus; Spatial homophily; Tweet geolocation,Computer networks; Information systems; Empirical analysis; Focus characteristic; Geolocations; Homophily; Learning to rank; State of the art; Text representation; User characteristics; Location
Selective cluster presentation on the search results page,2018,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042867483&doi=10.1145%2f3158672&partnerID=40&md5=4158f4f203e863b80ab39193d767a85e,"Web search engines present, for some queries, a cluster of results from the same specialized domain (“vertical”) on the search results page (SERP). We introduce a comprehensive analysis of the presentation of such clusters from seven different verticals based on the logs of a commercial Web search engine. This analysis reveals several unique characteristics—such as size, rank, and clicks—of result clusters from community question- and-answer websites. The study of properties of this result cluster—specifically as part of the SERP—has received little attention in previous work. Our analysis also motivates the pursuit of a long-standing challenge in ad hoc retrieval, namely, selective cluster retrieval. In our setting, the specific challenge is to select for presentation the documents most highly ranked either by a cluster-based approach (those in the top-retrieved cluster) or by a document-based approach. We address this classification task by representing queries with features based on those utilized for ranking the clusters, query-performance predictors, and properties of the document-clustering structure. Empirical evaluation performed with TREC data shows that our approach outperforms a recently proposed state-of-the-art cluster-based document-retrieval method as well as state-of-the-art document-retrieval methods that do not account for inter-document similarities. © 2018 ACM.",Aggregated search; Cluster-based retrieval,Information retrieval systems; Search engines; Websites; Aggregated search; Classification tasks; Cluster based approach; Cluster-based retrieval; Comprehensive analysis; Document Clustering; Document similarity; Empirical evaluations; Information retrieval
Towards efficient framework for time-aware spatial keyword qeries on road networks,2017,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040312840&doi=10.1145%2f3143802&partnerID=40&md5=82822e2158536b35b62ea771e40bc164,"The spatial keyword query takes as inputs a query location and a set of query keywords and returns the answer objects by considering both their spatial distances to the query location and textual similarity with the query keywords. However, temporal information plays an important role in the spatial keyword query (where there is, to our knowledge, no prior work considering temporal information of the objects), since objects are not always valid. For instance, visitors may plan their trips according to the opening hours of attractions. Moreover, in real-life applications, objects are located on a predefined road network, and the spatial proximity of two objects is measured by the shortest path distance or travelling time between them. In this article, we study the problem of time-aware spatial keyword (TSK) query, which assumes that objects are located on the road network, and finds the k objects satisfying users' spatio-temporal description and textual constraint. We first present the pruning strategy and algorithm based on an existing index. Then, we design an efficient index structure called TG index and propose several algorithms using the TG index that can prune the search space with both spatio-temporal and textual information simultaneously. Further, we show that the TG index technique can also be applied to improve the performance of time-travel text search and spatial keyword query. Extensive experiments using both real and synthetic datasets demonstrate the effectiveness and efficiency of the presented index and algorithms. © 2017 ACM.",Indexing technique; Query processing; Road network; Spatial keyword query; Temporal information retrieval,Constraint satisfaction problems; Indexing (materials working); Motor transportation; Roads and streets; Transportation; Effectiveness and efficiencies; Indexing techniques; Keyword queries; Real-life applications; Road network; Spatio-temporal description; Temporal information retrievals; Textual similarities; Query processing
"Suggesting points-of-interest via content-based, collaborative, and hybrid fusion methods in mobile devices",2017,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030669273&doi=10.1145%2f3125620&partnerID=40&md5=5ff1e48305af23c5fbc7c40be4f47d7b,"Recommending venues or points-of-interest (POIs) is a hot topic in recent years, especially for tourism applications and mobile users. We propose and evaluate several suggestion methods, taking an effectiveness, feasibility, efficiency, and privacy perspective. The task is addressed by two content-based methods (aWeighted kNN classifier and a Rated Rocchio personalized query), Collaborative Filtering methods, as well as several (rank-based or rating-based) methods of merging results of different systems. Effectiveness is evaluated on two standard benchmark datasets, provided and used by TREC's Contextual Suggestion Tracks in 2015 and 2016. First, we enrich these datasets with more information on venues, collected from web services like Foursquare and Yelp; we make this extra data available for future experimentation. Then, we find that the content-based methods provide state-of-the-art effectiveness, the collaborative filtering variants mostly suffer from data sparsity problems in the current datasets, and the merging methods further improve results by mainly promoting the first relevant suggestion. Concerning mobile feasibility, efficiency, and user privacy, the content-based methods, especially Rated Rocchio, are the best. Collaborative filtering has the worst efficiency and privacy leaks. Our findings can be very useful for developing effective and efficient operational systems, respecting user privacy. Last, our experiments indicate that better benchmark datasets would be welcome, and the use of additional evaluation measures-more sensitive in recall-is recommended. Copyright is held by the owner/author(s).",Contextual suggestion; Privacy; Recommender systems,Data privacy; Efficiency; Merging; Mobile telecommunication systems; Rating; Recommender systems; Web services; Benchmark datasets; Collaborative filtering methods; Content-based methods; Contextual suggestion; Data sparsity problems; Evaluation measures; Operational systems; Tourism application; Collaborative filtering
Modeling and mining domain shared knowledge for sentiment analysis,2017,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028891291&doi=10.1145%2f3091995&partnerID=40&md5=8064e03dafe53f553339e4fc07c97571,"Sentiment classification aims to automatically predict sentiment polarity (e.g., positive or negative) of user generated sentiment data (e.g., reviews, blogs). In real applications, these user-generated sentiment data can span so many different domains that it is difficult to label the training data for all of them. Therefore, we study the problem of sentiment classification adaptation task in this article. That is, a system is trained to label reviews from one source domain but is meant to be used on the target domain. One of the biggest challenges for sentiment classification adaptation task is how to deal with the problem when two data distributions between the source domain and target domain are significantly different from one another. However, our observation is that there might exist some domain shared knowledge among certain input dimensions of different domains. In this article, we present a novel method for modeling and mining the domain shared knowledge from different sentiment review domains via a joint non-negative matrix factorization-based framework. In this proposed framework, we attempt to learn the domain shared knowledge and the domainspecific information from different sentiment review domains with several various regularization constraints. The advantage of the proposed method can promote the correspondence under the topic space between the source domain and the target domain, which can significantly reduce the data distribution gap across two domains. We conduct extensive experiments on two real-world balanced data sets from Amazon product reviews for sentence-level and document-level binary sentiment classification. Experimental results show that our proposed approach significantly outperforms several strong baselines and achieves an accuracy that is competitive with the most well-known methods for sentiment classification adaptation. © 2017 ACM.",Information retrieval; Natural language processing; Sentiment analysis,Data mining; Factorization; Information retrieval; Information retrieval systems; Knowledge management; Natural language processing systems; Data distribution; Different domains; Domain-specific information; Input dimensions; Nonnegative matrix factorization; Real applications; Sentiment analysis; Sentiment classification; Classification (of information)
SWIM: Stepped weighted shell decomposition influence maximization for large-scale networks,2017,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028655689&doi=10.1145%2f3072652&partnerID=40&md5=7dbd597a6d96b08d58eb35f257ce0f44,"A considerable amount of research has been devoted to the proposition of scalable algorithms for influence maximization. A number of such scalable algorithms exploit the community structure of the network. Besides the community structure, real-world social networks possess a different property, known as the layer structure. In this article, we propose a method based on the layer structure to maximize the influence in huge networks. Conducting experiments on a number of real-world networks, we will show that our method outperforms the state-of-the-art algorithms by its time complexity while having similar or slightly better final influence spread. Furthermore, unlike its predecessors, our method is able to show a high entanglement between structure and dynamics by giving insight on the reason why different networks have two contrasting behaviors in their saturation. By ""saturation,"" we mean a state during the seed selection process after which adjoining new nodes to the initial set will have a negligible effect on increasing the influence spread. We will demonstrate that how our method can predict the saturation dynamics in the networks. This prediction can be used to identify the network structures that are more vulnerable to the fast spread of the rumors. © 2017 ACM.",Independent cascade model; Influence maximization; Weighted k-shell,Scales (weighing instruments); Social sciences; CASCADE model; Community structures; Influence maximizations; K-shells; Large-scale network; Scalable algorithms; State-of-the-art algorithms; Structure and dynamics; Computer networks
What does affect the correlation among evaluation measures?,2017,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028652444&doi=10.1145%2f3106371&partnerID=40&md5=f33802b69c02fd014ab8f4febb78fd0e,"Information Retrieval (IR) is well-known for the great number of adopted evaluation measures, with new ones popping up more and more frequently. In this context, correlation analysis is the tool used to study the evaluation measures and to let us understand if two measures rank systems similarly, if they grasp different aspects of system performances or actually reflect different user models, if a new measure is well motivated or not. To this end, the two most commonly used correlation coefficients are the Kendall's ô correlation and the AP correlation ôAP . The goal of the article is to investigate the properties of the tool, that is, correlation analysis, we use to study evaluation measures. In particular, we investigate three research questions about these two correlation coefficients: (i) what is the effect of the number of systems and topics? (ii) what is the effect of removing low-performing systems? (iii) what is the effect of the experimental collections? To answer these research questions, we propose a methodology based on General Linear Mixed Model (GLMM) and ANalysis Of VAriance (ANOVA) to isolate the effects of the number of topics, number of systems, and experimental collections and to let us observe expected correlation values, net from these effects, which are stable and reliable. We learned that the effect of the number of topics is more prominent than the effect of the number of systems. Even if it produces different absolute values, the effect of removing low-performing systems does not seem to provide information substantially different from not removing them, especially when comparing a whole set of evaluation measures. Finally, we found out that both document corpora and topic sets affect the correlation among evaluation measures, the effect of the latter being more prominent. Moreover, there is a substantial interaction between evaluation measures, corpora and topic sets, meaning that the correlation between different evaluation measures can be substantially increased or decreased depending on the different corpora and topics at hand. © 2017 ACM.",Analysis of variance (ANOVA); AP correlation; Correlation analysis; Evaluation measures; General linear mixed models (GLMM); Grid of points (GoP); Kendall's tau correlation,Bit error rate; Correlation methods; Correlation analysis; Evaluation measures; Grid of points (GoP); Kendall's tau; Linear mixed models; Analysis of variance (ANOVA)
DBpedia-based entity linking via greedy search and adjusted Monte Carlo random walk,2017,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028893095&doi=10.1145%2f3086703&partnerID=40&md5=78bd9fa466aff3311ff64bce50e81ebe,"Facing a large amount of entities appearing on the web, entity linking has recently become useful. It assigns an entity from a resource to one name mention to help users grasp the meaning of this name mention. Unfortunately, many possible entities can be assigned to one name mention. Apparently, the usually cooccurring name mentions are related and can be considered together to determine their best assignments. This approach is called collective entity linking and is often conducted based on entity graph. However, traditional collective entity linking methods either consume much time due to the large scale of entity graph or obtain low accuracy due to simplifying graph. To improve both accuracy and efficiency, this article proposes a novel collective entity linking algorithm. It first constructs an entity graph by connecting any two related entities, and then a probability-based objective function is proposed on this graph to ensure the high accuracy of the linking result. Via this function, we convert entity linking to the process of finding the nodes with the highest PageRank Values. Greedy search and an adjusted Monte Carlo random walk are proposed to fulfill this work. Experimental results demonstrate that our algorithm performs much better than traditional linking methods. © 2017 ACM.",Adjusted monte carlo random walk; Collective entity linking; Greedy search; Multi-core; Pagerank,Random processes; Collective entity linking; Greedy search; Multi core; PageRank; Random Walk; Monte Carlo methods
AWARE: Exploiting evaluation measures to combine multiple assessors,2017,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028669422&doi=10.1145%2f3110217&partnerID=40&md5=3b37b39b4f56b0ff0e35addc8f17503d,"We propose the Assessor-drivenWeighted Averages for Retrieval Evaluation (AWARE) probabilistic framework, a novel methodology for dealing with multiple crowd assessors that may be contradictory and/or noisy. By modeling relevance judgements and crowd assessors as sources of uncertainty, AWARE takes the expectation of a generic performance measure, like Average Precision, composed with these random variables. In this way, it approaches the problem of aggregating different crowd assessors from a new perspective, that is, directly combining the performance measures computed on the ground truth generated by the crowd assessors instead of adopting some classification technique to merge the labels produced by them. We propose several unsupervised estimators that instantiate the AWARE framework and we compare them with state-of-theart approaches, that is,Majoriity Vote and Expectation Maximization, on TREC collections. We found that AWARE approaches improve in terms of their capability of correctly ranking systems and predicting their actual performance scores. © 2017 ACM.",AWARE; Crowdsourcing; Performance measure; Unsupervised estimators; Weighted average,Crowdsourcing; Maximum principle; AWARE; Classification technique; Expectation - maximizations; Performance measure; Probabilistic framework; Sources of uncertainty; Unsupervised estimators; Weighted averages; Uncertainty analysis
Using replicates in information retrieval evaluation,2017,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028653994&doi=10.1145%2f3086701&partnerID=40&md5=dc6484fcf2569585f1ebaad4b03b4996,"This article explores a method for more accurately estimating the main effect of the system in a typical testcollection- based evaluation of information retrieval systems, thus increasing the sensitivity of system comparisons. Randomly partitioning the test document collection allows for multiple tests of a given system and topic (replicates). Bootstrap ANOVA can use these replicates to extract system-topic interactions-something not possible without replicates-yielding a more precise value for the system effect and a narrower confidence interval around that value. Experiments using multiple TREC collections demonstrate that removing the topic-system interactions substantially reduces the confidence intervals around the system effect as well as increases the number of significant pairwise differences found. Further, the method is robust against small changes in the number of partitions used, against variability in the documents that constitute the partitions, and the measure of effectiveness used to quantify system effectiveness. © 2017 ACM.",Information retrieval; Statistical analysis; Test collections; Topic variance,Data mining; Financial data processing; Information retrieval; Information retrieval systems; Statistical methods; Confidence interval; Document collection; Measure of effectiveness; System comparisons; System effectiveness; System interactions; Test Collection; Topic variance; Search engines
Local representative-based matrix factorization for cold-start recommendation,2017,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028676929&doi=10.1145%2f3108148&partnerID=40&md5=12a461388604da09fc7db4a588b68b97,"Cold-start recommendation is one of the most challenging problems in recommender systems. An important approach to cold-start recommendation is to conduct an interview for new users, called the interview-based approach. Among the interview-based methods, Representative-Based Matrix Factorization (RBMF) [24] provides an effective solution with appealing merits: it represents users over selected representative items, which makes the recommendations highly intuitive and interpretable. However, RBMF only utilizes a global set of representative items to model all users. Such a representation is somehow too strict and may not be flexible enough to capture varying users' interests. To address this problem, we propose a novel interview-based model to dynamically create meaningful user groups using decision trees and then select local representative items for different groups. A two-round interview is performed for a new user. In the first round, l1 global questions are issued for group division, while in the second round, l2 local-group-specific questions are given to derive local representation.We collect the feedback on the (l1 + l2) items to learn the user representations. By putting these steps together, we develop a joint optimization model, named local representative-based matrix factorization, for new user recommendations. Extensive experiments on three public datasets have demonstrated the effectiveness of the proposed model compared with several competitive baselines. © 2017 ACM.",Cold start recommendation; Matrix factorization,Decision trees; Factorization; Matrix algebra; Optimization; Cold-start Recommendations; Effective solution; Joint optimization; Local groups; Matrix factorizations; User groups; User recommendations; Users' interests; Surveys
Tweet can be fit: Integrating data from wearable sensors and multiple social networks for wellness profile learning,2017,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028577442&doi=10.1145%2f3086676&partnerID=40&md5=afd8239df889c7a30d3eda19dab6564d,"Wellness is a widely popular concept that is commonly applied to fitness and self-help products or services. Inference of personal wellness-related attributes, such as body mass index (BMI) category or disease tendency as well as understanding of global dependencies between wellness attributes and users' behavior, is of crucial importance to various applications in personal and public wellness domains. At the same time, the emergence of social media platforms and wearable sensors makes it feasible to perform wellness profiling for users from multiple perspectives. However, research efforts on wellness profiling and integration of social media and sensor data are relatively sparse. This study represents one of the first attempts in this direction. Specifically, we infer personal wellness attributes by utilizing our proposed multisource multitask wellness profile learning framework-WellMTL-which can handle data incompleteness and perform wellness attributes inference from sensor and social media data simultaneously. To gain insights into the data at a global level, we also examine correlations between first-order data representations and personal wellness attributes. Our experimental results show that the integration of sensor data and multiple social media sources can substantially boost the performance of individual wellness profiling. 2017 Copyright is held by the owner/author.",Multiple sources integration; Multitask learning; Personal lifestyle assistance; Wearable sensors; Wellness profile learning,Data integration; Health; Integration; Social networking (online); Wearable technology; Body mass index; Multiple source; Multitask learning; Personal lifestyle assistance; Profile learning; Research efforts; Social media datum; Social media platforms; Wearable sensors
"Search, mining, and their applications on mobile devices: Introduction to the special issue",2017,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028669563&doi=10.1145%2f3086665&partnerID=40&md5=56e09f21f2807b5627402e05c125f25b,"In recent years, mobile devices have become the most popular interface for users to retrieve and access information: recent reports show that users spend significantly more time and issue more search queries on mobile devices than on desktops in the United States.1 The accelerated growth of mobile usage brings unique opportunities to the information retrieval and data mining research communities. Mobile devices capture rich contextual and personal signals that can be leveraged to accurately predict users' intent for serving more relevant content and can even proactively provide novel zero-query recommendations. Apple Siri, Google Now, and Microsoft Cortana are recent examples of such emerging systems. Furthermore, mobile devices constantly generate a huge amount of sensor footprints (e.g., GPS, motion sensors) and user activity data (e.g., used apps) that are often missing from their desktop counterparts. These new sources of implicit and explicit user feedback are valuable for discovering actionable knowledge, and designing better systems that serve each individual the right content at the right time and location. In addition, by aggregating mobile interactions across individuals, one can infer interesting conclusions beyond search and recommendation. Generating real-time traffic estimates is one example of such applications. This special issue focuses on research problems of search, mining, and their applications in mobile devices. Topics of interest in this special issue include but are not limited to mobile data mining and management, mobile search, personalization and recommendation, mobile user interfaces and human-computer interaction, and new applications in the mobile environment. The aim of this special issue is to bring together top experts across multiple disciplines, including information retrieval, data mining, mobile computing, and cyberphysical systems, such that academic and industrial researchers can exchange ideas and share the latest developments on the state of the art and practice of mobile search and mobile data mining. © 2017 ACM.",Mobile user interfaces; New applications in mobile environment; Personalization; Recommendation,Embedded systems; Human computer interaction; Information retrieval; Mobile telecommunication systems; Search engines; User interfaces; Cyber physical systems (CPSs); Mobile environments; Mobile user interface; Multiple disciplines; Personalizations; Query recommendations; Recommendation; Research communities; Data mining
Active learning for classification with maximum model change,2017,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029035511&doi=10.1145%2f3086820&partnerID=40&md5=1ec8b9848cf09d5e1c72a697062c630a,"Most existing active learning studies focus on designing sample selection algorithms. However, several fundamental problems deserve investigation to provide deep insight into active learning. In this article, we conduct an in-depth investigation on active learning for classification from the perspective of model change. We derive a general active learning framework for classification called maximum model change (MMC), which aims at querying the influential examples. The model change is quantified as the difference between the model parameters before and after training with the expanded training set. Inspired by the stochastic gradient update rule, the gradient of the loss with respect to a given candidate example is adopted to approximate the model change. This framework is applied to two popular classifiers: support vector machines and logistic regression. We analyze the convergence property of MMC and theoretically justify it. We explore the connection between MMC and uncertainty-based sampling to provide a uniform view. In addition, we discuss its potential usability to other learning models and show its applicability in a wide range of applications. We validate the MMC strategy on two kinds of benchmark datasets, the UCI repository and ImageNet, and show that it outperforms many state-of-the-art methods. © 2017 ACM.",Active learning; Classification; Logistic regression; Maximum model change; Support vector machines,Artificial intelligence; Classification (of information); Regression analysis; Stochastic systems; Support vector machines; Active Learning; Benchmark datasets; Convergence properties; Logistic regressions; Model change; State-of-the-art methods; Stochastic gradient; Uncertainty based samplings; Stochastic models
Understanding and leveraging the impact of response latency on user behaviour inweb search,2017,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028660419&doi=10.1145%2f3106372&partnerID=40&md5=14cf83ffcdde9360d42671074beec07a,"The interplay between the response latency of web search systems and users' search experience has only recently started to attract research attention, despite the important implications of response latency on monetisation of such systems. In this work, we carry out two complementary studies to investigate the impact of response latency on users' searching behaviour in web search engines. We first conduct a controlled user study to investigate the sensitivity of users to increasing delays in response latency. This study shows that the users of a fast search system are more sensitive to delays than the users of a slow search system.Moreover, the study finds that users are more likely to notice the response latency delays beyond a certain latency threshold, their search experience potentially being affected.We then analyse a large number of search queries obtained from YahooWeb Search to investigate the impact of response latency on users' click behaviour. This analysis demonstrates the significant change in click behaviour as the response latency increases. We also find that certain user, context, and query attributes play a role in the way increasing response latency affects the click behaviour. To demonstrate a possible use case for our findings, we devise a machine-learning framework that leverages the latency impact, together with other features, to predict whether a user will issue any clicks on web search results. As a further extension of this use case, we investigate whether this machine-learning framework can be exploited to help search engines reduce their energy consumption during query processing. © 2017 ACM.",Click prediction; Energy consumption; Green information retrieval; Response latency; Search experience; User behaviour; User engagement; Web search engine,Artificial intelligence; Behavioral research; Energy utilization; Information retrieval; Learning systems; Safety devices; Websites; Latency increase; Response latency; Search experience; Search queries; User behaviour; User engagement; Web search results; Web search system; Search engines
Social influence spectrum at scale: Near-optimal solutions for multiple budgets at once,2017,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028536772&doi=10.1145%2f3086700&partnerID=40&md5=4584641362c096dce9ff3b4361312d3d,"Given a social network, the Influence Maximization (InfMax) problem seeks a seed set of k people that maximizes the expected influence for a viral marketing campaign. However, a solution for a particular seed size k is often not enough to make an informed choice regarding budget and cost-effectiveness. In this article, we propose the computation of Influence Spectrum (InfSpec), the maximum influence at each possible seed set size k within a given range [klower, kupper], thus providing optimal decision making for any availability of budget or influence requirements. As none of the existing methods for InfMax are efficient enough for the task in large networks, we propose LISA (sub-Linear Influence Spectrum Approximation), an efficient approximation algorithm for InfSpec (and also InfMax) with the best-known worst-case guarantees for billion-scale networks. LISA returns an (1 - 1/e - ϵ)-approximate influence spectrum with high probability (1 - δ), where ϵ, δ are precision parameters provided by users. Using statistical decision theory, LISA has an asymptotic optimal running time (in addition to optimal approximation guarantee). In practice, LISA surpasses the state-of-the-art InfMax methods, taking less than 15 minutes to process a network of 41.7 million nodes and 1.5 billions edges. © 2017 ACM.",Approximation algorithms; Influence maximization; Influence spectrum,Behavioral research; Budget control; Computation theory; Cost effectiveness; Decision making; Decision theory; Marketing; Efficient approximation algorithms; Influence maximizations; Influence spectrum; Near-optimal solutions; Optimal approximation; Optimal decision making; Spectrum approximation; Statistical decision theory; Approximation algorithms
Enhancing topic modeling for short texts with auxiliary word embeddings,2017,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028533702&doi=10.1145%2f3091108&partnerID=40&md5=fcfdf5f300e7515f5a6a8b48a918555e,"Many applications require semantic understanding of short texts, and inferring discriminative and coherent latent topics is a critical and fundamental task in these applications. Conventional topic models largely rely on word co-occurrences to derive topics from a collection of documents. However, due to the length of each document, short texts are much more sparse in terms of word co-occurrences. Recent studies show that the Dirichlet Multinomial Mixture (DMM) model is effective for topic inference over short texts by assuming that each piece of short text is generated by a single topic. However, DMM has two main limitations. First, even though it seems reasonable to assume that each short text has only one topic because of its shortness, the definition of ""shortness"" is subjective and the length of the short texts is dataset dependent. That is, the single-topic assumption may be too strong for some datasets. To address this limitation, we propose to model the topic number as a Poisson distribution, allowing each short text to be associated with a small number of topics (e.g., one to three topics). This model is named PDMM. Second, DMM (and also PDMM) does not have access to background knowledge (e.g., semantic relations between words) when modeling short texts.When a human being interprets a piece of short text, the understanding is not solely based on its content words, but also their semantic relations. Recent advances in word embeddings offer effective learning of word semantic relations from a large corpus. Such auxiliary word embeddings enable us to address the second limitation. To this end, we propose to promote the semantically related words under the same topic during the sampling process, by using the generalized Polya urn (GPU) model. Through the GPU model, background knowledge about word semantic relations learned from millions of external documents can be easily exploited to improve topic modeling for short texts. By directly extending the PDMM model with the GPU model, we propose two more effective topic models for short texts, named GPU-DMM and GPU-PDMM. Through extensive experiments on two real-world short text collections in two languages, we demonstrate that PDMM achieves better topic representations than state-of-the-art models, measured by topic coherence. The learned topic representation leads to better accuracy in a text classification task, as an indirect evaluation. Both GPU-DMM and GPU-PDMM further improve topic coherence and text classification accuracy. GPUPDMM outperforms GPU-DMM at the price of higher computational costs. © 2017 ACM.",,Classification (of information); Poisson distribution; Semantics; Back-ground knowledge; Collection of documents; Computational costs; Semantic relations; Semantic understanding; Semantically-related words; Text classification; Word co-occurrence; Text processing
Fast and flexible top-k similarity search on large networks,2017,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028562955&doi=10.1145%2f3086695&partnerID=40&md5=5928daedc78577ca7ed95eed6f11b5e0,"Similarity search is a fundamental problem in network analysis and can be applied in many applications, such as collaborator recommendation in coauthor networks, friend recommendation in social networks, and relation prediction in medical information networks. In this article, we propose a sampling-based method using random paths to estimate the similarities based on both common neighbors and structural contexts efficiently in very large homogeneous or heterogeneous information networks. We give a theoretical guarantee that the sampling size depends on the error-bound ϵ, the confidence level (1 - δ), and the path length T of each random walk. We perform an extensive empirical study on a Tencent microblogging network of 1,000,000,000 edges. We show that our algorithm can return top-k similar vertices for any vertex in a network 300× faster than the state-of-the-art methods.We develop a prototype system of recommending similar authors to demonstrate the effectiveness of our method. © 2017 ACM.",Heterogeneous information network; Random path; Similarity search; Social network; Vertex similarity,Information services; Friend recommendations; Heterogeneous information; In-network analysis; Random paths; Sampling-based method; Similarity search; Theoretical guarantees; Vertex similarities; Social networking (online)
A neural network approach to jointly modeling social networks and mobile trajectories,2017,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028542436&doi=10.1145%2f3041658&partnerID=40&md5=478b967e98182f1a8924c247d51f1a38,"Two characteristics of location-based services are mobile trajectories and the ability to facilitate social networking. The recording of trajectory data contributes valuable resources towards understanding users' geographical movement behaviors. Social networking is possible when users are able to quickly connect to anyone nearby. A social network with location based services is known as location-based social network (LBSN). As shown in Cho et al. [2013], locations that are frequently visited by socially related persons tend to be correlated, which indicates the close association between social connections and trajectory behaviors of users in LBSNs. To better analyze and mine LBSN data, we need to have a comprehensive view of each of these two aspects, i.e., the mobile trajectory data and the social network. Specifically, we present a novel neural network model that can jointly model both social networks and mobile trajectories. Our model consists of two components: the construction of social networks and the generation of mobile trajectories. First we adopt a network embedding method for the construction of social networks: a networking representation can be derived for a user. The key to our model lies in generating mobile trajectories. Second, we consider four factors that influence the generation process of mobile trajectories: user visit preference, influence of friends, short-term sequential contexts, and long-term sequential contexts. To characterize the last two contexts, we employ the RNN and GRU models to capture the sequential relatedness in mobile trajectories at the short or long term levels. Finally, the two components are tied by sharing the user network representations. Experimental results on two important applications demonstrate the effectiveness of our model. In particular, the improvement over baselines is more significant when either network structure or trajectory data is sparse. © 2017 ACM.",Friend recommendation; Link prediction; Next-location recommendation; Recurrent neural network,Location; Location based services; Neural networks; Recurrent neural networks; Telecommunication services; Trajectories; Friend recommendations; Generation process; Link prediction; Location-based social networks; Network structures; Next-location recommendation; Novel neural network; Social connection; Social networking (online)
Mining exploratory behavior to improve mobile app recommendations,2017,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028542544&doi=10.1145%2f3072588&partnerID=40&md5=7a61e297a9a9100224578e70d27f7669,"With the widespread usage of smart phones, more and more mobile apps are developed every day, playing an increasingly important role in changing our lifestyles and business models. In this trend, it becomes a hot research topic for developing effective mobile app recommender systems in both industry and academia. Compared with existing studies about mobile app recommendations, our research aims to improve the recommendation effectiveness based on analyzing a psychological trait of human beings, exploratory behavior, which refers to a type of variety-seeking behavior in unfamiliar domains. To this end, we propose a novel probabilistic model named Goal-oriented Exploratory Model (GEM), integrating exploratory behavior identification with personalized item recommendation. An algorithm combining collapsed Gibbs sampling and Expectation Maximization is developed for model learning and inference. Through extensive experiments conducted on a real dataset, the proposed model demonstrates superior recommendation performances and good interpretability compared with state-of-art recommendation methods. Moreover, empirical analyses on exploratory behavior find that individuals with a strong exploratory tendency exhibit behavioral patterns of variety seeking, risk taking, and higher involvement. Besides, mobile apps that are less popular or in the long tail possess greater potential of arousing exploratory behavior in individuals. © 2017 ACM.",Exploratory behavior; Mobile app recommendation; Personalized item recommendation; Probabilistic generative model; Topic model,Inference engines; Maximum principle; Risk management; Exploratory behavior; Generative model; Mobile app; Personalized item recommendation; Topic Modeling; Smartphones
Parallelization of massive textstream compression based on compressed sensing,2017,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028546416&doi=10.1145%2f3086702&partnerID=40&md5=5a63ce3c2ea3f3922f4aae5579f52950,"Compressing textstreams generated by social networks can both reduce storage consumption and improve efficiency such as fast searching. However, the compression process is a challenge due to the large scale of textstreams. In this article, we propose a textstream compression framework based on compressed sensing theory and design a series of matching parallel procedures. The new approach uses a linear projection technique in the textstream compression process, achieving fast compression speed and low compression ratio. Two processes are executed by designing elaborated parallel procedures for efficient compressing and decompressing of large-scale textstreams. The decompression process is implemented for approximate solutions of underdetermined linear systems. Experimental results show that the new method can efficiently achieve the compression and decompression tasks on a large amount of text generated by social networks. © 2017 ACM.",Compressed sensing; Parallelization; Text stream compression,Compressed sensing; Compression ratio (machinery); Linear systems; Approximate solution; Compression process; Linear projection techniques; Low compression ratios; Parallelizations; Text streams; Theory and designs; Underdetermined linear systems; Signal reconstruction
"Curious cat-mobile, context-aware conversational crowdsourcing knowledge acquisition",2017,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028552402&doi=10.1145%2f3086686&partnerID=40&md5=b891d2a874c5743634a52e0297c0ac52,"Scaled acquisition of high-quality structured knowledge has been a longstanding goal of Artificial Intelligence research. Recent advances in crowdsourcing, the sheer number of Internet and mobile users, and the commercial availability of supporting platforms offer new tools for knowledge acquisition. This article applies context-aware knowledge acquisition that simultaneously satisfies users' immediate information needs while extending its own knowledge using crowdsourcing. The focus is on knowledge acquisition on a mobile device, which makes the approach practical and scalable; in this context, we propose and implement a new KA approach that exploits an existing knowledge base to drive the KA process, communicate with the right people, and check for consistency of the user-provided answers. We tested the viability of the approach in experiments using our platform with real users around the world, and an existing large source of commonsense background knowledge. These experiments show that the approach is promising: the knowledge is estimated to be true and useful for users 95% of the time. Using context to proactively drive knowledge acquisition increased engagement and effectiveness (the number of new assertions/day/user increased for 175%). Using pre-existing and newly acquired knowledge also proved beneficial. © 2017 ACM.",Chatbots; Crowdsourcing; Dialogue systems; Knowledge systems; Location and context based knowledge acquisition; Reasoning; Sensor and location mining,Crowdsourcing; Knowledge acquisition; Knowledge based systems; Mobile devices; Speech processing; Chatbots; Context-based; Dialogue systems; Knowledge system; Reasoning; Mergers and acquisitions
Collaborative intent prediction with real-time contextual data,2017,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028524487&doi=10.1145%2f3041659&partnerID=40&md5=28156f780af5c77afb51e0e1627aa998,"Intelligent personal assistants on mobile devices such as Apple's Siri and Microsoft Cortana are increasingly important. Instead of passively reacting to queries, they provide users with brand new proactive experiences that aim to offer the right information at the right time. It is, therefore, crucial for personal assistants to understand users' intent, that is, what information users need now. Intent is closely related to context. Various contextual signals, including spatio-temporal information and users' activities, can signify users' intent. It is, however, challenging to model the correlation between intent and context. Intent and context are highly dynamic and often sequentially correlated. Contextual signals are usually sparse, heterogeneous, and not simultaneously available. We propose an innovative collaborative nowcasting model to jointly address all these issues. The model effectively addresses the complex sequential and concurring correlation between context and intent and recognizes users' real-time intent with continuously arrived contextual signals. We extensively evaluate the proposed model with real-world data sets from a commercial personal assistant. The results validate the effectiveness the proposed model, and demonstrate its capability of handling the real-time flow of contextual signals. The studied problem and model also provide inspiring implications for new paradigms of recommendation on mobile intelligent devices. © 2017 ACM.",Nowcasting; Personal assistants; Proactive experiences; Streaming Context,Information systems; Information users; Intelligent devices; Nowcasting; Personal assistants; Proactive experiences; Real-world; Realtime flows; Spatiotemporal information; Computer networks
User modeling on demographic attributes in big mobile social networks,2017,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024487232&doi=10.1145%2f3057278&partnerID=40&md5=58c4c4b1634e23e662aecb7c428f5c37,"Users with demographic profiles in social networks offer the potential to understand the social principles that underpin our highly connected world, from individuals, to groups, to societies. In this article, we harness the power of network and data sciences to model the interplay between user demographics and social behavior and further study to what extent users' demographic profiles can be inferred from their mobile communication patterns. By modeling over 7 million users and 1 billion mobile communication records, we find that during the active dating period (i.e., 18-35 years old), users are active in broadening social connections with males and females alike, while after reaching 35 years of age people tend to keep small, closed, and same-gender social circles. Further, we formalize the demographic prediction problem of inferring users' gender and age simultaneously. We propose a factor graph-based WhoAmI method to address the problem by leveraging not only the correlations between network features and users' gender/age, but also the interrelations between gender and age. In addition, we identify a new problem-coupled network demographic prediction across multiple mobile operators- and present a coupled variant of the WhoAmI method to address its unique challenges. Our extensive experiments demonstrate the effectiveness, scalability, and applicability of the WhoAmI methods. Finally, our study finds a greater than 80% potential predictability for inferring users' gender from phone call behavior and 73% for users' age from text messaging interactions. © 2017 ACM.",Computational social science; Demographic prediction; Ego networks; Gender and age; Mobile communication; Mobile phone data; Node attributes; Social tie and triad,Forecasting; Message passing; Population statistics; Social networking (online); Social sciences; Telephone sets; Text messaging; Computational social science; Demographic predictions; Ego networks; Gender and age; Mobile communications; Mobile phone datum; Node attribute; Social ties; Mobile telecommunication systems
IDF for word N-grams,2017,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027025123&doi=10.1145%2f3052775&partnerID=40&md5=46c8d71b921413e6bfa4c464ded97fac,"Inverse Document Frequency (IDF) is widely accepted term weighting scheme whose robustness is supported by many theoretical justifications. However, applying IDF to word N-grams (or simply N-grams) of any length without relying on heuristics has remained a challenging issue. This article describes a theoretical extension of IDF to handle N-grams. First, we elucidate the theoretical relationship between IDF and information distance, a universal metric defined by the Kolmogorov complexity. Based on our understanding of this relationship, we propose N-gram IDF, a new IDF family that gives fair weights to words and phrases of any length. Based only on the magnitude relation of N-gram IDF weights, dominant N-grams among overlapping N-grams can be determined. We also propose an efficient method to compute the N-gram IDF weights of all N-grams by leveraging the enhanced suffix array and wavelet tree. Because the exact computation of N-gram IDF provably requires significant computational cost, we modify it to a fast approximation method that can estimate weight errors analytically and maintain application-level performance. Empirical evaluations with unsupervised/supervised key term extraction and web search query segmentation with various experimental settings demonstrate the robustness and language-independent nature of the proposed N-gram IDF. © 2017 ACM.",Information distance; Kolmogorov complexity; Multiword expression; Poisson distribution; Term weighting; Wavelet tree,Forestry; Inverse problems; Poisson distribution; Text processing; Information distance; Kolmogorov complexity; Multi-word expressions; Term weighting; Wavelet tree; Computational linguistics
Deriving user preferences of mobile apps from their management activities,2017,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024479361&doi=10.1145%2f3015462&partnerID=40&md5=b70a0b3aaea1a338e2615c62e75a0218,"App marketplaces host millions of mobile apps that are downloaded billions of times. Investigating how people manage mobile apps in their everyday lives creates a unique opportunity to understand the behavior and preferences of mobile device users, infer the quality of apps, and improve user experience. Existing literature provides very limited knowledge about app management activities, due to the lack of app usage data at scale. This article takes the initiative to analyze a very large app management log collected through a leading Android app marketplace. The dataset covers 5 months of detailed downloading, updating, and uninstallation activities, which involve 17 million anonymized users and 1 million apps. We present a surprising finding that the metrics commonly used to rank apps in app stores do not truly reflect the users' real attitudes. We then identify behavioral patterns from the app management activities that more accurately indicate user preferences of an app even when no explicit rating is available. A systematic statistical analysis is designed to evaluate machine learning models that are trained to predict user preferences using these behavioral patterns, which features an inverse probability weighting method to correct the selection biases in the training process. © 2017 ACM.",App management activities; Behavior analysis; Mobile apps,Commerce; Inverse problems; Mobile devices; Behavior analysis; Behavioral patterns; Machine learning models; Management activities; Mobile apps; Mobile device users; Probability weighting; Training process; User interfaces
Re-finding behaviour in vertical domains,2017,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027061553&doi=10.1145%2f2975590&partnerID=40&md5=30178b2bccc993a23a0aff5b8799a4e6,"Re-finding is the process of searching for information that a user has previously encountered and is a common activity carried out with information retrieval systems. In this work, we investigate re-finding in the context of vertical search, differentiating and modeling user re-finding behavior within different media and topic domains, including images, news, reference material, and movies. We distinguish the re-finding behavior in vertical domains from re-finding in a general search context and engineer features that are effective in differentiating re-finding across the domains. The features are then used to build machine-learned models, achieving an accuracy of re-finding detection in verticals of 85.7% on average. Our results demonstrate that detecting re-finding in specific verticals is more difficult than examining re-finding for general search tasks. We then investigate the effectiveness of differentiating re-finding behavior in two restricted contexts: We consider the case where the history of a searcher's interactions with the search system is not available. In this scenario, our features and models achieve an average accuracy of 77.5% across the domains. We then examine the detection of re-finding during the early part of a search session. Both of these restrictions represent potential real-world search scenarios, where a system is attempting to learn about a user but may have limited information available. Finally, we investigate in which types of domains re-finding is most difficult. Here, it would appear that re-finding images is particularly challenging for users. This research has implications for search engine design, in terms of adapting search results by predicting the type of user tasks and potentially enabling the presentation of vertical-specific results when re-finding is identified. To the best of our knowledge, this is the first work to investigate the issue of vertical re-finding. © 2017 ACM.",Difficulty; Predictivemodels; Re-finding behavior; Search feature; Vertical,Behavioral research; Information retrieval systems; Machine design; Difficulty; Limited information; Predictivemodels; Real world search; Reference material; Search feature; Searching for informations; Vertical; Search engines
Cross-platform app recommendation by jointly modeling ratings and texts,2017,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024495336&doi=10.1145%2f3017429&partnerID=40&md5=5768b0abd15ce9315eac0a54377f3efc,"Over the last decade, the renaissance of Web technologies has transformed the online world into an application (App) driven society. While the abundant Apps have provided great convenience, their sheer number also leads to severe information overload, making it difficult for users to identify desired Apps. To alleviate the information overloading issue, recommender systems have been proposed and deployed for the App domain. However, existing work on App recommendation has largely focused on one single platform (e.g., smartphones), while it ignores the rich data of other relevant platforms (e.g., tablets and computers). In this article, we tackle the problem of cross-platform App recommendation, aiming at leveraging users' and Apps' data on multiple platforms to enhance the recommendation accuracy. The key advantage of our proposal is that by leveraging multiplatform data, the perpetual issues in personalized recommender systems-data sparsity and cold-start-can be largely alleviated. To this end, we propose a hybrid solution, STAR (short for ""croSs-plaTform App Recommendation"") that integrates both numerical ratings and textual content from multiple platforms. In STAR, we innovatively represent an App as an aggregation of common features across platforms (e.g., App's functionalities) and specific features that are dependent on the resided platform. In light of this, STAR can discriminate a user's preference on an App by separating the user's interest into two parts (either in the App's inherent factors or platform-aware features). To evaluate our proposal, we construct two real-world datasets that are crawled from the App stores of iPhone, iPad, and iMac. Through extensive experiments, we show that our STAR method consistently outperforms highly competitive recommendation methods, justifying the rationality of our cross-platform App recommendation proposal and the effectiveness of our solution. © 2017 ACM.",App recommendation; Cold-start; Cross-platform; Hybrid system,Hybrid systems; Smartphones; Stars; Cold start; Cross-platform; Information overloading; Information overloads; Personalized recommender systems; Real-world datasets; Recommendation accuracy; Recommendation methods; Recommender systems
Computing urban traffic congestions by incorporating sparse GPS probe data and social media data,2017,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024477082&doi=10.1145%2f3057281&partnerID=40&md5=1875886e818adb5d4602914235952a3c,"Estimating urban traffic conditions of an arterial network with GPS probe data is a practically important while substantially challenging problem, and has attracted increasing research interests recently. Although GPS probe data is becoming a ubiquitous data source for various traffic related applications currently, they are usually insufficient for fully estimating traffic conditions of a large arterial network due to the low sampling frequency. To explore other data sources for more effectively computing urban traffic conditions, we propose to collect various traffic events such as traffic accident and jam from social media as complementary information. In addition, to further explore other factors that might affect traffic conditions, we also extract rich auxiliary information including social events, road features, Point of Interest (POI), and weather. With the enriched traffic data and auxiliary information collected from different sources, we first study the traffic co-congestion pattern mining problem with the aim of discovering which road segments geographically close to each other are likely to co-occur traffic congestion. A search tree based approach is proposed to efficiently discover the co-congestion patterns. These patterns are then used to help estimate traffic congestions and detect anomalies in a transportation network. To fuse the multisourced data, we finally propose a coupled matrix and tensor factorization model named TCE-R to more accurately complete the sparse traffic congestion matrix by collaboratively factorizing it with other matrices and tensors formed by other data. We evaluate the proposed model on the arterial network of downtown Chicago with 1,257 road segments whose total length is nearly 700 miles. The results demonstrate the superior performance of TCE-R by comprehensive comparison with existing approaches. © 2017 ACM.",Data fusion; Matrix factorization; Social media; Traffic congestion,Data fusion; Factorization; Matrix algebra; Motor transportation; Probes; Roads and streets; Social networking (online); Tensors; Transportation; Auxiliary information; Comprehensive comparisons; Matrix factorizations; Sampling frequencies; Social media; Tensor factorization; Transportation network; Urban traffic congestion; Traffic congestion
Encoding syntactic knowledge in neural networks for sentiment classification,2017,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027058618&doi=10.1145%2f3052770&partnerID=40&md5=2bbf623268baa6e203420f23a2723caf,"Phrase/Sentence representation is one of themost important problems in natural language processing. Many neural network models such as Convolutional Neural Network (CNN), Recursive Neural Network (RNN), and Long Short-Term Memory (LSTM) have been proposed to learn representations of phrase/sentence, however, rich syntactic knowledge has not been fully explored when composing a longer text from its shorter constituent words. Inmost traditional models, only word embeddings are utilized to compose phrase/sentence representations, while the syntactic information of words is yet to be explored. In this article, we discover that encoding syntactic knowledge (part-of-speech tag) in neural networks can enhance sentence/phrase representation. Specifically, we propose to learn tag-specific composition functions and tag embeddings in recursive neural networks, and propose to utilize POS tags to control the gates of tree-structured LSTM networks. We evaluate these models on two benchmark datasets for sentiment classification, and demonstrate that improvements can be obtained with such syntactic knowledge encoded. © 2017 ACM.",Deep learning; Long short-term memory; Neural networks; Recursive neural network; Representation learning; Sentiment analysis; Sentiment classification,Brain; Classification (of information); Deep learning; Encoding (symbols); Natural language processing systems; Network coding; Neural networks; Signal encoding; Syntactics; Composition functions; Convolutional neural network; Neural network model; Recursive neural networks; Representation learning; Sentiment analysis; Sentiment classification; Syntactic information; Long short-term memory
Incorporating user expectations and behavior into the measurement of search effectiveness,2017,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027069589&doi=10.1145%2f3052768&partnerID=40&md5=398ab227eedc2d1bfbd786be9d7864b3,"Information retrieval systems aim to help users satisfy information needs. We argue that the goal of the person using the system, and the pattern of behavior that they exhibit as they proceed to attain that goal, should be incorporated into the methods and techniques used to evaluate the effectiveness of IR systems, so that the resulting effectiveness scores have a useful interpretation that corresponds to the users' search experience. In particular, we investigate the role of search task complexity, and show that it has a direct bearing on the number of relevant answer documents sought by users in response to an information need, suggesting that useful effectiveness metrics must be goal sensitive. We further suggest that user behavior while scanning results listings is affected by the rate at which their goal is being realized, and hence that appropriate effectiveness metrics must be adaptive to the presence (or not) of relevant documents in the ranking. In response to these two observations, we present a new effectiveness metric, INST, that has both of the desired properties: INST employs a parameter T, a direct measure of the user's search goal that adjusts the top-weightedness of the evaluation score;moreover, as progress towards the target T is made, the modeled user behavior is adapted, to reflect the remaining expectations. INST is experimentally compared to previous effectiveness metrics, including Average Precision (AP), Normalized Discounted Cumulative Gain (NDCG), and Rank-Biased Precision (RBP), demonstrating our claims as to INST's usefulness. Like RBP, INST is a weighted-precision metric, meaning that each score can be accompanied by a residual that quantifies the extent of the score uncertainty caused by unjudged documents. As part of our experimentation, we use crowdsourced data and score residuals to demonstrate that a wide range of queries arise for even quite specific information needs, and that these variant queries introduce significant levels of residual uncertainty into typical experimental evaluations. These causes of variability have wide-reaching implications for experiment design, and for the construction of test collections. © 2017 ACM.",Effectivenessmetric; Query; Relevancemeasures; Search; Test collections; User behavior,Behavioral research; Information retrieval systems; Effectivenessmetric; Query; Relevancemeasures; Search; Test Collection; User behaviors; Search engines
Understanding the purpose of permission use in mobile apps,2017,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024862702&doi=10.1145%2f3086677&partnerID=40&md5=604036110bc01ade5f29282501114e6f,"Mobile apps frequently request access to sensitive data, such as location and contacts. Understanding the purpose of why sensitive data is accessed could help improve privacy as well as enable new kinds of access control. In this article, we propose a text mining based method to infer the purpose of sensitive data access by Android apps. The key idea we propose is to extract multiple features from app code and then use those features to train a machine learning classifier for purpose inference. We present the design, implementation, and evaluation of two complementary approaches to infer the purpose of permission use, first using purely static analysis, and then using primarily dynamic analysis. We also discuss the pros and cons of both approaches and the trade-offs involved. © 2017 ACM 1046-8188/2017/07-ART43 $15.00.",Access control; Android; Mobile applications; Permission; Privacy; Purpose,Access control; Data mining; Data privacy; Learning systems; Static analysis; Android; Android apps; Mobile applications; Mobile apps; Multiple features; Permission; Purpose; Sensitive datas; Android (operating system)
DeepMob: Learning deep knowledge of human emergency behavior and mobility from big and heterogeneous data,2017,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021793018&doi=10.1145%2f3057280&partnerID=40&md5=1dc989ac8ade0fe58b307e8bc91e5be9,"The frequency and intensity of natural disasters has increased significantly in recent decades, and this trend is expected to continue. Hence, understanding and predicting human evacuation behavior and mobility will play a vital role in planning effective humanitarian relief, disaster management, and long-Term societal reconstruction. However, existing models are shallow models, and it is difficult to apply them for understanding the ""deep knowledge"" of human mobility. Therefore, in this study, we collect big and heterogeneous data (e.g., GPS records of 1.6million users over 3 years, data on earthquakes that have occurred in Japan over 4 years, news report data, and transportation network data), and we build an intelligent system, namely, DeepMob, for understanding and predicting human evacuation behavior and mobility following different types of natural disasters. The key component of DeepMob is based on a deep learning architecture that aims to understand the basic laws that govern human behavior and mobility following natural disasters, from big and heterogeneous data. Furthermore, based on the deep learning model, DeepMob can accurately predict or simulate a person's future evacuation behaviors or evacuation routes under different disaster conditions. Experimental results and validations demonstrate the efficiency and superior performance of our system, and suggest that human mobility following disasters may be predicted and simulated more easily than previously thought. © 2017 ACM.",Disaster informatics; Human mobility; Spatiotemporal data mining; Urban computing,Behavioral research; Deep learning; Disaster prevention; Education; Forecasting; Intelligent systems; Disaster conditions; Disaster management; Human mobility; Informatics; Learning architectures; Spatio-temporal data mining; Transportation network; Urban computing; Disasters
A time-Aware personalized point-of-interest recommendation via high-order tensor factorization,2017,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021795168&doi=10.1145%2f3057283&partnerID=40&md5=fa1870dab909d910ab200e0c8ff2cdfb,"Recently, location-based services (LBSs) have been increasingly popular for people to experience new possibilities, for example, personalized point-of-interest (POI) recommendations that leverage on the overlapping of user trajectories to recommend POI collaboratively. POI recommendation is yet challenging as it suffers from the problems known for the conventional recommendation tasks such as data sparsity and cold start, and to a much greater extent. In the literature, most of the related works apply collaborate filtering to POI recommendation while overlooking the personalized time-variant human behavioral tendency. In this article, we put forward a fourth-order tensor factorization-based ranking methodology to recommend users their interested locations by considering their time-varying behavioral trends while capturing their longterm preferences and short-Term preferences simultaneously. We also propose to categorize the locations to alleviate data sparsity and cold-start issues, and accordingly new POIs that users have not visited can thus be bubbled up during the category ranking process. The tensor factorization is carefully studied to prune the irrelevant factors to the ranking results to achieve efficient POI recommendations. The experimental results validate the efficacy of our proposed mechanism, which outperforms the state-of-The-Art approaches significantly. © 2017 ACM.",HITS algorithm; Tensor factorization; Time-Aware POI recommendation,Behavioral research; Factorization; Location; Tensors; Fourth-order tensors; High order tensors; HITS algorithms; Point of interest; Ranking methodologies; State-of-the-art approach; Tensor factorization; Time-Aware POI recommendation; Location based services
Improving the quality of recommendations for users and items in the tail of distribution,2017,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85022329028&doi=10.1145%2f3052769&partnerID=40&md5=2c1203484c5ef940675c05c0743984fd,"Short-head and long-tail distributed data are widely observed in the real world. The same is true of recommender systems (RSs), where a small number of popular items dominate the choices and feedback data while the rest only account for a small amount of feedback. As a result, most RS methods tend to learn user preferences from popular items since they account for most data. However, recent research in e-commerce and marketing has shown that future businesses will obtain greater profit from long-tail selling. Yet, although the number of long-tail items and users is much larger than that of short-head items and users, in reality, the amount of data associated with long-tail items and users is much less. As a result, user preferences tend to be popularity-biased. Furthermore, insufficient data makes long-tail items and users more vulnerable to shilling attack. To improve the quality of recommendations for items and users in the tail of distribution, we propose a coupled regularization approach that consists of two latent factor models: C-HMF, for enhancing credibility, and S-HMF, for emphasizing specialty on user choices. Specifically, the estimates learned from C-HMF and S-HMF recurrently serve as the empirical priors to regularize one another. Such coupled regularization leads to the comprehensive effects of final estimates, which produce more qualitative predictions for both tail users and tail items. To assess the effectiveness of our model, we conduct empirical evaluations on large real-world datasets with various metrics. The results prove that our approach significantly outperforms the compared methods. © 2017 ACM.",Long tail; Multi-objective learning; Recommender systems; Recurrent mutual regularization; Trust and reputation systems,Electronic commerce; Recommender systems; Empirical evaluations; Long tail; Multi-objective learning; Qualitative predictions; Quality of recommendations; Recurrent mutual regularization; Regularization approach; Trust and reputation systems; Education
GVoS: A general system for near-duplicate video-related applications on storm,2017,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85022204045&doi=10.1145%2f3041657&partnerID=40&md5=0a6e11f28584d55398f7d2a4d98dab1d,"The exponential increase of online videos greatly enriches the life of users but also brings huge numbers of near-duplicate videos (NDVs) that seriously challenge the video websites. The video websites entail NDV-related applications such as detection of copyright violation, video monitoring, video re-ranking, and video recommendation. Since these applications adopt different features and different processing procedures due to diverse scenarios, constructing separate and special-purpose systems for them incurs considerable costs on design, implementation, and maintenance. In this article, we propose a general NDV system on Storm (GVoS)-a popular distributed real-time stream processing platform-to simultaneously support a wide variety of video applications. The generality of GVoS is achieved in two aspects. First, we extract the reusable components from various applications. Second, we conduct the communication between components via a mechanism called Stream Shared Message (SSM) that contains the video-related data. Furthermore, we present an algorithm to reduce the size of SSM in order to avoid the data explosion and decrease the network latency. The experimental results demonstrate that GVoS can achieve performance almost the same as the customized systems. Meanwhile, GVoS accomplishes remarkably higher systematic versatility and efficiently facilitates the development of various NDV-related applications. © 2017 ACM.",General system; Near duplicate video; Phrases; Real-time processing; Retrieval and detection,Storms; Websites; Exponential increase; General systems; Near-duplicate videos; Phrases; Processing procedures; Realtime processing; Reusable components; Video applications; Video signal processing
"Comparing the archival rate of Arabic, English, Danish, and Korean language web pages",2017,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85022331441&doi=10.1145%2f3041656&partnerID=40&md5=2d04a6f4033f188b62316525abd5d7b6,"It has long been suspected that web archives and search engines favor Western and English language webpages. In this article, we quantitatively explore howwell indexed and archived Arabic languagewebpages are as compared to those from other languages. We began by sampling 15,092 unique URIs from three different website directories: DMOZ (multilingual), Raddadi, and Star28 (the last two primarily Arabic language). Using language identification tools, we eliminated pages not in the Arabic language (e.g., Englishlanguage versions of Aljazeera pages) and culled the collection to 7,976 Arabic language webpages. We then used these 7,976 pages and crawled the live web and web archives to produce a collection of 300,646 Arabic language pages. We compared the analysis of Arabic language pages with that of English, Danish, and Korean language pages. First, for each language, we sampled unique URIs from DMOZ; then, using language identification tools, we kept only pages in the desired language. Finally, we crawled the archived and live web to collect a larger sample of pages in English, Danish, or Korean. In total for the four languages, we analyzed over 500,000 webpages. We discovered: (1) English has a higher archiving rate than Arabic, with 72.04% archived. However, Arabic has a higher archiving rate than Danish and Korean, with 53.36% of Arabic URIs archived, followed by Danish and Korean with 35.89% and 32.81% archived, respectively. (2)Most Arabic and English language pages are located in the United States; only 14.84% of the Arabic URIs had an Arabic country code top-level domain (e.g., .sa) and only 10.53% had a GeoIP in an Arabic country. Most Danish-language pages were located in Denmark, and most Korean-language pages were located in South Korea. (3) The presence of a webpage in a directory positively impacts indexing and presence in the DMOZ directory, specifically, positively impacts archiving in all four languages. In this work, we show that web archives and search engines favor English pages. However, it is not universally true for all Westernlanguage webpages because, in this work, we show that Arabic webpages have a higher archival rate than Danish language webpages. © 2017 ACM.",Arabic web; Danish web; Digital preservation; English web; Indexing; Korean web; Phrases; Web archiving,Digital storage; Indexing (of information); Natural language processing systems; Search engines; Arabic web; Danish web; Digital preservation; English web; Korean web; Phrases; Web Archiving; Websites
Version-Aware rating prediction for mobile app recommendation,2017,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021806966&doi=10.1145%2f3015458&partnerID=40&md5=21ada787a0c39cc90fcc25733d84a096,"With the great popularity of mobile devices, the amount of mobile apps has grown at a more dramatic rate than ever expected. A technical challenge is how to recommend suitable apps to mobile users. In this work, we identify and focus on a unique characteristic that exists in mobile app recommendation-That is, an app usually corresponds to multiple release versions. Based on this characteristic, we propose a fine-grain version-Aware app recommendation problem. Instead of directly learning the users' preferences over the apps, we aim to infer the ratings of users on a specific version of an app. However, the user-version rating matrix will be sparser than the corresponding user-App rating matrix, making existing recommendation methods less effective. In view of this, our approach has made two major extensions. First, we leverage the review text that is associated with each rating record; more importantly, we consider two types of versionbased correlations. The first type is to capture the temporal correlations between multiple versions within the same app, and the second type of correlation is to capture the aggregation correlations between similar apps. Experimental results on a large dataset demonstrate the superiority of our approach over several competitive methods. © 2017 ACM.",App rating prediction; Recommender systems; Version correlation,Mobile telecommunication systems; Recommender systems; Fine grains; Large dataset; Mobile apps; Mobile users; Multiple release; Recommendation methods; Technical challenges; Temporal correlations; Rating
Search by screenshots for universal article clipping in mobile apps,2017,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021792034&doi=10.1145%2f3091107&partnerID=40&md5=388f1083bde58f827da9deaf2f248707,"To address the difficulty in clipping articles from various mobile applications (apps), we propose a novel framework called UniClip, which allows a user to snap a screen of an article to save the whole article in one place. The key task of the framework is search by screenshots, which has three challenges: (1) how to represent a screenshot; (2) how to formulate queries for effective article retrieval; and (3) how to identify the article from search results.We solve these by (1) segmenting a screenshot into structural units called blocks, (2) formulating effective search queries by considering the role of each block, and (3) aggregating the search result lists of multiple queries. To improve efficiency, we also extend our approach with learning-To-rank techniques so that we can find the desired article with only one query. Experimental results show that our approach achieves high retrieval performance (F1 = 0.868), which outperforms baselines based on keyword extraction and chunking methods. Learning-To-rank models improve our approach without learning by about 6%. A user study conducted to investigate the usability of UniClip reveals that ours is preferred by 21 out of 22 participants for its simplicity and effectiveness. © 2017 ACM.",Article clipping; Mobile apps; Search by screenshots; Universal clipping,Computer networks; Information systems; Article clipping; Keyword extraction; Learning to rank; Mobile applications; Mobile apps; Retrieval performance; Screenshots; Universal clipping; Education
Compact indexing and judicious searching for billion-scale microblog retrieval,2017,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027073660&doi=10.1145%2f3052771&partnerID=40&md5=1e81d00c6ff73853c1ec55eb379e7cd2,"In this article, we study the problem of efficient top-k disjunctive query processing in a huge microblog dataset. In terms of compact indexing, we categorize the keywords into rare terms and common terms based on inverse document frequency (idf) and propose tailored block-oriented organization to save memory consumption. In terms of fast searching, we classify the queries into three types based on term category and judiciously design an efficient search algorithm for each type. We conducted extensive experiments on a billion-scale Twitter dataset and examined the performance with both simple and more advanced ranking functions. The results showed that with much smaller index size, our search algorithm achieves a factor of 2-3 times faster speedup over state-of-the-art solutions in both ranking scenarios. © 2017 ACM.",Billion-scale; Disjunctive keyword search; Microblg; Top-k,Learning algorithms; Search engines; Billion-scale; Inverse Document Frequency; Keyword search; Memory consumption; Microblg; Ranking functions; Search Algorithms; State of the art; Indexing (of information)
Processing long queries against short text: Top-k advertisement matching in news stream applications,2017,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027015960&doi=10.1145%2f3052772&partnerID=40&md5=96ee41f7cade3b3c283384223dbba726,"Many real applications in real-time news stream advertising call for efficient processing of long queries against short text. In such applications, dynamic news feeds are regarded as queries to match against an advertisement (ad) database for retrieving the k most relevant ads. The existing approaches to keyword retrieval cannot work well in this search scenario when queries are triggered at a very high frequency. To address the problem, we introduce new techniques to significantly improve search performance. First, we devise a two-level partitioning for tight upper bound estimation and a lazy evaluation scheme to delay full evaluation of unpromising candidates, which can bring three to four times performance boosting in a database with 7 million ads. Second, we propose a novel rank-aware block-oriented inverted index to further improve performance. In this index scheme, each entry in an inverted list is assigned a rank according to its importance in the ad. Then, we introduce a block-at-a-time search strategy based on the index scheme to support amuch tighter upper bound estimation and a very early termination. We have conducted experiments with real datasets, and the results show that the rank-aware method can further improve performance by an order of magnitude. © 2017 ACM.",Inverted index; Long queries; Rank-aware partitioning; Short text; Top-k retrieval,Damage detection; Indexing (of information); Information retrieval; Information services; Inverted indices; Long queries; Rank-aware partitioning; Short texts; Top-k retrieval; Query languages
Search result prefetching on desktop and mobile,2017,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027053019&doi=10.1145%2f3015466&partnerID=40&md5=1d8eff51fec7f90d7514156f36a6f106,"Search result examination is an important part of searching. High page load latency for landing pages (clicked search results) can reduce the efficiency of the search process. Proactively prefetching landing pages in advance of clickthrough can save searchers valuable time. However, prefetching consumes resources (primarily bandwidth and battery) that are wasted unless the prefetched results are requested by searchers. Balancing the costs in prefetching particular results against the benefits in reduced latency to searchers represents the search result prefetching challenge. In this article, we introduce this challenge and present methods to address it in both desktop and mobile settings. Our methods leverage searchers' cursor movements (on desktop) and viewport-based viewing behavior (on mobile) on search engine result pages (SERPs) in real time to dynamically estimate the result that searchers will request next. We demonstrate through large-scale log analysis that our approach significantly outperforms three strong baselines that prefetch results based on (i) the search engine result ranking (prefetch top-ranked results), (ii) past SERP clicks from all searchers for the query (prefetch popular results), or (iii) past SERP clicks from the current searcher for the query (prefetch results that the searcher prefers). Our promising findings have implications for the design of search support in desktop and mobile settings that makes the search process more efficient. © 2017 ACM.",Attention modeling; Mobile; Mouse cursor; Search result prefetching; Viewport,Computer networks; Information systems; Attention model; Mobile; Mouse cursor; Prefetching; Viewport; Search engines
Inferring dynamic user interests in streams of short texts for user clustering,2017,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026458484&doi=10.1145%2f3072606&partnerID=40&md5=4b7afe8f1858690baf710e7fcab9bb5c,"User clustering has been studied from different angles. In order to identify shared interests, behaviorbased methods consider similar browsing or search patterns of users, whereas content-based methods use information from the contents of the documents visited by the users. So far, content-based user clustering has mostly focused on static sets of relatively long documents. Given the dynamic nature of social media, there is a need to dynamically cluster users in the context of streams of short texts. User clustering in this setting is more challenging than in the case of long documents, as it is difficult to capture the users' dynamic topic distributions in sparse data settings. To address this problem, we propose a dynamic user clustering topic model (UCT). UCT adaptively tracks changes of each user's time-varying topic distributions based both on the short texts the user posts during a given time period and on previously estimated distributions. To infer changes, we propose a Gibbs sampling algorithm where a set of word pairs from each user is constructed for sampling. UCT can be used in two ways: (1) as a short-term dependency model that infers a user's current topic distribution based on the user's topic distributions during the previous time period only, and (2) as a long-term dependency model that infers a user's current topic distributions based on the user's topic distributions during multiple time periods in the past. The clustering results are explainable and humanunderstandable, in contrast to many other clustering algorithms. For evaluation purposes, we work with a dataset consisting of users and tweets from each user. Experimental results demonstrate the effectiveness of our proposed short-term and long-term dependency user clustering models compared to state-of-the-art baselines. © 2017 ACM.",Ad hoc retrieval; Data streams; Diversity,Computer networks; Information systems; Ad Hoc retrieval; Behavior-based methods; Content-based methods; Data stream; Diversity; Long-term dependencies; Multiple-time periods; Topic distributions; Clustering algorithms
Search result diversification in short text streams,2017,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026456548&doi=10.1145%2f3057282&partnerID=40&md5=3b8992d510c9c83ce7097518f45d4982,"We consider the problem of search result diversification for streams of short texts. Diversifying search results in short text streams is more challenging than in the case of long documents, as it is difficult to capture the latent topics of short documents. To capture the changes of topics and the probabilities of documents for a given query at a specific time in a short text stream, we propose a dynamic Dirichlet multinomial mixture topic model, called D2M3, as well as a Gibbs sampling algorithm for the inference. We also propose a streaming diversification algorithm, SDA, that integrates the information captured by D2M3 with our proposed modified version of the PM-2 (Proportionality-based diversification Method - second version) diversification algorithm. We conduct experiments on a Twitter dataset and find that SDA statistically significantly outperforms state-of-the-art non-streaming retrieval methods, plain streaming retrieval methods, as well as streaming diversification methods that use other dynamic topic models. © 2017 ACM.",Ad hoc retrieval; Data streams; Diversity,Information retrieval; Ad Hoc retrieval; Data stream; Diversity; Dynamic topic models; Gibbs sampling; Retrieval methods; State of the art; Topic Modeling; Inference engines
Learning to align comments to news topics,2017,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026465032&doi=10.1145%2f3072591&partnerID=40&md5=302f4019eea1a71db68d053b307e2490,"With the rapid proliferation of social media, increasingly more people express their opinions and reviews (user-generated content (UGC)) on recent news articles through various online services, such as news portals, forums, discussion groups, and microblogs. Clearly, identifying hot topics that users greatly care about can improve readers' news browsing experience and facilitate research into interaction analysis between news and UGC. Furthermore, it is of great benefit to public opinion monitoring and management for both industry and government agencies. However, it is extremely time consuming, if not impossible, to manually examine the large amount of available social content. In this article, we formally define the news comment alignment problem and propose a novel framework that: (1) automatically extracts topics from a given news article and its associated comments, (2) identifies and extends positive examples with different degrees of confidence using three methods (i.e., hypersphere, density, and cluster chain), and (3) completes the alignment between news sentences and comments through a weighted-SVM classifier. Extensive experiments show that our proposed framework significantly outperforms state-of-the-art methods. © 2017 ACM.",Alignment; Cluster chain; Density; Dependent topic model; Pu learning; User-generated content,Alignment; Chains; Data mining; Density (specific gravity); Social aspects; Government agencies; Interaction analysis; Monitoring and management; Pu learning; State-of-the-art methods; Topic Modeling; User generated content (UGC); User-generated content; Education
Yum-Me: A personalized nutrient-based meal recommender system,2017,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026445680&doi=10.1145%2f3072614&partnerID=40&md5=6beffa4737a4674096bdde53dee8a072,"Nutrient-based meal recommendations have the potential to help individuals prevent or manage conditions such as diabetes and obesity. However, learning people's food preferences and making recommendations that simultaneously appeal to their palate and satisfy nutritional expectations are challenging. Existing approaches either only learn high-level preferences or require a prolonged learning period. We propose Yum-me, a personalized nutrient-based meal recommender system designed to meet individuals' nutritional expectations, dietary restrictions, and fine-grained food preferences. Yum-me enables a simple and accurate food preference profiling procedure via a visual quiz-based user interface and projects the learned profile into the domain of nutritionally appropriate food options to find ones that will appeal to the user.We present the design and implementation of Yum-me and further describe and evaluate two innovative contributions. The first contriution is an open source state-of-the-art food image analysis model, named FoodDist. We demonstrate FoodDist's superior performance through careful benchmarking and discuss its applicability across a wide array of dietary applications. The second contribution is a novel online learning framework that learns food preference from itemwise and pairwise image comparisons. We evaluate the framework in a field study of 227 anonymous users and demonstrate that it outperforms other baselines by a significant margin. We further conducted an end-to-end validation of the feasibility and effectiveness of Yum-me through a 60-person user study, in which Yum-me improves the recommendation acceptance rate by 42.63%. Copyright 2017 is held by the owner/author(s). Publication rights licensed to ACM.",Food preferences; Nutrient-based meal recommendation; Online learning; Personalization; Visual interface,Benchmarking; E-learning; Education; Nutrients; Nutrition; User interfaces; Acceptance rate; Design and implementations; Food preference; Image comparison; Online learning; Personalizations; State of the art; Visual Interface; Recommender systems
Unifying virtual and physical worlds: Learning toward local and global consistency,2017,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017606872&doi=10.1145%2f3052774&partnerID=40&md5=3a679746aac41c62f32182cccda2888d,"Event-based social networking services, such as Meetup, are capable of linking online virtual interactions to offline physical activities. Compared to mono online social networking services (e.g., Twitter and Google+), such dual networks provide a complete picture of users' online and offline behaviors that more often than not are compatible and complementary. In the light of this, we argue that joint learning over dual networks offers us a better way to comprehensively understand user behaviors and their underlying organizational principles. Despite its value, few efforts have been dedicated to jointly considering the following factors within a unified model: (1) local user contextualization, (2) global structure coherence, and (3) effectiveness evaluation. Toward this end, we propose a novel dual clustering model for community detection over dual networks to jointly model local consistency for a specific user and global consistency of partitioning results across networks. We theoretically derived its solution. In addition, we verified our model regarding multiple metrics from different aspects and applied it to the application of event attendance prediction. © 2017 ACM.",Event-based social networks; Global consistency; Local consistency,Behavioral research; E-learning; Large scale systems; Effectiveness evaluation; Event-based; Global consistency; Local consistency; Online social networkings; Organizational principles; Social networking services; Virtual interactions; Social networking (online)
Clustered Elias-Fano indexes,2017,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017605202&doi=10.1145%2f3052773&partnerID=40&md5=54ab937ea292cd8a32811e1b27b20f11,"State-of-the-art encoders for inverted indexes compress each posting list individually. Encoding clusters of posting lists offers the possibility of reducing the redundancy of the lists while maintaining a noticeable query processing speed. In this article, we propose a new index representation based on clustering the collection of posting lists and, for each created cluster, building an ad hoc reference list with respect to which all lists in the cluster are encoded with Elias-Fano. We describe a posting lists clustering algorithm tailored for our encoder and two methods for building the reference list for a cluster. Both approaches are heuristic and differ in the way postings are added to the reference list: according to their frequency in the cluster or according to the number of bits necessary for their representation. The extensive experimental analysis indicates that significant space reductions are indeed possible, beating the best state-of-the-art encoders. ©2017 ACM.",Elias-Fano encoding; Inverted indexes; Performance,Encoding (symbols); Best state; Experimental analysis; Inverted indices; Performance; Reference list; Space reductions; State of the art; Clustering algorithms
Inverted treaps,2017,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011347330&doi=10.1145%2f3007186&partnerID=40&md5=131d3d8bffc7362de6e6049449d197ed,"We introduce a new representation of the inverted index that performs faster ranked unions and intersections while using similar space. Our index is based on the treap data structure, which allows us to intersect/merge the document identifiers while simultaneously thresholding by frequency, instead of the costlier two-step classical processing methods. To achieve compression, we represent the treap topology using different alternative compact data structures. Further, the treap invariants allow us to elegantly encode differentially both document identifiers and frequencies. We also show how to extend this representation to support incremental updates over the index. Results show that, under the tf-idf scoring scheme, our index uses about the same space as state-of-the-art compact representations, while performing up to 2-20 times faster on ranked single-word, union, or intersection queries. Under the BM25 scoring scheme, our index may use up to 40% more space than the others and outperforms them less frequently but still reaches improvement factors of 2-20 in the best cases. The index supporting incremental updates poses an overhead of 50%-100% over the static variants in terms of space, construction, and query time. © 2017 ACM.",Compact data structure; Top-k document retrieval,Data structures; Compact data structure; Compact representation; Document identifiers; Document Retrieval; Improvement factors; Incremental updates; Inverted indices; Processing method; Query processing
Targeted advertising in public transportation systems with quantitative evaluation,2017,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011342717&doi=10.1145%2f3003725&partnerID=40&md5=f762ce7ce7cbe77b7a639ccfea960e52,"In spite of vast business potential, targeted advertising in public transportation systems is a grossly unexplored research area. For instance, SBS Transit in Singapore can reach 1 billion passengers per year but the annual advertising revenue contributes less than $35 million. To bridge the gap, we propose a probabilistic data model that captures the motion patterns and user interests so as to quantitatively evaluate the impact of an advertisement among the passengers. In particular, we leverage hundreds of millions of bus/train boarding transaction records to quantitatively estimate the probability as well as the extent of a user being influenced by an ad. Based on the influence model, we study a top-k retrieval problem for bus/train ad recommendation, which acts as a primitive operator to support various advanced applications. We solve the retrieval problem efficiently to support real-Time decision making. In the experimental study, we use the dataset from SBS Transit as a case study to verify the effectiveness and efficiency of our proposed methodologies. © 2017 ACM.",Mobility pattern; Probabilistic data model; Public transportation systems; Quantitative evaluation; Targeted advertising; User interest,Decision making; Marketing; Mobility pattern; Probabilistic data; Public transportation systems; Quantitative evaluation; Targeted advertising; User interests; Publishing
On crowdsourcing relevance magnitudes for information retrieval evaluation,2017,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011319492&doi=10.1145%2f3002172&partnerID=40&md5=cc3051f14bb50f20fb3b532be34e7352,"Magnitude estimation is a psychophysical scaling technique for the measurement of sensation, where observers assign numbers to stimuli in response to their perceived intensity. We investigate the use of magnitude estimation for judging the relevance of documents for information retrieval evaluation, carrying out a large-scale user study across 18 TREC topics and collecting over 50,000 magnitude estimation judgments using crowdsourcing. Our analysis shows that magnitude estimation judgments can be reliably collected using crowdsourcing, are competitive in terms of assessor cost, and are, on average, rank-aligned with ordinal judgments made by expert relevance assessors. We explore the application of magnitude estimation for IR evaluation, calibrating two gain-based effectiveness metrics, nDCG and ERR, directly from user-reported perceptions of relevance. A comparison of TREC system effectiveness rankings based on binary, ordinal, and magnitude estimation relevance shows substantial variation; in particular, the top systems ranked using magnitude estimation and ordinal judgments differ substantially. Analysis of the magnitude estimation scores shows that this effect is due in part to varying perceptions of relevance: different users have different perceptions of the impact of relative differences in document relevance. These results have direct implications for IR evaluation, suggesting that current assumptions about a single view of relevance being sufficient to represent a population of users are unlikely to hold. © 2017 ACM.",Evaluation; Magnitude estimation; Relevance; Relevance assessments,Information retrieval; Effectiveness metrics; Evaluation; Magnitude estimation; Psychophysical scaling; Relevance; Relevance assessments; Substantial variations; System effectiveness; Crowdsourcing
Hilbert exclusion: Improved metric search through finite isometric embeddings,2016,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85008222749&doi=10.1145%2f3001583&partnerID=40&md5=59a724680d75655788df20fa2b5d3543,"Most research into similarity search in metric spaces relies on the triangle inequality property. This property allows the space to be arranged according to relative distances to avoid searching some subspaces. We show that many common metric spaces, notably including those using Euclidean and Jensen-Shannon distances, also have a stronger property, sometimes called the four-point property: In essence, these spaces allow an isometric embedding of any four points in three-dimensional Euclidean space, as well as any three points in two-dimensional Euclidean space. In fact, we show that any space that is isometrically embeddable in Hilbert space has the stronger property. This property gives stronger geometric guarantees, and one in particular, which we name the Hilbert Exclusion property, allows any indexing mechanism which uses hyperplane partitioning to perform better. One outcome of this observation is that a number of state-of-the-art indexing mechanisms over high-dimensional spaces can be easily refined to give a significant increase in performance; furthermore, the improvement given is greater in higher dimensions. This therefore leads to a significant improvement in the cost of metric search in these spaces. © 2016 ACM.",Four-point property; Hilbert embedding; Metric indexing; Metric Space; Similarity search,Geometry; Indexing (of information); Topology; Four-point; Hilbert; Metric indexing; Metric spaces; Similarity search; Set theory
Learning informative priors from heterogeneous domains to improve recommendation in cold-start user domains,2016,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85008883789&doi=10.1145%2f2976737&partnerID=40&md5=3fc37855ec45c7d304614b2ddc195d09,"In the real-world environment, users have sufficient experience in their focused domains but lack experience in other domains. Recommender systems are very helpful for recommending potentially desirable items to users in unfamiliar domains, and cross-domain collaborative filtering is therefore an important emerging research topic. However, it is inevitable that the cold-start issue will be encountered in unfamiliar domains due to the lack of feedback data. The Bayesian approach shows that priors play an important role when there are insufficient data, which implies that recommendation performance can be significantly improved in cold-start domains if informative priors can be provided. Based on this idea, we propose a Weighted Irregular Tensor Factorization (WITF) model to leverage multi-domain feedback data across all users to learn the cross-domain priors w.r.t. both users and items. The features learned from WITF serve as the informative priors on the latent factors of users and items in terms of weighted matrix factorization models. Moreover, WITF is a unified framework for dealing with both explicit feedback and implicit feedback. To prove the effectiveness of our approach, we studied three typical real-world cases in which a collection of empirical evaluations were conducted on real-world datasets to compare the performance of our model and other state-of-the-art approaches. The results show the superiority of our model over comparison models. © 2016 ACM.",Cross-domain collaborative filtering; Multi-task learning; Probabilistic matrix factorization; Recommender systems; Weighted irregular tensor factorization,Bayesian networks; Collaborative filtering; Factorization; Recommender systems; Tensors; Cross-domain; Matrix factorizations; Multitask learning; Probabilistic matrix factorizations; Real world environments; Recommendation performance; State-of-the-art approach; Tensor factorization; Matrix algebra
Time-aware click model,2016,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85008224766&doi=10.1145%2f2988230&partnerID=40&md5=04863fa2ca129855e12a6f6f053e72e2,"Click-through information is considered as a valuable source of users' implicit relevance feedback for commercial search engines. As existing studies have shown that the search result position in a search engine result page (SERP) has a very strong influence on users' examination behavior, most existing click models are position based, assuming that users examine results from top to bottom in a linear fashion. Although these click models have been successful, most do not take temporal information into account. As many existing studies have shown, click dwell time and click sequence information are strongly correlated with users' perceived relevance and search satisfaction. Incorporating temporal information may be important to improve performance of user click models for Web searches. In this article, we investigate the problem of properly incorporating temporal information into click models. We first carry out a laboratory eye-tracking study to analyze users' examination behavior in different click sequences and find that the user common examination path among adjacent clicks is linear. Next, we analyze the user dwell time distribution in different search logs and find that we cannot simply use a click dwell time threshold (e.g., 30 seconds) to distinguish relevant/irrelevant results. Finally, we propose a novel time-aware click model (TACM), which captures the temporal information of user behavior. We compare the TACM to several existing click models using two real-world search engine logs. Experimental results show that the TACM outperforms other click models in terms of both predicting click behavior (perplexity) and estimating result relevance (NDCG). © 2016 ACM.",Click dwell time; Click model; Click sequence,Behavioral research; Click sequence; Dwell time; Eye-tracking studies; Improve performance; Perceived relevances; Search engine results; Sequence informations; Temporal information; Search engines
Fast ranking with additive ensembles of oblivious and non-oblivious regression trees,2016,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85008912325&doi=10.1145%2f2987380&partnerID=40&md5=72947ad892f61dabe27d8dde397d98ea,"Learning-to-Rank models based on additive ensembles of regression trees have been proven to be very effective for scoring query results returned by large-scale Web search engines. Unfortunately, the computational cost of scoring thousands of candidate documents by traversing large ensembles of trees is high. Thus, several works have investigated solutions aimed at improving the efficiency of document scoring by exploiting advanced features of modern CPUs and memory hierarchies. In this article, we present QUICKSCORER, a new algorithm that adopts a novel cache-efficient representation of a given tree ensemble, performs an interleaved traversal by means of fast bitwise operations, and supports ensembles of oblivious trees. An extensive and detailed test assessment is conducted on two standard Learning-to-Rank datasets and on a novel very large dataset we made publicly available for conducting significant efficiency tests. The experiments show unprecedented speedups over the best state-of-the-art baselines ranging from 1.9× to 6.6×. The analysis of low-level profiling traces shows that QUICKSCORER efficiency is due to its cache-aware approach in terms of both data layout and access patterns and to a control flow that entails very low branch mis-prediction rates. © 2016 ACM.",Additive ensembles of regression trees; Cache-awareness; Document scoring; Efficiency; Learning to rank,Efficiency; Forestry; Program processors; Regression analysis; Search engines; Statistical tests; Bitwise operations; Cache-awareness; Computational costs; Document scoring; Learning to rank; Memory hierarchy; Prediction rate; Regression trees; Trees (mathematics)
Cost-effective online trending topic detection and popularity prediction in microblogging,2016,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85008240000&doi=10.1145%2f3001833&partnerID=40&md5=4099fbc1dd3a8b10d0a2d3c34247f75b,"Identifying topic trends on microblogging services such as Twitter and estimating those topics' future popularity have great academic and business value, especially when the operations can be done in real time. For any third party, however, capturing and processing such huge volumes of real-time data in microblogs are almost infeasible tasks, as there always exist API (Application Program Interface) request limits, monitoring and computing budgets, as well as timeliness requirements. To deal with these challenges, we propose a cost-effective system framework with algorithms that can automatically select a subset of representative users in microblogging networks in offline, under given cost constraints. Then the proposed system can online monitor and utilize only these selected users' real-time microposts to detect the overall trending topics and predict their future popularity among the whole microblogging network. Therefore, our proposed system framework is practical for real-time usage as it avoids the high cost in capturing and processing full real-time data, while not compromising detection and prediction performance under given cost constraints. Experiments with real microblogs dataset show that by tracking only 500 users out of 0.6 million users and processing no more than 30,000 microposts daily, about 92% trending topics could be detected and predicted by the proposed system and, on average, more than 10 hours earlier than they appear in official trends lists. © 2016 ACM.",Cost; Microblogging; Prediction; Topic detection,Application programming interfaces (API); Application programs; Budget control; Cost effectiveness; Forecasting; Application program interfaces; Cost effective systems; Micro-blogging services; Microblogging; On-line monitors; Popularity predictions; Prediction performance; Topic detection; Costs
Boosting recommendation in unexplored categories by user price preference,2016,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994093860&doi=10.1145%2f2978579&partnerID=40&md5=df718a7f6910e0e0e2091ee04cb10460,"State-of-the-art methods for product recommendation encounter a significant performance drop in categories where a user has no purchase history. This problem needs to be addressed since current online retailers are moving beyond single category and attempting to be diversified. In this article, we investigate the challenging problem of product recommendation in unexplored categories and discover that the price, a factor comparable across categories, can improve the recommendation performance significantly. We introduce the price utility concept to characterize users' sense of price and propose three different utility functions. We show that user price preference in a category is a distribution and we mine typical user price preference patterns based on three different types of distance between distributions. We fuse user price preference through regularization and joint factorization to boost recommendation performance in both browsing and buying shopping orientations. Experimental results show that fusing user price preference improves performance in a series of recommendation tasks: unexplored category recommendation, product recommendation under a given unexplored category, and product recommendation under generic unexplored categories. © 2016 ACM.",Price; Product recommendation; Unexplored category,Computer networks; Information systems; nocv1; Preference pattern; Price; Product recommendation; Recommendation performance; State-of-the-art methods; Unexplored category; Utility functions; Costs
Context trees: Augmenting geospatial trajectories with context,2016,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994097603&doi=10.1145%2f2978578&partnerID=40&md5=38810a7af2c6c08b6eb5e588081b0a44,"Exposing latent knowledge in geospatial trajectories has the potential to provide a better understanding of the movements of individuals and groups. Motivated by such a desire, this work presents the context tree, a new hierarchical data structure that summarises the context behind user actions in a single model. We propose a method for context tree construction that augments geospatial trajectories with land usage data to identify such contexts. Through evaluation of the construction method and analysis of the properties of generated context trees,we demonstrate the foundation for understanding andmodelling behaviour afforded. Summarising user contexts into a single data structure gives easy access to information that would otherwise remain latent, providing the basis for better understanding and predicting the actions and behaviours of individuals and groups. Finally, we also present a method for pruning context trees for use in applications where it is desirable to reduce the size of the tree while retaining useful information. © 2016 ACM.",Clustering; Context; Land usage; Spatiotemporal data; Trajectories,Data structures; Trajectories; Clustering; Construction method; Context; Hierarchical data structure; Land usage; Single models; Spatio-temporal data; Tree construction; Trees (mathematics)
Ranking-oriented collaborative filtering: A listwise approach,2016,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994071494&doi=10.1145%2f2960408&partnerID=40&md5=3a3d7c91e1acf3133d6060f73ee3875c,"Collaborative filtering (CF) is one of the most effective techniques in recommender systems, which can be either rating oriented or ranking oriented. Ranking-oriented CF algorithms demonstrated significant performance gains in terms of ranking accuracy, being able to estimate a precise preference ranking of items for each user rather than the absolute ratings (as rating-oriented CF algorithms do). Conventional memory-based ranking-oriented CF can be referred to as pairwise algorithms. They represent each user as a set of preferences on each pair of items for similarity calculations and predictions. In this study, we propose ListCF, a novel listwise CF paradigm that seeks improvement in both accuracy and efficiency in comparison with pairwise CF. In ListCF, each user is represented as a probability distribution of the permutations over rated items based on the Plackett-Luce model, and the similarity between users is measured based on the Kullback-Leibler divergence between their probability distributions over the set of commonly rated items. Given a target user and the most similar users, ListCF directly predicts a total order of items for each user based on similar users' probability distributions over permutations of the items. Besides, we also reveal insightful connections among pointwise, pairwise, and listwise CF algorithms from the perspective of the matrix representations. In addition, to make our algorithm more scalable and adaptive, we present an incremental algorithm for ListCF, which allows incrementally updating the similarities between users when certain user submits a new rating or updates an existing rating. Extensive experiments on benchmark datasets in comparison with the state-of-the-art approaches demonstrate the promise of our approach. © 2016 ACM.",Collaborative filtering; Ranking-oriented collaborative filtering; Recommender systems,Probability; Probability distributions; Rating; Recommender systems; Benchmark datasets; Conventional memory; Incremental algorithm; Incrementally Updating; Kullback Leibler divergence; Matrix representation; Similarity calculation; State-of-the-art approach; Collaborative filtering
Cross-lingual topic discovery from multilingual search engine query log,2016,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994156583&doi=10.1145%2f2956235&partnerID=40&md5=8aa4ecdb85042ef55f8d6878f27c92c7,"Today, major commercial search engines are operating in a multinational fashion to provide web search services for millions of users who compose search queries by different languages. Hence, the search engine query log, which serves as the backbone of many search engine applications, records millions of users' search history in a wide spectrum of human languages and demonstrates a strong multilingual phenomenon. However, with its salience, the multilingual nature of a search engine query log is usually ignored by existing works, which usually consider query log entries of different languages as being orthogonal and independent. This kind of oversimplified assumption heavily distorts the underlying structure of web search data. In this article, we pioneer in recognition of the multilingual nature of a query log and make the first attempt to cross the language barrier in query logs. We propose a novel model named Cross-Lingual Query Log Topic Model (CL-QLTM) to analyze query logs from a cross-lingual perspective and derive the latent topics of web search data. The CL-QLTM comprehensively integrates web search data in different languages by collectively utilizing cross-lingual dictionaries, as well as the co-occurrence relations in the query log. In order to relieve the efficiency bottleneck of applying the CL-QLTM on voluminous query logs, we propose an efficient parameter inference algorithm based on the MapReduce computing paradigm. Both qualitative and quantitative experimental results show that the CL-QLTM is able to effectively derive cross-lingual topics from multilingual query logs and spawn a wide spectrum of new search engine applications. © 2016 ACM.",Probabilistic topic model; Query log; Search engine,Data mining; Inference engines; Search engines; Websites; Computing paradigm; Engine application; Language barriers; Multi-lingual search; Parameter inference; Probabilistic topic models; Query logs; Topic Discovery; Information retrieval
Joint modeling of user check-in behaviors for real-time point-of-interest recommendation,2016,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994176261&doi=10.1145%2f2873055&partnerID=40&md5=1db4a977386fa3316d4004c65cc8881b,"Point-of-Interest (POI) recommendation has become an important means to help people discover attractive and interesting places, especially when users travel out of town. However, the extreme sparsity of a user- POI matrix creates a severe challenge. To cope with this challenge, we propose a unified probabilistic generative model, the Topic-Region Model (TRM), to simultaneously discover the semantic, temporal, and spatial patterns of users' check-in activities, and to model their joint effect on users' decision making for selection of POIs to visit. To demonstrate the applicability and flexibility of TRM, we investigate how it supports two recommendation scenarios in a unified way, that is, hometown recommendation and outof- town recommendation. TRM effectively overcomes data sparsity by the complementarity and mutual enhancement of the diverse information associated with users' check-in activities (e.g., check-in content, time, and location) in the processes of discovering heterogeneous patterns and producing recommendations. To support real-time POI recommendations, we further extend the TRM model to an online learning model, TRM-Online, to track changing user interests and speed up the model training. In addition, based on the learned model, we propose a clustering-based branch and bound algorithm (CBB) to prune the POI search space and facilitate fast retrieval of the top-k recommendations. We conduct extensive experiments to evaluate the performance of our proposals on two real-world datasets, including recommendation effectiveness, overcoming the cold-start problem, recommendation efficiency, and model-training efficiency. The experimental results demonstrate the superiority of our TRM models, especially TRM-Online, compared with state-of-the-art competitive methods, by making more effective and efficient mobile recommendations. In addition, we study the importance of each type of pattern in the two recommendation scenarios, respectively, and find that exploiting temporal patterns is most important for the hometown recommendation scenario, while the semantic patterns play a dominant role in improving the recommendation effectiveness for out-of-town users. © 2016 ACM.",Efficient retrieval algorithm; Location-based service; Online learning; POI; Real-time recommendation,Behavioral research; Branch and bound method; Decision making; E-learning; Efficiency; Semantics; Telecommunication services; Branch-and-bound algorithms; Mobile recommendations; Online learning; Real time; Real-world datasets; Recommendation efficiency; Retrieval algorithms; Top-K recommendations; Location based services
"Trip Recommendation Meets Real-World Constraints: POI Availability, Diversity, and Traveling Time Uncertainty",2016,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84990062638&doi=10.1145%2f2948065&partnerID=40&md5=bb57958e2ac2760b377c768974053cd7,"As location-based social network (LBSN) services become increasingly popular, trip recommendation that recommends a sequence of points of interest (POIs) to visit for a user emerges as one of many important applications of LBSNs. Personalized trip recommendation tailors to users' specific tastes by learning from past check-in behaviors of users and their peers. Finding the optimal trip that maximizes user's experiences for a given time budget constraint is an NP-hard problem and previous solutions do not consider three practical and important constraints. One constraint is POI availability, where a POI may be only available during a certain time window. Another constraint is uncertain traveling time, where the traveling time between two POIs is uncertain. In addition, the diversity of the POIs included in the trip plays an important role in user's final adoptions. This work presents efficient solutions to personalized trip recommendation by incorporating these constraints and leveraging them to prune the search space. We evaluated the efficiency and effectiveness of our solutions on real-life LBSN datasets. © 2016 ACM.",location-based social network; recommender systems; Trip plan,Budget control; Computational complexity; Recommender systems; Location-based social networks; Points of interest; Real world constraints; Search spaces; Time windows; Traveling time; Trip plan; Uncertain traveling time; Location based services
The effects of aggregated search coherence on search behavior,2016,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84989949146&doi=10.1145%2f2935747&partnerID=40&md5=9622fd6f1b49a82b0e8251d72bf305cf,"Aggregated search is the task of combining results from multiple independent search systems in a single Search Engine Results Page (SERP). Aggregated search coherence refers to the extent to which different sources on the SERP focus on similar senses of an ambiguous or underspecified query. In previous studies, we found that the query senses in a set of vertical results can influence user engagement with the web results (the so-called ""spillover"" effect). In this work, we investigate five research questions (RQ1-RQ5) that extend our prior work. First, we investigate the extent to which results from different sources focus on different senses of an ambiguous query (RQ1). Second, we investigate how the vertical-to-web spillover effect varies across different verticals (RQ2). Then, we examine whether the level of spillover depends on the vertical position (RQ3) and on whether the vertical results are displayed with a border and different-colored background to distinguish them from the web results (RQ4). Finally, we propose a new method for displaying results from a particular vertical that are more consistent with the query senses in the web results (RQ5). We evaluate this new method based on how it influences users to make more correct decisions with respect to the web results- to engage with the web results when at least one of them is relevant and to avoid engaging with the web results otherwise. Our results show the following trends. In terms of RQ1, our analysis suggests that the top results from the web search engine are more diversified than the top results from our four different verticals considered (images, news, shopping, and video). In terms of RQ2, we found a stronger spillover effect for the images vertical than the news, shopping, and video verticals. In terms of RQ3, we found a stronger level of spillover when the vertical was positioned at the top of the SERP versus to the right side of the web results. In terms of RQ4, we found an interesting additive effect between the vertical's position and displaying the vertical results enclosed in a border and with a different-colored background-the image vertical had no spillover when presented to the right side of the web results and with a border and background. Finally, in terms of RQ5, we found that our proposed vertical results selection approach can influence users to make more correct predictions about their level of engagement with the web results. © 2016 ACM.",Aggregated search; Aggregated search coherence; Aggregated search evaluation; Search behavior; Vertical results selection,Computer networks; Information systems; Additive effects; Aggregated search; Aggregated search evaluation; Research questions; Search behavior; Spillover effects; Vertical positions; Vertical results selection; Search engines
Accounting for language changes over time in document similarity search,2016,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988420189&doi=10.1145%2f2934671&partnerID=40&md5=5178b36d6bddd295f50a480d66aeae59,"Given a query document, ranking the documents in a collection based on how similar they are to the query is an essential task with extensive applications. For collections that contain documents whose creation dates span several decades, this task is further complicated by the fact that the language changes over time. For example, many terms add or lose one ormore senses tomeet people's evolving needs. To address this problem, we present methods that take advantage of two types of information to account for the language change. The first is the citation network that often exists within the collection, which can be used to link related documents with significantly different creation dates (and hence different language use). The second is the changes in the usage frequency of terms that occur over time, which can indicate changes in their senses and uses. These methods utilize the preceding informationwhile estimating the representation of both documents and terms within the context of nonprobabilistic static and dynamic topic models. Our experiments on two real-world datasets that span more than 40 years show that our proposed methods improve the retrieval performance of existing models and that these improvements are statistically significant. © 2016 ACM.",Citation network; Language change; Longitudinal document collections; Regularization; Similarity search; Terms usage frequency changes,Information systems; Citation networks; Document collection; Frequency changes; Language change; Regularization; Similarity search; Computer networks
Answering Arabic why-questions: Baseline vs. RST-based approach,2016,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988432303&doi=10.1145%2f2950049&partnerID=40&md5=eac8a27150ca90996df9ad212fff10b0,"A Question Answering (QA) system is concerned with building a system that automatically answer questions posed by humans in a natural language. Compared to other languages, little effort was directed towards QA systems for Arabic. Due to the difficulty of handling why-questions, most Arabic QA systems tend to ignore it. In this article, we specifically address the why-question for Arabic using two different approaches and compare their performance and the quality of their answer. The first is the baseline approach, a generic method that is used to answer all types of questions, including factoid; and for the second approach, we use Rhetorical Structure Theory (RST). We evaluate both schemes using a corpus of 700 textual documents in different genres collected from Open Source Arabic Corpora (OSAC), and a set of 100 question-answer pairs. Overall, the performance measures of recall, precision, and c@1 was 68% (all three measures) for the baseline approach, and 71%, 78%, and 77.4%, respectively, for the RST-based approach. The recently introduced extension of the accuracy, the c@1 measure, rewards unanswered questions over those wrongly answered. © 2016 ACM.",Arabic question-answer; baseline; RST; Why question-answer,Text processing; Arabic question-answer; baseline; Performance measure; Question answering systems; Question-answer pairs; Rhetorical structure theory; Textual documents; Why question-answer; Natural language processing systems
"Social question answering: Textual, user, and network features for best answer prediction",2016,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988416919&doi=10.1145%2f2948063&partnerID=40&md5=74b785b970d439b714fcc446638a7e6e,"Community question answering (CQA) sites use a collaborative paradigm to satisfy complex information needs. Although the task of matching questions to their best answers has been tackled for more than a decade, the social question-answering practice is a complex process. The factors influencing the accuracy of question-answer matching are many and hard to disentangle. We approach the task from an applicationoriented perspective, probing the space of several dimensions relevant to this problem: features, algorithms, and topics. We gather under a learning to rank framework the most extensive feature set used in literature to date, including 225 features from five different families. We test the power of such features in predicting the best answer to a question on the largest dataset from Yahoo Answers used for this task so far (40M answers) and provide a faceted analysis of the results along different topical areas and question types. We propose a novel family of distributional semantics measures that most of the time can seamlessly replace widely used linguistic similarity features, being more than one order of magnitude faster to compute and providing greater predictive power. The best feature set reaches an improvement between 11% and 26% in P@1 compared to recent well-established state-of-the-art methods. © 2016 ACM.",Best answer prediction; Community question answering; Distributional semantics; Expert finding; Expertise networks; Yahoo Answers,Forecasting; Semantics; Statistical tests; Community question answering; Distributional semantics; Expert finding; Expertise networks; Yahoo Answers; Complex networks
A probabilistic lifestyle-based trajectory model for social strength inference from human trajectory data,2016,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988384764&doi=10.1145%2f2948064&partnerID=40&md5=286a2f17d0170f939390fd10136eb654,"With the pervasiveness of location-based social networks, it becomes increasingly important to consider the social characteristics of locations shared among persons. Several studies have been proposed to infer social strength by using trajectory similarity. However, these studies have two major shortcomings. First, they rely on the explicit co-occurrence of check-in locations. In this situation, a user pair of two friends who seldom share common locations or a user pair of two strangers who heavily share common visited locations will receive an unreliable estimation of the real social strength between them. Second, these studies do not consider how the overall trajectory patterns of users change with the varying of living styles. In this article, we propose a probabilistic generative model to mine latent lifestyle-related patterns from human trajectory data for inferring social strength. It can automatically learn functionality topics consisting of locations with similar service functions and transition probabilities over the set of functionality topics. Furthermore, a lifestyle is modeled as a unique transition probability matrix over the set of functionality topics. A user has a preference distribution over the set of lifestyles, and he or she is able to select over multiple lifestyles to adapt to different living contexts. The learned lifestyle-related patterns are subsequently used as features in a supervised learner for both strength estimation and link prediction. We conduct extensive experiments to evaluate the performance of the proposed method on two real-world datasets. The experimental results demonstrate the effectiveness of our proposed method. © 2016 ACM.",Link prediction; Probabilistic generative model; Social strength inference; Trajectory data,Location; Generative model; Link prediction; Location-based social networks; Social strength inference; Trajectory data; Trajectory similarities; Transition probabilities; Transition probability matrix; Trajectories
TopPRF: A probabilistic framework for integrating topic space into pseudo relevance feedback,2016,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84986596754&doi=10.1145%2f2956234&partnerID=40&md5=0e01f2a90d0175c7a96762007c73738b,"Traditional pseudo relevance feedback (PRF) models choose top k feedback documents for query expansion and treat those documents equally. When k is determined, feedback terms are selected without considering the reliability of these documents for relevance. Because the performance of PRF is sensitive to the selection of feedback terms, noisy terms imported from these irrelevant documents or partially relevant documents will harm the final results extensively. Intuitively, terms in these documents should be considered less important for feedback term selection. Nonetheless, how to measure the reliability of feedback documents is a difficult problem. Recently, topic modeling has become more and more popular in the information retrieval (IR) area. In order to identify how reliable a feedback document is to be relevant, we attempt to adapt the topical information into PRF. However, topics are hard to be quantified and therefore the identification of topic is usually fuzzy. It is very challenging for integrating the obtained topical information effectively into IR and other text-processing-related areas. Current research work mainly focuses on mining relevant information from particular topics. This is extremely difficult when the boundaries of different topics are hard to define. In this article, we investigate a key factor of this problem, the topic number for topic modeling and how it makes topics ""fuzzy."" To effectively and efficiently apply topical information, we propose a new probabilistic framework, ""TopPRF,"" and threemodels, TS-COS, TS-EU, and TS-Entropy, via integrating ""Topic Space"" (TS) information into pseudo relevance feedback. Thesemethods discover how reliable a document is to be relevant through both term and topical information.When selecting feedback terms, candidate terms in more reliable feedback documents should obtain extra weights. Experimental results on various public collections justify that our proposed methods can significantly reduce the influence of ""fuzzy topics"" and obtain stable, good results over the strong baseline models. Our proposed probabilistic framework, TopPRF, and three topicspace- based models are capable of searching documents beyond traditional term matching only and provide a promising avenue for constructing better topic-space-based IR systems. Moreover, in-depth discussions and conclusions are made to help other researchers apply topical information effectively. © 2016 ACM.",Pseudo relevance feedback; Text mining; Topic modeling,Natural language processing systems; Text processing; Feedback documents; Probabilistic framework; Pseudo relevance feedback; Pseudo-relevance feedbacks; Query expansion; Relevant documents; Text mining; Topic Modeling; Data mining
A meta-framework for modeling the human reading process in sentiment analysis,2016,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006408105&doi=10.1145%2f2950050&partnerID=40&md5=9388693d05fca490b09a793cba95d1ea,"This article introduces a sentiment analysis approach that adopts the way humans read, interpret, and extract sentiment from text. Our motivation builds on the assumption that human interpretation should lead to the most accurate assessment of sentiment in text. We call this automated process Human Reading for Sentiment (HRS). Previous research in sentiment analysis has produced many frameworks that can fit one or more of the HRS aspects; however, none of these methods has addressed them all in one approach. HRS provides a meta-framework for developing new sentiment analysis methods or improving existing ones. The proposed framework provides a theoretical lens for zooming in and evaluating aspects of any sentiment analysis method to identify gaps for improvements towards matching the human reading process. Key steps in HRS include the automation of humans low-level and high-level cognitive text processing. This methodology paves the way towards the integration of psychology with computational linguistics and machine learning to employ models of pragmatics and discourse analysis for sentiment analysis. HRS is tested with two state-of-the-art methods; one is based on feature engineering, and the other is based on deep learning. HRS highlighted the gaps in both methods and showed improvements for both. © 2016 ACM",Human reading; Notions; Psychology; Sentiment analysis; Supervised learning,Automation; Deep learning; Sentiment analysis; Supervised learning; Text processing; Automated process; Discourse analysis; Feature engineerings; Human reading; Meta-frameworks; Notions; Psychology; Zooming-in; Data mining
Comparing pointwise and listwise objective functions for random-forest-based learning-to-rank,2016,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84984662574&doi=10.1145%2f2866571&partnerID=40&md5=8d447cc232870bc5e28931d141b7306f,"Current random-forest (RF)-based learning-to-rank (LtR) algorithms use a classification or regression framework to solve the ranking problem in a pointwise manner. The success of this simple yet effective approach coupled with the inherent parallelizability of the learning algorithm makes it a strong candidate for widespread adoption. In this article, we aim to better understand the effectiveness of RF-based rank-learning algorithms with a focus on the comparison between pointwise and listwise approaches. We introduce what we believe to be the first listwise version of an RF-based LtR algorithm. The algorithm directly optimizes an information retrieval metric of choice (in our case, NDCG) in a greedy manner. Direct optimization of the listwise objective functions is computationally prohibitive for most learning algorithms, but possible in RF since each tree maximizes the objective in a coordinate-wise fashion. Computational complexity of the listwise approach is higher than the pointwise counterpart; hence for larger datasets, we design a hybrid algorithm that combines a listwise objective in the early stages of tree construction and a pointwise objective in the latter stages. We also study the effect of the discount function of NDCG on the listwise algorithm. Experimental results on several publicly available LtR datasets reveal that the listwise/hybrid algorithm outperforms the pointwise approach on the majority (but not all) of the datasets. We then investigate several aspects of the two algorithms to better understand the inevitable performance tradeoffs. The aspects include examining an RF-based unsupervised LtR algorithm and comparing individual tree strength. Finally, we compare the the investigated RF-based algorithms with several other LtR algorithms. © 2016 ACM.",Computational complexity; Evaluation metrics; Learning-to-rank; Objective function; Random forest; Splitting criterion,Algorithms; Computational complexity; Decision trees; Forestry; Optimization; Evaluation metrics; Learning to rank; Objective functions; Random forests; Splitting criterion; Learning algorithms
Causal relationship detection in archival collections of product reviews for understanding technology evolution,2016,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041765424&doi=10.1145%2f2937752&partnerID=40&md5=8095b66a1892277b59e034623fcd24b9,"Technology progress is one of the key reasons behind today’s rapid changes in lifestyles. Knowing how products and objects evolve can not only help with understanding the evolutionary patterns in our society but can also provide clues on effective product design and can offer support for predicting the future. We propose a general framework for analyzing technology’s impact on our lives through detecting cause–effect relationships, where causes represent changes in technology while effects are changes in social life, such as new activities or new ways of using products. We address the challenge of viewing technology evolution through the “social impact lens” by mining causal relationships from the long-term collections of product reviews. In particular, we first propose dividing vocabulary into two groups: terms describing product features (called physical terms) and terms representing product usage (called conceptual terms). We then search for two kinds of changes related to the appearance of terms: frequency-based and context-based changes. The former indicate periods when a word was significantly more frequently used, whereas the latter indicate periods of high change in the word’s context. Based on the detected changes, we then search for causal term pairs such that the change in the physical term triggers the change in the conceptual term. We next extend our approach to finding causal relationships between word groups such as a group of words representing the same technology and causing a given conceptual change or group of words representing two different technologies that simultaneously “co-cause” a conceptual change. We conduct experiments on different product types using the Amazon Product Review Dataset, which spans 1995 to 2013, and we demonstrate that our approaches outperform state-of-the-art baselines. © 2016 ACM",Causality detection; Product evolution analysis; Social influence; Technology evolution analysis,Product design; Causal relationships; Conceptual change; Product evolution; Product reviews; Social influence; State of the art; Technology evolution; Technology progress; Economic and social effects
Examining additivity and weak baselines,2016,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84975297793&doi=10.1145%2f2882782&partnerID=40&md5=2c99e4bc206dc4b42609a3d4b6846ea5,"We present a study of which baseline to use when testing a new retrieval technique. In contrast to past work, we show that measuring a statistically significant improvement over a weak baseline is not a good predictor of whether a similar improvement will be measured on a strong baseline. Sometimes strong baselines are made worse when a new technique is applied. We investigate whether conducting comparisons against a range of weaker baselines can increase confidence that an observed effect will also show improvements on a stronger baseline. Our results indicate that this is not the case - at best, testing against a range of baselines means that an experimenter can be more confident that the new technique is unlikely to significantly harm a strong baseline. Examining recent past work, we present evidence that the information retrieval (IR) community continues to test against weak baselines. This is unfortunate as, in light of our experiments, we conclude that the only way to be confident that a new technique is a contribution is to compare it against nothing less than the state of the art. © 2016 ACM.",Baselines; Evaluation; Information retrieval,Computer networks; Information systems; Additivity; Baselines; Evaluation; Retrieval techniques; State of the art; Information retrieval
Probabilistic models for contextual agreement in preferences,2016,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84975321423&doi=10.1145%2f2854147&partnerID=40&md5=80d005c4e4c71cd521d6f75c275229df,"The long-tail theory for consumer demand implies the need for more accurate personalization technologies to target items to the users who most desire them. A key tenet of personalization is the capacity to model user preferences. Most of the previous work on recommendation and personalization has focused primarily on individual preferences. While some focus on shared preferences between pairs of users, they assume that the same similarity value applies to all items. Here we investigate the notion of ""context,"" hypothesizing that while two users may agree on their preferences on some items, they may also disagree on other items. To model this, we design probabilistic models for the generation of rating differences between pairs of users across different items. Since this model also involves the estimation of rating differences on unseen items for the purpose of prediction, we further conduct a systematic analysis of matrix factorization and tensor factorization methods in this estimation, and propose a factorization model with a novel objective function of minimizing error in rating differences. Experiments on several real-life rating datasets show that our proposed model consistently yields context-specific similarity values that perform better on a prediction task than models relying on shared preferences. © 2016 ACM.",Contextual agreement; Generative model; User preference,Factorization; Factorization model; Generative model; Individual preference; Matrix factorizations; Personalization technology; Probabilistic models; Tensor factorization; User preference; Rating
Measuring the semantic uncertainty of news events for evolution potential estimation,2016,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84975253361&doi=10.1145%2f2903719&partnerID=40&md5=ab7778b1b76576e2acbaf3f819f58368,"The evolution potential estimation of news events can support the decision making of both corporations and governments. For example, a corporation could manage its public relations crisis in a timely manner if a negative news event about this corporation is known with large evolution potential in advance. However, existing state-of-the-art methods are mainly based on time series historical data, which are not suitable for the news events with limited historical data and bursty properties. In this article, we propose a purely content-based method to estimate the evolution potential of the news events. The proposed method considers a news event at a given time point as a system composed of different keywords, and the uncertainty of this system is defined and measured as the Semantic Uncertainty of this news event. At the same time, an uncertainty space is constructed with two extreme states: the most uncertain state and the most certain state. We believe that the Semantic Uncertainty has correlation with the content evolution of the news events, so it can be used to estimate the evolution potential of the news events. In order to verify the proposed method, we present detailed experimental setups and results measuring the correlation of the Semantic Uncertainty with the Content Change of news events using collected news events data. The results show that the correlation does exist and is stronger than the correlation of value from the time-series-based method with the Content Change. Therefore, we can use the Semantic Uncertainty to estimate the evolution potential of news events. © 2016 ACM.",Information search and retrieval; Natural language processing; News event; Semantic analysis; Text mining,Data mining; Decision making; Information retrieval; Natural language processing systems; Public relations; Semantics; Time series; Information search and retrieval; NAtural language processing; News event; Semantic analysis; Text mining; Uncertainty analysis
Diversifying query auto-completion,2016,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84975300027&doi=10.1145%2f2910579&partnerID=40&md5=c71fb4c1ce08b3712ffc25fcc5a43893,"Query auto-completion assists web search users in formulating queries with a few keystrokes, helping them to avoid spelling mistakes and to produce clear query expressions, and so on. Previous work on query auto-completion mainly centers around returning a list of completions to users, aiming to push queries that are most likely intended by the user to the top positions but ignoring the redundancy among the query candidates in the list. Thus, semantically related queries matching the input prefix are often returned together. This may push valuable suggestions out of the list, given that only a limited number of candidates can be shown to the user, which may result in a less than optimal search experience. In this article, we consider the task of diversifying query auto-completion, which aims to return the correct query completions early in a ranked list of candidate completions and at the same time reduce the redundancy among query auto-completion candidates. We develop a greedy query selection approach that predicts query completions based on the current search popularity of candidate completions and on the aspects of previous queries in the same search session. The popularity of completion candidates at query time can be directly aggregated from query logs. However, query aspects are implicitly expressed by previous clicked documents in the search context. To determine the query aspect, we categorize clicked documents of a query using a hierarchy based on the open directory project. Bayesian probabilistic matrix factorization is applied to derive the distribution of queries over all aspects. We quantify the improvement of our greedy query selection model against a state-of-the-art baseline using two large-scale, real-world query logs and show that it beats the baseline in terms of well-known metrics used in query auto-completion and diversification. In addition, we conduct a side-by-side experiment to verify the effectiveness of our proposal. © 2016 ACM.",Diversification; Query auto-completion; Query suggestion; Web search,Factorization; Probability distributions; Query processing; Redundancy; Websites; World Wide Web; Auto completion; Diversification; Open directory projects; Probabilistic matrix factorizations; Query expression; Query suggestion; State of the art; Web searches; Information retrieval
Query performance prediction using reference lists,2016,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84975252812&doi=10.1145%2f2926790&partnerID=40&md5=dbe79b298af17ac62126b0313ccd21bb,"The task of query performance prediction is to estimate the effectiveness of search performed in response to a query when no relevance judgments are available. We present a novel probabilistic analysis of the performance prediction task. The analysis gives rise to a general prediction framework that uses pseudo-effective or ineffective document lists that are retrieved in response to the query. These lists serve as reference to the result list at hand, the effectiveness of which we want to predict. We show that many previously proposed prediction methods can be explained using our framework. More generally, we shed new light on existing prediction methods and establish formal common grounds to seemingly different prediction approaches. In addition,we formally demonstrate the connection between prediction using reference lists and fusion of retrieved lists, and provide empirical support to this connection. Through an extensive empirical exploration, we study various factors that affect the quality of prediction using reference lists. © 2016 ACM.",Query performance prediction; Reference lists,Computer networks; Information systems; Common ground; Performance prediction; Prediction methods; Probabilistic analysis; Quality of predictions; Query performance prediction; Reference list; Relevance judgment; Forecasting
TISON: Trust inference in trust-oriented social networks,2016,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85022073735&doi=10.1145%2f2858791&partnerID=40&md5=2d38b8b00d41eacf92d4b9b7940665eb,"Trust systems represent a significant trend in decision support for social networks' service provision. The basic idea is to allow users to rate each other even without being direct neighbours. In this case, the purpose is to derive a trust score for a given user, which could be of help to decide whether to trust other users or not. In this article, we investigate the properties of trust propagation within social networks, based on the notion of transitivity, and we introduce the TISoN model to generate and evaluate Trust Inference within online Social Networks. To do so, (i) we develop a novel TPS algorithm for Trust Path Searching where we define neighbours' priority based on their direct trust degrees, and then select trusted paths while controlling the path length; and, (ii) we develop different TIM algorithms for Trust Inference Measuring and build a trust network. In addition, we analyse existing algorithms and we demonstrate that our proposed model better computes transitive trust values than do the existing models. We conduct extensive experiments on a real online social network dataset, Advogato. Experimental results show that our work is scalable and generates better results than do the pioneering approaches of the literature. © 2016 ACM.",Indirect trust; transitivity; trust paths,Decision support systems; Inference engines; Decision supports; Indirect trusts; On-line social networks; Service provisions; Transitive trusts; transitivity; Trust path; Trust propagation; Social networking (online)
Transfer learning to infer social ties across heterogeneous networks,2016,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84971420030&doi=10.1145%2f2746230&partnerID=40&md5=cb12ad32407063952279500053c5ebdc,"Interpersonal ties are responsible for the structure of social networks and the transmission of information through these networks. Different types of social ties have essentially different influences on people. Awareness of the types of social ties can benefit many applications, such as recommendation and community detection. For example, our close friends tend to move in the same circles that we do, while our classmates may be distributed into different communities. Though a bulk of research has focused on inferring particular types of relationships in a specific social network, few publications systematically study the generalization of the problem of predicting social ties across multiple heterogeneous networks. In this work, we develop a framework referred to as TranFG for classifying the type of social relationships by learning across heterogeneous networks. The framework incorporates social theories into a factor graph model, which effectively improves the accuracy of predicting the types of social relationships in a target network by borrowing knowledge from a different source network. We also present several active learning strategies to further enhance the inferring performance. To scale up the model to handle really large networks, we design a distributed learning algorithm for the proposed model. We evaluate the proposed framework (TranFG) on six different networks and compare with several existing methods. TranFG clearly outperforms the existing methods on multiple metrics. For example, by leveraging information from a coauthor network with labeled advisor-advisee relationships, TranFG is able to obtain an F1-score of 90% (8%-28% improvements over alternative methods) for predicting manager-subordinate relationships in an enterprise email network. The proposed model is efficient. It takes only a few minutes to train the proposed transfer model on large networks containing tens of thousands of nodes. © 2016 ACM 1046-8188/2016/04-ART7 $15.00.",Predictive model; Social influence; Social network; Social ties,Artificial intelligence; Forecasting; Graph theory; Heterogeneous networks; Learning algorithms; Social aspects; Social networking (online); Active learning strategies; Co-author networks; Community detection; Distributed learning algorithms; Predictive modeling; Social influence; Social relationships; Social ties; Economic and social effects
On effective location-aware music recommendation,2016,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84980342883&doi=10.1145%2f2846092&partnerID=40&md5=5d6ac54f2364823b5bb5f3b79e7e07e9,"Rapid advances in mobile devices and cloud-based music service now allow consumers to enjoy music anytime and anywhere. Consequently, there has been an increasing demand in studying intelligent techniques to facilitate context-aware music recommendation. However, one important context that is generally overlooked is user’s venue, which often includes surrounding atmosphere, correlates with activities, and greatly influences the user’s music preferences. In this article, we present a novel venue-aware music recommender system called VenueMusic to effectively identify suitable songs for various types of popular venues in our daily lives. Toward this goal, a Location-aware Topic Model (LTM) is proposed to (i) mine the common features of songs that are suitable for a venue type in a latent semantic space and (ii) represent songs and venue types in the shared latent space, in which songs and venue types can be directly matched. It is worth mentioning that to discover meaningful latent topics with the LTM, a Music Concept Sequence Generation (MCSG) scheme is designed to extract effective semantic representations for songs. An extensive experimental study based on two large music test collections demonstrates the effectiveness of the proposed topic model and MCSG scheme. The comparisons with state-of-the-art music recommender systems demonstrate the superior performance of VenueMusic system on recommendation accuracy by associating venue and music contents using a latent semantic space. This work is a pioneering study on the development of a venue-aware music recommender system. The results show the importance of considering the influence of venue types in the development of context-aware music recommender systems. © 2016 ACM",Algorithms; Design; Experimentation; Human factors,Algorithms; Data mining; Design; Human engineering; Semantics; Effective location; Experimentation; Intelligent techniques; Music recommendation; Music recommender systems; Recommendation accuracy; Semantic representation; Sequence generation; Recommender systems
Overview of the special issue on trust and veracity of information in social media,2016,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84966397463&doi=10.1145%2f2870630&partnerID=40&md5=47f3a63bf82a5c71daf6aa8fa532ac31,CCS Concepts: • Information systems→Information retrieval; Web searching and information discovery; • Human-centered computing→Social networking sites. © 2016 ACM.,Fake content; Information veracity; News verification; Rumour propagation; Social media,Social networking (online); Social sciences computing; Fake content; Human-centered computing; Information discovery; Information veracity; Social media; Social networking sites; Web-searching; Search engines
Misinformation in online social networks: Detect them all with a limited budget,2016,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84966292019&doi=10.1145%2f2885494&partnerID=40&md5=9c78c9282465ac529a244ecd8757ee00,"Online social networks have become an effective and important social platform for communication, opinions exchange, and information sharing. However, they also make it possible for rapid and wide misinformation diffusion, which may lead to pernicious influences on individuals or society. Hence, it is extremely important and necessary to detect the misinformation propagation by placing monitors. In this article, we first define a general misinformation-detection problem for the case where the knowledge about misinformation sources is lacking, and show its equivalence to the influence-maximization problem in the reverse graph. Furthermore, considering node vulnerability, we aim to detect the misinformation reaching to a specific user. Therefore, we study a τ-Monitor Placement problem for cases where partial knowledge of misinformation sources is available and prove its #P complexity.We formulate a corresponding integer program, tackle exponential constraints, and propose a Minimum Monitor Set Construction (MMSC) algorithm, in which the cut-set2 has been exploited in the estimation of reachability of node pairs. Moreover, we generalize the problem from a single target tomultiple central nodes and propose another algorithm based on a Monte Carlo sampling technique. Extensive experiments on real-world networks show the effectiveness of proposed algorithms with respect to minimizing the number of monitors. © 2016 ACM.",Misinformation detection; Monitor placement; Online social networks,Budget control; Human computer interaction; Integer programming; Monte Carlo methods; Exponential constraint; Influence maximizations; Information sharing; Minimizing the number of; Mis-information propagation; Monte Carlo sampling technique; On-line social networks; Real-world networks; Social networking (online)
"Digital wildfires: Propagation, verification, regulation, and responsible innovation",2016,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84969945231&doi=10.1145%2f2893478&partnerID=40&md5=fa54a5eba8dd2614d09d4dfceaf76ff7,"Social media platforms provide an increasingly popular means for individuals to share content online. Whilst this produces undoubted societal benefits, the ability for content to be spontaneously posted and reposted creates an ideal environment for rumour and false/malicious information to spread rapidly.When this occurs it can cause significant harm and can be characterised as a ""digital wildfire."" In this article, we demonstrate that the propagation and regulation of digital wildfires form important topics for research and conduct an overview of existing work in this area.We outline the relevance of a range of work from the computational and social sciences, including a series of insights into the propagation of rumour and false/malicious information. We argue that significant research gaps remain-for instance, there is an absence of systematic studies on the effects of digital wildfires and there is a need to combine empirical research with a consideration of how the responsible governance of social media can be determined. We propose an agenda for research that establishes a methodology to explore in full the propagation and regulation of unverified content on social media. This agenda promotes high-quality interdisciplinary research that will also inform policy debates. © 2016 ACM.",Rumour; Social media,Social networking (online); Empirical research; Interdisciplinary research; Rumour; Significant harm; Social media; Social media platforms; Societal benefits; Systematic study; Fires
Geoparsing and geosemantics for social media: Spatiotemporal grounding of content propagating rumors to support trust and veracity analysis during breaking news,2016,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84966335642&doi=10.1145%2f2842604&partnerID=40&md5=a4be7d71c6f405e683cdff450bd39367,"In recent years, there has been a growing trend to use publicly available social media sources within the field of journalism. Breaking news has tight reporting deadlines, measured in minutes not days, but content must still be checked and rumors verified. As such, journalists are looking at automated content analysis to prefilter large volumes of social media content prior to manual verification. This article describes a real-time social media analytics framework for journalists. We extend our previously published geoparsing approach to improve its scalability and efficiency. We develop and evaluate a novel approach to geosemantic feature extraction, classifying evidence in terms of situatedness, timeliness, confirmation, and validity. Our approach works for new unseen news topics. We report results from four experiments using five Twitter datasets crawled during different English-language news events. One of our datasets is the standard TREC 2012 microblog corpus. Our classification results are promising, with F1 scores varying by class from 0.64 to 0.92 for unseen event types. We lastly report results from two case studies during real-world news stories, showcasing different ways our system can assist journalists filter and cross-check content as they examine the trust and veracity of content and sources. © 2016 ACM.",Breaking news; Credibility; Geoparsing; Geosemantics; Journalism; News; Rumors; Social media; Trust; Veracity,Feature extraction; Breaking news; Credibility; Geo-semantics; Geoparsing; Journalism; News; Rumors; Social media; Trust; Veracity; Social networking (online)
Volunteerism Tendency Prediction via Harvesting Multiple Social Networks,2016,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964689138&doi=10.1145%2f2832907&partnerID=40&md5=c6c343c68521b20cba4e2292d65b29b6,"Volunteers have always been extremely crucial and in urgent need for nonprofit organizations (NPOs) to sustain their continuing operations. However, it is expensive and time-consuming to recruit volunteers using traditional approaches. In the Web 2.0 era, abundant and ubiquitous social media data opens a door to the possibility of automatic volunteer identification. In this article, we aim to fully explore this possibility by proposing a scheme that is able to predict users' volunteerism tendency from user-generated contents collected from multiple social networks based on a conceptual volunteering decision model. We conducted comprehensive experiments to investigate the effectiveness of our proposed scheme and further discussed its generalizibility and extendability. This novel interdisciplinary research will potentially inspire more promising and important human-centered applications. © 2016 ACM.",Multiple sources; Network-centric; User classification; User-centric; Volunteerism tendency prediction,Social networking (online); Multiple source; Network-centric; Tendency prediction; User classification; User-centric; Forecasting
Power law distributions in information retrieval,2016,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84968909097&doi=10.1145%2f2816815&partnerID=40&md5=053422e02af046402e69f93d76f3f7a6,"Several properties of information retrieval (IR) data, such as query frequency or document length, are widely considered to be approximately distributed as a power law. This common assumption aims to focus on specific characteristics of the empirical probability distribution of such data (e.g., its scale-free nature or its long/fat tail). This assumption, however, may not be always true. Motivated by recent work in the statistical treatment of power law claims, we investigate two research questions: (i) To what extent do power law approximations hold for term frequency, document length, query frequency, query length, citation frequency, and syntactic unigram frequency? And (ii) what is the computational cost of replacing ad hoc power law approximations with more accurate distribution fitting? We study 23 TREC and 5 non-TREC datasets and compare the fit of power laws to 15 other standard probability distributions. We find that query frequency and 5 out of 24 term frequency distributions are best approximated by a power law. All remaining properties are better approximated by the Inverse Gaussian, Generalized Extreme Value, Negative Binomial, or Yule distribution. We also find the overhead of replacing power law approximations by more informed distribution fitting to be negligible, with potential gains to IR tasks like index compression or test collection generation for IR evaluation. © 2016 ACM.",Power laws; Statistical model selection,Information retrieval; Computational costs; Distribution fitting; Generalized extreme value; Power law distribution; Power-law; Research questions; Statistical modeling; Statistical treatment; Probability distributions
Subspace Frequency Analysis Based Field Indices Extraction for Electricity Customer Classification,2016,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84958824880&doi=10.1145%2f2858657&partnerID=40&md5=29d7fc833578f6e34f54aafc132c5ada,"In electricity customer classification, themost important task is to avoid the curse of dimensionality problem, as the consumption diagrams have a large number of dimensions. To avoid the curse of dimensionality problem, field indices (load shape factor) are often used instead of consumption diagrams. Field indices are directly extracted from consumption diagrams according to a predefined formula. Previous studies show that the most important thing for defining such a formula is to find meaningful time intervals from consumption diagrams. However, the inconvenient thing is that there are still a lack of details to explain how to define such time intervals. In our study, we propose a data mining based method named SFATIE to support the extraction of field indices. The performance of the proposed method is evaluated by comparing it with other dimensionality reduction methods during the classification. For the classification, most often we have used classification methods like C5.0, SVM, Neural Net, Bayes Net, and Logistic. The experimental results show that our method is better or close to other dimensionality reduction methods. In addition, the experimental results show that our proposed method can produce the good quality of field indices and that these indices can improve the performance of electricity customer classification.",Classification; Dimensionality reduction; Feature extraction; Subspace analysis,Data mining; Extraction; Feature extraction; Graphic methods; Sales; Classification methods; Curse of dimensionality; Dimensionality reduction; Dimensionality reduction method; Electricity customer classification; Frequency Analysis; Subspace analysis; Time interval; Classification (of information)
A Tensor-Based Information Framework for Predicting the Stock Market,2016,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84958759335&doi=10.1145%2f2838731&partnerID=40&md5=9b28fe78a56dc1f427a495221ffc55e0,"To study the influence of information on the behavior of stock markets, a common strategy in previous studies has been to concatenate the features of various information sources into one compound feature vector, a procedure thatmakes it more difficult to distinguish the effects of different information sources.We maintain that capturing the intrinsic relations among multiple information sources is important for predicting stock trends. The challenge lies in modeling the complex space of various sources and types of information and studying the effects of this information on stock market behavior. For this purpose, we introduce a tensorbased information framework to predict stock movements. Specifically, our framework models the complex investor information environment with tensors. A global dimensionality-reduction algorithm is used to capture the links among various information sources in a tensor, and a sequence of tensors is used to represent information gathered over time. Finally, a tensor-based predictive model to forecast stock movements, which is in essence a high-order tensor regression learning problem, is presented. Experiments performed on an entire year of data for China Securities Index stocks demonstrate that a trading system based on our framework outperforms the classic Top-N trading strategy and two state-of-the-art media-aware trading.",News; Predictive model; Social media; Stock; Tensor; Trading strategy,Algorithms; Commerce; Electronic trading; Finance; Financial markets; Forecasting; Investments; News; Predictive modeling; Social media; Stock; Trading strategies; Tensors
Influence Estimation and Maximization in Continuous-Time Diffusion Networks,2016,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84958774319&doi=10.1145%2f2824253&partnerID=40&md5=fdbe7dc6b09f8378bee489e73ea9c22d,"If a piece of information is released from a set ofmedia sites, can it spread, in 1month, to a million web pages? Can we efficiently find a small set ofmedia sites among millions that canmaximize the spread of the information, in 1 month? The two problems are called influence estimation and maximization problems respectively, which are very challenging since both the time-sensitive nature of the problems and the issue of scalability need to be addressed simultaneously. In this article, we propose two algorithms for influence estimation in continuous-time diffusion networks. The first one uses continuous-time Markov chains to estimate influence exactly on networks with exponential, or, more generally, phase-type transmission functions, but does not scale to large-scale networks, and the second one is a highly efficient randomized algorithm, which estimates the influence of every node in a network with general transmission functions, |ν| nodes and |ε| edges to an accuracy of ε using n = O(1/ε2) randomizations and up to logarithmic factors O(n|ε|+n|V|) computations.We then show that finding the set of most influential source nodes in a continuous time diffusion network is an NP-hard problem and develop an efficient greedy algorithm with provable near-optimal performance. When used as subroutines in the influence maximization algorithm, the exact influence estimation algorithm is guaranteed to find a set of C nodes with an influence of at least (1-1/e)OPT and the randomized algorithm is guaranteed to find a set with an influence of at least (1-1/e)OPT-2Cε, where OPT is the optimal value. Experiments on both synthetic and real-world data show that the proposed algorithms significantly improve over previous state-of-the-art methods in terms of the accuracy of the estimated influence and the quality of the selected nodes to maximize the influence, and the randomized algorithm can easily scale up to networks of millions of nodes.",Influence estimation; Influence maximization; Networks of diffusion; Social networks,Algorithms; Computational complexity; Diffusion; Markov processes; Optimization; Social networking (online); Websites; Continuous time Markov chain; Estimation algorithm; Influence maximizations; Maximization problem; Near-optimal performance; Randomized Algorithms; State-of-the-art methods; Transmission function; Continuous time systems
Personality-affected Emotion Generation in Dialog Systems,2024,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195102809&doi=10.1145%2f3655616&partnerID=40&md5=e82bf0551ed03296eeb71d5116fe9699,"Generating appropriate emotions for responses is essential for dialogue systems to provide human-like interaction in various application scenarios. Most previous dialogue systems tried to achieve this goal by learning empathetic manners from anonymous conversational data. However, emotional responses generated by those methods may be inconsistent, which will decrease user engagement and service quality. Psychological findings suggest that the emotional expressions of humans are rooted in personality traits. Therefore, we propose a new task, Personality-affected Emotion Generation, to generate emotion based on the personality given to the dialogue system and further investigate a solution through the personality-affected mood transition. Specifically, we first construct a daily dialogue dataset, Personality EmotionLines Dataset (PELD), with emotion and personality annotations. Subsequently, we analyze the challenges in this task, i.e., (1) heterogeneously integrating personality and emotional factors and (2) extracting multi-granularity emotional information in the dialogue context. Finally, we propose to model the personality as the transition weight by simulating the mood transition process in the dialogue system and solve the challenges above. We conduct extensive experiments on PELD for evaluation. Results suggest that by adopting our method, the emotion generation performance is improved by 13% in macro-F1 and 5% in weighted-F1 from the BERT-base model.  Copyright © 2024 held by the owner/author(s).",dialogue systems; emotion; personality,Application scenario; Dialogue systems; Emotion; Emotion generation; Emotional response; Human like; Personality; Service Quality; User engagement; User services
Cross-Domain NER under a Divide-and-Transfer Paradigm,2024,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195091223&doi=10.1145%2f3655618&partnerID=40&md5=4a932ae7aad5ac1b7fdf3fb44291831c,"Cross-domain Named Entity Recognition (NER) transfers knowledge learned from a rich-resource source domain to improve the learning in a low-resource target domain. Most existing works are designed based on the sequence labeling framework, defining entity detection and type prediction as a monolithic process. However, they typically ignore the discrepant transferability of these two sub-tasks: the former locating spans corresponding to entities is largely domain-robust, whereas the latter owns distinct entity types across domains. Combining them into an entangled learning problem may contribute to the complexity of domain transfer. In this work, we propose the novel divide-and-transfer paradigm in which different sub-tasks are learned using separate functional modules for respective cross-domain transfer. To demonstrate the effectiveness of divide-and-transfer, we concretely implement two NER frameworks by applying this paradigm with different cross-domain transfer strategies. Experimental results on 10 different domain pairs show the notable superiority of our proposed frameworks. Experimental analyses indicate that significant advantages of the divide-and-transfer paradigm over prior monolithic ones originate from its better performance on low-resource data and a much greater transferability. It gives us a new insight into cross-domain NER. Our code is available on GitHub.  © 2024 Copyright held by the owner/author(s).",cross-domain transfer; information extraction; knowledge acquisition; Named entity recognition; task decomposition,Knowledge acquisition; Knowledge management; Cross-domain; Cross-domain transfer; Domain transfers; Entity-types; Information extraction; Monolithics; Named entity recognition; Subtask; Target domain; Task decomposition; Information retrieval
SSR: Solving Named Entity Recognition Problems via a Single-stream Reasoner,2024,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195052260&doi=10.1145%2f3655619&partnerID=40&md5=95361e16d4dc9aa9c68952c86b673769,"Information Extraction (IE) focuses on transforming unstructured data into structured knowledge, of which Named Entity Recognition (NER) is a fundamental component. In the realm of Information Retrieval (IR), effectively recognizing entities can substantially enhance the precision of search and recommendation systems. Existing methods frame NER as a sequence labeling task, which requires extra data and, therefore may be limited in terms of sustainability. One promising solution is to employ a Machine Reading Comprehension (MRC) approach for NER tasks, thereby eliminating the dependence on additional data. This process encounters key challenges, including: (1) Unconventional predictions; (2) Inefficient multi-stream processing; (3) Absence of a proficient reasoning strategy. To this end, we present the Single-Stream Reasoner (SSR), a solution utilizing a reasoning strategy and standardized inputs. This yields a type-agnostic solution for both flat and nested NER tasks, without the need for additional data. On ten NER benchmarks, SSR achieved state-of-the-art results, highlighting its robustness. Furthermore, we illustrated its efficiency through convergence, inference speed, and low-resource scenario performance comparisons. Our architecture displays adaptability and can effortlessly merge with various foundational models and reasoning strategies, fostering advancements in both the IR and IE fields.  Copyright © 2024 held by the owner/author(s).",information extraction; named entity recognition; natural language understanding; reasoning,Metadata; Search engines; Additional datum; Fundamental component; Information extraction; Named entity recognition; Natural language understanding; Reasoners; Reasoning; Sequence Labeling; Structured knowledge; Unstructured data; Recommender systems
Average User-Side Counterfactual Fairness for Collaborative Filtering,2024,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195025045&doi=10.1145%2f3656639&partnerID=40&md5=6f884b05696ac917d0f668cc48303fd2,"Recently, the user-side fairness issue in Collaborative Filtering (CF) algorithms has gained considerable attention, arguing that results should not discriminate an individual or a sub-user group based on users' sensitive attributes (e.g., gender). Researchers have proposed fairness-aware CF models by decreasing statistical associations between predictions and sensitive attributes. A more natural idea is to achieve model fairness from a causal perspective. The remaining challenge is that we have no access to interventions, i.e., the counterfactual world that produces recommendations when each user has changed the sensitive attribute value. To this end, we first borrow the Rubin-Neyman potential outcome framework to define average causal effects of sensitive attributes. Next, we show that removing causal effects of sensitive attributes is equal to average counterfactual fairness in CF. Then, we use the propensity re-weighting paradigm to estimate the average causal effects of sensitive attributes and formulate the estimated causal effects as an additional regularization term. To the best of our knowledge, we are one of the first few attempts to achieve counterfactual fairness from the causal effect estimation perspective in CF, which frees us from building sophisticated causal graphs. Finally, experiments on three real-world datasets show the superiority of our proposed model.  Copyright © 2024 held by the owner/author(s).",Fairness issues in collaborative filtering; potential outcome framework,Attribute values; Collaborative filtering algorithms; Counterfactuals; Fairness issue in collaborative filtering; Filtering models; Group-based; Potential outcome framework; Potential outcomes; Sensitive attribute; User groups; Collaborative filtering
Privacy-preserving Cross-domain Recommendation with Federated Graph Learning,2024,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195241307&doi=10.1145%2f3653448&partnerID=40&md5=2bc425bf9c07763927b269cd6ec6d739,"As people inevitably interact with items across multiple domains or various platforms, cross-domain recommendation (CDR) has gained increasing attention. However, the rising privacy concerns limit the practical applications of existing CDR models, since they assume that full or partial data are accessible among different domains. Recent studies on privacy-aware CDR models neglect the heterogeneity from multiple-domain data and fail to achieve consistent improvements in cross-domain recommendation; thus, it remains a challenging task to conduct effective CDR in a privacy-preserving way. In this article, we propose a novel, as far as we know, federated graph learning approach for Privacy-Preserving Cross-Domain Recommendation (PPCDR) to capture users' preferences based on distributed multi-domain data and improve recommendation performance for all domains without privacy leakage. The main idea of PPCDR is to model both global preference among multiple domains and local preference at a specific domain for a given user, which characterizes the user's shared and domain-specific tastes toward the items for interaction. Specifically, in the private update process of PPCDR, we design a graph transfer module for each domain to fuse global and local user preferences and update them based on local domain data. In the federated update process, through applying the local differential privacy technique for privacy-preserving, we collaboratively learn global user preferences based on multi-domain data and adapt these global preferences to heterogeneous domain data via personalized aggregation. In this way, PPCDR can effectively approximate the multi-domain training process that directly shares local interaction data in a privacy-preserving way. Extensive experiments on three CDR datasets demonstrate that PPCDR consistently outperforms competitive single- and cross-domain baselines and effectively protects domain privacy. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",cross-domain recommendation; federated learning; privacy-preserving; Recommender systems,Clock and data recovery circuits (CDR circuits); Learning systems; Cross-domain recommendations; Different domains; Federated learning; Multi-domains; Multiple domains; Partial data; Preference-based; Privacy concerns; Privacy preserving; User's preferences; Privacy-preserving techniques
FDKT: Towards an Interpretable Deep Knowledge Tracing via Fuzzy Reasoning,2024,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195041086&doi=10.1145%2f3656167&partnerID=40&md5=9a5baeb1d3740ea6218135f14db470b0,"In educational data mining, knowledge tracing (KT) aims to model learning performance based on student knowledge mastery. Deep-learning-based KT models perform remarkably better than traditional KT and have attracted considerable attention. However, most of them lack interpretability, making it challenging to explain why the model performed well in the prediction. In this paper, we propose an interpretable deep KT model, referred to as fuzzy deep knowledge tracing (FDKT) via fuzzy reasoning. Specifically, we formalize continuous scores into several fuzzy scores using the fuzzification module. Then, we input the fuzzy scores into the fuzzy reasoning module (FRM). FRM is designed to deduce the current cognitive ability, based on which the future performance was predicted. FDKT greatly enhanced the intrinsic interpretability of deep-learning-based KT through the interpretation of the deduction of student cognition. Furthermore, it broadened the application of KT to continuous scores. Improved performance with regard to both the advantages of FDKT was demonstrated through comparisons with the state-of-the-art models.  Copyright © 2024 held by the owner/author(s).",deep learning; Educational data mining; fuzzy reasoning; knowledge tracing; model interpretability,Deep learning; Learning systems; Students; Deep knowledge; Deep learning; Educational data mining; Fuzzy reasoning; Fuzzy score; Interpretability; Knowledge tracings; Model interpretability; Model learning; Tracing model; Data mining
Passage-aware Search Result Diversification,2024,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195037632&doi=10.1145%2f3653672&partnerID=40&md5=91e7026606a7603e8210602b99563469,"Research on search result diversification strives to enhance the variety of subtopics within the list of search results. Existing studies usually treat a document as a whole and represent it with one fixed-length vector. However, considering that a long document could cover different aspects of a query, using a single vector to represent the document is usually insufficient. To tackle this problem, we propose to exploit multiple passages to better represent documents in search result diversification. Different passages of each document may reflect different subtopics of the query and comparison among the passages can improve result diversity. Specifically, we segment the entire document into multiple passages and train a classifier to filter out the irrelevant ones. Then the document diversity is measured based on several passages that can offer the information needs of the query. Thereafter, we devise a passage-aware search result diversification framework that takes into account the topic information contained in the selected document sequence and candidate documents. The candidate documents' novelty is evaluated based on their passages while considering the dynamically selected document sequence. We conducted experiments on a commonly utilized dataset, and the results indicate that our proposed method performs better than the most leading methods.  Copyright © 2024 held by the owner/author(s).",intra-document attention; passage ranking; Search result diversification,Intra-document attention; Multiple passage; Multiple trains; Passage ranking; Search results diversifications; Single vectors
DHyper: A Recurrent Dual Hypergraph Neural Network for Event Prediction in Temporal Knowledge Graphs,2024,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195063590&doi=10.1145%2f3653015&partnerID=40&md5=6c3f11f2f2e3283cc4ac84799307f772,"Event prediction is a vital and challenging task in temporal knowledge graphs (TKGs), which have played crucial roles in various applications. Recently, many graph neural networks based approaches are proposed to model the graph structure information in TKGs. However, these approaches only construct graphs based on quadruplets and model the pairwise correlation between entities, which fail to capture the high-order correlations among entities. To this end, we propose DHyper, a recurrent Dual Hypergraph neural network for event prediction in TKGs, which simultaneously models the influences of the high-order correlations among both entities and relations. Specifically, a dual hypergraph learning module is proposed to discover the high-order correlations among entities and among relations in a parameterized way. A dual hypergraph message passing network is introduced to perform the information aggregation and representation fusion on the entity hypergraph and the relation hypergraph. Extensive experiments on six real-world datasets demonstrate that DHyper achieves the state-of-the-art performances, outperforming the best baseline by an average of 13.09%, 4.26%, 17.60%, and 18.03% in MRR, Hits@1, Hits@3, and Hits@10, respectively.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Event prediction; graph neural networks; hypergraph; temporal knowledge graphs,Forecasting; Graph neural networks; Graphic methods; Message passing; Recurrent neural networks; Event prediction; Graph neural networks; High order correlation; Higher order correlation; Hyper graph; Knowledge graphs; Network-based approach; Neural-networks; Temporal knowledge; Temporal knowledge graph; Knowledge graph
Listwise Generative Retrieval Models via a Sequential Learning Process,2024,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195054632&doi=10.1145%2f3653712&partnerID=40&md5=1ae7d0dc4ab514aefafc9dbba4e004c9,"Recently, a novel generative retrieval (GR) paradigm has been proposed, where a single sequence-to-sequence model is learned to directly generate a list of relevant document identifiers (docids) given a query. Existing GR models commonly employ maximum likelihood estimation (MLE) for optimization: This involves maximizing the likelihood of a single relevant docid given an input query, with the assumption that the likelihood for each docid is independent of the other docids in the list. We refer to these models as the pointwise approach in this article. While the pointwise approach has been shown to be effective in the context of GR, it is considered sub-optimal due to its disregard for the fundamental principle that ranking involves making predictions about lists. In this article, we address this limitation by introducing an alternative listwise approach, which empowers the GR model to optimize the relevance at the docid list level. Specifically, we view the generation of a ranked docid list as a sequence learning process: At each step, we learn a subset of parameters that maximizes the corresponding generation likelihood of the ith docid given the (preceding) top i-1 docids. To formalize the sequence learning process, we design a positional conditional probability for GR. To alleviate the potential impact of beam search on the generation quality during inference, we perform relevance calibration on the generation likelihood of model-generated docids according to relevance grades. We conduct extensive experiments on representative binary and multi-graded relevance datasets. Our empirical results demonstrate that our method outperforms state-of-the-art GR baselines in terms of retrieval performance.  © 2024 Copyright held by the owner/author(s).",Document retrieval; generative retrieval; listwise approach,Information retrieval; Learning systems; Document identifiers; Document Retrieval; Generative retrieval; Learning process; Listwise approach; Pointwise approach; Relevant documents; Retrieval models; Sequence learning; Sequential learning; Maximum likelihood estimation
Beyond Relevance: Factor-level Causal Explanation for User Travel Decisions with Counterfactual Data Augmentation,2024,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192997119&doi=10.1145%2f3653673&partnerID=40&md5=fab4d031ed6639f33ed6cae266911050,"Point-of-Interest (POI) recommendation, an important research hotspot in the field of urban computing, plays a crucial role in urban construction. While understanding the process of users' travel decisions and exploring the causality of POI choosing is not easy due to the complex and diverse influencing factors in urban travel scenarios. Moreover, the spurious explanations caused by severe data sparsity, i.e., misrepresenting universal relevance as causality, may also hinder us from understanding users' travel decisions. To this end, in this article, we propose a factor-level causal explanation generation framework based on counterfactual data augmentation for user travel decisions, named Factor-level Causal Explanation for User Travel Decisions (FCE-UTD), which can distinguish between true and false causal factors and generate true causal explanations. Specifically, we first assume that a user decision is composed of a set of several different factors. Then, by preserving the user decision structure with a joint counterfactual contrastive learning paradigm, we learn the representation of factors and detect the relevant factors. Next, we further identify true causal factors by constructing counterfactual decisions with a counterfactual representation generator, in particular, it can not only augment the dataset and mitigate the sparsity but also contribute to clarifying the causal factors from other false causal factors that may cause spurious explanations. Besides, a causal dependency learner is proposed to identify causal factors for each decision by learning causal dependency scores. Extensive experiments conducted on three real-world datasets demonstrate the superiority of our approach in terms of check-in rate, fidelity, and downstream tasks under different behavior scenarios. The extra case studies also demonstrate the ability of FCE-UTD to generate causal explanations in POI choosing.  © 2024 Copyright held by the owner/author(s).",Causal explanation generation; contrastive learning; counterfactual data augmentation; urban travel decisions,Causal dependencies; Causal explanation generation; Causal explanations; Contrastive learning; Counterfactual data augmentation; Counterfactuals; Data augmentation; Travel decisions; Urban travel decision; Urban travels
Special Section on Efficiency in Neural Information Retrieval,2024,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195310556&doi=10.1145%2f3641203&partnerID=40&md5=4c6553b2892b2ade202ad34d29b25f45,[No abstract available],,
Revisiting Bag of Words Document Representations for Efficient Ranking with Transformers,2024,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195067025&doi=10.1145%2f3640460&partnerID=40&md5=e8f8fd66dda5d05c8cfcfcc0972f3924,"Modern transformer-based information retrieval models achieve state-of-the-art performance across various benchmarks. The self-attention of the transformer models is a powerful mechanism to contextualize terms over the whole input but quickly becomes prohibitively expensive for long input as required in document retrieval. Instead of focusing on the model itself to improve efficiency, this paper explores different bag of words document representations that encode full documents by only a fraction of their characteristic terms, allowing us to control and reduce the input length. We experiment with various models for document retrieval on MS MARCO data, as well as zero-shot document retrieval on Robust04, and show large gains in efficiency while retaining reasonable effectiveness. Inference time efficiency gains are both lowering the time and memory complexity in a controllable way, allowing for further trading off memory footprint and query latency. More generally, this line of research connects traditional IR models with neural ""NLP""models and offers novel ways to explore the space between (efficient, but less effective) traditional rankers and (effective, but less efficient) neural rankers elegantly.  © 2024 Copyright held by the owner/author(s).",Efficiency; neural bag-of-words; neural ranking models; re-ranking,Benchmarking; Information retrieval; Zero-shot learning; Bag of words; Document Representation; Document Retrieval; Information retrieval models; Neural bag-of-word; Neural ranking model; Ranking model; Re-ranking; State-of-the-art performance; Word documents; Efficiency
Distributional Fairness-aware Recommendation,2024,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195087208&doi=10.1145%2f3652854&partnerID=40&md5=564cf471c308aced4992f03adeba38b8,"Fairness has been gradually recognized as a significant problem in the recommendation domain. Previous models usually achieve fairness by reducing the average performance gap between different user groups. However, the average performance may not sufficiently represent all the characteristics of the performances in a user group. Thus, equivalent average performance may not mean the recommender model is fair, for example, the variance of the performances can be different. To alleviate this problem, in this article, we define a novel type of fairness, where we require that the performance distributions across different user groups should be similar. We prove that with the same performance distribution, the numerical characteristics of the group performance, including the expectation, variance, and any higher-order moment, are also the same. To achieve distributional fairness, we propose a generative and adversarial training framework. Specifically, we regard the recommender model as the generator to compute the performance for each user in different groups, and then we deploy a discriminator to judge which group the performance is drawn from. By iteratively optimizing the generator and the discriminator, we can theoretically prove that the optimal generator (the recommender model) can indeed lead to the equivalent performance distributions. To smooth the adversarial training process, we propose a novel dual curriculum learning strategy for optimal scheduling of training samples. Additionally, we tailor our framework to better suit top-N recommendation tasks by incorporating softened ranking metrics as measures of performance discrepancies. We conduct extensive experiments based on real-world datasets to demonstrate the effectiveness of our model.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",adversarial training; Distributional fairness; recommender system,Learning systems; Adversarial training; Distributional fairness; Expectation-variance; Group performance; Higher order moments; Numerical characteristics; Performance; Performance distribution; Performance gaps; User groups; Recommender systems
SPContrastNet: A Self-Paced Contrastive Learning Model for Few-Shot Text Classification,2024,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195070747&doi=10.1145%2f3652600&partnerID=40&md5=377c0d03514a7d586ffb691bba6f02fe,"Meta-learning has recently promoted few-shot text classification, which identifies target classes based on information transferred from source classes through a series of small tasks or episodes. Existing works constructing their meta-learner on Prototypical Networks need improvement in learning discriminative text representations between similar classes that may lead to conflicts in label prediction. The overfitting problems caused by a few training instances need to be adequately addressed. In addition, efficient episode sampling procedures that could enhance few-shot training should be utilized. To address the problems mentioned above, we first present a contrastive learning framework that simultaneously learns discriminative text representations via supervised contrastive learning while mitigating the overfitting problem via unsupervised contrastive regularization, and then we build an efficient self-paced episode sampling approach on top of it to include more difficult episodes as training progresses. Empirical results on eight few-shot text classification datasets show that our model outperforms the current state-of-the-art models. The extensive experimental analysis demonstrates that our supervised contrastive representation learning and unsupervised contrastive regularization techniques improve the performance of few-shot text classification. The episode-sampling analysis reveals that our self-paced sampling strategy improves training efficiency.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",contrastive learning; few-shot learning; self-paced learning; Text classification,Data mining; Learning systems; Text processing; Class-based; Contrastive learning; Few-shot learning; Learning models; Metalearning; Over fitting problem; Self-paced learning; Target class; Text classification; Text representation; Classification (of information)
Towards Effective and Efficient Sparse Neural Information Retrieval,2024,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195037207&doi=10.1145%2f3634912&partnerID=40&md5=0997c568fc35305f99e8347e635100e6,"Sparse representation learning based on Pre-trained Language Models has seen a growing interest in Information Retrieval. Such approaches can take advantage of the proven efficiency of inverted indexes and inherit desirable IR priors such as explicit lexical matching or some degree of interpretability. In this work, we thoroughly develop the framework of sparse representation learning in IR, which unifies term weighting and expansion in a supervised setting. We then build on SPLADE - a sparse expansion-based retriever - and show to which extent it is able to benefit from the same training improvements as dense bi-encoders by studying the effect of distillation, hard negative mining, as well as the Pre-trained Language Model's initialization on its effectiveness, leading to state-of-the-art results in both in- and out-of-domain evaluation settings (SPLADE++). We furthermore propose efficiency improvements, allowing us to reach latency requirements on par with traditional keyword-based approaches (Efficient-SPLADE).  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",effectiveness; efficiency; Information retrieval; sparse representations,Computational linguistics; Distillation; Expansion; Information retrieval; Petroleum reservoir evaluation; Effectiveness; Interpretability; Inverted indices; Language model; Lexical matching; Negative minings; Neural information; Sparse expansions; Sparse representation; Term weighting; Efficiency
Diversifying Sequential Recommendation with Retrospective and Prospective Transformers,2024,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195044918&doi=10.1145%2f3653016&partnerID=40&md5=84698a0b5729805ae7f001191a5a74fa,"Previous studies on sequential recommendation (SR) have predominantly concentrated on optimizing recommendation accuracy. However, there remains a significant gap in enhancing recommendation diversity, particularly for short interaction sequences. The limited availability of interaction information in short sequences hampers the recommender's ability to comprehensively model users' intents, consequently affecting both the diversity and accuracy of recommendation. In light of the above challenge, we propose reTrospective and pRospective Transformers for dIversified sEquential Recommendation (TRIER). The TRIER addresses the issue of insufficient information in short interaction sequences by first retrospectively learning to predict users' potential historical interactions, thereby introducing additional information and expanding short interaction sequences, and then capturing users' potential intents from multiple augmented sequences. Finally, the TRIER learns to generate diverse recommendation lists by covering as many potential intents as possible.To evaluate the effectiveness of TRIER, we conduct extensive experiments on three benchmark datasets. The experimental results demonstrate that TRIER significantly outperforms state-of-the-art methods, exhibiting diversity improvement of up to 11.36% in terms of intra-list distance (ILD@5) on the Steam dataset, 3.43% ILD@5 on the Yelp dataset and 3.77% in terms of category coverage (CC@5) on the Beauty dataset. As for accuracy, on the Yelp dataset, we observe notable improvement of 7.62% and 8.63% in HR@5 and NDCG@5, respectively. Moreover, we found that TRIER reveals more significant accuracy and diversity improvement for short interaction sequences.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",diverse; prospective transformer; Retrospective transformer; sequential recommendation,Diverse; Diversity improvement; Interaction information; Prospective transformer; Prospectives; Recommendation accuracy; Recommendation diversities; Retrospective transformer; Sequential recommendation; Short sequences
Teach and Explore: A Multiplex Information-guided Effective and Efficient Reinforcement Learning for Sequential Recommendation,2024,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195053350&doi=10.1145%2f3630003&partnerID=40&md5=f31235b7a160e2d8d788b8cff6882c2f,"Casting sequential recommendation (SR) as a reinforcement learning (RL) problem is promising and some RL-based methods have been proposed for SR. However, these models are sub-optimal due to the following limitations: (a) they fail to leverage the supervision signals in the RL training to capture users' explicit preferences, leading to slow convergence; and (b) they do not utilize auxiliary information (e.g., knowledge graph) to avoid blindness when exploring users' potential interests. To address the above-mentioned limitations, we propose a multiplex information-guided RL model (MELOD), which employs a novel RL training framework with Teach and Explore components for SR. We adopt a Teach component to accurately capture users' explicit preferences and speed up RL convergence. Meanwhile, we design a dynamic intent induction network (DIIN) as a policy function to generate diverse predictions. We utilize the DIIN for the Explore component to mine users' potential interests by conducting a sequential and knowledge information joint-guided exploration. Moreover, a sequential and knowledge-aware reward function is designed to achieve stable RL training. These components significantly improve MELOD's performance and convergence against existing RL algorithms to achieve effectiveness and efficiency. Experimental results on seven real-world datasets show that our model significantly outperforms state-of-the-art methods.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",explicit and potential interests; knowledge graph; reinforcement learning; Sequential recommendation,Knowledge graph; Knowledge management; Auxiliary information; Explicit and potential interest; Knowledge graphs; Learning problem; Learning-based methods; Reinforcement learning models; Reinforcement learnings; Sequential recommendation; Slow convergences; Training framework; Reinforcement learning
Retrieval for Extremely Long Queries and Documents with RPRS: A Highly Efficient and Effective Transformer-based Re-Ranker,2024,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195075151&doi=10.1145%2f3631938&partnerID=40&md5=b3c057198758336f0e52e1a62920dae9,"Retrieval with extremely long queries and documents is a well-known and challenging task in information retrieval and is commonly known as Query-by-Document (QBD) retrieval. Specifically designed Transformer models that can handle long input sequences have not shown high effectiveness in QBD tasks in previous work. We propose a Re-Ranker based on the novel Proportional Relevance Score (RPRS) to compute the relevance score between a query and the top-k candidate documents. Our extensive evaluation shows RPRS obtains significantly better results than the state-of-the-art models on five different datasets. Furthermore, RPRS is highly efficient, since all documents can be pre-processed, embedded, and indexed before query time that gives our re-ranker the advantage of having a complexity of O(N), where N is the total number of sentences in the query and candidate documents. Furthermore, our method solves the problem of the low-resource training in QBD retrieval tasks as it does not need large amounts of training data and has only three parameters with a limited range that can be optimized with a grid search even if a small amount of labeled data is available. Our detailed analysis shows that RPRS benefits from covering the full length of candidate documents and queries.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Neural information retrieval; Query-by-document retrieval \cdot Sentence-BERT based ranking,ART model; Document Retrieval; Input sequence; Long queries; Neural information; Neural information retrieval; Query-by-document retrieval \cdot sentence-BERT based ranking; Relevance score; State of the art; Transformer modeling; Information retrieval
Cooking with Conversation: Enhancing User Engagement and Learning with a Knowledge-Enhancing Assistant,2024,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195029904&doi=10.1145%2f3649500&partnerID=40&md5=d733ab6827b92b022e1466c5b36ea8b3,"We present two empirical studies to investigate users' expectations and behaviours when using digital assistants, such as Alexa and Google Home, in a kitchen context: First, a survey (N = 200) queries participants on their expectations for the kinds of information that such systems should be able to provide. While consensus exists on expecting information about cooking steps and processes, younger participants who enjoy cooking express a higher likelihood of expecting details on food history or the science of cooking. In a follow-up Wizard-of-Oz study (N = 48), users were guided through the steps of a recipe either by an active wizard that alerted participants to information it could provide or a passive wizard who only answered questions that were provided by the user. The active policy led to almost double the number of conversational utterances and 1.5 times more knowledge-related user questions compared to the passive policy. Also, it resulted in 1.7 times more knowledge communicated than the passive policy. We discuss the findings in the context of related work and reveal implications for the design and use of such assistants for cooking and other purposes such as DIY and craft tasks, as well as the lessons we learned for evaluating such systems.  © 2024 Copyright held by the owner/author(s).",Conversational agents; conversational search; interactive search; wizard-of-oz,Conversational agents; Conversational search; Digital assistants; Empirical studies; Google+; Interactive search; User behaviors; User engagement; User expectations; Wizard of Oz; Cooking
Discrete Federated Multi-behavior Recommendation for Privacy-Preserving Heterogeneous One-Class Collaborative Filtering,2024,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195097180&doi=10.1145%2f3652853&partnerID=40&md5=acecfd48b4ed48cd3edfe422fbd1fe2b,"Recently, federated recommendation has become a research hotspot mainly because of users' awareness of privacy in data. As a recent and important recommendation problem, in heterogeneous one-class collaborative filtering (HOCCF), each user may involve of two different types of implicit feedback, that is, examinations and purchases. So far, privacy-preserving HOCCF has received relatively little attention. Existing federated recommendation works often overlook the fact that some privacy sensitive behaviors such as purchases should be collected to ensure the basic business imperatives in e-commerce for example. Hence, the user privacy constraints can and should be relaxed while deploying a recommendation system in real scenarios. In this article, we study the federated multi-behavior recommendation problem under the assumption that purchase behaviors can be collected. Moreover, there are two additional challenges that need to be addressed when deploying federated recommendation. One is the low storage capacity for users' devices to store all the item vectors, and the other is the low computational power for users to participate in federated learning. To release the potential of privacy-preserving HOCCF, we propose a novel framework, named discrete federated multi-behavior recommendation (DFMR), which allows the collection of the business necessary behaviors (i.e., purchases) by the server. As to reduce the storage overhead, we use discrete hashing techniques, which can compress the parameters down to 1.56% of the real-valued parameters. To further improve the computation-efficiency, we design a memorization strategy in the cache updating module to accelerate the training process. Extensive experiments on four public datasets show the superiority of our DFMR in terms of both accuracy and efficiency.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",discrete hashing; Federated recommendation; heterogeneous implicit feedback,Digital storage; Efficiency; Privacy-preserving techniques; Sales; Discrete hashing; E- commerces; Federated recommendation; Heterogeneous implicit feedback; Hotspots; Implicit feedback; Low-storage; Privacy constraints; Privacy preserving; User privacy; Collaborative filtering
Data Augmentation for Sample Efficient and Robust Document Ranking,2024,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195026365&doi=10.1145%2f3634911&partnerID=40&md5=37375f1eb99bdd637ba8437a29ee8a86,"Contextual ranking models have delivered impressive performance improvements over classical models in the document ranking task. However, these highly over-parameterized models tend to be data-hungry and require large amounts of data even for fine-tuning. In this article, we propose data-augmentation methods for effective and robust ranking performance. One of the key benefits of using data augmentation is in achieving sample efficiency or learning effectively when we have only a small amount of training data. We propose supervised and unsupervised data augmentation schemes by creating training data using parts of the relevant documents in the query-document pairs. We then adapt a family of contrastive losses for the document ranking task that can exploit the augmented data to learn an effective ranking model. Our extensive experiments on subsets of the MS MARCO and TREC-DL test sets show that data augmentation, along with the ranking-adapted contrastive losses, results in performance improvements under most dataset sizes. Apart from sample efficiency, we conclusively show that data augmentation results in robust models when transferred to out-of-domain benchmarks. Our performance improvements in in-domain and more prominently in out-of-domain benchmarks show that augmentation regularizes the ranking model and improves its robustness and generalization capability.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",contrastive loss; data augmentation; document ranking; Information retrieval; interpolation; IR; ranking; ranking performance,Benchmarking; Information retrieval; Search engines; Statistical tests; Classical modeling; Contrastive loss; Data augmentation; Document ranking; IR; Performance; Ranking; Ranking model; Ranking performance; Training data; Efficiency
Generalized Weak Supervision for Neural Information Retrieval,2024,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195075735&doi=10.1145%2f3647639&partnerID=40&md5=23ec89a5ed0653b16086d006dfdac2e7,"Neural ranking models (NRMs) have demonstrated effective performance in several information retrieval (IR) tasks. However, training NRMs often requires large-scale training data, which is difficult and expensive to obtain. To address this issue, one can train NRMs via weak supervision, where a large dataset is automatically generated using an existing ranking model (called the weak labeler) for training NRMs. Weakly supervised NRMs can generalize from the observed data and significantly outperform the weak labeler. This paper generalizes this idea through an iterative re-labeling process, demonstrating that weakly supervised models can iteratively play the role of weak labeler and significantly improve ranking performance without using manually labeled data. The proposed Generalized Weak Supervision (GWS) solution is generic and orthogonal to the ranking model architecture. This paper offers four implementations of GWS: self-labeling, cross-labeling, joint cross- and self-labeling, and greedy multi-labeling. GWS also benefits from a query importance weighting mechanism based on query performance prediction methods to reduce noise in the generated training data. We further draw a theoretical connection between self-labeling and Expectation-Maximization. Our experiments on four retrieval benchmarks suggest that our implementations of GWS lead to substantial improvements compared to weak supervision if the weak labeler is sufficiently reliable.  © 2024 Copyright held by the owner/author(s).",distant supervision; neural ranking models; unsupervised learning; Weak supervision; zero-shot learning,Information retrieval; Large datasets; Learning systems; Maximum principle; Zero-shot learning; Distant supervision; Effective performance; Labelings; Large datasets; Large-scales; Neural information; Neural ranking model; Ranking model; Training data; Weak supervision; Iterative methods
Towards Unified Representation Learning for Career Mobility Analysis with Trajectory Hypergraph,2024,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193499703&doi=10.1145%2f3651158&partnerID=40&md5=bcaff5416899e0c562acf95ed337627a,"Career mobility analysis aims at understanding the occupational movement patterns of talents across distinct labor market entities, which enables a wide range of talent-centered applications, such as job recommendation, labor demand forecasting, and company competitive analysis. Existing studies in this field mainly focus on a single fixed scale, investigating either individual trajectories at the micro-level or crowd flows among market entities at the macro-level. Consequently, the intrinsic cross-scale interactions between talents and the labor market are largely overlooked. To bridge this gap, we propose UniTRep, a novel unified representation learning framework for cross-scale career mobility analysis. Specifically, we first introduce a trajectory hypergraph structure to organize the career mobility patterns in a low-information-loss manner, where market entities and talent trajectories are represented as nodes and hyperedges, respectively. Then, for learning the market-aware talent representations, we attentively propagate the node information to the hyperedges and incorporate the market contextual features into the process of individual trajectory modeling. For learning the trajectory-enhanced market representations, we aggregate the message from hyperedges associated with a specific node to integrate the fine-grained semantics of trajectories into labor market modeling. Moreover, we design two auxiliary tasks to optimize both intra-scale and cross-scale learning with a self-supervised strategy. Extensive experiments on a real-world dataset clearly validate that UniTRep can significantly outperform state-of-the-art baselines for various tasks.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesCareer mobility; graph neural networks; representation learning,Commerce; Employment; Graph neural networks; Semantics; Additional key word and phrasescareer mobility; Career mobilities; Graph neural networks; Hyper graph; Hyperedges; Key words; Labour market; Mobility analysis; Movement pattern; Representation learning; Trajectories
Improving Semi-Supervised Text Classification with Dual Meta-Learning,2024,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193467603&doi=10.1145%2f3648612&partnerID=40&md5=f341c8dcccbb5b38ac738983be2296f3,"The goal of semi-supervised text classification (SSTC) is to train a model by exploring both a small number of labeled data and a large number of unlabeled data, such that the learned semi-supervised classifier performs better than the supervised classifier trained on solely the labeled samples. Pseudo-labeling is one of the most widely used SSTC techniques, which trains a teacher classifier with a small number of labeled examples to predict pseudo labels for the unlabeled data. The generated pseudo-labeled examples are then utilized to train a student classifier, such that the learned student classifier can outperform the teacher classifier. Nevertheless, the predicted pseudo labels may be inaccurate, making the performance of the student classifier degraded. The student classifier may perform even worse than the teacher classifier. To alleviate this issue, in this paper, we introduce a dual meta-learning (DML) technique for semi-supervised text classification, which improves the teacher and student classifiers simultaneously in an iterative manner. Specifically, we propose a meta-noise correction method to improve the student classifier by proposing a Noise Transition Matrix (NTM) with meta-learning to rectify the noisy pseudo labels. In addition, we devise a meta pseudo supervision method to improve the teacher classifier. Concretely, we exploit the feedback performance from the student classifier to further guide the teacher classifier to produce more accurate pseudo labels for the unlabeled data. In this way, both teacher and student classifiers can co-evolve in the iterative training process. Extensive experiments on four benchmark datasets highlight the effectiveness of our DML method against existing state-of-the-art methods for semi-supervised text classification. We release our code and data of this paper publicly at https://github.com/GRIT621/DML.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesSemi-supervised text classification; consistency regularization; meta learning; noise transition matrix; pseudo labeling,Classification (of information); Iterative methods; Learning systems; Matrix algebra; Personnel training; Supervised learning; Text processing; Additional key word and phrasessemi-supervised text classification; Consistency regularization; Key words; Labelings; Metalearning; Noise transition matrix; Pseudo labeling; Regularisation; Text classification; Transition matrices; Students
Filter-based Stance Network for Rumor Verification,2024,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193499294&doi=10.1145%2f3649462&partnerID=40&md5=eb04d596fee713aa587ea522669349a4,"Rumor verification on social media aims to identify the truth value of a rumor, which is important to decrease the detrimental public effects. A rumor might arouse heated discussions and replies, conveying different stances of users that could be helpful in identifying the rumor. Thus, several works have been proposed to verify a rumor by modelling its entire stance sequence in the time domain. However, these works ignore that such a stance sequence could be decomposed into controversies with different intensities, which could be used to cluster the stance sequences with the same consensus. In addition, the existing stance extractors fail to consider both the impact of all previously posted tweets and the reply chain on obtaining the stance of a new reply. To address the above problems, in this article, we propose a novel stance-based network to aggregate the controversies of the stance sequence for rumor verification, termed Filter-based Stance Network (FSNet). As controversies with different intensities are reflected as the different changes of stances, it is convenient to represent different controversies in the frequency domain, but it is hard in the time domain. Our proposed FSNet decomposes the stance sequence into multiple controversies in the frequency domain and obtains the weighted aggregation of them. Specifically, FSNet consists of two modules: the stance extractor and the filter block. To obtain better stance features toward the source, the stance extractor contains two stages. In the first stage, the tweet representation of each reply is obtained by aggregating information from all previously posted tweets in a conversation. Then, the features of stance toward the source, i.e., rumor-aware stance, are extracted with the reply chains in the second stage. In the filter block module, a rumor-aware stance sequence is constructed by sorting all the tweets of a conversation in chronological order. Fourier Transform thereafter is employed to convert the stance sequence into the frequency domain, where different frequency components reflect controversies of different intensities. Finally, a frequency filter is applied to explore the different contributions of controversies. We supervise our FSNet with both stance labels and rumor labels to strengthen the relations between rumor veracity and crowd stances. Extensive experiments on two benchmark datasets demonstrate that our model substantially outperforms all the baselines.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesSocial media; frequency filter; rumor verification; rumor-aware stance,Conveying; Additional key word and phrasessocial medium; Filter-based; Frequency domains; Frequency filters; Key words; Rumor verification; Rumor-aware stance; Social media; Time domain; Truth values; Frequency domain analysis
Few-shot Learning for Heterogeneous Information Networks,2024,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193492132&doi=10.1145%2f3649311&partnerID=40&md5=6a07a99c3bfc49323956814e2fe78e93,"Heterogeneous information networks (HINs) are a key resource in many domain-specific retrieval and recommendation scenarios and in conversational environments. Current approaches to mining graph data often rely on abundant supervised information. However, supervised signals for graph learning tend to be scarce for a new task and only a handful of labeled nodes may be available. Meta-learning mechanisms are able to harness prior knowledge that can be adapted to new tasks. In this article, we design meta-learning framework for heterogeneous information networks (META-HIN), for few-shot learning problems on HINs. To the best of our knowledge, we are among the first to design a unified framework to realize the few-shot learning of HINs and facilitate different downstream tasks across different domains of graphs. Unlike most previous models, which focus on a single task on a single graph, META-HIN is able to deal with different tasks (node classification, link prediction, and anomaly detection are used as examples) across multiple graphs. Subgraphs are sampled to build the support and query set. Before being processed by the meta-learning module, subgraphs are modeled via a structure module to capture structural features. Then, a heterogeneous Graph Neural Network module is used as the base model to express the features of subgraphs. We also design a Generative Adversarial Network-based contrastive learning module that is able to exploit unsupervised information of the subgraphs. In our experiments, we fuse several datasets from multiple domains to verify META-HIN's broad applicability in a multiple-graph scenario. META-HIN consistently and significantly outperforms state-of-the-art alternatives on every task and across all datasets that we consider.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesHeterogeneous information network; few-shot learning; graph mining; meta-learning,Anomaly detection; Data mining; Generative adversarial networks; Graph theory; Information services; Additional key word and phrasesheterogeneous information network; Few-shot learning; Graph mining; Heterogeneous information; Information networks; Key words; Learning modules; Meta-learning frameworks; Metalearning; Subgraphs; Graph neural networks
Efficient Neural Ranking Using Forward Indexes and Lightweight Encoders,2024,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178950416&doi=10.1145%2f3631939&partnerID=40&md5=c3495572b59e5c81196c797662cd17d2,"Dual-encoder-based dense retrieval models have become the standard in IR. They employ large Transformer-based language models, which are notoriously inefficient in terms of resources and latency.We propose Fast-Forward indexes - vector forward indexes which exploit the semantic matching capabilities of dual-encoder models for efficient and effective re-ranking. Our framework enables re-ranking at very high retrieval depths and combines the merits of both lexical and semantic matching via score interpolation. Furthermore, in order to mitigate the limitations of dual-encoders, we tackle two main challenges: Firstly, we improve computational efficiency by either pre-computing representations, avoiding unnecessary computations altogether, or reducing the complexity of encoders. This allows us to considerably improve ranking efficiency and latency. Secondly, we optimize the memory footprint and maintenance cost of indexes; we propose two complementary techniques to reduce the index size and show that, by dynamically dropping irrelevant document tokens, the index maintenance efficiency can be improved substantially.We perform an evaluation to show the effectiveness and efficiency of Fast-Forward indexes - our method has low latency and achieves competitive results without the need for hardware acceleration, such as GPUs.  © 2024 Copyright held by the owner/author(s).",dual-encoders; efficiency; Information retrieval; IR; latency; ranking,Computer hardware; Information retrieval; Maintenance; Program processors; Semantics; Signal encoding; Dual-encoder; Fast forward; IR; Language model; Latency; Lexical matching; Ranking; Re-ranking; Retrieval models; Semantic matching; Computational efficiency
Cross-Model Comparative Loss for Enhancing Neuronal Utility in Language Understanding,2024,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195077272&doi=10.1145%2f3652599&partnerID=40&md5=159e20bcd6a77b256d57c05dddccbe4b,"Current natural language understanding (NLU) models have been continuously scaling up, both in terms of model size and input context, introducing more hidden and input neurons. While this generally improves performance on average, the extra neurons do not yield a consistent improvement for all instances. This is because some hidden neurons are redundant, and the noise mixed in input neurons tends to distract the model. Previous work mainly focuses on extrinsically reducing low-utility neurons by additional post- or pre-processing, such as network pruning and context selection, to avoid this problem. Beyond that, can we make the model reduce redundant parameters and suppress input noise by intrinsically enhancing the utility of each neuron? If a model can efficiently utilize neurons, no matter which neurons are ablated (disabled), the ablated submodel should perform no better than the original full model. Based on such a comparison principle between models, we propose a cross-model comparative loss for a broad range of tasks. Comparative loss is essentially a ranking loss on top of the task-specific losses of the full and ablated models, with the expectation that the task-specific loss of the full model is minimal. We demonstrate the universal effectiveness of comparative loss through extensive experiments on 14 datasets from three distinct NLU tasks based on five widely used pre-trained language models and find it particularly superior for models with few parameters or long input.  © 2024 Copyright held by the owner/author(s).",loss function; Natural language understanding; pseudo-relevance feedback; question answering,Natural language processing systems; 'current; Cross model; Full model; Hidden neurons; Input neurons; Language understanding; Loss functions; Natural language understanding; Pseudo-relevance feedbacks; Question Answering; Neurons
An Analysis on Matching Mechanisms and Token Pruning for Late-interaction Models,2024,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85188264331&doi=10.1145%2f3639818&partnerID=40&md5=627c50fcbd589d001c098573d635adaa,"With the development of pre-trained language models, the dense retrieval models have become promising alternatives to the traditional retrieval models that rely on exact match and sparse bag-of-words representations. Different from most dense retrieval models using a bi-encoder to encode each query or document into a dense vector, the recently proposed late-interaction multi-vector models (i.e., ColBERT and COIL) achieve state-of-the-art retrieval effectiveness by using all token embeddings to represent documents and queries and modeling their relevance with a sum-of-max operation. However, these fine-grained representations may cause unacceptable storage overhead for practical search systems. In this study, we systematically analyze the matching mechanism of these late-interaction models and show that the sum-of-max operation heavily relies on the co-occurrence signals and some important words in the document. Based on these findings, we then propose several simple document pruning methods to reduce the storage overhead and compare the effectiveness of different pruning methods on different late-interaction models. We also leverage query pruning methods to further reduce the retrieval latency. We conduct extensive experiments on both in-domain and out-domain datasets and show that some of the used pruning methods can significantly improve the efficiency of these late-interaction models without substantially hurting their retrieval effectiveness.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",efficiency optimization; late-interaction models; Neural networks; pre-trained language model; token pruning,Computational linguistics; Information retrieval; Modeling languages; Efficiency optimization; Interaction modeling; Language model; Late-interaction model; Matching mechanisms; Neural-networks; Pre-trained language model; Pruning methods; Retrieval models; Token pruning; Efficiency
Target-constrained Bidirectional Planning for Generation of Target-oriented Proactive Dialogue,2024,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195100261&doi=10.1145%2f3652598&partnerID=40&md5=54c083d437f85b7a4efcf72fb4ce64d1,"Target-oriented proactive dialogue systems aim at leading conversations from a dialogue context toward a pre-determined target, such as making recommendations on designated items or introducing new specific topics. To this end, it is critical for such dialogue systems to plan reasonable actions to drive the conversation proactively, and meanwhile, to plan appropriate topics to move the conversation forward to the target topic smoothly. In this work, we mainly focus on effective dialogue planning for target-oriented dialogue generation. Inspired by decision-making theories in cognitive science, we propose a novel target-constrained bidirectional planning (TRIP) approach, which plans an appropriate dialogue path by looking ahead and looking back. By formulating the planning as a generation task, our TRIP bidirectionally generates a dialogue path consisting of a sequence of <action, topic> pairs using two Transformer decoders. They are expected to supervise each other and converge on consistent actions and topics by minimizing the decision gap and contrastive generation of targets. Moreover, we propose a target-constrained decoding algorithm with a bidirectional agreement to better control the planning process. Subsequently, we adopt the planned dialogue paths to guide dialogue generation in a pipeline manner, where we explore two variants: prompt-based generation and plan-controlled generation. Extensive experiments are conducted on two challenging dialogue datasets, which are re-purposed for exploring target-oriented dialogue. Our automatic and human evaluations demonstrate that the proposed methods significantly outperform various baseline models.  © 2024 Copyright held by the owner/author(s).",bidirectional planning; dialogue generation; Target-oriented dialogue,Decision making; Speech processing; Bidirectional planning; Cognitive science; Constrained decoding; Decision-making theories; Dialog planning; Dialogue generations; Dialogue systems; Sequence of actions; Target oriented; Target-oriented dialog; Decoding
Multi-grained Document Modeling for Search Result Diversification,2024,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195028718&doi=10.1145%2f3652852&partnerID=40&md5=dff358fdc2f6bc934bddcb3fec58c2c7,"Search result diversification plays a crucial role in improving users' search experience by providing users with documents covering more subtopics. Previous studies have made great progress in leveraging inter-document interactions to measure the similarity among documents. However, different parts of the document may embody different subtopics and existing models ignore the subtle similarities and differences of content within each document. In this article, we propose a hierarchical attention framework to combine intra-document interactions with inter-document interactions in a complementary manner in order to conduct multi-grained document modeling. Specifically, we separate the document into passages to model the document content from multi-grained perspectives. Then, we design stacked interaction blocks to conduct inter-document and intra-document interactions. Moreover, to measure the subtopic coverage of each document more accurately, we propose a passage-aware document-subtopic interaction to perform fine-grained document-subtopic interaction. Experimental results demonstrate that our model achieves state-of-the-art performance compared with existing methods.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Intra-document relations; multi-grained document modeling; search result diversification,Document contents; Document modeling; Document relations; Fine grained; Intra-document relation; Multi-grained document modeling; Search results diversifications; State-of-the-art performance; Information retrieval
Invisible Black-Box Backdoor Attack against Deep Cross-Modal Hashing Retrieval,2024,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193471230&doi=10.1145%2f3650205&partnerID=40&md5=5dd2c55011a436364650d7ea1959da0c,"Deep cross-modal hashing has promoted the field of multi-modal retrieval due to its excellent efficiency and storage, but its vulnerability to backdoor attacks is rarely studied. Notably, current deep cross-modal hashing methods inevitably require large-scale training data, resulting in poisoned samples with imperceptible triggers that can easily be camouflaged into the training data to bury backdoors in the victim model. Nevertheless, existing backdoor attacks focus on the uni-modal vision domain, while the multi-modal gap and hash quantization weaken their attack performance. In addressing the aforementioned challenges, we undertake an invisible black-box backdoor attack against deep cross-modal hashing retrieval in this article. To the best of our knowledge, this is the first attempt in this research field. Specifically, we develop a flexible trigger generator to generate the attacker's specified triggers, which learns the sample semantics of the non-poisoned modality to bridge the cross-modal attack gap. Then, we devise an input-aware injection network, which embeds the generated triggers into benign samples in the form of sample-specific stealth and realizes cross-modal semantic interaction between triggers and poisoned samples. Owing to the knowledge-agnostic of victim models, we enable any cross-modal hashing knockoff to facilitate the black-box backdoor attack and alleviate the attack weakening of hash quantization. Moreover, we propose a confusing perturbation and mask strategy to induce the high-performance victim models to focus on imperceptible triggers in poisoned samples. Extensive experiments on benchmark datasets demonstrate that our method has a state-of-the-art attack performance against deep cross-modal hashing retrieval. Besides, we investigate the influences of transferable attacks, few-shot poisoning, multi-modal poisoning, perceptibility, and potential defenses on backdoor attacks. Our codes and datasets are available at https://github.com/tswang0116/IB3A.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesBackdoor attack; black-box attack; deep cross-modal hashing retrieval; imperceptible trigger,Benchmarking; Digital storage; Additional key word and phrasesbackdoor attack; Backdoors; Black boxes; Black-box attack; Cross-modal; Deep cross-modal hashing retrieval; Imperceptible trigger; Key words; Multi-modal; Performance; Semantics
ELAKT: Enhancing Locality for Attentive Knowledge Tracing,2024,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193526605&doi=10.1145%2f3652601&partnerID=40&md5=6ecfaa047c840db40785ab65645fa456,"Knowledge tracing models based on deep learning can achieve impressive predictive performance by leveraging attention mechanisms. However, there still exist two challenges in attentive knowledge tracing (AKT): First, the mechanism of classical models of AKT demonstrates relatively low attention when processing exercise sequences with shifting knowledge concepts (KC), making it difficult to capture the comprehensive state of knowledge across sequences. Second, classical models do not consider stochastic behaviors, which negatively affects models of AKT in terms of capturing anomalous knowledge states. This article proposes a model of AKT, called Enhancing Locality for Attentive Knowledge Tracing (ELAKT), that is a variant of the deep KT model. The proposed model leverages the encoder module of the transformer to aggregate knowledge embedding generated by both exercises and responses over all timesteps. In addition, it uses causal convolutions to aggregate and smooth the states of local knowledge. The ELAKT model uses the states of comprehensive KCs to introduce a prediction correction module to forecast the future responses of students to deal with noise caused by stochastic behaviors. The results of experiments demonstrated that the ELAKT model consistently outperforms state-of-the-art baseline KT models.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesKnowledge tracing; causal convolution; knowledge aggregation; self-attention,Convolution; Deep learning; Knowledge management; Stochastic models; Stochastic systems; Additional key word and phrasesknowledge tracing; Causal convolutions; Classical modeling; Key words; Knowledge aggregation; Knowledge tracings; Model-based OPC; Self-attention; Stochastic behavior; Tracing model; Aggregates
Deep Coupling Network for Multivariate Time Series Forecasting,2024,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192814412&doi=10.1145%2f3653447&partnerID=40&md5=e69882df01b12063dba06a156c1cc32d,"Multivariate time series (MTS) forecasting is crucial in many real-world applications. To achieve accurate MTS forecasting, it is essential to simultaneously consider both intra- and inter-series relationships among time series data. However, previous work has typically modeled intra- and inter-series relationships separately and has disregarded multi-order interactions present within and between time series data, which can seriously degrade forecasting accuracy. In this article, we reexamine intra- and inter-series relationships from the perspective of mutual information and accordingly construct a comprehensive relationship learning mechanism tailored to simultaneously capture the intricate multi-order intra- and inter-series couplings. Based on the mechanism, we propose a novel deep coupling network for MTS forecasting, named DeepCN, which consists of a coupling mechanism dedicated to explicitly exploring the multi-order intra- and inter-series relationships among time series data concurrently, a coupled variable representation module aimed at encoding diverse variable patterns, and an inference module facilitating predictions through one forward step. Extensive experiments conducted on seven real-world datasets demonstrate that our proposed DeepCN achieves superior performance compared with the state-of-the-art baselines.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",deep coupling network; Multivariate time series forecasting; mutual information,Time series; Coupling network; Deep coupling network; Forecasting accuracy; Multi-ordering; Multivariate time series; Multivariate time series forecasting; Mutual informations; Real-world; Time series forecasting; Time-series data; Forecasting
MultiCBR: Multi-view Contrastive Learning for Bundle Recommendation,2024,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193461738&doi=10.1145%2f3640810&partnerID=40&md5=d7b715ba195506cda55ffd56c42406f8,"Bundle recommendation seeks to recommend a bundle of related items to users to improve both user experience and the profits of platform. Existing bundle recommendation models have progressed from capturing only user-bundle interactions to the modeling of multiple relations among users, bundles, and items. CrossCBR, in particular, incorporates cross-view contrastive learning into a two-view preference learning framework, significantly improving SOTA performance. It does, however, have two limitations: (1) the two-view formulation does not fully exploit all the heterogeneous relations among users, bundles, and items; and (2) the ""early contrast and late fusion""framework is less effective in capturing user preference and difficult to generalize to multiple views.In this article, we present MultiCBR, a novel Multi-view Contrastive learning framework for Bundle Recommendation. First, we devise a multi-view representation learning framework capable of capturing all the user-bundle, user-item, and bundle-item relations, especially better utilizing the bundle-item affiliations to enhance sparse bundles' representations. Second, we innovatively adopt an ""early fusion and late contrast""design that first fuses the multi-view representations before performing self-supervised contrastive learning. In comparison to existing approaches, our framework reverses the order of fusion and contrast, introducing the following advantages: (1) Our framework is capable of modeling both cross-view and ego-view preferences, allowing us to achieve enhanced user preference modeling; and (2) instead of requiring quadratic number of cross-view contrastive losses, we only require two self-supervised contrastive losses, resulting in minimal extra costs. Experimental results on three public datasets indicate that our method outperforms SOTA methods. The code and dataset can be found in the github repo https://github.com/HappyPointer/MultiCBR. © 2024 Copyright held by the owner/author(s).",Additional Key Words and PhrasesBundle recommendation; contrastive learning; graph neural network,Data mining; User profile; Additional key word and phrasesbundle recommendation; Contrastive learning; Graph neural networks; Key words; Learning frameworks; Multi-views; Performance; Preference learning; Two views; Users' experiences; Graph neural networks
Token-Event-Role Structure-Based Multi-Channel Document-Level Event Extraction,2024,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193020283&doi=10.1145%2f3643885&partnerID=40&md5=2dc46d3688f3d7e12c7544cd82279aac,"Document-level event extraction is a long-standing challenging information retrieval problem involving a sequence of sub-tasks: entity extraction, event type judgment, and event type-specific multi-event extraction. However, addressing the problem as multiple learning tasks leads to increased model complexity. Also, existing methods insufficiently utilize the correlation of entities crossing different events, resulting in limited event extraction performance. This article introduces a novel framework for document-level event extraction, incorporating a new data structure called token-event-role and a multi-channel argument role prediction module. The proposed data structure enables our model to uncover the primary role of tokens in multiple events, facilitating a more comprehensive understanding of event relationships. By leveraging the multi-channel prediction module, we transform entity and multi-event extraction into a single task of predicting token-event pairs, thereby reducing the overall parameter size and enhancing model efficiency. The results demonstrate that our approach outperforms the state-of-the-art method by 9.5 percentage points in terms of the F1 score, highlighting its superior performance in event extraction. Furthermore, an ablation study confirms the significant value of the proposed data structure in improving event extraction tasks, further validating its importance in enhancing the overall performance of the framework.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesDocument-level event extraction; joint learning; multi-channel; neural network; token-event-role data structure,Data mining; Extraction; Forecasting; Learning systems; Additional key word and phrasesdocument-level event extraction; Event Types; Events extractions; Joint learning; Key words; Multi channel; Neural-networks; Performance; Structure-based; Token-event-role data structure; Data structures
On the Effectiveness of Sampled Softmax Loss for Item Recommendation,2024,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185814551&doi=10.1145%2f3637061&partnerID=40&md5=498624ff43d509075fb8c3d296f6ec39,"The learning objective plays a fundamental role to build a recommender system. Most methods routinely adopt either pointwise (e.g., binary cross-entropy) or pairwise (e.g., BPR) loss to train the model parameters, while rarely pay attention to softmax loss, which assumes the probabilities of all classes sum up to 1, due to its computational complexity when scaling up to large datasets or intractability for streaming data where the complete item space is not always available. The sampled softmax (SSM) loss emerges as an efficient substitute for softmax loss. Its special case, InfoNCE loss, has been widely used in self-supervised learning and exhibited remarkable performance for contrastive learning. Nonetheless, limited recommendation work uses the SSM loss as the learning objective. Worse still, none of them explores its properties thoroughly and answers ""Does SSM loss suit for item recommendation?""and ""What are the conceptual advantages of SSM loss, as compared with the prevalent losses?"", to the best of our knowledge.In this work, we aim at offering a better understanding of SSM for item recommendation. Specifically, we first theoretically reveal three model-agnostic advantages: (1) mitigating popularity bias, which is beneficial to long-tail recommendation; (2) mining hard negative samples, which offers informative gradients to optimize model parameters; and (3) maximizing the ranking metric, which facilitates top-K performance. However, based on our empirical studies, we recognize that the default choice of cosine similarity function in SSM limits its ability in learning the magnitudes of representation vectors. As such, the combinations of SSM with the models that also fall short in adjusting magnitudes (e.g., matrix factorization) may result in poor representations. One step further, we provide mathematical proof that message passing schemes in graph convolution networks can adjust representation magnitude according to node degree, which naturally compensates for the shortcoming of SSM. Extensive experiments on four benchmark datasets justify our analyses, demonstrating the superiority of SSM for item recommendation. Our implementations are available in both TensorFlow1 and PyTorch.2. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesSampled softmax loss; collaborative filtering; graph neural networks; long-tail recommendation,Data mining; Graph neural networks; Large datasets; Learning systems; Matrix factorization; Message passing; Additional key word and phrasessampled softmax loss; Cross entropy; Graph neural networks; Key words; Learning objectives; Long tail; Long-tail recommendation; Modeling parameters; Performance; Point wise; Collaborative filtering
Counterfactual Explanation for Fairness in Recommendation,2024,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193501443&doi=10.1145%2f3643670&partnerID=40&md5=e094f72ec8cf15fc166f03104a9953f2,"Fairness-aware recommendation alleviates discrimination issues to build trustworthy recommendation systems. Explaining the causes of unfair recommendations is critical, as it promotes fairness diagnostics, and thus secures users' trust in recommendation models. Existing fairness explanation methods suffer high computation burdens due to the large-scale search space and the greedy nature of the explanation search process. Besides, they perform feature-level optimizations with continuous values, which are not applicable to discrete attributes such as gender and age. In this work, we adopt counterfactual explanations from causal inference and propose to generate attribute-level counterfactual explanations, adapting to discrete attributes in recommendation models. We use real-world attributes from Heterogeneous Information Networks (HINs) to empower counterfactual reasoning on discrete attributes. We propose a Counterfactual Explanation for Fairness (CFairER) that generates attribute-level counterfactual explanations from HINs for item exposure fairness. Our CFairER conducts off-policy reinforcement learning to seek high-quality counterfactual explanations, with attentive action pruning reducing the search space of candidate counterfactuals. The counterfactual explanations help to provide rational and proximate explanations for model fairness, while the attentive action pruning narrows the search space of attributes. Extensive experiments demonstrate our proposed model can generate faithful explanations while maintaining favorable recommendation performance.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesExplainable recommendation; counterfactual explanation; fairness; reinforcement learning,Information services; Recommender systems; Additional key word and phrasesexplainable recommendation; Attribute levels; Counterfactual explanation; Counterfactuals; Discrete attributes; Fairness; Heterogeneous information; Key words; Reinforcement learnings; Search spaces; Reinforcement learning
Transferring Causal Mechanism over Meta-representations for Target-Unknown Cross-domain Recommendation,2024,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85188734832&doi=10.1145%2f3643807&partnerID=40&md5=09b82311c237f949062b0fa870e8c82c,"Tackling the pervasive issue of data sparsity in recommender systems, we present an insightful investigation into the burgeoning area of non-overlapping cross-domain recommendation, a technique that facilitates the transfer of interaction knowledge across domains without necessitating inter-domain user/item correspondence. Existing approaches have predominantly depended on auxiliary information, such as user reviews and item tags, to establish inter-domain connectivity, but these resources may become inaccessible due to privacy and commercial constraints.To address these limitations, our study introduces an in-depth exploration of Target-unknown Cross-domain Recommendation (CDR), which contends with the distinct challenge of lacking target domain information during the training phase in the source domain. We illustrate two critical obstacles inherent to Target-unknown CDR: the lack of an inter-domain bridge due to insufficient user/item correspondence or side information and the potential pitfalls of source-domain training biases when confronting distribution shifts across domains. To surmount these obstacles, we propose the CMCDR framework, a novel approach that leverages causal mechanisms extracted from meta-user/item representations. The CMCDR framework employs a vector-quantized encoder-decoder architecture, enabling the disentanglement of user/item characteristics. We posit that domain-transferable knowledge is more readily discernible from user/item characteristics, i.e., the meta-representations, rather than raw users and items. Capitalizing on these meta-representations, our CMCDR framework adeptly incorporates an attention-driven predictor that approximates the front-door adjustment method grounded in causal theory. This cutting-edge strategy effectively mitigates source-domain training biases and enhances generalization capabilities against distribution shifts. Extensive experiments demonstrate the empirical effectiveness and the rationality of CMCDR for target-unknown cross-domain recommendation.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesTarget-unknown cross-domain recommendation; front-door adjustment; vector quantization,Vector quantization; Additional key word and phrasestarget-unknown cross-domain recommendation; Auxiliary information; Cross-domain recommendations; Data sparsity; Front-door adjustment; Inter-domain; Key words; Target domain; User reviews; Vector quantisation; Clock and data recovery circuits (CDR circuits)
Should Fairness be a Metric or a Model? A Model-based Framework for Assessing Bias in Machine Learning Pipelines,2024,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189636976&doi=10.1145%2f3641276&partnerID=40&md5=d9c2edcfe0dfdef4fddb7cb53ae80f2b,"Fairness measurement is crucial for assessing algorithmic bias in various types of machine learning (ML) models, including ones used for search relevance, recommendation, personalization, talent analytics, and natural language processing. However, the fairness measurement paradigm is currently dominated by fairness metrics that examine disparities in allocation and/or prediction error as univariate key performance indicators (KPIs) for a protected attribute or group. Although important and effective in assessing ML bias in certain contexts such as recidivism, existing metrics don't work well in many real-world applications of ML characterized by imperfect models applied to an array of instances encompassing a multivariate mixture of protected attributes, that are part of a broader process pipeline. Consequently, the upstream representational harm quantified by existing metrics based on how the model represents protected groups doesn't necessarily relate to allocational harm in the application of such models in downstream policy/decision contexts. We propose FAIR-Frame, a model-based framework for parsimoniously modeling fairness across multiple protected attributes in regard to the representational and allocational harm associated with the upstream design/development and downstream usage of ML models. We evaluate the efficacy of our proposed framework on two testbeds pertaining to text classification using pretrained language models. The upstream testbeds encompass over fifty thousand documents associated with twenty-eight thousand users, seven protected attributes and five different classification tasks. The downstream testbeds span three policy outcomes and over 5.41 million total observations. Results in comparison with several existing metrics show that the upstream representational harm measures produced by FAIR-Frame and other metrics are significantly different from one another, and that FAIR-Frame's representational fairness measures have the highest percentage alignment and lowest error with allocational harm observed in downstream applications. Our findings have important implications for various ML contexts, including information retrieval, user modeling, digital platforms, and text classification, where responsible and trustworthy AI is becoming an imperative.  © 2024 Copyright held by the owner/author(s).",Additional Key Words and PhrasesMachine learning fairness; AI governance; algorithmic bias; machine learning pipelines; model framework; prediction and explanation,Benchmarking; Classification (of information); Information retrieval systems; Natural language processing systems; Supervised learning; Testbeds; Additional key word and phrasesmachine learning fairness; AI governance; Algorithmic bias; Algorithmics; Down-stream; Key words; Machine learning pipeline; Machine-learning; Modelling framework; Prediction and explanation; Pipelines
MCN4Rec: Multi-level Collaborative Neural Network for Next Location Recommendation,2024,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193493010&doi=10.1145%2f3643669&partnerID=40&md5=94aea3001c470c58a29c0ff3b1f59ee5,"Next location recommendation plays an important role in various location-based services, yielding great value for both users and service providers. Existing methods usually model temporal dependencies with explicit time intervals or learn representation from customized point of interest (POI) graphs with rich context information to capture the sequential patterns among POIs. However, this problem is perceptibly complex, because various factors, e.g., users' preferences, spatial locations, time contexts, activity category semantics, and temporal relations, need to be considered together, while most studies lack sufficient consideration of the collaborative signals. Toward this goal, we propose a novel Multi-Level Collaborative Neural Network for next location Recommendation (MCN4Rec). Specifically, we design a multi-level view representation learning with level-wise contrastive learning to collaboratively learn representation from local and global perspectives to capture complex heterogeneous relationships among user, POI, time, and activity categories. Then, a causal encoder-decoder is applied to the learned representations of check-in sequences to recommend the next location. Extensive experiments on four real-world check-in mobility datasets demonstrate that our model significantly outperforms the existing state-of-the-art baselines for the next location recommendation. Ablation study further validates the benefits of the collaboration of the designed sub-modules. The source code is available at https://github.com/quai-mengxiang/MCN4Rec.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesNext location recommendation; collaborative learning; human mobility; representation learning,Complex networks; Location based services; Semantics; Telecommunication services; Additional key word and phrasesnext location recommendation; Check-in; Collaborative learning; Human mobility; Key words; Learn+; Location-based services; Multilevels; Neural-networks; Representation learning; Location
Can Perturbations Help Reduce Investment Risks? Risk-aware Stock Recommendation via Split Variational Adversarial Training,2024,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193552150&doi=10.1145%2f3643131&partnerID=40&md5=68a732c0f820f00724428899babff5b4,"In the stock market, a successful investment requires a good balance between profits and risks. Based on the learning to rank paradigm, stock recommendation has been widely studied in quantitative finance to recommend stocks with higher return ratios for investors. Despite the efforts to make profits, many existing recommendation approaches still have some limitations in risk control, which may lead to intolerable paper losses in practical stock investing. To effectively reduce risks, we draw inspiration from adversarial learning and propose a novel Split Variational Adversarial Training (SVAT) method for risk-aware stock recommendation. Essentially, SVAT encourages the stock model to be sensitive to adversarial perturbations of risky stock examples and enhances the model's risk awareness by learning from perturbations. To generate representative adversarial examples as risk indicators, we devise a variational perturbation generator to model diverse risk factors. Particularly, the variational architecture enables our method to provide a rough risk quantification for investors, showing an additional advantage of interpretability. Experiments on several real-world stock market datasets demonstrate the superiority of our SVAT method. By lowering the volatility of the stock-recommendation model, SVAT effectively reduces investment risks and outperforms state-of-the-art baselines by more than 30% in terms of risk-adjusted profits. All the experimental data and source code are available at https://drive.google.com/drive/folders/14AdM7WENEvIp5x5bV3zV_i4Aev21C9g6?usp=sharing.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesStock recommendation; risk control; variational autoencoder,Commerce; Data mining; Financial markets; Investments; Learning systems; Risk perception; Additional key word and phrasesstock recommendation; Adversarial learning; Auto encoders; Investment risks; Key words; Return ratio; Risk aware; Risks controls; Training methods; Variational autoencoder; Profitability
Tagging Items with Emerging Tags: A Neural Topic Model Based Few-Shot Learning Approach,2024,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193519935&doi=10.1145%2f3641859&partnerID=40&md5=0a478225de7d081613a41fc685b63240,"The tagging system has become a primary tool to organize information resources on the Internet, which benefits both users and the platforms. To build a successful tagging system, automatic tagging methods are desired. With the development of society, new tags keep emerging. The problem of tagging items with emerging tags is an open challenge for an automatic tagging system, and it has not been well studied in the literature. We define this problem as a tag-centered cold-start problem in this study and propose a novel neural topic model based few-shot learning method named NTFSL to solve the problem. In our proposed method, we innovatively fuse the topic modeling task with the few-shot learning task, endowing the model with the capability to infer effective topics to solve the tag-centered cold-start problem with the property of interpretability. Meanwhile, we propose a novel neural topic model for the topic modeling task to improve the quality of inferred topics, which helps enhance the tagging performance. Furthermore, we develop a novel inference method based on the variational auto-encoding framework for model inference. We conducted extensive experiments on two real-world datasets, and the results demonstrate the superior performance of our proposed model compared with state-of-the-art machine learning methods. Case studies also show the interpretability of the model.  © 2024 Copyright held by the owner/author(s).",Additional Key Words and PhrasesFew-shot learning; automatic tagging; classification; deep learning; generative probabilistic model; neural topic model,Deep learning; Thesauri; User interfaces; Additional key word and phrasesfew-shot learning; Automatic tagging; Deep learning; Generative probabilistic model; Key words; Model-based OPC; Neural topic model; Probabilistic models; Tagging systems; Topic Modeling; Learning systems
Dense Text Retrieval Based on Pretrained Language Models: A Survey,2024,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186365769&doi=10.1145%2f3637870&partnerID=40&md5=5f39858426b6d1bfe179d25783735077,"Text retrieval is a long-standing research topic on information seeking, where a system is required to return relevant information resources to user's queries in natural language. From heuristic-based retrieval methods to learning-based ranking functions, the underlying retrieval models have been continually evolved with the ever-lasting technical innovation. To design effective retrieval models, a key point lies in how to learn text representations and model the relevance matching. The recent success of pretrained language models (PLM) sheds light on developing more capable text-retrieval approaches by leveraging the excellent modeling capacity of PLMs. With powerful PLMs, we can effectively learn the semantic representations of queries and texts in the latent representation space, and further construct the semantic matching function between the dense vectors for relevance modeling. Such a retrieval approach is called dense retrieval, since it employs dense vectors to represent the texts. Considering the rapid progress on dense retrieval, this survey systematically reviews the recent progress on PLM-based dense retrieval. Different from previous surveys on dense retrieval, we take a new perspective to organize the related studies by four major aspects, including architecture, training, indexing and integration, and thoroughly summarize the mainstream techniques for each aspect. We extensively collect the recent advances on this topic, and include 300+ reference papers. To support our survey, we create a website for providing useful resources, and release a code repository for dense retrieval. This survey aims to provide a comprehensive, practical reference focused on the major progress for dense text retrieval.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesText retrieval; dense retrieval; pretrained language models,Computational linguistics; Heuristic methods; Information retrieval; Modeling languages; Natural language processing systems; Query processing; Search engines; Vector spaces; Additional key word and phrasestext retrieval; Dense retrieval; Information seeking; Key words; Language model; Learn+; Pretrained language model; Research topics; Retrieval models; Text retrieval; Semantics
Triple Sequence Learning for Cross-domain Recommendation,2024,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189900083&doi=10.1145%2f3638351&partnerID=40&md5=87a08e889970971df6061f2f050508d7,"Cross-domain recommendation (CDR) aims at leveraging the correlation of users' behaviors in both the source and target domains to improve the user preference modeling in the target domain. Conventional CDR methods typically explore the dual-relations between the source and target domains' behaviors. However, this may ignore the informative mixed behaviors that naturally reflect the user's global preference. To address this issue, we present a novel framework, termed triple sequence learning for cross-domain recommendation (Tri-CDR), which jointly models the source, target, and mixed behavior sequences to highlight the global and target preference and precisely model the triple correlation in CDR. Specifically, Tri-CDR independently models the hidden representations for the triple behavior sequences and proposes a triple cross-domain attention (TCA) method to emphasize the informative knowledge related to both user's global and target-domain preference. To comprehensively explore the cross-domain correlations, we design a triple contrastive learning (TCL) strategy that simultaneously considers the coarse-grained similarities and fine-grained distinctions among the triple sequences, ensuring the alignment while preserving information diversity in multi-domain. We conduct extensive experiments and analyses on six cross-domain settings. The significant improvements of Tri-CDR with different sequential encoders verify its effectiveness and universality. The source code is available at https://github.com/hulkima/Tri-CDR.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesCross-domain recommendation; contrastive learning; triple learning,Clock and data recovery circuits (CDR circuits); HTTP; Additional key word and phrasescross-domain recommendation; Behavior sequences; Contrastive learning; Cross-domain; Cross-domain recommendations; Key words; Sequence learning; Target domain; Triple learning; User behaviors; User profile
Relevance Feedback with Brain Signals,2024,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85187428343&doi=10.1145%2f3637874&partnerID=40&md5=04d3d176f586484a7a7792412a644e5e,"The Relevance Feedback (RF) process relies on accurate and real-time relevance estimation of feedback documents to improve retrieval performance. Since collecting explicit relevance annotations imposes an extra burden on the user, extensive studies have explored using pseudo-relevance signals and implicit feedback signals as substitutes. However, such signals are indirect indicators of relevance and suffer from complex search scenarios where user interactions are absent or biased. Recently, the advances in portable and high-precision brain-computer interface (BCI) devices have shown the possibility to monitor user's brain activities during search process. Brain signals can directly reflect user's psychological responses to search results and thus it can act as additional and unbiased RF signals. To explore the effectiveness of brain signals in the context of RF, we propose a novel RF framework that combines BCI-based RF with pseudo-relevance signals and implicit signals to improve the performance of document re-ranking. The experimental results on the user study dataset show that incorporating brain signals leads to significant performance improvement in our RF framework. Besides, we observe that brain signals perform particularly well in several hard search scenarios, especially when implicit signals as feedback are missing or noisy. This reveals when and how to exploit brain signals in the context of RF.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesRelevance feedback; brain computer interface; interactive information retrieval,Brain; Information retrieval; Search engines; User interfaces; Additional key word and phrasesrelevance feedback; Brain signals; Feedback process; Feedback signal; Interactive information retrieval; Key words; Performance; Real- time; Relevance feedback; Time relevances; Brain computer interface
Predicting Representations of Information Needs from Digital Activity Context,2024,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193535810&doi=10.1145%2f3639819&partnerID=40&md5=839d30be3866597f23394789f5fd8282,"Information retrieval systems often consider search-session and immediately preceding web-browsing history as the context for predicting users' present information needs. However, such context is only available when a user's information needs originate from web context or when users have issued preceding queries in the search session. Here, we study the effect of more extensive context information recorded from users' everyday digital activities by monitoring all information interacted with and communicated using personal computers. Twenty individuals were recruited for 14 days of 24/7 continuous monitoring of their digital activities, including screen contents, clicks, and operating system logs on Web and non-Web applications. Using this data, a transformer architecture is applied to model the digital activity context and predict representations of personalized information needs. Subsequently, the representations of information needs are used for query prediction, query auto-completion, selected search result prediction, and Web search re-ranking. The predictions of the models are evaluated against the ground truth data obtained from the activity recordings. The results reveal that the models accurately predict representations of information needs improving over the conventional search session and web-browsing contexts. The results indicate that the present practice for utilizing users' contextual information is limited and can be significantly extended to achieve improved search interaction support and performance. © 2024 Copyright held by the owner/author(s).",Additional Key Words and PhrasesInformation need; digital activity; query auto-completion; query prediction; selected search result prediction,Information retrieval; Information retrieval systems; Personal computers; Search engines; Activity contexts; Additional key word and phrasesinformation need; Auto completion; Digital activities; Information-retrieval systems; Key words; Query auto-completion; Query prediction; Search sessions; Selected search result prediction; Forecasting
FairGap: Fairness-Aware Recommendation via Generating Counterfactual Graph,2024,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85190659117&doi=10.1145%2f3638352&partnerID=40&md5=4e7a7cd530da0a1409d9a4ede173c305,"The emergence of Graph Neural Networks (GNNs) has greatly advanced the development of recommendation systems. Recently, many researchers have leveraged GNN-based models to learn fair representations for users and items. However, current GNN-based models suffer from biased user-item interaction data, which negatively impacts recommendation fairness. Although there have been several studies employing adversarial learning to mitigate this issue in recommendation systems, they mostly focus on modifying the model training approach with fairness regularization and neglect direct intervention of biased interaction. In contrast to these models, this article introduces a novel perspective by directly intervening in observed interactions to generate a counterfactual graph (called FairGap) that is not influenced by sensitive node attributes, enabling us to learn fair representations for users and items easily. We design FairGap to answer the key counterfactual question: ""Would interactions with an item remain unchanged if a user's sensitive attributes were concealed?"". We also provide theoretical proofs to show that our learning strategy via the counterfactual graph is unbiased in expectation. Moreover, we propose a fairness-enhancing mechanism to continuously improve user fairness in the graph-based recommendation. Extensive experimental results against state-of-the-art competitors and base models on three real-world datasets validate the effectiveness of our proposed model.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesFairness; counterfactual; graph neural network; recommendation,Data mining; Graphic methods; Learning systems; Recommender systems; 'current; Additional key word and phrasesfairness; Adversarial learning; Counterfactuals; Fair representation; Graph neural networks; Key words; Learn+; Network-based modeling; Recommendation; Graph neural networks
Causal Inference in Recommender Systems: A Survey and Future Directions,2024,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191725963&doi=10.1145%2f3639048&partnerID=40&md5=ba4e2f83594bc4d2056bbe12e982a02b,"Recommender systems have become crucial in information filtering nowadays. Existing recommender systems extract user preferences based on the correlation in data, such as behavioral correlation in collaborative filtering, feature-feature, or feature-behavior correlation in click-through rate prediction. However, unfortunately, the real world is driven by causality, not just correlation, and correlation does not imply causation. For instance, recommender systems might recommend a battery charger to a user after buying a phone, where the latter can serve as the cause of the former; such a causal relation cannot be reversed. Recently, to address this, researchers in recommender systems have begun utilizing causal inference to extract causality, thereby enhancing the recommender system. In this survey, we offer a comprehensive review of the literature on causal inference-based recommendation. Initially, we introduce the fundamental concepts of both recommender system and causal inference as the foundation for subsequent content. We then highlight the typical issues faced by non-causality recommender system. Following that, we thoroughly review the existing work on causal inference-based recommender systems, based on a taxonomy of three-aspect challenges that causal inference can address. Finally, we discuss the open problems in this critical research area and suggest important potential future works.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesRecommender systems; causal inference; information retrieval,Collaborative filtering; Search engines; Additional key word and phrasesrecommende system;; Battery chargers; Causal inference;; Causal inferences; Clickthrough rates (CTR); Key words; Preference-based; Rate predictions; Real-world; User's preferences; Recommender systems
Using Neural and Graph Neural Recommender Systems to Overcome Choice Overload: Evidence From a Music Education Platform,2024,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193503494&doi=10.1145%2f3637873&partnerID=40&md5=6e3244d13ab940bbb93d89b0f8a2394d,"The application of recommendation technologies has been crucial in the promotion of physical and digital content across numerous global platforms such as Amazon, Apple, and Netflix. Our study aims to investigate the advantages of employing recommendation technologies on educational platforms, with a particular focus on an educational platform for learning and practicing music. Our research is based on data from Tomplay, a music platform that offers sheet music with professional audio recordings, enabling users to discover and practice music content at varying levels of difficulty. Through our analysis, we emphasize the distinct interaction patterns on educational platforms like Tomplay, which we compare with other commonly used recommendation datasets. We find that interactions are comparatively sparse on educational platforms, with users often focusing on specific content as they learn, rather than interacting with a broader range of material. Therefore, our primary goal is to address the issue of data sparsity. We achieve this through entity resolution principles and propose a neural network (NN)-based recommendation model. Further, we improve this model by utilizing graph neural networks (GNNs), which provide superior predictive accuracy compared to NNs. Notably, our study demonstrates that GNNs are highly effective even for users with little or no historical preferences (cold-start problem). Our cold-start experiments also provide valuable insights into an independent issue, namely, the number of historical interactions needed by a recommendation model to gain a comprehensive understanding of a user. Our findings demonstrate that a platform acquires a solid knowledge of a user's general preferences and characteristics with 50 past interactions. Overall, our study makes significant contributions to information systems research on business analytics and prescriptive analytics. Moreover, our framework and evaluation results offer implications for various stakeholders, including online educational institutions, education policymakers, and learning platform users.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",digital education; embeddings; entity resolution; Neural networks,Audio acoustics; Audio recordings; Data mining; E-learning; Embeddings; Graph neural networks; Predictive analytics; Recommender systems; Digital contents; Digital education; Educational platforms; Embeddings; Entity resolutions; Graph neural networks; Music education; Netflix; Neural-networks; Recommendation technologies; Music
Intent-Oriented Dynamic Interest Modeling for Personalized Web Search,2024,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186221755&doi=10.1145%2f3639817&partnerID=40&md5=3f252e939c4e55c40dc54be516b257c5,"Given a user, a personalized search model relies on her historical behaviors, such as issued queries and their clicked documents, to generate an interest profile and personalize search results accordingly. In interest profiling, most existing personalized search approaches use ""static""document representations as the inputs, which do not change with the current search. However, a document is usually long and contains multiple pieces of information, a static fix-length document vector is usually insufficient to represent the important information related to the original query or the current query, and makes the profile noisy and ambiguous. To tackle this problem, we propose building dynamic and intent-oriented document representations which highlight important parts of a document rather than simply encode the entire text. Specifically, we divide each document into multiple passages, and then separately use the original query and the current query to interact with the passages. Thereafter we generate two ""dynamic""document representations containing the key information around the historical and the current user intent, respectively. We then profile interest by capturing the interactions between these document representations, the historical queries, and the current query. Experimental results on a real-world search log dataset demonstrate that our model significantly outperforms state-of-the-art personalization methods.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesPersonalized search; document representation; user interest,Information retrieval; 'current; Additional key word and phrasespersonalized search; Document Representation; Document vectors; Key words; Personalized search; Personalized web searches; Search models; Static documents; Users' interests; User profile
Understanding or Manipulation: Rethinking Online Performance Gains of Modern Recommender Systems,2024,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193484947&doi=10.1145%2f3637869&partnerID=40&md5=ea7980ce8c26ff587736162393112751,"Recommender systems are expected to be assistants that help human users find relevant information automatically without explicit queries. As recommender systems evolve, increasingly sophisticated learning techniques are applied and have achieved better performance in terms of user engagement metrics such as clicks and browsing time. The increase in the measured performance, however, can have two possible attributions: a better understanding of user preferences, and a more proactive ability to utilize human bounded rationality to seduce user over-consumption. A natural following question is whether current recommendation algorithms are manipulating user preferences. If so, can we measure the manipulation level? In this article, we present a general framework for benchmarking the degree of manipulations of recommendation algorithms, in both slate recommendation and sequential recommendation scenarios. The framework consists of four stages, initial preference calculation, training data collection, algorithm training and interaction, and metrics calculation that involves two proposed metrics, Manipulation Score and Preference Shift. We benchmark some representative recommendation algorithms in both synthetic and real-world datasets under the proposed framework. We have observed that a high online click-through rate does not necessarily mean a better understanding of user initial preference, but ends in prompting users to choose more documents they initially did not favor. Moreover, we find that the training data have notable impacts on the manipulation degrees, and algorithms with more powerful modeling abilities are more sensitive to such impacts. The experiments also verified the usefulness of the proposed metrics for measuring the degree of manipulations. We advocate that future recommendation algorithm studies should be treated as an optimization problem with constrained user preference manipulations.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesRecommender system; bounded rationality; user model,Constrained optimization; Learning systems; Online systems; User profile; Additional key word and phrasesrecommende system; Bounded rationality; Key words; On-line performance; Performance; Performance Gain; Recommendation algorithms; Training data; User Modelling; User's preferences; Recommender systems
"MCRPL: A Pretrain, Prompt, and Fine-tune Paradigm for Non-overlapping Many-to-one Cross-domain Recommendation",2024,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193535319&doi=10.1145%2f3641860&partnerID=40&md5=03d3b6d4d3c0e3a2ec614b1e8c9c98ff,"Cross-domain Recommendation is the task that tends to improve the recommendations in the sparse target domain by leveraging the information from other rich domains. Existing methods of cross-domain recommendation mainly focus on overlapping scenarios by assuming users are totally or partially overlapped, which are taken as bridges to connect different domains. However, this assumption does not always hold, since it is illegal to leak users' identity information to other domains. Conducting Non-overlapping MCR (NMCR) is challenging, since (1) the absence of overlapping information prevents us from directly aligning different domains, and this situation may get worse in the MCR scenario, and (2) the distribution between source and target domains makes it difficult for us to learn common information across domains. To overcome the above challenges, we focus on NMCR and devise MCRPL as our solution. To address Challenge 1, we first learn shared domain-agnostic and domain-dependent prompts and pre-train them in the pre-training stage. To address Challenge 2, we further update the domain-dependent prompts with other parameters kept fixed to transfer the domain knowledge to the target domain. We conduct experiments on five real-world domains, and the results show the advance of our MCRPL method compared with several recent SOTA baselines. Moreover, our source codes have been publicly released.1 © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesCross-domain recommendation; sequential recommendation; unsupervised cross-domain recommendation,Additional key word and phrasescross-domain recommendation; Cross-domain recommendations; Different domains; Key words; Learn+; Many-to-one; Sequential recommendation; Target domain; Unsupervised cross-domain recommendation; User identity
Less is More: Removing Redundancy of Graph Convolutional Networks for Recommendation,2024,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193209270&doi=10.1145%2f3632751&partnerID=40&md5=5493d91004dbf92bf941c5b84371c40f,"While Graph Convolutional Networks (GCNs) have shown great potential in recommender systems and collaborative filtering (CF), they suffer from expensive computational complexity and poor scalability. On top of that, recent works mostly combine GCNs with other advanced algorithms which further sacrifice model efficiency and scalability. In this work, we unveil the redundancy of existing GCN-based methods in three aspects: (1) Feature redundancy. By reviewing GCNs from a spectral perspective, we show that most spectral graph features are noisy for recommendation, while stacking graph convolution layers can suppress but cannot completely remove the noisy features, which we mostly summarize from our previous work; (2) Structure redundancy. By providing a deep insight into how user/item representations are generated, we show that what makes them distinctive lies in the spectral graph features, while the core idea of GCNs (i.e., neighborhood aggregation) is not the reason making GCNs effective; and (3) Distribution redundancy. Following observations from (1), we further show that the number of required spectral features is closely related to the spectral distribution, where important information tends to be concentrated in more (fewer) spectral features on a flatter (sharper) distribution. To make important information be concentrated in as few features as possible, we sharpen the spectral distribution by increasing the node similarity without changing the original data, thereby reducing the computational cost. To remove these three kinds of redundancies, we propose a Simplified Graph Denoising Encoder (SGDE) only exploiting the top-K singular vectors without explicitly aggregating neighborhood, which significantly reduces the complexity of GCN-based methods. We further propose a scalable contrastive learning framework to alleviate data sparsity and to boost model robustness and generalization, leading to significant improvement. Extensive experiments on three real-world datasets show that our proposed SGDE not only achieves state-of-the-art but also shows higher scalability and efficiency than our previously proposed GDE as well as traditional and GCN-based CF methods.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Collaborative filtering; graph convolutional network,Collaborative filtering; Complex networks; Convolution; Efficiency; Scalability; Convolutional networks; De-noising; Graph convolutional network; Graph features; Less is mores; Model efficiency; Neighbourhood; Network-based; Spectral distribution; Spectral feature; Redundancy
Privacy-Preserving Individual-Level COVID-19 Infection Prediction via Federated Graph Learning,2024,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193233239&doi=10.1145%2f3633202&partnerID=40&md5=f6163aa592d153190927b6b5fd08f05c,"Accurately predicting individual-level infection state is of great value since its essential role in reducing the damage of the epidemic. However, there exists an inescapable risk of privacy leakage in the fine-grained user mobility trajectories required by individual-level infection prediction. In this article, we focus on developing a framework of privacy-preserving individual-level infection prediction based on federated learning (FL) and graph neural networks (GNN). We propose Falcon, a Federated grAph Learning method for privacy-preserving individual-level infeCtion predictiON. It utilizes a novel hypergraph structure with spatio-temporal hyperedges to describe the complex interactions between individuals and locations in the contagion process. By organically combining the FL framework with hypergraph neural networks, the information propagation process of the graph machine learning is able to be divided into two stages distributed on the server and the clients, respectively, so as to effectively protect user privacy while transmitting high-level information. Furthermore, it elaborately designs a differential privacy perturbation mechanism as well as a plausible pseudo location generation approach to preserve user privacy in the graph structure. Besides, it introduces a cooperative coupling mechanism between the individual-level prediction model and an additional region-level model to mitigate the detrimental impacts caused by the injected obfuscation mechanisms. Extensive experimental results show that our methodology outperforms state-of-the-art algorithms and is able to protect user privacy against actual privacy attacks. Our code and datasets are available at the link: https://github.com/wjfu99/FL-epidemic.  © 2024 Copyright held by the owner/author(s).",COVID-19 infection detection; Human mobility; privacy protection,Forecasting; Graph neural networks; Information dissemination; Learning systems; Privacy-preserving techniques; COVID-19 infection detection; Fine grained; Human mobility; Hyper graph; Individual levels; Privacy leakages; Privacy preserving; Privacy protection; User privacy; Users' mobility; COVID-19
Exploring Dense Retrieval for Dialogue Response Selection,2024,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189305315&doi=10.1145%2f3632750&partnerID=40&md5=648cf550b25d26c40c59b8c3a9e79e67,"Recent progress in deep learning has continuously improved the accuracy of dialogue response selection. However, in real-world scenarios, the high computation cost forces existing dialogue response selection models to rank only a small number of candidates, recalled by a coarse-grained model, precluding many high-quality candidates. To overcome this problem, we present a novel and efficient response selection model and a set of tailor-designed learning strategies to train it effectively. The proposed model consists of a dense retrieval module and an interaction layer, which could directly select the proper response from a large corpus. We conduct re-rank and full-rank evaluations on widely used benchmarks to evaluate our proposed model. Extensive experimental results demonstrate that our proposed model notably outperforms the state-of-the-art baselines on both re-rank and full-rank evaluations. Moreover, human evaluation results show that the response quality could be improved further by enlarging the candidate pool with nonparallel corpora. In addition, we also release high-quality benchmarks that are carefully annotated for more accurate dialogue response selection evaluation. All source codes, datasets, model parameters, and other related resources have been publicly available.1 © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",deep semantic hashing; dense retrieval; dialogue evaluation; Retrieval-based dialogue system,Benchmarking; Coarse-grained modeling; Deep learning; Learning systems; Quality control; Speech processing; Deep semantic hashing; Dense retrieval; Dialogue evaluation; Dialogue systems; High quality; Real-world scenario; Recent progress; Response selection; Retrieval-based dialog system; Selection model; Semantics
Understanding Feeling-of-Knowing in Information Search: An EEG Study,2024,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85190496703&doi=10.1145%2f3611384&partnerID=40&md5=3631c2c35e3e19a269181f8d0437f895,"The realisation and the variability of information needs (IN) with respect to a searcher's gap in knowledge is driven by the perceived Anomalous State of Knowledge (ASK). The concept of Feeling-of-Knowing (FOK), as the introspective feeling of knowledge awareness, shares the characteristics of an ASK state. From an IR perspective, FOK as a premise to trigger IN is unexplored. Motivated by the neuroimaging studies in IR, we investigate the neurophysiological drivers associated with FOK, to provide evidence validating FOK as a distinctive state in IN realisation. We employ Electroencephalography to capture the brain activity of 24 healthy participants performing a textual Question Answering IR scenario. We analyse the evoked neural patterns corresponding to three states of knowledge: i.e., (1)""I know"", (2)""FOK"", (3)""I do not know"". Our findings show the distinct neurophysiological signatures (N1, P2, N400, P6) in response to information segments processed in the context of our three levels. They further reveal that the brain manifestation associated with ""FOK""does not significantly differ from the ones associated with ""I do not know"", indicating their association with recognition of a gap in knowledge and as such could further inform the IN formation on different levels of knowing.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",anomalous state of knowledge; EEG; ERP; feeling-of-knowing; Information need; information retrieval; IR; metacognition; metamemory,Brain; Electrophysiology; Information retrieval; Neuroimaging; Neurophysiology; Anomalous state of knowledge; Feeling of knowledge; Feeling-of-knowing; Information need; Information search; IR; Knowledge awareness; Knowledge state; Metacognition; Metamemory; Electroencephalography
DGEKT: A Dual Graph Ensemble Learning Method for Knowledge Tracing,2024,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192563133&doi=10.1145%2f3638350&partnerID=40&md5=aeafaeb27d008147173fba1867da8208,"Knowledge tracing aims to trace students' evolving knowledge states by predicting their future performance on concept-related exercises. Recently, some graph-based models have been developed to incorporate the relationships between exercises to improve knowledge tracing, but only a single type of relationship information is generally explored. In this article, we present a novel Dual Graph Ensemble learning method for Knowledge Tracing (DGEKT), which establishes a dual graph structure of students' learning interactions to capture the heterogeneous exercise-concept associations and interaction transitions by hypergraph modeling and directed graph modeling, respectively. To combine the dual graph models, we introduce the technique of online knowledge distillation. This choice arises from the observation that, while the knowledge tracing model is designed to predict students' responses to the exercises related to different concepts, it is optimized merely with respect to the prediction accuracy on a single exercise at each step. With online knowledge distillation, the dual graph models are adaptively combined to form a stronger ensemble teacher model, which provides its predictions on all exercises as extra supervision for better modeling ability. In the experiments, we compare DGEKT against eight knowledge tracing baselines on three benchmark datasets, and the results demonstrate that DGEKT achieves state-of-the-art performance.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",dual graph structure; graph convolutional networks; Knowledge tracing; online knowledge distillation,Association reactions; Benchmarking; Directed graphs; Distillation; E-learning; Forecasting; Graphic methods; Students; Convolutional networks; Dual graph structure; Dual graphs; Ensemble learning; Graph convolutional network; Graph model; Graph structures; Knowledge tracings; Learning methods; Online knowledge distillation; Learning systems
Robust Collaborative Filtering to Popularity Distribution Shift,2024,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191017839&doi=10.1145%2f3627159&partnerID=40&md5=3e816b8c4b9edd3f7d685b58f9dd62bb,"In leading collaborative filtering (CF) models, representations of users and items are prone to learn popularity bias in the training data as shortcuts. The popularity shortcut tricks are good for in-distribution (ID) performance but poorly generalized to out-of-distribution (OOD) data, i.e., when popularity distribution of test data shifts w.r.t. the training one. To close the gap, debiasing strategies try to assess the shortcut degrees and mitigate them from the representations. However, there exist two deficiencies: (1) when measuring the shortcut degrees, most strategies only use statistical metrics on a single aspect (i.e., item frequency on item and user frequency on user aspect), failing to accommodate the compositional degree of a user-item pair; (2) when mitigating shortcuts, many strategies assume that the test distribution is known in advance. This results in low-quality debiased representations. Worse still, these strategies achieve OOD generalizability with a sacrifice on ID performance.In this work, we present a simple yet effective debiasing strategy, PopGo, which quantifies and reduces the interaction-wise popularity shortcut without any assumptions on the test data. It first learns a shortcut model, which yields a shortcut degree of a user-item pair based on their popularity representations. Then, it trains the CF model by adjusting the predictions with the interaction-wise shortcut degrees. By taking both causal- and information-theoretical looks at PopGo, we can justify why it encourages the CF model to capture the critical popularity-agnostic features while leaving the spurious popularity-relevant patterns out. We use PopGo to debias two high-performing CF models (matrix factorization [28] and LightGCN [19]) on four benchmark datasets. On both ID and OOD test sets, PopGo achieves significant gains over the state-of-the-art debiasing strategies (e.g., DICE [71] and MACR [58]). Codes and datasets are available at https://github.com/anzhang314/PopGo.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Collaborative Filtering; Popularity Bias; Recommendation,Data mining; Factorization; Information theory; De-biasing; Filtering models; Learn+; Model representation; Performance; Popularity bias; Popularity distribution; Recommendation; Test data; Training data; Collaborative filtering
On the Impact of Showing Evidence from Peers in Crowdsourced Truthfulness Assessments,2024,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193223512&doi=10.1145%2f3637872&partnerID=40&md5=dfe66e13a67ed0cb5c5d58faada73aa7,"Misinformation has been rapidly spreading online. The common approach to dealing with it is deploying expert fact-checkers who follow forensic processes to identify the veracity of statements. Unfortunately, such an approach does not scale well. To deal with this, crowdsourcing has been looked at as an opportunity to complement the work done by trained journalists. In this article, we look at the effect of presenting the crowd with evidence from others while judging the veracity of statements. We implement variants of the judgment task design to understand whether and how the presented evidence may or may not affect the way crowd workers judge truthfulness and their performance. Our results show that, in certain cases, the presented evidence and the way in which it is presented may mislead crowd workers who would otherwise be more accurate if judging independently from others. Those who make appropriate use of the provided evidence, however, can benefit from it and generate better judgments.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",crowdsourcing; information credibility; metadata; Misinformation,Forensic process; Information credibilities; Misinformation; Performance; Task design; Workers'; Crowdsourcing
Cross-domain Recommendation via Dual Adversarial Adaptation,2024,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193216960&doi=10.1145%2f3632524&partnerID=40&md5=33f8e91abe7f8dfad1796506eb8a1948,"Data scarcity is a perpetual challenge of recommendation systems, and researchers have proposed a variety of cross-domain recommendation methods to alleviate the problem of data scarcity in target domains. However, in many real-world cross-domain recommendation systems, the source domain and the target domain are sampled from different data distributions, which obstructs the cross-domain knowledge transfer. In this article, we propose to specifically align the data distributions between the source domain and the target domain to alleviate imbalanced sample distribution and thus challenge the data scarcity issue in the target domain. Technically, our proposed approach builds a dual adversarial adaptation (DAA) framework to adversarially train the target model together with a pre-trained source model. Two domain discriminators play the two-player minmax game with the target model and guide the target model to learn reliable domain-invariant features that can be transferred across domains. At the same time, the target model is calibrated to learn domain-specific information of the target domain. In addition, we formulate our approach as a plug-and-play module to boost existing recommendation systems. We apply the proposed method to address the issues of insufficient data and imbalanced sample distribution in real-world Click-through Rate/Conversion Rate predictions on two large-scale industrial datasets. We evaluate the proposed method in scenarios with and without overlapping users/items, and extensive experiments verify that the proposed method is able to significantly improve the prediction performance on the target domain. For instance, our method can boost PLE with a performance improvement of 15.4% in terms of Area Under Curve compared with single-domain PLE on our private game dataset. In addition, our method is able to surpass single-domain MMoE by 6.85% on the public datasets. Code: https://github.com/TL-UESTC/DAA.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Adversarial domain adaptation; cross-domain recommendation,Domain Knowledge; Knowledge management; Large datasets; Adversarial domain adaptation; Cross-domain recommendations; Data distribution; Data scarcity; Domain adaptation; Learn+; Real-world; Sample distributions; Target domain; Target model; Recommender systems
Personalized and Diversified: Ranking Search Results in an Integrated Way,2024,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193261127&doi=10.1145%2f3631989&partnerID=40&md5=0c2feee42606b16d0d84a2a1cc738b1a,"Ambiguity in queries is a common problem in information retrieval. There are currently two solutions: search result personalization and diversification. The former aims to tailor results for different users based on their preferences, but the limitations are redundant results and incomplete capture of user intents. The goal of the latter is to return results that cover as many aspects related to the query as possible. It improves diversity yet loses personality and cannot return the exact results the user wants. Intuitively, such two solutions can complement each other and bring more satisfactory reranking results. In this article, we propose a novel framework, namely, PnD, to integrate personalization and diversification reasonably. We employ the degree of refinding to determine the weight of personalization dynamically. Moreover, to improve the diversity and relevance of reranked results simultaneously, we design a reset RNN structure (RRNN) with the ""reset gate""to measure the influence of the newly selected document on novelty. Besides, we devise a ""subtopic learning layer""to learn the virtual subtopics, which can yield fine-grained representations of queries, documents, and user profiles. Experimental results illustrate that our model can significantly outperform existing search result personalization and diversification methods.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",integration; Personalized search; search result diversification,Exact results; Fine grained; Learn+; Personalizations; Personalized search; Query documents; Re-ranking; Refinding; Search results diversifications; Solution searches; User profile
SMLP4Rec: An Efficient All-MLP Architecture for Sequential Recommendations,2024,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193275425&doi=10.1145%2f3637871&partnerID=40&md5=0cbd185b423509d119b0e7094dff3d11,"Self-attention models have achieved the state-of-the-art performance in sequential recommender systems by capturing the sequential dependencies among user-item interactions. However, they rely on adding positional embeddings to the item sequence to retain the sequential information, which may break the semantics of item embeddings due to the heterogeneity between these two types of embeddings. In addition, most existing works assume that such dependencies exist solely in the item embeddings, but neglect their existence among the item features. In our previous study, we proposed a novel sequential recommendation model, i.e., MLP4Rec, based on the recent advances of MLP-Mixer architectures, which is naturally sensitive to the order of items in a sequence because matrix elements related to different positions of a sequence will be given different weights in training. We developed a tri-directional fusion scheme to coherently capture sequential, cross-channel, and cross-feature correlations with linear computational complexity as well as much fewer model parameters than existing self-attention methods. However, the cascading mixer structure, the large number of normalization layers between different mixer layers, and the noise generated by these operations limit the efficiency of information extraction and the effectiveness of MLP4Rec. In this extended version, we propose a novel framework - SMLP4Rec for sequential recommendation to address the aforementioned issues. The new framework changes the flawed cascading structure to a parallel mode, and integrates normalization layers to minimize their impact on the model's efficiency while maximizing their effectiveness. As a result, the training speed and prediction accuracy of SMLP4Rec are vastly improved in comparison to MLP4Rec. Extensive experimental results demonstrate that the proposed method is significantly superior to the state-of-the-art approaches. The implementation code is available online to ease reproducibility.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",multi-layer perceptron; Recommender system; sequential recommendation,Efficiency; Embeddings; Mixers (machinery); Semantics; Attention model; Embeddings; Feature correlation; Matrix elements; Multilayers perceptrons; Normalisation; Sequential dependencies; Sequential information; Sequential recommendation; State-of-the-art performance; Recommender systems
(Un)likelihood Training for Interpretable Embedding,2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184804381&doi=10.1145%2f3632752&partnerID=40&md5=fed00e6272066097349115510ac8890c,"Cross-modal representation learning has become a new normal for bridging the semantic gap between text and visual data. Learning modality agnostic representations in a continuous latent space, however, is often treated as a black-box data-driven training process. It is well known that the effectiveness of representation learning depends heavily on the quality and scale of training data. For video representation learning, having a complete set of labels that annotate the full spectrum of video content for training is highly difficult, if not impossible. These issues, black-box training and dataset bias, make representation learning practically challenging to be deployed for video understanding due to unexplainable and unpredictable results. In this article, we propose two novel training objectives, likelihood and unlikelihood functions, to unroll the semantics behind embeddings while addressing the label sparsity problem in training. The likelihood training aims to interpret semantics of embeddings beyond training labels, while the unlikelihood training leverages prior knowledge for regularization to ensure semantically coherent interpretation. With both training objectives, a new encoder-decoder network, which learns interpretable cross-modal representation, is proposed for ad-hoc video search. Extensive experiments on TRECVid and MSR-VTT datasets show that the proposed network outperforms several state-of-the-art retrieval models with a statistically significant performance margin.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",ad-hoc video search; cross-modal representation learning; Explainable embedding,Semantics; Video recording; Ad-hoc video search; Black boxes; Cross-modal representation learning; Cross-modal representations; Embeddings; Explainable embedding; Semantic gap; Text data; Video search; Visual data; Embeddings
Stopping Methods for Technology-assisted Reviews Based on Point Processes,2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193283075&doi=10.1145%2f3631990&partnerID=40&md5=e23a4f8fcb126ac16191acc7fd5b314c,"Technology-assisted Review (TAR), which aims to reduce the effort required to screen collections of documents for relevance, is used to develop systematic reviews of medical evidence and identify documents that must be disclosed in response to legal proceedings. Stopping methods are algorithms that determine when to stop screening documents during the TAR process, helping to ensure that workload is minimised while still achieving a high level of recall. This article proposes a novel stopping method based on point processes, which are statistical models that can be used to represent the occurrence of random events. The approach uses rate functions to model the occurrence of relevant documents in the ranking and compares four candidates, including one that has not previously been used for this purpose (hyperbolic). Evaluation is carried out using standard datasets (CLEF e-Health, TREC Total Recall, TREC Legal), and this work is the first to explore stopping method robustness by reporting performance on a range of rankings of varying effectiveness. Results show that the proposed method achieves the desired level of recall without requiring an excessive number of documents to be examined in the majority of cases and also compares well against multiple alternative approaches.  © 2023 Copyright held by the owner/author(s).",Technology assisted review; TAR; total recall; stopping criteria; point processes; Cox Process; Poisson Process,Hyperbolic functions; Information retrieval; Tar; Cox process; Cox process;; Point process; Point process;; Poisson process; Stopping criteria;; Stopping criterion; Technology assisted review;; Technology-assisted review;; Total recall;; Diagnosis
rHDP: An Aspect Sharing-Enhanced Hierarchical Topic Model for Multi-Domain Corpus,2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191573890&doi=10.1145%2f3631352&partnerID=40&md5=d269655ea3739b72ec1253cefeb3cac3,"Learning topic hierarchies from a multi-domain corpus is crucial in topic modeling as it reveals valuable structural information embedded within documents. Despite the extensive literature on hierarchical topic models, effectively discovering inter-topic correlations and differences among subtopics at the same level in the topic hierarchy, obtained from multiple domains, remains an unresolved challenge. This article proposes an enhanced nested Chinese restaurant process (nCRP), nCRP+, by introducing an additional mechanism based on Chinese restaurant franchise (CRF) for aspect-sharing pattern extraction in the original nCRP. Subsequently, by employing the distribution extracted from nCRP+ as the prior distribution for topic hierarchy in the hierarchical Dirichlet processes (HDP), we develop a hierarchical topic model for multi-domain corpus, named rHDP. We describe the model with the analogy of Chinese restaurant franchise based on the central kitchen and propose a hierarchical Gibbs sampling scheme to infer the model. Our method effectively constructs well-established topic hierarchies, accurately reflecting diverse parent-child topic relationships, explicit topic aspect sharing correlations for inter-topics, and differences between these shared topics. To validate the efficacy of our approach, we conduct experiments using a renowned public dataset and an online collection of Chinese financial documents. The experimental results confirm the superiority of our method over the state-of-the-art techniques in identifying multi-domain topic hierarchies, according to multiple evaluation metrics.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",aspect sharing pattern; Chinese restaurant franchise; hierarchical Dirichlet processes; Hierarchical topic model,Aspect sharing pattern; Chinese restaurant franchise; Hierarchical Dirichlet process; Hierarchical topic models; Mechanism-based; Multi-domains; Multiple domains; Structural information; Topic hierarchy; Topic Modeling; Data mining
H3GNN: Hybrid Hierarchical HyperGraph Neural Network for Personalized Session-based Recommendation,2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193266873&doi=10.1145%2f3630002&partnerID=40&md5=e213a974bb1eefeb9dc5e57f48e68dfd,"Personalized Session-based recommendation (PSBR) is a general and challenging task in the real world, aiming to recommend a session's next clicked item based on the session's item transition information and the corresponding user's historical sessions. A session is defined as a sequence of interacted items during a short period. The PSBR problem has a natural hierarchical architecture in which each session consists of a series of items, and each user owns a series of sessions. However, the existing PSBR methods can merely capture the pairwise relation information within items and users. To effectively capture the hierarchical information, we propose a novel hierarchical hypergraph neural network to model the hierarchical architecture. Moreover, considering that the items in sessions are sequentially ordered, while the hypergraph can only model the set relation, we propose a directed graph aggregator (DGA) to aggregate the sequential information from the directed global item graph. By attentively combining the embeddings of the above two modules, we propose a framework dubbed H3GNN (Hybrid Hierarchical HyperGraph Neural Network). Extensive experiments on three benchmark datasets demonstrate the superiority of our proposed model compared to the state-of-the-art methods, and ablation experiment results validate the effectiveness of all the proposed components.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Hypergraph neural networks; recommender system; session,Graph neural networks; Network architecture; Recommender systems; Hierarchical architectures; Hyper graph; Hypergraph neural network; Item-based; Neural-networks; Personalized sessions; Real-world; Recommendation methods; Session; Short periods; Directed graphs
DiffuRec: A Diffusion Model for Sequential Recommendation,2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192577384&doi=10.1145%2f3631116&partnerID=40&md5=df3024059e6b187b95f0f588289e2914,"Mainstream solutions to sequential recommendation represent items with fixed vectors. These vectors have limited capability in capturing items' latent aspects and users' diverse preferences. As a new generative paradigm, diffusion models have achieved excellent performance in areas like computer vision and natural language processing. To our understanding, its unique merit in representation generation well fits the problem setting of sequential recommendation. In this article, we make the very first attempt to adapt the diffusion model to sequential recommendation and propose DiffuRec for item representation construction and uncertainty injection. Rather than modeling item representations as fixed vectors, we represent them as distributions in DiffuRec, which reflect a user's multiple interests and an item's various aspects adaptively. In the diffusion phase, DiffuRec corrupts the target item embedding into a Gaussian distribution via noise adding, which is further applied for sequential item distribution representation generation and uncertainty injection. Afterward, the item representation is fed into an approximator for target item representation reconstruction. In the reverse phase, based on a user's historical interaction behaviors, we reverse a Gaussian noise into the target item representation, then apply a rounding operation for target item prediction. Experiments over four datasets show that DiffuRec outperforms strong baselines by a large margin.1 © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Diffusion model; sequential recommendation; user preference learning,Diffusion; Gaussian distribution; Gaussian noise (electronic); Natural language processing systems; Recommender systems; User profile; As distribution; Diffusion model; Language processing; Natural languages; Performance; Preference learning; Sequential recommendation; Uncertainty; User preference learning; User's preferences; Large datasets
Information Retrieval Evaluation Measures Defined on Some Axiomatic Models of Preferences,2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193244622&doi=10.1145%2f3632171&partnerID=40&md5=cd04dfe2705a865dbf1c96ff9929aec9,"Information retrieval (IR) evaluation measures are essential for capturing the relevance of documents to topics and determining the task performance efficiency of retrieval systems. The study of IR evaluation measures through their formal properties enables a better understanding of their suitability for a specific task. Some works have modeled the effectiveness of retrieval measures with axioms, heuristics, or desirable properties, leading to order relationships on the set where they are defined. Each of these ordering structures constitutes an axiomatic model of preferences (AMP), which can be considered as an ""ideal""scenario of retrieval. Based on lattice theory and on the representational theory of measurement, this work formally explores numeric, metric, and scale properties of some effectiveness measures defined on AMPs. In some of these scenarios, retrieval measures are completely determined from the scores of a subset of document rankings: join-irreducible elements. All the possible metrics and pseudometrics, defined on these structures are expressed in terms of the join-irreducible elements. The deduced scale properties of the precision, recall, F-measure, RBP, DCG, and AP confirm some recent results in the IR field.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",evaluation metric; Information retrieval; lattice theory,Lattice theory; Search engines; Axiomatic models; Evaluation measures; Evaluation metrics; Formal properties; Join irreducible; Performance efficiency; Retrieval evaluation; Retrieval systems; Scale properties; Task performance; Information retrieval
Improving First-stage Retrieval of Point-of-interest Search by Pre-training Models,2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184657261&doi=10.1145%2f3631937&partnerID=40&md5=9499fc3a799c000f93358d6ad5b31a83,"Point-of-interest (POI) search is important for location-based services, such as navigation and online ride-hailing service. The goal of POI search is to find the most relevant destinations from a large-scale POI database given a text query. To improve the effectiveness and efficiency of POI search, most existing approaches are based on a multi-stage pipeline that consists of an efficiency-oriented retrieval stage and one or more effectiveness-oriented re-rank stages. In this article, we focus on the first efficiency-oriented retrieval stage of the POI search. We first identify the limitations of existing first-stage POI retrieval models in capturing the semantic-geography relationship and modeling the fine-grained geographical context information. Then, we propose a Geo-Enhanced Dense Retrieval framework for POI search to alleviate the above problems. Specifically, the proposed framework leverages the capacity of pre-trained language models (e.g., BERT) and designs a pre-training approach to better model the semantic match between the query prefix and POIs. With the POI collection, we first perform a token-level pre-training task based on a geographical-sensitive masked language prediction and design two retrieval-oriented pre-training tasks that link the address of each POI to its name and geo-location. With the user behavior logs collected from an online POI search system, we design two additional pre-training tasks based on users' query reformulation behavior and the transitions between POIs. We also utilize a late-interaction network structure to model the fine-grained interactions between the text and geographical context information within an acceptable query latency. Extensive experiments on the real-world datasets collected from the Didichuxing application demonstrate that the proposed framework can achieve superior retrieval performance over existing first-stage POI retrieval methods.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",First-stage Retrieval; Geographical Context; Interaction-based Model; Point-of-interest (POI) Search; Pre-training Model; User Behavior,Behavioral research; Efficiency; Information retrieval; Location based services; Query processing; Telecommunication services; Context information; Fine grained; First-stage retrieval; Geographical context; Interaction-based model; Point-of-interest  search; Pre-training; Pre-training model; Training model; User behaviors; Semantics
Bi-preference Learning Heterogeneous Hypergraph Networks for Session-based Recommendation,2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193228664&doi=10.1145%2f3631940&partnerID=40&md5=1e254f3fd2d394cce2d37afa2c0a5c93,"Session-based recommendation intends to predict next purchased items based on anonymous behavior sequences. Numerous economic studies have revealed that item price is a key factor influencing user purchase decisions. Unfortunately, existing methods for session-based recommendation only aim at capturing user interest preference, while ignoring user price preference. Actually, there are primarily two challenges preventing us from accessing price preference. First, the price preference is highly associated to various item features (i.e., category and brand), which asks us to mine price preference from heterogeneous information. Second, price preference and interest preference are interdependent and collectively determine user choice, necessitating that we jointly consider both price and interest preference for intent modeling. To handle above challenges, we propose a novel approach Bi-Preference Learning Heterogeneous Hypergraph Networks (BiPNet) for session-based recommendation. Specifically, the customized heterogeneous hypergraph networks with a triple-level convolution are devised to capture user price and interest preference from heterogeneous features of items. Besides, we develop a Bi-Preference Learning schema to explore mutual relations between price and interest preference and collectively learn these two preferences under the multi-task learning architecture. Extensive experiments on multiple public datasets confirm the superiority of BiPNet over competitive baselines. Additional research also supports the notion that the price is crucial for the task.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",heterogeneous hypergraph; multi-task learning; price and interest preference; Session-based recommendation,User profile; Behavior sequences; Economic study; Heterogeneous hypergraph; Hyper graph; Item-based; Key factors; Multitask learning; Preference learning; Price and interest preference; Session-based recommendation; Learning systems
Manipulating Visually Aware Federated Recommender Systems and Its Countermeasures,2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85188150635&doi=10.1145%2f3630005&partnerID=40&md5=9f243cce36e0fe48308ef67b69cbdd81,"Federated recommender systems (FedRecs) have been widely explored recently due to their capability to safeguard user data privacy. These systems enable a central server to collaboratively learn recommendation models by sharing public parameters with clients, providing privacy-preserving solutions. However, this collaborative approach also creates a vulnerability that allows adversaries to manipulate FedRecs. Existing works on FedRec security already reveal that items can easily be promoted by malicious users via model poisoning attacks, but all of them mainly focus on FedRecs with only collaborative information (i.e., user-item interactions). We contend that these attacks are effective primarily due to the data sparsity of collaborative signals. In light of this, we propose a method to address data sparsity and model poisoning threats by incorporating product visual information. Intriguingly, our empirical findings demonstrate that the inclusion of visual information renders all existing model poisoning attacks ineffective.Nevertheless, the integration of visual information also introduces a new avenue for adversaries to manipulate federated recommender systems, as this information typically originates from external sources. To assess such threats, we propose a novel form of poisoning attack tailored for visually aware FedRecs, namely image poisoning attacks, where adversaries can gradually modify the uploaded image with human-unaware perturbations to manipulate item ranks during the FedRecs' training process. Moreover, we provide empirical evidence showcasing a heightened threat when image poisoning attacks are combined with model poisoning attacks, resulting in easier manipulation of the federated recommendation systems. To ensure the safe utilization of visual information, we employ a diffusion model in visually aware FedRecs to purify each uploaded image and detect the adversarial images. Extensive experiments conducted with two FedRecs on two datasets demonstrate the effectiveness and generalization of our proposed attacks and defenses.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",diffusion model; Federated learning; image pollution and purification; multimodal recommendation; poisoning attack,Diffusion; Privacy-preserving techniques; Central servers; Data sparsity; Diffusion model; Federated learning; Image pollution and purification; Multi-modal; Multimodal recommendation; Poisoning attacks; User data; Visual information; Recommender systems
"Better Understanding Procedural Search Tasks: Perceptions, Behaviors, and Challenges",2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193222279&doi=10.1145%2f3630004&partnerID=40&md5=bc555ddab16120c2efef7bbe06365af5,"People often search for information to acquire procedural knowledge-""how to""knowledge about step-by-step procedures, methods, algorithms, techniques, heuristics, and skills. A procedural search task might involve implementing a solution to a problem, evaluating different approaches to a problem, and brainstorming on the types of problems that can be solved with a specific resource. We report on a study (N=36) that aimed to better understand how people search for procedural knowledge. Much research has investigated how search task characteristics impact people's perceptions and behaviors. Along these lines, we manipulated procedural search tasks along two orthogonal dimensions: product and goal. The product dimension relates to the main outcome of the task and the goal dimension relates to task's success criteria. We manipulated tasks across three product categories and two goal categories. The study investigated four research questions. First, we examined the effects of the product and goal on participants' (RQ1) pre-task perceptions, (RQ2) post-task perceptions, and (RQ3) search behaviors. Second, regardless of the task product and goal, by analyzing participants' think-aloud comments and screen activities we closely examined how people search for procedural knowledge. Specifically, we report on (RQ4) important relevance criteria, types of information sought, and challenges.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",General TermsExperimentation; Human Factors; Measurement,Behavioral research; Human engineering; General termsexperimentation; People searches; Post-tasks; Procedural knowledge; Product categories; Research questions; Search behavior; Search tasks; Step by step procedure; Task characteristics; Heuristic methods
Community Preserving Social Recommendation with Cyclic Transfer Learning,2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193226100&doi=10.1145%2f3631115&partnerID=40&md5=309b9aefcbe7994603910bccbac3eee4,"Transfer learning-based recommendation mitigates the sparsity of user-item interactions by introducing auxiliary domains. Social influence extracted from direct connections between users typically serves as an auxiliary domain to improve prediction performance. However, direct social connections also face severe data sparsity problems that limit model performance. In contrast, users' dependency on communities is another valuable social information that has not yet received sufficient attention. Although studies have incorporated community information into recommendation by aggregating users' preferences within the same community, they seldom capture the structural discrepancies among communities and the influence of structural discrepancies on users' preferences. To address these challenges, we propose a community-preserving recommendation framework with cyclic transfer learning, incorporating heterogeneous community influence into the rating domain. We analyze the characteristics of the community domain and its inter-influence on the rating domain, and construct link constraints and preference constraints in the community domain. The shared vectors that bridge the rating domain and the community domain are allowed to be more consistent with the characteristics of both domains. Extensive experiments are conducted on four real-world datasets. The results manifest the excellent performance of our approach in capturing real users' preferences compared with other state-of-the-art methods.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",community; matrix factorization; rating prediction; social recommendation; Transfer learning,Economic and social effects; Community; Data sparsity problems; Matrix factorizations; Prediction performance; Rating prediction; Social connection; Social influence; Social recommendation; Transfer learning; User's preferences; Factorization
Contrastive Multi-view Interest Learning for Cross-domain Sequential Recommendation,2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85188262883&doi=10.1145%2f3632402&partnerID=40&md5=b815a2c921ef11bfcbd742308981ba29,"Cross-domain recommendation (CDR), which leverages information collected from other domains, has been empirically demonstrated to effectively alleviate data sparsity and cold-start problems encountered in traditional recommendation systems. However, current CDR methods, including those considering time information, do not jointly model the general and current interests within and across domains, which is pivotal for accurately predicting users' future interactions. In this article, we propose a Contrastive learning-enhanced Multi-View interest learning model (CMVCDR) for cross-domain sequential recommendation. Specifically, we design a static view and a sequential view to model uses' general interests and current interests, respectively. We divide a user's general interest representation into a domain-invariant part and a domain-specific part. A cross-domain contrastive learning objective is introduced to impose constraints for optimizing these representations. In the sequential view, we first devise an attention mechanism guided by users' domain-invariant interest representations to distill cross-domain knowledge pertaining to domain-invariant factors while reducing noise from irrelevant factors. We further design a domain-specific interest-guided temporal information aggregation mechanism to generate users' current interest representations. Extensive experiments demonstrate the effectiveness of our proposed model compared with state-of-the-art methods.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",contrastive learning; Cross-domain recommendation; multi-view learning; sequential recommendation,Clock and data recovery circuits (CDR circuits); 'current; Cold start problems; Contrastive learning; Cross-domain; Cross-domain recommendations; Data sparsity; Domain specific; Multi-view learning; Multi-views; Sequential recommendation; Domain Knowledge
Contextualizing and Expanding Conversational Queries without Supervision,2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193239761&doi=10.1145%2f3632622&partnerID=40&md5=864534a962b4dd773ef07612d8dcc1ac,"Most conversational passage retrieval systems try to resolve conversational dependencies by using an intermediate query resolution step. To do so, they synthesize conversational data or assume the availability of large-scale question rewriting datasets. To relax those conditions, we propose a zero-shot unified resolution-retrieval approach, that (i) contextualizes and (ii) expands query embeddings using the conversation history and without fine-tuning on conversational data. Contextualization biases the last user question embeddings towards the conversation. Query expansion is used in two ways: (i) abstractive expansion generates embeddings based on the current question and previous history, whereas (ii) extractive expansion tries to identify history term embeddings based on attention weights from the retriever. Our experiments demonstrate the effectiveness of both contextualization and unified expansion in improving conversational retrieval. Contextualization does so mostly by resolving anaphoras to the conversation and bringing their embeddings closer to the important resolution terms that were omitted. By adding embeddings to the query, expansion targets phenomena of ellipsis more explicitly, with our analysis verifying its effectiveness on identifying and adding important resolutions to the query. By combining contextualization and expansion, we find that our zero-shot unified resolution-retrieval methods are competitive and can even outperform supervised methods.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",conversational search; dense retrieval; Information retrieval; query expansion,Information retrieval; Large datasets; Search engines; Zero-shot learning; Condition; Contextualization; Conversational search; Dense retrieval; Embeddings; Large-scales; Passage retrieval; Query expansion; Query resolution; Retrieval systems; Embeddings
Decoupled Progressive Distillation for Sequential Prediction with Interaction Dynamics,2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193245480&doi=10.1145%2f3632403&partnerID=40&md5=b57519b539d921c9e1486b64b16881b5,"Sequential prediction has great value for resource allocation due to its capability in analyzing intents for next prediction. A fundamental challenge arises from real-world interaction dynamics where similar sequences involving multiple intents may exhibit different next items. More importantly, the character of volume candidate items in sequential prediction may amplify such dynamics, making deep networks hard to capture comprehensive intents. This article presents a sequential prediction framework with Decoupled Progressive Distillation (DePoD), drawing on the progressive nature of human cognition. We redefine target and non-target item distillation according to their different effects in the decoupled formulation. This can be achieved through two aspects: (1) Regarding how to learn, our target item distillation with progressive difficulty increases the contribution of low-confidence samples in the later training phase while keeping high-confidence samples in the earlier phase. And, the non-target item distillation starts from a small subset of non-target items from which size increases according to the item frequency. (2) Regarding whom to learn from, a difference evaluator is utilized to progressively select an expert that provides informative knowledge among items from the cohort of peers. Extensive experiments on four public datasets show DePoD outperforms state-of-the-art methods in terms of accuracy-based metrics.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",interaction dynamics; knowledge distillation; representation learning; Sequential prediction,Dynamics; Forecasting; Different effects; Human cognition; Interaction dynamics; Knowledge distillation; Learn+; Real-world; Representation learning; Resources allocation; Sequential prediction; Target and non targets; Distillation
Triple Dual Learning for Opinion-based Explainable Recommendation,2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193262116&doi=10.1145%2f3631521&partnerID=40&md5=069c20c5ba58bbdbf1cc6740fba9bfcc,"Recently, with the aim of enhancing the trustworthiness of recommender systems, explainable recommendation has attracted much attention from the research community. Intuitively, users' opinions toward different aspects of an item determine their ratings (i.e., users' preferences) for the item. Therefore, rating prediction from the perspective of opinions can realize personalized explanations at the level of item aspects and user preferences. However, there are several challenges in developing an opinion-based explainable recommendation: (1) The complicated relationship between users' opinions and ratings. (2) The difficulty of predicting the potential (i.e., unseen) user-item opinions because of the sparsity of opinion information. To tackle these challenges, we propose an overall preference-aware opinion-based explainable rating prediction model by jointly modeling the multiple observations of user-item interaction (i.e., review, opinion, rating). To alleviate the sparsity problem and raise the effectiveness of opinion prediction, we further propose a triple dual learning-based framework with a novelly designed triple dual constraint. Finally, experiments on three popular datasets show the effectiveness and great explanation performance of our framework.  © 2023 Copyright held by the owner/author(s).",Explainable recommendation; triple dual learning; opinion-based explanation,Forecasting; Dual constraints; Explainable recommendation;; Opinion-based explanation; Performance; Prediction modelling; Research communities; Sparsity problems; Triple dual learning;; User's preferences; User profile
Personalized Query Expansion with Contextual Word Embeddings,2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181682540&doi=10.1145%2f3624988&partnerID=40&md5=ccee83469d1aa90644da158d4f611df6,"Personalized Query Expansion, the task of expanding queries with additional terms extracted from the user-related vocabulary, is a well-known solution to improve the retrieval performance of a system w.r.t. short queries. Recent approaches rely on word embeddings to select expansion terms from user-related texts. Although promising results have been delivered with former word embedding techniques, we argue that these methods are not suited for contextual word embeddings, which produce a unique vector representation for each term occurrence. In this article, we propose a Personalized Query Expansion method designed to solve the issues arising from the use of contextual word embeddings with the current Personalized Query Expansion approaches based on word embeddings. Specifically, we employ a clustering-based procedure to identify the terms that better represent the user interests and to improve the diversity of those selected for expansion, achieving improvements of up to 4% w.r.t. the best-performing baseline in terms of MAP@100. Moreover, our approach outperforms previous ones in terms of efficiency, allowing us to achieve sub-millisecond expansion times even in data-rich scenarios. Finally, we introduce a novel metric to evaluate the expansion terms’ diversity and empirically show the unsuitability of previous approaches based on word embeddings when employed along with contextual word embeddings, which cause the selection of semantically overlapping expansion terms. © 2023 Association for Computing Machinery. All rights reserved.",contextual word embeddings; dense retrieval; Personalization; Query Expansion,Information retrieval; Contextual word embedding; Contextual words; Dense retrieval; Embedding technique; Embeddings; Personalizations; Query expansion; Retrieval performance; Term occurrences; Vector representations; Embeddings
An Intent Taxonomy of Legal Case Retrieval,2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181565017&doi=10.1145%2f3626093&partnerID=40&md5=e291cc0d0dfcaaf24673d94636483258,"Legal case retrieval is a special Information Retrieval (IR) task focusing on legal case documents. Depending on the downstream tasks of the retrieved case documents, users’ information needs in legal case retrieval could be significantly different from those in Web search and traditional ad hoc retrieval tasks. While there are several studies that retrieve legal cases based on text similarity, the underlying search intents of legal retrieval users, as shown in this article, are more complicated than that yet mostly unexplored. To this end, we present a novel hierarchical intent taxonomy of legal case retrieval. It consists of five intent types categorized by three criteria, i.e., search for Particular Case(s), Characterization, Penalty, Procedure, and Interest. The taxonomy was constructed transparently and evaluated extensively through interviews, editorial user studies, and query log analysis. Through a laboratory user study, we reveal significant differences in user behavior and satisfaction under different search intents in legal case retrieval. Furthermore, we apply the proposed taxonomy to various downstream legal retrieval tasks, e.g., result ranking and satisfaction prediction, and demonstrate its effectiveness. Our work provides important insights into the understanding of user intents in legal case retrieval and potentially leads to better retrieval techniques in the legal domain, such as intent-aware ranking strategies and evaluation methodologies. © 2023 Association for Computing Machinery. All rights reserved.",Legal case retrieval; search intent; taxonomy; user behavior; user satisfaction,Behavioral research; Information retrieval; Query processing; Case retrieval; Down-stream; Legal case; Legal case retrieval; Search intents; User behaviors; User information need; User study; Users' satisfactions; Web searches; Taxonomies
Contrastive Graph Prompt-tuning for Cross-domain Recommendation,2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85177777784&doi=10.1145%2f3618298&partnerID=40&md5=0eb1600f85a9ff4999f91bdcb875796f,"Recommender systems commonly suffer from the long-standing data sparsity problem where insufficient user-item interaction data limits the systems’ ability to make accurate recommendations. This problem can be alleviated using cross-domain recommendation techniques. In particular, in a cross-domain setting, knowledge sharing between domains permits improved effectiveness on the target domain. While recent cross-domain recommendation techniques used a pre-training configuration, we argue that such techniques lead to a low fine-tuning efficiency, especially when using large neural models. In recent language models, prompts have been used for parameter-efficient and time-efficient tuning of the models on the downstream tasks—these prompts represent a tunable latent vector that permits to freeze the rest of the language model’s parameters. To address the cross-domain recommendation task in an efficient manner, we propose a novel Personalised Graph Prompt-based Recommendation (PGPRec) framework, which leverages the efficiency benefits from prompt-tuning. In such a framework, we develop personalised and item-wise graph prompts based on relevant items to those items the user has interacted with. In particular, we apply Contrastive Learning to generate the pre-trained embeddings, to allow an increased generalisability in the pre-training stage, and to ensure an effective prompt-tuning stage. To evaluate the effectiveness of our PGPRec framework in a cross-domain setting, we conduct an extensive evaluation with the top-k recommendation task and perform a cold-start analysis. The obtained empirical results on four Amazon Review datasets show that our proposed PGPRec framework can reduce up to 74% of the tuned parameters with a competitive performance and achieves an 11.41% improved performance compared to the strongest baseline in a cold-start scenario. © 2023 Association for Computing Machinery. All rights reserved.",graph neural network; Personalisation; recommender system,Computational linguistics; Data mining; Efficiency; Graph neural networks; Scattering parameters; Cold-start; Cross-domain; Cross-domain recommendations; Data sparsity problems; Graph neural networks; Knowledge-sharing; Language model; Personalizations; Pre-training; Recommendation techniques; Recommender systems
NIR-Prompt: A Multi-task Generalized Neural Information Retrieval Training Framework,2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181667157&doi=10.1145%2f3626092&partnerID=40&md5=15dbd153df7be0035184208e9b3c4710,"Information retrieval aims to find information that meets users’ needs from the corpus. Different needs correspond to different IR tasks such as document retrieval, open-domain question answering, retrieval-based dialogue, and so on, while they share the same schema to estimate the relationship between texts. It indicates that a good IR model can generalize to different tasks and domains. However, previous studies indicate that state-of-the-art neural information retrieval (NIR) models, e.g., pre-trained language models (PLMs) are hard to generalize. It is mainly because the end-to-end fine-tuning paradigm makes the model overemphasize task-specific signals and domain biases but loses the ability to capture generalized essential signals. To address this problem, we propose a novel NIR training framework named NIR-Prompt for retrieval and reranking stages based on the idea of decoupling signal capturing and combination. NIR-Prompt exploits Essential Matching Module (EMM) to capture the essential matching signals and gets the description of tasks by Matching Description Module (MDM). The description is used as task-adaptation information to combine the essential matching signals to adapt to different tasks. Experiments under in-domain multi-task, out-of-domain multitask, and new task adaptation settings show that NIR-Prompt can improve the generalization of PLMs in NIR for both retrieval and reranking stages compared with baselines. © 2023 Association for Computing Machinery. All rights reserved.",,Infrared devices; Natural language processing systems; Document Retrieval; Language model; Matchings; Multi tasks; Neural information; Open domain question answering; Re-ranking; Task adaptation; Training framework; User need; Information retrieval
SLED: Structure Learning based Denoising for Recommendation,2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181579882&doi=10.1145%2f3611385&partnerID=40&md5=f8a4aaddae6fc60f94521c23bd03b44e,"In recommender systems, click behaviors play a fundamental role in mining users’ interests and training models (clicked items as positive samples). Such signals are implicit feedback and are arguably less representative of users’ inherent interests. Most existing works denoise implicit feedback by introducing external signals, such as gaze, dwell time, and “like” behaviors. However, such explicit feedback is not always routinely available, or might be problematic to collect on a large scale. In this paper, we identify that an interaction’s related structural patterns in its neighborhood graph are potentially correlated with some outcome of implicit feedback (i.e., users’ ratings after consuming items), analogous to findings in other domains such as social networks. Inspired by this finding, we propose a novel Structure LEarning based Denoising (SLED) framework for denoising recommendation without explicit signals, which consists of two phases: center-aware graph structure learning and denoised recommendation. Phase 1 pre-trains a structural encoder in a self-supervised manner and learns to capture an interaction’s related structural patterns in its neighborhood graph. Phase 2 transfers the structure encoder to downstream recommendation datasets, which helps to down-weight the effect of noisy interactions on user interest modeling and loss calculation. We collect a relatively noisy industrial dataset across several days during a period of product promotion festival. Extensive experiments on this dataset and multiple public datasets demonstrate that the proposed SLED framework can significantly improve the recommendation quality over various base recommendation models. © 2023 Association for Computing Machinery. All rights reserved.",denoise; implicit feedback; Recommender system; structure learning; user-item graph,Data mining; Information filtering; Learning systems; Signal encoding; User profile; De-Noise; De-noising; Implicit feedback; Neighborhood graphs; Structural pattern; Structure-learning; Training model; User training; User-interest models; User-item graph; Recommender systems
Spatio-temporal Contrastive Learning-enhanced GNNs for Session-based Recommendation,2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181454862&doi=10.1145%2f3626091&partnerID=40&md5=1b69322692bc5cac11fe20aef6c7c7d0,"Session-based recommendation (SBR) systems aim to utilize the user’s short-term behavior sequence to predict the next item without the detailed user profile. Most recent works try to model the user preference by treating the sessions as between-item transition graphs and utilize various graph neural networks (GNNs) to encode the representations of pair-wise relations among items and their neighbors. Some of the existing GNN-based models mainly focus on aggregating information from the view of spatial graph structure, which ignores the temporal relations within neighbors of an item during message passing and the information loss results in a sub-optimal problem. Other works embrace this challenge by incorporating additional temporal information but lack sufficient interaction between the spatial and temporal patterns. To address this issue, inspired by the uniformity and alignment properties of contrastive learning techniques, we propose a novel framework called Session-based Recommendation with Spatio-temporal Contrastive Learning-enhanced GNNs (RESTC). The idea is to supplement the GNN-based main supervised recommendation task with the temporal representation via an auxiliary cross-view contrastive learning mechanism. Furthermore, a novel global collaborative filtering graph embedding is leveraged to enhance the spatial view in the main task. Extensive experiments demonstrate the significant performance of RESTC compared with the state-of-the-art baselines. We release our source code at https://github.com/SUSTechBruce/RESTC-Source-code. © 2023 Association for Computing Machinery. All rights reserved.",contrastive learning; graph neural network; Recommendation system; session-based recommendation; temporal information,Collaborative filtering; Data mining; Learning systems; Message passing; User profile; Behavior sequences; Contrastive learning; Graph neural networks; Session-based recommendation; Short-term behavior; Spatio-temporal; Temporal information; Transition graphs; User's preferences; User's profiles; Graph neural networks
Policy-driven Knowledge Selection and Response Generation for Document-grounded Dialogue,2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181536769&doi=10.1145%2f3617829&partnerID=40&md5=1c30f62c748c33fb27cfaa39e637efc4,"Document-grounded dialogue (DGD) uses documents as external knowledge for dialogue generation. Correctly understanding the dialogue context is crucial for selecting knowledge from the document and generating proper responses. In this article, we propose using a dialogue policy to help the dialogue understanding in DGD. Our dialogue policy consists of two kinds of guiding signals: utterance function and topic transfer intent. The utterance function reflects the purpose and style of an utterance, and the topic transfer intent reflects the topic and content of an utterance. We propose a novel framework exploiting our dialogue policy for two core tasks in DGD, namely, knowledge selection (KS) and response generation (RG). The framework consists of two modules: the policy planner leverages policy-aware dialogue representation to select knowledge and predict the policy of the response; the generator uses policy/knowledge-aware dialogue representation for response generation. Our policy-driven model gets state-of-the-art performance on three public benchmarks, and we provide a detailed analysis of the experimental results. Our code/data will be released on GitHub. © 2023 Association for Computing Machinery. All rights reserved.",Document-grounded dialogue; knowledge selection; response generation,Dialogue generations; Dialogue representation; Document-grounded dialog; External knowledge; Knowledge selection; Policy driven; Response generation; State-of-the-art performance; Benchmarking
Incorporating Structural Information into Legal Case Retrieval,2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180587072&doi=10.1145%2f3609796&partnerID=40&md5=bb310ecf2eba6a4e01fcba30b171168c,"Legal case retrieval has received increasing attention in recent years. However, compared to ad hoc retrieval tasks, legal case retrieval has its unique challenges. First, case documents are rather lengthy and contain complex legal structures. Therefore, it is difficult for most existing dense retrieval models to encode an entire document and capture its inherent complex structure information. Most existing methods simply truncate part of the document content to meet the input length limit of PLMs, which will lead to information loss. Additionally, the definition of relevance in the legal domain differs from that in the general domain. Previous semantic-based or lexical-based methods fail to provide a comprehensive understanding of the relevance of legal cases. In this article, we propose a Structured Legal case Retrieval (SLR) framework, which incorporates internal and external structural information to address the above two challenges. Specifically, to avoid the truncation of long legal documents, the internal structural information, which is the organization pattern of legal documents, can be utilized to split a case document into segments. By dividing the document-level semantic matching task into segment-level subtasks, SLR can separately process segments using different methods based on the characteristic of each segment. In this way, the key elements of a case document can be highlighted without losing other content information. Second, toward a better understanding of relevance in the legal domain, we investigate the connections between criminal charges appearing in large-scale case corpus to generate a chargewise relation graph. Then, the similarity between criminal charges can be pre-computed as the external structural information to enhance the recognition of relevant cases. Finally, a learning-to-rank algorithm integrates the features collected from internal and external structures to output the final retrieval results. Experimental results on public legal case retrieval benchmarks demonstrate the superior effectiveness of SLR over existing state-of-the-art baselines, including traditional bag-of-words and neural-based methods. Furthermore, we conduct a case study to visualize how the proposed model focuses on key elements and improves retrieval performance. © 2023 Association for Computing Machinery. All rights reserved.",Legal case retrieval; relevance; structural information,Authentication; Crime; Information retrieval; Ad-hoc retrieval tasks; Case retrieval; Criminal charges; Key elements; Legal case; Legal case retrieval; Legal documents; Legal domains; Relevance; Structural information; Semantics
An Approximate Algorithm for Maximum Inner Product Search over Streaming Sparse Vectors,2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173042472&doi=10.1145%2f3609797&partnerID=40&md5=12a4c6218c1c13ec2d272232e507719f,"Maximum Inner Product Search or top-k retrieval on sparse vectors is well understood in information retrieval, with a number of mature algorithms that solve it exactly. However, all existing algorithms are tailored to text and frequency-based similarity measures. To achieve optimal memory footprint and query latency, they rely on the near stationarity of documents and on laws governing natural languages. We consider, instead, a setup in which collections are streaming—necessitating dynamic indexing—and where indexing and retrieval must work with arbitrarily distributed real-valued vectors. As we show, existing algorithms are no longer competitive in this setup, even against naïve solutions. We investigate this gap and present a novel approximate solution, called Sinnamon, that can efficiently retrieve the top-k results for sparse real valued vectors drawn from arbitrary distributions. Notably, Sinnamon offers levers to trade off memory consumption, latency, and accuracy, making the algorithm suitable for constrained applications and systems. We give theoretical results on the error introduced by the approximate nature of the algorithm and present an empirical evaluation of its performance on two hardware platforms and synthetic and real-valued datasets. We conclude by laying out concrete directions for future research on this general top-k retrieval problem over sparse vectors. © 2023 Association for Computing Machinery. All rights reserved.",Approximate algorithms; maximum inner product search; sparse vectors,Economic and social effects; Indexing (of information); Vectors; Approximate algorithms; Inner product; Maximum inner product search; Memory footprint; Optimal memory; Query latency; Real valued vector; Similarity measure; Sparse vectors; Stationarity; Information retrieval
Understanding and Predicting User Satisfaction with Conversational Recommender Systems,2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181531566&doi=10.1145%2f3624989&partnerID=40&md5=0d6864cbca0f79877cb87be51d23782b,"User satisfaction depicts the effectiveness of a system from the user’s perspective. Understanding and predicting user satisfaction is vital for the design of user-oriented evaluation methods for conversational recommender systems (CRSs). Current approaches rely on turn-level satisfaction ratings to predict a user’s overall satisfaction with CRS. These methods assume that all users perceive satisfaction similarly, failing to capture the broader dialogue aspects that influence overall user satisfaction. We investigate the effect of several dialogue aspects on user satisfaction when interacting with a CRS. To this end, we annotate dialogues based on six aspects (i.e., relevance, interestingness, understanding, task-completion, interest-arousal, and efficiency) at the turn and dialogue levels. We find that the concept of satisfaction varies per user. At the turn level, a system’s ability to make relevant recommendations is a significant factor in satisfaction. We adopt these aspects as features for predicting response quality and user satisfaction. We achieve an F1-score of 0.80 in classifying dissatisfactory dialogues, and a Pearson’s r of 0.73 for turn-level response quality estimation, demonstrating the effectiveness of the proposed dialogue aspects in predicting user satisfaction and being able to identify dialogues where the system is failing. © 2023 Association for Computing Machinery. All rights reserved.",Conversational recommender system; predicting user satisfaction; understanding user satisfaction; user experience,Forecasting; 'current; Conversational recommender systems; Evaluation methods; Interestingness; Predicting user satisfaction; Understanding user satisfaction; User oriented; Users' experiences; Users' satisfactions; Recommender systems
Contrastive Self-supervised Learning in Recommender Systems: A Survey,2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180800062&doi=10.1145%2f3627158&partnerID=40&md5=01359eaeb288bf9981f8d5421928575c,"Deep learning-based recommender systems have achieved remarkable success in recent years. However, these methods usually heavily rely on labeled data (i.e., user-item interactions), suffering from problems such as data sparsity and cold-start. Self-supervised learning, an emerging paradigm that extracts information from unlabeled data, provides insights into addressing these problems. Specifically, contrastive self-supervised learning, due to its flexibility and promising performance, has attracted considerable interest and recently become a dominant branch in self-supervised learning-based recommendation methods. In this survey, we provide an up-to-date and comprehensive review of current contrastive self-supervised learning-based recommendation methods. Firstly, we propose a unified framework for these methods. We then introduce a taxonomy based on the key components of the framework, including view generation strategy, contrastive task, and contrastive objective. For each component, we provide detailed descriptions and discussions to guide the choice of the appropriate method. Finally, we outline open issues and promising directions for future research. © 2023 Association for Computing Machinery. All rights reserved.",Contrastive learning; deep learning; self-supervised learning; survey; unsupervised learning,Deep learning; Learning systems; Recommender systems; Unsupervised learning; Cold-start; Contrastive learning; Data sparsity; Data users; Deep learning; Extract informations; Labeled data; Recommendation methods; Self-supervised learning; Unlabeled data; Supervised learning
Relieving Popularity Bias in Interactive Recommendation: A Diversity-Novelty-Aware Reinforcement Learning Approach,2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181558078&doi=10.1145%2f3618107&partnerID=40&md5=0c44756e898f0dc2afec0dbcf9dbe924,"While personalization increases the utility of item recommendation, it also suffers from the issue of popularity bias. However, previous methods emphasize adopting supervised learning models to relieve popularity bias in the static recommendation, ignoring the dynamic transfer of user preference and amplification effects of the feedback loop in the recommender system (RS). In this paper, we focus on studying this issue in the interactive recommendation. We argue that diversification and novelty are both equally crucial for improving user satisfaction of IRS in the aforementioned setting. To achieve this goal, we propose a Diversity-Noveltyaware Interactive Recommendation framework (DNaIR) that augments offline reinforcement learning (RL) to increase the exposure rate of long-tail items with high quality. Its main idea is first to aggregate the item similarity, popularity, and quality into the reward model to help the planning of RL policy. It then designs a diversity-aware stochastic action generator to achieve an efficient and lightweight DNaIR algorithm. Extensive experiments are conducted on the three real-world datasets and an authentic RL environment (VirtualTaobao). The experiments show that our model can better and full use of the long-tail items to improve recommendation satisfaction, especially those low popularity items with high-quality ones, thus achieving state-of-the-art performance. © 2023 Association for Computing Machinery. All rights reserved.",Interactive recommendation; item fairness; popularity bias; reinfor cement learning,Learning systems; Recommender systems; Stochastic systems; High quality; Interactive recommendation; Item fairness; Learning models; Long tail; Personalizations; Popularity bias; Reinfor cement learning; Reinforcement learning approach; Reinforcement learnings; Reinforcement learning
BERD+: A Generic Sequential Recommendation Framework by Eliminating Unreliable Data with Item- and Attribute-level Signals,2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181540415&doi=10.1145%2f3611008&partnerID=40&md5=28164a1c3b1db56cb44d8cd017fff178,"A preliminary work of our study was published at IJCAI’21 [47], which is substantially extended in the following aspects: (1) In Section 1, we analyze the necessity of introducing item attributes for detecting unreliable instances, together with the problems and challenges that attributes may bring in. (2) In Section 2, we add discussions about the limitations of existing attribute-aware recommender systems (Section 2.2) and denoising methods (Section 2.3) in the context of detecting unreliable instances. (3) In Section 4.2, we further conduct an in-depth analysis at the attribute level to demonstrate the capability of attributes for rectifying instance loss and uncertainty, as well as the disturbance caused by attributes. (4) We generalize BERD to a generic framework BERD+ in Section 5.1, equipped with novel modules, i.e., HU-GCN (Section 5.2) and EPE (Section 5.4), which properly incorporate item attributes while reducing their disturbance for rectifying instance uncer tainty (Section 5.5) and loss (Section 5.6). The generic BERD+ can be flexibly plugged into existing SRSs for performance enhanced recommendation via eliminating unreliable data. (5) In Section 6.2, we apply our BERD+ framework to seven state-of-the-art SRSs on five real-world datasets to illustrate its superiority. (6) To avoid unfair comparison caused by item attributes, we build and compare with the baseline that combines the original BERD and an advanced attribute-aware recommender system, KSR [19]. (7) For more comprehensive comparison, in Section 6.2.2, we compare BRED+ with two state-of-the-art denoising approaches; in Section 6.2.3, to examine the efficacy of HU-GCN and EPE, we compare HU-GCN with various attribute embedding techniques, i.e., variants of graph neural networks, and compare EPE with different attribute fusing methods, i.e., adding, concatenation, and weighted sum. (8) In Section 6.2.4, a detailed ablation study is conducted to verify the effectiveness of each module of BERD+. (9) In Sections 6.2.6 and 6.2.7, we visualize the instance loss and uncertainty, together with a case study, showcasing the effectiveness of our proposed BERD+. © 2023 Association for Computing Machinery. All rights reserved.",graph convolution network; heterogeneous graph; Sequential recommender systems; unreliable instances,Data mining; Graph neural networks; Recommender systems; Uncertainty analysis; Attribute levels; Denoising methods; Graph convolution network; Heterogeneous graph; Item-level; Problems and challenges; Sequential recommende system; State of the art; Uncertainty; Unreliable instance; Convolution
Multimodal Dialog Systems with Dual Knowledge-enhanced Generative Pretrained Language Model,2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181541995&doi=10.1145%2f3606368&partnerID=40&md5=18f378f59cef66ba8a58654f9f676fb4,"Text response generation for multimodal task-oriented dialog systems, which aims to generate the proper text response given the multimodal context, is an essential yet challenging task. Although existing efforts have achieved compelling success, they still suffer from two pivotal limitations: (1) overlook the benefit of generative pretraining and (2) ignore the textual context-related knowledge. To address these limitations, we propose a novel dual knowledge-enhanced generative pretrained language mode for multimodal task-oriented dialog systems (DKMD), consisting of three key components: dual knowledge selection, dual knowledge-enhanced context learning, and knowledge-enhanced response generation. To be specific, the dual knowledge selection component aims to select the related knowledge according to both textual and visual modalities of the given context. Thereafter, the dual knowledge-enhanced context learning component targets seamlessly, integrating the selected knowledge into the multimodal context learning from both global and local perspectives, where the cross-modal semantic relation is also explored. Moreover, the knowledge-enhanced response generation component comprises a revised BART decoder, where an additional dot-product knowledge-decoder attention sub-layer is introduced for explicitly utilizing the knowledge to advance the text response generation. Extensive experiments on a public dataset verify the superiority of the proposed DKMD over state-of-the-art competitors. © 2023 Association for Computing Machinery. All rights reserved.",dual knowledge selection; generative pretrained language model; Multimodal task-oriented dialog systems; text response generation,Computational linguistics; Semantics; Context learning; Dialogue systems; Dual knowledge selection; Generative pretrained language model; Language model; Multi-modal; Multimodal task-oriented dialog system; Response generation; Task-oriented; Text response generation; Decoding
Dynamic Multimodal Fusion via Meta-Learning Towards Micro-Video Recommendation,2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181573214&doi=10.1145%2f3617827&partnerID=40&md5=bde4cbdae1cad418987fa4ba7a6b696f,"Multimodal information (e.g., visual, acoustic, and textual) has been widely used to enhance representation learning for micro-video recommendation. For integrating multimodal information into a joint representation of micro-video, multimodal fusion plays a vital role in the existing micro-video recommendation approaches. However, the static multimodal fusion used in previous studies is insufficient to model the various relationships among multimodal information of different micro-videos. In this article, we develop a novel meta-learning-based multimodal fusion framework called Meta Multimodal Fusion (MetaMMF), which dynamically assigns parameters to the multimodal fusion function for each micro-video during its representation learning. Specifically, MetaMMF regards the multimodal fusion of each micro-video as an independent task. Based on the meta information extracted from the multimodal features of the input task, MetaMMF parameterizes a neural network as the item-specific fusion function via a meta learner. We perform extensive experiments on three benchmark datasets, demonstrating the significant improvements over several state-of-the-art multimodal recommendation models, like MMGCN, LATTICE, and InvRL. Furthermore, we lighten our model by adopting canonical polyadic decomposition to improve the training efficiency, and validate its effectiveness through experimental results. Codes are available at https://github.com/hanliu95/MetaMMF. © 2023 Association for Computing Machinery. All rights reserved.",Dynamic multimodal fusion; meta-learning; micro-video recommendation; representation learning,Dynamic multimodal fusion; Fusion functions; Independent tasks; Meta information; Metalearning; Micro-video recommendation; Multi-modal fusion; Multi-modal information; Multimodal features; Representation learning; Recommender systems
Semantic Collaborative Learning for Cross-Modal Moment Localization,2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181728017&doi=10.1145%2f3620669&partnerID=40&md5=956baef4766f56ad6571e0f92c811aae,"Localizing a desired moment within an untrimmed video via a given natural language query, i.e., cross-modal moment localization, has attracted widespread research attention recently. However, it is a challenging task because it requires not only accurately understanding intra-modal semantic information, but also explicitly capturing inter-modal semantic correlations (consistency and complementarity). Existing efforts mainly focus on intra-modal semantic understanding and inter-modal semantic alignment, while ignoring necessary semantic supplement. Consequently, we present a cross-modal semantic perception network for more effective intra-modal semantic understanding and inter-modal semantic collaboration. Concretely, we design a dual-path representation network for intra-modal semantic modeling. Meanwhile, we develop a semantic collaborative network to achieve multi-granularity semantic alignment and hierarchical semantic supplement. Thereby, effective moment localization can be achieved based on sufficient semantic collaborative learning. Extensive comparison experiments demonstrate the promising performance of our model compared with existing state-of-the-art competitors. © 2023 Association for Computing Machinery. All rights reserved.",Cross-modal moment localization; inter-modal semantic collaboration; intra-modal semantic understanding,Natural language processing systems; Collaborative learning; Cross-modal; Cross-modal moment localization; Inter-modal semantic collaboration; Intra-modal semantic understanding; Localisation; Modal semantics; Semantic alignments; Semantics understanding; Semantics
Person-action Instance Search in Story Videos: An Experimental Study,2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181536955&doi=10.1145%2f3617892&partnerID=40&md5=36f4b34fdac803803291a6dd39f0f34f,"Person-Action instance search (P-A INS) aims to retrieve the instances of a specific person doing a specific action, which appears in the 2019–2021 INS tasks of the world-famous TREC Video Retrieval Evaluation (TRECVID). Most of the top-ranking solutions can be summarized with a Division-Fusion-Optimization (DFO) framework, in which person and action recognition scores are obtained separately, then fused, and, optionally, further optimized to generate the final ranking. However, TRECVID only evaluates the final ranking results, ignoring the effects of intermediate steps and their implementation methods. We argue that conducting the fine-grained evaluations of intermediate steps of DFO framework will (1) provide a quantitative analysis of the different methods’ performance in intermediate steps; (2) find out better design choices that contribute to improving retrieval performance; and (3) inspire new ideas for future research from the limitation analysis of current techniques. Particularly, we propose an indirect evaluation method motivated by the leave-one-out strategy, which finds an optimal solution surpassing the champion teams in 2020–2021 INS tasks. Moreover, to validate the generalizability and robustness of the proposed solution under various scenarios, we specifically construct a new large-scale P-A INS dataset and conduct comparative experiments with both the leading NIST TRECVID INS solution and the state-of-the-art P-A INS method. Finally, we discuss the limitations of our evaluation work and suggest future research directions. © 2023 Association for Computing Machinery. All rights reserved.",composite concepts; Movie video; person-action instance search,Information retrieval; Action recognition; Composite concept; Fusion optimization; Movie video; Optimization framework; Person recognition; Person-action instance search; Retrieval evaluation; TREC video; Video retrieval; Large dataset
Heterogeneous Evolution Network Embedding with Temporal Extension for Intelligent Tutoring Systems,2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181538737&doi=10.1145%2f3617828&partnerID=40&md5=40ce3b9632a08e37da4055eb751a5ee9,"Graph embedding (GE) aims to acquire low-dimensional node representations while maintaining the graph’s structural and semantic attributes. Intelligent tutoring systems (ITS) signify a noteworthy achievement in the fusion of AI and education. Utilizing GE to model ITS can elevate their performance in predictive and annotation tasks. Current GE techniques, whether applied to heterogeneous or dynamic graphs, struggle to efficiently model ITS data. The GEs within ITS should retain their semidynamic, independent, and smooth characteristics. This article introduces a heterogeneous evolution network (HEN) for illustrating entities and relations within an ITS. Additionally, we introduce a temporal extension graph neural network (TEGNN) to model both evolving and static nodes within the HEN. In the TEGNN framework, dynamic nodes are initially improved over time through temporal extension (TE), providing an accurate depiction of each learner’s implicit state at each time step. Subsequently, we propose a stochastic temporal pooling (STP) strategy to estimate the embedding sets of all evolving nodes. This effectively enhances model efficiency and usability. Following this, a heterogeneous aggregation network is devised to proficiently extract heterogeneous features from the HEN. This network employs both node-level and relation-level attention mechanisms to craft aggregated node features. To emphasize the superiority of TEGNN, we perform experiments on several real ITS datasets and show that our method significantly outperforms the state-of-the-art approaches. The experiments validate that TE serves as an efficient framework for modeling temporal information in GE, and STP not only accelerates the training process but also enhances the resultant accuracy. © 2023 Association for Computing Machinery. All rights reserved.",dynamic graph; graph embedding; heterogeneous information network; Intelligent education; knowledge tracing,Computer aided instruction; Data mining; Graph embeddings; Graph theory; Information services; Intelligent vehicle highway systems; Network embeddings; Semantics; Stochastic systems; Dynamic graph; Graph embeddings; Heterogeneous information; Heterogeneous information network; Information networks; Intelligent educations; Intelligent tutoring; Knowledge tracings; Temporal extensions; Tutoring system; Graph neural networks
DA-DAN: A Dual Adversarial Domain Adaption Network for Unsupervised Non-overlapping Cross-domain Recommendation,2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181715491&doi=10.1145%2f3617825&partnerID=40&md5=6da2624a7f8e9ebc78241a13b6f22e57,"Unsupervised Non-overlapping Cross-domain Recommendation (UNCR) is the task that recommends source domain items to the target domain users, which is more challenging as the users are non-overlapped, and its learning process is unsupervised. UNCR is still unsolved due to the following: (1) Previous studies need extra auxiliary information to learn transferable features when aligning two domains, which is unrealistic and hard to obtain due to privacy concerns. (2) Since the adoption of the shared network, existing works cannot well eliminate the domain-specific features in the common feature space, which may incorporate domain noise and harm the cross-domain recommendation. In this work, we propose a domain adaption-based method, namely DA-DAN, to address the above challenges. Specifically, to let DA-DAN be free of auxiliary information, we learn users’ preferences by only exploring their sequential patterns, and propose an improved self-attention layer to model them. To well eliminate the domain-specific features from the common feature space, we resort to a dual generative adversarial network with a multi-target adversarial loss, where two generators and discriminators are leveraged to model each domain separately. Experimental results on three real-world datasets demonstrate the advantage of DA-DAN compared with the state-of-the-art recommendation baselines. Moreover, our source codes have been publicly released.1 © 2023 Association for Computing Machinery. All rights reserved.",Cross-domain recommendation; sequential recommendation; unsupervised cross-domain recommendation,Auxiliary information; Common features; Cross-domain recommendations; Domain adaptions; Domain specific; Feature space; Learn+; Sequential recommendation; Unsupervised cross-domain recommendation; Generative adversarial networks
A Diffusion Model for POI Recommendation,2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181502566&doi=10.1145%2f3624475&partnerID=40&md5=a524383011694f67b538c5cd3e5c86b7,"Next Point-of-Interest (POI) recommendation is a critical task in location-based services that aim to provide personalized suggestions for the user’s next destination. Previous works on POI recommendation have laid focus on modeling the user’s spatial preference. However, existing works that leverage spatial information are only based on the aggregation of users’ previous visited positions, which discourages the model from recommending POIs in novel areas. This trait of position-based methods will harm the model’s performance in many situations. Additionally, incorporating sequential information into the user’s spatial preference remains a challenge. In this article, we propose Diff-POI: a Diff usion-based model that samples the user’s spatial preference for the next POI recommendation. Inspired by the wide application of diffusion algorithm in sampling from distributions, Diff-POI encodes the user’s visiting sequence and spatial character with two tailor-designed graph encoding modules, followed by a diffusion-based sampling strategy to explore the user’s spatial visiting trends. We leverage the diffusion process and its reverse form to sample from the posterior distribution and optimized the corresponding score function. We design a joint training and inference framework to optimize and evaluate the proposed Diff-POI. Extensive experiments on four real-world POI recommendation datasets demonstrate the superiority of our Diff-POI over state-of-the-art baseline methods. Further ablation and parameter studies on Diff-POI reveal the functionality and effectiveness of the proposed diffusion-based sampling strategy for addressing the limitations of existing methods. © 2023 Association for Computing Machinery. All rights reserved.",diffusion model; graph neural network; Next POI recommendation,Encoding (symbols); Graph neural networks; Location based services; Recommender systems; Telecommunication services; User profile; Critical tasks; Diffusion algorithm; Diffusion model; Graph neural networks; Location-based services; Next point-of-interest recommendation; Performance; Sampling strategies; Sequential information; Spatial informations; Diffusion
SetRank: A Setwise Bayesian Approach for Collaborative Ranking in Recommender System,2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181731380&doi=10.1145%2f3626194&partnerID=40&md5=61855b350d53d3bb91acf1903c0ccaf0,"The recent development of recommender systems has a focus on collaborative ranking, which provides users with a sorted list rather than rating prediction. The sorted item lists can more directly reflect the preferences for users and usually perform better than rating prediction in practice. While considerable efforts have been made in this direction, the well-known pairwise and listwise approaches have still been limited by various challenges. Specifically, for the pairwise approaches, the assumption of independent pairwise preference is not always held in practice. Also, the listwise approaches cannot efficiently accommodate “ties” and unobserved data due to the precondition of the entire list permutation. To this end, in this article, we propose a novel setwise Bayesian approach for collaborative ranking, namely, SetRank, to inherently accommodate the characteristics of user feedback in recommender systems. SetRank aims to maximize the posterior probability of novel setwise preference structures and three implementations for SetRank are presented. We also theoretically prove that the bound of excess risk in SetRank can be proportional to √M/N, where M and N are the numbers of items and users, respectively. Finally, extensive experiments on four real-world datasets clearly validate the superiority of SetRank compared with various state-of-the-art baselines. © 2023 Association for Computing Machinery. All rights reserved.",bayesian; collaborative ranking; explicit feedback; implicit feedback; Recommender system,Bayesian networks; Bayesian; Bayesian approaches; Collaborative ranking; Explicit feedback; Feedback in recommender systems; Implicit feedback; Posterior probability; Preference structures; Real-world datasets; User feedback; Recommender systems
Alleviating Video-length Effect for Micro-video Recommendation,2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181533662&doi=10.1145%2f3617826&partnerID=40&md5=c867ec0c1cccae2f2bf57935c7a7ccc7,"Micro-video platforms such as TikTok are extremely popular nowadays. One important feature is that users no longer select interested videos from a set; instead, they either watch the recommended video or skip to the next one. As a result, the time length of users’ watching behavior becomes the most important signal for identifying preferences. However, our empirical data analysis has shown a video-length effect that long videos can more easily receive a higher value of average view time, and thus adopting such view-time labels for measuring user preferences can easily induce a biased model that favors the longer videos. In this article, we propose a Video Length Debiasing Recommendation (VLDRec) method to alleviate such an effect for micro-video recommendation. VLDRec designs the data labeling approach and the sample generation module that better capture user preferences in a view-time-oriented manner. It further leverages the multi-task learning technique to jointly optimize the above samples with the original biased ones. Extensive experiments show that VLDRec can improve users’ view time by 1.81% and 11.32% on two real-world datasets, given a recommendation list of a fixed overall video length, compared with the best baseline method. Moreover, VLDRec is also more effective in matching users’ interests in terms of the video content. © 2023 Association for Computing Machinery. All rights reserved.",Debias; micro-video recommendation; multi-task learning,De-biasing; Debias; Empirical data; Important features; Length effects; Micro-video recommendation; Multitask learning; Time length; User's preferences; Video-platforms; Learning systems
Multi-aspect Graph Contrastive Learning for Review-enhanced Recommendation,2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181664994&doi=10.1145%2f3618106&partnerID=40&md5=5485ed29fc02f8245b4e801f3046fa12,"Review-based recommender systems explore semantic aspects of users’ preferences by incorporating user-generated reviews into rating-based models. Recent works have demonstrated the potential of review information to improve the recommendation capacity. However, most existing studies rely on optimizing review-based representation learning part, thus failing to explicitly capture the fine-grained semantic aspects, and also ignoring the intrinsic correlation between ratings and reviews. To address these problems, we propose a multi-aspect graph contrastive learning framework, named MAGCL, with three distinctive designs: (i) a multi-aspect representation learning module, which projects semantic relations to different subspaces by decoupling review information, and then obtains high-order decoupled representations in each aspect via graph encoder. (ii) the contrastive learning module performs graph contrastive learning to capture the correlation between rating and review patterns, which utilize unlabeled data to generate self-supervised signals and, in turn, relieve the data sparsity problem of supervision signals. (iii) the multi-task learning module conducts joint training to learn high-order structure-aware yet self-discriminative node representations by combining recommendation task and self-supervised task, which helps alleviate the over-smoothing problem. Extensive experiments are conducted on four real-world review datasets and the results show the superiority of the proposed framework MAGCL compared with several state of the arts. We also provide further analysis on multi-aspect representations and graph contrastive learning to verify the advantage of proposed framework. © 2023 Association for Computing Machinery. All rights reserved.",graph contrastive learning; Multi-aspect representations; multi-task learning; self-supervised signals,Arts computing; Graph neural networks; Learning systems; Aspect-graph; Fine grained; Graph contrastive learning; Learning modules; Multi aspects; Multi-aspect representation; Multitask learning; Self-supervised signal; User's preferences; User-generated; Semantics
Hierarchical Transformer with Spatio-temporal Context Aggregation for Next Point-of-interest Recommendation,2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181535550&doi=10.1145%2f3597930&partnerID=40&md5=30d851067f7fdb7ad81556dc4124616f,"Next point-of-interest (POI) recommendation is a critical task in location-based social networks, yet remains challenging due to a high degree of variation and personalization exhibited in user movements. In this work, we explore the latent hierarchical structure composed of multi-granularity short-term structural patterns in user check-in sequences. We propose a Spatio-Temporal context AggRegated Hierarchical Transformer (STAR-HiT) for next POI recommendation, which employs stacked hierarchical encoders to recursively encode the spatio-temporal context and explicitly locate subsequences of different granularities. More specifically, in each encoder, the global attention layer captures the spatio-temporal context of the sequence, while the local attention layer performed within each subsequence enhances subsequence modeling using the local context. The sequence partition layer infers positions and lengths of subsequences from the global context adaptively, such that semantics in subsequences can be well preserved. Finally, the subsequence aggregation layer fuses representations within each subsequence to form the corresponding subsequence representation, thereby generating a new sequence of higher-level granularity. The stacking of hierarchical encoders captures the latent hierarchical structure of the check-in sequence, which is used to predict the next visiting POI. Extensive experiments on three public datasets demonstrate that the proposed model achieves superior performance while providing explanations for recommendations. © 2023 Association for Computing Machinery. All rights reserved.",context modeling; hierarchical transformer; Next point-of-interest recommendation; spatio-temporal data mining,Data mining; Recommender systems; Signal encoding; User profile; Websites; Check-in; Context models; Critical tasks; Hierarchical structures; Hierarchical transformer; Location-based social networks; Next point-of-interest recommendation; Personalizations; Spatio-temporal; Spatio-temporal data mining; Semantics
Towards Efficient Coarse-grained Dialogue Response Selection,2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181538476&doi=10.1145%2f3597609&partnerID=40&md5=a57c5f338faa6069056c7a64ac332653,"oarse-grained response selection is a fundamental and essential subsystem for the widely used retrievalbased chatbots, aiming to recall a coarse-grained candidate set from a large-scale dataset. The dense retrievaltechnique has recently been proven very effective in building such a subsystem. However, dialogue denseretrieval models face two problems in real scenarios: (1) the multi-turn dialogue history is re-computed ineach turn, leading to inefficient inference; (2) the index storage of the offline index is enormous, significantlyincreasing the deployment cost. To address these problems, we propose an efficient coarse-grained responseselection subsystem consisting of two novel methods. Specifically, to address the first problem, we proposethe Hierarchical Dense Retrieval. It caches rich multi-vector representations of the dialogue history and onlyencodes the latest user’s utterance, leading to better inference efficiency. Then, to address the second problem,we design the Deep Semantic Hashing to reduce the index storage while effectively saving its recall accuracynotably. Extensive experimental results prove the advantages of the two proposed methods over previousworks. Specifically, with the limited performance loss, our proposed coarse-grained response selection modelachieves over 5x FLOPs speedup and over 192x storage compression ratio. Moreover, our source codes havebeen publicly released. © 2023 Association for Computing Machinery. All rights reserved.",deep semantic hashing; dense retrieval; Retrieval-based dialogue system,Coarse-grained modeling; Large dataset; Speech processing; Candidate sets; Chatbots; Coarse-grained; Deep semantic hashing; Dense retrieval; Dialogue systems; In-buildings; Large-scale datasets; Response selection; Retrieval-based dialog system; Semantics
A Dual-branch Learning Model with Gradient-balanced Loss for Long-tailed Multi-label Text Classification,2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181535469&doi=10.1145%2f3597416&partnerID=40&md5=c5fbf4333e5b811f83eeea2afdb11239,"Multi-label text classification has a wide range of applications in the real world. However, the data distribution in the real world is often imbalanced, which leads to serious long-tailed problems. For multi-label classification, due to the vast scale of datasets and existence of label co-occurrence, how to effectively improve the prediction accuracy of tail labels without degrading the overall precision becomes an important challenge. To address this issue, we propose A Dual-Branch Learning Model with Gradient-Balanced Loss (DBGB) based on the paradigm of existing pre-trained multi-label classification SOTA models. Our model consists of two main long-tailed module improvements. First, with the shared text representation, the dual-classifier is leveraged to process two kinds of label distributions; one is the original data distribution and the other is the under-sampling distribution for head labels to strengthen the prediction for tail labels. Second, the proposed gradient-balanced loss can adaptively suppress the negative gradient accumulation problem related to labels, especially tail labels. We perform extensive experiments on three multi-label text classification datasets. The results show that the proposed method achieves competitive performance on overall prediction results compared to the state-of-the-art methods in solving the multi-label classification, with significant improvement on tail-label accuracy. © 2023 Association for Computing Machinery. All rights reserved.",dual-branch structure; long-tailed learning; Multi-label text classification; re-weighting loss function,Classification (of information); Learning systems; Text processing; Balanced loss; Branch structure; Dual-branch structure; Learning models; Long-tailed learning; Loss functions; Multi-label classifications; Multi-label text classification; Re-weighting; Re-weighting loss function; Forecasting
Contextualized Knowledge Graph Embedding for Explainable Talent Training Course Recommendation,2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85165636537&doi=10.1145%2f3597022&partnerID=40&md5=203a3b2760ce14aae74a718a7b0e1c9a,"Learning and development, or L&D, plays an important role in talent management, which aims to improve the knowledge and capabilities of employees through a variety of performance-oriented training activities. Recently, with the rapid development of enterprise management information systems, many research efforts and industrial practices have been devoted to building personalized employee training course recommender systems. Nevertheless, a widespread challenge is how to provide explainable recommendations with the consideration of different learning motivations from talents. To this end, we propose CKGE, a contextualized knowledge graph (KG) embedding approach for developing an explainable training course recommender system. A novel perspective of CKGE is to integrate both the contextualized neighbor semantics and high-order connections as motivation-aware information for learning effective representations of talents and courses. Specifically, in CKGE, for each entity pair (i.e., the talent-course pair), we first construct a meta-graph, including the neighbors of each entity and the meta-paths between entities as motivation-aware information. Then, we develop a novel KG-based Transformer, which can serialize entities and paths in the meta-graph as a sequential input, with the specially designed relational attention and structural encoding mechanisms to better model the global dependence of KG structured data. Meanwhile, the local path mask prediction can effectively reveal the importance of different paths. As a result, CKGE not only can make precise predictions but also can discriminate the saliencies of meta-paths in characterizing corresponding preferences. Extensive experiments on real-world and public datasets clearly validate the effectiveness and interpretability of CKGE compared with state-of-the-art baselines. © 2023 Association for Computing Machinery. All rights reserved.",Course recommendation; knowledge graph; Transformer,Curricula; Graph theory; Human resource management; Industrial research; Information management; Knowledge graph; Personnel training; Recommender systems; Semantics; Contextualized knowledge; Course recommendation; Graph embeddings; Knowledge graphs; Meta-graph; Performance-oriented; Talent management; Talent trainings; Training course; Transformer; Motivation
Syntactic-Informed Graph Networks for Sentence Matching,2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181534539&doi=10.1145%2f3609795&partnerID=40&md5=20f1d07e2a01ba9d9fc86a547d0779d3,"Matching two natural language sentences is a fundamental problem in both natural language processing and information retrieval. Preliminary studies have shown that the syntactic structures help improve the matching accuracy, and different syntactic structures in natural language are complementary to sentence semantic understanding. Ideally, a matching model would leverage all syntactic information. Existing models, however, are only able to combine limited (usually one) types of syntactic information due to the complex and heterogeneous nature of the syntactic information. To deal with the problem, we propose a novel matching model, which formulates sentence matching as a representation learning task on a syntactic-informed heterogeneous graph. The model, referred to as SIGN (Syntactic-Informed Graph Network), first constructs a heterogeneous matching graph based on the multiple syntactic structures of two input sentences. Then the graph attention network algorithm is applied to the matching graph to learn the high-level representations of the nodes. With the help of the graph learning framework, the multiple syntactic structures, as well as the word semantics, can be represented and interacted in the matching graph and therefore collectively enhance the matching accuracy. We conducted comprehensive experiments on three public datasets. The results demonstrate that SIGN outperforms the state of the art and also can discriminate the sentences in an interpretable way. © 2023 Association for Computing Machinery. All rights reserved.",graph learning; Sentence matching; syntactic structures,Graph Databases; Graph neural networks; Graphic methods; Learning algorithms; Learning systems; Natural language processing systems; Semantics; Graph learning; Graph networks; Language informations; Matching graph; Matching models; Matchings; Natural languages; Sentence matching; Syntactic information; Syntactic structure; Syntactics
Toward Best Practices for Training Multilingual Dense Retrieval Models,2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181531658&doi=10.1145%2f3613447&partnerID=40&md5=cdbb76fff7f14531ae2b43df6f223a05,"Dense retrieval models using a transformer-based bi-encoder architecture have emerged as an active area of research. In this article, we focus on the task of monolingual retrieval in a variety of typologically diverse languages using such an architecture. Although recent work with multilingual transformers demonstrates that they exhibit strong cross-lingual generalization capabilities, there remain many open research questions, which we tackle here. Our study is organized as a “best practices” guide for training multilingual dense retrieval models, broken down into three main scenarios: when a multilingual transformer is available, but training data in the form of relevance judgments are not available in the language and domain of interest (“have model, no data”); when both models and training data are available (“have model and data”); and when training data are available but not models (“have data, no model”). In considering these scenarios, we gain a better understanding of the role of multi-stage fine-tuning, the strength of cross-lingual transfer under various conditions, the usefulness of out-of-language data, and the advantages of multilingual vs. monolingual transformers. Our recommendations offer a guide for practitioners building search applications, particularly for low-resource languages, and while our work leaves open a number of research questions, we provide a solid foundation for future work. © 2023 Association for Computing Machinery. All rights reserved.",Dense retrieval; multiliingual retrieval,Active area; Best practices; Cross-lingual; Dense retrieval; Encoder architecture; Generalization capability; Multiliingual retrieval; Research questions; Retrieval models; Training data
PARADE: Passage Representation Aggregation for Document Reranking,2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85172903667&doi=10.1145%2f3600088&partnerID=40&md5=aae5eea801561f2ccbf8950d7853257d,"Pre-trained transformer models, such as BERT and T5, have shown to be highly effective at ad hoc passage and document ranking. Due to the inherent sequence length limits of these models, they need to process document passages one at a time rather than processing the entire document sequence at once. Although several approaches for aggregating passage-level signals into a document-level relevance score have been proposed, there has yet to be an extensive comparison of these techniques. In this work, we explore strategies for aggregating relevance signals from a document’s passages into a final ranking score. We find that passage representation aggregation techniques can significantly improve over score aggregation techniques proposed in prior work, such as taking the maximum passage score. We call this new approach PARADE. In particular, PARADE can significantly improve results on collections with broad information needs where relevance signals can be spread throughout the document (such as TREC Robust04 and GOV2). Meanwhile, less complex aggregation techniques may work better on collections with an information need that can often be pinpointed to a single passage (such as TREC DL and TREC Genomics). We also conduct efficiency analyses and highlight several strategies for improving transformer-based aggregation. © 2023 Association for Computing Machinery. All rights reserved.",Document reranking; passage representation aggregation; pre-trained language models,Genome; Information retrieval; Aggregation techniques; Document ranking; Document reranking; Language model; Passage representation aggregation; Pre-trained language model; Re-ranking; Relevance score; Sequence lengths; Transformer modeling; Search engines
Document-level Relation Extraction via Separate Relation Representation and Logical Reasoning,2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176765271&doi=10.1145%2f3597610&partnerID=40&md5=34aad1ef966eae0ea14b123d34bf0df7,"Document-level relation extraction (RE) extends the identification of entity/mentions' relation from the single sentence to the long document. It is more realistic and poses new challenges to relation representation and reasoning skills. In this article, we propose a novel model, SRLR, using Separate Relation Representation and Logical Reasoning considering the indirect relation representation and complex reasoning of evidence sentence problems. Specifically, we first expand the judgment of relational facts from the entity-level to the mention-level, highlighting fine-grained information to capture the relation representation for the entity pair. Second, we propose a logical reasoning module to identify evidence sentences and conduct relational reasoning. Extensive experiments on two publicly available benchmark datasets demonstrate the effectiveness of our proposed SRLR as compared to 19 baseline models. Further ablation study also verifies the effects of the key components.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Document-level Relation Extraction; Logical Reasoning; Mention-level; Separate Relation Representation,Baseline models; Benchmark datasets; Document-level relation extraction; Entity-level; Fine grained; Logical reasoning; Mention-level; Relation extraction; Relational reasoning; Separate relation representation; Extraction
A Multi-channel Next POI Recommendation Framework with Multi-granularity Check-in Signals,2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176748524&doi=10.1145%2f3592789&partnerID=40&md5=5eb7feb705395c4f9e685182e1b7435f,"Current study on next point-of-interest (POI) recommendation mainly explores user sequential transitions with the fine-grained individual-user POI check-in trajectories only, which suffers from the severe check-in data sparsity issue. In fact, coarse-grained signals (i.e., region- and global-level check-ins) in such sparse check-ins would also benefit to augment user preference learning. Specifically, our data analysis unveils that user movement exhibits noticeable patterns w.r.t. the regions of visited POIs. Meanwhile, the global all-user check-ins can help reflect sequential regularities shared by the crowd. We are, therefore, inspired to propose the MCMG: a Multi-Channel next POI recommendation framework with Multi-Granularity signals categorized from two orthogonal perspectives, i.e., fine-coarse grained check-ins at either POI/region level or local/global level. The MCMG is equipped with three modules, namely, global user behavior encoder, local multi-channel (i.e., region, category, and POI channels) encoder, and region-aware weighting strategy. Such design enables MCMG to be capable of capturing both fine- and coarse-grained sequential regularities as well as exploring the dynamic impact of multi-channel by differentiating the check-in patterns w.r.t. visited regions. Extensive experiments on four real-world datasets show that our MCMG significantly outperforms state-of-the-art next POI recommendation approaches. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",geographical region; graph neural network; multi-channel encoder; multi-granularity; PhrasesNext POI recommendation; self-attention,Behavioral research; Channel coding; Data mining; Network coding; Recommender systems; 'current; Check-in; Coarse-grained; Fine grained; Graph neural networks; Multi channel; Multi-channel encoder; Multi-granularity; Phrasesnext point-of-interest recommendation; Self-attention; Graph neural networks
A Versatile Framework for Evaluating Ranked Lists in Terms of Group Fairness and Relevance,2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176738836&doi=10.1145%2f3589763&partnerID=40&md5=c48e65dcecd10519cf9182b0991e6eea,"We present a simple and versatile framework for evaluating ranked lists in terms of Group Fairness and Relevance, in which the groups (i.e., possible attribute values) can be either nominal or ordinal in nature. First, we demonstrate that when our framework is applied to a binary hard group membership setting, our Group Fairness and Relevance (GFR) measures can easily quantify the overall polarity of each ranked list. Second, by utilising an existing diversified search test collection and treating each intent as an attribute value, we demonstrate that our framework can also handle soft group membership and that the GFR measures are highly correlated with a diversified information retrieval (IR) measure in this context as well. Third, using real data from a Japanese local search service, we demonstrate how our framework enables researchers to study intersectional group fairness based on multiple attribute sets. We also show that the similarity function for comparing the achieved and target distributions over the attribute values should be chosen carefully when the attribute values are ordinal. For such situations, our recommendation is to use multiple similarity functions with our framework: for example, one based on Jensen-Shannon Divergence (which disregards the ordinal nature of the groups) and another based on Root Normalised Order-aware Divergence (which has been designed specifically for handling ordinal groups). In addition, we highlight the fundamental differences between our framework and Attention-Weighted Rank Fairness (AWRF), a group fairness measure used at the TREC Fair Ranking Track.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Evaluation; evaluation measures; fairness; group fairness; ordinal quantification,Information retrieval; Attribute values; Evaluation; Evaluation measures; Fairness; Fairness measures; Group fairness; Group memberships; Ordinal quantification; Relevance measure; Similarity functions; Function evaluation
Understanding Diversity in Session-based Recommendation,2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176725511&doi=10.1145%2f3600226&partnerID=40&md5=6805987a10299f954014005616c9b567,"Current session-based recommender systems (SBRSs) mainly focus on maximizing recommendation accuracy, while few studies have been devoted to improve diversity beyond accuracy. Meanwhile, it is unclear how the accuracy-oriented SBRSs perform in terms of diversity. In addition, the asserted ""tradeoff""relationship between accuracy and diversity has been increasingly questioned in the literature. Toward the aforementioned issues, we conduct a holistic study to particularly examine the recommendation performance of representative SBRSs w.r.t. both accuracy and diversity, striving for better understanding of the diversity-related issues for SBRSs and providing guidance on designing diversified SBRSs. Particularly, for a fair and thorough comparison, we deliberately select state-of-the-art non-neural, deep neural, and diversified SBRSs by covering more scenarios with appropriate experimental setups, e.g., representative datasets, evaluation metrics, and hyper-parameter optimization technique. The source code can be obtained via github.com/qyin863/Understanding-Diversity-in-SBRSs. Our empirical results unveil that (1) non-diversified methods can also obtain satisfying performance on diversity, which can even surpass diversified ones, and (2) the relationship between accuracy and diversity is quite complex. Besides the ""tradeoff""relationship, they can be positively correlated with each other, that is, having a same-trend (win-win or lose-lose) relationship, which varies across different methods and datasets. Additionally, we further identify three possible influential factors on diversity in SBRSs (i.e., granularity of item categorization, session diversity of datasets, and length of recommendation lists) and offer an intuitive guideline and a potential solution regarding learned item embeddings for more effective session-based recommendation. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",diversification; diversified recommendation; PhrasesRecommender systems; session-based recommendation,'current; Diversification; Diversified recommendation; Evaluation metrics; Phrasesrecommende system; Recommendation accuracy; Recommendation performance; Session-based recommendation; State of the art; Trade-off relationship; Recommender systems
"On the Ordering of Pooled Web Pages, Gold Assessments, and Bronze Assessments",2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176749836&doi=10.1145%2f3600227&partnerID=40&md5=5510f4b9d4073250c57c882483569c82,"The present study leverages a recent opportunity we had to create a new English web search test collection for the NTCIR-16 We Want Web (WWW-4) task, which concluded in June 2022. More specifically, through the test collection construction effort, we examined two factors that may affect the relevance assessments of depth-k pools, which in turn may affect the relative evaluation of different IR systems. The first factor is the document ordering strategy for the assessors, namely, prioritisation (PRI) and randomisation (RND). PRI is a method that has been used in NTCIR tasks for over a decade; it ranks the pooled documents by a kind of pseudorelevance for the assessors. The second factor is assessor type, i.e., Gold or Bronze. Gold assessors are the topic creators and therefore they ""know""which documents are (highly) relevant and which are not; Bronze assessors are not the topic creators and may lack sufficient knowledge about the topics. We believe that our study is unique in that the authors of this article served as the Gold assessors when creating the WWW-4 test collection, which enabled us to closely examine why Bronze assessments differ from the Gold ones. Our research questions examine assessor efficiency (RQ1), inter-assessor agreement (RQ2), system ranking similarity with different qrels files (RQ3), system ranking robustness to the choice of test topics (RQ4), and the reasons why Bronze assessors tend to be more liberal than Gold assessors (RQ5). The most remarkable of our results are as follows: First, in the comparisons for RQ1 through RQ4, it turned out that what may matter more than the document ordering strategy (PRI vs. RND) and the assessor type (Gold vs. Bronze) is how well-motivated and/or well-trained the Bronze assessors are. Second, regarding RQ5, of the documents originally judged nonrelevant by the Gold assessors contrary to the Bronze assessors in our experiments, almost one half were truly relevant according to the Gold assessors' own reconsiderations. This result suggests that even Gold assessors are far from perfect; budget permitting, it may be beneficial to hire highly motivated Bronze assessors in addition to Gold assessors so they can complement each other.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Information retrieval; pooling; relevance assessments; test collections; web search,Bronze; Budget control; Efficiency; Gold; Websites; Construction efforts; Ordering strategy; Pooling; Prioritization; Randomisation; Relevance assessments; System rankings; Test Collection; Web searches; Web-page; Information retrieval
"M3GAT: A Multi-modal, Multi-task Interactive Graph Attention Network for Conversational Sentiment Analysis and Emotion Recognition",2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85175605040&doi=10.1145%2f3593583&partnerID=40&md5=d7e44936c4d99e8e434e83718795867c,"Sentiment and emotion, which correspond to long-term and short-lived human feelings, are closely linked to each other, leading to the fact that sentiment analysis and emotion recognition are also two interdependent tasks in natural language processing (NLP). One task often leverages the shared knowledge from another task and performs better when solved in a joint learning paradigm. Conversational context dependency, multi-modal interaction, and multi-task correlation are three key factors that contribute to this joint paradigm. However, none of the recent approaches have considered them in a unified framework. To fill this gap, we propose a multi-modal, multi-task interactive graph attention network, termed M3GAT, to simultaneously solve the three problems. At the heart of the model is a proposed interactive conversation graph layer containing three core sub-modules, which are: (1) local-global context connection for modeling both local and global conversational context, (2) cross-modal connection for learning multi-modal complementary and (3) cross-task connection for capturing the correlation across two tasks. Comprehensive experiments on three benchmarking datasets, MELD, MEISD, and MSED, show the effectiveness of M3GAT over state-of-the-art baselines with the margin of 1.88%, 5.37%, and 0.19% for sentiment analysis, and 1.99%, 3.65%, and 0.13% for emotion recognition, respectively. In addition, we also show the superiority of multi-task learning over the single-task framework.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",emotion recognition; graph neural network; Multi-modal sentiment analysis; multi-task learning,Graph neural networks; Learning systems; Modal analysis; Speech recognition; Emotion recognition; Graph neural networks; Human feelings; Inter-dependent tasks; Multi tasks; Multi-modal; Multi-modal sentiment analyse; Multitask learning; Natural languages; Sentiment analysis; Emotion Recognition
Invariant Node Representation Learning under Distribution Shifts with Multiple Latent Environments,2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176769312&doi=10.1145%2f3604427&partnerID=40&md5=636489c6f9e7e0d47d8779f16807a76c,"Node representation learning methods, such as graph neural networks, show promising results when testing and training graph data come from the same distribution. However, the existing approaches fail to generalize under distribution shifts when the nodes reside in multiple latent environments. How to learn invariant node representations to handle distribution shifts with multiple latent environments remains unexplored. In this article, we propose a novel Invariant Node representation Learning (INL) approach capable of generating invariant node representations based on the invariant patterns under distribution shifts with multiple latent environments by leveraging the invariance principle. Specifically, we define invariant and variant patterns as ego-subgraphs of each node and identify the invariant ego-subgraphs through jointly accounting for node features and graph structures. To infer the latent environments of nodes, we propose a contrastive modularity-based graph clustering method based on the variant patterns. We further propose an invariant learning module to learn node representations that can generalize to distribution shifts. We theoretically show that our proposed method can achieve guaranteed performance under distribution shifts. Extensive experiments on both synthetic and real-world node classification benchmarks demonstrate that our method greatly outperforms state-of-the-art baselines under distribution shifts.  © 2023 Copyright held by the owner/author(s).",distribution shift; Graph neural networks; node representation learning,Data mining; Graph structures; Graph theory; Learning systems; AS graph; Distribution shift; Graph data; Graph neural networks; Invariance principle; Learn+; Learning approach; Learning methods; Node representation learning; Subgraphs; Graph neural networks
Selective Query Processing: A Risk-Sensitive Selection of Search Configurations,2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176799917&doi=10.1145%2f3608474&partnerID=40&md5=9973a81bc6e45c1621e34f8855877b38,"In information retrieval systems, search parameters are optimized to ensure high effectiveness based on a set of past searches, and these optimized parameters are then used as the search configuration for all subsequent queries. A better approach, however, would be to adapt the parameters to fit the query at hand. Selective query expansion is one such an approach, in which the system decides automatically whether or not to expand the query, resulting in two possible search configurations. This approach was extended recently to include many other parameters, leading to many possible search configurations where the system automatically selects the best configuration on a per-query basis. One problem with this approach is the system training, which requires evaluation of each training query with every possible configuration. In real-world systems, so many parameters and possible values must be evaluated that this approach is impractical, especially when the system must be updated frequently, as is the case for commercial search engines. In general, the more configurations, the greater the effectiveness when configuration selection is appropriate but also the greater the risk of decreasing effectiveness in the case of an inappropriate configuration selection. To determine the ideal configurations to be used for each query in real-world systems, we have developed a method in which a limited number of possible configurations are pre-selected, then used in a meta-search engine that decides the best search configuration for each query. We define a risk-sensitive approach for configuration pre-selection that considers the risk-reward tradeoff between the number of configurations kept and system effectiveness. We define two alternative risk functions to apply to different goals. For final configuration selection, the decision is based on query feature similarities. We compare two alternative risk functions on two query types (ad hoc and diversity) and compare these to more sophisticated machine learning based methods. We find that a relatively small number of configurations (20) selected by our risk-sensitive model is sufficient to obtain results close to the best achievable results for each query. Effectiveness is increased by about 15% according to the P@10 and nDCG@10 evaluation metrics when compared to traditional grid search using a single configuration and by about 20% when compared to learning to rank documents. Our risk-sensitive approach works for both diversity- and ad hoc oriented searches. Moreover, the similarity-based selection method outperforms the more sophisticated approaches. Thus, we demonstrate the feasibility of developing per-query information retrieval systems, which will guide future research in this direction.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",adaptive information retrieval; Information retrieval; learning to rank; query driven parameterisation; risk-sensitive systems; search engine parameters,Function evaluation; Information retrieval; Information retrieval systems; Learning systems; Query processing; Adaptive information retrieval; Engine parameter; Information-retrieval systems; Query driven parameterization; Real-world system; Risk function; Risk-sensitive system; Search engine parameter; Search parameters; Sensitive systems; Search engines
Metaphorical User Simulators for Evaluating Task-oriented Dialogue Systems,2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176385059&doi=10.1145%2f3596510&partnerID=40&md5=c97c9971c33fac94468498ea38f9d31a,"Task-oriented dialogue systems (TDSs) are assessed mainly in an offline setting or through human evaluation. The evaluation is often limited to single-turn or is very time-intensive. As an alternative, user simulators that mimic user behavior allow us to consider a broad set of user goals to generate human-like conversations for simulated evaluation. Employing existing user simulators to evaluate TDSs is challenging as user simulators are primarily designed to optimize dialogue policies for TDSs and have limited evaluation capabilities. Moreover, the evaluation of user simulators is an open challenge.In this work, we propose a metaphorical user simulator for end-to-end TDS evaluation, where we define a simulator to be metaphorical if it simulates a user's analogical thinking in interactions with systems. We also propose a tester-based evaluation framework to generate variants, i.e., dialogue systems with different capabilities. Our user simulator constructs a metaphorical user model that assists the simulator in reasoning by referring to prior knowledge when encountering new items. We estimate the quality of simulators by checking the simulated interactions between simulators and variants. Our experiments are conducted using three TDS datasets. The proposed user simulator demonstrates better consistency with manual evaluation than an agenda-based simulator and a seq2seq model on three datasets; our tester framework demonstrates efficiency and has been tested on multiple tasks, such as conversational recommendation and e-commerce dialogues. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",evaluation; Task-oriented dialogue; user simulation,Speech processing; Dialogue systems; Evaluation; Human evaluation; Human like; Offline; Task-oriented; Task-oriented dialog; User behaviors; User goals; User simulation; Behavioral research
A Review Selection Method Based on Consumer Decision Phases in E-commerce,2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176777179&doi=10.1145%2f3587265&partnerID=40&md5=71b2baa7e2065d8f39a6ccd7f5852b0f,"A valuable small subset strategically selected from massive online reviews is beneficial to improve consumers' decision-making efficiency in e-commerce. Existing review selection methods primarily concentrate on the informativeness of reviews and aim to find a subset of reviews that can reflect the informational properties of the original review set. However, changes in consumers' review diets during the two-phase decision process are not fully considered. In this study, we propose a novel review selection problem of finding a diet-matched review subset with high diversity and representativeness, which can better adapt to consumers' review-diet conversion from attribute-oriented to experience-oriented reviews between two decision phases. A novel decision-phase-based review selection method named DPRS is further proposed, which involves two steps: review classification and review selection. In the review classification step, the probability of a review being attribute-oriented or experience-oriented is estimated by prior knowledge-aware attentive neural network. In the second step, a novel heuristic algorithm, namely, stepwise non-dominated selection with superiority strategy, is introduced to seek the solution to the review selection problem. Extensive experiments on a real-world dataset demonstrate that DPRS outperforms state-of-the-art methods in terms of both review classification and review selection. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",consumer decision process; diet matching; e-commerce; PhrasesReview selection,Classification (of information); Decision making; Electronic commerce; Consumer decision making; Consumer decision process; Consumer reviews; Diet matching; E- commerces; Matchings; Online reviews; Phrasesreview selection; Selection methods; Selection problems; Heuristic algorithms
Unifying Token- and Span-level Supervisions for Few-shot Sequence Labeling,2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176444756&doi=10.1145%2f3610403&partnerID=40&md5=b58e2ea1f511a1dacebc8b7ed7ecfc49,"Few-shot sequence labeling aims to identify novel classes based on only a few labeled samples. Existing methods solve the data scarcity problem mainly by designing token-level or span-level labeling models based on metric learning. However, these methods are only trained at a single granularity (i.e., either token-level or span-level) and have some weaknesses of the corresponding granularity. In this article, we first unify token- and span-level supervisions and propose a Consistent Dual Adaptive Prototypical (CDAP) network for few-shot sequence labeling. CDAP contains the token- and span-level networks, jointly trained at different granularities. To align the outputs of two networks, we further propose a consistent loss to enable them to learn from each other. During the inference phase, we propose a consistent greedy inference algorithm that first adjusts the predicted probability and then greedily selects non-overlapping spans with maximum probability. Extensive experiments show that our model achieves new state-of-the-art results on three benchmark datasets. All the code and data of this work will be released at https://github.com/zifengcheng/CDAP. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",few-shot learning; Few-shot sequence labeling; sequence labeling,Class-based; Data scarcity; Different granularities; Few-shot learning; Few-shot sequence labeling; Labelings; Metric learning; Model-based OPC; Sequence Labeling; Inference engines
Meta-CRS: A Dynamic Meta-Learning Approach for Effective Conversational Recommender System,2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176775972&doi=10.1145%2f3604804&partnerID=40&md5=4358c0229af0636dab64905321bf6ed4,"Conversational recommender system (CRS) enhances the recommender system by acquiring the latest user preference through dialogues, where an agent needs to decide ""whether to ask or recommend"", ""which attributes to ask"", and ""which items to recommend""in each round. To explore these questions, reinforcement learning is adopted in most CRS frameworks. However, existing studies somewhat ignore to consider the connection between the previous rounds and the current round of the conversation, which might lead to the lack of prior knowledge and inaccurate decisions. In this view, we propose to facilitate the connections between different rounds of conversations in a dialogue session through deep transformer-based multi-channel meta-reinforcement learning, so that the CRS agent can decide each action/decision based on previous states, actions, and their rewards. Besides, to better utilize a user's historical preferences, we propose a more dynamic and personalized graph structure to support the conversation module and the recommendation module. Experiment results on five real-world datasets and an online evaluation with real users in an industrial environment validate the improvement of our method over the state-of-the-art approaches and the effectiveness of our designs.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesConversational recommender system; dynamic graph; knowledge graph; meta learning; prior knowledge; reinforcement learning,Knowledge graph; Learning systems; Recommender systems; Additional key word and phrasesconversational recommende system; Conversational recommender systems; Dynamic graph; Key words; Knowledge graphs; Meta-learning approach; Metalearning; Prior-knowledge; Reinforcement learnings; User's preferences; Reinforcement learning
A Reusable Model-agnostic Framework for Faithfully Explainable Recommendation and System Scrutability,2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176424122&doi=10.1145%2f3605357&partnerID=40&md5=941a005d1ebb61155fda583ea5d066f8,"State-of-the-art industrial-level recommender system applications mostly adopt complicated model structures such as deep neural networks. While this helps with the model performance, the lack of system explainability caused by these nearly blackbox models also raises concerns and potentially weakens the users' trust in the system. Existing work on explainable recommendation mostly focuses on designing interpretable model structures to generate model-intrinsic explanations. However, most of them have complex structures, and it is difficult to directly apply these designs onto existing recommendation applications due to the effectiveness and efficiency concerns. However, while there have been some studies on explaining recommendation models without knowing their internal structures (i.e., model-agnostic explanations), these methods have been criticized for not reflecting the actual reasoning process of the recommendation model or, in other words, faithfulness. How to develop model-agnostic explanation methods and evaluate them in terms of faithfulness is mostly unknown. In this work, we propose a reusable evaluation pipeline for model-agnostic explainable recommendation. Our pipeline evaluates the quality of model-agnostic explanation from the perspectives of faithfulness and scrutability. We further propose a model-agnostic explanation framework for recommendation and verify it with the proposed evaluation pipeline. Extensive experiments on public datasets demonstrate that our model-agnostic framework is able to generate explanations that are faithful to the recommendation model. We additionally provide quantitative and qualitative study to show that our explanation framework could enhance the scrutability of blackbox recommendation model. With proper modification, our evaluation pipeline and model-agnostic explanation framework could be easily migrated to existing applications. Through this work, we hope to encourage the community to focus more on faithfulness evaluation of explainable recommender systems.  © 2023 Copyright held by the owner/author(s).",Explainable recommendation; faithfulness; scrutability,Database systems; Deep neural networks; Quality control; Recommender systems; Black box modelling; Complexes structure; Effectiveness and efficiencies; Explainable recommendation; Faithfulness; Internal structure; Modeling performance; Scrutability; State of the art; System applications; Pipelines
Training Robust Deep Collaborative Filtering Models via Adversarial Noise Propagation,2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176788371&doi=10.1145%2f3589000&partnerID=40&md5=6deb7d01e71f27c80b4f9c28fc021b47,"The recommendation performance of deep collaborative filtering models drops sharply under imperceptible adversarial perturbations. Some methods promote the robustness of recommendation systems by adversarial training. However, these methods only study shallow models and lack the exploration of deep models. Furthermore, the way these methods add adversarial noise to the weight parameters of users and items is not fully applicable to deep collaborative filtering models, because the adversarial noise is not sufficient to fully affect its network structure with multiple hidden layers. In this article, we propose a novel adversarial training framework, Random Layer-wise Adversarial Training (RAT), which trains a robust deep collaborative filtering model via adversarial noise propagation. Specifically, we inject adversarial noise into the output of the hidden layer in a random layer-wise manner. The adversarial noise propagates forward from the injected position to obtain more flexible model parameters during the adversarial training process. We validate the effectiveness of RAT on multilayer perceptron (MLP) and implement RAT on MLP-based and convolutional neural networks-based deep collaborative filtering models. Experiments on three publicly available datasets show that the deep collaborative filtering model trained by RAT not only defends against adversarial noise but also guarantees recommendation performance.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",adversarial training; deep collaborative filtering; Recommendation systems,Backpropagation; Collaborative filtering; Deep neural networks; Multilayer neural networks; Rats; Adversarial training; Deep collaborative filtering; Filtering models; Hidden layers; Layer-wise; Multilayers perceptrons; Network structures; Noise propagation; Recommendation performance; Weight parameters; Recommender systems
MAN: Memory-augmented Attentive Networks for Deep Learning-based Knowledge Tracing,2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176794401&doi=10.1145%2f3589340&partnerID=40&md5=2ee472ec1fcf0ec1846e129744ed4730,"Knowledge Tracing (KT) is the task of modeling a learner's knowledge state to predict future performance in e-learning systems based on past performance. Deep learning-based methods, such as recurrent neural networks, memory-augmented neural networks, and attention-based neural networks, have recently been used in KT. Such methods have demonstrated excellent performance in capturing the latent dependencies of a learner's knowledge state on recent exercises. However, these methods have limitations when it comes to dealing with the so-called Skill Switching Phenomenon (SSP), i.e., when learners respond to exercises in an e-learning system, the latent skills in the exercises typically switch irregularly. SSP will deteriorate the performance of deep learning-based approaches for simulating the learner's knowledge state during skill switching, particularly when the association between the switching skills and the previously learned skills is weak. To address this problem, we propose the Memory-augmented Attentive Network (MAN), which combines the advantages of memory-augmented neural networks and attention-based neural networks. Specifically, in MAN, memory-augmented neural networks are used to model learners' longer term memory knowledge, while attention-based neural networks are used to model learners' recent term knowledge. In addition, we design a context-aware attention mechanism that automatically weighs the tradeoff between these two types of knowledge. With extensive experiments on several e-learning datasets, we show that MAN effectively improve predictive accuracies of existing state-of-the-art DLKT methods.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",deep learning; E-learning; knowledge tracing; memory-augmented neural network; multi-head attention mechanism,Learning systems; Recurrent neural networks; Attention mechanisms; Deep learning; E - learning; Knowledge state; Knowledge tracings; Memory-augmented neural network; Multi-head attention mechanism; Network memory; Neural-networks; Performance; E-learning
The Impact of Judgment Variability on the Consistency of Offline Effectiveness Measures,2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176784837&doi=10.1145%2f3596511&partnerID=40&md5=616fae66d2e62e561e097de434b1066d,"Measurement of the effectiveness of search engines is often based on use of relevance judgments. It is well known that judgments can be inconsistent between judges, leading to discrepancies that potentially affect not only scores but also system relativities and confidence in the experimental outcomes. We take the perspective that the relevance judgments are an amalgam of perfect relevance assessments plus errors; making use of a model of systematic errors in binary relevance judgments that can be tuned to reflect the kind of judge that is being used, we explore the behavior of measures of effectiveness as error is introduced. Using a novel methodology in which we examine the distribution of ""true""effectiveness measurements that could be underlying measurements based on sets of judgments that include error, we find that even moderate amounts of error can lead to conclusions such as orderings of systems that statistical tests report as significant but are nonetheless incorrect. Further, in these results the widely used recall-based measures AP and NDCG are notably more fragile in the presence of judgment error than is the utility-based measure RBP, but all the measures failed under even moderate error rates. We conclude that knowledge of likely error rates in judgments is critical to interpretation of experimental outcomes.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Evaluation; relevance assessment; significance testing,Systematic errors; Binary relevances; Effectiveness measure; Error rate; Evaluation; Measure of effectiveness; Measurements of; Offline; Relevance assessments; Relevance judgement; Significance testing; Search engines
Learning from Hierarchical Structure of Knowledge Graph for Recommendation,2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176424985&doi=10.1145%2f3595632&partnerID=40&md5=d08f7906934f57d58ad981b10c69c095,"Knowledge graphs (KGs) can help enhance recommendations, especially for the data-sparsity scenarios with limited user-item interaction data. Due to the strong power of representation learning of graph neural networks (GNNs), recent works of KG-based recommendation deploy GNN models to learn from both knowledge graph and user-item bipartite interaction graph. However, these works have not well considered the hierarchical structure of knowledge graph, leading to sub-optimal results. Despite the benefit of hierarchical structure, leveraging it is challenging since the structure is always partly-observed. In this work, we first propose to reveal unknown hierarchical structures with a supervised signal detection method and then exploit the hierarchical structure with disentangling representation learning. We conduct experiments on two large-scale datasets, of which the results well verify the superiority and rationality of the proposed method. Further experiments of ablation study with respect to key model designs have demonstrated the effectiveness and rationality of our proposed model. The code is available at https://github.com/tsinghua-fib-lab/HIKE. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",disentangled representation learning; graph neural network; knowledge graph; Personalized recommendation,Knowledge graph; Large dataset; Learning systems; Data sparsity; Disentangled representation learning; Graph neural networks; Graph-based; Hierarchical structures; Knowledge graphs; Neural network model; Personalized recommendation; Power; Structure of knowledge; Graph neural networks
CIRS: Bursting Filter Bubbles by Counterfactual Interactive Recommender System,2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85175599739&doi=10.1145%2f3594871&partnerID=40&md5=e2436d571627fcd1c4fa71df4d409adc,"While personalization increases the utility of recommender systems, it also brings the issue of filter bubbles. e.g., if the system keeps exposing and recommending the items that the user is interested in, it may also make the user feel bored and less satisfied. Existing work studies filter bubbles in static recommendation, where the effect of overexposure is hard to capture. In contrast, we believe it is more meaningful to study the issue in interactive recommendation and optimize long-term user satisfaction. Nevertheless, it is unrealistic to train the model online due to the high cost. As such, we have to leverage offline training data and disentangle the causal effect on user satisfaction. To achieve this goal, we propose a counterfactual interactive recommender system (CIRS) that augments offline reinforcement learning (offline RL) with causal inference. The basic idea is to first learn a causal user model on historical data to capture the overexposure effect of items on user satisfaction. It then uses the learned causal user model to help the planning of the RL policy. To conduct evaluation offline, we innovatively create an authentic RL environment (KuaiEnv) based on a real-world fully observed user rating dataset. The experiments show the effectiveness of CIRS in bursting filter bubbles and achieving long-term success in interactive recommendation. The implementation of CIRS is available via https://github.com/chongminggao/ CIRS-codes.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",causal inference; Filter bubble; interactive recommendation; offline reinforcement learning,HTTP; Recommender systems; Causal inferences; Counterfactuals; Filter bubble; Interactive recommendation; Offline; Offline reinforcement learning; Personalizations; Reinforcement learnings; User Modelling; Users' satisfactions; Reinforcement learning
Neural Architecture Search for GNN-Based Graph Classification,2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174074424&doi=10.1145%2f3584945&partnerID=40&md5=7ab45170081003ae424bdd34193570dd,"Graph classification is an important problem with applications across many domains, for which graph neural networks (GNNs) have been state-of-the-art (SOTA) methods. In the literature, to adopt GNNs for the graph classification task, there are two groups of methods: global pooling and hierarchical pooling. The global pooling methods obtain the graph representation vectors by globally pooling all of the node embeddings together at the end of several GNN layers, whereas the hierarchical pooling methods provide one extra pooling operation between the GNN layers to extract hierarchical information and improve the graph representations. Both global and hierarchical pooling methods are effective in different scenarios. Due to highly diverse applications, it is challenging to design data-specific pooling methods with human expertise. To address this problem, we propose PAS (Pooling Architecture Search) to design adaptive pooling architectures by using the neural architecture search (NAS). To enable the search space design, we propose a unified pooling framework consisting of four modules: Aggregation, Pooling, Readout, and Merge. Two variants, PAS-G and PAS-NE, are provided to design the pooling operations in different scales. A set of candidate operations is designed in the search space using this framework. Then, existing human-designed pooling methods, including global and hierarchical ones, can be incorporated. To enable efficient search, a coarsening strategy is developed to continuously relax the search space, and then a differentiable search method can be adopted. We conduct extensive experiments on six real-world datasets, including the large-scale datasets MR and ogbg-molhiv. Experimental results in this article demonstrate the effectiveness and efficiency of the proposed PAS in designing the pooling architectures for graph classification. The Top-1 performance on two Open Graph Benchmark (OGB) datasets1 further indicates the utility of PAS when facing diverse realistic data. The implementation of PAS is available at: https://github.com/AutoML-Research/PAS.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Graph classification; Graph Neural Networks; neural architecture search,Benchmarking; Coarsening; Large dataset; Network architecture; Classification tasks; Embeddings; Graph classification; Graph neural networks; Graph representation; Network-based; Neural architecture search; Neural architectures; Search spaces; State-of-the-art methods; Graph neural networks
Coarse-to-Fine Knowledge-Enhanced Multi-Interest Learning Framework for Multi-Behavior Recommendation,2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85169452033&doi=10.1145%2f3606369&partnerID=40&md5=536b591c7a328663503c12659737efbf,"Multi-types of behaviors (e.g., clicking, carting, purchasing, etc.) widely exist in most real-world recommendation scenarios, which are beneficial to learn users' multi-faceted preferences. As dependencies are explicitly exhibited by the multiple types of behaviors, effectively modeling complex behavior dependencies is crucial for multi-behavior prediction. The state-of-the-art multi-behavior models learn behavior dependencies indistinguishably with all historical interactions as input. However, different behaviors may reflect different aspects of user preference, which means that some irrelevant interactions may play as noises to the target behavior to be predicted. To address the aforementioned limitations, we introduce multi-interest learning to the multi-behavior recommendation. More specifically, we propose a novel Coarse-to-fine Knowledge-enhanced Multi-interest Learning (CKML) framework to learn shared and behavior-specific interests for different behaviors. CKML introduces two advanced modules, namely Coarse-grained Interest Extracting (CIE) and Fine-grained Behavioral Correlation (FBC), which work jointly to capture fine-grained behavioral dependencies. CIE uses knowledge-aware information to extract initial representations of each interest. FBC incorporates a dynamic routing scheme to further assign each behavior among interests. Empirical results on three real-world datasets verify the effectiveness and efficiency of our model in exploiting multi-behavior data. © 2023 Copyright held by the owner/author(s).",coarse-to-fine; multi-behavior recommendation; PhrasesMulti-interest learning,Coarse to fine; Coarse-grained; Fine grained; Learn+; Learning frameworks; Model complexes; Multi interests; Multi-behavior recommendation; Phrasesmulti-interest learning; Real-world
Causal Disentangled Recommendation against User Preference Shifts,2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168667243&doi=10.1145%2f3593022&partnerID=40&md5=ac128818e006284ab8ba98f3a0c2e09c,"Recommender systems easily face the issue of user preference shifts. User representations will become out-of-date and lead to inappropriate recommendations if user preference has shifted over time. To solve the issue, existing work focuses on learning robust representations or predicting the shifting pattern. There lacks a comprehensive view to discover the underlying reasons for user preference shifts. To understand the preference shift, we abstract a causal graph to describe the generation procedure of user interaction sequences. Assuming user preference is stable within a short period, we abstract the interaction sequence as a set of chronological environments. From the causal graph, we find that the changes of some unobserved factors (e.g., becoming pregnant) cause preference shifts between environments. Besides, the fine-grained user preference over item categories sparsely affects the interactions with different items. Inspired by the causal graph, our key considerations to handle preference shifts lie in modeling the interaction generation procedure by: (1) capturing the preference shifts across environments for accurate preference prediction and (2) disentangling the sparse influence from user preference to interactions for accurate effect estimation of preference. To this end, we propose a Causal Disentangled Recommendation (CDR) framework, which captures preference shifts via a temporal variational autoencoder and learns the sparse influence from multiple environments. Specifically, an encoder is adopted to infer the unobserved factors from user interactions while a decoder is to model the interaction generation process. Besides, we introduce two learnable matrices to disentangle the sparse influence from user preference to interactions. Last, we devise a multi-objective loss to optimize CDR. Extensive experiments on three datasets show the superiority of CDR in enhancing the generalization ability under user preference shifts.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesCausal disentangled recommendation; generalizable recommendation; out-of-distribution generalization; preference shifts,Clock and data recovery circuits (CDR circuits); Additional key word and phrasescausal disentangled recommendation; Causal graph; Generalisation; Generalizable recommendation; Key words; Out-of-distribution generalization; Preference shift; Shifting patterns; User interaction; User's preferences; User profile
A Variational Neural Architecture for Skill-based Team Formation,2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85166000363&doi=10.1145%2f3589762&partnerID=40&md5=e60f60e55272137a9209b0fc60830581,"Team formation is concerned with the identification of a group of experts who have a high likelihood of effectively collaborating with each other to satisfy a collection of input skills. Solutions to this task have mainly adopted graph operations and at least have the following limitations: (1) they are computationally demanding, as they require finding shortest paths on large collaboration networks; (2) they use various types of heuristics to reduce the exploration space over the collaboration network to become practically feasible; therefore, their results are not necessarily optimal; and (3) they are not well-suited for collaboration network structures given the sparsity of these networks. Our work proposes a variational Bayesian neural network architecture that learns representations for teams whose members have collaborated with each other in the past. The learned representations allow our proposed approach to mine teams that have a past collaborative history and collectively cover the requested desirable set of skills. Through our experiments, we demonstrate that our approach shows stronger performance compared to a range of strong team formation techniques from both quantitative and qualitative perspectives.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",expert networks; task assignment; Team formation; variational Bayesian neural network,Network architecture; Neural networks; Bayesian neural networks; Collaboration network; Expert networks; Graph operations; Neural architectures; Short-path; Tasks assignments; Team formation; Variational bayesian; Variational bayesian neural network; Bayesian networks
Stylized Data-to-text Generation: A Case Study in the E-Commerce Domain,2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85169043120&doi=10.1145%2f3603374&partnerID=40&md5=fddc519117e1e5a84176443bfc834f1f,"Existing data-to-text generation efforts mainly focus on generating a coherent text from non-linguistic input data, such as tables and attribute-value pairs, but overlook that different application scenarios may require texts of different styles. Inspired by this, we define a new task, namely stylized data-to-text generation, whose aim is to generate coherent text for the given non-linguistic data according to a specific style. This task is non-trivial, due to three challenges: the logic of the generated text, unstructured style reference and biased training samples. To address these challenges, we propose a novel stylized data-to-text generation model, named StyleD2T, comprising three components: logic planning-enhanced data embedding, mask-based style embedding, and unbiased stylized text generation. In the first component, we introduce a graph-guided logic planner for attribute organization to ensure the logic of generated text. In the second component, we devise feature-level mask-based style embedding to extract the essential style signal from the given unstructured style reference. In the last one, pseudo triplet augmentation is utilized to achieve unbiased text generation, and a multi-condition based confidence assignment function is designed to ensure the quality of pseudo samples. Extensive experiments on a newly collected dataset from Taobao1 have been conducted, and the results show the superiority of our model over existing methods.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",advertising; e-commerce; logical text generation; Stylized data-to-text generation,Computer circuits; Electronic commerce; Linguistics; Signal processing; Advertizing; Case-studies; E- commerces; E-commerce domains; Embeddings; Input datas; Logical text generation; Stylized data-to-text generation; Text generations; Embeddings
An Analysis of Fusion Functions for Hybrid Retrieval,2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85172994780&doi=10.1145%2f3596512&partnerID=40&md5=80a5522cb0c1bc10354947cafdc4805a,"We study hybrid search in text retrieval where lexical and semantic search are fused together with the intuition that the two are complementary in how they model relevance. In particular, we examine fusion by a convex combination of lexical and semantic scores, as well as the reciprocal rank fusion (RRF) method, and identify their advantages and potential pitfalls. Contrary to existing studies, we find RRF to be sensitive to its parameters; that the learning of a convex combination fusion is generally agnostic to the choice of score normalization; that convex combination outperforms RRF in in-domain and out-of-domain settings; and finally, that convex combination is sample efficient, requiring only a small set of training examples to tune its only parameter to a target domain.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",fusion functions; Hybrid retrieval; lexical and semantic search,Information retrieval; Semantics; Convex combinations; Fusion functions; Fusion methods; Hybrid retrieval; Hybrid search; Lexical and semantic search; Score normalization; Semantic search; Text retrieval; Training example; Semantic Web
"A Systematic Review of Cost, Effort, and Load Research in Information Search and Retrieval, 1972-2020",2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168685282&doi=10.1145%2f3583069&partnerID=40&md5=ee56c961d5cb85e7c5f9828e372c531f,"During the information search and retrieval (ISR) process, user-system interactions such as submitting queries, examining results, and engaging with information impose some degree of demand on the user's resources. Within ISR, these demands are well recognised, and numerous studies have demonstrated that the cost, effort, and load (CEL) experienced during the search process are affected by a variety of factors. Despite this recognition, there is no universally accepted definition of the constructs of CEL within the field of ISR. Ultimately, this has led to problems with how these constructs have been interpreted and subsequently measured. This systematic review contributes a synthesis of literature, summarising key findings relating to how researchers have been defining and measuring CEL within ISR over the past 50 years. After manually screening 1,109 articles, we detailed and analysed 91 articles which examine CEL within ISR. The discussion focuses on comparing the similarities and differences between CEL definitions and measures before identifying the limitations of the current state of the nomenclature. Opportunities for future research are also identified. Going forward, we propose a CEL taxonomy that integrates the relationships between CEL and their related constructs, which will help focus and disambiguate future research in this important area.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",cognitive load; Cost; effort; workload,Information retrieval; Cognitive loads; Effort; Information search and retrieval; Information search process; Load research; Retrieval process; Systematic Review; User-system interaction; Workload; Search engines
Multi-View Enhanced Graph Attention Network for Session-Based Music Recommendation,2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85170042521&doi=10.1145%2f3592853&partnerID=40&md5=3a7c4f9e8d5e346a1a86b05a42debe90,"Traditional music recommender systems are mainly based on users' interactions, which limit their performance. Particularly, various kinds of content information, such as metadata and description can be used to improve music recommendation. However, it remains to be addressed how to fully incorporate the rich auxiliary/side information and effectively deal with heterogeneity in it. In this paper, we propose a Multi-view Enhanced Graph Attention Network (named MEGAN) for session-based music recommendation. MEGAN can learn informative representations (embeddings) of music pieces and users from heterogeneous information based on graph neural network and attention mechanism. Specifically, the proposed approach MEGAN firstly models users' listening behaviors and the textual content of music pieces with a Heterogeneous Music Graph (HMG). Then, a devised Graph Attention Network is used to learn the low-dimensional embedding of music pieces and users and by integrating various kinds of information, which is enhanced by multi-view from HMG in an adaptive and unified way. Finally, users' hybrid preferences are learned from users' listening behaviors and music pieces that satisfy users real-time requirements are recommended. Comprehensive experiments are conducted on two real-world datasets, and the results show that MEGAN achieves better performance than baselines, including several state-of-the-art recommendation methods.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",attention; graph neural network; music recommendation; Recommender systems; session-based,Embeddings; Graph neural networks; Music; Attention; Content information; Graph neural networks; Learn+; Multi-views; Music recommendation; Music recommender systems; Performance; Session-based; User interaction; Recommender systems
Conditional Cross-Platform User Engagement Prediction,2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171337477&doi=10.1145%2f3589226&partnerID=40&md5=2a5056413e99eeeaa468f1ccfc9c268f,"The bursting of media sharing platforms like TikTok, YouTube, and Kwai enables normal users to create and share content with worldwide audiences. The most popular YouTuber can attract up to 100 million followers. Since there are multiple popular platforms, it's quite common that a YouTuber publishes the same media to multiple platforms, or replicates all media from one platform to another. However, the users of different platforms have different tastes. The media that is popular on one platform may not be a great vogue on other platforms. Observing such cross-platform variance, we propose a new task: estimating the user engagement score of a media on one platform given its popularity on other platforms. This task can benefit both the YouTubers and the platform. On one hand, YouTubers can use the predicted engagement to guide the media reworking; on the other hand, the platform can use the predicted engagement to establish promotion and advertising plans. Therefore, this task is of great practical value. To tackle this task, we propose a disentangled neural network that can separate the general media adorability from platform inclinations. In this manner, by substituting the inclination from the source platform to the target platform, we are able to predict the user engagement in the target platform. To validate the proposed model, we manage to build a dataset of micro-videos which are published on four platforms TikTok, Kwai, Bilibili, and WESEE. The experimental results prove the effectiveness of the proposed model.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Conditional user engagement prediction; cross-platform recommendation; feature disentanglement,Advertizing; Conditional user engagement prediction; Cross-platform; Cross-platform recommendation; Feature disentanglement; Multiple platforms; Popular platform; Sharing platforms; User engagement; YouTube; Forecasting
Cascading Residual Graph Convolutional Network for Multi-Behavior Recommendation,2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164109659&doi=10.1145%2f3587693&partnerID=40&md5=8b427e120f81095b2ffe91ee34c886a0,"Multi-behavior recommendation exploits multiple types of user-item interactions, such as view and cart, to learn user preferences and has demonstrated to be an effective solution to alleviate the data sparsity problem faced by the traditional models that often utilize only one type of interaction for recommendation. In real scenarios, users often take a sequence of actions to interact with an item, in order to get more information about the item and thus accurately evaluate whether an item fits their personal preferences. Those interaction behaviors often obey a certain order, and more importantly, different behaviors reveal different information or aspects of user preferences towards the target item. Most existing multi-behavior recommendation methods take the strategy to first extract information from different behaviors separately and then fuse them for final prediction. However, they have not exploited the connections between different behaviors to learn user preferences. Besides, they often introduce complex model structures and more parameters to model multiple behaviors, largely increasing the space and time complexity. In this work, we propose a lightweight multi-behavior recommendation model named Cascading Residual Graph Convolutional Network (CRGCN for short) for multi-behavior recommendation, which can explicitly exploit the connections between different behaviors into the embedding learning process without introducing any additional parameters (with comparison to the single-behavior based recommendation model). In particular, we design a cascading residual graph convolutional network (GCN) structure, which enables our model to learn user preferences by continuously refining the embeddings across different types of behaviors. The multi-task learning method is adopted to jointly optimize our model based on different behaviors. Extensive experimental results on three real-world benchmark datasets show that CRGCN can substantially outperform the state-of-the-art methods, achieving 24.76%, 27.28%, and 25.10% relative gains on average in terms of HR@K (K = {10,20,50,80}) over the best baseline across the three datasets. Further studies also analyze the effects of leveraging multi-behaviors in different numbers and orders on the final performance.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",cold-start; Collaborative filtering; Graph Convolutional Network; multi-behavior recommendation; multi-task learning,Behavioral research; Complex networks; Convolution; Embeddings; Model structures; Recommender systems; User profile; Cold-start; Convolutional networks; Data sparsity problems; Effective solution; Embeddings; Graph convolutional network; Learn+; Multi-behavior recommendation; Multitask learning; User's preferences; Collaborative filtering
Automatic Skill-Oriented Question Generation and Recommendation for Intelligent Job Interviews,2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85165678858&doi=10.1145%2f3604552&partnerID=40&md5=edf9dbe65a338cee51274b09ab58ae0c,"Job interviews are the most widely accepted method for companies to select suitable candidates, and a critical challenge is finding the right questions to ask job candidates. Moreover, there is a lack of integrated tools for automatically generating interview questions and recommending the right questions to interviewers. To this end, in this paper, we propose an intelligent system for assisting job interviews, namely, DuerQues. To build this system, we first investigate how to automatically generate skill-oriented interview questions in a scalable way by learning external knowledge from online knowledge-sharing communities. Along this line, we develop a novel distantly supervised skill entity recognition method to identify skill entities from large-scale search queries and web page titles with less need for human annotation. Additionally, we propose a neural generative model for generating skill-oriented interview questions. In particular, we introduce a data-driven solution to create high-quality training instances and design a learning algorithm to improve the performance of question generation. Furthermore, we exploit click-through data from query logs and design a recommender system for recommending suitable questions to interviewers. Specifically, we introduce a graph-enhanced algorithm to efficiently recommend suitable questions given a set of queried skills. Finally, extensive experiments on real-world datasets demonstrate the effectiveness of our DuerQues system in terms of the quality of generated skill-oriented questions and the performance of question recommendation.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Job interview assessment; question generation; question recommendation,Learning algorithms; Learning systems; Recommender systems; Search engines; Websites; Critical challenges; External knowledge; Integrated tools; Job interview assessment; Job interviews; Knowledge sharing communities; Online knowledge sharing; Performance; Question generation; Question recommendation; Intelligent systems
How Many Crowd Workers Do i Need? on Statistical Power when Crowdsourcing Relevance Judgments,2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164481047&doi=10.1145%2f3597201&partnerID=40&md5=bb00c4c8e9c1b15749ed1cff8c63f455,"To scale the size of Information Retrieval collections, crowdsourcing has become a common way to collect relevance judgments at scale. Crowdsourcing experiments usually employ 100-10,000 workers, but such a number is often decided in a heuristic way. The downside is that the resulting dataset does not have any guarantee of meeting predefined statistical requirements as, for example, have enough statistical power to be able to distinguish in a statistically significant way between the relevance of two documents.We propose a methodology adapted from literature on sound topic set size design, based on t-test and ANOVA, which aims at guaranteeing the resulting dataset to meet a predefined set of statistical requirements. We validate our approach on several public datasets.Our results show that we can reliably estimate the recommended number of workers needed to achieve statistical power, and that such estimation is dependent on the topic, while the effect of the relevance scale is limited. Furthermore, we found that such estimation is dependent on worker features such as agreement. Finally, we describe a set of practical estimation strategies that can be used to estimate the worker set size, and we also provide results on the estimation of document set sizes.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesCrowdsourcing; relevance judgments; statistical analysis,Information retrieval; Statistical tests; Statistics; Additional key word and phrasescrowdsourcing; Estimation strategies; Key words; Practical estimation; Predefined sets; Public dataset; Relevance judgement; Statistical power; T-tests; Workers'; Crowdsourcing
FASTER: A Dynamic Fairness-assurance Strategy for Session-based Recommender Systems,2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85165722683&doi=10.1145%2f3586993&partnerID=40&md5=41d9ef397ce71c2e09e1e89d69300c23,"When only users' preferences and interests are considered by a recommendation algorithm, it will lead to the severe long-tail problem over items. Therefore, the unfair exposure phenomenon of recommended items caused by this problem has attracted widespread attention in recent years. For the first time, we reveal the fact that there is a more serious unfair exposure problem in session-based recommender systems (SRSs), which learn the short-term and dynamic preferences of users from anonymous sessions. Considering the fact that in SRSs, recommendations are provided multiple times and item exposures are accumulated over interactions in a session, we define new metrics both for the fairness of item exposure and recommendation quality among sessions. Moreover, we design a dynamic Fairness-Assurance STrategy for sEssion-based Recommender systems (FASTER). FASTER is a post-processing strategy that tries to keep a balance between item exposure fairness and recommendation quality. It can also maintain the fairness of recommendation quality among sessions. The effectiveness of FASTER is verified on three real-world datasets and five original algorithms. The experiment results show that FASTER can generally reduce the unfair exposure of different session-based recommendation algorithms while still ensuring a high level of recommendation quality.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",exposure; fairness; Session-based recommender system,Exposure; Exposure problem; Fairness; Learn+; Long tail; Post-processing; Recommendation algorithms; Session-based recommende system; User's preferences; Users' interests; Recommender systems
Group-Based Personalized News Recommendation with Long- and Short-Term Fine-Grained Matching,2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168375793&doi=10.1145%2f3584946&partnerID=40&md5=bc3688baf3afa6cf2925476fb4b8c218,"Personalized news recommendation aims to help users find news content they prefer, which has attracted increasing attention recently. There are two core issues in news recommendation: learning news representation and matching candidate news with user interests. In this context, ""candidate""indicates potential for interest. Due to the superior ability to understand natural language demonstrated by Pretrained Language Models (PLMs), recent works utilize PLMs (e.g., BERT) to strengthen news modeling, obtaining more accurate user interest matching and achieving notable improvement in news recommendation. However, the existing PLM-based methods are usually incapable of fully exploring the fine-grained (i.e., word-level) relatedness between user behaviors and candidate news due to the heavy computational cost brought by PLMs. In this article, we propose a group-based personalized news recommendation method with long- and short-term matching mechanisms between users and candidate news based on PLMs to learn fine-grained matching efficiently and effectively. In our approach, we design to group user historical clicked news into chunks with quite shorter news sequences according to their clicked timestamps, which could alleviate the computation issues of PLMs. PLMs are applied in each group jointly with the candidate news to capture their word-level interaction, and global group-level matching is learned across different groups. In addition, the group-based mechanism could be naturally adapted for long- and short-term user representation learning, in which we build users' long preferences from the representations of all groups and treat the last group as short interests, respectively. Finally, we employ a gate network to dynamically unify the group-level, long- and short-term representations, yielding comprehensive user-news matching effectively. Extensive experiments are conducted on two real-world datasets. The results show that our proposed method achieves superior performance in news recommendations.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesNews recommendation; long-term user interest; personalized; Pretrained Language Model; short-term user preference,Behavioral research; Computational linguistics; User profile; Additional key word and phrasesnews recommendation; Key words; Language model; Long-term user interest; News recommendation; Personalized; Pretrained language model; Short-term user preference; User's preferences; Users' interests; Modeling languages
Quotation Recommendation for Multi-party Online Conversations Based on Semantic and Topic Fusion,2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85172417256&doi=10.1145%2f3594633&partnerID=40&md5=c422a083c00ead80740fa94f4dfa45d1,"Quotations are crucial for successful explanations and persuasions in interpersonal communications. However, finding what to quote in a conversation is challenging for humans. This work studies automatic quotation recommendation for online conversations. Unlike the previous works that only consider semantic-level modeling, we adopt topic-level representation to facilitate the recommendation. A hierarchical architecture that is based on a pretrained language model is adopted to model the semantic-level conversation representation, and a neural topic model is employed to learn the topic-level representation. Moreover, the semantic-level conversation modeling is enhanced by a topic-Aware attention mechanism, which is adopted to capture the interactive conversation structure from the perspective of word co-occurrence. The joint training of semantic-and topic-based recommendation leads to significantly better performance than the state-of-The-Art models on two large-scale datasets. Apart from the novel and advanced recommendation framework, we conduct extensive quantitative experiments to investigate the difficulty of the quotation recommendation task, validate the topic-based recommendation assumption, and explore the stability of the recommendation. Some qualitative experiments and analyses are also included to interpret the quotation and topic distribution for some instances. All the extensive experiments and analyses provide persuasive explanations and interpretations of the module design and the recommendation results. © 2023 Copyright held by the owner/author(s).",Additional Key Words and PhrasesRecommender systems; explainable recommendation; interpretation; quotation recommendation,Large dataset; Online systems; Social networking (online); Additional key word and phrasesrecommende system; Experiment and analysis; Explainable recommendation; Inter-personal communications; Interpretation; Key words; Level model; Quotation recommendation; Semantic levels; Work study; Semantics
AdaTaskRec: An Adaptive Task Recommendation Framework in Spatial Crowdsourcing,2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85172422979&doi=10.1145%2f3593582&partnerID=40&md5=aeb6379d44ddaee2f7df60381e5a0040,"Spatial crowdsourcing is one of the prime movers for the orchestration of location-based tasks, and task recommendation is a crucial means to help workers discover attractive tasks. While a number of existing studies have focused on modeling workers' geographical preferences in task recommendation, they ignore the phenomenon of workers' travel intention drifts across geographical areas, i.e., workers tend to have different intentions when they travel in different areas, which discounts the task recommendation quality of existing methods especially for workers that travel in unfamiliar out-of-Town areas. To address this problem, we propose an Adaptive Task Recommendation (AdaTaskRec) framework. Specifically, we first give a novel two-module worker preference learning architecture that can calculate workers' preferences for POIs (that tasks are associated with) in different areas adaptively based on workers' current locations. If we detect that a worker is in the hometown area, then we apply the hometown preference learning module, which hybrids different strategies to aggregate workers' travel intentions into their preferences while considering the transition and the sequence patterns among locations. Otherwise, we invoke the out-of-Town preference learning module, which is to capture workers' preferences by learning their travel intentions and transferring their hometown preferences into their out-of-Town ones. Additionally, to improve task recommendation effectiveness, we propose a dynamic top-k recommendation method that sets different k values dynamically according to the numbers of neighboring workers and tasks. We also give an extra-reward-based and a fair top-k recommendation method, which introduce the extra rewards for tasks based on their recommendation rounds and consider exposure-based fairness of tasks, respectively. Extensive experiments offer insight into the effectiveness of the proposed framework. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesTask recommendation; spatial crowdsourcing; travel intention,Learning systems; Location; Additional key word and phrasestask recommendation; Key words; Learning modules; Preference learning; Recommendation methods; Spatial crowdsourcing; Top-K recommendations; Travel intention; Worker's preference; Workers'; Crowdsourcing
Trustworthy Recommendation and Search: Introduction to the Special Section-Part 2,2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85172464254&doi=10.1145%2f3604776&partnerID=40&md5=a98c5f63e8d8318dacbe9904d21c0cb1,[No abstract available],,
Contrastive Learning for Legal Judgment Prediction,2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85148279429&doi=10.1145%2f3580489&partnerID=40&md5=5fea5aba2da284946af46c4498f0d532,"Legal judgment prediction (LJP) is a fundamental task of legal artificial intelligence. It aims to automatically predict the judgment results of legal cases. Three typical subtasks are relevant law article prediction, charge prediction, and term-of-penalty prediction. Due to the wide range of potential applications, LJP has attracted a great deal of interest, prompting the development of numerous approaches. These methods mainly focus on building a more accurate representation of a case's fact description in order to improve the performance of judgment prediction. They overlook, however, the practical judicial scenario in which human judges often compare similar law articles or possible charges before making a final decision. To this end, we propose a supervised contrastive learning framework for the LJP task. Specifically, we train the model to distinguish (1) various law articles within the same chapter of a Law and (2) similar charges of the same law article or related law articles. By this means, the fine-grained differences between similar articles/charges can be captured, which are important for making a judgment. Besides, we optimize our model by identifying cases with the same article/charge labels, allowing it to more effectively model the relationship between the case's fact description and its associated labels. By jointly learning the LJP task with the aforementioned contrastive learning tasks, our model achieves better performance than the state-of-The-Art models on two real-world datasets.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Deep learning; law; legal artificial intelligence; legal judgment prediction; supervised contrastive learning,Deep learning; Learning systems; Deep learning; Fact descriptions; Law; Legal artificial intelligence; Legal case; Legal judgements; Legal judgment prediction; Performance; Prediction tasks; Supervised contrastive learning; Forecasting
Debiased Recommendation with User Feature Balancing,2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85170854288&doi=10.1145%2f3580594&partnerID=40&md5=79248d42ba10031b2b29f4c5804da77c,"Debiased recommendation has recently attracted increasing attention from both industry and academic communities. Traditional models mostly rely on the inverse propensity score (IPS), which can be hard to estimate and may suffer from the high variance issue. To alleviate these problems, in this article, we propose a novel debiased recommendation framework based on user feature balancing. The general idea is to introduce a projection function to adjust user feature distributions, such that the ideal unbiased learning objective can be upper bounded by a solvable objective purely based on the offline dataset. In the upper bound, the projected user distributions are expected to be equal given different items. From the causal inference perspective, this requirement aims to remove the causal relation from the user to the item, which enables us to achieve unbiased recommendation, bypassing the computation of IPS. To efficiently balance the user distributions upon each item pair, we propose three strategies, including clipping, sampling, and adversarial learning to improve the training process. For more robust optimization, we deploy an explicit model to capture the potential latent confounders in recommendation systems. To the best of our knowledge, this article is the first work on debiased recommendation based on confounder balancing. In the experiments, we compare our framework with many state-of-The-Art methods based on synthetic, semi-synthetic, and real-world datasets. Extensive experiments demonstrate that our model is effective in promoting the recommendation performance.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Debias Representation; Recommendation system,Inverse problems; Optimization; Academic community; Debias representation; Feature distribution; Learning objectives; Offline; Projection function; Propensity score; Traditional models; Upper Bound; User feature; Recommender systems
Ontology-Aware Prescription Recommendation in Treatment Pathways Using Multi-evidence Healthcare Data,2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85170206565&doi=10.1145%2f3579994&partnerID=40&md5=af767f45bc20c1c836eaa6693a66de85,"For care of chronic diseases (e.g., depression, diabetes, hypertension), it is critical to identify effective treatment pathways that aim to promptly update the medication following the change of patient state and disease progression. This task is challenging because the optimal treatment pathway for each patient needs to be personalized due to the significant heterogeneity among individuals. Therefore, it is naturally promising to investigate how to use the abundant electronic health records to recommend effective and safe prescriptions. However, prescription recommendation needs to consider multiple aspects of life-critical evidence, such as the information relevance in terms of medical concepts, the health condition in terms of diagnosis history, and the further constraint in terms of side information (e.g., patient demographics and drug side effects). To this end, in this article, we propose a novel prescription recommendation framework named OntoPath to predict the next drug in disease treatment pathways, by building an ontology-Aware hierarchical-Attention model that integrates multiple medical evidence from domain knowledge guidance, medical history profiling, and side information utilization. Specifically, our method can be characterized from three aspects: (1) by incorporating the longitudinal diagnosis history, we enrich the profiling of patients in terms of comprehensive health conditions, which can largely influence a drug's outcome on individual patients; (2) using the hierarchical disease and drug ontology structures, we are able to model the domain-specific relevance between patients and drugs at multiple levels of granularity and achieve in-depth collaborative filtering; (3) we introduce a pre-Training stage to enhance the discriminativeness of network representations, which helps us obtain a premium model initialization to further boost the final recommendation training. We perform extensive experiments on a large-scale depression cohort with over 37,000 patients from a real-world medical claims database. The quantitative and qualitative results demonstrate the effectiveness of OntoPath through the consistent outperformance over state-of-The-Art prescription recommendation baselines and the interpretation of model mechanism in case studies.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Drug Recommendation; Electronic Health Records; Ontology,Collaborative filtering; Diagnosis; Diseases; Drug interactions; Patient treatment; Chronic disease; Drug recommendation; Electronic health; Electronic health record; Health condition; Health records; Ontology's; OntoPath; Patient state; Side information; Ontology
Modeling User Reviews through Bayesian Graph Attention Networks for Recommendation,2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159646292&doi=10.1145%2f3570500&partnerID=40&md5=a2c9030683790efa9fc6323fdc9b362f,"Recommender systems relieve users from cognitive overloading by predicting preferred items for users. Due to the complexity of interactions between users and items, graph neural networks (GNN) use graph structures to effectively model user-item interactions. However, existing GNN approaches have the following limitations: (1) User reviews are not adequately modeled in graphs. Therefore, user preferences and item properties that are described in user reviews are lost for modeling users and items; and (2) GNNs assume deterministic relations between users and items, which lack the stochastic modeling to estimate the uncertainties in neighbor relations. To mitigate the limitations, we build tripartite graphs to model user reviews as nodes that connect with users and items. We estimate neighbor relations with stochastic variables and propose a Bayesian graph attention network (i.e., ContGraph) to accurately predict user ratings. ContGraph incorporates the prior knowledge of user preferences to regularize the posterior inference of attention weights. Our experimental results show that ContGraph significantly outperforms 13 state-of-the-art models and improves the best performing baseline (i.e., ANR) by 5.23% on 25 datasets in the five-core version. Moreover, we show that correctly modeling the semantics of user reviews in graphs can help express the semantics of users and items.  © 2023 Association for Computing Machinery.",Bayesian graph attention network; graph neural network; Recommender systems; tripartite graph; user reviews,Bayesian networks; Cognitive systems; Graph neural networks; Graphic methods; Semantics; Stochastic systems; Uncertainty analysis; User profile; Bayesian; Bayesian graph attention network; Deterministics; Graph neural networks; Graph structures; Property; Stochastic-modeling; Tripartite graphs; User reviews; User's preferences; Recommender systems
A Next Basket Recommendation Reality Check,2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168572614&doi=10.1145%2f3587153&partnerID=40&md5=d3fbb9e846012cd75d899ad4eea5ff65,"The goal of a next basket recommendation (NBR) system is to recommend items for the next basket for a user, based on the sequence of their prior baskets. We examine whether the performance gains of the NBR methods reported in the literature hold up under a fair and comprehensive comparison. To clarify the mixed picture that emerges from our comparison, we provide a novel angle on the evaluation of next basket recommendation (NBR) methods, centered on the distinction between repetition and exploration: The next basket is typically composed of previously consumed items (i.e., repeat items) and new items (i.e., explore items). We propose a set of metrics that measure the repetition/exploration ratio and performance of NBR models. Using these new metrics, we provide a second analysis of state-of-The-Art NBR models. The results help to clarify the extent of the actual progress achieved by existing NBR methods as well as the underlying reasons for any improvements that we observe. Overall, our work sheds light on the evaluation problem of NBR, provides a new evaluation protocol, and yields useful insights for the design of models for this task. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesNext basket recommendation; repeat behavior; reproducibility,Additional key word and phrasesnext basket recommendation; Comprehensive comparisons; Hold up; Key words; Performance; Performance Gain; Recommendation methods; Repeat behavior; Reproducibilities; State of the art
Learning Multi-Turn Response Selection in Grounded Dialogues with Reinforced Knowledge and Context Distillation,2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85172416905&doi=10.1145%2f3584701&partnerID=40&md5=25a560568c651ff8b9576908d015e94c,"Recently, knowledge-grounded dialogue systems have gained increasing attention. Great efforts have been made to build response matching models where all dialogue content and knowledge sentences are leveraged. However, knowledge redundancy and distraction of irrelevant dialogue content often exist in knowledge-grounded conversations, which may affect the matching process and lead to inferior performance. In addition, irrelevant dialogue history and excessive knowledge also hinder the exploitation of popular pre-Trained language models (PLMs) due to the limitation of input length. To address these challenges, we propose a new knowledge-grounded dialogue model based on PLMs, where a knowledge selector and a context selector are designed for filtering out irrelevant knowledge sentences and redundant dialogue history, respectively. Considering the lack of labeled data for the learning of two selectors, we pre-Train them with weakly-supervised tasks and then jointly conduct the optimization of knowledge and context selection and fine-Tuning of PLMs for response ranking with reinforcement learning (RL). By this means, the dialogue model can distill more accurate and concise knowledge and dialogue content for subsequent response ranking module, and the overall model can converge and perform better. We conduct experiments on two benchmarks and evaluation results indicate that our model can significantly outperform the state-of-The-Art methods. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesKnowledge-grounded dialogue systems; content planing; context-response matching; multi-Turn dialogues; pre-Trained language models; pre-Training; reinforcement learning; response ranking,Computational linguistics; Distillation; Knowledge management; Learning systems; Speech processing; Additional key word and phrasesknowledge-grounded dialog system; Content planing; Context-response matching; Dialogue systems; Key words; Language model; Matchings; Multi-turn; Multi-turn dialog; Pre-trained language model; Pre-training; Reinforcement learnings; Response ranking; Reinforcement learning
Personal or General? A Hybrid Strategy with Multi-factors for News Recommendation,2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159200784&doi=10.1145%2f3555373&partnerID=40&md5=5b4464269fff15de7d19a9c4a521c2bf,"News recommender systems have become an effective manner to help users make decisions by suggesting the potential news that users may click and read, which has shown the proliferation nowadays. Many representative algorithms made great efforts to discover users' preferences from the histories for triggering news recommendations. However, there exist some limitations due to the following two main issues. First, they mainly rely on the sufficient user data, which cannot well capture users' temporal interests with very limited records. Second, always perceiving users' histories for recommendation may ignore some important news (e.g., breaking news). In this article, we propose a novel Multi-factors Fusion model for news recommendation by integrating both user-dependent preference effect and user-independent timeliness effect together. First, to track the preference of a certain user, we decompose her reading history into two user-related factors, including the long-term habit and the short-term interest. Specifically, we extract her persistent habit by exploring the category effect of news that she focuses on from her whole records. Then, we characterize her temporary interests by proposing a recurrent neural network of analyzing the homogeneous relations between her latest clicked news and the candidate ones. Second, to describe the user-independent news timeliness effect, we propose a novel survival analysis model to estimate the instantaneous click probability of a certain news as the occurring probability of an event, where much sensational news tends to be picked out. Last, we fuse all effects to determine the probability of a user clicking on a certain news under the independent event assumption. We conduct extensive experiments on two real-world datasets. Experimental results demonstrate that our model can generate better news recommendations on both general scenario and cold-start scenario.  © 2023 Association for Computing Machinery.",News recommendation; survival analysis; user-dependent preference; user-independent timeliness,Bioinformatics; Database systems; Hybrid strategies; Multi-factor; News recommendation; News recommender systems; Survival analysis; User independents; User's preferences; User-dependent; User-dependent preference; User-independent timeliness; Recurrent neural networks
Pseudo Relevance Feedback with Deep Language Models and Dense Retrievers: Successes and Pitfalls,2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159646060&doi=10.1145%2f3570724&partnerID=40&md5=b7c87e3f4df8c2a8a8493579a299aa7c,"Pseudo Relevance Feedback (PRF) is known to improve the effectiveness of bag-of-words retrievers. At the same time, deep language models have been shown to outperform traditional bag-of-words rerankers. However, it is unclear how to integrate PRF directly with emergent deep language models. This article addresses this gap by investigating methods for integrating PRF signals with rerankers and dense retrievers based on deep language models. We consider text-based, vector-based and hybrid PRF approaches and investigate different ways of combining and scoring relevance signals. An extensive empirical evaluation was conducted across four different datasets and two task settings (retrieval and ranking).Text-based PRF results show that the use of PRF had a mixed effect on deep rerankers across different datasets. We found that the best effectiveness was achieved when (i) directly concatenating each PRF passage with the query, searching with the new set of queries, and then aggregating the scores; (ii) using Borda to aggregate scores from PRF runs.Vector-based PRF results show that the use of PRF enhanced the effectiveness of deep rerankers and dense retrievers over several evaluation metrics. We found that higher effectiveness was achieved when (i) the query retains either the majority or the same weight within the PRF mechanism, and (ii) a shallower PRF signal (i.e., a smaller number of top-ranked passages) was employed, rather than a deeper signal. Our vector-based PRF method is computationally efficient; thus, this represents a general PRF method others can use with deep rerankers and dense retrievers.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",BERT; dense retrievers; pre-trained language models for information retrieval; Pseudo relevance feedback,Computational linguistics; Bag of words; BERT; Dense retriever; Empirical evaluations; Feedback approach; Feedback signal; Language model; Pre-trained language model for information retrieval; Pseudo-relevance feedbacks; Relevance feedback method; Information retrieval
Multi-Auxiliary Augmented Collaborative Variational Auto-encoder for Tag Recommendation,2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85172412479&doi=10.1145%2f3578932&partnerID=40&md5=3e977b8de5e494f16832f9e740a3887c,"Recommending appropriate tags to items can facilitate content organization, retrieval, consumption, and other applications, where hybrid tag recommender systems have been utilized to integrate collaborative information and content information for better recommendations. In this article, we propose a multi-Auxiliary augmented collaborative variational auto-encoder (MA-CVAE) for tag recommendation, which couples item collaborative information and item multi-Auxiliary information, i.e., content and social graph, by defining a generative process. Specifically, the model learns deep latent embeddings from different item auxiliary information using variational auto-encoders (VAE), which could form a generative distribution over each auxiliary information by introducing a latent variable parameterized by deep neural network. Moreover, to recommend tags for new items, item multi-Auxiliary latent embeddings are utilized as a surrogate through the item decoder for predicting recommendation probabilities of each tag, where reconstruction losses are added in the training phase to constrain the generation for feedback predictions via different auxiliary embeddings. In addition, an inductive variational graph auto-encoder is designed to infer latent embeddings of new items in the test phase, such that item social information could be exploited for new items. Extensive experiments on MovieLens and citeulike datasets demonstrate the effectiveness of our method.  © 2023 Copyright held by the owner/author(s).",Additional Key Words and PhrasesTag recommendations; deep generative models; hybrid systems; variational auto-encoder,Deep neural networks; Embeddings; Recommender systems; Search engines; Signal encoding; Additional key word and phrasestag recommendation; Auto encoders; Auxiliary information; Collaborative information; Deep generative model; Embeddings; Generative model; Key words; Tag recommendations; Variational auto-encoder; Hybrid systems
PRADA: Practical Black-box Adversarial Attacks against Neural Ranking Models,2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160720896&doi=10.1145%2f3576923&partnerID=40&md5=eefd4dc75937038cd73be59731c2befb,"Neural ranking models (NRMs) have shown remarkable success in recent years, especially with pre-Trained language models. However, deep neural models are notorious for their vulnerability to adversarial examples. Adversarial attacks may become a new type of web spamming technique given our increased reliance on neural information retrieval models. Therefore, it is important to study potential adversarial attacks to identify vulnerabilities of NRMs before they are deployed.In this article, we introduce the Word Substitution Ranking Attack (WSRA) task against NRMs, which aims at promoting a target document in rankings by adding adversarial perturbations to its text. We focus on the decision-based black-box attack setting, where the attackers cannot directly get access to the model information, but can only query the target model to obtain the rank positions of the partial retrieved list. This attack setting is realistic in real-world search engines. We propose a novel Pseudo Relevance-based ADversarial ranking Attack method (PRADA) that learns a surrogate model based on Pseudo Relevance Feedback (PRF) to generate gradients for finding the adversarial perturbations.Experiments on two web search benchmark datasets show that PRADA can outperform existing attack strategies and successfully fool the NRM with small indiscernible perturbations of text. © 2023 Association for Computing Machinery.",Adversarial attack; decision-based black-box attack setting; neural ranking models,Information retrieval; Adversarial attack; Attack methods; Black boxes; Decision-based; Decision-based black-box attack setting; Language model; Neural modelling; Neural ranking model; Ranking model; Web spamming; Search engines
Enhancing Recommendation with Search Data in a Causal Learning Manner,2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168686376&doi=10.1145%2f3582425&partnerID=40&md5=b18a2d24aeaba6e2d0bc5fed0d21dfb5,"Recommender systems are currently widely used in various applications helping people filter information. Existing models always embed the rich information for recommendation, such as items, users, and contexts in real-value vectors, and make predictions based on these vectors. In the view of causal inference, the associations between representation vectors and user feedback are inevitably a mixture of the causal part that describes why a user prefers an item, and the non-causal part that merely reflects the statistical dependencies, for example, the display ranking position and sales promotion. However, most recommender systems assume the user-item interactions are only affected by user preferences, neglecting the striking differences between these two associations. To address this problem, we propose a model-Agnostic causal learning framework called IV4Rec+ that can effectively decompose the embedding vectors into these two parts. Moreover, two strategies are proposed to utilize search queries as instrumental variables: IV4Rec+(I) only decomposes the item embeddings, while IV4Rec+(UI) decomposes both user and item embeddings. IV4Rec+ is a model-Agnostic design that can be applied to many existing recommender systems, e.g., DIN, NRHUB, and SRGNN. Extensive experiments on three datasets show that IV4Rec+ significantly facilitates the performance of recommender systems and outperforms state-of-The-Art frameworks. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesRecommendation; causal learning; instrumental variables; search,Association reactions; Embeddings; Sales; Vectors; Additional key word and phrasesrecommendation; Causal inferences; Causal learning; Embeddings; Instrumental variables; Key words; Prediction-based; Real values; Search; User feedback; Recommender systems
TME: Tree-guided Multi-Task Embedding Learning towards Semantic Venue Annotation,2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85170403045&doi=10.1145%2f3582553&partnerID=40&md5=230d14a08d9bd665f915757408355752,"The prevalence of location-based services has generated a deluge of check-ins, enabling the task of human mobility understanding. Among the various types of information associated with the check-in venues, categories (e.g., Bar and Museum) are vital to the task, as they often serve as excellent semantic characterization of the venues. Despite its significance and importance, a large portion of venues in the check-in services do not have even a single category label, such as up to 30% of venues in the Foursquare system lacking category labels. We, therefore, address the problem of semantic venue annotation, i.e., labeling the venue with a semantic category. Existing methods either fail to fully exploit the contextual information in the check-in sequences, or do not consider the semantic correlations across related categories. As such, we devise a Tree-guided Multi-Task Embedding model (TME for short) to learn effective representations of venues and categories for the semantic annotation. TME jointly learns a common feature space by modeling multi-contexts of check-ins and utilizes the predefined category hierarchy to regularize the relatedness among categories. We evaluate TME over the task of semantic venue annotation on two check-in datasets. Experimental results show the superiority of TME over several state-of-The-Art baselines. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesSemantic venue annotation; check-in analysis; embedding learning; human mobility,Embeddings; Location based services; Telecommunication services; Additional key word and phrasessemantic venue annotation; Check-in; Check-in analyse; Embedding learning; Embeddings; Human mobility; Key words; Learn+; Location-based services; Multi tasks; Semantics
MEGCF: Multimodal Entity Graph Collaborative Filtering for Personalized Recommendation,2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159269658&doi=10.1145%2f3544106&partnerID=40&md5=ba209d0d5443881da32c41cce262896d,"In most E-commerce platforms, whether the displayed items trigger the user's interest largely depends on their most eye-catching multimodal content. Consequently, increasing efforts focus on modeling multimodal user preference, and the pressing paradigm is to incorporate complete multimodal deep features of the items into the recommendation module. However, the existing studies ignore the mismatch problem between multimodal feature extraction (MFE) and user interest modeling (UIM). That is, MFE and UIM have different emphases. Specifically, MFE is migrated from and adapted to upstream tasks such as image classification. In addition, it is mainly a content-oriented and non-personalized process, while UIM, with its greater focus on understanding user interaction, is essentially a user-oriented and personalized process. Therefore, the direct incorporation of MFE into UIM for purely user-oriented tasks, tends to introduce a large number of preference-independent multimodal noise and contaminate the embedding representations in UIM.This paper aims at solving the mismatch problem between MFE and UIM, so as to generate high-quality embedding representations and better model multimodal user preferences. Towards this end, we develop a novel model, multimodal entity graph collaborative filtering, short for MEGCF. The UIM of the proposed model captures the semantic correlation between interactions and the features obtained from MFE, thus making a better match between MFE and UIM. More precisely, semantic-rich entities are first extracted from the multimodal data, since they are more relevant to user preferences than other multimodal information. These entities are then integrated into the user-item interaction graph. Afterwards, a symmetric linear Graph Convolution Network (GCN) module is constructed to perform message propagation over the graph, in order to capture both high-order semantic correlation and collaborative filtering signals. Finally, the sentiment information from the review data are used to fine-grainedly weight neighbor aggregation in the GCN, as it reflects the overall quality of the items, and therefore it is an important modality information related to user preferences. Extensive experiments demonstrate the effectiveness and rationality of MEGCF.1  © 2023 Association for Computing Machinery.",Collaborative filtering; collaborative signal; graph convolutional network; multimodal semantic entity; multimodal user preference; semantic correlation; sentiment analysis,Collaborative filtering; Convolution; Embeddings; Semantic Web; Semantics; User profile; Collaborative signal; Convolutional networks; Graph convolutional network; Multi-modal; Multimodal semantic entity; Multimodal user preference; Semantic correlation; Semantic entity; Sentiment analysis; User's preferences; Sentiment analysis
Enhancing Multi-View Smoothness for Sequential Recommendation Models,2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85165191206&doi=10.1145%2f3582495&partnerID=40&md5=ae7baf923317cb55a2a2cb465b6504fb,"Sequential recommendation models aim to predict the interested items to a user based on his historical behaviors. To train sequential recommenders, implicit feedback data is widely adopted since it is easier to obtain than explicit feedback data. In the setting of implicit feedback, a user's historical behaviors can be characterized as a chronologically ordered sequence of interacted items. From a perspective of machine learning, the historical interaction sequence and the recommended items can be considered as context and label, respectively, which are usually in one-hot representations in the recommendation models.However, due to the discrete nature, one-hot representations are hard to sufficiently reflect the underlying user preference, and might also contain noise from implicit feedback that will mislead the model training. To solve these issues, we propose a general optimization framework, Multi-View Smoothness (MVS), to enhance the smoothness of sequential recommendation models in both data representations and model learning. Specifically, with the help of a complementary model, we smooth and enrich the one-hot representations of contexts and labels to better depict the underlying user preference (i.e., context smoothness and label smoothness), and devise a model regularization strategy to enforce the neighborhood smoothness of the model itself (i.e., model smoothness). Based on these strategies, we design three regularizers to constrain and improve the training of sequential recommendation models. Extensive experiments on five datasets show that our approach is able to improve the performance of various base models consistently and outperform other regularization training methods. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesSequential recommendation; data smoothness; model smoothness,Behavioral research; Learning systems; Additional key word and phrasessequential recommendation; Data smoothness; Explicit feedback; Implicit feedback; Key words; Machine-learning; Model smoothness; Model training; Multi-views; User's preferences; Recommender systems
Knowledge Base Embedding for Sampling-Based Prediction,2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159273271&doi=10.1145%2f3533769&partnerID=40&md5=b965497c661d7c931e6c4d998cc56dfc,"Each link prediction task requires different degrees of answer diversity. While a link prediction task may expect up to a couple of answers, another may expect nearly a hundred answers. Given this fact, the performance of a link prediction model can be estimated more accurately if a flexible number of obtained answers are estimated instead of a predefined number of answers. Inspired by this, in this article, we analyze two evaluation criteria for link prediction tasks, respectively ranking-based protocol and sampling-based protocol. Furthermore, we study two classes of models on link prediction task, direct model and latent-variable model respectively, to demonstrate that latent-variable model performs better under the sampling-based protocol. We then propose a latent-variable model where the framework of Conditional Variational AutoEncoder (CVAE) is applied. Experimental study suggests that the proposed model performs comparably to the current state-of-the-art even under the conventional rank-based protocol. Under the sampling-based protocol, the proposed model is shown to outperform various state-of-the-art models.  © 2023 Association for Computing Machinery.",Conditional Variational AutoEncoder; Knowledge Base Embedding; Link prediction,Forecasting; Knowledge based systems; Auto encoders; Conditional variational autoencoder; Embeddings; Knowledge base embedding; Latent variable modeling; Link prediction; Performance; Prediction tasks; Sampling-based; State of the art; Embeddings
Improving Transformer-based Sequential Recommenders through Preference Editing,2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159360887&doi=10.1145%2f3564282&partnerID=40&md5=00313813964ae8e974cccefd8919f954,"One of the key challenges in sequential recommendation is how to extract and represent user preferences. Traditional methods rely solely on predicting the next item. But user behavior may be driven by complex preferences. Therefore, these methods cannot make accurate recommendations when the available information user behavior is limited. To explore multiple user preferences, we propose a transformer-based sequential recommendation model, named MrTransformer (Multi-preference Transformer). For training MrTransformer, we devise a preference-editing-based self-supervised learning (SSL) mechanism that explores extra supervision signals based on relations with other sequences. The idea is to force the sequential recommendation model to discriminate between common and unique preferences in different sequences of interactions. By doing so, the sequential recommendation model is able to disentangle user preferences into multiple independent preference representations so as to improve user preference extraction and representation.We carry out extensive experiments on five benchmark datasets. MrTransformer with preference editing significantly outperforms state-of-the-art sequential recommendation methods in terms of Recall, MRR, and NDCG. We find that long sequences of interactions from which user preferences are harder to extract and represent benefit most from preference editing.  © 2023 Association for Computing Machinery.",self-supervised learning; Transformer-based sequential recommendation; user preference extraction and representation,Behavioral research; Extraction; Recommender systems; Complex preferences; Information users; Multiple user; Preference extractions; Preference representation; Self-supervised learning; Transformer-based sequential recommendation; User behaviors; User preference extraction and representation; User's preferences; Supervised learning
Learning to Retrieve User Behaviors for Click-Through Rate Estimation,2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85170370891&doi=10.1145%2f3579354&partnerID=40&md5=2d384d284cca6fade5918275f4323750,"Click-Through rate (CTR) estimation plays a crucial role in modern online personalization services. It is essential to capture users' drifting interests by modeling sequential user behaviors to build an accurate CTR estimation model. However, as the users accumulate a large amount of behavioral data on the online platforms, the current CTR models have to truncate user behavior sequences and utilize the most recent behaviors, which leads to a problem that sequential patterns such as periodicity or long-Term dependency are not contained in the recent behaviors but in far back history. However, it is non-Trivial to model the entire user sequence by directly using it for two reasons. Firstly, the very long input sequences will make online inference time and system load infeasible. Secondly, the very long sequences contain much noise, thus making it difficult for CTR models to capture useful patterns effectively. To tackle this issue, we consider it from the input data perspective instead of designing more sophisticated yet complex models. As the entire user behavior sequence contains much noise, it is unnecessary to input the entire sequence. Instead, we could just retrieve only a small part of it as the input to the CTR model. In this article, we propose the User Behavior Retrieval (UBR) framework which aims at learning to retrieve the most informative user behaviors according to each CTR estimation request. Retrieving only a small set of behaviors could alleviate the two problems of utilizing very long sequences (i.e., inference efficiency and noisy input). The distinguishing property of UBR is that it supports arbitrary and learnable retrieval functions instead of utilizing a fixed pre-defined function, which is different from the current retrieval-based methods. Offline evaluations on three large-scale real-world datasets demonstrate the superiority and efficacy of the UBR framework. We further deploy UBR at the Huawei App Store, where it achieves 6.6% of eCPM gain in the online A/B test and now serves the main traffic in the Huawei App Store advertising scenario.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",CTR estimation; information retrieval; sequential user behavior modeling,Behavioral research; Information retrieval; Large dataset; Learning systems; 'current; Behavior sequences; Behaviour retrievals; Click-through rate estimation; Clickthrough rates (CTR); Rate estimation; Rate models; Sequential user behavior modeling; User behavior modeling; User behaviors; User profile
TSSuBERT: How to Sum Up Multiple Years of Reading in a Few Tweets,2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161865870&doi=10.1145%2f3581786&partnerID=40&md5=865a11f8e259b4c446367f2780e86632,"The development of deep neural networks and the emergence of pre-Trained language models such as BERT allow to increase performance on many NLP tasks. However, these models do not meet the same popularity for tweet stream summarization, which is probably because their computation limitation requires to drastically truncate the textual input.Our contribution in this article is threefold. First, we propose a neural model to automatically and incrementally summarize huge tweet streams. This extractive model combines in an original way pre-Trained language models and vocabulary frequency based representations to predict tweet salience. An additional advantage of the model is that it automatically adapts the size of the output summary according to the input tweet stream. Second, we detail an original methodology to construct tweet stream summarization datasets requiring little human effort. Third, we release the TES 2012-2016 dataset constructed using the aforementioned methodology. Baselines, oracle summaries, gold standard, and qualitative assessments are made publicly available.To evaluate our approach, we conducted extensive quantitative experiments using three different tweet collections as well as an additional qualitative evaluation. Results show that our method outperforms state-of-The-Art ones. We believe that this work opens avenues of research for incremental summarization, which has not received much attention yet.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesText summarization; BERT; dataset; tweet stream; Twitter,Computational linguistics; Computer hardware description languages; Petroleum reservoir evaluation; Additional key word and phrasestext summarization; BERT; Dataset; Gold standards; Key words; Language model; Neural modelling; Performance; Tweet stream; Twitter; Deep neural networks
A Dense Representation Framework for Lexical and Semantic Matching,2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171400596&doi=10.1145%2f3582426&partnerID=40&md5=ae952bbbe7893f68a0277cf3638d6d44,"Lexical and semantic matching capture different successful approaches to text retrieval and the fusion of their results has proven to be more effective and robust than either alone. Prior work performs hybrid retrieval by conducting lexical and semantic matching using different systems (e.g., Lucene and Faiss, respectively) and then fusing their model outputs. In contrast, our work integrates lexical representations with dense semantic representations by densifying high-dimensional lexical representations into what we call low-dimensional dense lexical representations (DLRs). Our experiments show that DLRs can effectively approximate the original lexical representations, preserving effectiveness while improving query latency. Furthermore, we can combine dense lexical and semantic representations to generate dense hybrid representations (DHRs) that are more flexible and yield faster retrieval compared to existing hybrid techniques. In addition, we explore jointly training lexical and semantic representations in a single model and empirically show that the resulting DHRs are able to combine the advantages of the individual components. Our best DHR model is competitive with state-of-The-Art single-vector and multi-vector dense retrievers in both in-domain and zero-shot evaluation settings. Furthermore, our model is both faster and requires smaller indexes, making our dense representation framework an attractive approach to text retrieval. Our code is available at https://github.com/castorini/dhr. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesSparse retrieval; dense retrieval; hybrid retrieval; vector compression,Information retrieval; Zero-shot learning; Additional key word and phrasessparse retrieval; Dense retrieval; Hybrid representations; Hybrid retrieval; Key words; Lexical matching; Semantic matching; Semantic representation; Text retrieval; Vector compression; Semantics
Learning Implicit and Explicit Multi-task Interactions for Information Extraction,2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159271841&doi=10.1145%2f3533020&partnerID=40&md5=682650706333173c425598c8386ae74b,"Information extraction aims at extracting entities, relations, and so on, in text to support information retrieval systems. To extract information, researchers have considered multitask learning (ML) approaches. The conventional ML approach learns shared features across tasks, with the assumption that these features capture sufficient task interactions to learn expressive shared representations for task classification. However, such an assumption is flawed in different perspectives. First, the shared representation may contain noise introduced by another task; tasks coupled for multitask learning may have different complexities but this approach treats all tasks equally; the conventional approach has a flat structure that hinders the learning of explicit interactions. This approach, however, learns implicit interactions across tasks and often has a generalization ability that has benefited the learning of multitasks. In this article, we take advantage of implicit interactions learned by conventional approaches while alleviating the issues mentioned above by developing a Recurrent Interaction Network with an effective Early Prediction Integration (RIN-EPI) for multitask learning. Specifically, RIN-EPI learns implicit and explicit interactions across two different but related tasks. To effectively learn explicit interactions across tasks, we consider the correlations among the outputs of related tasks. It is, however, obvious that task outputs are unobservable during training, so we leverage the predictions at intermediate layers (referred to as early predictions) as proxies as well as shared features across tasks to learn explicit interactions through attention mechanisms and sequence learning models. By recurrently learning explicit interactions, we gradually improve predictions for the individual tasks in the multitask learning. We demonstrate the effectiveness of RIN-EPI on the learning of two mainstream multitasks for information extraction: (1) entity recognition and relation classification and (2) aspect and opinion term co-extraction. Extensive experiments demonstrate the effectiveness of the RIN-EPI architecture, where we achieve state-of-the-art results on several benchmark datasets.  © 2023 Association for Computing Machinery.",information extraction; Multitask learning,Classification (of information); Forecasting; Information retrieval; Learning systems; Search engines; Conventional approach; Early prediction; Implicit interaction; Information extraction; Information-retrieval systems; Learn+; Learning approach; Multi tasks; Multitask learning; Shared representations; Information retrieval systems
Bounding System-Induced Biases in Recommender Systems with a Randomized Dataset,2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85172422259&doi=10.1145%2f3582002&partnerID=40&md5=3510891ff9c3f109ccca59176861b0b7,"Debiased recommendation with a randomized dataset has shown very promising results in mitigating system-induced biases. However, it still lacks more theoretical insights or an ideal optimization objective function compared with the other more well-studied routes without a randomized dataset. To bridge this gap, we study the debiasing problem from a new perspective and propose to directly minimize the upper bound of an ideal objective function, which facilitates a better potential solution to system-induced biases. First, we formulate a new ideal optimization objective function with a randomized dataset. Second, according to the prior constraints that an adopted loss function may satisfy, we derive two different upper bounds of the objective function: A generalization error bound with triangle inequality and a generalization error bound with separability. Third, we show that most existing related methods can be regarded as the insufficient optimization of these two upper bounds. Fourth, we propose a novel method called debiasing approximate upper bound (DUB) with a randomized dataset, which achieves a more sufficient optimization of these upper bounds. Finally, we conduct extensive experiments on a public dataset and a real product dataset to verify the effectiveness of our DUB.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",randomized dataset; recommender systems; System-induced bias; upper bound minimization,Database systems; De-biasing; Generalization error bounds; Minimisation; Objective functions; Optimisations; Optimization objective function; Randomized dataset; System-induced bias; Upper Bound; Upper bound minimization; Recommender systems
Achieving Human Parity on Visual Question Answering,2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159667337&doi=10.1145%2f3572833&partnerID=40&md5=d2653a887407887a0d6ec6172c808bde,"The Visual Question Answering (VQA) task utilizes both visual image and language analysis to answer a textual question with respect to an image. It has been a popular research topic with an increasing number of real-world applications in the last decade. This paper introduces a novel hierarchical integration of vision and language AliceMind-MMU (ALIbaba's Collection of Encoder-decoders from Machine IntelligeNce lab of Damo academy - MultiMedia Understanding), which leads to similar or even slightly better results than a human being does on VQA. A hierarchical framework is designed to tackle the practical problems of VQA in a cascade manner including: (1) diverse visual semantics learning for comprehensive image content understanding; (2) enhanced multi-modal pre-training with modality adaptive attention; and (3) a knowledge-guided model integration with three specialized expert modules for the complex VQA task. Treating different types of visual questions with corresponding expertise needed plays an important role in boosting the performance of our VQA architecture up to the human level. An extensive set of experiments and analysis are conducted to demonstrate the effectiveness of the new research work.  © 2023 Association for Computing Machinery.",cross-modal interaction; multi-modal pre-training; text and image content analysis; Visual Question Answering; visual reasoning,Image analysis; Semantics; Visual languages; Cross-modal interaction; Image content analysis; Multi-modal; Multi-modal pre-training; Pre-training; Question Answering; Text and image content analyse; Text content; Visual question answering; Visual reasoning; Image enhancement
Federated User Modeling from Hierarchical Information,2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159215507&doi=10.1145%2f3560485&partnerID=40&md5=cf7527926dcea5319e37c76ea0c71483,"The generation of large amounts of personal data provides data centers with sufficient resources to mine idiosyncrasy from private records. User modeling has long been a fundamental task with the goal of capturing the latent characteristics of users from their behaviors. However, centralized user modeling on collected data has raised concerns about the risk of data misuse and privacy leakage. As a result, federated user modeling has come into favor, since it expects to provide secure multi-client collaboration for user modeling through federated learning. Unfortunately, to the best of our knowledge, existing federated learning methods that ignore the inconsistency among clients cannot be applied directly to practical user modeling scenarios, and moreover, they meet the following critical challenges: 1) Statistical heterogeneity. The distributions of user data in different clients are not always independently identically distributed (IID), which leads to unique clients with needful personalized information; 2) Privacy heterogeneity. User data contains both public and private information, which have different levels of privacy, indicating that we should balance different information shared and protected; 3) Model heterogeneity. The local user models trained with client records are heterogeneous, and thus require a flexible aggregation in the server; 4) Quality heterogeneity. Low-quality information from inconsistent clients poisons the reliability of user models and offsets the benefit from high-quality ones, meaning that we should augment the high-quality information during the process. To address the challenges, in this paper, we first propose a novel client-server architecture framework, namely Hierarchical Personalized Federated Learning (HPFL), with a primary goal of serving federated learning for user modeling in inconsistent clients. More specifically, the client train and deliver the local user model via the hierarchical components containing hierarchical information from privacy heterogeneity to join collaboration in federated learning. Moreover, the client updates the personalized user model with a fine-grained personalized update strategy for statistical heterogeneity. Correspondingly, the server flexibly aggregates hierarchical components from heterogeneous user models in the case of privacy and model heterogeneity with a differentiated component aggregation strategy. In order to augment high-quality information and generate high-quality user models, we expand HPFL to the Augmented-HPFL (AHPFL) framework by incorporating the augmented mechanisms, which filters out low-quality information such as noise, sparse information and redundant information. Specially, we construct two implementations of AHPFL, i.e., AHPFL-SVD and AHPFL-AE, where the augmented mechanisms follow SVD (singular value decomposition) and AE (autoencoder), respectively. Finally, we conduct extensive experiments on real-world datasets, which demonstrate the effectiveness of both HPFL and AHPFL frameworks.  © 2023 Association for Computing Machinery.",augmented mechanism; federated learning; model personalization; privacy heterogeneity; User modeling,Client server computer systems; Learning systems; User profile; Augmented mechanism; Federated learning; Hierarchical information; Low qualities; Model personalization; Personalizations; Privacy heterogeneity; Statistical heterogeneities; User data; User Modelling; Data privacy
Topic-Aware Intention Network for Explainable Recommendation with Knowledge Enhancement,2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85172422269&doi=10.1145%2f3579993&partnerID=40&md5=feb9cd59d51042d18bf3aa37ca3648d9,"Recently, recommender systems based on knowledge graphs (KGs) have become a popular research direction. Graph neural network (GNN) is the key technology of KG-based recommendation systems. However, existing GNNs have a significant flaw: They cannot explicitly model users' intent in recommendations. Intent plays an essential role in users' behaviors. For example, users may first generate an intent to purchase a certain group of items and then select a specific item from the group based on their preferences. Therefore, explicitly modeling intent has a positive significance for improving recommendation performance and providing explanations for recommendations. In this article, we propose a new model called Topic-Aware Intention Network (TIN) for explainable recommendations with KGs. TIN models user representations from both preference and intent views. Specifically, we design a relational attention graph neural network to selectively aggregate information in KG to learn user preferences, and we propose a knowledge-enhanced topic model to learn user intent, which is viewed as topics hidden in user behavior sequences. Finally, we obtain the user representation by fusing user preference and intent through an attention network. The experimental results show that our proposed model outperforms the state-of-The-Art methods and can generate reasonable explanations for the recommendation results.  © 2023 Copyright held by the owner/author(s).",Knowledge graph; recommender system; topic model,Behavioral research; Graph neural networks; Knowledge graph; Tin; User profile; Graph neural networks; Graph-based; Group-based; Key technologies; Knowledge graphs; Learn+; Recommendation performance; Topic Modeling; User behaviors; User's preferences; Recommender systems
Recognize News Transition from Collective Behavior for News Recommendation,2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164002491&doi=10.1145%2f3578362&partnerID=40&md5=0c0ccb0cc314b86f164718fb7b472a94,"In the news recommendation, users are overwhelmed by thousands of news daily, which makes the users' behavior data have high sparsity. Therefore, only considering a single user's personalized preferences cannot support the news recommendation. How to improve the relatedness of news and users and reduce data sparsity has become a hot issue. Recent studies have attempted to use graph models to enrich the relationship between users and news, but they are still limited to modeling the historical behaviors of a single user. To fill the gap, we integrate user-news relationships and the overall user historical clicked news sequences to construct a global heterogeneous transition graph. And a refinement approach is proposed to recognize the news transition patterns in the graph. Based on the global heterogeneous transition graph, we propose a heterogeneous transition graph attention network to capture the common behavior patterns of most users to enhance the representation of user interest. Fusing the users' personalized and common interest, we propose the GAINRec model to recommend news effectively. Extensive experiments are conducted on two public news recommendation datasets, and the results show the superiority of the proposed GAINRec model compared with the state-of-the-art news recommendation models. The implementation of our model is available at https://github.com/newsrec/GAINRec. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",collective behavior; graph attention network; News recommendation; transition graph,Behaviour patterns; Collective behaviour; Data sparsity; Graph attention network; Graph model; News recommendation; Single users; Transition graphs; Transition patterns; User behaviors; User profile
GDESA: Greedy Diversity Encoder with Self-attention for Search Results Diversification,2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159354676&doi=10.1145%2f3544103&partnerID=40&md5=2ee0745dadc0e0d44fe350cf0b3cf132,"Search result diversification aims to generate diversified search results so as to meet the various information needs of users. Most of those existing diversification methods greedily select the optimal documents one-by-one comparing with the selected document sequences. Due to the fact that the information utilities of the candidate documents are not independent, a model based on greedy document selection may not produce the global optimal ranking results. To address this issue, some work proposes to model global document interactions regardless of whether a document is selected, which is inconsistent with actual user behavior. In this article, we propose a new supervised diversification framework as an ensemble of global interaction and document selection. Based on a self-attention encoder-decoder structure and an RNN-based document selection component, the model can simultaneously leverage both the global interactions among all the documents and the interactions between the selected sequence and each unselected document. This framework is called Greedy Diversity Encoder with Self-Attention (GDESA). Experimental results show that GDESA outperforms previous methods that rely just on global interactions, and our further analysis demonstrates that using both global interactions and document selection is necessary and beneficial.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",greedy selection; Search result diversification; self-attention,Behavioral research; Information retrieval; Decoder structures; Document selection; Encoder-decoder; Global interaction; Greedy selections; Information utility; Model-based OPC; Search results diversifications; Self-attention; User behaviors; Signal encoding
A Stack-Propagation Framework for Low-Resource Personalized Dialogue Generation,2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149929651&doi=10.1145%2f3563389&partnerID=40&md5=fe69c82446a3e297ec5b255b169756c3,"With the resurgent interest in building open-domain dialogue systems, the dialogue generation task has attracted increasing attention over the past few years. This task is usually formulated as a conditional generation problem, which aims to generate a natural and meaningful response given dialogue contexts and specific constraints, such as persona. And maintaining a consistent persona is essential for the dialogue systems to gain trust from the users. Although tremendous advancements have been brought, traditional persona-based dialogue models are typically trained by leveraging a large number of persona-dense dialogue examples. Yet, such persona-dense training data are expensive to obtain, leading to a limited scale. This work presents a novel approach to learning from limited training examples by regarding consistency understanding as a regularization of response generation. To this end, we propose a novel stack-propagation framework for learning a generation and understanding pipeline. Specifically, the framework stacks a Transformer encoder and two Transformer decoders, where the first decoder models response generation and the second serves as a regularizer and jointly models response generation and consistency understanding. The proposed framework can benefit from the stacked encoder and decoders to learn from much smaller personalized dialogue data while maintaining competitive performance. Under different low-resource settings, subjective and objective evaluations prove that the stack-propagation framework outperforms strong baselines in response quality and persona consistency and largely overcomes the shortcomings of traditional models that rely heavily on the persona-dense dialogue data.  © 2023 Association for Computing Machinery.",low-resource; Open-domain dialogue; personalized dialogue generation; stack-propagation,Learning systems; Signal encoding; Speech processing; Conditional generation; Dialogue generations; Dialogue systems; In-buildings; Low-resource; Model response; Open-domain dialog; Personalized dialog generation; Response generation; Stack-propagation; Decoding
A Consistent Dual-MRC Framework for Emotion-cause Pair Extraction,2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168196603&doi=10.1145%2f3558548&partnerID=40&md5=18b888a0179946d9ecd70313c672f675,"Emotion-cause pair extraction (ECPE) is a recently proposed task that aims to extract the potential clause pairs of emotions and its corresponding causes in a document. In this article, we propose a new paradigm for the ECPE task. We cast the task as a two-turn machine reading comprehension (MRC) task, i.e., the extraction of emotions and causes is transformed to the task of identifying answer clauses from the input document specific to a query. This two-turn MRC formalization brings several key advantages: First, the QA manner provides an explicit pairing way to identify causes specific to the target emotion; second, it provides a natural way of jointly modeling the emotion extraction, the cause extraction, and the pairing of emotion and cause; and third, it allows us to exploit the well-developed MRC models. Based on the two-turn MRC formalization, we propose a dual-MRC framework to extract emotion-cause pairs in a dual-direction way, which enables a more comprehensive coverage of all pairing cases. Furthermore, we propose a consistent training strategy for the second-turn query, so the model is able to filter the errors produced by the first turn at inference. Experiments on two benchmark datasets demonstrate that our method outperforms previous methods and achieves state-of-the-art performance. All the code and data of this work can be obtained at https://github.com/zifengcheng/CD-MRC.  © 2023 Association for Computing Machinery.",Emotion-cause pair extraction; machine reading comprehension; sentiment analysis,Benchmarking; Extraction; Query processing; Benchmark datasets; Comprehension models; Comprehension tasks; Emotion extractions; Emotion-cause pair extraction; Formalisation; Machine reading comprehension; Reading comprehension; Sentiment analysis; Training strategy; Sentiment analysis
Multi-level Attention-based Domain Disentanglement for BCDR,2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85172674055&doi=10.1145%2f3576925&partnerID=40&md5=ddd256466ac2e1787dc986bb1baf2435,"Cross-domain recommendation aims to exploit heterogeneous information from a data-sufficient domain (source domain) to transfer knowledge to a data-scarce domain (target domain). A majority of existing methods focus on unidirectional transfer that leverages the domain-shared information to facilitate the recommendation of the target domain. Nevertheless, it is more beneficial to improve the recommendation performance of both domains simultaneously via a dual transfer learning schema, which is known as bidirectional cross-domain recommendation (BCDR). Existing BCDR methods have their limitations, since they only perform bidirectional transfer learning based on domain-shared representations while neglecting rich information that is private to each domain. In this article, we argue that users may have domain-biased preferences due to the characteristics of that domain. Namely, the domain-specific preference information also plays a critical role in the recommendation. To effectively leverage the domain-specific information, we propose a Multi-level Attention-based Domain Disentanglement framework dubbed MADD for BCDR, which explicitly leverages the attention mechanism to construct personalized preference with both domain-invariant and domain-specific features obtained by disentangling raw user embeddings. Specifically, the domain-invariant feature is exploited by domain-Adversarial learning while the domain-specific feature is learned by imposing an orthogonal loss. We then conduct a reconstruction process on disentangled features to ensure semantic-sufficiency. After that, we devise a multi-level attention mechanism for these disentangled features, which determines their contributions to the final personalized user preference embedding by dynamically learning the attention scores of individual features. We train the model in a multi-Task learning fashion to benefit both domains. Extensive experiments on real-world datasets demonstrate that our model significantly outperforms state-of-The-Art cross-domain recommendation approaches. © 2023 Association for Computing Machinery.",attention mechanism; Cross-domain recommendation; transfer learning,Knowledge management; Semantics; Attention mechanisms; Cross-domain recommendations; Domain specific; Embeddings; Heterogeneous information; Multilevels; Recommendation performance; Shared information; Target domain; Transfer learning; Embeddings
Efficient On-Device Session-Based Recommendation,2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85172413443&doi=10.1145%2f3580364&partnerID=40&md5=bcfdefc37f3f128c832655b02d29fb73,"On-device session-based recommendation systems have been achieving increasing attention on account of the low energy/resource consumption and privacy protection while providing promising recommendation performance. To fit the powerful neural session-based recommendation models in resource-constrained mobile devices, tensor-Train decomposition and its variants have been widely applied to reduce memory footprint by decomposing the embedding table into smaller tensors, showing great potential in compressing recommendation models. However, these model compression techniques significantly increase the local inference time due to the complex process of generating index lists and a series of tensor multiplications to form item embeddings. The resultant on-device recommender fails to provide real-Time responses and recommendations. To improve the online recommendation efficiency, we propose to learn compositional encoding-based compact item representations. Specifically, each item is represented by a compositional code that consists of several codewords, and we learn embedding vectors to represent each codeword instead of each item. Then the composition of the codeword embedding vectors from different embedding matrices (i.e., codebooks) forms the item embedding. Since the size of codebooks can be extremely small, the recommender model is thus able to fit in resource-constrained devices and save the codebooks for fast local inference. In addition, to prevent the loss of model capacity caused by compression, we propose a bidirectional self-supervised knowledge distillation framework. Extensive experimental results on two benchmark datasets demonstrate that compared with existing methods, the proposed on-device recommender not only achieves an 8× inference speedup with a large compression ratio but also shows superior recommendation performance. The code is released at https://github.com/xiaxin1998/EODRec.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesModel compression; knowledge distillation; next-item recommendation; on-device learning; self-supervised learning,Benchmarking; Codes (symbols); Embeddings; Large dataset; Machine learning; Recommender systems; Tensors; Additional key word and phrasesmodel compression; Code-words; Codebooks; Embeddings; Key words; Knowledge distillation; Next-item recommendation; On-device learning; Recommendation performance; Self-supervised learning; Distillation
Efficient Document-At-A-Time and Score-At-A-Time Query Evaluation for Learned Sparse Representations,2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85172706530&doi=10.1145%2f3576922&partnerID=40&md5=7e4a76e64fd854d2b83da4f216e4a686,"Researchers have had much recent success with ranking models based on so-called learned sparse representations generated by transformers. One crucial advantage of this approach is that such models can exploit inverted indexes for top-k retrieval, thereby leveraging decades of work on efficient query evaluation. Yet, there remain many open questions about how these learned representations fit within the existing literature, which our work aims to tackle using four representative learned sparse models. We find that impact weights generated by transformers appear to greatly reduce opportunities for skipping and early exiting optimizations in well-studied document-At-A-Time (DaaT) approaches. Similarly, ""off-The-shelf""application of score-At-A-Time (SaaT) processing exhibits a mismatch between these weights and assumptions behind accumulator management strategies. Building on these observations, we present solutions to address deficiencies with both DaaT and SaaT approaches, yielding substantial speedups in query evaluation. Our detailed empirical analysis demonstrates that both methods lie on the effectiveness-efficiency Pareto frontier, indicating that the optimal choice for deployment depends on operational constraints. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Efficiency; indexing; learned sparse retrieval; query processing,Indexing (of information); Information retrieval; Query processing; Efficient query evaluation; Indexing; Inverted indices; Learned sparse retrieval; Model-based OPC; Optimisations; Query evaluation; Ranking model; Sparse models; Sparse representation; Efficiency
Examining User Heterogeneity in Digital Experiments,2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85172416704&doi=10.1145%2f3578931&partnerID=40&md5=cc6e8988cfea3673dc260714b96ae487,"Digital experiments are routinely used to test the value of a treatment relative to a status-quo control setting-for instance, a new search relevance algorithm for a website or a new results layout for a mobile app. As digital experiments have become increasingly pervasive in organizations and a wide variety of research areas, their growth has prompted a new set of challenges for experimentation platforms. One challenge is that experiments often focus on the average treatment effect (ATE) without explicitly considering differences across major sub-groups: heterogeneous treatment effect (HTE). This is especially problematic, because ATEs have decreased in many organizations as the more obvious benefits have already been realized. However, questions abound regarding the pervasiveness of user HTEs and how best to detect them. We propose a framework for detecting and analyzing user HTEs in digital experiments. Our framework combines an array of user characteristics with double machine learning. Analysis of 27 real-world experiments spanning 1.76 billion sessions and simulated data demonstrates the effectiveness of our detection method relative to existing techniques. We also find that transaction, demographic, engagement, satisfaction, and lifecycle characteristics exhibit statistically significant HTEs in 10% to 20% of our real-world experiments, underscoring the importance of considering user heterogeneity when analyzing experiment results; otherwise, personalized features and experiences cannot happen, thus reducing effectiveness. In terms of the number of experiments and user sessions, we are not aware of any study that has examined user HTEs at this scale. Our findings have important implications for information retrieval, user modeling, platforms, and digital experience contexts, in which online experiments are often used to evaluate the effectiveness of design artifacts.  © 2023 Copyright held by the owner/author(s).",Additional Key Words and PhrasesHeterogeneous treatment effects; digital experiments; double machine learning; user heterogeneity; user modeling,E-learning; Life cycle; User profile; Websites; Additional key word and phrasesheterogeneous treatment effect; Digital experiment; Double machine learning; Key words; Machine-learning; Real world experiment; Status quo; Treatment effects; User heterogeneity; User Modelling; Machine learning
LightFR: Lightweight Federated Recommendation with Privacy-preserving Matrix Factorization,2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85172703378&doi=10.1145%2f3578361&partnerID=40&md5=4b92a0dce237c3fa12ac98b61e9dc680,"Federated recommender system (FRS), which enables many local devices to train a shared model jointly without transmitting local raw data, has become a prevalent recommendation paradigm with privacy-preserving advantages. However, previous work on FRS performs similarity search via inner product in continuous embedding space, which causes an efficiency bottleneck when the scale of items is extremely large. We argue that such a scheme in federated settings ignores the limited capacities in resource-constrained user devices (i.e., storage space, computational overhead, and communication bandwidth), and makes it harder to be deployed in large-scale recommender systems. Besides, it has been shown that transmitting local gradients in real-valued form between server and clients may leak users' private information. To this end, we propose a lightweight federated recommendation framework with privacy-preserving matrix factorization, LightFR, that is able to generate high-quality binary codes by exploiting learning to hash technique under federated settings, and thus enjoys both fast online inference and economic memory consumption. Moreover, we devise an efficient federated discrete optimization algorithm to collaboratively train model parameters between the server and clients, which can effectively prevent real-valued gradient attacks from malicious parties. Through extensive experiments on four real-world datasets, we show that our LightFR model outperforms several state-of-The-Art FRS methods in terms of recommendation accuracy, inference efficiency, and data privacy. © 2023 Association for Computing Machinery.",Federated recommender system; learning to hash; matrix factorization; privacy preservation,Data mining; Digital storage; Efficiency; Learning systems; Matrix algebra; Matrix factorization; Optimization; Privacy-preserving techniques; Embeddings; Federated recommende system; Inner product; Learning to hash; Limited capacity; Matrix factorizations; Privacy preservation; Privacy preserving; Shared model; Similarity search; Recommender systems
Fine-Grained Interaction Modeling with Multi-Relational Transformer for Knowledge Tracing,2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85172418631&doi=10.1145%2f3580595&partnerID=40&md5=eed805af088566ee93a38905a1111d14,"Knowledge tracing, the goal of which is predicting students' future performance given their past question response sequences to trace their knowledge states, is pivotal for computer-Aided education and intelligent tutoring systems. Although many technical efforts have been devoted to modeling students based on their question-response sequences, fine-grained interaction modeling between question-response pairs within each sequence is underexplored. This causes question-response representations less contextualized and further limits student modeling. To address this issue, we first conduct a data analysis and reveal the existence of complex cross effects between different question-response pairs within a sequence. Consequently, we propose MRT-KT, a multi-relational transformer for knowledge tracing, to enable fine-grained interaction modeling between question-response pairs. It introduces a novel relation encoding scheme based on knowledge concepts and student performance. Comprehensive experimental results show that MRT-KT outperforms state-of-The-Art knowledge tracing methods on four widely-used datasets, validating the effectiveness of considering fine-grained interaction for knowledge tracing.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Knowledge tracing; multi-relational transformer; user behavior modeling,Behavioral research; Computer aided instruction; Education computing; Computer-aided education; Fine grained; Future performance; Intelligent tutoring; Interaction modeling; Knowledge state; Knowledge tracings; Multi-relational transformer; Tutoring system; User behavior modeling; Students
Extractive Explanations for Interpretable Text Ranking,2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85172691686&doi=10.1145%2f3576924&partnerID=40&md5=63fc2be6478748a262f4465202234e36,"Neural document ranking models perform impressively well due to superior language understanding gained from pre-Training tasks. However, due to their complexity and large number of parameters these (typically transformer-based) models are often non-interpretable in that ranking decisions can not be clearly attributed to specific parts of the input documents.In this article, we propose ranking models that are inherently interpretable by generating explanations as a by-product of the prediction decision. We introduce the Select-And-Rank paradigm for document ranking, where we first output an explanation as a selected subset of sentences in a document. Thereafter, we solely use the explanation or selection to make the prediction, making explanations first-class citizens in the ranking process. Technically, we treat sentence selection as a latent variable trained jointly with the ranker from the final output. To that end, we propose an end-To-end training technique for Select-And-Rank models utilizing reparameterizable subset sampling using the Gumbel-max trick.We conduct extensive experiments to demonstrate that our approach is competitive to state-of-The-Art methods. Our approach is broadly applicable to numerous ranking tasks and furthers the goal of building models that are interpretable by design. Finally, we present real-world applications that benefit from our sentence selection method. © 2023 Association for Computing Machinery.",fact checking; information retrieval; interpretability; Ranking; sentence selection,Information retrieval; Document ranking; Fact checking; Interpretability; Language understanding; Pre-training; Ranking; Ranking decisions; Ranking model; Sentence selection; Text rankings; Search engines
Evaluating the Robustness of Click Models to Policy Distributional Shift,2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85172692379&doi=10.1145%2f3569086&partnerID=40&md5=3510d64ccd69426fc2f6f52d96be820b,"Many click models have been proposed to interpret logs of natural interactions with search engines and extract unbiased information for evaluation or learning. The experimental setup used to evaluate them typically involves measuring two metrics, namely the test perplexity for click prediction and normalized discounted cumulative gain for relevance estimation. In both cases, the data used for training and testing is assumed to be collected using the same ranking policy. We question this assumption.Important downstream tasks based on click models involve evaluating a different policy than the training policy-that is, click models need to operate under policy distributional shift (PDS). We show that click models are sensitive to it. This can severely hinder their performance on the targeted task: conventional evaluation metrics cannot guarantee that a click model will perform equally well under distributional shift.To more reliably predict click model performance under PDS, we propose a new evaluation protocol. It allows us to compare the relative robustness of six types of click models under various shifts, training configurations, and downstream tasks. We obtain insights into the factors that worsen the sensitivity to PDS and formulate guidelines to mitigate the risks of deploying policies based on click models. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesClick models; distributional shift; offline evaluation; web search,Information retrieval; Petroleum reservoir evaluation; Additional key word and phrasesclick model; Distributional shift; Down-stream; Key words; Natural interactions; Offline evaluation; Relevance estimations; Task-based; Training and testing; Web searches; Search engines
Disentangled Representations Learning for Multi-Target Cross-domain Recommendation,2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164397749&doi=10.1145%2f3572835&partnerID=40&md5=002b93bb6acfc5c6111bcff116e84b90,"Data sparsity has been a long-standing issue for accurate and trustworthy recommendation systems (RS). To alleviate the problem, many researchers pay much attention to cross-domain recommendation (CDR), which aims at transferring rich knowledge from related source domains to enhance the recommendation performance of sparse target domain. To reach the knowledge transferring purpose, recent CDR works always focus on designing different pairwise directed or undirected information transferring strategies between source and target domains. However, such pairwise transferring idea is difficult to adapt to multi-Target CDR scenarios directly, e.g., transferring knowledge between multiple domains and improving their performance simultaneously, as such strategies may lead the following issues: (1) When the number of domains increases, the number of transferring modules will grow exponentially, which causes heavy computation complexity. (2) A single pairwise transferring module could only capture the relevant information of two domains, but ignores the correlated information of other domains, which may limit the transferring effectiveness. (3) When a sparse domain serves as the source domain during the pairwise transferring, it would easily leads the negative transfer problem, and the untrustworthy information may hurt the target domain recommendation performance. In this article, we consider the key challenge of the multi-Target CDR task: How to identify the most valuable trustworthy information over multiple domains and transfer such information efficiently to avoid the negative transfer problem? To fulfill the above challenge, we propose a novel end-To-end model termed as DR-MTCDR, standing for Disentangled Representations learning for Multi-Target CDR. DR-MTCDR aims at transferring the trustworthy domain-shared information across domains, which has the two major advantages in both efficiency and effectiveness: (1) For efficiency, DR-MTCDR utilizes a unified module on all domains to capture disentangled domain-shared information and domain-specific information, which could support all domain recommendation and be insensitive to the number of domains. (2) For effectiveness, based on the disentangled domain-shared and domain-specific information, DR-MTCDR has the capability to lead positive effect and make trustworthy recommendation for each domain. Empirical evaluations on datasets from both public datasets and real-world large-scale financial datasets have shown that the proposed framework outperforms other state-of-The-Art baselines. © 2023 Association for Computing Machinery.",cross domain recommendation; Disentanglement representation learning; trustworthy recommendation,Clock and data recovery circuits (CDR circuits); Large dataset; Cross-domain recommendations; Disentanglement representation learning; Domain-specific information; Multi-targets; Multiple domains; Recommendation performance; Shared information; Target domain; Transfer problems; Trustworthy recommendation; Efficiency
On the Vulnerability of Graph Learning-based Collaborative Filtering,2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163954405&doi=10.1145%2f3572834&partnerID=40&md5=4f506f8a0518529e8cfd076e816afa23,"Graph learning-based collaborative filtering (GLCF), which is built upon the message-passing mechanism of graph neural networks (GNNs), has received great recent attention and exhibited superior performance in recommender systems. However, although GNNs can be easily compromised by adversarial attacks as shown by the prior work, little attention has been paid to the vulnerability of GLCF. Questions like can GLCF models be just as easily fooled as GNNs remain largely unexplored. In this article, we propose to study the vulnerability of GLCF. Specifically, we first propose an adversarial attack against CLCF. Considering the unique challenges of attacking GLCF, we propose to adopt the greedy strategy in searching for the local optimal perturbations and design a reasonable attacking utility function to handle the non-differentiable ranking-oriented metrics. Next, we propose a defense to robustify GCLF. The defense is based on the observation that attacks usually introduce suspicious interactions into the graph to manipulate the message-passing process. We then propose to measure the suspicious score of each interaction and further reduce the message weight of suspicious interactions. We also give a theoretical guarantee of its robustness. Experimental results on three benchmark datasets show the effectiveness of both our attack and defense. © 2023 Association for Computing Machinery.",adversarial attack; collaborative filtering; defense; graph neural network; Recommender system,Graph neural networks; Message passing; Network security; Recommender systems; Adversarial attack; AS graph; Defense; Filtering models; Graph neural networks; Greedy strategies; Local optimal; Message-passing; Optimal design; Performance; Collaborative filtering
An Efficient and Robust Semantic Hashing Framework for Similar Text Search,2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85172670629&doi=10.1145%2f3570725&partnerID=40&md5=065eb1ebc4d2045fdcd972c3733a2fbb,"Similar text search aims to find texts relevant to a given query from a database, which is fundamental in many information retrieval applications, such as question search and exercise search. Since millions of texts always exist behind practical search engine systems, a well-developed text search system usually consists of recall and ranking stages. Specifically, the recall stage serves as the basis in the system, where the main purpose is to find a small set of relevant candidates accurately and efficiently. Towards this goal, deep semantic hashing, which projects original texts into compact hash codes, can support good search performance. However, learning desired textual hash codes is extremely difficult due to the following problems. First, compact hash codes (with short length) can improve retrieval efficiency, but the demand for learning compact hash codes cannot guarantee accuracy due to severe information loss. Second, existing methods always learn the unevenly distributed codes in the space from a local perspective, leading to unsatisfactory code-balance results. Third, a large fraction of textual data contains various types of noise in real-world applications, which causes the deviation of semantics in hash codes. To this end, in this article, we first propose a general unsupervised encoder-decoder semantic hashing framework, namely MASH (short for Memory-bAsed Semantic Hashing), to learn the balanced and compact hash codes for similar text search. Specifically, with a target of retaining semantic information as much as possible, the encoder introduces a novel relevance constraint among informative high-dimensional representations to guide the compact hash code learning. Then, we design an external memory where the hashing learning can be optimized in the global space to ensure the code balance of the learning results, which can promote search efficiency. Besides, to alleviate the performance degradation problem of the model caused by text noise, we propose an improved SMASH (short for denoiSing Memory-bAsed Semantic Hashing) model by incorporating a noise-Aware encoder-decoder framework. This framework considers the noise degree for each text from the semantic deviation aspect, ensuring the robustness of hash codes. Finally, we conduct extensive experiments in three real-world datasets. The experimental results clearly demonstrate the effectiveness and efficiency of MASH and SMASH in generating balanced and compact hash codes, as well as the superior denoising ability of SMASH. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesSemantic hashing; efficient codes; robust codes; similarity search,Decoding; Efficiency; Hash functions; Learning systems; Query processing; Search engines; Semantic Web; Signal encoding; Additional key word and phrasessemantic hashing; De-noising; Efficient code; Encoder-decoder; Key words; Learn+; Retrieval applications; Robust code; Similarity search; Text search; Semantics
Explainable Hyperbolic Temporal Point Process for User-Item Interaction Sequence Generation,2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85172691317&doi=10.1145%2f3570501&partnerID=40&md5=6c1b71de5cbea9abb38f7c5eb7e377f4,"Recommender systems which captures dynamic user interest based on time-ordered user-item interactions plays a critical role in the real-world. Although existing deep learning-based recommendation systems show good performances, these methods have two main drawbacks. Firstly, user interest is the consequence of the coaction of many factors. However, existing methods do not fully explore potential influence factors and ignore the user-item interaction formation process. The coarse-grained modeling patterns cannot accurately reflect complex user interest and leads to suboptimal recommendation results. Furthermore, these methods are implicit and largely operate in a black-box fashion. It is difficult to interpret their modeling processes and recommendation results. Secondly, recommendation datasets usually exhibit scale-free distributions and some existing recommender systems take advantage of hyperbolic space to match the data distribution. But they ignore that the operations in hyperbolic space are more complex than that in Euclidean space which further increases the difficulty of model interpretation. To tackle the above shortcomings, we propose an Explainable Hyperbolic Temporal Point Process for User-Item Interaction Sequence Generation (EHTPP). Specifically, EHTPP regards each user-item interaction as an event in hyperbolic space and employs a temporal point process framework to model the probability of event occurrence. Considering that the complexity of user interest and the interpretability of the model,EHTPP explores four potential influence factors related to user interest and uses them to explicitly guide the probability calculation in the temporal point process. In order to validate the effectiveness of EHTPP, we carry out a comprehensive evaluation of EHTPP on three datasets compared with a few competitive baselines. Experimental results demonstrate the state-of-the-art performances of EHTPP. © 2023 Association for Computing Machinery.",Additional Key Words and PhrasesExplainable recommendation; hyperbolic embedding; temporal point process; temporal recommender system,Coarse-grained modeling; Deep learning; Real time systems; User profile; Additional key word and phrasesexplainable recommendation; Embeddings; Hyperbolic embedding; Hyperbolic spaces; Key words; Point process; Sequence generation; Temporal point process; Temporal recommende system; Users' interests; Recommender systems
Learning Dual-view User Representations for Enhanced Sequential Recommendation,2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85165408626&doi=10.1145%2f3572028&partnerID=40&md5=869b55a99598cfa37e39dfd19aea59c0,"Sequential recommendation (SR) aims to predict a user's next interacted item given his/her historical interactions. Most existing sequential recommendation systems model user preferences only with item-level representations, where a user's interaction sequence are often modeled with sequential or graph-based method to infer the user's sequential interaction pattern. However, since a user's preference factors may vary over time, the user modeling on item-level could hardly represent the user's preference precisely and sufficiently, resulting in suboptimal recommendation performance. In addition, the recommendation results based on the item-level user representations lack the interpretability of preference factors. To address these problems, we propose a novel SR model with dual-view user representations in this article, namely DUVRec, where a user's preference is learned based on the representations of two distinct views, i.e., item view and factor view. Specifically, the item-view user representation is learned as the previous SR models to encode the user preference of item level, while the factor-view user representation is learned by an coarse-grained graph embedding method to explicitly represent the user in terms of preference factors. As a result, such dual-view user representations are more comprehensive than that in the previous SR models, leading to enhanced SR performance. Furthermore, we design a contrastive learning strategy to achieve mutual complementation between these two views. Our extensive experiments upon three benchmark datasets justify DUVRec's superior performance over the state-of-The-Art SR models, including the advantage of the dual-view contrastive learning. In addition, DUVRec's capability of providing explanations on recommendation results is also demonstrated through some specific case studies. © 2023 Association for Computing Machinery.",Additional Key Words and PhrasesSequential recommendation; contrastive learning; factor-view; graph neural networks; item-view; user representation,Benchmarking; Data mining; Graphic methods; Learning systems; Recommender systems; User profile; Additional key word and phrasessequential recommendation; Contrastive learning; Factor-view; Graph neural networks; Item-level; Item-view; Key words; Preference factors; User representation; User's preferences; Graph neural networks
Personalized Prompt Learning for Explainable Recommendation,2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85148647490&doi=10.1145%2f3580488&partnerID=40&md5=fba3a680a33d046efe83cd363ba8eb66,"Providing user-understandable explanations to justify recommendations could help users better understand the recommended items, increase the system's ease of use, and gain users' trust. A typical approach to realize it is natural language generation. However, previous works mostly adopt recurrent neural networks to meet the ends, leaving the potentially more effective pre-Trained Transformer models under-explored. In fact, user and item IDs, as important identifiers in recommender systems, are inherently in different semantic space as words that pre-Trained models were already trained on. Thus, how to effectively fuse IDs into such models becomes a critical issue. Inspired by recent advancement in prompt learning, we come up with two solutions: find alternative words to represent IDs (called discrete prompt learning) and directly input ID vectors to a pre-Trained model (termed continuous prompt learning). In the latter case, ID vectors are randomly initialized but the model is trained in advance on large corpora, so they are actually in different learning stages. To bridge the gap, we further propose two training strategies: sequential tuning and recommendation as regularization. Extensive experiments show that our continuous prompt learning approach equipped with the training strategies consistently outperforms strong baselines on three datasets of explainable recommendation.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesExplainable recommendation; pre-Trained language model; prompt learning; Transformer,Learning systems; Natural language processing systems; Recommender systems; Semantics; Additional key word and phrasesexplainable recommendation; Ease-of-use; Key words; Language model; Natural language generation; Pre-trained language model; Prompt learning; Training strategy; Transformer; Transformer modeling; Recurrent neural networks
AutoML for Deep Recommender Systems: A Survey,2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85172321870&doi=10.1145%2f3579355&partnerID=40&md5=be98a80664b973c1a92e30cf79293d29,"Recommender systems play a significant role in information filtering and have been utilized in different scenarios, such as e-commerce and social media. With the prosperity of deep learning, deep recommender systems show superior performance by capturing non-linear information and item-user relationships. However, the design of deep recommender systems heavily relies on human experiences and expert knowledge. To tackle this problem, Automated Machine Learning (AutoML) is introduced to automatically search for the proper candidates for different parts of deep recommender systems. This survey performs a comprehensive review of the literature in this field. First, we propose an abstract concept for AutoML for deep recommender systems (AutoRecSys) that describes its building blocks and distinguishes it from conventional AutoML techniques and recommender systems. Second, we present a taxonomy as a classification framework containing feature selection search, embedding dimension search, feature interaction search, model architecture search, and other components search. Furthermore, we put a particular emphasis on the search space and search strategy, as they are the common thread to connect all methods within each category and enable practitioners to analyze and compare various approaches. Finally, we propose four future promising research directions that will lead this line of research.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesAutoML; survey; taxonomy,Deep learning; Electronic commerce; Information filtering; Taxonomies; Additional key word and phrasesautoml; Automated machines; E- commerces; Key words; Linear information; Machine-learning; Non linear; Performance; Social media; Users' relationships; Recommender systems
The Influences of a Knowledge Representation Tool on Searchers with Varying Cognitive Abilities,2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149396093&doi=10.1145%2f3527661&partnerID=40&md5=81d8550ff0be83fe2ecd919cc59a0dbc,"While current systems are effective in helping searchers resolve simple information needs (e.g., fact-finding), they provide less support for searchers working on complex information-seeking tasks. Complex search tasks involve a wide range of (meta)cognitive activities, including goal-setting, organizing information, drawing inferences, monitoring progress, and revising mental models and search strategies. We report on a lab study (N = 32) that investigated the influences of a knowledge representation tool called the OrgBox, developed to support searchers with complex tasks. The OrgBox tool was integrated into a custom-built search system and allowed study participants to drag-and-drop textual passages into the tool, organize passages into logical groupings called ""boxes"", and make notes on passages and boxes. The OrgBox was compared to a baseline tool (called the Bookmark) that allowed participants to save textual passages, but not organize them nor make notes. Knowledge representation tools such as the OrgBox may provide special benefits for users with different cognitive profiles. In this article, we explore two cognitive abilities: (1) working memory (WM) capacity and (2) switching (SW) ability. Participants in the study were asked to gather information on a complex subject and produce an outline for a hypothetical research article. We investigate the influences of the tool (OrgBox vs. Bookmark) and the participant's working memory capacity and switching ability on three types of outcomes: (RQ1) search behaviors, (RQ2) post-task perceptions, and (RQ3) the quality of outlines produces by participants. © 2023 Association for Computing Machinery.",Additional Key Words and PhrasesKnowledge representation; complex search; switching ability; working memory capacity,Additional key word and phrasesknowledge representation; Cognitive ability; Complex searches; Current system; Key words; Knowledge-representation; Memory capacity; Switching ability; Working memory; Working memory capacity; Knowledge representation
Revisiting Negative Sampling vs. Non-sampling in Implicit Recommendation,2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149379053&doi=10.1145%2f3522672&partnerID=40&md5=fe7dea06099e09d54ba44db1f2c7622f,"Recommendation systems play an important role in alleviating the information overload issue. Generally, a recommendation model is trained to discern between positive (liked) and negative (disliked) instances for each user. However, under the open-world assumption, there are only positive instances but no negative instances from users' implicit feedback, which poses the imbalanced learning challenge of lacking negative samples. To address this, two types of learning strategies have been proposed before, the negative sampling strategy and non-sampling strategy. The first strategy samples negative instances from missing data (i.e., unlabeled data), while the non-sampling strategy regards all the missing data as negative. Although learning strategies are known to be essential for algorithm performance, the in-depth comparison of negative sampling and non-sampling has not been sufficiently explored by far. To bridge this gap, we systematically analyze the role of negative sampling and non-sampling for implicit recommendation in this work. Specifically, we first theoretically revisit the objection of negative sampling and non-sampling. Then, with a careful setup of various representative recommendation methods, we explore the performance of negative sampling and non-sampling in different scenarios. Our results empirically show that although negative sampling has been widely applied to recent recommendation models, it is non-Trivial for uniform sampling methods to show comparable performance to non-sampling learning methods. Finally, we discuss the scalability and complexity of negative sampling and non-sampling and present some open problems and future research topics that are worth being further explored. © 2023 Association for Computing Machinery.",Additional Key Words and PhrasesRecommender systems; implicit feedback; negative sampling; non-sampling,Recommender systems; Additional key word and phrasesrecommende system; Implicit feedback; Key words; Learning strategy; Missing data; Negative instances; Negative sampling; Non-sampling; Performance; Sampling strategies; Learning systems
Understanding Relevance Judgments in Legal Case Retrieval,2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159683922&doi=10.1145%2f3569929&partnerID=40&md5=75a0663987265678f6a4e8c0c7ec3821,"Legal case retrieval, which aims to retrieve relevant cases given a query case, has drawn increasing research attention in recent years. While much research has worked on developing automatic retrieval models, how to characterize relevance in this specialized information retrieval (IR) task is still an open question. Towards an in-depth understanding of relevance judgments, we conduct a laboratory user study that involves 72 participants of different domain expertise. In the user study, we collect the relevance score along with detailed explanations for the relevance judgment and various measures of the judgment process. From the collected data, we observe that both the subjective (e.g., domain expertise) and objective (e.g., query/case property) factors influence the relevance judgment process. By investigating the collected user explanations, we identify task-specific patterns of user attention distribution and re-think the criteria for relevance judgments. Moreover, we investigate the similarity in attention distribution between models and users. Further, we propose a two-stage framework that utilizes user attention to improve relevance estimation for legal case retrieval. Our study sheds light on understanding relevance judgments in legal case retrieval and provides implications for improving the design of corresponding retrieval systems.  © 2023 Copyright held by the owner/author(s).",Legal case retrieval; relevance judgment; user study,Automatic retrieval; Case retrieval; Domain expertise; In-depth understanding; Legal case; Legal case retrieval; Relevance judgement; Retrieval models; User attention; User study; Information retrieval
Efficient Query-based Black-box Attack against Cross-modal Hashing Retrieval,2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149338811&doi=10.1145%2f3559758&partnerID=40&md5=77928446e2df1e75b72726b0731b4c74,"Deep cross-modal hashing retrieval models inherit the vulnerability of deep neural networks. They are vulnerable to adversarial attacks, especially for the form of subtle perturbations to the inputs. Although many adversarial attack methods have been proposed to handle the robustness of hashing retrieval models, they still suffer from two problems: (1) Most of them are based on the white-box settings, which is usually unrealistic in practical application. (2) Iterative optimization for the generation of adversarial examples in them results in heavy computation. To address these problems, we propose an Efficient Query-based Black-Box Attack (EQB2A) against deep cross-modal hashing retrieval, which can efficiently generate adversarial examples for the black-box attack. Specifically, by sending a few query requests to the attacked retrieval system, the cross-modal retrieval model stealing is performed based on the neighbor relationship between the retrieved results and the query, thus obtaining the knockoffs to substitute the attacked system. A multi-modal knockoffs-driven adversarial generation is proposed to achieve efficient adversarial example generation. While the entire network training converges, EQB2A can efficiently generate adversarial examples by forward-propagation with only given benign images. Experiments show that EQB2A achieves superior attacking performance under the black-box setting.  © 2023 Association for Computing Machinery.",Adversarial attack; adversarial generation; black-box attack; cross-modal hashing retrieval,Backpropagation; Information retrieval; Iterative methods; Adversarial attack; Adversarial generation; Attack methods; Black boxes; Black-box attack; Cross-modal; Cross-modal hashing retrieval; Iterative Optimization; Retrieval models; White box; Deep neural networks
Poincaré Heterogeneous Graph Neural Networks for Sequential Recommendation,2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159619242&doi=10.1145%2f3568395&partnerID=40&md5=078753e6ab4b9d01c1b9e698a8089474,"Sequential recommendation (SR) learns users' preferences by capturing the sequential patterns from users' behaviors evolution. As discussed in many works, user-item interactions of SR generally present the intrinsic power-law distribution, which can be ascended to hierarchy-like structures. Previous methods usually handle such hierarchical information by making user-item sectionalization empirically under Euclidean space, which may cause distortion of user-item representation in real online scenarios. In this article, we propose a Poincaré-based heterogeneous graph neural network named Poincaré Heterogeneous Graph Neural Networks for Sequential Recommendation (PHGR) to model the sequential pattern information as well as hierarchical information contained in the data of SR scenarios simultaneously. Specifically, for the purpose of explicitly capturing the hierarchical information, we first construct a weighted user-item heterogeneous graph by aliening all the user-item interactions to improve the perception domain of each user from a global view. Then the output of the global representation would be used to complement the local directed item-item homogeneous graph convolution. By defining a novel hyperbolic inner product operator, the global and local graph representation learning are directly conducted in Poincaré ball instead of commonly used projection operation between Poincaré ball and Euclidean space, which could alleviate the cumulative error issue of general bidirectional translation process. Moreover, for the purpose of explicitly capturing the sequential dependency information, we design two types of temporal attention operations under Poincaré ball space. Empirical evaluations on datasets from the public and financial industry show that PHGR outperforms several comparison methods.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",behaviors modeling; Hyperbolic learning; session-based recommendation,Directed graphs; Geometry; Behaviour models; Euclidean spaces; Graph neural networks; Heterogeneous graph; Hierarchical information; Hyperbolic learning; Learn+; Poincare; Sequential patterns; Session-based recommendation; Graph neural networks
ReFRS: Resource-efficient Federated Recommender System for Dynamic and Diversified User Preferences,2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152948340&doi=10.1145%2f3560486&partnerID=40&md5=68380e513bc9cc936661444dde4d947f,"Owing to its nature of scalability and privacy by design, federated learning (FL) has received increasing interest in decentralized deep learning. FL has also facilitated recent research on upscaling and privatizing personalized recommendation services, using on-device data to learn recommender models locally. These models are then aggregated globally to obtain a more performant model while maintaining data privacy. Typically, federated recommender systems (FRSs) do not take into account the lack of resources and data availability at the end-devices. In addition, they assume that the interaction data between users and items is i.i.d. and stationary across end-devices (i.e., users), and that all local recommender models can be directly averaged without considering the user's behavioral diversity. However, in real scenarios, recommendations have to be made on end-devices with sparse interaction data and limited resources. Furthermore, users' preferences are heterogeneous and they frequently visit new items. This makes their personal preferences highly skewed, and the straightforwardly aggregated model is thus ill-posed for such non-i.i.d. data. In this article, we propose Resource Efficient Federated Recommender System (ReFRS) to enable decentralized recommendation with dynamic and diversified user preferences. On the device side, ReFRS consists of a lightweight self-supervised local model built upon the variational autoencoder for learning a user's temporal preference from a sequence of interacted items. On the server side, ReFRS utilizes a scalable semantic sampler to adaptively perform model aggregation within each identified cluster of similar users. The clustering module operates in an asynchronous and dynamic manner to support efficient global model update and cope with shifting user interests. As a result, ReFRS achieves superior performance in terms of both accuracy and scalability, as demonstrated by comparative experiments on real datasets.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Decentralized recommender systems; resource efficiency,Behavioral research; Data privacy; Deep learning; Learning systems; Scalability; Semantics; User profile; Decentralised; Decentralized recommende system; End-devices; Personalized recommendation; Recent researches; Resource efficiencies; Resource-efficient; System resources; Upscaling; User's preferences; Recommender systems
Addressing Confounding Feature Issue for Causal Recommendation,2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151275497&doi=10.1145%2f3559757&partnerID=40&md5=aae11b3c329738550542ee056c898255,"In recommender systems, some features directly affect whether an interaction would happen, making the happened interactions not necessarily indicate user preference. For instance, short videos are objectively easier to finish even though the user may not like the video. We term such feature as confounding feature, and video length is a confounding feature in video recommendation. If we fit a model on such interaction data, just as done by most data-driven recommender systems, the model will be biased to recommend short videos more, and deviate from user actual requirement.This work formulates and addresses the problem from the causal perspective. Assuming there are some factors affecting both the confounding feature and other item features, e.g., the video creator, we find the confounding feature opens a backdoor path behind user-item matching and introduces spurious correlation. To remove the effect of backdoor path, we propose a framework named Deconfounding Causal Recommendation(DCR), which performs intervened inference with do-calculus. Nevertheless, evaluating do-calculus requires to sum over the prediction on all possible values of confounding feature, significantly increasing the time cost. To address the efficiency challenge, we further propose a mixture-of-experts (MoE) model architecture, modeling each value of confounding feature with a separate expert module. Through this way, we retain the model expressiveness with few additional costs. We demonstrate DCR on the backbone model of neural factorization machine (NFM), showing that DCR leads to more accurate prediction of user preference with small inference time cost. We release our code at: https://github.com/zyang1580/DCR.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",bias; causal inference; causal recommendation; fairness; Recommender system,User profile; Backdoors; Bias; Causal inferences; Causal recommendation; Data driven; Do-calculus; Fairness; Matchings; Time cost; User's preferences; Recommender systems
Revisiting Graph-based Recommender Systems from the Perspective of Variational Auto-Encoder,2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159724296&doi=10.1145%2f3573385&partnerID=40&md5=169ad2113f29984bc7f5b73520e41188,"Graph-based recommender system has attracted widespread attention and produced a series of research results. Because of the powerful high-order connection modeling capabilities of the Graph Neural Network, the performance of these graph-based recommender systems are far superior to those of traditional neural network-based collaborative filtering models. However, from both analytical and empirical perspectives, the apparent performance improvement is accompanied with a significant time overhead, which is noticeable in large-scale graph topologies. More importantly, the intrinsic data-sparsity problem substantially limits the performance of graph-based recommender systems, which compelled us to revisit graph-based recommendation from a novel perspective. In this article, we focus on analyzing the time complexity of graph-based recommender systems to make it more suitable for real large-scale application scenarios. We propose a novel end-to-end graph recommendation model called the Collaborative Variational Graph Auto-Encoder (CVGA), which uses the information propagation and aggregation paradigms to encode user-item collaborative relationships on the user-item interaction bipartite graph. These relationships are utilized to infer the probability distribution of user behavior for parameter estimation rather than learning user or item embeddings. By doing so, we reconstruct the whole user-item interaction graph according to the known probability distribution in a feasible and elegant manner. From the perspective of the graph auto-encoder, we convert the graph recommendation task into a graph generation problem and are able to do it with approximately linear time complexity. Extensive experiments on four real-world benchmark datasets demonstrate that CVGA can be trained at a faster speed while maintaining comparable performance over state-of-the-art baselines for graph-based recommendation tasks. Further analysis shows that CVGA can effectively mitigate the data sparsity problem and performs equally well on large-scale datasets.  © 2023 Association for Computing Machinery.",collaborative filtering; Graph Neural Networks; Recommendation; Variational Inference,Backpropagation; Behavioral research; Benchmarking; Collaborative filtering; Complex networks; Graph neural networks; Graphic methods; Information dissemination; Large dataset; Network coding; Probability distributions; Topology; User profile; Auto encoders; Data sparsity problems; Graph neural networks; Graph-based; High-order; Performance; Probability: distributions; Recommendation; Research results; Variational inference; Recommender systems
Dual Preference Distribution Learning for Item Recommendation,2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85158025698&doi=10.1145%2f3565798&partnerID=40&md5=542db61e252d655b1873cb6144acb6a9,"Recommender systems can automatically recommend users with items that they probably like. The goal of them is to model the user-item interaction by effectively representing the users and items. Existing methods have primarily learned the user's preferences and item's features with vectorized embeddings, and modeled the user's general preferences to items by the interaction of them. In fact, users have their specific preferences to item attributes and different preferences are usually related. Therefore, exploring the fine-grained preferences as well as modeling the relationships among user's different preferences could improve the recommendation performance. Toward this end, we propose a dual preference distribution learning framework (DUPLE), which aims to jointly learn a general preference distribution and a specific preference distribution for a given user, where the former corresponds to the user's general preference to items and the latter refers to the user's specific preference to item attributes. Notably, the mean vector of each Gaussian distribution can capture the user's preferences, and the covariance matrix can learn their relationship. Moreover, we can summarize a preferred attribute profile for each user, depicting his/her preferred item attributes. We then can provide the explanation for each recommended item by checking the overlap between its attributes and the user's preferred attribute profile. Extensive quantitative and qualitative experiments on six public datasets demonstrate the effectiveness and explainability of the DUPLE method.  © 2023 Association for Computing Machinery.",explainable recommendation; preference distribution learning; Recommender system,Covariance matrix; Learning systems; User profile; Covariance matrices; Embeddings; Explainable recommendation; Fine grained; Learn+; Learning frameworks; Mean vector; Preference distribution learning; Recommendation performance; User's preferences; Recommender systems
Doubly Robust Estimation for Correcting Position Bias in Click Feedback for Unbiased Learning to Rank,2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159676273&doi=10.1145%2f3569453&partnerID=40&md5=5a4b8f6a757a428ab26b610790a8fd08,"Clicks on rankings suffer from position bias: generally items on lower ranks are less likely to be examined - and thus clicked - by users, in spite of their actual preferences between items. The prevalent approach to unbiased click-based learning-to-rank (LTR) is based on counterfactual inverse-propensity-scoring (IPS) estimation. In contrast with general reinforcement learning, counterfactual doubly robust (DR) estimation has not been applied to click-based LTR in previous literature. In this article, we introduce a novel DR estimator that is the first DR approach specifically designed for position bias. The difficulty with position bias is that the treatment - user examination - is not directly observable in click data. As a solution, our estimator uses the expected treatment per rank, instead of the actual treatment that existing DR estimators use. Our novel DR estimator has more robust unbiasedness conditions than the existing IPS approach, and in addition, provides enormous decreases in variance: our experimental results indicate it requires several orders of magnitude fewer datapoints to converge at optimal performance. For the unbiased LTR field, our DR estimator contributes both increases in state-of-the-art performance and the most robust theoretical guarantees of all known LTR estimators.  © 2023 Copyright held by the owner/author(s).",counterfactual learning; Unbiased learning to rank,Condition; Counterfactual learning; Counterfactuals; Datapoints; Orders of magnitude; Reinforcement learnings; Robust approaches; Robust estimation; Robust estimators; Unbiased learning to rank; Reinforcement learning
Exploring Time-aware Multi-pattern Group Venue Recommendation in LBSNs,2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159617914&doi=10.1145%2f3564280&partnerID=40&md5=af8fde2ad62a70e6de92a2b417c3af47,"Location-based social networks (LBSNs) have become a popular platform for users to share their activities with friends and families, which provide abundant information for us to study issues of group venue recommendation by utilizing the characteristics of check-in data. Although there are some studies on group recommendation for venues, few studies consider the group's venue preference in different temporal patterns. In this article, we discover that the group's activity venue has a temporal effect, that is, the group's preference for the activity venue is different at different times. For example, a couple of lovers prefer to travel to tropical regions in winter and relax in bars in the evening. Based on this discovery, we present a Time-aware Multi-pattern (TaMp) topic model to capture the group's interest in the activity venue in multiple temporal patterns (including the daily pattern, the weekly pattern, the monthly pattern, and the quarterly pattern). The TaMp model takes into account the topic, members, temporality, and venue information of group activities and the latent relations among them, especially the strong correlation between the activity time and the corresponding activity venue. Then, we propose a group venue recommendation method based on the TaMp model. In addition, an improved grouping algorithm (iGA) in LBSNs is put forward to enhance the rationality of grouping and the accuracy of group venue recommendation. We conduct comprehensive experiments to evaluate the performance of TaMp on two real-world datasets. The results show that our proposed method outperforms the state-of-the-art group venue recommendation and demonstrates the significance of temporal effects in explaining group activities.  © 2023 Association for Computing Machinery.",Group venue recommendation; location-based social networks; multi-pattern; temporal effects; topic model,Check-in; Group activities; Group recommendations; Group venue recommendation; Location-based social networks; Multi patterns; Popular platform; Temporal effects; Temporal pattern; Topic Modeling; Social networking (online)
A Data-Driven Analysis of Behaviors in Data Curation Processes,2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159619424&doi=10.1145%2f3567419&partnerID=40&md5=26f0b5a2e11a6602cdb1a9b945ec3a25,"Understanding how data workers interact with data, and various pieces of information related to data preparation, is key to designing systems that can better support them in exploring datasets. To date, however, there is a paucity of research studying the strategies adopted by data workers as they carry out data preparation activities. In this work, we investigate a specific data preparation activity, namely data quality discovery, and aim to (i) understand the behaviors of data workers in discovering data quality issues, (ii) explore what factors (e.g., prior experience) can affect their behaviors, as well as (iii) understand how these behavioral observations relate to their performance. To this end, we collect a multi-modal dataset through a data-driven experiment that relies on the use of eye-tracking technology with a purpose-designed platform built on top of iPython Notebook. The experiment results reveal that: (i) 'copy-paste-modify' is a typical strategy for writing code to complete tasks; (ii) proficiency in writing code has a significant impact on the quality of task performance, while perceived difficulty and efficacy can influence task completion patterns; and (iii) searching in external resources is a prevalent action that can be leveraged to achieve better performance. Furthermore, our experiment indicates that providing sample code within the system can help data workers get started with their task, and surfacing underlying data is an effective way to support exploration. By investigating data worker behaviors prior to each search action, we also find that the most common reasons that trigger external search actions are the need to seek assistance in writing or debugging code and to search for relevant code to reuse. Based on our experiment results, we showcase a systematic approach to select from the top best code snippets created by data workers and assemble them to achieve better performance than the best individual performer in the dataset. By doing so, our findings not only provide insights into patterns of interactions with various system components and information resources when performing data curation tasks, but also build effective and efficient data curation processes through data workers' collective intelligence.  © 2023 Association for Computing Machinery.",data curation; Interaction behavior; search pattern,Codes (symbols); Search engines; Data curation; Data preparation; Data quality; Data-driven analysis; Designing systems; Interaction behavior; Performance; Search patterns; Workers'; Writing codes; Eye tracking
Trustworthy Recommendation and Search: Introduction to the Special Issue - Part 1,2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160267130&doi=10.1145%2f3579995&partnerID=40&md5=cc387547ee1172932cc0dc8eb8600425,[No abstract available],fairness; information retrieval; interpretability; privacy; Recommender systems; robustness; security; trustworthiness,
A Critical Study on Data Leakage in Recommender System Offline Evaluation,2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159681430&doi=10.1145%2f3569930&partnerID=40&md5=b44942d244040e5db9f4ab46be551751,"Recommender models are hard to evaluate, particularly under offline setting. In this article, we provide a comprehensive and critical analysis of the data leakage issue in recommender system offline evaluation. Data leakage is caused by not observing global timeline in evaluating recommenders e.g., train/test data split does not follow global timeline. As a result, a model learns from the user-item interactions that are not expected to be available at the prediction time. We first show the temporal dynamics of user-item interactions along global timeline, then explain why data leakage exists for collaborative filtering models. Through carefully designed experiments, we show that all models indeed recommend future items that are not available at the time point of a test instance, as the result of data leakage. The experiments are conducted with four widely used baseline models - BPR, NeuMF, SASRec, and LightGCN, on four popular offline datasets - MovieLens-25M, Yelp, Amazon-music, and Amazon-electronic, adopting leave-last-one-out data split.1 We further show that data leakage does impact models' recommendation accuracy. Their relative performance orders thus become unpredictable with different amount of leaked future data in training. To evaluate recommendation systems in a realistic manner in offline setting, we propose a timeline scheme, which calls for a revisit of the recommendation model design.  © 2023 Association for Computing Machinery.",data leakage; evaluation; Recommender systems,Collaborative filtering; Data mining; Comprehensive analysis; Critical analysis; Data leakage; Evaluation; Learn+; Offline; Offline evaluation; Prediction time; Temporal dynamics; Test data; Recommender systems
Graph Neural Pre-training for Recommendation with Side Information,2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153881182&doi=10.1145%2f3568953&partnerID=40&md5=2a8d9ac38e868ca78aa612104d53505e,"Leveraging the side information associated with entities (i.e., users and items) to enhance recommendation systems has been widely recognized as an essential modeling dimension. Most of the existing approaches address this task by the integration-based scheme, which incorporates the entity side information by combining the recommendation objective with an extra side information-aware objective. Despite the growing progress made by the existing integration-based approaches, they are largely limited by the potential conflicts between the two objectives. Moreover, the heterogeneous side information among entities is still under-explored in these systems. In this article, we propose a novel pre-training scheme to leverage the entity side information by pre-training entity embeddings using the multi-graph neural network. Instead of jointly training with two objectives, our pre-training scheme first pre-trains two representation models under the entity multi/single relational graphs constructed by their side information and then fine-tunes their embeddings under an existing general representation-based recommendation model. Our proposed multi-graph and single-graph neural networks can generate within-entity knowledge-encapsulated embeddings, while capturing the heterogeneity from the entity side information simultaneously, thereby improving the performance of the underlying recommendation model. An extensive evaluation of our pre-training scheme fine-tuned under four general representation-based recommender models, namely, MF, NCF, NGCF, and LightGCN, shows that effectively pre-training embeddings with both the user's and item's side information can significantly improve these original models in terms of both effectiveness and stability.  © 2023 Association for Computing Machinery.",Graph neural networks; pre-training,Embeddings; Knowledge management; Recommender systems; User profile; Embeddings; Graph neural networks; Information-aware; Model dimensions; Potential conflict; Pre-training; Relational graph; Representation model; Side information; Training schemes; Graph neural networks
Decentralized Collaborative Learning Framework for Next POI Recommendation,2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149967539&doi=10.1145%2f3555374&partnerID=40&md5=103971ce04c347273a22116ac55e7a9b,"Next Point-of-Interest (POI) recommendation has become an indispensable functionality in Location-based Social Networks (LBSNs) due to its effectiveness in helping people decide the next POI to visit. However, accurate recommendation requires a vast amount of historical check-in data, thus threatening user privacy as the location-sensitive data needs to be handled by cloud servers. Although there have been several on-device frameworks for privacy-preserving POI recommendations, they are still resource intensive when it comes to storage and computation, and show limited robustness to the high sparsity of user-POI interactions. On this basis, we propose a novel decentralized collaborative learning framework for POI recommendation (DCLR), which allows users to train their personalized models locally in a collaborative manner. DCLR significantly reduces the local models' dependence on the cloud for training, and can be used to expand arbitrary centralized recommendation models. To counteract the sparsity of on-device user data when learning each local model, we design two self-supervision signals to pretrain the POI representations on the server with geographical and categorical correlations of POIs. To facilitate collaborative learning, we innovatively propose to incorporate knowledge from either geographically or semantically similar users into each local model with attentive aggregation and mutual information maximization. The collaborative learning process makes use of communications between devices while requiring only minor engagement from the central server for identifying user groups, and is compatible with common privacy preservation mechanisms like differential privacy. We evaluate DCLR with two real-world datasets, where the results show that DCLR outperforms state-of-the-art on-device frameworks and yields competitive results compared with centralized counterparts.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",decentralized collaborative learning; Point-of-Interest recommendation; user privacy,Digital storage; Learning systems; Privacy-preserving techniques; Centralised; Check-in; Collaborative learning; Decentralised; Decentralized collaborative learning; Learning frameworks; Local model; Location-based social networks; Point-of-interest recommendation; User privacy; Sensitive data
Poisoning GNN-based Recommender Systems with Generative Surrogate-based Attacks,2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159590134&doi=10.1145%2f3567420&partnerID=40&md5=3cb78f759ddff6fb482728e873e67632,"With recent advancements in graph neural networks (GNN), GNN-based recommender systems (gRS) have achieved remarkable success in the past few years. Despite this success, existing research reveals that gRSs are still vulnerable to poison attacks, in which the attackers inject fake data to manipulate recommendation results as they desire. This might be due to the fact that existing poison attacks (and countermeasures) are either model-agnostic or specifically designed for traditional recommender algorithms (e.g., neighborhood-based, matrix-factorization-based, or deep-learning-based RSs) that are not gRS. As gRSs are widely adopted in the industry, the problem of how to design poison attacks for gRSs has become a need for robust user experience. Herein, we focus on the use of poison attacks to manipulate item promotion in gRSs. Compared to standard GNNs, attacking gRSs is more challenging due to the heterogeneity of network structure and the entanglement between users and items. To overcome such challenges, we propose GSPAttack - a generative surrogate-based poison attack framework for gRSs. GSPAttack tailors a learning process to surrogate a recommendation model as well as generate fake users and user-item interactions while preserving the data correlation between users and items for recommendation accuracy. Although maintaining high accuracy for other items rather than the target item seems counterintuitive, it is equally crucial to the success of a poison attack. Extensive evaluations on four real-world datasets revealed that GSPAttack outperforms all baselines with competent recommendation performance and is resistant to various countermeasures.  © 2023 Association for Computing Machinery.",generative models; graph neural networks; Manipulative item promotion; poison attacks; surrogate modeling,Deep learning; Graph neural networks; Learning systems; Matrix factorization; User profile; Generative model; Graph neural networks; Manipulative item promotion; Matrix factorizations; Neighbourhood; Network-based; Networks/graphs; Poison attack; Recommender algorithms; Surrogate modeling; Recommender systems
Towards Robust Neural Graph Collaborative Filtering via Structure Denoising and Embedding Perturbation,2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151085805&doi=10.1145%2f3568396&partnerID=40&md5=6e541987e636194fc8923dad556c06d5,"Neural graph collaborative filtering has received great recent attention due to its power of encoding the high-order neighborhood via the backbone graph neural networks. However, their robustness against noisy user-item interactions remains largely unexplored. Existing work on robust collaborative filtering mainly improves the robustness by denoising the graph structure, while recent progress in other fields has shown that directly adding adversarial perturbations in the embedding space can significantly improve the model robustness. In this work, we propose to improve the robustness of neural graph collaborative filtering via both denoising in the structure space and perturbing in the embedding space. Specifically, in the structure space, we measure the reliability of interactions and further use it to affect the message propagation process of the backbone graph neural networks; in the embedding space, we add in-distribution perturbations by mimicking the behavior of adversarial attacks and further combine it with contrastive learning to improve the performance. Extensive experiments have been conducted on four benchmark datasets to evaluate the effectiveness and efficiency of the proposed approach. The results demonstrate that the proposed approach outperforms the recent neural graph collaborative filtering methods especially when there are injected noisy interactions in the training data.  © 2023 Association for Computing Machinery.",contrastive learning; embedding perturbation; graph neural networks; Neural graph collaborative filtering; structure denoising,Backpropagation; Collaborative filtering; Electric power distribution; Graph embeddings; Contrastive learning; De-noising; Embedding perturbation; Embeddings; Encodings; Graph neural networks; High-order; Neural graph collaborative filtering; Power; Structure denoising; Graph neural networks
On the User Behavior Leakage from Recommender System Exposure,2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159625676&doi=10.1145%2f3568954&partnerID=40&md5=189b78946971be13eab57c69ea381a5c,"Modern recommender systems are trained to predict users' potential future interactions from users' historical behavior data. During the interaction process, despite the data coming from the user side, recommender systems also generate exposure data to provide users with personalized recommendation slates. Compared with the sparse user behavior data, the system exposure data are much larger in volume since only very few exposed items would be clicked by the user. In addition, user historical behavior data are privacy sensitive and commonly protected with careful access authorization. However, the large volume of recommender exposure data generated by the service provider itself usually receives less attention and could be accessed within a relatively larger scope of various information seekers or even potential adversaries. In this article, we investigate the problem of user behavior data leakage in the field of recommender systems. We show that the privacy-sensitive user past behavior data can be inferred through the modeling of system exposure. In other words, one can infer which items the userhas clicked just from the observation of current systemexposure for this user. Given the fact that system exposure data could be widely accessed from a relatively larger scope, we believe that user past behavior privacy has a high risk of leakage in recommender systems. More precisely, we conduct an attack model whose input is the current recommended item slate (i.e., system exposure) for the user while the output is the user's historical behavior. Specifically, we exploit an encoder-decoder structure to construct the attack model and apply different encoding and decoding strategies to verify attack performance. Experimental results on two real-world datasets indicate a great danger of user behavior data leakage. To address the risk, we propose a two-stage privacy-protection mechanism that first selects a subset of items from the exposure slate and then replaces the selected items with uniform or popularity-based exposure. Experimental evaluation reveals a trade-off effect between the recommendation accuracy and the privacy disclosure risk, which is an interesting and important topic for privacy concerns in recommender systems.  © 2023 Association for Computing Machinery.",information security; privacy leakage; privacy protection; Recommender system,Behavioral research; Cryptography; Decoding; Encoding (symbols); Recommender systems; Sensitive data; Signal encoding; User profile; 'current; Attack modeling; Data leakage; Exposure data; Interaction process; Large volumes; Personalized recommendation; Privacy leakages; Privacy protection; User behaviors; Economic and social effects
The Power of Selecting Key Blocks with Local Pre-ranking for Long Document Information Retrieval,2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159621358&doi=10.1145%2f3568394&partnerID=40&md5=43a248ed32b81a7630187e3a1eb40231,"On a wide range of natural language processing and information retrieval tasks, transformer-based models, particularly pre-trained language models like BERT, have demonstrated tremendous effectiveness. Due to the quadratic complexity of the self-attention mechanism, however, such models have difficulties processing long documents. Recent works dealing with this issue include truncating long documents, in which case one loses potential relevant information, segmenting them into several passages, which may lead to miss some information and high computational complexity when the number of passages is large, or modifying the self-attention mechanism to make it sparser as in sparse-attention models, at the risk again of missing some information. We follow here a slightly different approach in which one first selects key blocks of a long document by local query-block pre-ranking, and then few blocks are aggregated to form a short document that can be processed by a model such as BERT. Experiments conducted on standard Information Retrieval datasets demonstrate the effectiveness of the proposed approach.  © 2023 Association for Computing Machinery.",BERT-based language models; long-document neural information retrieval,Computational linguistics; Natural language processing systems; Attention mechanisms; BERT-based language model; Key block; Language informations; Language model; Language processing; Long-document neural information retrieval; Natural languages; Neural information; Power; Information retrieval
User Cold-Start Recommendation via Inductive Heterogeneous Graph Neural Network,2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159618072&doi=10.1145%2f3560487&partnerID=40&md5=5a86cd825417dc9eb1f74d9a4f0deba5,"Recently, user cold-start recommendations have attracted a lot of attention from industry and academia. In user cold-start recommendation systems, the user attribute information is often used by existing approaches to learn user preferences due to the unavailability of user action data. However, most existing recommendation methods often ignore the sparsity of user attributes in cold-start recommendation systems. To tackle this limitation, this article proposes a novel Inductive Heterogeneous Graph Neural Network (IHGNN) model, which utilizes the relational information in user cold-start recommendation systems to alleviate the sparsity of user attributes. Our model converts new users, items, and associated multimodal information into a Modality-aware Heterogeneous Graph (M-HG) that preserves the rich and heterogeneous relationship information among them. Specifically, to utilize rich and heterogeneous relational information in an M-HG for enriching the sparse attribute information of new users, we design a strategy based on random walk operations to collect associated neighbors of new users by multiple times sampling operation. Then, a well-designed multiple hierarchical attention aggregation model consisting of the intra- and inter-type attention aggregating module is proposed, focusing on useful connected neighbors and neglecting meaningless and noisy connected neighbors to generate high-quality representations for user cold-start recommendations. Experimental results on three real datasets demonstrate that the IHGNN outperforms the state-of-the-art baselines.  © 2023 Association for Computing Machinery.",Heterogeneous Graph; Multimodal; user cold-start recommendation,Data mining; Graph neural networks; Attribute information; Cold-start Recommendations; Graph neural networks; Heterogeneous graph; Learn+; Multi-modal; Recommendation methods; User action; User cold-start recommendation; User's preferences; Recommender systems
A Survey on the Fairness of Recommender Systems,2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159587236&doi=10.1145%2f3547333&partnerID=40&md5=98c8987129b013a70d040caee898bcaf,"Recommender systems are an essential tool to relieve the information overload challenge and play an important role in people's daily lives. Since recommendations involve allocations of social resources (e.g., job recommendation), an important issue is whether recommendations are fair. Unfair recommendations are not only unethical but also harm the long-term interests of the recommender system itself. As a result, fairness issues in recommender systems have recently attracted increasing attention. However, due to multiple complex resource allocation processes and various fairness definitions, the research on fairness in recommendation is scattered. To fill this gap, we review over 60 papers published in top conferences/journals, including TOIS, SIGIR, and WWW. First, we summarize fairness definitions in the recommendation and provide several views to classify fairness issues. Then, we review recommendation datasets and measurements in fairness studies and provide an elaborate taxonomy of fairness methods in the recommendation. Finally, we conclude this survey by outlining some promising future directions.  © 2023 Copyright held by the owner/author(s).",fairness; Recommendation; survey,Daily lives; Fairness; Information overloads; Long-term interests; Recommendation; Resource allocation process; Review recommendations; Social resources; Recommender systems
Mitigating Popularity Bias for Users and Items with Fairness-centric Adaptive Recommendation,2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159556189&doi=10.1145%2f3564286&partnerID=40&md5=7aaa5696cb479d66be55b4ddea7683a7,"Recommendation systems are popular in many domains. Researchers usually focus on the effectiveness of recommendation (e.g., precision) but neglect the popularity bias that may affect the fairness of the recommendation, which is also an important consideration that could influence the benefits of users and item providers. A few studies have been proposed to deal with the popularity bias, but they often face two limitations. Firstly, most studies only consider fairness for one side - either users or items, without achieving fairness jointly for both. Secondly, existing methods are not sufficiently tailored to each individual user or item to cope with the varying extent and nature of popularity bias. To alleviate these limitations, in this paper, we propose FAiR, a fairness-centric model that adaptively mitigates the popularity bias in both users and items for recommendation. Concretely, we design explicit fairness discriminators to mitigate the popularity bias for each user and item locally, and an implicit discriminator to preserve fairness globally. Moreover, we dynamically adapt the model to different input users and items to handle the differences in their popularity bias. Finally, we conduct extensive experiments to demonstrate that our model significantly outperforms state-of-the-art baselines in fairness metrics, while remaining competitive in effectiveness.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Fairness; popularity bias,Fairness; Popularity bias; State of the art
Preference-aware Graph Attention Networks for Cross-Domain Recommendations with Collaborative Knowledge Graph,2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159692788&doi=10.1145%2f3576921&partnerID=40&md5=fe6c8996556d952e87f4dbd654dc1091,"Knowledge graphs (KGs) can provide users with semantic information and relations among numerous entities and nodes, which can greatly facilitate the performance of recommender systems. However, existing KG-based approaches still suffer from severe data sparsity and may not be effective in capturing the preference features of similar entities across domains. Therefore, in this article, we propose a Preference-aware Graph Attention network model with Collaborative Knowledge Graph (PGACKG) for cross-domain recommendations. Preference-aware entity embeddings with some collaborative signals are first obtained by exploiting the graph-embedding model, which can transform entities and items in the collaborative knowledge graph into semantic preference spaces. To better learn user preference features, we devise a preference-aware graph attention network framework that aggregates the preference features of similar entities within domains and across domains. In this framework, multi-hop reasoning is employed to assist in the generation of preference features within domains, and the node random walk based on frequency visits is proposed to gather similar preferences across domains for target entities. Then, the final preference features of entities are fused, while a novel Cross-domain Bayesian Personalized Ranking (CBPR) is proposed to improve cross-domain recommendation accuracy. Extensive empirical experiments on four real-world datasets demonstrate that our proposed approach consistently outperforms state-of-the-art baselines. Furthermore, our PGACKG achieves strong performance in different ablation scenarios, and the interaction sparsity experiments also demonstrate that our proposed approach can significantly alleviate the data sparsity issue.  © 2023 Association for Computing Machinery.",graph attention network; Knowledge graph; preference-aware embeddings,Graph embeddings; Semantics; Collaborative knowledge; Cross-domain recommendations; Data sparsity; Embeddings; Graph attention network; Knowledge graphs; Network models; Preference-aware embedding; Semantic relations; Semantics Information; Knowledge graph
Studying the Impact of Data Disclosure Mechanism in Recommender Systems via Simulation,2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159645074&doi=10.1145%2f3569452&partnerID=40&md5=c3a07e01cf22e5b6c3f082e43f56b273,"Recently, privacy issues in web services that rely on users' personal data have raised great attention. Despite that recent regulations force companies to offer choices for each user to opt-in or opt-out of data disclosure, real-world applications usually only provide an ""all or nothing""binary option for users to either disclose all their data or preserve all data with the cost of no personalized service. In this article, we argue that such a binary mechanism is not optimal for both consumers and platforms. To study how different privacy mechanisms affect users' decisions on information disclosure and how users' decisions affect the platform's revenue, we propose a privacy-aware recommendation framework that gives users fine control over their data. In this new framework, users can proactively control which data to disclose based on the tradeoff between anticipated privacy risks and potential utilities. Then we study the impact of different data disclosure mechanisms via simulation with reinforcement learning due to the high cost of real-world experiments. The results show that the platform mechanisms with finer split granularity and more unrestrained disclosure strategy can bring better results for both consumers and platforms than the ""all or nothing""mechanism adopted by most real-world applications.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",GDPR; privacy; Recommender system,Data privacy; Reinforcement learning; Web services; All or nothings; GDPR; Information disclosure; Personalized service; Privacy; Privacy aware; Privacy issue; Privacy mechanisms; Real-world; Webs services; Recommender systems
RESUS: Warm-up Cold Users via Meta-learning Residual User Preferences in CTR Prediction,2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159593504&doi=10.1145%2f3564283&partnerID=40&md5=8c1171baafbb10852354c2776ef27b24,"Click-through Rate (CTR) prediction on cold users is a challenging task in recommender systems. Recent researches have resorted to meta-learning to tackle the cold-user challenge, which either perform few-shot user representation learning or adopt optimization-based meta-learning. However, existing methods suffer from information loss or inefficient optimization process, and they fail to explicitly model global user preference knowledge, which is crucial to complement the sparse and insufficient preference information of cold users. In this article, we propose a novel and efficient approach named RESUS, which decouples the learning of global preference knowledge contributed by collective users from the learning of residual preferences for individual users. Specifically, we employ a shared predictor to infer basis user preferences, which acquires global preference knowledge from the interactions of different users. Meanwhile, we develop two efficient algorithms based on the nearest neighbor and ridge regression predictors, which infer residual user preferences via learning quickly from a few user-specific interactions. Extensive experiments on three public datasets demonstrate that our RESUS approach is efficient and effective in improving CTR prediction accuracy on cold users, compared with various state-of-the-art methods.  © 2023 Association for Computing Machinery.",Cold-start recommendation; CTR prediction; few-shot learning; metric-based meta learning,Regression analysis; Click-through rate prediction; Clickthrough rates (CTR); Cold-start Recommendations; Few-shot learning; Metalearning; Metric-based meta learning; Rate predictions; Recent researches; User's preferences; Warm up; Forecasting
A Unified Multi-task Learning Framework for Multi-goal Conversational Recommender Systems,2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159644776&doi=10.1145%2f3570640&partnerID=40&md5=c6bdfc7849950d19e84c8324eceb441b,"Recent years witnessed several advances in developing multi-goal conversational recommender systems (MG-CRS) that can proactively attract users' interests and naturally lead user-engaged dialogues with multiple conversational goals and diverse topics. Four tasks are often involved in MG-CRS, including Goal Planning, Topic Prediction, Item Recommendation, and Response Generation. Most existing studies address only some of these tasks. To handle the whole problem of MG-CRS, modularized frameworks are adopted where each task is tackled independently without considering their interdependencies. In this work, we propose a novel Unified MultI-goal conversational recommeNDer system (UniMIND). Specifically, we unify these four tasks with different formulations into the same sequence-to-sequence paradigm. Prompt-based learning strategies are investigated to endow the unified model with the capability of multi-task learning. Finally, the overall learning and inference procedure consists of three stages, including multi-task learning, prompt-based tuning, and inference. Experimental results on two MG-CRS benchmarks (DuRecDial and TG-ReDial) show that UniMIND achieves state-of-the-art performance on all tasks with a unified model. Extensive analyses and discussions are provided for shedding some new perspectives for MG-CRS.  © 2023 Association for Computing Machinery.",Conversational recommender system; dialogue generation; paradigm shift; prompt-based learning,Learning systems; Recommender systems; Conversational recommender systems; Dialogue generations; Lead users; Learning frameworks; Multitask learning; Paradigm shifts; Prompt-based learning; Response generation; Unified Modeling; Users' interests; Benchmarking
Bias and Debias in Recommender System: A Survey and Future Directions,2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159562769&doi=10.1145%2f3564284&partnerID=40&md5=911105561580bf70ccdf7956e954dfc0,"While recent years have witnessed a rapid growth of research papers on recommender system (RS), most of the papers focus on inventing machine learning models to better fit user behavior data. However, user behavior data is observational rather than experimental. This makes various biases widely exist in the data, including but not limited to selection bias, position bias, exposure bias, and popularity bias. Blindly fitting the data without considering the inherent biases will result in many serious issues, e.g., the discrepancy between offline evaluation and online metrics, hurting user satisfaction and trust on the recommendation service, and so on. To transform the large volume of research models into practical improvements, it is highly urgent to explore the impacts of the biases and perform debiasing when necessary. When reviewing the papers that consider biases in RS, we find that, to our surprise, the studies are rather fragmented and lack a systematic organization. The terminology ""bias""is widely used in the literature, but its definition is usually vague and even inconsistent across papers. This motivates us to provide a systematic survey of existing work on RS biases. In this paper, we first summarize seven types of biases in recommendation, along with their definitions and characteristics. We then provide a taxonomy to position and organize the existing work on recommendation debiasing. Finally, we identify some open challenges and envision some future directions, with the hope of inspiring more research work on this important yet less investigated topic. The summary of debiasing methods reviewed in this survey can be found at https://github.com/jiawei-chen/RecDebiasing.  © 2023 Association for Computing Machinery.",adaption; efficiency; recommendation; Sampling,Behavioral research; HTTP; Adaption; De-biasing; Machine learning models; Offline evaluation; Rapid growth; Recommendation; Research papers; Selection bias; User behaviors; Users' satisfactions; Recommender systems
Proactive Privacy-preserving Learning for Cross-modal Retrieval,2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85158163580&doi=10.1145%2f3545799&partnerID=40&md5=f8ac642de443be3098647a9e7ec58a43,"Deep cross-modal retrieval techniques have recently achieved remarkable performance, which also poses severe threats to data privacy potentially. Nowadays, enormous user-generated contents that convey personal information are released and shared on the Internet. One may abuse a retrieval system to pinpoint sensitive information of a particular Internet user, causing privacy leakage. In this article, we propose a data-centric Proactive Privacy-preserving Cross-modal Learning algorithm that fulfills the protection purpose by employing a generator to transform original data into adversarial data with quasi-imperceptible perturbations before releasing them. When the data source is infiltrated, the inside adversarial data can confuse retrieval models under the attacker's control to make erroneous predictions. We consider the protection under a realistic and challenging setting where the prior knowledge of malicious models is agnostic. To handle this, a surrogate retrieval model is instead introduced, acting as the target to fool. The whole network is trained under a game-theoretical framework, where the generator and the retrieval model persistently evolve to fight against each other. To facilitate the optimization, a Gradient Reversal Layer module is inserted between two models, enabling a one-step learning fashion. Extensive experiments on widely used realistic datasets prove the effectiveness of the proposed method.  © 2023 Association for Computing Machinery.",adversarial data; cross-modal retrieval; deep learning; Privacy protection,Game theory; Learning algorithms; Privacy-preserving techniques; Search engines; Sensitive data; Adversarial data; Cross-modal; Cross-modal retrieval; Deep learning; Performance; Privacy preserving; Privacy protection; Retrieval models; Retrieval techniques; User-generated; Deep learning
A Multi-strategy-based Pre-training Method for Cold-start Recommendation,2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151083766&doi=10.1145%2f3544107&partnerID=40&md5=7dd05a2b19fae2462c743a7088a17629,"The cold-start issue is a fundamental challenge in Recommender Systems. The recent self-supervised learning (SSL) on Graph Neural Networks (GNNs) model, PT-GNN, pre-trains the GNN model to reconstruct the cold-start embeddings and has shown great potential for cold-start recommendation. However, due to the over-smoothing problem, PT-GNN can only capture up to 3-order relation, which cannot provide much useful auxiliary information to depict the target cold-start user or item. Besides, the embedding reconstruction task only considers the intra-correlations within the subgraph of users and items, while ignoring the inter-correlations across different subgraphs. To solve the above challenges, we propose a multi-strategy-based pre-training method for cold-start recommendation (MPT), which extends PT-GNN from the perspective of model architecture and pretext tasks to improve the cold-start recommendation performance.1 Specifically, in terms of the model architecture, in addition to the short-range dependencies of users and items captured by the GNN encoder, we introduce a Transformer encoder to capture long-range dependencies. In terms of the pretext task, in addition to considering the intra-correlations of users and items by the embedding reconstruction task, we add an embedding contrastive learning task to capture inter-correlations of users and items. We train the GNN and Transformer encoders on these pretext tasks under the meta-learning setting to simulate the real cold-start scenario, making the model able to be easily and rapidly adapted to new cold-start users and items. Experiments on three public recommendation datasets show the superiority of the proposed MPT model against the vanilla GNN models, the pre-training GNN model on user/item embedding inference, and the recommendation task.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",cold-start; pre-training; Recommender System; self-supervised learning,Data mining; Embeddings; Graph neural networks; Network architecture; Neural network models; Signal encoding; Supervised learning; Cold-start; Cold-start Recommendations; Embeddings; Graph neural networks; Modeling architecture; Neural network model; Pre-training; Self-supervised learning; Subgraphs; Training methods; Recommender systems
User Perception of Recommendation Explanation: Are Your Explanations What Users Need?,2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159222055&doi=10.1145%2f3565480&partnerID=40&md5=231eb9e0821f7d866d5d6d0ccfb5fed9,"As recommender systems become increasingly important in daily human decision-making, users are demanding convincing explanations to understand why they get the specific recommendation results. Although a number of explainable recommender systems have recently been proposed, there still lacks an understanding of what users really need in a recommendation explanation. The actual reason behind users' intention to examine and consume (e.g., click and watch a movie) can be the window to answer this question and is named as self-explanation in this work. In addition, humans usually make recommendations accompanied by explanations, but there remain fewer studies on how humans explain and what we can learn from human-generated explanations. To investigate these questions, we conduct a novel multi-role, multi-session user study in which users interact with multiple types of system-generated explanations as well as human-generated explanations, namely peer-explanation. During the study, users' intentions, expectations, and experiences are tracked in several phases, including before and after the users are presented with an explanation and after the content is examined. Through comprehensive investigations, three main findings have been made: First, we observe not only the positive but also the negative effects of explanations, and the impact varies across different types of explanations. Moreover, human-generated explanation, peer-explanation, performs better in increasing user intentions and helping users to better construct preferences, which results in better user satisfaction. Second, based on users' self-explanation, the information accuracy is measured and found to be a major factor associated with user satisfaction. Some other factors, such as unfamiliarity and similarity, are also discovered and summarized. Third, through annotations of the information aspects used in the human-generated self-explanation and peer-explanation, patterns of how humans explain are investigated, including what information and how much information is utilized. In addition, based on the findings, a human-inspired explanation approach is proposed and found to increase user satisfaction, revealing the potential improvement of further incorporating more human patterns in recommendation explanations. These findings have shed light on the deeper understanding of the recommendation explanation and further research on its evaluation and generation. Furthermore, the collected data, including human-generated explanations by both the external peers and the users' selves, will be released to support future research works on explanation evaluation.  © 2023 Association for Computing Machinery.",recommendation explanation; Recommender system; user modeling,Decision making; User profile; Human decision-making; Learn+; Recommendation explanation; Self explanations; User Modelling; User need; User perceptions; User study; User's intentions; Users' satisfactions; Recommender systems
Few-shot Aspect Category Sentiment Analysis via Meta-learning,2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143846850&doi=10.1145%2f3529954&partnerID=40&md5=a042c59eef05b6551eb797f227e9bcff,"Existing aspect-based/category sentiment analysis methods have shown great success in detecting sentiment polarity toward a given aspect in a sentence with supervised learning, where the training and inference stages share the same pre-defined set of aspects. However, in practice, the aspect categories are changing rather than keeping fixed over time. Dealing with unseen aspect categories is under-explored in existing methods. In this article, we formulate a new few-shot aspect category sentiment analysis (FSACSA) task, which aims to effectively predict the sentiment polarity of previously unseen aspect categories. To this end, we propose a novel Aspect-Focused Meta-Learning (AFML) framework that constructs aspect-Aware and aspect-contrastive representations from external knowledge to match the target aspect with aspects in the training set. Concretely, we first construct two auxiliary contrastive sentences for a given sentence with the incorporation of external knowledge, enabling the learning of sentence representations with a better generalization. Then, we devise an aspect-focused induction network to leverage the contextual sentiment toward a given aspect to refine the label vectors. Furthermore, we employ the episode-based meta-learning algorithm to train the whole network, so as to learn to generalize to novel aspects. Extensive experiments on multiple real-life datasets show that our proposed AFML framework achieves the state-of-The-Art results for the FSACSA task. © 2023 Association for Computing Machinery.",Few-shot aspect category sentiment analysis; few-shot learning; meta-learning; sentiment analysis,Data mining; Knowledge management; Learning algorithms; Analysis method; External knowledge; Few-shot aspect category sentiment analyse; Few-shot learning; Inference stages; Meta-learning frameworks; Metalearning; Sentiment analysis; Training sets; Sentiment analysis
Position-Enhanced and Time-Aware Graph Convolutional Network for Sequential Recommendations,2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149401498&doi=10.1145%2f3511700&partnerID=40&md5=f8d8fcb9928e31f5dc2b28083efed5cc,"The sequential recommendation (also known as the next-item recommendation), which aims to predict the following item to recommend in a session according to users' historical behavior, plays a critical role in improving session-based recommender systems. Most of the existing deep learning-based approaches utilize the recurrent neural network architecture or self-Attention to model the sequential patterns and temporal influence among a user's historical behavior and learn the user's preference at a specific time. However, these methods have two main drawbacks. First, they focus on modeling users' dynamic states from a user-centric perspective and always neglect the dynamics of items over time. Second, most of them deal with only the first-order user-item interactions and do not consider the high-order connectivity between users and items, which has recently been proved helpful for the sequential recommendation. To address the above problems, in this article, we attempt to model user-item interactions by a bipartite graph structure and propose a new recommendation approach based on a Position-enhanced and Time-Aware Graph Convolutional Network (PTGCN) for the sequential recommendation. PTGCN models the sequential patterns and temporal dynamics between user-item interactions by defining a position-enhanced and time-Aware graph convolution operation and learning the dynamic representations of users and items simultaneously on the bipartite graph with a self-Attention aggregator. Also, it realizes the high-order connectivity between users and items by stacking multi-layer graph convolutions. To demonstrate the effectiveness of PTGCN, we carried out a comprehensive evaluation of PTGCN on three real-world datasets of different sizes compared with a few competitive baselines. Experimental results indicate that PTGCN outperforms several state-of-The-Art sequential recommendation models in terms of two commonly-used evaluation metrics for ranking. In particular, it can make a better trade-off between recommendation performance and model training efficiency, which holds great potential for online session-based recommendation scenarios in the future. © 2023 Association for Computing Machinery.",dynamic item embedding; graph convolution; high-order connectivity; self-Attention aggregator; Sequential recommendation,Behavioral research; Data mining; Economic and social effects; Graph neural networks; Graph theory; Network architecture; Recommender systems; Recurrent neural networks; User profile; Convolutional networks; Dynamic item embedding; Embeddings; Graph convolution; High-order; High-order connectivity; Higher-order; Self-attention aggregator; Sequential patterns; Sequential recommendation; Convolution
Sequential Recommendation with Multiple Contrast Signals,2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149399631&doi=10.1145%2f3522673&partnerID=40&md5=414f668834872dc09b8c22372176917d,"Sequential recommendation has become a trending research topic for its capability to capture dynamic user intents based on historical interaction sequence. To train a sequential recommendation model, it is a common practice to optimize the next-item recommendation task with a pairwise ranking loss. In this paper, we revisit this typical training method from the perspective of contrastive learning and find it can be taken as a specialized contrastive learning task conceptually and mathematically, named context-Target contrast. Further, to leverage other self-supervised signals in user interaction sequences, we propose another contrastive learning task to encourage sequences after augmentation, as well as sequences with the same target item, to have similar representations, called context-context contrast. A general framework, ContraRec, is designed to unify the two kinds of contrast signals, leading to a holistic joint-learning framework for sequential recommendation with different contrastive learning tasks. Besides, various sequential recommendation methods (e.g., GRU4Rec, Caser, and BERT4Rec) can be easily integrated as the base sequence encoder in our ContraRec framework. Extensive experiments on three public datasets demonstrate that ContraRec achieves superior performance compared to state-of-The-Art sequential recommendation methods. © 2023 Association for Computing Machinery.",contrastive learning; Recommender system; self-supervised learning; sequential recommendation,Learning systems; Contrastive learning; Joint learning; Learning frameworks; Learning tasks; Recommendation methods; Research topics; Self-supervised learning; Sequential recommendation; Training methods; User interaction; Recommender systems
Follow the Timeline! Generating an Abstractive and Extractive Timeline Summary in Chronological Order,2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149386675&doi=10.1145%2f3517221&partnerID=40&md5=88d565e3412e957dce5e9d218a63923b,"Today, timestamped web documents related to a general news query flood the Internet, and timeline summarization targets this concisely by summarizing the evolution trajectory of events along the timeline. Unlike traditional document summarization, timeline summarization needs to model the time series information of the input events and summarize important events in chronological order. To tackle this challenge, in this article we propose our Unified Timeline Summarizer, which can generate abstractive and extractive timeline summaries in time order. Concretely, in the encoder part, we propose a graph-based event encoder that relates multiple events according to their content dependency and learns a global representation of each event. In the decoder part, to ensure the chronological order of the abstractive summary, we propose to extract the feature of event-level attention in its generation process with sequential information retained and use it to simulate the evolutionary attention of the ground truth summary. The event-level attention can also be used to assist in extracting a summary, where the extracted summary also comes in time sequence. We augment the previous Chinese large-scale timeline summarization dataset and collect a new English timeline dataset. Extensive experiments conducted on these datasets and on the out-of-domain Timeline 17 dataset show that our Unified Timeline Summarizer achieves state-of-The-Art performance in terms of both automatic and human evaluations.1. © 2023 Association for Computing Machinery.",abstractive summarization; extractive summarization; Timeline summarization,Graphic methods; Signal encoding; Abstractive summarization; Chronological order; Document summarization; Extractive summarizations; Graph-based; Multiple events; Time ordering; Time series informations; Timeline summarization; Web document; Large dataset
PerCLTV: A General System for Personalized Customer Lifetime Value Prediction in Online Games,2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149361255&doi=10.1145%2f3530012&partnerID=40&md5=2f0d1f5418520fed306153d22be630d1,"Online games make up the largest segment of the booming global game market in terms of revenue as well as players. Unlike games that sell games at one time for profit, online games make money from in-game purchases by a large number of engaged players. Therefore, Customer Lifetime Value (CLTV) is particularly vital for game companies to improve marketing decisions and increase game revenues. Nowadays, as virtual game worlds are becoming increasingly innovative, complex, and diverse, the CLTV of massive players is highly personalized. That is, different players may have very different patterns of CLTV, especially on churn and payment. However, current solutions are inadequate in terms of personalization and thus limit predictive performance. First, most methods just attempt to address either task of CLTV, i.e., churn or payment, and only consider the personalization from one of them. Second, the correlation between churn and payment has not received enough attention and its personalization has not been fully explored yet. Last, most solutions around this line are conducted based on historical data where the evaluation is not convincing enough without real-world tests. To tackle these problems, we propose a general system to predict personalized customer lifetime value in online games, named perCLTV. To be specific, we revisit the personalized CLTV prediction problem from the two sub-Tasks of churn prediction and payment prediction in a sequential gated multi-Task learning fashion. On this basis, we develop a generalized framework to model CLTV across games in distinct genres by heterogeneous player behavior data, including individual behavior sequential data and social behavior graph data. Comprehensive experiments on three real-world datasets validate the effectiveness and rationality of perCLTV, which significantly outperforms other baseline methods. Our work has been implemented and deployed in many online games released from NetEase Games. Online A/B testing in production shows that perCLTV achieves a prominent improvement in two precision marketing applications of popup recommendation and churn intervention. © 2023 Association for Computing Machinery.",churn prediction; multi-Task learning; online games; payment prediction; Personalized customer lifetime value,Commerce; Computer games; E-learning; Learning systems; Sales; Social networking (online); Value engineering; Churn predictions; Customer lifetime value; General systems; Multitask learning; On-line games; One-time; Payment prediction; Personalizations; Personalized customer lifetime value; Value prediction; Forecasting
Users Meet Clarifying Questions: Toward a Better Understanding of User Interactions for Search Clarification,2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149409564&doi=10.1145%2f3524110&partnerID=40&md5=0790c2a6124db51035416e1f917030a2,"The use of clarifying questions (CQs) is a fairly new and useful technique to aid systems in recognizing the intent, context, and preferences behind user queries. Yet, understanding the extent of the effect of CQs on user behavior and the ability to identify relevant information remains relatively unexplored. In this work, we conduct a large user study to understand the interaction of users with CQs in various quality categories, and the effect of CQ quality on user search performance in terms of finding relevant information, search behavior, and user satisfaction. Analysis of implicit interaction data and explicit user feedback demonstrates that high-quality CQs improve user performance and satisfaction. By contrast, low-And mid-quality CQs are harmful, and thus allowing the users to complete their tasks without CQ support may be preferred in this case. We also observe that user engagement, and therefore the need for CQ support, is affected by several factors, such as search result quality or perceived task difficulty. The findings of this study can help researchers and system designers realize why, when, and how users interact with CQs, leading to a better understanding and design of search clarification systems. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",clarifying questions; information seeking systems; User study,Behavioral research; Clarification; Clarifiers; Clarifying question; Information seeking; Information seeking system; Large users; Search performance; User behaviors; User interaction; User query; User study; Users' satisfactions; Search engines
Contextual Path Retrieval: A Contextual Entity Relation Embedding-based Approach,2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149413808&doi=10.1145%2f3502720&partnerID=40&md5=6e54368593a8c548437fd901119a19da,"Contextual path retrieval (CPR) refers to the task of finding contextual path(s) between a pair of entities in a knowledge graph that explains the connection between them in a given context. For this novel retrieval task, we propose the Embedding-based Contextual Path Retrieval (ECPR) framework. ECPR is based on a three-component structure that includes a context encoder and path encoder that encode query context and path, respectively, and a path ranker that assigns a ranking score to each candidate path to determine the one that should be the contextual path. For context encoding, we propose two novel context encoding methods, i.e., context-fused entity embeddings and contextualized embeddings. For path encoding, we propose PathVAE, an inductive embedding approach to generate path representations. Finally, we explore two path-ranking approaches. In our evaluation, we construct a synthetic dataset from Wikipedia and two real datasets of Wikinews articles constructed through crowdsourcing. Our experiments show that methods based on ECPR framework outperform baseline methods, and that our two proposed context encoders yield significantly better performance than baselines. We also analyze a few case studies to show the distinct features of ECPR-based methods. © 2023 Association for Computing Machinery.",embedding learning; information retrieval; Knowledge base; reasoning,Encoding (symbols); Information retrieval; Knowledge graph; Knowledge management; Query processing; Signal encoding; Embedding learning; Embeddings; Knowledge base; Knowledge graphs; Path retrieval; Query paths; Reasoning; Relation embedding; Retrieval frameworks; Three-component structures; Embeddings
Reconciling the Quality vs Popularity Dichotomy in Online Cultural Markets,2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149376238&doi=10.1145%2f3530790&partnerID=40&md5=5bfbb5f60dae6d3f59b7f12480c58df8,"We propose a simple model of an idealized online cultural market in which N items, endowed with a hidden quality metric, are recommended to users by a ranking algorithm possibly biased by the current items' popularity. Our goal is to better understand the underlying mechanisms of the well-known fact that popularity bias can prevent higher-quality items from becoming more popular than lower-quality items, producing an undesirable misalignment between quality and popularity rankings. We do so under the assumption that users, having limited time/attention, are able to discriminate the best-quality only within a random subset of the items. We discover the existence of a harmful regime in which improper use of popularity can seriously compromise the emergence of quality, and a benign regime in which wise use of popularity, coupled with a small discrimination effort on behalf of users, guarantees the perfect alignment of quality and popularity ranking. Our findings clarify the effects of algorithmic popularity bias on quality outcomes, and may inform the design of more principled mechanisms for techno-social cultural markets. © 2023 Association for Computing Machinery.",Online cultural markets; popularity bias; ranking algorithms; retrieval diversity,Information retrieval; 'current; Item popularities; Online cultural market; Popularity bias; Popularity ranking; Quality metrices; Quality ranking; Ranking algorithm; Retrieval diversity; Simple modeling; Commerce
Curriculum Pre-Training Heterogeneous Subgraph Transformer for Top-N Recommendation,2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85148634766&doi=10.1145%2f3528667&partnerID=40&md5=614f780cb8b4da56d3484055561a9398,"To characterize complex and heterogeneous side information in recommender systems, the heterogeneous information network (HIN) has shown superior performance and attracted much research attention. In HIN, the rich entities, relations, and paths can be utilized to model the correlations of users and items; such a task setting is often called HIN-based recommendation. Although HIN provides a general approach to modeling rich side information, it lacks special consideration on the goal of the recommendation task. The aggregated context from the heterogeneous graph is likely to incorporate irrelevant information, and the learned representations are not specifically optimized according to the recommendation task. Therefore, there is a need to rethink how to leverage the useful information from HIN to accomplish the recommendation task. To address the above issues, we propose a Curriculum pre-Training based HEterogeneous Subgraph Transformer (called CHEST) with new data characterization, representation model, and learning algorithm. Specifically, we consider extracting useful information from HIN to compose the interaction-specific heterogeneous subgraph, containing highly relevant context information for recommendation. Then, we capture the rich semantics (e.g., graph structure and path semantics) within the subgraph via a heterogeneous subgraph Transformer, where we encode the subgraph into multi-slot sequence representations. Besides, we design a curriculum pre-Training strategy to provide an elementary-To-Advanced learning process. The elementary course focuses on capturing local context information within the subgraph, and the advanced course aims to learn global context information. In this way, we gradually capture useful semantic information from HIN for modeling user-item interactions. Extensive experiments conducted on four real-world datasets demonstrate the superiority of our proposed method over a number of competitive baselines, especially when only limited training data is available. © 2023 Association for Computing Machinery.",Curriculum pre-Training; heterogeneous information network; recommender systems,Information services; Information use; Learning algorithms; Learning systems; Recommender systems; Semantics; User profile; Context information; Curriculum pre-training; Heterogeneous information; Heterogeneous information network; Information networks; Network-based; Performance; Pre-training; Side information; Subgraphs; Curricula
Learning Relation Ties with a Force-Directed Graph in Distant Supervised Relation Extraction,2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149360420&doi=10.1145%2f3520082&partnerID=40&md5=a1476468e1caad245ef8ab9ec9c5294b,"Relation ties, defined as the correlation and mutual exclusion between different relations, are critical for distant supervised relation extraction. Previous studies usually obtain this property by greedily learning the local connections between relations. However, they are essentially limited because of failing to capture the global topology structure of relation ties and may easily fall into a locally optimal solution. To address this issue, we propose a novel force-directed graph to comprehensively learn relation ties. Specifically, we first construct a graph according to the global co-occurrence of all relations. Then, we borrow the idea of Coulomb's law from physics and introduce the concept of attractive force and repulsive force into this graph to learn correlation and mutual exclusion between relations. Finally, the obtained relation representations are applied as an inter-dependent relation classifier. Extensive experimental results demonstrate that our method is capable of modeling global correlation and mutual exclusion between relations, and outperforms the state-of-The-Art baselines. In addition, the proposed force-directed graph can be used as a module to augment existing relation extraction systems and improve their performance. © 2023 Association for Computing Machinery.",Distant supervision; force-directed graph; relation extraction; relation ties,Data mining; Extraction; Distant supervision; Force-Directed; Force-directed graph; Learn+; Learning relations; Local connections; Mutual exclusions; Property; Relation extraction; Relation tie; Directed graphs
Interaction-Aware Drug Package Recommendation via Policy Gradient,2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149389885&doi=10.1145%2f3511020&partnerID=40&md5=6651cf1b809505ebff3f45e01e148fbe,"Recent years have witnessed the rapid accumulation of massive electronic medical records, which highly support intelligent medical services such as drug recommendation. However, although there are multiple interaction types between drugs, e.g., synergism and antagonism, which can influence the effect of a drug package significantly, prior arts generally neglect the interaction between drugs or consider only a single type of interaction. Moreover, most existing studies generally formulate the problem of package recommendation as getting a personalized scoring function for users, despite the limits of discriminative models to achieve satisfactory performance in practical applications. To this end, in this article, we propose a novel end-To-end Drug Package Generation (DPG) framework, which develops a new generative model for drug package recommendation that considers the interaction effects between drugs that are affected by patient conditions. Specifically, we propose to formulate the drug package generation as a sequence generation process. Along this line, we first initialize the drug interaction graph based on medical records and domain knowledge. Then, we design a novel message-passing neural network to capture the drug interaction, as well as a drug package generator based on a recurrent neural network. In detail, a mask layer is utilized to capture the impact of patient condition, and the deep reinforcement learning technique is leveraged to reduce the dependence on the drug order. Finally, extensive experiments on a real-world dataset from a first-rate hospital demonstrate the effectiveness of our DPG framework compared with several competitive baseline methods. © 2023 Association for Computing Machinery.",Drug recommendation; graph neural network; package recommendation; reinforcement learning,Data mining; Domain Knowledge; Drug interactions; Graph neural networks; Graphic methods; Learning systems; Medical computing; Message passing; Recurrent neural networks; Drug recommendation; Graph neural networks; Medical record; Medical services; Multiple interactions; Package generation; Package recommendations; Patients' conditions; Policy gradient; Reinforcement learnings; Reinforcement learning
A Static and Dynamic Attention Framework for Multi Turn Dialogue Generation,2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149404449&doi=10.1145%2f3522763&partnerID=40&md5=46b35116ab7387d08911ea9b722a03ad,"Recently, research on open domain dialogue systems have attracted extensive interests of academic and industrial researchers. The goal of an open domain dialogue system is to imitate humans in conversations. Previous works on single turn conversation generation have greatly promoted the research of open domain dialogue systems. However, understanding multiple single turn conversations is not equal to the understanding of multi turn dialogue due to the coherent and context dependent properties of human dialogue. Therefore, in open domain multi turn dialogue generation, it is essential to modeling the contextual semantics of the dialogue history rather than only according to the last utterance. Previous research had verified the effectiveness of the hierarchical recurrent encoder-decoder framework on open domain multi turn dialogue generation. However, using an RNN-based model to hierarchically encoding the utterances to obtain the representation of dialogue history still face the problem of a vanishing gradient. To address this issue, in this article, we proposed a static and dynamic attention-based approach to model the dialogue history and then generate open domain multi turn dialogue responses. Experimental results on the Ubuntu and Opensubtitles datasets verify the effectiveness of the proposed static and dynamic attention-based approach on automatic and human evaluation metrics in various experimental settings. Meanwhile, we also empirically verify the performance of combining the static and dynamic attentions on open domain multi turn dialogue generation. © 2023 Association for Computing Machinery.",attentive neural network; dialogue generation; multi turn dialogue; Open domain dialogue systems,Industrial research; Signal encoding; Speech processing; Attentive neural network; Context dependent; Dialogue generations; Dialogue systems; Multi turn dialog; Multi-turn; Neural-networks; Open domain dialog system; Property; Statics and dynamics; Semantics
Characterization and Prediction of Mobile Tasks,2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149401178&doi=10.1145%2f3522711&partnerID=40&md5=08eb7f5770bfbab40bd1a1cddf975061,"Mobile devices have become an increasingly ubiquitous part of our everyday life. We use mobile services to perform a broad range of tasks (e.g., booking travel or conducting remote office work), leading to often lengthy interactions with several distinct apps and services. Existing mobile systems handle mostly simple user needs, where a single app is taken as the unit of interaction. To understand users' expectations and to provide context-Aware services, it is important to model users' interactions with their performed task in mind. To provide a comprehensive picture of common mobile tasks, we first conduct a small-scale user study to understand annotated mobile tasks in-depth, while we demonstrate that by using a set of features (temporal, similarity, and log sequence), we can identify if a pair of app usage belong to the same task effectively. Secondly, the proposed best task detection model is applied to a large-scale data set of commercial mobile app usage logs to infer characteristics of complex (multi-App) mobile tasks in the wild. By applying an unsupervised learning framework, we discover common mobile task types that span multiple apps based on various extracted characteristics. We observe that users generally perform 17 common tasks with 47 sub-Tasks, ranging from ""social media browsing""to ""dining out""and ""family entertainments"". Finally, we demonstrate that we can predict the next complex mobile task that users are likely to perform by leveraging features from the historically inferred mobile tasks and user contexts. Our work facilitates an in-depth understanding of mobile tasks at scale, enabling applications for promoting task-Aware services. © 2023 Copyright held by the owner/author(s).",clustering; mobile apps; Mobile task; multi-App tasks; prediction,Information services; Mobile telecommunication systems; Clusterings; Mobile app; Mobile service; Mobile systems; Mobile task; Multi-app task; Office works; Simple++; User expectations; User need; Forecasting
Integrating Representation and Interaction for Context-Aware Document Ranking,2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149407285&doi=10.1145%2f3529955&partnerID=40&md5=b60923405f2f312e2f0709089809cd14,"Recent studies show that historical behaviors (such as queries and their clicks) contained in a search session can benefit the ranking performance of subsequent queries in the session. Existing neural context-Aware ranking models usually rank documents based on either latent representations of user search behaviors or the word-level interactions between the candidate document and each historical behavior in the search session. However, these two kinds of models both have their own drawbacks. Representation-based models neglect fine-grained information on word-level interactions, whereas interaction-based models suffer from the length restriction of session sequence because of the large cost of word-level interactions. To complement the limitations of these two kinds of models, we propose a unified context-Aware document ranking model that takes full advantage of both representation and interaction. Specifically, instead of matching a candidate document with every single historical query in a session, we encode the session history into a latent representation and use this representation to enhance the current query and the candidate document. We then just match the enhanced query and candidate document with several matching components to capture the fine-grained information of word-level interactions. Rich experiments on two public query logs prove the effectiveness and efficiency of our model for leveraging representation and interaction. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Document ranking; neural-IR; session search,Behavioral research; Information retrieval; Context-Aware; Document ranking; Fine grained; Matchings; Neural-IR; Ranking model; Ranking performance; Search sessions; Session search; Word level; Search engines
KR-GCN: Knowledge-Aware Reasoning with Graph Convolution Network for Explainable Recommendation,2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147508471&doi=10.1145%2f3511019&partnerID=40&md5=999b075393276b39354422c3ba8b5847,"Incorporating knowledge graphs (KGs) into recommender systems to provide explainable recommendation has attracted much attention recently. The multi-hop paths in KGs can provide auxiliary facts for improving recommendation performance as well as explainability. However, existing studies may suffer from two major challenges: error propagation and weak explainability. Considering all paths between every user-item pair might involve irrelevant ones, which leads to error propagation of user preferences. Defining meta-paths might alleviate the error propagation, but the recommendation performance would heavily depend on the pre-defined meta-paths. Some recent methods based on graph convolution network (GCN) achieve better recommendation performance, but fail to provide explainability. To tackle the above problems, we propose a novel method named Knowledge-aware Reasoning with Graph Convolution Network (KR-GCN). Specifically, to alleviate the effect of error propagation, we design a transition-based method to determine the triple-level scores and utilize nucleus sampling to select triples within the paths between every user-item pair adaptively. To improve the recommendation performance and guarantee the diversity of explanations, user-item interactions and knowledge graphs are integrated into a heterogeneous graph, which is performed with the graph convolution network. A path-level self-attention mechanism is adopted to discriminate the contributions of different selected paths and predict the interaction probability, which improves the relevance of the final explanation. Extensive experiments conducted on three real-world datasets show that KR-GCN consistently outperforms several state-of-the-art baselines. And human evaluation proves the superiority of KR-GCN on explainability. © 2023 Association for Computing Machinery.",error propagation; Explainable recommendation; graph convolution network; knowledge graphs,Data mining; Errors; Knowledge graph; Recommender systems; Error propagation; Explainable recommendation; Graph convolution network; Interaction graphs; Knowledge graphs; Multi-hop path; Network knowledge; Novel methods; Recommendation performance; User's preferences; Convolution
A Multi-channel Hierarchical Graph Attention Network for Open Event Extraction,2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85148324751&doi=10.1145%2f3528668&partnerID=40&md5=1efeed0741d718de35ff69ac3e2ab7f3,"Event extraction is an essential task in natural language processing. Although extensively studied, existing work shares issues in three aspects, including (1) the limitations of using original syntactic dependency structure, (2) insufficient consideration of the node level and type information in Graph Attention Network (GAT), and (3) insufficient joint exploitation of the node dependency type and part-of-speech (POS) encoding on the graph structure. To address these issues, we propose a novel framework for open event extraction in documents. Specifically, to obtain an enhanced dependency structure with powerful encoding ability, our model is capable of handling an enriched parallel structure with connected ellipsis nodes. Moreover, through a bidirectional dependency parsing graph, it considers the sequence of order structure and associates the ancestor and descendant nodes. Subsequently, we further exploit node information, such as the node level and type, to strengthen the aggregation of node features in our GAT. Finally, based on the coordination of triple-channel features (i.e., semantic, syntactic dependency and POS), the performance of event extraction is significantly improved. Extensive experiments are conducted to validate the effectiveness of our method, and the results confirm its superiority over the state-of-The-Art baselines. Furthermore, in-depth analyses are provided to explore the essential factors determining the extraction performance. © 2023 Association for Computing Machinery.",bidirectional dependency parsing graph; Hierarchical Graph Attention Network; multiple channels; Open event extraction,Data mining; Encoding (symbols); Extraction; Graph structures; Graph theory; Graphic methods; Natural language processing systems; Signal encoding; Syntactics; Bidirectional dependency parsing graph; Dependency parsing; Dependency structures; Events extractions; Hierarchical graph attention network; Hierarchical graphs; Multiple channels; Open event extraction; Part Of Speech; Syntactic dependencies; Semantics
Personalized News Recommendation: Methods and Challenges,2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137796228&doi=10.1145%2f3530257&partnerID=40&md5=74d06f666efd49f1149e4c7171beffe9,"Personalized news recommendation is important for users to find interesting news information and alleviate information overload. Although it has been extensively studied over decades and has achieved notable success in improving user experience, there are still many problems and challenges that need to be further studied. To help researchers master the advances in personalized news recommendation, in this article, we present a comprehensive overview of personalized news recommendation. Instead of following the conventional taxonomy of news recommendation methods, in this article, we propose a novel perspective to understand personalized news recommendation based on its core problems and the associated techniques and challenges. We first review the techniques for tackling each core problem in a personalized news recommender system and the challenges they face. Next, we introduce the public datasets and evaluation methods for personalized news recommendation. We then discuss the key points on improving the responsibility of personalized news recommender systems. Finally, we raise several research directions that are worth investigating in the future. This article can provide up-To-date and comprehensive views on personalized news recommendation. We hope this article can facilitate research on personalized news recommendation as well as related fields in natural language processing and data mining. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",natural language processing; News recommendation; personalization; survey; user modeling,Data handling; Modeling languages; Natural language processing systems; Recommender systems; User profile; Core problems; Language processing; Natural language processing; Natural languages; News recommendation; News recommender systems; Personalizations; Personalized news; Recommendation methods; User Modelling; Data mining
Knowledge-Enhanced Attributed Multi-Task Learning for Medicine Recommendation,2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141260083&doi=10.1145%2f3527662&partnerID=40&md5=9756dd08eac951e5255f77b57ef8c90d,"Medicine recommendation systems target to recommend a set of medicines given a set of symptoms which play a crucial role in assisting doctors in their daily clinics. Existing approaches are either rule-based or supervised. However, the former heavily relies on expert labeling, which is time-consuming and costly to collect, and the latter suffers from the data sparse problem. To automate medicine recommendation on sparse data, we propose MedRec, which introduces two graphs in modeling: (1) a knowledge graph connecting diseases, medicines, symptoms, and examinations; (2) an attribute graph connecting medicines via shared attributes and molecular structures. These two graphs enhance the connectivity between symptoms and medicines, which thus alleviate the data sparse problem. By learning the interrelationship between diseases, medicines, symptoms and examinations and the inner relationship within medicine, we can acquire unified embedding representations of symptoms and medicines which can be used in medicine recommendation. The experimental results show that the proposed model outperforms state-of-the-art methods. In addition, we find that these two tasks: learning graph representation and medical recommendation can benefit each other. © 2023 Association for Computing Machinery.",graph neural network; knowledge graph embedding; Medicine recommendation; multi-task,Data mining; Graph embeddings; Graph theory; Knowledge graph; Graph embeddings; Graph neural networks; Knowledge graph embedding; Knowledge graphs; Labelings; Medicine recommendation; Multi tasks; Multitask learning; Rule based; Two-graphs; Graph neural networks
Reinforcement Routing on Proximity Graph for Efficient Recommendation,2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149378472&doi=10.1145%2f3512767&partnerID=40&md5=f5a981ce2f780b9adefe0f11d295200a,"We focus on Maximum Inner Product Search (MIPS), which is an essential problem in many machine learning communities. Given a query, MIPS finds the most similar items with the maximum inner products. Methods for Nearest Neighbor Search (NNS) which is usually defined on metric space do not exhibit the satisfactory performance for MIPS problem since inner product is a non-metric function. However, inner products exhibit many good properties compared with metric functions, such as avoiding vanishing and exploding gradients. As a result, inner product is widely used in many recommendation systems, which makes efficient Maximum Inner Product Search a key for speeding up many recommendation systems. Graph-based methods for NNS problem show the superiorities compared with other class methods. Each data point of the database is mapped to a node of the proximity graph. Nearest neighbor search in the database can be converted to route on the proximity graph to find the nearest neighbor for the query. This technique can be used to solve MIPS problem. Instead of searching the nearest neighbor for the query, we search the item with a maximum inner product with query on the proximity graph. In this article, we propose a reinforcement model to train an agent to search on the proximity graph automatically for MIPS problem if we lack the ground truths of training queries. If we know the ground truths of some training queries, our model can also utilize these ground truths by imitation learning to improve the agent's searchability. By experiments, we can see that our proposed mode which combines reinforcement learning with imitation learning shows the superiorities over the state-of-The-Art methods. © 2023 Association for Computing Machinery.",graph convolutional network; MIPS; non-metric; proximity graph; reinforcement learning; reward shaping,Graph Databases; Graphic methods; Query processing; Recommender systems; Reinforcement learning; Convolutional networks; Graph convolutional network; Inner product; Maximum inner product search; Near neighbor searches; Non-Metric; Proximity graphs; Reinforcement learnings; Reward shaping; Search problem; Nearest neighbor search
Generating Relevant and Informative Questions for Open-Domain Conversations,2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149406251&doi=10.1145%2f3510612&partnerID=40&md5=4fc7e9c158fa89da9897431c0e5ef2a7,"Recent research has highlighted the importance of mixed-initiative interactions in conversational search. To enable mixed-initiative interactions, information retrieval systems should be able to ask diverse questions, such as information-seeking, clarification, and open-ended ones. question generation (QG) of open-domain conversational systems aims at enhancing the interactiveness and persistence of human-machine interactions. The task is challenging because of the sparsity of question generation (QG)-specific data in conversations. Current work is limited to single-Turn interaction scenarios. We propose a context-enhanced neural question generation(CNQG) model that leverages the conversational context to predict question content and pattern, then perform question decoding. A hierarchical encoder framework is employed to obtain the discourse-level context representation. Based on this, we propose Review and Transit mechanisms to respectively select contextual keywords and predict new topic words to further construct the question content. Conversational context and the predicted question content are used to produce the question pattern, which in turn guides the question decoding process implemented by a recurrent decoder with a joint attention mechanism. To fully utilize the limited QG-specific data to train our question generator, we perform multi-Task learning with three auxiliary training objectives, i.e., question pattern prediction, Review, and Transit mechanisms. The required additional labeled data is obtained in a self-supervised way. We also design a weight decaying strategy to adjust the influences of various auxiliary learning tasks. To the best of our acknowledge, we are the first to extend the application of QG to the multi-Turn open-domain conversational scenario. Extensive experimental results demonstrate the effectiveness of our proposal and its main components on generating relevant and informative questions, with robust performance for contexts with various lengths. © 2023 Association for Computing Machinery.",context modeling; Conversational search; neural question generation; open-domain conversations,Decoding; Forecasting; Learning systems; Search engines; Context models; Conversational search; Conversational systems; Information seeking; Information-retrieval systems; Interaction information; Mixed-initiative interactions; Neural question generation; Open-domain conversation; Recent researches; Information retrieval systems
Hierarchical Sliding Inference Generator for Question-driven Abstractive Answer Summarization,2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85148133434&doi=10.1145%2f3511891&partnerID=40&md5=0c04dfb0daa83485024c81338d4bf4ac,"Text summarization on non-factoid question answering (NQA) aims at identifying the core information of redundant answer guidance using questions, which can dramatically improve answer readability and comprehensibility. Most existing approaches focus on extracting query-related sentences to construct a summary, where the logical connection of natural language and the hierarchical interpretable semantic association are often neglected, thus degrading performance. To address these issues, we propose a novel question-driven abstractive answer summarization model, called the Hierarchical Sliding Inference Generator (HSIG), to form inferable and interpretable summaries by explicitly introducing hierarchical information reasoning between questions and corresponding answers. Specifically, we first apply an elaborately designed hierarchical sliding fusion inference model to determine the most relevant question sentence-level representation that provides a deeper interpretable basis for sentence selection in summarization, which further increases computational performance on the premise of following the semantic inheritance structure. Additionally, to improve summary fluency, we construct a double-driven selective generator to integrate various semantic information from two mutual question-And-Answer perspectives. Experimental results illustrate that compared with state-of-The-Art baselines, our model achieves remarkable improvement on two benchmark datasets and specifically improves the 2.46 ROUGE-1 points on PubMedQA, which demonstrates the superiority of our model on abstractive summarization with hierarchical sequential reasoning. © 2023 Association for Computing Machinery.",Abstractive summarization; hierarchical sliding fusion; pointer generation network; question-driven,Natural language processing systems; Query processing; Abstractive summarization; Factoid questions; Hierarchical sliding fusion; Logical connections; Natural languages; Pointer generation network; Question Answering; Question-driven; Semantic associations; Text Summarisation; Semantics
User Behavior Simulation for Search Result Re-ranking,2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149390325&doi=10.1145%2f3511469&partnerID=40&md5=5a8b5a311cfbdfe51e38631281996bcd,"Result ranking is one of the major concerns for Web search technologies. Most existing methodologies rank search results in descending order of relevance. To model the interactions among search results, reinforcement learning (RL algorithms have been widely adopted for ranking tasks. However, the online training of RL methods is time and resource consuming at scale. As an alternative, learning ranking policies in the simulation environment is much more feasible and efficient. In this article, we propose two different simulation environments for the offline training of the RL ranking agent: The Context-Aware Click Simulator (CCS) and the Fine-grained User Behavior Simulator with GAN (UserGAN). Based on the simulation environment, we also design a User Behavior Simulation for Reinforcement Learning (UBS4RL) re-ranking framework, which consists of three modules: A feature extractor for heterogeneous search results, a user simulator for collecting simulated user feedback, and a ranking agent for generation of optimized result lists. Extensive experiments on both simulated and practical Web search datasets show that (1) the proposed user simulators can capture and simulate fine-grained user behavior patterns by training on large-scale search logs, (2) the temporal information of user searching process is a strong signal for ranking evaluation, and (3) learning ranking policies from the simulation environment can effectively improve the search ranking performance. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",generative adversarial networks; Information retrieval; ranking; reinforcement learning; user simulation,Behavioral research; Information retrieval; Large dataset; Websites; Behaviors simulation; Fine grained; Ranking; Reinforcement learnings; Search results re rankings; Search technology; Simulation environment; User behaviors; User simulation; Web searches; Reinforcement learning
Toward Equivalent Transformation of User Preferences in Cross Domain Recommendation,2023,ACM Transactions on Information Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149378298&doi=10.1145%2f3522762&partnerID=40&md5=89dbd61b7ebd2e545344a8b108655c60,"Cross domain recommendation (CDR) is one popular research topic in recommender systems. This article focuses on a popular scenario for CDR where different domains share the same set of users but no overlapping items. The majority of recent methods have explored the shared-user representation to transfer knowledge across domains. However, the idea of shared-user representation resorts to learning the overlapped features of user preferences and suppresses the domain-specific features. Other works try to capture the domain-specific features by an MLP mapping but require heuristic human knowledge of choosing samples to train the mapping. In this article, we attempt to learn both features of user preferences in a more principled way. We assume that each user's preferences in one domain can be expressed by the other one, and these preferences can be mutually converted to each other with the so-called equivalent transformation. Based on this assumption, we propose an equivalent transformation learner (ETL), which models the joint distribution of user behaviors across domains. The equivalent transformation in ETL relaxes the idea of shared-user representation and allows the learned preferences in different domains to preserve the domain-specific features as well as the overlapped features. Extensive experiments on three public benchmarks demonstrate the effectiveness of ETL compared with recent state-of-The-Art methods. Codes and data are available online: https://github.com/xuChenSJTU/ETL-master. © 2023 Association for Computing Machinery.",collaborative filtering; Cross domain recommendation; domain-specific features; equivalent transformation; knowledge transfer; variational inference,Behavioral research; Clock and data recovery circuits (CDR circuits); Collaborative filtering; Knowledge management; Cross-domain recommendations; Different domains; Domain specific; Domain-specific feature; Equivalent transformations; Heuristic human knowledge; Knowledge transfer; Research topics; User's preferences; Variational inference; Mapping
