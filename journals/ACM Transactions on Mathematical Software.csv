Title,Year,Source title,Link,Abstract,Author Keywords,Index Keywords
Algorithm 953: Parallel library software for the multishift qr algorithm with aggressive early deflation,2015,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84943313977&doi=10.1145%2f2699471&partnerID=40&md5=cdae6af5c5a3446789ef209445718d9f,"Library software implementing a parallel small-bulge multishift QR algorithm with Aggressive Early Deflation (AED) targeting distributed memory high-performance computing systems is presented. Starting from recent developments of the parallel multishift QR algorithm [Granat et al., SIAM J. Sci. Comput. 32(4), 2010], we describe a number of algorithmic and implementation improvements. These include communication avoiding algorithms via data redistribution and a refined strategy for balancing between multishift QR sweeps and AED. Guidelines concerning several important tunable algorithmic parameters are provided. As a result of these improvements, a computational bottleneck within AED has been removed in the parallel multishift QR algorithm. A performance model is established to explain the scalability behavior of the new parallel multishift QR algorithm. Numerous computational experiments confirm that our new implementation significantly outperforms previous parallel implementations of the QR algorithm. © 2015 ACM.",Aggressive early deflation; Distributed memory architectures; Multishift QR algorithm; Parallel algorithms,Computer software; Parallel algorithms; Software engineering; Aggressive early deflation; Communication avoiding algorithms; Computational bottlenecks; Computational experiment; Distributed memory architecture; High performance computing systems; Parallel implementations; QR algorithms; Memory architecture
CGALmesh: A generic framework for delaunay mesh generation,2015,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84944585924&doi=10.1145%2f2699463&partnerID=40&md5=2fc65cb3104d5ed74086a1b44fc9683e,"CGALmesh is the mesh generation software package of the Computational Geometry Algorithm Library (CGAL). It generates isotropic simplicial meshesâ€""surface triangular meshes or volume tetrahedral meshesâ€""from input surfaces, 3D domains, and 3D multidomains, with or without sharp features. The underlying meshing algorithm relies on restricted Delaunay triangulations to approximate domains and surfaces and on Delaunay refinement to ensure both approximation accuracy and mesh quality. CGALmesh provides guarantees on approximation quality and on the size and shape of the mesh elements. It provides four optional mesh optimization algorithms to further improve the mesh quality. A distinctive property of CGALmesh is its high flexibility with respect to the input domain representation. Such a flexibility is achieved through a careful software design, gathering into a single abstract concept, denoted by the oracle, all required interface features between themeshing engine and the input domain.We already provide oracles for domains defined by polyhedral and implicit surfaces. © 2015 ACM.",Delaunay refinement; Isosurface extraction; Mesh generation; Mesh optimization,Approximation algorithms; Software design; Approximation accuracy; Approximation quality; Computational geometry algorithms; Delaunay refinements; Domain representations; Isosurface extraction; Mesh optimization; Restricted delaunay triangulation; Mesh generation
Algorithmic differentiation of numerical methods: Tangent and adjoint solvers for parameterized systems of nonlinear equations,2015,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84939206796&doi=10.1145%2f2700820&partnerID=40&md5=d745c797942f45fa979861a4eebe17a2,"We discuss software tool support for the algorithmic differentiation (AD), also known as automatic differentiation, of numerical simulation programs that contain calls to solvers for parameterized systems of n nonlinear equations. The local computational overhead and the additional memory requirement for the computation of directional derivatives or adjoints of the solution of the nonlinear system with respect to the parameters can quickly become prohibitive for large values of n. Both are reduced drastically by analytical (and symbolic) approaches to differentiation of the underlying numerical methods. Following the discussion of the proposed terminology, we develop the algorithmic formalism building on prior work by other colleagues and present an implementation based on the AD software dco/c++. A representative case study supports the theoretically obtained computational complexity results with practical runtime measurements. © 2015 ACM.",Algorithmic differentiation; Tangent/adjoint nonlinear solver,Computer aided software engineering; Computer software; Numerical methods; Algorithmic differentiations; Automatic differentiations; Computational overheads; Directional derivative; Memory requirements; Non-linear solver; Parameterized system; Run-time measurement; Nonlinear equations
Algorithm 954: An accurate and efficient cubic and quartic equation solver for physical applications,2015,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84944627326&doi=10.1145%2f2699468&partnerID=40&md5=570cabffd7acc23ef340936f26efde80,"We report on an accurate and efficient algorithm for obtaining all roots of general real cubic and quartic polynomials. Both the cubic and quartic solvers give highly accurate roots and place no restrictions on the magnitude of the polynomial coefficients. The key to the algorithm is a proper rescaling of both polynomials. This puts upper bounds on the magnitude of the roots and is very useful in stabilizing the root finding process. The cubic solver is based on dividing the cubic polynomial into six classes. By analyzing the root surface for each class, a fast convergent Newton-Raphson starting point for a real root is obtained at a cost no higher than three additions and four multiplications. The quartic solver uses the cubic solver in getting information about stationary points and, when the quartic has real roots, stable Newton-Raphson iterations give one of the extreme real roots. The remaining roots follow by composite deflation to a cubic. If the quartic has only complex roots, the present article shows that a stable Newton-Raphson iteration on a derived symmetric sixth degree polynomial can be formulated for the real parts of the complex roots. The imaginary parts follow by solving suitable quadratics. © 2015 ACM.",Cubic equation; Newton-Raphson scheme; Polynomial roots; Quartic equation,Iterative methods; Linear stability analysis; Cubic equations; Newton Raphson iteration; Newton Raphson Scheme; Physical application; Polynomial coefficients; Polynomial roots; Quartic equation; Quartic polynomial; Polynomials
Algorithm 952: PHquintic: A library of basic functions for the construction and analysis of planar quintic pythagorean-hodograph curves,2015,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84944575890&doi=10.1145%2f2699467&partnerID=40&md5=4e833e33d542f3fe55c73fdac34f7b21,"The implementation of a library of basic functions for the construction and analysis of planar quintic Pythagorean-hodograph (PH) curves is presented using the complex representation. The special algebraic structure of PH curves permits exact algorithms for the computation of key properties, such as arc length, elastic bending energy, and offset (parallel) curves. Single planar PH quintic segments are constructed as interpolants to first-order Hermite data (end points and derivatives), and this construction is then extended to open or closed C2 PH quintic spline curves interpolating a sequence of points in the plane. The nonlinear nature of PH curves incurs a multiplicity of formal solutions to such interpolation problems, and a key aspect of the algorithms is to efficiently single out the unique ""good"" interpolant among them. © 2015 ACM.","Arc length bending energy; Computerâ€""aided geometric design; Hermite and spline interpolation; Newton-Raphson method; Offset curves; Pythagorean-hodograph curves",Curve fitting; Kinematics; Newton-Raphson method; Bending energies; Geometric design; Offset curve; Pythagorean hodograph curves; Spline interpolation; Interpolation
Algorithm 951: Cayley analysis of mechanism configuration spaces using caymos: Software functionalities and architecture,2015,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84944619836&doi=10.1145%2f2699462&partnerID=40&md5=0ac859713be9591c1c036b9be7f8890c,"For a common class of two-dimensional (2D)mechanisms called 1-dof tree-decomposable linkages, we present a software package, CayMos,which uses new theoretical results from Sitharam andWang [2014] and Sitharam et al. [2011a, 2011b] to implement efficient algorithmic solutions for (a) meaningfully representing and visualizing the connected components in the Euclidean realization space; (b) finding a path of continuous motion between two realizations in the same connected component, with or without restricting the realization type (sometimes called orientation type); and (c) finding two â€œclosestâ€ realizations in different connected components. © 2015 ACM.",1-degree-of-freedom linkage; Computer-aided design (CAD); Motion space; Realization space,Computerized tomography; 1-degree-of-freedom linkage; Algorithmic solutions; Analysis of mechanisms; Configuration space; Motion space; Realization space; Software functionality; Two Dimensional (2 D); Computer aided design
Optimizing sparse matrix-matrix multiplication for the GPU,2015,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84942000185&doi=10.1145%2f2699470&partnerID=40&md5=e6fb61dfdb1c69aef62783455c2c4344,"Sparse matrix-matrix multiplication (SpGEMM) is a key operation in numerous areas from information to the physical sciences. Implementing SpGEMM efficiently on throughput-oriented processors, such as the graphics processing unit (GPU), requires the programmer to expose substantial fine-grained parallelism while conserving the limited off-chip memory bandwidth. Balancing these concerns, we decompose the SpGEMM operation into three highly parallel phases: expansion, sorting, and contraction, and introduce a set of complementary bandwidth-saving performance optimizations. Our implementation is fully general and our optimization strategy adaptively processes the SpGEMM workload row-wise to substantially improve performance by decreasing the work complexity and utilizing the memory hierarchy more effectively. ï¿½ 2015 ACM 0098-3500/2015/10-ART25 $15.00.",GPU; Matrix-matrix; Parallel; Sparse,Bandwidth; Computer graphics; Image coding; Matrix algebra; Program processors; Fine-grained parallelism; GPU; Graphics Processing Unit; Optimization strategy; Parallel; Performance optimizations; Sparse; Sparse matrix-matrix multiplications; Computer graphics equipment
Efficient calculations of faithfully rounded l2-norms of n-vectors,2015,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84944538934&doi=10.1145%2f2699469&partnerID=40&md5=afd595d5ad1cf45e2fd9056dab0cbd74,"In this article, we present an efficient algorithm to compute the faithful rounding of the l2-norm of a floatingpoint vector. This means that the result is accurate to within 1 bit of the underlying floating-point type. This algorithm does not generate overflows or underflows spuriously, but does so when the final result calls for such a numerical exception to be raised. Moreover, the algorithm is well suited for parallel implementation and vectorization. The implementation runs up to 3 times faster than the netlib version on current processors. © 2015 ACM.",2-norm; Error-free transformations; Faithful rounding; Floating-point arithmetic; Overflow; Underflow,Computer software; Software engineering; 2-norm; Error-free transformations; Faithful rounding; Overflow; Underflows; Digital arithmetic
Complex root finding algorithm based on Delaunay triangulation,2015,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930836711&doi=10.1145%2f2699457&partnerID=40&md5=625008116a369c498e81c9cb56de46e8,"A simple and flexible algorithm for finding zeros of a complex function is presented. An arbitrary-shaped search region can be considered and a very wide class of functions can be analyzed, including those containing singular points or even branch cuts. The proposed technique is based on sampling the function at nodes of a regular or a self-adaptive mesh and on the analysis of the function sign changes. As a result, a set of candidate points is created, where the signs of the real and imaginary parts of the function change simultaneously. To verify and refine the results, an iterative algorithm is applied. The validity of the presented technique is supported by the results obtained in numerical tests involving three different types of functions. © 2015 ACM.",Algorithms; Complex root finding; Delaunay triangulation; G.1.5 [numerical analysis]: roots of nonlinear equations-iterative methods; Global algorithm; Performance,Algorithms; Nonlinear equations; Numerical methods; Triangulation; Complex functions; Complex roots; Delau-nay triangulations; Global algorithms; Iterative algorithm; Performance; Real and imaginary; Singular points; Iterative methods
Algorithm 949: MATLAB tools for HDG in three dimensions,2015,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930835593&doi=10.1145%2f2658992&partnerID=40&md5=a0d22d50a5958293251ab19dc7e25d79,"In this article, we provide some MATLAB tools for efficient vectorized implementation of the Hybridizable Discontinuous Galerkin for linear variable coefficient reaction-diffusion problems in polyhedral domains. The resulting tools are modular and include enhanced structures to deal with convection-diffusion problems, plus several projection operators and the postprocessing implementation that is necessary to realize the superconvergence property of the method. Loops over the elements are exclusively local and, as such, have been parallelized. © 2015 ACM.",Algorithms; Design; Discontinuous Galerkin methods; Finite element methods; G.1.8 [numerical analysis]: partial differential equations-elliptic equations; Hybridization; Vectorization,Algorithms; Design; Differential equations; Diffusion in liquids; Finite element method; Numerical methods; Convection diffusion problems; Discontinuous galerkin; Discontinuous Galerkin methods; Elliptic equations; Hybridization; Reaction diffusion problems; Variable coefficients; Vectorization; Galerkin methods
Reliable generation of high-performance matrix algebra,2015,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930853350&doi=10.1145%2f2629698&partnerID=40&md5=6e1e76342959ecbbcfc90182e417efd2,"Scientific programmers often turn to vendor-tuned Basic Linear Algebra Subprograms (BLAS) to obtain portable high performance. However, many numerical algorithms require several BLAS calls in sequence, and those successive calls do not achieve optimal performance. The entire sequence needs to be optimized in concert. Instead of vendor-tuned BLAS, a programmer could start with source code in Fortran or C (e.g., based on the Netlib BLAS) and use a state-of-the-art optimizing compiler. However, our experiments show that optimizing compilers often attain only one-quarter of the performance of hand-optimized code. In this article, we present a domain-specific compiler for matrix kernels, the Build to Order BLAS (BTO), that reliably achieves high performance using a scalable search algorithm for choosing the best combination of loop fusion, array contraction, and multithreading for data parallelism. The BTO compiler generates code that is between 16% slower and 39% faster than hand-optimized code. © 2015, Association for Computing Machinery. All rights reserved.",Autotuning; Code generation; D.3.4 [programming languages]: processors-compilers; Domain-specific languages; Genetic algorithms; Languages; Linear algebra; Optimization; Performance,Codes (symbols); Genetic algorithms; Linear algebra; Matrix algebra; Multitasking; Optimization; Problem oriented languages; Program compilers; Query languages; Autotuning; Basic linear algebra subprograms; Code Generation; Domain specific languages; Numerical algorithms; Optimizing compilers; Performance; Performance matrices; C (programming language)
Editorial: ACM TOMS replicated computational results initiative,2015,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930855259&doi=10.1145%2f2743015&partnerID=40&md5=214ee917802ffa22f441ce1e19221ee8,"The scientific community relies on the peer review process for assuring the quality of published material, the goal of which is to build a body of work we can trust. Computational journals such as the ACM Transactions on Mathematical Software (TOMS) use this process for rigorously promoting the clarity and completeness of content, and citation of prior work. At the same time, it is unusual to independently confirm computational results. ACM TOMS has established a Replicated Computational Results (RCR) review process as part of the manuscript peer review process. The purpose is to provide independent confirmation that results contained in a manuscript are replicable. Successful completion of the RCR process awards a manuscript with the Replicated Computational Results Designation. This issue of ACM TOMS contains the first [Van Zee and van de Geijn 2015] of what we anticipate to be a growing number of articles to receive the RCR designation, and the related RCR reviewer report [Willenbring 2015]. We hope that the TOMS RCR process will serve as a model for other publications and increase the confidence in and value of computational results in TOMS articles. © 2015, Association for Computing Machinery. All rights reserved.",Publication; Replicated computational results; Reproducibility; Validation; Validation; Verification; •General and reference → verification; •Software and its engineering → formal software verfication,Computer software; Software engineering; Computational results; Publication; Reproducibilities; Validation; Verfication; Verification
Remark on algorithm 897: VTDIRECT95: Serial and parallel codes for the global optimization algorithm DIRECT,2015,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930847491&doi=10.1145%2f2699459&partnerID=40&md5=52a7a3885cabcdd08a9d640dad68d446,"The Fortran95 code VTDIRECT95, based on the original MPI, has been modified to use MPI-2. An option for VTDIRECT95 is to divide the feasible box into subdomains, and concurrently apply the global direct search algorithm DIRECT within each subdomain. When the number of subdomains is greater than one, a bug causes VTDIRECT95 to occasionally sample outside the given feasible box, which is serious if the objective function is not defined outside the given box. This bug has been fixed, and the sample output files have been updated to reflect the correction. For completeness, the package VTDIRECT95 now contains both the MPI-1 (with the multiple subdomain bug fixed) and the MPI-2 versions of the code. © 2015, Association for Computing Machinery. All rights reserved.",Algorithms; Checkpointing; Data structures; Design; DIRECT; Documentation; G.4 [mathematics of computing]: mathematical software; Global optimization; J.2 [computer applications]: physical science and engineering-mathematics; Parallel schemes,Application programs; Codes (symbols); Data structures; Design; Global optimization; Optimization; System program documentation; Check pointing; DIRECT; Mathematical software; Parallel scheme; Physical science; Algorithms
Numerical integration of discontinuous functions in many dimensions,2015,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930836999&doi=10.1145%2f2629476&partnerID=40&md5=e0e840541d45d9f770910b79f48f4e87,"We consider the problem of numerically integrating functions with hyperplane discontinuities over the entire Euclidean space in many dimensions. We describe a simple process through which the Euclidean space is partitioned into simplices on which the integrand is smooth, generalising the standard practice of dividing the interval used in one-dimensional problems. Our procedure is combined with existing adaptive cubature algorithms to significantly reduce the necessary number of function evaluations and memory requirements of the integrator. The method is embarrassingly parallel and can be trivially scaled across many cores with virtually no overhead. Our method is particularly pertinent to the integration of Green's functions, a problem directly related to the perturbation theory of impurity models. In three spatial dimensions, we observe a speed-up of order 100 which increases with increasing dimensionality. © 2015 Association for Computing Machinery.",Adaptive integration; Mathematical software performance; Multidimensional cubature; Numerical integration; Performance; ∗Mathematics of computing → quadrature,Computation theory; Geometry; Integration; Perturbation techniques; Adaptive integration; Mathematical software; Mathematics of computing; Multidimensional cubature; Numerical integrations; Performance; Integral equations
Algorithm 950: Ncpol2sdpa - Sparse semidefinite programming relaxations for polynomial optimization problems of noncommuting variables,2015,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930850259&doi=10.1145%2f2699464&partnerID=40&md5=3ea594d5a5f2748d35c9d0569130621c,"A hierarchy of semidefinite programming (SDP) relaxations approximates the global optimum of polynomial optimization problems of noncommuting variables. Generating the relaxation, however, is a computationally demanding task, and only problems of commuting variables have efficient generators. We develop an implementation for problems of noncommuting variables that creates the relaxation to be solved by SDPA - a high-performance solver that runs in a distributed environment. We further exploit the inherent sparsity of optimization problems in quantum physics to reduce the complexity of the resulting relaxations. Constrained problems with a relaxation of order two may contain up to a hundred variables. The implementation is available in Python. The tool helps solve such as finding the ground state energy or testing quantum correlations. © 2015, Association for Computing Machinery. All rights reserved.",Algorithms; Design; G.4 [mathematics of computing]: mathematical software; Ground state energy; Noncommuting variables; Performance; Quantum correlations; Semidefinite programming,Algorithms; Design; Ground state; Optimization; Quantum theory; Ground-state energies; Mathematical software; Noncommuting variables; Performance; Quantum correlations; Semi-definite programming; Computer programming
"Replicated computational results (RCR) report for ""BLIS: A framework for rapidly instantiating BLAS functionality""",2015,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930846583&doi=10.1145%2f2738033&partnerID=40&md5=3dbf0aa5edc0696cd9145f28a2890973,"""BLIS: A Framework for Rapidly Instantiating BLAS Functionality"" includes single-platform BLIS performance results for both level-2 and level-3 operations that is competitive with OpenBLAS, ATLAS, and Intel MKL. A detailed description of the configuration used to generate the performance results was provided to the reviewer by the authors. All the software components used in the comparison were reinstalled and new performance results were generated and compared to the original results. After completing this process, the published results are deemed replicable by the reviewer. © 2015, Association for Computing Machinery. All rights reserved.",Algorithms; BLAS; G.4 [mathematical software]: efficiency; High-performance; Libraries; Linear algebra; Matirx; Performance; Replicated computational results,Algorithms; Libraries; Linear algebra; BLAS; Computational results; High-performance; Mathematical software; Matirx; Performance; Computational efficiency
odeToJava: A PSE for the numerical solution of IVPs,2015,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930838315&doi=10.1145%2f2641563&partnerID=40&md5=b6e728c71bfdb76ceb01445f42fe9700,"Problem-solving environments (PSEs) offer a powerful yet flexible and convenient means for general experimentation with computational methods, algorithm prototyping, and visualization and manipulation of data. Consequently, PSEs have become the modus operandi of many computational scientists and engineers. However, despite these positive aspects, PSEs typically do not offer the level of granularity required by the specialist or algorithm designer to conveniently modify the details. In other words, the level at which PSEs are black boxes is often still too high for someone interested in modifying an algorithm as opposed to trying an alternative. In this article, we describe odeToJava, a Java-based PSE for initial-value problems in ordinary differential equations. odeToJava implements explicit and linearly implicit implicit-explicit Runge-Kutta methods with error and stepsize control and intra-step interpolation (dense output), giving the user control and flexibility over the implementational aspects of these methods. We illustrate the usage and functionality of odeToJava by means of computational case studies of initial-value problems (IVPs). © 2015 ACM.",Adaptive integration; Additive Runge-Kutta; Algorithms; D.2.11 [software engineering]: software architectures-domain-specific architectures; Design; Experimentation; G.1.7 [numerical analysis]: ordinary differential equations-initial value problems; G.4 [mathematical software]: algorithm design and analysis; Geometric integration; Object-oriented; One-step (single step) methods; Patterns; Problem solving environment; Runge-Kutta; Software architecture; Stepsize-control; Störmer-Verlet; User interfaces,Algorithms; Data visualization; Design; Initial value problems; Numerical methods; Object oriented programming; Ordinary differential equations; Software architecture; Software engineering; User interfaces; Adaptive integration; Additive Runge-Kutta; Algorithm design and analysis; D.2.11 [software engineering]: software architectures - domain specific architectures; Experimentation; Geometric integration; Object oriented; Patterns; Problem solving environments; Single-step; Stepsize control; Runge Kutta methods
Replicated computational results certified: BLIS: A framework for rapidly instantiating BLAS functionality,2015,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930846218&doi=10.1145%2f2764454&partnerID=40&md5=4e17e399fe04e768c4d701b0c95966ef,"The BLAS-like Library Instantiation Software (BLIS) framework is a new infrastructure for rapidly instantiating Basic Linear Algebra Subprograms (BLAS) functionality. Its fundamental innovation is that virtually all computation within level-2 (matrix-vector) and level-3 (matrix-matrix) BLAS operations can be expressed and optimized in terms of very simple kernels. While others have had similar insights, BLIS reduces the necessary kernels to what we believe is the simplest set that still supports the high performance that the computational science community demands. Higher-level framework code is generalized and implemented in ISO C99 so that it can be reused and/or reparameterized for different operations (and different architectures) with little to no modification. Inserting high-performance kernels into the framework facilitates the immediate optimization of any BLAS-like operations which are cast in terms of these kernels, and thus the framework acts as a productivity multiplier. Users of BLAS-dependent applications are given a choice of using the traditional Fortran-77 BLAS interface, a generalized C interface, or any other higher level interface that builds upon this latter API. Preliminary performance of level-2 and level-3 operations is observed to be competitive with two mature open source libraries (OpenBLAS and ATLAS) as well as an established commercial product (Intel MKL). © 2015, Association for Computing Machinery. All rights reserved.",Algorithms; BLAS; G.4 [mathematical software]: efficiency; High-performance; Libraries; Linear algebra; Matrix; Performance,Algebra; Algorithms; Application programming interfaces (API); Libraries; Linear algebra; Open source software; Basic linear algebra subprograms; BLAS; Computational results; Computational science; High-performance; Mathematical software; Open-source libraries; Performance; Matrix algebra
Solving basis pursuit: Heuristic optimality check and solver comparison,2015,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84922761775&doi=10.1145%2f2689662&partnerID=40&md5=3b613cca01686a365b095f97a832cf55,"The problem of finding a minimum ℓ1-norm solution to an underdetermined linear system is an important problem in compressed sensing, where it is also known as basis pursuit. We propose a heuristic optimality check as a general tool for ℓ1-minimization, which often allows for early termination by ""guessing"" a primaldual optimal pair based on an approximate support.Moreover,we provide an extensive numerical comparison of various state-of-the-art ℓ1-solvers that have been proposed during the last decade, on a large test set with a variety of explicitly given matrices and several right-hand sides per matrix reflecting different levels of solution difficulty. The results, as well as improvements by the proposed heuristic optimality check, are analyzed in detail to provide an answer to the question which algorithm is the best. © 2015 ACM.",Algorithms; Experimentation; Performance; Reliability; Verification,Algorithms; Compressed sensing; Linear systems; Reliability; Verification; Basis Pursuits; Early termination; Experimentation; Numerical comparison; Performance; Right-hand sides; State of the art; Underdetermined linear systems; Optimization
Solving boundary integral problems with BEM++,2015,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84922766255&doi=10.1145%2f2590830&partnerID=40&md5=2144dcb0f011871ad2fe88c010881012,"Many important partial differential equation problems in homogeneous media, such as those of acoustic or electromagnetic wave propagation, can be represented in the form of integral equations on the boundary of the domain of interest. In order to solve such problems, the boundary element method (BEM) can be applied. The advantage compared to domain-discretisation-based methods such as finite element methods is that only a discretisation of the boundary is necessary, which significantly reduces the number of unknowns. Yet, BEM formulations are much more difficult to implement than finite element methods. In this article, we present BEM++, a novel open-source library for the solution of boundary integral equations for Laplace, Helmholtz and Maxwell problems in three space dimensions. BEM++ is a C++ library with Python bindings for all important features, making it possible to integrate the library into other C++ projects or to use it directly via Python scripts. The internal structure and design decisions for BEM++ are discussed. Several examples are presented to demonstrate the performance of the library for larger problems. © 2015 ACM.",Algorithms; Documentation; Performance,Acoustic wave propagation; Algorithms; Boundary element method; Boundary integral equations; C++ (programming language); Differential equations; Electromagnetic wave propagation; Finite element method; High level languages; Integral equations; Maxwell equations; Sailing vessels; System program documentation; Boundary integrals; Design decisions; Homogeneous media; Important features; Internal structure; Open-source libraries; Performance; Three space dimensions; Problem solving
"TetGen, a delaunay-based quality tetrahedral mesh generator",2015,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84922831263&doi=10.1145%2f2629697&partnerID=40&md5=37f450d40bb208ccba84b047b573518d,"TetGen is a C++ program for generating good quality tetrahedral meshes aimed to support numerical methods and scientific computing. The problem of quality tetrahedralmesh generation is challenged by many theoretical and practical issues. TetGen uses Delaunay-based algorithms which have theoretical guarantee of correctness. It can robustly handle arbitrary complex 3D geometries and is fast in practice. The source code of TetGen is freely available. This article presents the essential algorithms and techniques used to develop TetGen. The intended audience are researchers or developers in mesh generation or other related areas. It describes the key software components of TetGen, including an efficient tetrahedral mesh data structure, a set of enhanced local mesh operations (combination of flips and edge removal), and filtered exact geometric predicates. The essential algorithms include incremental Delaunay algorithms for inserting vertices, constrained Delaunay algorithms for inserting constraints (edges and triangles), a new edge recovery algorithm for recovering constraints, and a new constrained Delaunay refinement algorithm for adaptive quality tetrahedral mesh generation. Experimental examples as well as comparisons with other softwares are presented. © 2015 ACM.",Algorithms; Design; Performance,C++ (programming language); Computer software; Design; Mesh generation; Numerical methods; Complex 3D geometry; Constrained delaunay; Delaunay algorithm; Performance; Recovery algorithms; Software component; Tetrahedral mesh generation; Theoretical guarantees; Algorithms
"On the error of computing ab+cd using Cornea, Harrison and Tang's method",2015,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84922792907&doi=10.1145%2f2629615&partnerID=40&md5=6d1844efa178b971ecdd7ccfb453ec71,"In their book, Scientific Computing on the Itanium, Cornea et al. [2002] introduce an accurate algorithm for evaluating expressions of the form ab+cd in binary floating-point arithmetic, assuming an FMA instruction is available. They show that if p is the precision of the floating-point format and if u = 2-p, the relative error of the result is of order u. We improve their proof to show that the relative error is bounded by 2u + 7u2 + 6u3. Furthermore, by building an example for which the relative error is asymptotically (as p→∞or, equivalently, as u→ 0) equivalent to 2u, we show that our error bound is asymptotically optimal. © 2015 ACM.",Algorithms,Algorithms; Computer software; Software engineering; Asymptotically optimal; Error bound; Floating points; Itanium; Relative errors; Digital arithmetic
FSAIPACK: A software package for high-performance factored sparse approximate inverse preconditioning,2015,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84922767272&doi=10.1145%2f2629475&partnerID=40&md5=1070e8c64f18c7ba5db136401a329610,"The Factorized Sparse Approximate Inverse (FSAI) is an efficient technique for preconditioning parallel solvers of symmetric positive definite sparse linear systems. The key factor controlling FSAI efficiency is the identification of an appropriate nonzero pattern. Currently, several strategies have been proposed for building such a nonzero pattern, using both static and dynamic techniques. This article describes a fresh software package, called FSAIPACK, which we developed for shared memory parallel machines. It collects all available algorithms for computing FSAI preconditioners. FSAIPACK allows for combining different techniques according to any specified strategy, hence enabling the user to thoroughly exploit the potential of each preconditioner, in solving any peculiar problem. FSAIPACK is freely available as a compiled library at http://www.dmsa.unipd.it/~janna/software.html, together with an open-source command language interpreter. By writing a command ASCII file, one can easily perform and test any given strategy for building an FSAI preconditioner. Numerical experiments are discussed in order to highlight the FSAIPACK features and evaluate its computational performance. © 2015 ACM.",Algorithms; Design; Performance,Algorithms; Design; Linear systems; Software packages; Computational performance; Language interpreters; Numerical experiments; Performance; Shared-memory parallels; Sparse approximate inverse; Sparse linear systems; Symmetric positive definite; Open source software
Algorithm 948: DAESA-A Matlab tool for structural analysis of differential-algebraic equations: Software,2015,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84922821370&doi=10.1145%2f2700586&partnerID=40&md5=4d824de70daa219536350b1084d68034,"DAESA, Differential-Algebraic Equations Structural Analyzer, is a MATLAB tool for structural analysis of differential-algebraic equations (DAEs). It allows convenient translation of a DAE system into MATLAB and provides a small set of easy-to-use functions. DAESA can analyze systems that are fully nonlinear, high-index, and of any order. It determines structural index, number of degrees of freedom, constraints, variables to be initialized, and suggests a solution scheme. The structure of a DAE can be readily visualized by this tool. It can also construct a block-triangular form of the DAE, which can be exploited to solve it efficiently in a block-wise manner. © 2015 ACM.",Algorithms; Design; Performance,Algebra; Algorithms; Degrees of freedom (mechanics); Design; Indexing (materials working); MATLAB; Structural analysis; Block triangular forms; DAE systems; Differential algebraic equations; Fully nonlinear; MATLAB tools; Number of degrees of freedom; Performance; Structural indices; Differential equations
DAESA-A Matlab tool for structural analysis of differential-algebraic equations: Theory,2015,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84922818017&doi=10.1145%2f2689664&partnerID=40&md5=c202b9c54070e1228b329b52e3459367,"DAESA, Differential-Algebraic Equations Structural Analyzer, is a MATLAB tool for structural analysis of differential-algebraic equations (DAEs). It allows convenient translation of a DAE system into MATLAB and provides a small set of easy-to-use functions. DAESA can analyze systems that are fully nonlinear, high-index, and of any order. It determines structural index, number of degrees of freedom, constraints, variables to be initialized, and suggests a solution scheme. The structure of a DAE can be readily visualized by this tool. It also can construct a block-triangular form of the DAE, which can be exploited to solve it efficiently in a block-wise manner. © 2015 ACM.",Algorithms; Theory,Algebra; Algorithms; Degrees of freedom (mechanics); Indexing (materials working); Structural analysis; Block triangular forms; DAE systems; Differential algebraic equations; Fully nonlinear; MATLAB tools; Number of degrees of freedom; Structural indices; Theory; Differential equations
Algorithm 947: Paraperm-parallel generation of random permutations with MPI,2014,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84908510837&doi=10.1145%2f2669372&partnerID=40&md5=d8f725f1fcbbd1557a0bdaee5a4c0ff8,"An algorithm for parallel generation of a random permutation of a large set of distinct integers is presented. This algorithm is designed for massively parallel systems with distributed memory architectures and the MPI-based runtime environments. Scalability of the algorithm is analyzed according to the memory and communication requirements. An implementation of the algorithm in a form of a software library based on the C++ programming language and the MPI application programming interface is further provided. Finally, performed experiments are described and their results discussed. The biggest of these experiments resulted in a generation of a random permutation of 241 integers in slightly more than four minutes using 131072 CPU cores. © 2014 ACM.",C++; Distributed memory; Implementation; MPI; Parallel computing; Random permutation,Application programming interfaces (API); Application programs; C++ (programming language); Cesium; Parallel processing systems; Distributed Memory; Distributed memory architecture; Implementation; Massively parallel systems; MPI applications; Random permutations; Runtime environments; Software libraries; Memory architecture
GPOPS - II: A MATLAB software for solving multiple-phase optimal control problems using hp-adaptive gaussian quadrature collocation methods and sparse nonlinear programming,2014,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84908509298&doi=10.1145%2f2558904&partnerID=40&md5=9e6488c759472e83eed056bb531ba403,"A general-purpose MATLAB software program called GPOPS - II is described for solving multiple-phase optimal control problems using variable-order Gaussian quadrature collocation methods. The software employs a Legendre-Gauss-Radau quadrature orthogonal collocation method where the continuous-time optimal control problem is transcribed to a large sparse nonlinear programming problem (NLP). An adaptive mesh refinement method is implemented that determines the number of mesh intervals and the degree of the approximating polynomial within each mesh interval to achieve a specified accuracy. The software can be interfaced with either quasi-Newton (first derivative) or Newton (second derivative) NLP solvers, and all derivatives required by the NLP solver are approximated using sparse finite-differencing of the optimal control problem functions. The key components of the software are described in detail and the utility of the software is demonstrated on five optimal control problems of varying complexity. The software described in this article provides researchers a useful platform upon which to solve a wide variety of complex constrained optimal control problems. © 2014 ACM.",Applied mathematics; Direct collocation; Gaussian quadrature; Hp-adaptive methods; Matlab; Numerical methods; Optimal control; Scientific computation,Constrained optimization; Continuous time systems; Gaussian distribution; MATLAB; Mesh generation; Nonlinear programming; Numerical analysis; Numerical methods; Optimal control systems; Adaptive methods; Applied mathematics; Direct collocation; Gaussian quadratures; Optimal controls; Scientific computation; Computer control
StaRMAP-A second order staggered grid method for spherical harmonics moment equations of radiative transfer,2014,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84908508249&doi=10.1145%2f2590808&partnerID=40&md5=a84de30d7b242441d086d7ed7cacac40,"We present a simple method to solve spherical harmonics moment systems, such as the the time-dependent PN and SPN equations, of radiative transfer. Themethod, which works for arbitrary moment order N, makes use of the specific coupling between the moments in the PN equations. This coupling naturally induces staggered grids in space and time, which in turn give rise to a canonical, second-order accurate finite difference scheme. While the scheme does not possess TVD or realizability limiters, its simplicity allows for a very efficient implementation in MATLAB. We present several test cases, some of which demonstrate that the code solves problems with ten million degrees of freedom in space, angle, and time within a few seconds. The code for the numerical scheme, called StaRMAP (Staggered grid Radiation Moment Approximation), along with files for all presented test cases, can be downloaded so that all results can be reproduced by the reader. © 2014 ACM.",Hyperbolic balance law; Matlab; Method of moments; Numerical method; Radiative transfer; Staggered grid,Degrees of freedom (mechanics); Finite difference method; Harmonic analysis; MATLAB; Method of moments; Radiative transfer; Space time codes; Accurate finite difference scheme; Balance law; Efficient implementation; Moment approximation; Moment equations; Numerical scheme; Spherical harmonics; Staggered grid; Numerical methods
A parallel sparse direct solver via hierarchical DAG scheduling,2014,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84908556888&doi=10.1145%2f2629641&partnerID=40&md5=c1e14cbfac1987c19297c2f04f5cfa21,"We present a parallel sparse direct solver for multicore architectures based on Directed Acyclic Graph (DAG) scheduling. Recently, DAG scheduling has become popular in advanced Dense Linear Algebra libraries due to its efficient asynchronous parallel execution of tasks. However, its application to sparse matrix problems is more challenging as it has to deal with an enormous number of highly irregular tasks. This typically results in substantial scheduling overhead both in time and space, which causes overall parallel performance to be suboptimal. We describe a parallel solver based on two-level task parallelism: Tasks are first generated from a parallel tree traversal on the assembly tree; next, those tasks are further refined by using algorithmsby- blocks to gain fine-grained parallelism. The resulting fine-grained tasks are asynchronously executed after their dependencies are analyzed. Our approach is distinct from others in that we adopt two-level task scheduling to mirror the two-level parallelism. As a result, we reduce scheduling overhead, and increase efficiency and flexibility. The proposed parallel sparse direct solver is evaluated for the particular problems arising from the hp-Finite Element Method where conventional sparse direct solvers do not scale well. © 2014 ACM.",Direct method; Directed acyclic graph; Gaussian elimination; Lu; Multicore; Multifrontal; Openmp; Sparse matrix; Supernodes; Task parallelism; Unassembled hypermatrix,Application programming interfaces (API); Directed graphs; Forestry; Lutetium; Matrix algebra; Software architecture; Direct method; Directed acyclic graph (DAG); Gaussian elimination; Multi core; Multifrontal; Openmp; Sparse matrices; Supernodes; Task parallelism; Unassembled hypermatrix; Scheduling
A comparison of hp-adaptive strategies for elliptic partial differential equations,2014,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84908511434&doi=10.1145%2f2629459&partnerID=40&md5=05abd8b5f17f3d04a26d7f75c2bb4dd6,"The hp version of the finite element method (hp-FEM) combined with adaptive mesh refinement is a particularly efficient method for solving PDEs because it can achieve an exponential convergence rate in the number of degrees of freedom. hp-FEM allows for refinement in both the element size, h, and the polynomial degree, p. Like adaptive refinement for the h version of the finite element method, a posteriori error estimates can be used to determine where the mesh needs to be refined, but a single error estimate cannot simultaneously determine whether it is better to do the refinement by h or p. Several strategies for making this determination have been proposed over the years. These strategies are summarized, and the results of a numerical experiment to study the performance of these strategies is presented. It was found that the reference-solution-based methods are very effective, but also considerably more expensive, in terms of computation time, than other approaches. The method based on a priori knowledge is very effective when there are known point singularities. The method based on the decay rate of the expansion coefficients appears to be the best choice as a general strategy across all categories of problems, whereas many of the other strategies perform well in particular situations and are reasonable in general. © 2014 ACM.",Adaptive mesh refinement; Hp-adaptive strategy; Hp-FEM,Decay (organic); Degrees of freedom (mechanics); Mesh generation; Partial differential equations; A-posteriori error estimates; Adaptive mesh refinement; Adaptive strategy; Elliptic partial differential equation; Expansion coefficients; Exponential convergence rate; Hp version of the finite element methods; Number of degrees of freedom; Finite element method
Fast reverse-mode automatic differentiation using expression templates in C++,2014,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84904090235&doi=10.1145%2f2560359&partnerID=40&md5=901f35ee7aa98e9638207d6ff9e8746c,"Gradient-based optimization problems are encountered in many fields, but the associated task of differentiating large computer algorithms can be formidable. The operator-overloading approach to performing reverse-mode automatic differentiation is the most convenient for the user but current implementations are typically 10-35 times slower than the original algorithm. In this paper a fast new operator-overloading method is presented that uses the expression template programming technique in ++ to provide a compiletime representation of each mathematical expression as a computational graph that can be efficiently traversed in either direction. Benchmarking with four different numerical algorithms shows this approach to be 2.6-9 times faster than current operator-overloading libraries, and 1.3-7.7 times more efficient in memory usage. It is typically less than 4 times the computational cost of the original algorithm, although poorer performance is found for all libraries in the case of simple loops containing no mathematical functions. An implementation is freely available in the Adept C++ software library. © 2014 ACM.",Adjoint code; Jacobian matrix; Quasi-Newton; Template metaprogramming,C++ (programming language); Computer software; Functions; Jacobian matrices; Libraries; Mathematical operators; Object oriented programming; Optimization; Adjoint codes; Automatic differentiations; Gradient-based optimization; Mathematical expressions; Mathematical functions; Numerical algorithms; Quasi-Newton; Template metaprogramming; Algorithms
Algorithm 943: MSS: MATLAB software for L-BFGS trust-region subproblems for large-scale optimization,2014,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84904135958&doi=10.1145%2f2616588&partnerID=40&md5=2990de4279cbaa2470b6dbabfc3ccf5c,AMATLAB implementation of theMoré-Sorensen sequential (MSS) method is presented. The MSS method computes the minimizer of a quadratic function defined by a limited-memory BFGS matrix subject to a two-norm trust-region constraint. This solver is an adaptation of the Moré-Sorensen direct method into an L-BFGS setting for large-scale optimization. The MSS method makes use of a recently proposed stable fast direct method for solving large shifted BFGS systems of equations [Erway and Marcia 2012; Erway et al. 2012] and is able to compute solutions to any user-defined accuracy. This MATLAB implementation is a matrix-free iterative method for large-scale optimization. Numerical experiments on the CUTEr [Bongartz et al. 1995; Gould et al. 2003]) suggest that using the MSS method as a trust-region subproblem solver can require significantly fewer function and gradient evaluations needed by a trust-region method as compared with the Steihaug-Toint method. © 2014 ACM.,L-BFGS; Large-scale unconstrained optimization; Limitedmemory quasi-Newton methods; Trust-region methods,Algorithms; MATLAB; Newton-Raphson method; Optimization; L-BFGS; Large-scale optimization; Numerical experiments; Quasi-Newton methods; Systems of equations; Trust-region methods; Trust-region subproblem; Unconstrained optimization; Satellites
HSL-MI28: An efficient and robust limited-memory incomplete Cholesky factorization code,2014,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84904109881&doi=10.1145%2f2617555&partnerID=40&md5=938f3cde1f42f0c59178fe94efd09d23,"This article focuses on the design and development of a new robust and efficient general-purpose incomplete Cholesky factorization package HSL MI28, which is available within the HSL mathematical software library. It implements a limited memory approach that exploits ideas from the positive semidefinite Tismenetsky-Kaporin modification scheme and, through the incorporation of intermediate memory, is a generalization of the widely used ICFS algorithm of Lin and Moré. Both the density of the incomplete factor and the amount of memory used in its computation are under the user's control. The performance of HSL-MI28 is demonstrated using extensive numerical experiments involving a large set of test problems arising from a wide range of real-world applications. The numerical experiments are used to isolate the effects of scaling, ordering, and dropping strategies so as to assess their usefulness in the development of robust algebraic incomplete factorization preconditioners and to select default settings for HSL-MI28. They also illustrate the significant advantage of employing a modest amount of intermediate memory. Furthermore, the results demonstrate that, with limited memory, high-quality yet sparse general-purpose preconditioners are obtained. Comparisons are made with ICFS, with a level-based incomplete factorization code and, finally, with a state-of-the-art direct solver. © 2014 ACM.",Incomplete Cholesky; Iterative methods; Preconditioning; Symmetric and positive definite linear systems,Experiments; Iterative methods; Linear systems; Cholesky; Design and Development; Incomplete cholesky factorizations; Incomplete factorization; Incomplete factorization preconditioners; Numerical experiments; Positive definite; Preconditioning; Factorization
Algorithm 944: Talbot suite: Parallel implementations of talbot's method for the numerical inversion of laplace transforms,2014,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84904104592&doi=10.1145%2f2616909&partnerID=40&md5=95944afca4ce0a7934e89aed37b99398,"We present Talbot Suite, a C parallel software collection for the numerical inversion of Laplace Transforms, based on Talbot's method. It is designed to fit both single and multiple Laplace inversion problems, which arise in several application and research fields. In our software, we achieve high accuracy and efficiency, making full use of modern architectures and introducing two different levels of parallelism: coarse and fine grained parallelism. They offer a reasonable tradeoff between accuracy, the main aspect for a few inversions, and efficiency, the main aspect for multiple inversions. To take into account modern high-performance computing architectures, Talbot Suite provides different software versions: an OpenMP-based version for shared memory machines and a MPI-based version for distributed memory machines. Moreover, oriented to hybrid architectures, a combined MPI/OpenMPbased implementation is provided too. We describe our parallel algorithms and the software organization.We also report some performance results. Our software includes sample programs to call the Talbot Suite functions from C and from MATLAB. © 2014 ACM.",Hybrid architectures; Inverse Laplace transform; Parallel algorithms; Talbot's method,Application programming interfaces (API); C (programming language); Laplace transforms; Parallel algorithms; Parallel architectures; Distributed memory machines; Fine-grained parallelism; High-performance computing; Hybrid architectures; Inverse Laplace transform; Numerical inversion of Laplace transform; Parallel implementations; Talbot's method; MATLAB
Algorithm 945: Modred-a parallelized model reduction library,2014,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84904113852&doi=10.1145%2f2616912&partnerID=40&md5=a8117f9185794b74614b9897cac42e27,"We describe a new parallelized Python library for model reduction, modal analysis, and system identification of large systems and datasets. Our library, called modred, handles a wide range of problems and any data format. The modred library contains implementations of the Proper Orthogonal Decomposition (POD), balanced POD (BPOD) Petrov-Galerkin projection, and a more efficient variant of the Dynamic Mode Decomposition (DMD). The library contains two implementations of these algorithms, each with its own advantages. One is for smaller and simpler datasets, requires minimal knowledge to use, and follows a common matrix-based formulation. The second, for larger and more complicated datasets, preserves the abstraction of vectors as elements of a vector space and, as a result, allows the library to work with arbitrary data formats and eases distributed memory parallelization. We also include implementations of the Eigensystem Realization Algorithm (ERA), and Observer/Kalman Filter Identification (OKID). These methods are typically not computationally demanding and are not parallelized. The library is designed to be easy to use, with an object-oriented design, and includes comprehensive automated tests. In almost all cases, parallelization is done internally so that scripts that use the parallelized classes can be run in serial or in parallel without any modifications. © 2014 ACM.",Balanced Proper Orthogonal Decomposition; Dynamic Mode Decomposition; Eigensystem Realization Algorithm; Koopman modes; Model reduction; Observer/Kalman Filter Identification; Proper Orthogonal Decomposition; Python; System identification,High level languages; Identification (control systems); Modal analysis; Principal component analysis; Dynamic mode decompositions; Eigensystem realization algorithms; Koopman modes; Model reduction; Proper orthogonal decompositions; Python; Algorithms
Computing petaflops over terabytes of data: The case of genome-wide association studies,2014,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84904129286&doi=10.1145%2f2560421&partnerID=40&md5=0b9d9f8f0769fa3f00632f4288ffb69f,"In many scientific and engineering applications, one has to solve not one but multiple instances of the same problem. Often times, these problems are linked in a way that allows intermediate results to be reused. A characteristic example for this class of applications is given by the Genome-Wide Association Studies (GWAS), a widely spread tool in computational biology. GWAS entails the solution of up to trillions (1012) of correlated generalized least-squares problems, posing a daunting challenge: the performance of petaflops (1015 floating-point operations) over terabytes (10 12 bytes) of data. In this article, we design an algorithm for performing GWAS on multicore architectures. This is accomplished in three steps. First, we show how to exploit the relation among successive problems, thus reducing the overall computational complexity. Then, through an analysis of the required data transfers, we identify how to eliminate any overhead due to input/output operations. Finally, we study how to decompose computation into tasks to be distributed among the available cores, to attain high performance and scalability. With our algorithm, a GWAS that currently requires the use of a supercomputer may now be performed in matter of hours on a single multicore node. The discussion centers around the methodology to develop the algorithm rather than the specific application. We believe this article contributes valuable guidelines of general applicability for computational scientists on how to develop and optimize numerical algorithms. © 2014 ACM.",Genome-wide association studies; Multiple instances of problems; Numerical linear algebra; Out-of-core; Sharedmemory,Algorithms; Bioinformatics; Data transfer; Linear algebra; Software architecture; Supercomputers; Genome-wide association studies; Multiple instances; Numerical Linear Algebra; Out-of-core; Shared memory; Genes
Algorithm 946: ReLIADiff-A C++ software package for real Laplace transform inversion based on algorithmic differentiation,2014,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84904094528&doi=10.1145%2f2616971&partnerID=40&md5=373616147da2692bd0827a5daa3ff417,"Algorithm 662 of the ACM TOMS library is a software package, based on the Weeks method, which is used for calculating function values of the inverse Laplace transform. The software requires transform values at arbitrary points in the complex plane. We developed a software package, called ReLIADiff, which is a modification of Algorithm 662 using transform values at arbitrary points on real axis. ReLIADiff, implemented in C++, relies on TADIFF software package designed for Algorithmic Differentiation. In this article, we present ReLIADiff focusing on its design principles, performance, and use. © 2014 ACM.",Algorithmic differentiation; Inverse and ill-posed problems; Laplace transform inversion; Weeksmethod,C++ (programming language); Inverse problems; Laplace transforms; Software packages; Algorithmic differentiations; Arbitrary points; Design Principles; Function values; Ill posed problem; Inverse Laplace transform; Laplace transform inversions; Weeksmethod; Algorithms
High-performance evaluation of finite element variational forms via commuting diagrams and duality,2014,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84904101504&doi=10.1145%2f2559983&partnerID=40&md5=ab95a054125b90ef10887122c78ad399,"We revisit the question of optimizing the construction and application of finite element matrices. By using commuting properties of the reference mappings and duality, we reorganize stiffness matrix construction and matrix-free application so that the bulk of the work can be done by optimized matrix multiplication libraries. We provide examples, including numerical experiments, with the Laplace and curl-curl operators as well as develop a general framework. Our techniques are applicable in general geometry and are not restricted to constant coefficient operators. © 2014 ACM.",Commuting diagram; Finite element methods; High-performance computing,Computer software; Finite element method; Software engineering; Application of finite elements; Commuting diagram; Constant coefficients; High-performance computing; MAtrix multiplication; Matrix-free; Numerical experiments; Variational form; Stiffness matrix
Algorithm 942: Semi-stencil,2014,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84899070227&doi=10.1145%2f2591006&partnerID=40&md5=c5dc2d8732985e257fad02bd9cb2f543,"Finite Difference (FD) is a widely used method to solve Partial Differential Equations (PDE). PDEs are the core of many simulations in different scientific fields, such as geophysics, astrophysics, etc. The typical FD solver performs stencil computations for the entire computational domain, thus solving the differential operators. In general terms, the stencil computation consists of a weighted accumulation of the contribution of neighbor points along the cartesian axis. Therefore, optimizing stencil computations is crucial in reducing the application execution time. Stencil computation performance is bounded by two main factors: the memory access pattern and the inefficient reuse of the accessed data. We propose a novel algorithm, named Semi-stencil, that tackles these two problems. The main idea behind this algorithm is to change the way in which the stencil computation progresses within the computational domain. Instead of accessing all required neighbors and adding all their contributions at once, the Semi-stencil algorithm divides the computation into several updates. Then, each update gathers half of the axis neighbors, partially computing at the same time the stencil in a set of closely located points. As Semi-stencil progresses through the domain, the stencil computations are completed on precomputed points. This computation strategy improves the memory access pattern and efficiently reuses the accessed data. Our initial target architecture was the Cell/B.E., where the Semi-stencil in a SPE was 44% faster than the naive stencil implementation. Since then, we have continued our research on emerging multicore architectures in order to assess and extend this work on homogeneous architectures. The experiments presented combine the Semi-stencil strategy with space- and time-blocking algorithms used in hierarchical memory architectures. Two x86 (Intel Nehalem and AMD Opteron) and two POWER (IBM POWER6 and IBM BG/P) platforms are used as testbeds, where the best improvements for a 25-point stencil range from 1.27 to 1.76× faster. The results show that this novel strategy is a feasible optimization method which may be integrated into auto-tuning frameworks. Also, since all current architectures are multicore based, we have introduced a brief section where scalability results on IBM POWER7-, Intel Xeon-, and MIC-based systems are presented. In a nutshell, the algorithm scales as well as or better than other stencil techniques. For instance, the scalability of Semi-stencil on MIC for a certain testcase reached 93.8 × over 244 threads. © 2014 ACM.",Blocking; Cache oblivious; Code optimization; HPC; Numerical algorithms; Performance model; Semi-stencil; Stencil computation; Time skewing,Astrophysics; Finite difference method; Mathematical operators; Optimization; Partial differential equations; Scalability; Software architecture; Blocking; Cache-oblivious; Code optimization; HPC; Numerical algorithms; Performance Model; Semi-stencil; Stencil computations; Time skewing; Algorithms
Computing the Sparsity pattern of Hessians using automatic differentiation,2014,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84897002001&doi=10.1145%2f2490254&partnerID=40&md5=d8e79970fe0fa175f09982cc9cdc3cc1,"We compare two methods that calculate the sparsity pattern of Hessian matrices using the computational framework of automatic differentiation. The first method is a forward-mode algorithm by Andrea Walther in 2008 which has been implemented as the driver called hess pat in the automatic differentiation package ADOL-C. The second is edge push sp, a new reverse mode algorithm descended from the edge pushing algorithm for calculating Hessians by Gower andMello in 2012.We present complexity analysis and perform numerical tests for both algorithms. The results show that the new reverse algorithm is very promising.. © 2014 ACM.",Automatic differentiation; Hessian matrix; Second order derivatives; Sparsity patterns,Computer software; Software engineering; Automatic differentiations; Complexity analysis; Computational framework; Hessian matrices; Numerical tests; Reverse mode; Second order derivatives; Sparsity patterns; Algorithms
Algorithm 938: Compressing circular buffers,2014,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84897016800&doi=10.1145%2f2559995&partnerID=40&md5=d50c13d8a0d1e6ccb7de5bded98e0890,"Data sequences generated by on-line sensors can become arbitrarily large and must, therefore, be pared down to fit into available memory. For situations where only the most recent data is of interest, this problem can be solved with optimal efficiency by a simple circular buffer: it fills each memory location with useful data, and requires just one write to memory per update. The algorithm presented here provides essentially the same efficiency, but while maintaining a continuously updated, fixed-size, compressed representation of the entire data sequence. Each value in these compressed sequences represents a statistic (an average, maximum, random sample, etc.) computed over a contiguous chunk of the original sequence. Compressing circular buffers gain their efficiency by using an alternative indexing sequence, based on well-known principles of elementary number theory, to ensure that each newly written value gets stored in the unoccupied location created when the two oldest sequential over-sampled values are compressed into one. The associated Java implementation supports a variety of aggregating statistics and is used to compare the algorithm's performance with a more obvious approach (doubling).. © 2014 ACM.",Average; Complete residue system; Compressing circular buffer; Data aggregation; Data sequence; Doubling; Dynamically sized arrays; Lossy compression; Maximum; Minimum; Modular arithmetic; Number theory; Random sample; Relatively prime; Sum,Efficiency; Number theory; Average; Circular Buffer; Complete residue system; Data aggregation; Data sequences; Doubling; Dynamically sized arrays; Lossy compressions; Maximum; Minimum; Modular arithmetic; Random sample; Relatively prime; Sum; Algorithms
Restructuring the tridiagonal and bidiagonal QR algorithms for performance,2014,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84899054279&doi=10.1145%2f2535371&partnerID=40&md5=39bb14d263790bd044d3d35142a60153,"We show how both the tridiagonal and bidiagonal QR algorithms can be restructured so that they become rich in operations that can achieve near-peak performance on a modern processor. The key is a novel, cache-friendly algorithm for applying multiple sets of Givens rotations to the eigenvector/singular vector matrix. This algorithm is then implemented with optimizations that: (1) leverage vector instruction units to increase floating-point throughput, and (2) fuse multiple rotations to decrease the total number of memory operations. We demonstrate the merits of these new QR algorithms for computing the Hermitian eigenvalue decomposition (EVD) and singular value decomposition (SVD) of dense matrices when all eigenvectors/singular vectors are computed. The approach yields vastly improved performance relative to traditional QR algorithms for these problems and is competitive with two commonly used alternatives-Cuppen's Divide-and-Conquer algorithm and the method of Multiple Relatively Robust Representations-while inheriting the more modest O(n) workspace requirements of the original QR algorithms. Since the computations performed by the restructured algorithms remain essentially identical to those performed by the original methods, robust numerical properties are preserved. © 2014 ACM.",Bidiagonal; Eigenvalues; Evd; Givens rotations; High performance; Libraries; Linear algebra; QR algorithm; Singular values; SVD; Tridiagonal,Eigenvalues and eigenfunctions; Libraries; Linear algebra; Numerical methods; Singular value decomposition; Bidiagonal; Eigenvalues; Evd; Givens Rotation; High performance; QR algorithms; Singular values; Tridiagonal; Algorithms
"Algorithm 935: IIPBF, a MATLAB toolbox for infinite integral of products of two Bessel functions",2014,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896995962&doi=10.1145%2f2508435&partnerID=40&md5=857f70b0120560dbb2123a7706d87d57,"A MATLAB toolbox, IIPBF, for calculating infinite integrals involving a product of two Bessel functions Ja (?x)Jb (? x), Ja (?x)Yb (? x) and Y a (?x)Yb (? x), for non-negative integers a, b, and a well-behaved function f (x), is described. Based on the Lucas algorithm previously developed for Ja (?x)Jb (? x) only, IIPBF recasts each product as the sum of two functions whose oscillatory behavior is exploited in the three-step procedure of adaptive integration, summation, and extrapolation. The toolbox uses customised QUADPACK and IMSL functions from a MATLAB conversion of the SLATEC library. In addition, MATLAB's own quadgk function for adaptive Gauss-Kronrod quadrature results in a significant speed up compared with the original algorithm. Usage of IIPBF is described and eighteen test cases illustrate the robustness of the toolbox; five additional ones are used to compare IIPBF with the BESSELINT code for rational and exponential forms of f (x) with Ja (?x)Jb (? x). Reliability for a broad range of values of ? and ? for the three different product types as well as different orders in one case is demonstrated. An electronic appendix provides a novel derivation of formulae for five cases.. © 2014 ACM.",Adaptive quadrature; Bessel function; Extrapolation; Quadrature; Summation,Algorithms; Bessel functions; Extrapolation; Adaptive integration; Adaptive quadrature; Electronic appendix; Nonnegative integers; Original algorithms; Oscillatory behaviors; Quadrature; Summation; Integral equations
A pthreads wrapper for fortran 2003,2014,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84899099243&doi=10.1145%2f2558889&partnerID=40&md5=e4f80000df6a50813a0cdf1c165b68dc,"With the advent of multicore processors, numerical and mathematical software relies on parallelism in order to benefit from hardware performance increases. We present the design and use of a Fortran 2003 wrapper for POSIX threads, called forthreads. Forthreads is complete in the sense that is provides native Fortran 2003 interfaces to all pthreads routines where possible. We demonstrate the use and efficiency of forthreads for SIMD parallelism and task parallelism. We present forthreads/MPI implementations that enable hybrid shared-/distributed-memory parallelism in Fortran 2003. Our benchmarks show that forthreads offers performance comparable to that of OpenMP, but better thread control and more freedom. We demonstrate the latter by presenting a multithreaded Fortran 2003 library for POSIX Internet sockets, enabling interactive numerical simulations with runtime control. © 2014 ACM.",Fortran; Fortran 2003; Mathematical software; Parallel particle-mesh; POSIX threads; PPM library; Pthreads; Scientific computing; Shared-memory programming,Application programming interfaces (API); Benchmarking; Mathematical programming; Multicore programming; Natural sciences computing; Mathematical software; Parallel particle-mesh; POSIX threads; Pthreads; Shared memory; FORTRAN (programming language)
Algorithm 939: Computation of the marcum Q-function,2014,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84899087027&doi=10.1145%2f2591004&partnerID=40&md5=d97e7f1ee984442c76a41d0ca0a34f41,"Methods and an algorithm for computing the generalized Marcum Q?function (Qμ(x, y)) and the complementary function (Pμ(x, y)) are described. These functions appear in problems of different technical and scientific areas such as, for example, radar detection and communications, statistics, and probability theory, where they are called the noncentral chi-square or the noncentral gamma cumulative distribution functions. The algorithm for computing the Marcum functions combines different methods of evaluation in different regions: series expansions, integral representations, asymptotic expansions, and use of three-term homogeneous recurrence relations. A relative accuracy close to 10?12 can be obtained in the parameter region (x, y, μ) ε [0, A]×[0, A]×[1, A], A = 200, while for larger parameters the accuracy decreases (close to 10?11 for A = 1000 and close to 5 × 10?11 for A = 10000). © 2014 ACM.",Marcum Q-function; Noncentral chi-square distribution,Probability distributions; Tracking radar; Asymptotic expansion; Cumulative distribution function; Integral representation; Marcum Q-function; Noncentral Chi-square distribution; Noncentral chi-squares; Probability theory; Recurrence relations; Algorithms
Algorithm 940: Optimal accumulator-based expression evaluation through the use of expression templates,2014,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84899117392&doi=10.1145%2f2591005&partnerID=40&md5=85065d6abf755d818c1069a108a6dfdb,"In this article we present a compile-time algorithm, implemented using C++ template metaprogramming techniques, that minimizes the use of temporary storage when evaluating expressions. We present the basic building blocks of our algorithm-transformations that act locally on nodes of the expression parse tree-and demonstrate that the application of these local transformations generates a (nonunique) expression that requires a minimum number of temporary storage objects to evaluate. We discuss a C++ implementation of our algorithm using expression templates, and give results demonstrating the effectiveness of our approach. © 2014 ACM.",Dense linear algebra operations; Expression templates; Template metaprogramming; Temporary storage minimization,Algorithms; Object oriented programming; C; Dense linear algebra; Expression templates; Template metaprogramming; Temporary storage; Trees (mathematics)
Algorithm 941: Htucker - A MATLAB Toolbox for tensors in hierarchical tucker format,2014,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84899099097&doi=10.1145%2f2538688&partnerID=40&md5=cf3919c4cfc84d902d9063207c21c539,"The hierarchical Tucker format is a storage-efficient scheme to approximate and represent tensors of possibly high order. This article presents a MATLAB toolbox, along with the underlying methodology and algorithms, which provides a convenient way to work with this format. The toolbox not only allows for the efficient storage and manipulation of tensors in hierarchical Tucker format but also offers a set of tools for the development of higher-level algorithms. Several examples for the use of the toolbox are given. © 2014 ACM.",Higher-order tensors; MATLAB; Multilinear algebra; Tensor networks,Algebra; Algorithms; MATLAB; Higher-order tensor; Matlab toolboxes; Multi-linear algebras; Tensors
Optimally packed chains of bulges in multishift QR algorithms,2014,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896927269&doi=10.1145%2f2559986&partnerID=40&md5=7a1501069b3e1e42200e23152384d8a7,"The QR algorithm is the method of choice for computing all eigenvalues of a dense nonsymmetric matrix A. After an initial reduction to Hessenberg form, a QR iteration can be viewed as chasing a small bulge from the top left to the bottom right corner along the subdiagonal of A. To increase data locality and create potential for parallelism, modern variants of the QR algorithm perform several iterations simultaneously, which amounts to chasing a chain of several bulges instead of a single bulge. To make effective use of level 3 BLAS, it is important to pack these bulges as tightly as possible within the chain. In this work, we show that the tightness of the packing in existing approaches is not optimal and can be increased. This directly translates into a reduced chain length by 33% compared to the state-of-the-art LAPACK implementation of the QR algorithm. To demonstrate the impact of our idea, we have modified the LAPACK implementation to make use of the optimal packing. Numerical experiments reveal a uniform reduction of the execution time, without affecting stability or robustness.. © 2014 ACM.",Implicit shifts; Level 3 BLAS; Multishift QR algorithms,Algorithms; Chains; Eigenvalues and eigenfunctions; Optimization; Hessenberg form; Implicit shifts; Level-3 BLAS; Non-symmetric matrices; Numerical experiments; Optimal packing; QR algorithms; Uniform reduction; Iterative methods
Unified form language: A domain-specific language for weak formulations of partial differential equations,2014,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896904019&doi=10.1145%2f2566630&partnerID=40&md5=60addab605048e895aa3fb1da7058ab9,"We present the Unified Form Language (UFL), which is a domain-specific language for representing weak formulations of partial differential equations with a view to numerical approximation. Features of UFL include support for variational forms and functionals, automatic differentiation of forms and expressions, arbitrary function space hierarchies formultifield problems, general differential operators and flexible tensor algebra. With these features, UFL has been used to effortlessly express finite element methods for complex systems of partial differential equations in near-mathematical notation, resulting in compact, intuitive and readable programs. We present in this work the language and its construction. An implementation of UFL is freely available as an open-source software library. The library generates abstract syntax tree representations of variational problems, which are used by other software libraries to generate concrete low-level implementations. Some application examples are presented and libraries that support UFL are highlighted. © 2014 ACM.",Algorithmic differentiation; Automatic functional differentiation; Discretization; Domain specific language; DSEL; DSL; Einstein notation; Embedded language; FEM; Finite element method; Functional; Implicit summation; Index notation; Mixed element; Partial differential equation; PDE; Symbolic differentiation; Tensor algebra; Weak form; Weak formulation,Algebra; Computer programming languages; DSL; Finite element method; Mathematical operators; Problem oriented languages; Tensors; Algorithmic differentiations; Discretizations; Domain specific languages; DSEL; Einstein notation; Embedded Languages; Functional; Implicit summation; Index notation; Mixed element; PDE; Symbolic differentiation; Tensor algebra; Weak form; Weak formulation; Partial differential equations
How do you compute the midpoint of an interval?,2014,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896920510&doi=10.1145%2f2493882&partnerID=40&md5=0f12a61156c1bdda0c5e47a0441ea75d,The algorithm that computes the midpoint of an interval with floating-point bounds requires some careful devising to handle all possible inputs correctly. We review several implementations from prominent C/C++ interval arithmetic packages and analyze their potential failure to deliver the expected results. We then show how to amend them to avoid common pitfalls. The results presented are also relevant to noninterval arithmetic computation such as the implementation of bisection methods. Enough background on IEEE 754 floating-point arithmetic is provided for this article to serve as a practical introduction to the analysis of floating-point computation.. © 2014 ACM.,Floating-point number; IEEE 754 standard; Interval arithmetic; Midpoint; Rounding error,Computer software; Software engineering; Floating point numbers; IEEE-754 standard; Interval arithmetic; Midpoint; Rounding errors; Digital arithmetic
A parallel implementation of davidson methods for large-scale eigenvalue problems in SLEPc,2014,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896954066&doi=10.1145%2f2543696&partnerID=40&md5=042bd1020eb223a2da82cdc9194f82fd,"In the context of large-scale eigenvalue problems, methods of Davidson type such as Jacobi-Davidson can be competitive with respect to other types of algorithms, especially in some particularly difficult situations such as computing interior eigenvalues or when matrix factorization is prohibitive or highly inefficient. However, these types of methods are not generally available in the form of high-quality parallel implementations, especially for the case of non-Hermitian eigenproblems.We present our implementation of various Davidsontype methods in SLEPc, the Scalable Library for Eigenvalue Problem Computations. The solvers incorporate many algorithmic variants for subspace expansion and extraction, and cover a wide range of eigenproblems including standard and generalized, Hermitian and non-Hermitian, with either real or complex arithmetic. We provide performance results on a large battery of test problems.. © 2014 ACM.",Davidson; Eigenvalue computations; Jacobi-Davidson; Messagepassing parallelization; SLEPc,Switching systems; Davidson; Eigenvalue computation; Jacobi-Davidson; Message-passing parallelization; SLEPc; Eigenvalues and eigenfunctions
Algorithm 936: A Fortran message processor,2014,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896918916&doi=10.1145%2f2559993&partnerID=40&md5=bf3e7a40b5bb0b52595e617202747791,"A code is presented which offers a simple clean way to get output that is very easy to read. Special support is given for the output of error messages which are a part of an application package or subprogram library. The code uses many of the features in Fortran 2003, and the ""NEWUNIT="" in an open statement from Fortran 2008. The latter can easily be replaced with ""UNIT=99"". One goal here is to illustrate some of the nice features in recent incarnations of Fortran.. © 2014 ACM.",Error processing; Exception handling; Messages,Computer software; Software engineering; Error messages; Error processing; Exception handling; Messages; FORTRAN (programming language)
Algorithm 937: MINRES-QLP for symmetric and hermitian linear equations and least-squares problems,2014,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896941666&doi=10.1145%2f2527267&partnerID=40&md5=640d74375feb7acb6d4a214337d9c55c,"We describe algorithmMINRES-QLP and its FORTRAN 90 implementation for solving symmetric or Hermitian linear systems or least-squares problems. If the system is singular, MINRES-QLP computes the unique minimum-length solution (also known as the pseudoinverse solution), which generally eludes MINRES. In all cases, it overcomes a potential instability in the original MINRES algorithm. A positive-definite preconditioner may be supplied. Our FORTRAN 90 implementation illustrates a design pattern that allows users to make problem data known to the solver but hidden and secure from other program units. In particular, we circumvent the need for reverse communication. Example test programs input and solve real or complex problems specified in Matrix Market format. While we focus here on a FORTRAN 90 implementation, we also provide and maintain MATLAB versions of MINRES and MINRES-QLP.. © 2014 ACM.",Conjugate-gradient method; Data encapsulation; Ill-posed problem; Krylov subspace method; Lanczos process; Linear equations; Minimum-residual method; Pseudoinverse solution; Regression; Singular least-squares; Sparse matrix,Algorithms; Data encapsulation; FORTRAN (programming language); Gradient methods; Linear equations; Linear systems; MATLAB; Ill posed problem; Krylov subspace method; Lanczos process; Least Square; Minimum-residual method; Pseudo-inverse solution; Regression; Sparse matrices; Least squares approximations
Pivoting strategies for tough sparse indefinite systems,2013,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885641449&doi=10.1145%2f2513109.2513113&partnerID=40&md5=d98ab43474748aba00f55f95250a7e0e,"The performance of a sparse direct solver is dependent upon the pivot sequence that is chosen before the factorization begins. In the case of symmetric indefinite systems, it may be necessary to modify this sequence during the factorization to ensure numerical stability. These modifications can have serious consequences in terms of time as well as the memory and flops required for the factorization and subsequent solves. This study focuses on hard-to-solve sparse symmetric indefinite problems for which standard threshold partial pivoting leads to significant modifications. We perform a detailed review of pivoting strategies that are aimed at reducing the modifications without compromising numerical stability. Extensive numerical experiments are performed on a set of tough problems arising from practical applications. Based on our findings, we make recommendations on which strategy to use and, in particular, a matching-based approach is recommended for numerically challenging problems. © 2013 ACM.",Direct solver; Indefinite systems; Pivoting; Sparse symmetric linear systems,Linear systems; Direct solvers; Indefinite systems; Numerical experiments; Pivoting; Sparse direct solver; Symmetric indefinite; Symmetric linear systems; Factorization
ColPack: Software for graph coloring and related problems in scientific computing,2013,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885659337&doi=10.1145%2f2513109.2513110&partnerID=40&md5=a3ebda7419809ed137ff42e28f99f3dc,"We present a suite of fast and effective algorithms, encapsulated in a software package called ColPack, for a variety of graph coloring and related problems. Many of the coloring problems model partitioning needs arising in compression-based computation of Jacobian and Hessian matrices using Algorithmic Differentiation. Several of the coloring problems also find important applications in many areas outside derivative computation, including frequency assignment in wireless networks, scheduling, facility location, and concurrency discovery and data movement operations in parallel and distributed computing. The presentation in this article includes a high-level description of the various coloring algorithms within a common design framework, a detailed treatment of the theory and efficient implementation of known as well as new vertex ordering techniques upon which the coloring algorithms rely, a discussion of the package's software design, and an illustration of its usage. The article also includes an extensive experimental study of the major algorithms in the package using real-world as well as synthetically generated graphs. © 2013 ACM.",Automatic differentiation; Combinatorial optimization; Graph coloring; Greedy coloring algorithms; Nonlinear optimization; Sparse derivative computation; Vertex ordering techniques,Coloring; Combinatorial optimization; Graph theory; Jacobian matrices; Automatic differentiations; Coloring algorithms; Graph colorings; Non-linear optimization; Vertex ordering; Algorithms
GHull: A GPU algorithm for 3D convex hull,2013,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885676691&doi=10.1145%2f2513109.2513112&partnerID=40&md5=e3d9e532ed42a38d2ed80b36e91add77,"A novel algorithm is presented to compute the convex hull of a point set in R3 using the graphics processing unit (GPU). By exploiting the relationship between the Voronoi diagram and the convex hull, the algorithm derives the approximation of the convex hull from the former. The other extreme vertices of the convex hull are then found by using a two-round checking in the digital and the continuous space successively. The algorithm does not need explicit locking or any other concurrency control mechanism, thus it can maximize the parallelism available on the modern GPU. The implementation using the CUDA programming model on NVIDIA GPUs is exact and efficient. The experiments show that it is up to an order of magnitude faster than other sequential convex hull implementations running on the CPU for inputs of millions of points. The works demonstrate that the GPU can be used to solve nontrivial computational geometry problems with significant performance benefit. © 2013 ACM.",GPGPU; Star splaying; Voronoi diagram,Approximation algorithms; Computer graphics; Computer graphics equipment; Concurrency control; Graphic methods; Program processors; Continuous spaces; CUDA Programming; GPGPU; GPU algorithms; Graphics Processing Unit; Novel algorithm; Performance benefits; Voronoi diagrams; Computational geometry
Algorithm 934: Fortran 90 subroutines to compute Mathieu functions for complex values of the parameter,2013,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885678590&doi=10.1145%2f2513109.2513117&partnerID=40&md5=14e6e210300231a34767c00a546ef273,"Software to compute angular and radial Mathieu functions is provided in the case that the parameter q is a complex variable and the independent variable x is real. After an introduction on the notation and the definitions of Mathieu functions and their related properties, Fortran 90 subroutines to compute them are described and validated with some comparisons. A sample application is also provided. © 2013 ACM.",Computation; Mathieu function; Special function; Validation,Calculations; Computer software; Software engineering; Complex values; Complex variable; Fortran 90; Independent variables; Mathieu functions; Sample applications; Special functions; Validation; FORTRAN (programming language)
"Algorithm 933: Reliable calculation of numerical rank, null space bases, pseudoinverse solutions, and basic solutions using SuiteSparseQR",2013,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885611522&doi=10.1145%2f2513109.2513116&partnerID=40&md5=8ab40970c3416be9196474e86b12f86a,"The SPQR RANK package contains routines that calculate the numerical rank of large, sparse, numerically rank-deficient matrices. The routines can also calculate orthonormal bases for numerical null spaces, approximate pseudoinverse solutions to least squares problems involving rank-deficient matrices, and basic solutions to these problems. The algorithms are based on SPQR from SuiteSparseQR (ACM Transactions on Mathematical Software 38, Article 8, 2011). SPQR is a high-performance routine for forming QR factorizations of large, sparse matrices. It returns an estimate for the numerical rank that is usually, but not always, correct. The new routines improve the accuracy of the numerical rank calculated by SPQR and reliably determine the numerical rank in the sense that, based on extensive testing with matrices from applications, the numerical rank is almost always accurately determined when our methods report that the numerical rank should be correct. Reliable determination of numerical rank is critical to the other calculations in the package. The routines work well for matrices with either small or large null space dimensions. © 2013 ACM.",Null space; Numerical rank; Pseudoinverse; QR factorization; Rank revealing; Sparse matrices,Algorithms; Factorization; Null space; Numerical rank; Pseudo-inverses; QR factorizations; Rank-revealing; Sparse matrices; Numerical methods
Algorithm 931: An algorithm and software for computing multiplicity structures at zeros of nonlinear systems,2013,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885589983&doi=10.1145%2f2513109.2513114&partnerID=40&md5=2d7bb03f3cd0f355c3f1519c8cfed3a2,"A Matlab implementation, MULTIPLICITY, of a numerical algorithm for computing the multiplicity structure of a nonlinear system at an isolated zero is presented. The software incorporates a newly developed equationby- equation strategy that significantly improves the efficiency of the closedness subspace algorithm and substantially reduces the storage requirement. The equation-by-equation strategy is actually based on a variable-by-variable closedness subspace approach. As a result, the algorithm and software can handle much larger nonlinear systems and higher multiplicities than their predecessors, as shown in computational experiments on the included test suite of benchmark problems. © 2013 ACM.",Multiple roots; Multiplicity; Polynomial,Algorithms; Nonlinear systems; Polynomials; Bench-mark problems; Computational experiment; Multiple roots; Multiplicity; Multiplicity structures; Numerical algorithms; Storage requirements; Sub-space algorithms; MATLAB
CHEBINT: A MATLAB/Octave toolbox for fast multivariate integration and interpolation based on Chebyshev approximations over hypercubes,2013,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885625026&doi=10.1145%2f2513109.2513111&partnerID=40&md5=98280455dbf8200724ee9714bd707ddf,"We present the fast approximation of multivariate functions based on Chebyshev series for two types of Chebyshev lattices and show how a fast Fourier transform (FFT) based discrete cosine transform (DCT) can be used to reduce the complexity of this operation. Approximating multivariate functions using rank-1 Chebyshev lattices can be seen as a one-dimensional DCT while a full-rank Chebyshev lattice leads to a multivariate DCT. We also present a MATLAB/Octave toolbox which uses this fast algorithms to approximate functions on a axis aligned hyper-rectangle. Given a certain accuracy of this approximation, interpolation of the original function can be achieved by evaluating the approximation while the definite integral over the domain can be estimated based on this Chebyshev approximation. We conclude with an example for both operations and actual timings of the two methods presented. © 2013 ACM.",Chebyshev lattices; Fast Fourier transform; Godzina blending formulae; MATLAB/Octave toolbox; Multivariate integration; Multivariate interpolation,Blending; Chebyshev approximation; Discrete cosine transforms; Fast Fourier transforms; Approximate function; Chebyshev; Discrete Cosine Transform(DCT); Fast approximation; Godzina blending formulae; Multivariate function; Multivariate integration; Multivariate interpolation; Interpolation
Algorithm 932: PANG: Software for nonmatching grid projections in 2D and 3D with linear complexity,2013,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885607180&doi=10.1145%2f2513109.2513115&partnerID=40&md5=16942f662e99e294891715606c39a51c,"We design and analyze an algorithm with linear complexity to perform projections between 2D and 3D nonmatching grids. This algorithm, named the PANG algorithm, is based on an advancing front technique and neighboring information. Its implementation is surprisingly short, and we give the entire Matlab code. For computing the intersections, we use a direct and numerically robust approach. We show numerical experiments both for 2D and 3D grids, which illustrate the optimal complexity and negligible overhead of the algorithm. An outline of this algorithm has already been presented in a short proceedings paper of the 18th International Conference on Domain Decomposition Methods (see Gander and Japhet [2008]). © 2013 ACM.",Advancing front algorithm; Linear complexity; Non-matching grid projections,Domain decomposition methods; Three dimensional; Advancing front; Advancing front technique; Linear complexity; Neighboring information; Non-matching grid; Numerical experiments; Numerically robust; Optimal complexity; Algorithms
Algorithm 929: A suite on wavelet differentiation algorithms,2013,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880833168&doi=10.1145%2f2491491.2491497&partnerID=40&md5=7e7da7dd4cbfbb0e0017313c3a34065e,"A collection of the Matlab routines that compute the values of the scaling and wavelet functions (φ(x) and ψ(x) respectively) and the derivative of an arbitrary function (periodic or non periodic) using wavelet bases is presented. Initially, the case of Daubechies wavelets is taken and the procedure is explained for both collocation and Galerkin approaches. For each case a Matlab routine is provided to compute the differentiation matrix and the derivative of the function f (d) = D(d) f . Moreover, the convergence of the derivative is shown graphically as a function of different parameters (the wavelet genus, D and the scale, J) for two test functions. We then consider the use of spline wavelets. © 2013 ACM.",Algorithms; Theory,Algorithms; Wavelet analysis; Arbitrary functions; Daubechies Wavelet; Differentiation algorithms; Differentiation matrices; Galerkin approach; Spline wavelets; Theory; Wavelet function; Discrete wavelet transforms
Optimized code generation for finite element local assembly using symbolic manipulation,2013,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880841806&doi=10.1145%2f2491491.2491496&partnerID=40&md5=f6e21e2152ca5c6e68f581ba215234fd,"Automated code generators for finite element local assembly have facilitated exploration of alternative implementation strategies within generated code. However, even for a theoretical performance indicator such as operation count, an optimal strategy for local assembly is unknown.We explore a code generation strategy based on symbolic integration and polynomial common subexpression elimination (CSE). We present our implementation of a local assembly code generator using these techniques. We systematically evaluate the approach, measuring operation count, execution time and numerical error using a benchmark suite of synthetic variational forms, comparing against the FEniCS Form Compiler (FFC). Our benchmark forms span complexities chosen to expose the performance characteristics of different code generation approaches. We show that it is possible with additional computational cost, to consistently achieve much of, and sometimes substantially exceed, the performance of alternative approaches without compromising precision. Although the approach of using symbolic integration and CSE for optimizing local assembly is not new, we distinguish our work through our strategies for maintaining numerical precision and detecting common subexpressions. We discuss the benefits of the symbolic approach for inferring numerical relationships, and analyze the relationship to other proposed techniques which also have greater computational complexity than those of FFC. © 2013 ACM.",Algorithms; Performance,Algorithms; Automatic programming; Benchmarking; Network components; Optimization; Common subexpression elimination; Exploration of alternatives; Implementation strategies; Performance; Performance characteristics; Symbolic integration; Symbolic manipulation; Theoretical performance; Program compilers
Evaluating an element of the clarke generalized jacobian of a composite piecewise differentiable function,2013,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880812848&doi=10.1145%2f2491491.2491493&partnerID=40&md5=e49989cf2cd8e050d26ba70a1b948b2f,"Bundle methods for nonsmooth optimization and semismooth Newton methods for nonsmooth equation solving both require computation of elements of the (Clarke) generalized Jacobian, which provides slope information for locally Lipschitz continuous functions. Since the generalized Jacobian does not obey sharp calculus rules, this computation can be difficult. In this article, methods are developed for evaluating generalized Jacobian elements for a nonsmooth function that is expressed as a finite composition of known elemental piecewise differentiable functions. In principle, these elemental functions can include any piecewise differentiable function whose analytical directional derivatives are known. The methods are fully automatable, and are shown to be computationally tractable relative to the cost of a function evaluation. An implementation developed in C++ is discussed, and the methods are applied to several example problems for illustration. © 2013 ACM.",Algorithms; Performance; Theory,Algorithms; Newton-Raphson method; Differentiable functions; Directional derivative; Generalized Jacobian; Non-smooth functions; Nonsmooth optimization; Performance; Semismooth Newton method; Theory; Function evaluation
Scaling LAPACK panel operations using parallel cache assignment,2013,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880817871&doi=10.1145%2f2491491.2491493&partnerID=40&md5=2a60f5d6d2098e56072cc5a7d7b4a2d8,"In LAPACK many matrix operations are cast as block algorithms which iteratively process a panel using an unblocked algorithm and then update a remainder matrix using the high performance Level 3 BLAS. The Level 3 BLAS have excellent scaling, but panel processing tends to be bus bound, and thus scales with bus speed rather than the number of processors (p). Amdahl's law therefore ensures that as p grows, the panel computation will become the dominant cost of these LAPACK routines. Our contribution is a novel parallel cache assignment approach to the panel factorization which we show scales well with p.We apply this general approach to the QR, QL, RQ, LQ and LU panel factorizations. We show results for two commodity platforms: An 8-core Intel platform and a 32-core AMD platform. For both platforms and all twenty implementations (five factorizations each of which is available in 4 types), we present results that demonstrate that our approach yields significant speedup over the existing state of the art. © 2013 ACM.",Algorithms; Design; Experimentation; Performance,Algorithms; Design; Iterative methods; Block algorithm; Experimentation; General approach; Matrix operations; Performance; Performance level; Remainder matrix; State of the art; Factorization
Reducing the influence of tiny normwise relative errors on performance profiles,2013,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880783083&doi=10.1145%2f2491491.2491494&partnerID=40&md5=310e8e81d0e3ea8d823e51ef1943188b,"It is a widespread but little-noticed phenomenon that the normwise relative error x - y/x of vectors x and y of floating point numbers of the same prec sion, where y is an approximation to x, can be many orders of magnitude smaller than the unit roundoff. We analyze this phenomenon and show that in the ∞-norm it happens precisely when x has components of widely varying magnitude and every component of x of largest magnitude agrees with the corresponding component of y. Performance profiles are a popular way to compare competing algorithms according to particular measures of performance. We show that performance profiles based on normwise relative errors can give a misleading impression due to the influence of zero or tiny normwise relative errors. We propose a transformation that reduces the influence of these extreme errors in a controlled manner, while preserving the monotonicity of the underlying data and leaving the performance profile unchanged at its left end-point. Numerical examples with both artificial and genuine data illustrate the benefits of the transformation. © 2013 ACM.",Algorithms; Performance,Algorithms; Metadata; Competing algorithms; Floating point numbers; Measures of performance; Numerical example; Orders of magnitude; Performance; Performance profile; Relative errors; Errors
On ziv's rounding test,2013,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880771605&doi=10.1145%2f2491491.2491495&partnerID=40&md5=14819044a7a0517d5086bdd91a1fbe66,"A very simple test, introduced by Ziv, allows one to determine if an approximation to the value f (x) of an elementary function at a given point x suffices to return the floating-point number nearest f (x). The same test may be used when implementing floating-point operations with input and output operands of different formats, using arithmetic operators tailored for manipulating operands of the same format. That test depends on a ""magic constant"" e.We show how to choose that constant e to make the test reliable and efficient. Various cases are considered, depending on the availability of an fma instruction, and on the range of f (x). © 2013 ACM.",Algorithms,Algorithms; Digital arithmetic; Elementary function; Floating point numbers; Floating point operations; Input and outputs; Magic constants; Simple tests; Testing
Algorithm 930: Factorize: An object-oriented linear system solver for MATLAB,2013,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880826395&doi=10.1145%2f2491491.2491498&partnerID=40&md5=093dc1afea12a302dc002e4f1062cb86,"The MATLABTM backslash (x=A\b) is an elegant and powerful interface to a suite of high-performance factorization methods for the direct solution of the linear system Ax = b and the least-squares problem minx ||b - Ax||. It is a meta-algorithm that selects the best factorization method for a particular matrix, whether sparse or dense. However, the simplicity and elegance of its single-character interface prohibits the reuse of its factorization for subsequent systems. Requiring MATLAB users to find the best factorization method on their own can lead to suboptimal choices; even MATLAB experts can make the wrong choice. Furthermore, naive MATLAB users have a tendency to translate mathematical expressions from linear algebra directly into MATLAB, so that x = A-1b becomes the inferior yet all-to-prevalent x=inv(A)*b. To address these issues, an object-oriented FACTORIZE method is presented. Via simple-to-use operator overloading, solving two linear systems can be written as F=factorize(A); x=F\b; y=F\c, where A is factorized only once. The selection of the best factorization method (LU, Cholesky, LDLT , QR, or a complete orthogonal decomposition for rank-deficient matrices) is hidden from the user. The mathematical expression x = A-1b directly translates into the MATLAB expression x=inverse(A)*b, which does not compute the inverse at all, but does the right thing by factorizing A and solving the corresponding triangular systems. © 2013 ACM.",Algorithms; Experimentation; Performance,Algorithms; Factorization; Least squares approximations; Linear algebra; Linear systems; Experimentation; Factorization methods; Linear system solver; Mathematical expressions; Operator overloading; Orthogonal decomposition; Particular matrixes; Performance; MATLAB
An algorithm for the complete solution of quadratic eigenvalue problems,2013,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84877895528&doi=10.1145%2f2450153.2450156&partnerID=40&md5=c683ab01315e9b68ee4bb020d1a0265b,"We develop a new algorithm for the computation of all the eigenvalues and optionally the right and left eigenvectors of dense quadratic matrix polynomials. It incorporates scaling of the problem parameters prior to the computation of eigenvalues, a choice of linearization with favorable conditioning and backward stability properties, and a preprocessing step that reveals and deflates the zero and infinite eigenvalues contributed by singular leading and trailing matrix coefficients. The algorithm is backward-stable for quadratics that are not too heavily damped. Numerical experiments show that our MATLAB implementation of the algorithm, quadeig, outperforms the MATLAB function polyeig in terms of both stability and efficiency. © 2013 ACM.",Backward error; Companion form; Condition number; Deflation; Eigenvector; Linearization; Quadratic eigenvalue problem; Scaling,Algorithms; Linearization; Number theory; Stability; Backward error; Companion form; Condition numbers; Deflation; Quadratic eigenvalue problems; Scaling; Eigenvalues and eigenfunctions
An efficient overloaded method for computing derivatives of mathematical functions in MATLAB,2013,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84877902525&doi=10.1145%2f2450153.2450155&partnerID=40&md5=75a24f76620ac4aad76a9d752437bfc6,"An object-oriented method is presented that computes without truncation the error derivatives of functions defined by MATLAB computer codes. The method implements forward-mode automatic differentiation via operator overloading in a manner that produces a new MATLAB code that computes the derivatives of the outputs of the original function with respect to the differentiation variables. Because the derivative code has the same input as the original function code, the method can be used recursively to generate derivatives of any order desired. In addition, the approach developed in this article has the feature that the derivatives are generated by simply evaluating the function on an instance of the class, thus making the method straightforward to use while simultaneously enabling differentiation of highly complex functions. A detailed description of the method is presented and the approach is illustrated and shown to be efficient on four examples. © 2013 ACM.",Applied mathematics; Automatic differentiation; MATLAB; Numerical methods; Scientific computation,Functions; Numerical methods; Applied mathematics; Automatic differentiations; Complex functions; Computer codes; Mathematical functions; Object oriented method; Operator overloading; Scientific computation; MATLAB
Efficient generalized Hessenberg form and applications,2013,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84877913658&doi=10.1145%2f2450153.2450157&partnerID=40&md5=e535e25512cf31e767a99c8ff6759e08,"This article proposes an efficient algorithm for reducing matrices to generalized Hessenberg form by unitary similarity, and recommends using it as a preprocessor in a variety of applications. To illustrate its usefulness, two cases from control theory are analyzed in detail: a solution procedure for a sequence of shifted linear systems with multiple right hand sides (e.g. evaluating the transfer function of a MIMO LTI dynamical system at many points) and computation of the staircase form. The proposed algorithm for the generalized Hessenberg reduction uses two levels of aggregation of Householder reflectors, thus allowing efficient BLAS 3-based computation. Another level of aggregation is introduced when solving many shifted systems by processing the shifts in batches. Numerical experiments confirm that the proposed methods have superior efficiency. © 2013 ACM.",Hessenberg form; Shifted linear systems; Staircase algorithm; Transfer function,Algorithms; Dynamical systems; Linear systems; Numerical methods; Stairs; Transfer functions; Hessenberg form; Hessenberg reduction; Multiple right-hand sides; Numerical experiments; Shifted linear systems; Shifted systems; Solution procedure; Unitary similarity; Computational efficiency
"The tapenade automatic differentiation tool: Principles, model, and specification",2013,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84877890324&doi=10.1145%2f2450153.2450158&partnerID=40&md5=da413b53170fd6ef76204c54b30f3761,"Tapenade is an Automatic Differentiation (AD) tool which, given a Fortran or C code that computes a function, creates a new code that computes its tangent or adjoint derivatives. Tapenade puts particular emphasis on adjoint differentiation, which computes gradients at a remarkably low cost. This article describes the principles of Tapenade, a subset of the general principles of AD. We motivate and illustrate with examples the AD model of Tapenade, that is, the structure of differentiated codes and the strategies used to make them more efficient. Along with this informal description, we formally specify this model by means of data-flow equations and rules of Operational Semantics, making this the reference specification of the tangent and adjoint modes of Tapenade. One benefit we expect from this formal specification is the capacity to formally study the AD model itself, especially for the adjoint mode and its sophisticated strategies. This article also describes the architectural choices of the implementation of Tapenade. We describe the current performance of Tapenade on a set of codes that include industrial-size applications. We present the extensions of the tool that are planned in a foreseeable future, deriving from our ongoing research on AD. © 2013 ACM.",Adjoint compiler; Source transformation,Computer software; Software engineering; Adjoints; Automatic differentiation tool; Current performance; Dataflow; Formal Specification; Low costs; Operational semantics; Source transformation; Specifications
"Algorithm 928: A general, parallel implementation of Dantzig-Wolfe decomposition",2013,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84877881972&doi=10.1145%2f2450153.2450159&partnerID=40&md5=e2d864301a74becfc478b28df736e89f,"Dantzig-Wolfe Decomposition is recognized as a powerful, algorithmic tool for solving linear programs of block-angular form. While use of the approach has been reported in a wide variety of domains, there has not been a general implementation of Dantzig-Wolfe decomposition available. This article describes an open-source implementation of the algorithm. It is general in the sense that any properly decomposed linear program can be provided to the software for solving. While the original description of the algorithm was motivated by its reduced memory usage, modern computers can also take advantage of the algorithm's inherent parallelism. This implementation is parallel and built upon the POSIX threads (pthreads) library. Some computational results are provided to motivate use of such parallel solvers, as this implementation outperforms state-of-the-art commercial solvers in terms of wall-clock runtime by an order of magnitude or more on several problem instances. © 2013 ACM.",Linear programming; Optimization; Parallel implementations,Linear programming; Optimization; Algorithmic tools; Commercial solvers; Computational results; Dantzig-wolfe decomposition; Inherent parallelism; Open source implementation; Parallel implementations; Problem instances; Algorithms
Algorithm 927: The MATLAB code bvptwp.m for the numerical solution of two point boundary value problems,2013,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875150934&doi=10.1145%2f2427023.2427032&partnerID=40&md5=acacf45feba04d1686b43141993cd4ba,"In this article we describe the code bvptwp.m, a MATLAB code for the solution of two point boundary value problems. This code is based on the well-known Fortran codes, twpbvp.f, twpbvpl.f and acdc.f, that employ a mesh selection strategy based on the estimation of the local error, and on revisions of these codes, called twpbvpc.f, twpbvplc.f and acdcc.f, that employ a mesh selection strategy based on the estimation of the local error and the estimation of two parameters which characterize the conditioning of the problem. The codes twpbvp.f/tpbvpc.f use a deferred correction scheme based on Mono-Implicit Runge-Kutta methods (MIRK); the other codes use a deferred correction scheme based on Lobatto formulas. The acdc.f/acdcc.f codes implement an automatic continuation strategy. The performance and features of the new solver are checked by performing some numerical tests to show that the new code is robust and able to solve very difficult singularly perturbed problems. The results obtained show that bvptwp.m is often able to solve problems requiring stringent accuracies and problems with very sharp changes in the solution. This code, coupled with the existing boundary value codes such as bvp4c.m, makes the MATLAB BVP section an extremely powerful one for a very wide range of problems. © 2013 ACM.",Automatic continuation; Boundary value problems; Conditioning; Deferred correction; Mesh selection,Boundary value problems; Estimation; MATLAB; Natural gas conditioning; Perturbation techniques; Automatic continuation; Boundary values; Deferred correction; Mesh selection; Numerical solution; Numerical tests; Singularly perturbed problem; Two point boundary value problems; Problem solving
Algorithm 926: Incomplete gamma functions with negative arguments,2013,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875190839&doi=10.1145%2f2427023.2427031&partnerID=40&md5=5206a6d11870463602e2722ae1334852,"An algorithm for accurately computing the lower incomplete gamma function γ (a, t) in the case where a = n + 1/2, n ε ℤ and t < 0 is described. Series expansions and analytic continuation are employed to compute the function for certain critical values of n, and these results are used to initiate stable recurrence. The algorithm has been implemented in Fortran 2003, with precompuations carried out in Maple. © 2013 ACM.",Incomplete gamma function,Computer software; Software engineering; Analytic continuation; Critical value; Incomplete gamma functions; Series expansion; Algorithms
NLEVP: A collection of nonlinear eigenvalue problems,2013,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875145204&doi=10.1145%2f2427023.2427024&partnerID=40&md5=6204fe52d14f0e5eab520bfd3cf5f12d,"We present a collection of 52 nonlinear eigenvalue problems in the form of a MATLAB toolbox. The collection contains problems from models of real-life applications as well as ones constructed specifically to have particular properties. A classification is given of polynomial eigenvalue problems according to their structural properties. Identifiers based on these and other properties can be used to extract particular types of problems from the collection. A brief description of each problem is given. NLEVP serves both to illustrate the tremendous variety of applications of nonlinear eigenvalue problems and to provide representative problems for testing, tuning, and benchmarking of algorithms and codes. © 2013 ACM.",Benchmark; Elliptic; Even; Gyroscopic; Hermitian; Hyperbolic; MATLAB; Nonlinear eigenvalue problem; Octave; Odd; Overdamped; Palindromic; Polynomial eigenvalue problem; Proportionally-damped; Quadratic eigenvalue problem; Rational eigenvalue problem; Symmetric; Test problem,Benchmarking; MATLAB; Switching systems; Benchmark; Elliptic; Even; Gyroscopic; Hermitians; Hyperbolic; Nonlinear eigenvalue problem; Octave; Odd; Overdamped; Palindromic; Polynomial eigenvalue problems; Proportionally-damped; Quadratic eigenvalue problems; Rational eigenvalue problems; Symmetric; Test problem; Eigenvalues and eigenfunctions
Variants of Mersenne Twister suitable for Graphic Processors,2013,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875139814&doi=10.1145%2f2427023.2427029&partnerID=40&md5=4cf18a7b5dd425eac1e9afc059450449,"This article proposes a type of pseudorandom number generator, Mersenne Twister for Graphic Processor (MTGP), for efficient generation on graphic processessing units (GPUs). MTGP supports large state sizes such as 11213 bits, and uses the high parallelism of GPUs in computing many steps of the recursion in parallel. The second proposal is a parameter-set generator for MTGP, named MTGP Dynamic Creator (MTGPDC). MTGPDC creates up to 232 distinct parameter sets which generate sequences with high-dimensional uniformity. This facility is suitable for a large grid of GPUs where each GPU requires separate random number streams. MTGP is based on linear recursion over the two-element field, and has better high-dimensional equidistribution than the Mersenne Twister pseudorandom number generator. © 2013 ACM.",Dynamic creator; General-purpose computing on graphics processing units; Mersenne Twister; Pseudo random number generator,Computer graphics; Parallel processing systems; Program processors; Equidistribution; Graphic processors; Graphics Processing Unit; High-dimensional; Linear recursion; Mersenne twisters; Pseudo random number generators; Random Numbers; Random number generation
A Runge-Kutta BVODE solver with global error and defect control,2013,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875173450&doi=10.1145%2f2427023.2427028&partnerID=40&md5=ae9908796ff70cef5c3d16b02479b7be,"Boundary value ordinary differential equations (BVODEs) are systems of ODEs with boundary conditions imposed at two or more distinct points. The global error (GE) of a numerical solution to a BVODE is the amount by which the numerical solution differs from the exact solution. The defect is the amount by which the numerical solution fails to satisfy the ODEs and boundary conditions. Although GE control is often familiar to users, the defect controlled numerical solution can be interpreted as the exact solution to a perturbation of the original BVODE. Software packages based on GE control and on defect control are in wide use. The defect control solver, BVP SOLVER, can provide an a posteriori estimate of the GE using Richardson extrapolation. In this article, we consider three more strategies for GE estimation based on (i) the direct use of a higher-order discretization formula (HO), (ii) the use of a higher-order discretization formula within a deferred correction (DC) framework, and (iii) the product of an estimate of the maximum defect and an estimate of the BVODE conditioning constant, and demonstrate that the HO and DC approaches have superior performance. We also modify BVP SOLVER to introduce GE control. © 2013 ACM.",Boundary value ordinary differential equations; Conditioning; Defect control; Deferred correction; Global error estimation; Richardson extrapolation; Runge-Kutta methods,Boundary conditions; Estimation; Extrapolation; Natural gas conditioning; Ordinary differential equations; Runge Kutta methods; Boundary values; Defect control; Deferred correction; Global error estimation; Richardson extrapolation; Defects
Finite element integration on GPUs,2013,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875154515&doi=10.1145%2f2427023.2427027&partnerID=40&md5=48a78eeee388ffb389e04837b3ae114c,"We present a novel finite element integration method for low-order elements on GPUs. We achieve more than 100GF for element integration on first order discretizations of both the Laplacian and Elasticity operators on an NVIDIA GTX285, which has a nominal single precision peak flop rate of 1 TF/s and bandwidth of 159 GB/s, corresponding to a bandwidth limited peak of 40 GF/s. © 2013 ACM.",Finite element integration; GPGPUs,Bandwidth; Elasticity; Integration; Bandwidth limiteds; Discretizations; Finite Element; First order; GPGPUs; Integration method; Laplacians; Single precision; Program processors
Accelerating linear system solutions using randomization techniques,2013,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875140744&doi=10.1145%2f2427023.2427025&partnerID=40&md5=01e5fbc64273ef5372de27bea883e116,We illustrate how linear algebra calculations can be enhanced by statistical techniques in the case of a square linear system Ax = b. We study a random transformation of A that enables us to avoid pivoting and then to reduce the amount of communication. Numerical experiments show that this randomization can be performed at a very affordable computational price while providing us with a satisfying accuracy when compared to partial pivoting. This random transformation called Partial Random Butterfly Transformation (PRBT) is optimized in terms of data storage and flops count. We propose a solver where PRBT and the LU factorization with no pivoting take advantage of the current hybrid multicore/GPU machines and we compare its Gflop/s performance with a solver implemented in a current parallel library. © 2013 ACM.,Dense linear algebra; Graphics processing units; Linear systems; LU factorization; Multiplicative preconditioning; Randomization,Computer graphics; Factorization; Linear algebra; Linear systems; Program processors; Dense linear algebra; Graphics Processing Unit; LU factorization; Multiplicative preconditioning; Randomization; Random processes
Elemental: A new framework for distributed memory dense matrix computations,2013,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875133170&doi=10.1145%2f2427023.2427030&partnerID=40&md5=719bcc6c0e0d1df9b4f61c44160ef3d7,"Parallelizing dense matrix computations to distributed memory architectures is a well-studied subject and generally considered to be among the best understood domains of parallel computing. Two packages, developed in the mid 1990s, still enjoy regular use: ScaLAPACK and PLAPACK. With the advent of many-core architectures, which may very well take the shape of distributed memory architectures within a single processor, these packages must be revisited since the traditional MPI-based approaches will likely need to be extended. Thus, this is a good time to review lessons learned since the introduction of these two packages and to propose a simple yet effective alternative. Preliminary performance results show the new solution achieves competitive, if not superior, performance on large clusters. © 2013 ACM.",High-performance; Libraries; Linear algebra; Parallel computing,Libraries; Linear algebra; Memory architecture; Parallel architectures; Parallel processing systems; Dense matrices; Distributed Memory; Distributed memory architecture; High-performance; Lessons learned; Many-core architecture; Preliminary performance results; Single processors; Matrix algebra
High-performance bidiagonal reduction using tile algorithms on homogeneous multicore architectures,2013,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84877905452&doi=10.1145%2f2450153.2450154&partnerID=40&md5=18d731ddbf72c71afbc1bfe7dcd63758,"This article presents a new high-performance bidiagonal reduction (BRD) for homogeneous multicore architectures. This article is an extension of the high-performance tridiagonal reduction implemented by the same authors [Luszczek et al., IPDPS 2011] to the BRD case. The BRD is the first step toward computing the singular value decomposition of a matrix, which is one of the most important algorithms in numerical linear algebra due to its broad impact in computational science. The high performance of the BRD described in this article comes from the combination of four important features: (1) tile algorithms with tile data layout, which provide an efficient data representation in main memory; (2) a two-stage reduction approach that allows to cast most of the computation during the first stage (reduction to band form) into calls to Level 3 BLAS and reduces the memory traffic during the second stage (reduction from band to bidiagonal form) by using high-performance kernels optimized for cache reuse; (3) a data dependence translation layer that maps the general algorithm with column-major data layout into the tile data layout; and (4) a dynamic runtime system that efficiently schedules the newly implemented kernels across the processing units and ensures that the data dependencies are not violated. A detailed analysis is provided to understand the critical impact of the tile size on the total execution time, which also corresponds to the matrix bandwidth size after the reduction of the first stage. The performance results show a significant improvement over currently established alternatives. The new high-performance BRD achieves up to a 30-fold speedup on a 16-core Intel Xeon machine with a 12000×12000 matrix size against the state-of-the-art open source and commercial numerical software packages, namely LAPACK, compiled with optimized and multithreaded BLAS from MKL as well as Intel MKL version 10.2. © 2013 ACM.",Bidiagional reduction; Bulge chasing; Data translation layer; Dynamic scheduling; High performance kernels; Tile algorithms; Two-stage approach,Bandwidth; Cache memory; Computational efficiency; Memory architecture; Open source software; Open systems; Singular value decomposition; Software architecture; Bulge chasing; Data translations; Dynamic scheduling; High performance kernels; Two stage approach; Data reduction
Level-3 Cholesky factorization routines improve performance of many Cholesky algorithms,2013,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875206154&doi=10.1145%2f2427023.2427026&partnerID=40&md5=ec83de4e61532d7bbffdbc4e2f1e014d,"Four routines called DPOTF3i, i = a,b,c,d, are presented. DPOTF3i are a novel type of level-3 BLAS for use by BPF (Blocked Packed Format) Cholesky factorization and LAPACK routine DPOTRF. Performance of routines DPOTF3i are still increasing when the performance of Level-2 routine DPOTF2 of LAPACK starts decreasing. This is our main result and it implies, due to the use of larger block size nb, that DGEMM, DSYRK, and DTRSM performance also increases! The four DPOTF3i routines use simple register blocking. Different platforms have different numbers of registers. Thus, our four routines have different register blocking sizes. BPF is introduced. LAPACK routines for POTRF and PPTRF using BPF instead of full and packed format are shown to be trivial modifications of LAPACK POTRF source codes. We call these codes BPTRF. There are two variants of BPF: lower and upper. Upper BPF is ""identical"" to Square Block Packed Format (SBPF). ""LAPACK"" implementations on multicore processors use SBPF. Lower BPF is less efficient thanupper BPF. Vector inplace transposition converts lower BPF to upper BPF very efficiently. Corroborating performance results for DPOTF3i versus DPOTF2 on a variety of common platforms are given for n ≈ nb as well as results for large n comparing DBPTRF versus DPOTRF. © 2013 ACM.",BLAS; Cache Blocking; Cholesky factorization and solution; Complex Hermitian matrices; Inplace transposition; LAPACK; Novel blocked packed matrix data structures; Positive definite matrices; Real symmetric matrices,Codes (symbols); Factorization; BLAS; Cache blocking; Cholesky factorizations; Hermitian matrices; Inplace transposition; LAPACK; Packed matrixes; Positive-definite matrices; Symmetric matrices; Matrix algebra
"Algorithm 924: Tides, a taylor series integrator for differential equations",2012,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871240666&doi=10.1145%2f2382585.2382590&partnerID=40&md5=30041ab2c65cac1a5b401eaf96d7c525,"This article introduces the software package TIDES and revisits the use of the Taylor series method for the numerical integration of ODEs. The package TIDES provides an easy-to-use interface for standard double precision integrations, but also for quadruple precision and multiple precision integrations. The motivation for the development of this package is that more and more scientific disciplines need very high precision solution of ODEs, and a standard ODE method is not able to reach these precision levels. The TIDES package combines a preprocessor step in MATHEMATICA that generates Fortran or C programs with a library in C. Another capability of TIDES is the direct solution of sensitivities of the solution of ODE systems, which means that we can compute the solution of variational equations up to any order without formulating them explicitly. Different options of the software are discussed, and finally it is compared with other well-known available methods, as well as with different options of TIDES. From the numerical tests, TIDES is competitive, both in speed and accuracy, with standard methods, but it also provides new capabilities.","Taylor series method, automatic differentiation, high precision, variational equations, numerical integration of ODEs",Integration; Ordinary differential equations; Taylor series; Tides; Automatic differentiations; C programs; Direct solution; Double precision; High precision; Mathematica; Numerical integrations; Numerical tests; Precision integration; Quadruple precision; Scientific discipline; Standard method; Variational equations; Numerical methods
Families of algorithms for reducing a matrix to condensed form,2012,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871216360&doi=10.1145%2f2382585.2382587&partnerID=40&md5=6c118a651395a3ad179e212ea7fdfc0c,"In a recent paper it was shown how memory traffic can be diminished by reformulating the classic algorithm for reducing a matrix to bidiagonal form, a preprocess when computing the singular values of a dense matrix. The key is a reordering of the computation so that the most memory-intensive operations can be fused. In this article, we show that other operations that reduce matrices to condensed form (reduction to upper Hessenberg form and reduction to tridiagonal form) can be similarly reorganized, yielding different sets of operations that can be fused. By developing the algorithms with a common framework and notation, we facilitate the comparing and contrasting of the different algorithms and opportunities for optimization on sequential architectures. We discuss the algorithms, develop a simple model to estimate the speedup potential from fusing, and showcase performance improvements consistent with the what the model predicts.",Bidiagonal; Hessenberg; High performance; Libraries; Linear algebra; Reduction; Tridiagonal,Algorithms; Libraries; Linear algebra; Reduction; Bidiagonal; Classic algorithm; Condensed form; Dense matrices; Hessenberg; Hessenberg form; High performance; Performance improvements; Preprocess; Singular values; Tridiagonal; Matrix algebra
Algorithm 925: Parallel solver for semidefinite programming problem having sparse schur complement matrix,2012,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871204896&doi=10.1145%2f2382585.2382591&partnerID=40&md5=9510f60793875811757671decd322744,"A SemiDefinite Programming (SDP) problem is one of the most central problems in mathematical optimization. SDP provides an effective computation framework for many research fields. Some applications, however, require solving a large-scale SDP whose size exceeds the capacity of a single processor both in terms of computation time and available memory. SDPARA (SemiDefinite Programming Algorithm parallel package) [Yamashita et al. 2003b] was designed to solve such large-scale SDPs. Its parallel performance is outstanding for general SDPs in most cases. However, the parallel implementation is less successful for some sparse SDPs obtained from applications such as Polynomial Optimization Problems (POPs) or Sensor Network Localization (SNL) problems, since this version of SDPARA cannot directly handle sparse Schur Complement Matrices (SCMs). In this article we improve SDPARA by focusing on the sparsity of the SCM and we propose a new parallel implementation using the formula-cost-based distribution along with a replacement of the dense Cholesky factorization. We verify numerically that these features are key to solving SDPs with sparse SCMs more quickly on parallel computing systems. The performance is further enhanced by multithreading and the new SDPARA attains considerable scalability in general. It also finds solutions for extremely large-scale SDPs arising from POPs which cannot be obtained by other solvers.",Semidefinite programming,Algorithms; Identification (control systems); Matrix algebra; Multitasking; Sensor networks; Central problems; Cholesky factorizations; Computation time; Mathematical optimizations; Multi-threading; Parallel computing system; Parallel implementations; Parallel performance; Parallel solver; Polynomial optimization problem; Research fields; Schur complement; Schur complement matrix; Semi-definite programming; Semidefinite programming problem; Sensor network localization; Single processors; Mathematical programming
Graph-based software design for managing complexity and enabling concurrency in multiphysics PDE software,2012,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871235597&doi=10.1145%2f2382585.2382586&partnerID=40&md5=7e9b06bb8490e8df1c6236857fb22f35,"Multiphysics simulation software is plagued by complexity stemming from nonlinearly coupled systems of Partial Differential Equations (PDEs). Such software typically supports many models, which may require different transport equations, constitutive laws, and equations of state. Strong coupling and a multiplicity of models leads to complex algorithms (i.e., the properly ordered sequence of steps to assemble a discretized set of coupled PDEs) and rigid software. This work presents a design strategy that shifts focus away from high-level algorithmic concerns to lowlevel data dependencies. Mathematical expressions are represented as software objects that directly expose data dependencies. The entire system of expressions forms a directed acyclic graph and the high-level assembly algorithm is generated automatically through standard graph algorithms. This approach makes problems with complex dependencies entirely tractable, and removes virtually all logic from the algorithm itself. Changes are highly localized, allowing developers to implement models without detailed understanding of any algorithms (i.e., the overall assembly process). Furthermore, this approach complements existing MPI-based frameworks and can be implemented within them easily. Finally, this approach enables algorithmic parallelization via threads. By exposing dependencies in the algorithm explicitly, thread-based parallelism is implemented through algorithm decomposition, providing a basis for exploiting parallelism independent from domain decomposition approaches.",Multiphysics; Object-oriented design; Scientific computing; Task graph,Computer software; Directed graphs; Domain decomposition methods; Natural sciences computing; Partial differential equations; Assembly algorithm; Assembly process; Complex algorithms; Constitutive law; Coupled PDEs; Data dependencies; Design strategies; Directed acyclic graph (DAG); Domain decompositions; Entire system; Graph algorithms; Graph-based; Mathematical expressions; Multi-physics; Multiphysics simulations; Nonlinearly coupled; Object-oriented design; Parallelizations; Strong coupling; Task graph; Transport equation; Algorithms
Computing the crosscap number of a knot using integer programming and normal surfaces,2012,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863913613&doi=10.1145%2f2382585.2382589&partnerID=40&md5=e4ca7d6fb32556d4a7cba4e34239be78,"The crosscap number of a knot is an invariant describing the nonorientable surface of smallest genus that the knot bounds. Unlike knot genus (its orientable counterpart), crosscap numbers are difficult to compute and no general algorithm is known. We present three methods for computing crosscap number that offer varying trade-offs between precision and speed: (i) an algorithm based on Hilbert basis enumeration and (ii) an algorithm based on exact integer programming, both of which either compute the solution precisely or reduce it to two possible values, and (iii) a fast but limited precision integer programming algorithm that bounds the solution from above. The first two algorithms advance the theoretical state-of-the-art, but remain intractable for practical use. The third algorithm is fast and effective, which we show in a practical setting by making significant improvements to the current knowledge of crosscap numbers in knot tables. Our integer programming framework is general, with the potential for further applications in computational geometry and topology.",Crosscap number; Integer programming; Knot genus; Knot theory; Normal surfaces,Computational geometry; Integer programming; Crosscap number; Hilbert basis; Knot genus; Knot theory; Non-orientable surfaces; Orientable; Programming algorithms; Programming framework; Algorithms
PyDEC: Software and algorithms for discretization of exterior calculus,2012,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868205472&doi=10.1145%2f2382585.2382588&partnerID=40&md5=faa82ea27a3eae29095d2abdc3b5353d,"This article describes the algorithms, features, and implementation of PyDEC, a Python library for computations related to the discretization of exterior calculus. PyDEC facilitates inquiry into both physical problems on manifolds as well as purely topological problems on abstract complexes. We describe efficient algorithms for constructing the operators and objects that arise in discrete exterior calculus, lowest-order finite element exterior calculus, and in related topological problems. Our algorithms are formulated in terms of high-level matrix operations which extend to arbitrary dimension. As a result, our implementations map well to the facilities of numerical libraries such as NumPy and SciPy. The availability of such libraries makes Python suitable for prototyping numerical methods. We demonstrate how PyDEC is used to solve physical and topological problems through several concise examples.",Boundary operator; Chain; Coboundary operator; Cochain; Computational topology; Cubical complex; Discrete exterior calculus; Finite element exterior calculus; Simplicial complex; Vietoris-Rips complex; Whitney form,Abstracting; Algorithms; Chains; High level languages; Software prototyping; Topology; Boundary operators; Coboundary operator; Cochain; Computational topology; Cubical complex; Discrete exterior calculus; Finite Element; Simplicial complex; Vietoris-Rips complex; Whitney; Calculations
An empirical analysis of the performance of preconditioners for SPD systems,2012,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866516596&doi=10.1145%2f2331130.2331132&partnerID=40&md5=a03bc087d5ba123e38b10a0b5856a2b8,"Preconditioned iterative solvers have the potential to solve very large sparse linear systems with a fraction of the memory used by direct methods. However, the effectiveness and performance of most preconditioners is not only problem dependent, but also fairly sensitive to the choice of their tunable parameters. As a result, a typical practitioner is faced with an overwhelming number of choices of solvers, preconditioners, and their parameters. The diversity of preconditioners makes it difficult to analyze them in a unified theoretical model. A systematic empirical evaluation of existing preconditioned iterative solvers can help in identifying the relative advantages of various implementations. We present the results of a comprehensive experimental study of the most popular preconditioner and iterative solver combinations for symmetric positive-definite systems. We introduce a methodology for a rigorous comparative evaluation of various preconditioners, including the use of some simple but powerful metrics. The detailed comparison of various preconditioner implementations and a state-of-the-art direct solver gives interesting insights into their relative strengths and weaknesses. We believe that these results would be useful to researchers developing preconditioners and iterative solvers as well as practitioners looking for appropriate sparse solvers for their applications. © 2012 ACM.",Iterative solvers; Numerical software; Preconditioners; Sparse linear systems,Computer software; Software engineering; Comparative evaluations; Direct method; Direct solvers; Empirical analysis; Empirical evaluations; Experimental studies; Iterative solvers; Large sparse linear systems; Numerical software; Preconditioners; Relative strength; Sparse linear systems; Sparse solvers; Theoretical models; Tunable parameter; Linear systems
Algorithm 920: SFSDP: A sparse version of full semidefinite programming relaxation for sensor network localization problems,2012,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866514975&doi=10.1145%2f2331130.2331135&partnerID=40&md5=238efe531a95b43f907986348cb6b672,"SFSDP is a Matlab package for solving sensor network localization (SNL) problems. These types of problems arise in monitoring and controlling applications using wireless sensor networks. SFSDP implements the semidefinite programming (SDP) relaxation proposed in Kim et al. [2009] for sensor network localization problems, as a sparse version of the full semidefinite programming relaxation (FSDP) by Biswas and Ye [2004]. To improve the efficiency of FSDP, SFSDP exploits the aggregated and correlative sparsity of a sensor network localization problem. As a result, SFSDP can handle much larger problems than other software as well as three-dimensional anchor-free problems. SFSDP analyzes the input data of a sensor network localization problem, solves the problem, and displays the computed locations of sensors. SFSDP also includes the features of generating test problems for numerical experiments. © 2012 ACM.",Matlab software package; Semidefinite programming relaxation; Sensor network localization problems; Sparsity exploitation,Computer software; Software engineering; Correlative sparsity; Input datas; MATLAB software package; Monitoring and controlling; Numerical experiments; Semi-definite programming; Semidefinite programming relaxations; Sensor network localization; Sparsity exploitation; Test problem; Mathematical programming
Automatic fréchet differentiation for the numerical solution of boundary-value problems,2012,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866519003&doi=10.1145%2f2331130.2331134&partnerID=40&md5=96d4e109d11a1ba85cfa564a54e36597,"A new solver for nonlinear boundary-value problems (BVPs) in MATLAB is presented, based on the Chebfun software system for representing functions and operators automatically as numerical objects. The solver implements Newton's method in function space, where instead of the usual Jacobian matrices, the derivatives involved are Fŕechet derivatives. A major novelty of this approach is the application of automatic differentiation (AD) techniques to compute the operator-valued Fŕechet derivatives in the continuous context. Other novelties include the use of anonymous functions and numbering of each variable to enable a recursive, delayed evaluation of derivatives with forward mode AD. The AD techniques are applied within a new Chebfun class called chebop which allows users to set up and solve nonlinear BVPs, both scalar and systems of coupled equations, in a few lines of code, using the ""nonlinear backslash"" operator (\). This framework enables one to study the behaviour of Newton's method in function space. Categories and Subject Descriptors: G.1.4 [Numerical Analysis]: Quadrature and Numerical Differentiation-Automatic differentiation; G.1.7 [Numerical Analysis]: Ordinary Differential Equations-Boundary value problems. © 2012 ACM.",Chebfun; Linearization of boundaryvalue problems; Newton's method in function space; Object-oriented matlab,Differentiation (calculus); Functional analysis; Jacobian matrices; Newton-Raphson method; Ordinary differential equations; Application of automatic differentiation; Chebfun; Coupled equation; Descriptors; Forward mode; Function spaces; Lines of code; Newton's methods; Nonlinear boundary value problems; Numerical solution; Object oriented; Software systems; Value problems; Nonlinear equations
Algorithm 921: AlphaCertified: Certifying solutions to polynomial systems,2012,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866349065&doi=10.1145%2f2331130.2331136&partnerID=40&md5=697d5ee8eed2fbad79ea009afb96b504,"Smale's a-theory uses estimates related to the convergence of Newton's method to certify that Newton iterations will converge quadratically to solutions to a square polynomial system. The program alphaCertified implements algorithms based on a-theory to certify solutions of polynomial systems using both exact rational arithmetic and arbitrary precision floating point arithmetic. It also implements algorithms that certify whether a given point corresponds to a real solution, and algorithms to heuristically validate solutions to overdetermined systems. Examples are presented to demonstrate the algorithms. © 2012 ACM.",Alpha theory; Certified solutions; Numerical algebraic geometry; Polynomial system,Algorithms; Newton-Raphson method; Alpha theory; Arbitrary precision; Newton iterations; Newton's methods; Numerical algebraic geometry; Overdetermined systems; Polynomial systems; Real solutions; Polynomials
Object-oriented techniques for sparse matrix computations in fortran 2003,2012,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866512162&doi=10.1145%2f2331130.2331131&partnerID=40&md5=f47958e0fd7d7ff68bb3754e8db3a865,"The efficiency of a sparse linear algebra operation heavily relies on the ability of the sparse matrix storage format to exploit the computing power of the underlying hardware. Since no format is universally better than the others across all possible kinds of operations and computers, sparse linear algebra software packages should provide facilities to easily implement and integrate new storage formats within a sparse linear algebra application without the need to modify it; it should also allow to dynamically change a storage format at run-time depending on the specific operations to be performed. Aiming at these important features, we present an Object Oriented design model for a sparse linear algebra package which relies on Design Patterns. We show that an implementation of our model can be efficiently achieved through some of the unique features of the Fortran 2003 language. Experimental results show that the proposed software infrastructure improves the modularity and ease of use of the code at no performance loss. © 2012 ACM.",Mathematics of computing; Object-oriented design; Sparse matrices,FORTRAN (programming language); Computing power; Design Patterns; Ease-of-use; Linear algebra operations; Linear algebra package; Object oriented technique; Object-oriented design; Object-oriented design models; Performance loss; Runtimes; Software infrastructure; Sparse matrices; Sparse matrix computations; Storage formats; Unique features; Matrix algebra
A runtime system for programming out-of-core maatrix algorithms-by-tiles on multithreaded architectures,2012,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866480395&doi=10.1145%2f2331130.2331133&partnerID=40&md5=8aa2c29c0ca2fb5b27ca0b83fd63c845,"Out-of-core implementations of algorithms for dense matrix computations have traditionally focused on optimal use of memory so as to minimize I/O, often trading programmability for performance. In this article we show how the current state of hardware and software allows the programmability problem to be addressed without sacrificing performance. This comes from the realizations that memory is cheap and large, making it less necessary to optimally orchestrate I/O, and that new algorithms view matrices as collections of submatrices and computation as operations with those submatrices. This enables libraries to be coded at a high level of abstraction, leaving the tasks of scheduling the computations and data movement in the hands of a runtime system. This is in sharp contrast to more traditional approaches that leverage optimal use of in-core memory and, at the expense of introducing considerable programming complexity, explicit overlap of I/O with computation. Performance is demonstrated for this approach on multicore architectures as well as platforms equipped with hardware accelerators. © 2012 ACM.",High-performance; Libraries; Linear algebra; Multithreaded architectures; Out-of-core algorithms,Algorithms; Hardware; Libraries; Linear algebra; Optimization; Software architecture; Data movements; Dense matrices; Hardware accelerators; Hardware and software; High level of abstraction; High-performance; Multicore architectures; Multithreaded architecture; Out-of-core; Out-of-core algorithms; Programmability; Programming complexity; Runtime systems; Sharp contrast; Sub-matrices; Matrix algebra
Algorithm 922: A mixed finite element method for helmholtz transmission eigenvalues,2012,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863163473&doi=10.1145%2f2331130.2331137&partnerID=40&md5=b66ff485a11f7245f9a183ff6da61286,"Transmission eigenvalue problem has important applications in inverse scattering. Since the problem is non-self-adjoint, the computation of transmission eigenvalues needs special treatment. Based on a fourthorder reformulation of the transmission eigenvalue problem, a mixed finite element method is applied. The method has two major advantages: 1) the formulation leads to a generalized eigenvalue problem naturally without the need to invert a related linear system, and 2) the nonphysical zero transmission eigenvalue, which has an infinitely dimensional eigenspace, is eliminated. To solve the resulting non-Hermitian eigenvalue problem, an iterative algorithm using restarted Arnoldi method is proposed. To make the computation efficient, the search interval is decided using a Faber-Krahn type inequality for transmission eignevalues and the interval is updated at each iteration. The algorithm is implemented using Matlab. The code can be easily used in the qualitative methods in inverse scattering and modified to compute transmission eigenvalues for other models such as elasticity problem. © 2012 ACM.",Arnoldi method; Mixed finite element; Transmission eigenvalues,Algorithms; Elasticity; Finite element method; Linear systems; Arnoldi method; Eigen-value; Eigenspaces; Eigenvalue problem; Eigenvalues; Elasticity problems; Fourth-order; Generalized eigenvalue problems; Helmholtz; Inverse scattering; Iterative algorithm; Mixed finite element methods; Mixed finite elements; Non-self-adjoint; Qualitative method; Special treatments; Eigenvalues and eigenfunctions
Algorithm 923: Efficient numerical computation of the pfaffian for dense and banded skew-symmetric matrices,2012,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866506633&doi=10.1145%2f2331130.2331138&partnerID=40&md5=dec775f29472597dc82f9933c9dfb701,"Computing the Pfaffian of a skew-symmetric matrix is a problem that arises in various fields of physics. Both computing the Pfaffian and a related problem, computing the canonical form of a skew-symmetric matrix under unitary congruence, can be solved easily once the skew-symmetric matrix has been reduced to skew-symmetric tridiagonal form. We develop efficient numerical methods for computing this tridiagonal form based on Gaussian elimination, using a skew-symmetric, blocked form of the Parlett-Reid algorithm, or based on unitary transformations, using block Householder transformations and Givens rotations, that are applicable to dense and banded matrices, respectively. We also give a complete and fully optimized implementation of these algorithms in Fortran (including a C interface), and also provide Python, Matlab and Mathematica implementations for convenience. Finally, we apply these methods to compute the topological charge of a class D nanowire, and show numerically the equivalence of definitions based on the Hamiltonian and the scattering matrix. © 2012 ACM.",Canonical form; Pfaffian; Skew-symmetric matrix; Topological charge; Unitary congruence,Algorithms; Equivalence classes; Linear transformations; MATLAB; Matrix algebra; Nanowires; Scattering parameters; Topology; Canonical form; Pfaffian; Skew-symmetric matrices; Topological charges; Unitary congruence; Computational efficiency
Algorithm 917: Complex double-precision evaluation of the wright ω function,2012,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862082488&doi=10.1145%2f2168773.2168779&partnerID=40&md5=751d5f3866353425fc7c6ee3e03741b6,This article describes an efficient and robust algorithm and implementation for the evaluation of the Wright ω function in IEEE double precision arithmetic over the complex plane. © 2012 ACM.,Double-precision evaluation over ℂ; Wright omega function,Algorithms; Complex planes; Double precision; Robust algorithm; Function evaluation
Estimating derivatives of noisy simulations,2012,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862090077&doi=10.1145%2f2168773.2168777&partnerID=40&md5=a3476b3fdaeec1f613a20c0f715bb53a,We employ recent work on computational noise to obtain near-optimal difference estimates of the derivative of a noisy function. Our analysis relies on a stochastic model of the noise without assuming a specific form of distribution. We use this model to derive theoretical bounds for the errors in the difference estimates and obtain an easily computable difference parameter that is provably near-optimal. Numerical results closely resemble the theory and show that we obtain accurate derivative estimates even when the noisy function is deterministic. © 2012 ACM.,Computational noise,Optimization; Computational noise; Difference parameters; Numerical results; Theoretical bounds; Estimation
Algorithm 918: Specdicho: A MATLAB program for the spectral dichotomy of regular matrix pencils,2012,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862104432&doi=10.1145%2f2168773.2168780&partnerID=40&md5=99b1e3702a7eefe93910c49767854ced,"Given a regular matrix pencil λB-A and a positively oriented contour γ in the complex plane, the spectral dichotomy methods applied to λB-A and γ consist in determining whether λB.A possesses eigenvalues on or in a neighborhood of γ-When no such eigenvalues exist, these methods compute iteratively the spectral projector P onto the right deflating subspace of λB-A associated with the eigenvalues inside/outside γ . The computation of the projector is accompanied by the spectral norm ∥H∥ of a Hermitian positive definite matrix H called the dichotomy condition number, which indicates the numerical quality of the spectral projector P. The smaller ∥H∥ is, the better this quality. This article presents a MATLAB program (specdicho) implementing the main types of spectral dichotomy where γ is a circle, an ellipse, the imaginary axis or a parabola. © 2012 ACM.",Invariant subspace; Regular matrix pencil; Spectral dichotomy; Spectral projector,Eigenvalues and eigenfunctions; Number theory; Complex planes; Condition numbers; Deflating subspace; Eigenvalues; Hermitians; Imaginary axis; Invariant subspace; MATLAB program; Matrix pencil; Numerical quality; Positive-definite matrices; Spectral dichotomy; Spectral norms; Spectral projector; MATLAB
Parallel and cache-efficient in-place matrix storage format conversion,2012,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862107202&doi=10.1145%2f2168773.2168775&partnerID=40&md5=bae6f5e0bb22072a6edf409547dd069f,Techniques and algorithms for efficient in-place conversion to and from standard and blocked matrix storage formats are described. Such functionality is required by numerical libraries that use different data layouts internally. Parallel algorithms and a software package for in-place matrix storage format conversion based on in-place matrix transposition are presented and evaluated. A new algorithm for in-place transposition which efficiently determines the structure of the transposition permutation a priori is one of the key ingredients. It enables effective load balancing in a parallel environment. © 2012 ACM.,Blocked matrix data layout; In-place matrix transposition; Parallel and cache-efficient algorithms,Algorithms; Parallel architectures; Cache-efficient; Data layouts; Matrix transposition; Numerical library; Parallel environment; Storage formats; Matrix algebra
Interactive initialization and continuation of homoclinic and heteroclinic orbits in MATLAB,2012,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862073931&doi=10.1145%2f2168773.2168776&partnerID=40&md5=9962b1107540d4a3f6b06f5ae5b144ba,"MATCONT is a MATLAB continuation package for the interactive numerical study of a range of parameterized nonlinear dynamical systems, in particular ODEs, that allows to compute curves of equilibria, limit points, Hopf points, limit cycles, flip, fold and torus bifurcation points of limit cycles. It is now possible to continue homoclinic-to-hyperbolic-saddle and homoclinic-to-saddle- node orbits in MATCONT. The implementation is done using the continuation of invariant subspaces, with the Riccati equations included in the defining system. A key feature is the possibility to initiate both types of homoclinic orbits interactively, starting from an equilibrium point and using a homotopy method. All known codimension-two homoclinic bifurcations are tested for during continuation. The test functions for inclination-flip bifurcations are implemented in a new and more efficient way. Heteroclinic orbits can now also be continued and an analogous homotopy method can be used for the initialization. © 2012 ACM.",Global bifurcation; Homotopy; Numerical continuation,Hopf bifurcation; Nonlinear dynamical systems; Riccati equations; Codimension-two; Continuation of invariant subspaces; Equilibrium point; Global bifurcations; Heteroclinic orbit; Homoclinic; Homoclinic bifurcations; Homoclinic orbits; Homotopies; Homotopy method; Hopf point; Key feature; Limit cycle; Limit points; Numerical continuation; Numerical studies; Parameterized; Test functions; Torus bifurcation; Orbits
Algorithm 919: A Krylov subspace algorithm for evaluating the ψ-functions appearing in exponential integrators,2012,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857919533&doi=10.1145%2f2168773.2168781&partnerID=40&md5=580e449b8db3c1fb7941d7b9d703d816,"We develop an algorithm for computing the solution of a large system of linear ordinary differential equations (ODEs) with polynomial inhomogeneity. This is equivalent to computing the action of a certain matrix function on the vector representing the initial condition. The matrix function is a linear combination of the matrix exponential and other functions related to the exponential (the so-called ψ-functions). Such computations are the major computational burden in the implementation of exponential integrators, which can solve general ODEs. Our approach is to compute the action of the matrix function by constructing a Krylov subspace using Arnoldi or Lanczos iteration and projecting the function on this subspace. This is combined with time-stepping to prevent the Krylov subspace from growing too large. The algorithm is fully adaptive: it varies both the size of the time steps and the dimension of the Krylov subspace to reach the required accuracy. We implement this algorithm in the MATLAB function phipm and we give instructions on how to obtain and use this function. Various numerical experiments show that the phipm function is often significantly more efficient than the state-of-the-art. © 2012 ACM.",Exponential integrators; Krylov subspace methods; Matrix exponential,Adaptive algorithms; MATLAB; Ordinary differential equations; Polynomials; Arnoldi; Computational burden; Exponential integrators; Inhomogeneities; Initial conditions; Krylov subspace; Krylov subspace method; Lanczos; Large system; Linear combinations; Linear ordinary differential equations; Matlab functions; Matrix exponentials; Matrix functions; Numerical experiments; Time step; Time-stepping; Matrix algebra
Using multicomplex variables for automatic computation of high-order derivatives,2012,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862064904&doi=10.1145%2f2168773.2168774&partnerID=40&md5=6a2f36cb346cae6a6021630329e2becb,"The computations of the high-order partial derivatives in a given problem are often cumbersome or not accurate. To combat such shortcomings, a new method for calculating exact high-order sensitivities using multicomplex numbers is presented. Inspired by the recent complex step method that is only valid for firstorder sensitivities, the new multicomplex approach is valid to arbitrary order. The mathematical theory behind this approach is revealed, and an efficient procedure for the automatic implementation of the method is described. Several applications are presented to validate and demonstrate the accuracy and efficiency of the algorithm. The results are compared to conventional approaches such as finite differencing, the complex step method, and two separate automatic differentiation tools. Themulticomplexmethod performs favorably in the preliminary comparisons and is therefore expected to be useful for a variety of algorithms that exploit higher order derivatives. © 2012 ACM.",Automatic differentiation; High-order derivatives; Multicomplex numbers; Multicomplex step differentiation; Overloading; Sensitivity analysis,Computer software; Sensitivity analysis; Software engineering; Arbitrary order; Automatic differentiation tool; Automatic differentiations; Automatic implementation; Complex-step method; Conventional approach; Finite differencing; First-order sensitivity; High-order; High-order derivative; Higher order derivatives; Mathematical theory; Multicomplex numbers; Overloading; Partial derivatives; Algorithms
"Design, implementation, and analysis of maximum transversal algorithms",2011,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84856005674&doi=10.1145%2f2049673.2049677&partnerID=40&md5=599d049711249dc31c2ff4b2c079d4e9,"We report on careful implementations of seven algorithms for solving the problem of finding a maximum transversal of a sparse matrix. We analyze the algorithms and discuss the design choices. To the best of our knowledge, this is the most comprehensive comparison of maximum transversal algorithms based on augmenting paths. Previous papers with the same objective either do not have all the algorithms discussed in this article or they used nonuniform implementations from different researchers. We use a common base to implement all of the algorithms and compare their relative performance on a wide range of graphs and matrices. We systematize, develop, and use several ideas for enhancing performance. One of these ideas improves the performance of one of the existing algorithms in most cases, sometimes significantly. So much so that we use this as the eighth algorithm in comparisons. © 2011 ACM 0098-3500/2011/12-ART10 $10.00.",Assignment; Bipartite graphs; Breadth first search; Depth first search; Graph theory; Matching; Matrix transversals,Graph theory; Assignment; Bipartite graphs; Breadth-first search; Depth first search; Matching; matrix; Algorithms
A note on shifted hessenberg systems and frequency response computation,2011,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84856015109&doi=10.1145%2f2049673.2049676&partnerID=40&md5=8049141fd278144a648747e06ec08d16,"In this article, we propose a numerical algorithm for efficient and robust solution of a sequence of shifted Hessenberg linear systems. In particular, we show how the frequency response G(σ)= d-C(A- σI)-1b in the single input case can be computed more efficiently than with other state-of-the-art methods. We also provide a backward stability analysis of the proposed algorithm. © 2011 ACM 0098-3500/2011/12-ART10 $10.00.",Computer-aided control system design; Control theory; Controller Hessenberg form; Frequency response; Shifted linear systems,Algorithms; Computational efficiency; Computer control systems; Control theory; Linear systems; Systems analysis; Hessenberg systems; Numerical algorithms; Response computation; Robust solutions; Single input; Stability analysis; State-of-the-art methods; Frequency response
Partial factorization of a dense symmetric indefinite matrix,2011,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855991256&doi=10.1145%2f2049673.2049674&partnerID=40&md5=38826767e1c18d61573b37fd5fa0e4a4,"At the heart of a frontal or multifrontal solver for the solution of sparse symmetric sets of linear equations, there is the need to partially factorize dense matrices (the frontal matrices) and to be able to use their factorizations in subsequent forward and backward substitutions. For a large problem, packing (holding only the lower or upper triangular part) is important to save memory. It has long been recognized that blocking is the key to efficiency and this has become particularly relevant on modern hardware. For stability in the indefinite case, the use of interchanges and 2×2 pivots as well as 1×1 pivots is equally well established. In this article, the challenge of using these three ideas (packing, blocking, and pivoting) together is addressed to achieve stable factorizations of large real-world symmetric indefinite problems with good execution speed. The ideas are not restricted to frontal and multifrontal solvers and are applicable whenever partial or complete factorizations of dense symmetric indefinite matrices are needed. © 2011 ACM 0098-3500/2011/12- ART10 $10.00.",Frontal; Indefinite matrices; LDLT factorization; Multifrontal; Symmetric linear systems,Factorization; Linear systems; Dense matrices; Execution speed; Frontal; Indefinite matrix; Multifrontal; Matrix algebra
Algorithm 916: Computing the faddeyeva and voigt functions,2011,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84856014986&doi=10.1145%2f2049673.2049679&partnerID=40&md5=09d9dcdfac9a1e44373fbfe16d1b59fd,"We present a MATLAB function for the numerical evaluation of the Faddeyeva function w(z). The function is based on a newly developed accurate algorithm. In addition to its higher accuracy, the software provides a flexible accuracy vs efficiency trade-off through a controlling parameter that may be used to reduce accuracy and computational time and vice versa. Verification of the flexibility, reliability, and superior accuracy of the algorithm is provided through comparison with standard algorithms available in other libraries and software packages.© 2011 ACM 0098-3500/2011/12-ART10 $10.00.",Accuracy; Faddeyeva function; Function evaluation; Matlab,Algorithms; Computer simulation; MATLAB; Software reliability; Verification; Accuracy; Computational time; Controlling parameters; Matlab functions; Numerical evaluations; Standard algorithms; Voigt functions; Function evaluation
Exploiting parallelism in matrix-computation kernels for symmetric multiprocessor systems: Matrix-multiplication and matrix-addition algorithm optimizations by software pipelining and threads allocation,2011,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855210804&doi=10.1145%2f2049662.2049664&partnerID=40&md5=652e1849c90bbc94c9a301cc4027bce0,"We present a simple and efficient methodology for the development, tuning, and installation of matrix algorithms such as the hybrid Strassen's and Winograd's fast matrix multiply or their combination with the 3M algorithm for complex matrices (i.e., hybrid: a recursive algorithm as Strassen's until a highly tuned BLAS matrix multiplication allows performance advantages). We investigate how modern Symmetric Multiprocessor (SMP) architectures present old and new challenges that can be addressed by the combination of an algorithm design with careful and natural parallelism exploitation at the function level (optimizations) such as function-call parallelism, function percolation, and function software pipelining. We have three contributions: first, we present a performance overview for double- and double-complexprecision matrices for state-of-the-art SMP systems; second, we introduce new algorithm implementations: a variant of the 3M algorithm and two new different schedules of Winograd's matrix multiplication (achieving up to 20% speedup with respect to regular matrix multiplication). About the latter Winograd's algorithms: one is designed to minimize the number of matrix additions and the other to minimize the computation latency of matrix additions; third, we apply software pipelining and threads allocation to all the algorithms and we show how this yields up to 10% further performance improvements. © 2011 ACM 0098-3500/2011/11-ART2.",Fast algorithms; Matrix multiplications; Parallelism; Software pipeline,Algorithms; Computational efficiency; Multiprocessing systems; Optimization; Pipe; Solvents; Algorithm design; Algorithm implementation; Algorithm optimization; Complex matrices; Fast algorithms; matrix; Matrix algorithms; MAtrix multiplication; Natural parallelism; Parallelism; Performance improvements; Recursive algorithms; Software pipeline; Software pipelining; Symmetric multi-processors; Symmetric multiprocessor systems; Winograd; Winograd's algorithms; Matrix algebra
High-performance up-and-downdating via householder-like transformations,2011,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855253220&doi=10.1145%2f2049662.2049666&partnerID=40&md5=724eb4fd40e8bdd1545a5bb1a43166a2,"Dedicated to Paul Van Dooren on the occasion of his 60th birthday We present high-performance algorithms for up-and-downdating a Cholesky factor or QR factorization. The method uses Householder-like transformations, sometimes called hyperbolic Householder transformations, that are accumulated so that most computation can be cast in terms of high-performance matrix-matrix operations. The resulting algorithms can then be used as building blocks for an algorithm-by-blocks that allows computation to be conveniently scheduled to multithreaded architectures like multicore processors. Performance is shown to be similar to that achieved by a blocked QR factorization via Householder transformations. © 2011 ACM 0098-3500/2011/11-ART4.",High performance; Libraries; Linear algebra,Algorithms; Computer architecture; Factorization; Libraries; Linear algebra; Matrix algebra; Building blockes; Cholesky factor; High performance; Householder transformation; Multi-core processor; Multithreaded architecture; QR factorizations; Linear transformations
"Erratum: Algorithm 902: GPOPS, A MATLAB software for solving multiple-phase optimal control problems using the gauss pseudospectral method (ACM Transactions on Mathematical Software)",2011,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855215949&doi=10.1145%2f2049662.2049671&partnerID=40&md5=af796b265f7462252edc0e87a280709b,[No abstract available],,
"Algorithm 914: Parabolic cylinder function W(a,x) and its derivative",2011,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855209028&doi=10.1145%2f2049662.2049668&partnerID=40&md5=0c40731c64eefa615009602adc8e4628,"A Fortran 90 program for the computation of the real parabolic cylinder functions W(a,±x), x ≥ 0 and their derivatives is presented. The code also computes scaled functions for a > 50. The functions W(a,±x) are a numerically satisfactory pair of solutions of the parabolic cylinder equation y'' + (x 2/4 - a)y = 0, x > 0. Using Wronskian tests, we claim a relative accuracy better than 5 10 -13 in the computable range of unscaled functions, while for scaled functions the aimed relative accuracy is better than 5 10 -14. This code, together with the algorithm and related software described in Gil et al. [2006a, 2006b], completes the set of software for Parabolic Cylinder Functions (PCFs) for real arguments. © 2011 ACM 0098-3500/2011/11-ART6.",Asymptotic expansions; ODE integration; Parabolic cylinder functions,Algorithms; Asymptotic analysis; Bessel functions; Codes (symbols); Asymptotic expansion; Fortran 90; ODE integration; Parabolic cylinder; Relative accuracy; Cylinders (shapes)
"Remark on ""algorithm 778: L-BFGS-B: Fortran subroutines for large-scale bound constrained optimization""",2011,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855220977&doi=10.1145%2f2049662.2049669&partnerID=40&md5=ed5b3bc9cb918f0f4519f60f54506015,This remark describes an improvement and a correction to Algorithm 778. It is shown that the performance of the algorithm can be improved significantly by making a relatively simple modification to the subspace minimization phase. The correction concerns an error caused by the use of routine dpmeps to estimate machine precision. © 2011 ACM 0098-3500/2011/11-ART7.,Constrained optimization; Infeasibility; Nonlinear programming,Algorithms; Nonlinear programming; Bound constrained optimization; Fortran subroutine; Infeasibility; Machine precision; Minimization phase; Simple modifications; Constrained optimization
"Algorithm 915, SuiteSparseQR: Multifrontal multithreaded rank-revealing sparse QR factorization",2011,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855243155&doi=10.1145%2f2049662.2049670&partnerID=40&md5=b78fc8a2047129a986ccbcdef14c67b4,"SuiteSparseQR is a sparse QR factorization package based on the multifrontal method. Within each frontal matrix, LAPACK and the multithreaded BLAS enable the method to obtain high performance on multicore architectures. Parallelism across different frontalmatrices is handled with Intel's Threading Building Blocks library. The symbolic analysis and ordering phase pre-eliminates singletons by permuting the input matrix Ainto the form [R 11 R 12; 0 A 22] where R 11 is upper triangular with diagonal entries above a given tolerance. Next, the fill-reducing ordering, column elimination tree, and frontal matrix structures are found without requiring the formation of the pattern of A T A. Approximate rank-detection is performed within each frontal matrix usingHeath's method.While Heath'smethod is not always exact, it has the advantage of not requiring column pivoting and thus does not interfere with the fill-reducing ordering. For sufficiently large problems, the resulting sparse QR factorization obtains a substantial fraction of the theoretical peak performance of a multicore computer. © 2011 ACM 0098-3500/2011/11-ART8.",Least-square problems; QR factorization; Sparse matrices,Factorization; Least squares approximations; Software architecture; Elimination tree; Frontal matrix; Input matrices; Least square problems; Multi core; Multicore architectures; Multifrontal methods; Multithreaded; Ordering phase; Peak performance; QR factorizations; Rank-revealing; S-method; Sparse matrices; Symbolic analysis; Threading building blocks; Matrix algebra
Algorithm 913: An elegant IDR(s) variant that efficiently exploits biorthogonality properties,2011,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855225401&doi=10.1145%2f2049662.2049667&partnerID=40&md5=d6270d362054d0a0c37c98bf87fa820f,"The IDR(s) method that is proposed in Sonneveld and van Gijzen [2008] is a very efficient limited memory method for solving large nonsymmetric systems of linear equations. IDR(s) is based on the induced dimension reduction theorem, that provides a way to construct subsequent residuals that lie in a sequence of shrinking subspaces. The IDR(s) algorithm that is given in Sonneveld and van Gijzen [2008] is a direct translation of the theorem into an algorithm. This translation is not unique. This article derives a new IDR(s) variant, that imposes (one-sided) biorthogonalization conditions on the iteration vectors. The resulting method has lower overhead in vector operations than the original IDR(s) algorithms. In exact arithmetic, both algorithms give the same residual at every (s + 1)-st step, but the intermediate residuals and also the numerical properties differ. We show through numerical experiments that the new variant is more stable and more accurate than the original IDR(s) algorithm, and that it outperforms other state-of-the-art techniques for realistic test problems. © 2011 ACM 0098-3500/2011/11-ART5.",IDR; IDR(s); Iterative methods; Krylov-subspace methods; Sparse nonsymmetric linear systems,Iterative methods; Linear systems; Vectors; Biorthogonality; Dimension reduction; Exact arithmetic; IDR; IDR(s); Iteration vectors; Krylov-subspace methods; Limited-memory methods; Non-symmetric system; Nonsymmetric linear systems; Numerical experiments; Numerical properties; Show through; Test problem; Vector operations; Algorithms
Algorithms and data structures for massively parallel generic adaptive finite element codes,2011,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053101388&doi=10.1145%2f2049673.2049678&partnerID=40&md5=87cc561bce9df9299fd954832fb3f92f,"Today's largest supercomputers have 100,000s of processor cores and offer the potential to solve partial differential equations discretized by billions of unknowns. However, the complexity of scaling to such large machines and problem sizes has so far prevented the emergence of generic software libraries that support such computations, although these would lower the threshold of entry and enable many more applications to benefit from large-scale computing. We are concerned with providing this functionality for mesh-adaptive finite element computations. We assume the existence of an ""oracle"" that implements the generation and modification of an adaptive mesh distributed across many processors, and that responds to queries about its structure. Based on querying the oracle, we develop scalable algorithms and data structures for generic finite element methods. Specifically, we consider the parallel distribution of mesh data, global enumeration of degrees of freedom, constraints, and postprocessing. Our algorithms remove the bottlenecks that typically limit large-scale adaptive finite element analyses. We demonstrate scalability of complete finite element workflows on up to 16,384 processors. An implementation of the proposed algorithms, based on the open source software p4est as mesh oracle, is provided under an open source license through the widely used deal.II finite element software library. © 2011 ACM 0098-3500/2011/12-ART10 $10.00.",Adaptive mesh refinement; Objectorientation; Parallel algorithms; Software design,Algorithms; Computer software; Data structures; Open systems; Parallel algorithms; Parallel processing systems; Partial differential equations; Software design; Supercomputers; Adaptive finite element; Adaptive finite element analysis; Adaptive mesh refinement; Adaptive meshes; Algorithms and data structures; Finite Element; Finite element computations; Finite element software; Generic software library; Large machines; Large-scale computing; Mesh data; Object-orientation; Open Source Software; Open sources; Parallel distribution; Problem size; Processor cores; Scalable algorithms; Work-flows; Finite element method
Computing the volume of a union of balls: A certified algorithm,2011,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80052621389&doi=10.1145%2f2049662.2049665&partnerID=40&md5=0900a5378a751addf8aae40dce962ede,"Balls and spheres are amongst the simplest 3D modeling primitives, and computing the volume of a union of balls is an elementary problem. Although a number of strategies addressing this problem have been investigated in several communities, we are not aware of any robust algorithm, and present the first such algorithm. Our calculation relies on the decomposition of the volume of the union into convex regions, namely the restrictions of the balls to their regions in the power diagram. Theoretically, we establish a formula for the volume of a restriction, based on Gauss' divergence theorem. The proof being constructive, we develop the associated algorithm. On the implementation side, we carefully analyse the predicates and constructions involved in the volume calculation, and present a certified implementation relying on interval arithmetic. The result is certified in the sense that the exact volume belongs to the interval computed. Experimental results are presented on hand-crafted models illustrating various difficulties, as well as on the 58,898 models found in the tenth of July 2009 release of the Protein Data Bank. © 2011 ACM 0098-3500/2011/11-ART3.",α-shapes; C++ design; Certified numerics; Computational geometry; Interval arithmetic; Macro-molecular models; Medial axis transform; Protein modeling; Structural biology; Union of balls; Van der Waals models; Volume calculation,Algorithms; Computational geometry; Spheres; Three dimensional; Van der Waals forces; C++ design; Certified numerics; Interval arithmetic; Medial axis transforms; Protein modeling; Structural biology; Union of balls; Van der waals; Volume calculation; Three dimensional computer graphics
The University of Florida Sparse Matrix Collection,2011,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-81355161778&doi=10.1145%2f2049662.2049663&partnerID=40&md5=18550e4370a4832c19afbc0b76671205,"We describe the University of Florida Sparse Matrix Collection, a large and actively growing set of sparse matrices that arise in real applications. The Collection is widely used by the numerical linear algebra community for the development and performance evaluation of sparse matrix algorithms. It allows for robust and repeatable experiments: robust because performance results with artificially generated matrices can be misleading, and repeatable because matrices are curated and made publicly available in many formats. Its matrices cover a wide spectrum of domains, include those arising from problems with underlying 2D or 3D geometry (as structural engineering, computational fluid dynamics, model reduction, electromagnetics, semiconductor devices, thermodynamics, materials, acoustics, computer graphics/vision, robotics/kinematics, and other discretizations) and those that typically do not have such geometry (optimization, circuit simulation, economic and financial modeling, theoretical and quantum chemistry, chemical process simulation, mathematics and statistics, power networks, and other networks and graphs). We provide software for accessing andmanaging the Collection, from MATLAB™, Mathematica™, Fortran, and C, as well as an online search capability. Graph visualization of the matrices is provided, and a new multilevel coarsening scheme is proposed to facilitate this task. © 2011 ACM 0098-3500/2011/11-ART1.",Graph drawing; Multilevel algorithms; Performance evaluation; Sparse Matrices,Algorithms; Circuit simulation; Computational chemistry; Computational electromagnetics; Computational fluid dynamics; Computational geometry; Drawing (graphics); MATLAB; Quantum chemistry; Semiconductor device models; Semiconductor devices; Thermodynamics; Three dimensional; Visualization; 3D geometry; Chemical process simulation; Discretizations; Electromagnetics; Financial modeling; Graph drawing; Graph visualization; Mathematica; Model reduction; Multilevel algorithm; Networks and graphs; Numerical Linear Algebra; Online search; Performance evaluation; Power networks; Real applications; Sparse matrices; University of Florida; Wide spectrum; Matrix algebra
Partitioned triangular tridiagonalization,2011,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79952496435&doi=10.1145%2f1916461.1916462&partnerID=40&md5=48d35662b6958762f7237d1a06ee5a0c,"We present a partitioned algorithm for reducing a symmetric matrix to a tridiagonal form, with partial pivoting. That is, the algorithm computes a factorization PAPT = LTLT, where, P is a permutation matrix, L is lower triangular with a unit diagonal and entries' magnitudes bounded by 1, and T is symmetric and tridiagonal. The algorithm is based on the basic (nonpartitioned) methods of Parlett and Reid and of Aasen. We show that our factorization algorithm is componentwise backward stable (provided that the growth factor is not too large), with a similar behavior to that of Aasen's basic algorithm. Our implementation also computes the QRfactorization of T and solves linear systems of equations using the computed factorization. The partitioning allows our algorithm to exploit modern computer architectures (in particular, cache memories and high-performance BLAS libraries). Experimental results demonstrate that our algorithms achieve approximately the same level of performance as the partitioned Bunch-Kaufman factor and solve routines in LAPACK. © 2011 ACM.",Aasen's tridagonalization; Parlett-Reid tridagonalization; Partitioned factorizations; Recursive factorizations; Symmetric indefinite matrices; Tridiagonalization,Algorithms; Cache memory; Computer architecture; Linear systems; Matrix algebra; Aasen's tridagonalization; Parlett-Reid tridagonalization; Partitioned factorizations; Recursive factorizations; Symmetric indefinite matrices; Tridiagonalization; Factorization
Algorithm 912: A module for calculating cylindrical functions of complex order and complex argument,2011,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79952499026&doi=10.1145%2f1916461.1916471&partnerID=40&md5=5c6d2ee01c6e799a254b3f2d84fbdc97,"The present algorithm provides a module for calculating the cylindrical functions Jv(z), Yv(z), H(1)v(z), andH(2)v(z), where the order v is complex and the complex argument z satisfies -π < arg z ≤ π. The algorithmis written in Fortran 90 and calculates the functions using real and complex numbers of any intrinsic data type whose kind type parameter the user's Fortran system accepts. The methods of calculating the functions are based on two kinds of series expansions and numerical integration. Wronskian tests examine the functional values computed by this algorithm with double precision at 4,100,625 pseudorandom test points in the region |Rev| ≤ 60, |Imv| ≤ 60, |Rez| ≤ 300, |Imz| ≤ 300. From the results of the tests, we find that the errors of two kinds of Wronskians are less than 6.42 × 10-14. © 2011 ACM.",Complex argument; Complex order; Cylindrical functions; Numerical calculation,Algorithms; Cylinders (shapes); FORTRAN (programming language); Complex argument; Complex number; Complex order; Cylindrical function; Cylindrical functions; Double precision; Fortran; Fortran 90; Intrinsic data; Numerical calculation; Numerical integrations; Pseudo-random tests; Series expansion; Numerical methods
Algorithm 910: A portable C++ multiple-precision system for special-function calculations,2011,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79952501321&doi=10.1145%2f1916461.1916469&partnerID=40&md5=4327a530eaa853c179b197adf40245dc,"This article presents a portable C++ system for multiple precision calculations of special functions called e-float. It has an extendable architecture with a uniform C++ layer which can be used with any suitably prepared MP type. The system implements many high-precision special functions and extends some of these to very large parameter ranges. It supports calculations with 30 ⋯ 300 decimal digits of precision. Interoperabilities with Microsoft's® CLR, Python, and Mathematica® are supported. The e-float system and its usage are described in detail. Implementation notes, testing results, and performance measurements are provided. © 2011 ACM.",Airy; Bessel; C++; Elliptic integrals; Gamma; Generic programming; Hermite; Hypergeometric; Laguerre; Legendre; Multiple precision; Object oriented; Orthogonal polynomials; Parabolic cylinder; Polylogarithm; Polymorphism; Special functions; Zeta,Cylinders (shapes); Object oriented programming; Polynomials; Airy; Bessel; C++; Elliptic integrals; Gamma; Generic programming; Hermite; Hypergeometric; Laguerre; Legendre; Multiple precision; Object oriented; Orthogonal polynomial; Parabolic cylinder; Polylogarithms; Special functions; Zeta; Orthogonal functions
Algorithm 911: Multiple-precision exponential integral and related functions,2011,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79952509436&doi=10.1145%2f1916461.1916470&partnerID=40&md5=7dc094e62d9463512db3209f984b3837,"This article describes a collection of Fortran-95 routines for evaluating the exponential integral function, error function, sine and cosine integrals, Fresnel integrals, Bessel functions, and related mathematical special functions using the FM multiple-precision arithmetic package. © 2011 ACM.",Accuracy; Exponential integral function; Floating point; Fortran; Function evaluation; Mathematical library; Multiple precision; Portable software,Digital arithmetic; Exponential functions; FORTRAN (programming language); Harmonic analysis; Integral equations; Portable equipment; Accuracy; Exponential integral function; Floating points; Fortran; Mathematical library; Multiple precision; Portable software; Function evaluation
An optimal iterative solver for symmetric indefinite systems stemming from mixed approximation,2011,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79952529992&doi=10.1145%2f1916461.1916466&partnerID=40&md5=1bd2ff0cc112aab278d6e9cbf00d7ed7,"We discuss the design and implementation of a suite of functions for solving symmetric indefinite linear systems associated with mixed approximation of systems of PDEs. The novel feature of our iterative solver is the incorporation of error control in the natural ""energy"" norm in combination with an a posteriori estimator for the PDE approximation error. This leads to a robust and optimally efficient stopping criterion: the iteration is terminated as soon as the algebraic error is insignificant compared to the approximation error. We describe a ""proof of concept"" MATLAB implementation of this algorithm, which we call EST-MINRES, and we illustrate its effectiveness when integrated into the Incompressible Flow Iterative Solution Software (IFISS) package (cf. ACM Transactions on Mathematical Software 33, Article 14, 2007). © 2011 ACM.",EST-MINRES; Finite elements; Incompressible flow; Iterative solvers; MATLAB; Stopping criteria,Linear systems; Approximation errors; Error control; EST-MINRES; Finite Element; Indefinite systems; Iterative solutions; Iterative solvers; Mathematical software; Mixed approximation; Posteriori; Proof of concept; Stopping criteria; Incompressible flow
SelInv - An algorithm for selected inversion of a sparse symmetric matrix,2011,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79952505499&doi=10.1145%2f1916461.1916464&partnerID=40&md5=8d07ab73843fcede1fe8af9dba51ff10,"We describe an efficient implementation of an algorithm for computing selected elements of a general sparse symmetric matrix A that can be decomposed as A = LDLT, where L is lower triangular and D is diagonal. Our implementation, which is called SelInv, is built on top of an efficient supernodal left-looking LDLT factorization of A. We discuss how computational efficiency can be gained by making use of a relative index array to handle indirect addressing. We report the performance of SelInv on a collection of sparse matrices of various sizes and nonzero structures. We also demonstrate how SelInv can be used in electronic structure calculations. © 2011 ACM.",Electronic structure calculation; Elimination tree; Selected inversion; Sparse LDL<sup>T</sup> factorization; Supernodes,Algorithms; Computational efficiency; Density functional theory; Electronic properties; Electronic structure; Factorization; Electronic structure calculations; Elimination tree; Selected inversion;  factorization; Supernodes; Matrix algebra
Algorithm 909: NOMAD: Nonlinear optimization with the MADS algorithm,2011,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79952528772&doi=10.1145%2f1916461.1916468&partnerID=40&md5=868d6726c110ab493e74f9cc291af58c,"NOMAD is software that implements the Mesh Adaptive Direct Search (MADS) algorithm for blackbox optimization under general nonlinear constraints. Blackbox optimization is about optimizing functions that are usually given as costly programs with no derivative information and no function values returned for a significant number of calls attempted. NOMAD is designed for such problems and aims for the best possible solution with a small number of evaluations. The objective of this article is to describe the underlying algorithm, the software's functionalities, and its implementation. © 2011 ACM.",Blackbox optimization; Constrained optimization; MADS (Mesh Adaptive Direct Search); Nonlinear optimization; Optimization software,Algorithms; Black-box optimization; Derivative information; Function values; Mesh adaptive direct search; Non-linear constraints; Nonlinear optimization; Optimization software; Optimizing functions; Possible solutions; Constrained optimization
"Costas arrays: Survey, standardization, and MATLAB toolbox",2011,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79952498839&doi=10.1145%2f1916461.1916465&partnerID=40&md5=907ff8c4bbcb266faa181606b8598385,"A Costas array is an arrangement of N dots on an N-by-N grid, one per row, one per column, such that no two dots share the same displacement vector with any other pair. Costas arrays have applications in SONAR/RADAR systems, communication systems, cryptography, and other areas. We present a standardization of notation and language which can be used to discuss Costas array generation techniques and array manipulations. Using this standardization we can concisely and clearly state various theorems about Costas arrays, including several new theorems about the symmetries of Costas arrays. We also define labels for each array (generated, emergent, and sporadic), which describe whether the array is generated using a known technique, generated using a semiempirical variation of a known technique, or of unexplained origin. A new method for obtaining emergent Costas arrays, the DRT expansion, is also given here for the first time. A MATLAB Costas array toolbox has also been developed which implements the proposed standardization. The toolbox contains a comprehensive set of functions covering Costas array generation, manipulation and classification. © 2011 ACM.",Costas arrays; Finite fields; Golomb construction; Lempel construction; MATLAB toolbox; Welch construction,Communication systems; Finite element method; Frequency hopping; Costas array; Finite fields; Golomb construction; Lempel construction; MATLAB toolbox; Welch construction; Standardization
Solving very sparse rational systems of equations,2011,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79952496652&doi=10.1145%2f1916461.1916463&partnerID=40&md5=b0cee8b794456c0699385358784bf22f,"Efficient methods for solving linear-programming problems in exact precision rely on the solution of sparse systems of linear equations over the rational numbers. We consider a test set of instances arising from exact-precision linear programming and use this test set to compare the performance of several techniques designed for symbolic sparse linear-system solving. We compare a direct exact solver based on LU factorization, Wiedemann's method for black-box linear algebra, Dixon's p-adic-lifting algorithm, and the use of iterative numerical methods and rational reconstruction as developed by Wan. © 2011 ACM.",Dixon's algorithm; Linear programming; LU factorization; Rational systems; Sparse matrices; Wiedemann's method,Algorithms; Factorization; Linear programming; Matrix algebra; Dixon's algorithm; LU factorization; Rational systems; Sparse matrices; Wiedemann's method; Numerical methods
A supernodal approach to incomplete LU factorization with partial pivoting,2011,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79952497175&doi=10.1145%2f1916461.1916467&partnerID=40&md5=262404aea5b91b8c11eb312920070663,"We present a new supernode-based incomplete LU factorization method to construct a preconditioner for solving sparse linear systems with iterative methods. The new algorithm is primarily based on the ILUTP approach by Saad, and we incorporate a number of techniques to improve the robustness and performance of the traditional ILUTP method. These include new dropping strategies that accommodate the use of supernodal structures in the factored matrix and an area-based fill control heuristic for the secondary dropping strategy. We present numerical experiments to demonstrate that our new method is competitive with the other ILU approaches and is well suited for modern architectures with memory hierarchy. © 2011 ACM.",Incomplete LU factorization; Supernode,Factorization; Linear systems; Memory architecture; Area-based; Incomplete lu factorizations; matrix; Memory hierarchy; Modern architectures; Numerical experiments; Preconditioners; Sparse linear systems; Supernode; Numerical methods
Validated computation of certain hypergeometric functions,2011,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855987614&doi=10.1145%2f2049673.2049675&partnerID=40&md5=146c314fcd19152c69c182bcbb371785,"We present an efficient algorithm for the validated high-precision computation of real continued fractions, accurate to the last digit. The algorithm proceeds in two stages. In the first stage, computations are done in double precision. A forward error analysis and some heuristics are used to obtain an a priori error estimate. This estimate is used in the second stage to compute the fraction to the requested accuracy in high precision (adaptively incrementing the precision for reasons of efficiency). A running error analysis and techniques from interval arithmetic are used to validate the result. As an application, we use this algorithm to compute Gauss and confluent hypergeometric functions when one of the numerator parameters is a positive integer. © 2011 ACM 0098-3500/2011/12-ART10 $10.00.",Continued fractions; Hypergeometric functions; Validated software,Error analysis; Confluent hypergeometric functions; Continued fraction; High precision computation; Hypergeometric functions; Interval arithmetic; Numerator parameters; Priori error estimate; Validated softwares; Computational efficiency
A code based on the two-stage runge-kutta gauss formula for second-order initial value problems,2010,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77957556817&doi=10.1145%2f1824801.1824803&partnerID=40&md5=a5825cc06057bc80d953ea2f395e9747,"A code based on the two-stage Gauss formula (order four) for second-order initial value problems of a special type is developed. This code can be used to obtain a low- to medium-precision integration for a wide range of problems in the class of oscillatory type, Hamiltonian problems, and timedependent partial differential equations discretized in space by finite differences or finite elements. The iteration process used in solving for the stage values of the Gauss formula, the selection of the initial step size, and the choice of an appropriate local error estimator for determining the step size change according to a particular tolerance specified by the user are studied. Moreover, a global error estimate and a dense output at equidistant points in the integration interval are supplied with the code. Numerical experiments and some comparisons with certain standard codes on relevant test problems are also given. © 2010 ACM.",Implicit Runge-Kutta Nyström methods; Initial step size; Local error estimators; Predictors; Second-order problems; Stage values,Differential equations; Estimation; Gaussian distribution; Initial value problems; Integration; Local error; M method; Predictors; Second-order problems; Step size; Runge Kutta methods
Algorithm 906: Elrint3d - A three-dimensional nonadaptive automatic cubature routine using a sequence of embedded lattice rules,2010,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77957604803&doi=10.1145%2f1824801.1824813&partnerID=40&md5=63d6ff17e47911932308dddc9b260ca5,"A three-dimensional automatic cubature routine, called elrint3d, is described and numerical results are presented that demonstrate its applicability across a wide range of domains and integrand types. The underlying algorithm is based on a 2s-copy lattice augmentation sequence, the seed lattice for which has been determined by exhaustive search based on optimization of index of merit and trigonometric degree of precision. © 2010 ACM.",2s-copy lattice augmentation sequence; Automatic cubature routine; Index of merit; Seed lattice; Trigonometric degree of precision,Three dimensional; 2s-copy lattice augmentation sequence; Automatic cubature routine; Degree of precision; Exhaustive search; Index of merit; Lattice rules; Numerical results; Seed
"Algorithm 904: The SCASY library - Parallel solvers for sylvester-type matrix equations with applications in condition estimation, Part II",2010,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77957576160&doi=10.1145%2f1824801.1824811&partnerID=40&md5=ac4584516e02240a628e689b30af65c6,"We continue our presentation of parallel ScaLAPACK-style algorithms for solving Sylvester-type matrix equations. In Part II, we present SCASY (SCAlable SYlvester solvers), a state-of-the-art HPC software library for solving 44 sign and transpose variants of eight common standard and generalized Sylvester-type matrix equations. © 2010 ACM.",Condition estimation; Eigenvalue problems; Parallel algorithms; Parallel computing; Sylvester matrix equations,Eigenvalues and eigenfunctions; Equations of state; Estimation; Parallel algorithms; Parallel architectures; Switching systems; Common standards; Condition estimation; Eigenvalue problem; Parallel Computing; Parallel solver; ScaLAPACK; Software libraries; Sylvester matrix equations; Sylvester-type matrices; Matrix algebra
Algorithm 908: Online exact summation of floating-point streams,2010,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77957571319&doi=10.1145%2f1824801.1824815&partnerID=40&md5=965b9d8ca2c8503e25b5563734b8c5cf,"We present a novel, online algorithm for exact summation of a stream of floating-point numbers. By ""online"" we mean that the algorithm needs to see only one input at a time, and can take an arbitrary length input stream of such inputs while requiring only constant memory. By ""exact"" we mean that the sum of the internal array of our algorithm is exactly equal to the sum of all the inputs, and the returned result is the correctly-rounded sum. The proof of correctness is valid for all inputs (including nonnormalized numbers but modulo intermediate overflow), and is independent of the number of summands or the condition number of the sum. The algorithm asymptotically needs only 5 FLOPs per summand, and due to instruction-level parallelism runs only about 2-3 times slower than the obvious, fast-but-dumb ""ordinary recursive summation"" loop when the number of summands is greater than 10,000. Thus, to our knowledge, it is the fastest, most accurate, and most memory efficient among known algorithms. Indeed, it is difficult to see how a faster algorithm or one requiring significantly fewer FLOPs could exist without hardware improvements. An application for a large number of summands is provided. © 2010 ACM.",Floating-point summation; Rounding error,Digital arithmetic; Hydraulics; Number theory; Condition numbers; Floating point numbers; Floating-point summation; Input streams; Instruction level parallelism; Memory efficient; On-line algorithms; Proof of correctness; Rounding errors; Summands; Algorithms
MLD2P4: A package of parallel algebraic multilevel domain decomposition preconditioners in Fortran 95,2010,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77957583809&doi=10.1145%2f1824801.1824808&partnerID=40&md5=639f8e9ec1f81be8ce3b63af09204b74,"Domain decomposition ideas have long been an essential tool for the solution of PDEs on parallel computers. In recent years many research efforts have been focused on recursively employing domain decomposition methods to obtain multilevel preconditioners to be used with Krylov solvers. In this context, we developed MLD2P4 (MultiLevel Domain Decomposition Parallel Preconditioners Package based on PSBLAS), a package of parallel multilevel preconditioners that combines additive Schwarz domain decomposition methods with a smoothed aggregation technique to build a hierarchy of coarse-level corrections in an algebraic way. The design of MLD2P4 was guided by objectives such as extensibility, flexibility, performance, portability, and ease of use. They were achieved by following an object-based approach while using the Fortran 95 language, as well as by employing the PSBLAS library as a basic framework. In this article, we present MLD2P4 focusing on its design principles, software architecture, and use. © 2010 ACM.",Algebraic multilevel; Domain decomposition; Mathematics of computing; Object-based design; Parallel preconditioners,Algebra; Design; Operations research; Software architecture; Algebraic multilevel; Domain decompositions; Mathematics of computing; Object based; Parallel preconditioners; Domain decomposition methods
"Algorithm 907: KLU, a direct sparse solver for circuit simulation problems",2010,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77957571020&doi=10.1145%2f1824801.1824814&partnerID=40&md5=0a72ea9fc54bd6a69c2756e1ba7885c7,"KLU is a software package for solving sparse unsymmetric linear systems of equations that arise in circuit simulation applications. It relies on a permutation to Block Triangular Form (BTF), several methods for finding a fill-reducing ordering (variants of approximate minimum degree and nested dissection), and Gilbert/Peierls' sparse left-looking LU factorization algorithm to factorize each block. The package is written in C and includes a MATLAB interface. Performance results comparing KLU with SuperLU, Sparse 1.3, and UMFPACK on circuit simulation matrices are presented. KLU is the default sparse direct solver in the Xyce™circuit simulation package developed by Sandia National Laboratories. © 2010 ACM.",Circuit simulation; LU factorization; Sparse matrices,Computer software; Factorization; Linear systems; Matrix algebra; Block triangular forms; Linear systems of equations; LU factorization; Minimum degree; Nested dissection; Sandia National Laboratories; Simulation applications; Simulation matrices; Sparse direct solver; Sparse matrices; Sparse solvers; Circuit simulation
Algorithm 905: SHEPPACK: Modified Shepard algorithm for interpolation of scattered multivariate data,2010,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77957596661&doi=10.1145%2f1824801.1824812&partnerID=40&md5=408f6f86b52a7a5dc65bb09064d5ebfa,"Scattered data interpolation problems arise in many applications. Shepard's method for constructing a global interpolant by blending local interpolants using local-support weight functions usually creates reasonable approximations. SHEPPACK is a Fortran 95 package containing five versions of the modified Shepard algorithm: quadratic (Fortran 95 translations of Algorithms 660, 661, and 798), cubic (Fortran 95 translation of Algorithm 791), and linear variations of the original Shepard algorithm. An option to the linear Shepard code is a statistically robust fit, intended to be used when the data is known to contain outliers. SHEPPACK also includes a hybrid robust piecewise linear estimation algorithm RIPPLE (residual initiated polynomial-time piecewise linear estimation) intended for data from piecewise linear functions in arbitrary dimension m. Themain goal of SHEPPACK is to provide users with a single consistent package containing most existing polynomial variations of Shepard's algorithm. The algorithms target data of different dimensions. The linear Shepard algorithm, robust linear Shepard algorithm, and RIPPLE are the only algorithms in the package that are applicable to arbitrary dimensional data. © 2010 ACM.",M-estimation; RIPPLE; Shepard's algorithm,Blending; Estimation; Function evaluation; Interpolation; Piecewise linear techniques; Arbitrary dimension; FORTRAN 95; Interpolants; Linear variation; M-estimation; Multivariate data; Piece-wise linear functions; Piecewise linear; Polynomial-time; RIPPLE; Robust fit; Scattered data interpolation; Shepard's algorithm; Weight functions; Algorithms
Increasing the reliability of adaptive quadrature using explicit interpolants,2010,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77957578733&doi=10.1145%2f1824801.1824804&partnerID=40&md5=92ce123f195cd632ff72c59f694ba206,"We present two new adaptive quadrature routines. Both routines differ from previously published algorithms in many aspects, most significantly in how they represent the integrand, how they treat nonnumerical values of the integrand, how they deal with improper divergent integrals, and how they estimate the integration error. The main focus of these improvements is to increase the reliability of the algorithms without significantly impacting their efficiency. Both algorithms are implemented in MATLAB and tested using both the ""families"" suggested by Lyness and Kaganove and the battery test used by Gander and Gautschi and Kahaner. They are shown to be more reliable, albeit in some cases less efficient, than other commonly-used adaptive integrators. © 2010 ACM.",Adaptive quadrature; Error estimation; Interpolation; Orthogonal polynomials,Interpolation; Orthogonal functions; Adaptive quadrature; Battery tests; Error estimation; Gautschi; Integration error; Interpolants; Orthogonal polynomial; Algorithms
Parallel colt: A high-performance java library for scientific computing and image processing,2010,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77957572870&doi=10.1145%2f1824801.1824809&partnerID=40&md5=b42de45e95dc8f4fd70b4d8fb09bee90,"Major breakthroughs in chip and software design have been observed for the last nine years. In October 2001, IBM released the world's first multicore processor: POWER4. Six years later, in February 2007, NVIDIA made a public release of CUDA SDK, a set of development tools to write algorithms for execution on Graphic Processing Units (GPUs). Although software vendors have started working on parallelizing their products, the vast majority of existing code is still sequential and does not effectively utilize modern multicore CPUs and manycore GPUs. This article describes Parallel Colt, a multithreaded Java library for scientific computing and image processing. In addition to describing the design and functionality of Parallel Colt, a comparison to MATLAB is presented. Two ImageJ plugins for iterative image deblurring and motion correction of PET brain images are described as typical applications of this library. Performance comparisons with MATLAB including GPU computations via AccelerEyes' Jacket toolbox are also given. © 2010 ACM.",Deconvolution; FFT; Inverse problems; Iterative methods; Motion correction; Multithreading; PET; Regularization,Differential equations; Fast Fourier transforms; Image processing; Imaging systems; Inverse problems; Multitasking; Parallel architectures; Program processors; Software design; FFT; Motion correction; Multi-threading; PET; Regularization; Iterative methods
"Parallel solvers for sylvester-type matrix equations with applications in condition estimation, part I: Theory and algorithms",2010,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77957584129&doi=10.1145%2f1824801.1824810&partnerID=40&md5=5c52bec9a7ff574999e006f0218d1e36,"Parallel ScaLAPACK-style algorithms for solving eight common standard and generalized Sylvester-type matrix equations and various sign and transposed variants are presented. All algorithms are blocked variants based on the Bartels-Stewart method and involve four major steps: reduction to triangular form, updating the right-hand side with respect to the reduction, computing the solution to the reduced triangular problem, and transforming the solution back to the original coordinate system. Novel parallel algorithms for solving reduced triangular matrix equations based on wavefront-like traversal of the right-hand side matrices are presented together with a generic scalability analysis. These algorithms are used in condition estimation and new robust parallel sep -1-estimators are developed. Experimental results from three parallel platforms, including results from a mixed OpenMP/MPI platform, are presented and analyzed using several performance and accuracy metrics. The analysis includes results regarding general and triangular parallel solvers as well as parallel condition estimators. © 2010 ACM.",Condition estimation; Eigenvalue problems; Library software; Sylvester matrix equations,Estimation; Parallel algorithms; Switching systems; Co-ordinate system; Common standards; Condition estimation; Eigenvalue problem; Parallel condition; Parallel platforms; Parallel solver; Right-hand sides; Scalability analysis; ScaLAPACK; Sylvester matrix equations; Sylvester-type matrices; Triangular matrices; Eigenvalues and eigenfunctions
Unified tables for exponential and logarithm families,2010,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77957592255&doi=10.1145%2f1824801.1824806&partnerID=40&md5=a57d740708834ec784799a6da9f93786,"Accurate table methods allow for very accurate and efficient evaluation of elementary functions. We present new single-table approaches to logarithm and exponential evaluation, by which we mean that a single table of values works for both log(x) and log(1 + x), and a single table for ex and ex-1. This approach eliminates special cases normally required to evaluate log(1 + x) and ex-1 accurately near zero, which will significantly improve performance on architectures which use SIMD parallelism, or on which data-dependent branching is expensive. We have implemented it on the Cell/B.E. SPU (SIMD compute engine) and found the resulting functions to be up to twice as fast as the conventional implementations distributed in the IBM Mathematical Acceleration Subsystem (MASS). We include the literate code used to generate all the variants of exponential and log functions in the article, and discuss relevant language and hardware features. © 2010 ACM.",Accurate tables method; Cell/B.E.; IEEE arithmetic; SIMD; Vector library,Algebra; Accurate tables method; Cell/B.E.; Elementary function; Hardware features; IEEE arithmetic; SIMD; Exponential functions
Adaptive projection subspace dimension for the thick-restart lanczos method,2010,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77955416445&doi=10.1145%2f1824801.1824805&partnerID=40&md5=fc6e8da34733f703b2f0952e24b60085,"The Thick-Restart Lanczos (TRLan) method is an effective method for solving large-scale Hermitian eigenvalue problems. The performance of the method strongly depends on the dimension of the projection subspace used at each restart. In this article, we propose an objective function to quantify the effectiveness of the selection of subspace dimension, and then introduce an adaptive scheme to dynamically select the dimension to optimize the performance. We have developed an open-source software package a-TRLan to include this adaptive scheme in the TRLan method. When applied to calculate the electronic structure of quantum dots, a-TRLan runs up to 2.3x faster than a state-of-the-art preconditioned conjugate gradient eigensolver. © 2010 ACM.",Adaptive subspace dimension; Electronic structure calculation; Lanczos; Thick-restart,Conjugate gradient method; Eigenvalues and eigenfunctions; Electronic properties; Quantum theory; Adaptive scheme; Adaptive subspaces; Eigensolvers; Eigenvalue problem; Electronic structure calculation; Hermitians; Lanczos; Lanczos methods; Objective functions; Open-source softwares; Preconditioned conjugate gradient; Quantum Dot; Thick-restart; Electronic structure
Computing tutte polynomials,2010,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77957551843&doi=10.1145%2f1824801.1824802&partnerID=40&md5=198da824de1bbf9b82a7dac4a66d8313,"The Tutte polynomial of a graph, also known as the partition function of the q-state Potts model is a 2-variable polynomial graph invariant of considerable importance in both combinatorics and statistical physics. It contains several other polynomial invariants, such as the chromatic polynomial and flow polynomial as partial evaluations, and various numerical invariants such as the number of spanning trees as complete evaluations. However despite its ubiquity, there are no widely available effective computational tools able to compute the Tutte polynomial of a general graph of reasonable size. In this article we describe the implementation of a program that exploits isomorphisms in the computation tree to extend the range of graphs for which it is feasible to compute their Tutte polynomials, and we demonstrate the utility of the program by finding counterexamples to a conjecture of Welsh on the location of the real flow roots of a graph. © 2010 ACM.",Chromatic polynomial; Flow polynomial; Graph polynomial; Graph theory; Tutte polynomial,Graph theory; Trees (mathematics); Chromatic polynomials; Combinatorics; Computation trees; Computational tools; Flow polynomials; General graph; Graph polynomials; Partial evaluation; Partition functions; Polynomial graph; Polynomial invariants; Q-state Potts model; Real flow; Spanning tree; Statistical physics; Tutte polynomial; Polynomials
Stepsize selection for ordinary differential equations,2010,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77951906870&doi=10.1145%2f1731022.1731025&partnerID=40&md5=79cf20b5a387bdf352a15f5dbd2aa0a3,"This note offers a new approach based on a least squares fit to past data in order to select the stepsize when solving an ordinary differential equation. The approach used may have applicability to other situations where one wants to repeatedly make short term predictions given somewhat noisy data. Additional ad hoc rules help significantly for reliability and efficiency. Comparisons with some Runge-Kutta codes, an Adams code, and an extrapolation code are also included. © 2010 ACM.",ODE; Prediction; Stepsize,Approximation theory; Dynamical systems; Runge Kutta methods; Least squares fit; New approaches; Noisy data; Runge-Kutta; Short term prediction; Stepsize; Ordinary differential equations
Algorithm 903: FRB - Fortran routines for the exact computation of free rigid body motions,2010,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77951900922&doi=10.1145%2f1731022.1731033&partnerID=40&md5=b7e53f0471850b8bf77d6c267b0afb9a,"We present two algorithms and their corresponding Fortran routines for the exact computation of free rigid body motions. The methods use the same description of the angular momentum part m by Jacobi elliptic functions, and suitably chosen frames for the attitude matrix/quaternion Q/q, respectively. The frame transformation requires the computation of elliptic integrals of the third kind. Implementation and usage of the routines are described, and some examples of drivers are included. Accuracy and performance are also tested to provide reliable numerical results. © 2010 ACM.",Attitude rotation; Jacobi elliptic integrals; Numerical methods; Rigid body; Splitting methods,Geometry; Number theory; Numerical analysis; Rotation; Jacobi elliptic; Jacobi elliptic integrals; Rigid body; Splitting method; Splitting methods; Numerical methods
"Rectangular full packed format for cholesky's algorithm: Factorization, solution, and inversion",2010,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77951937022&doi=10.1145%2f1731022.1731028&partnerID=40&md5=fdf94c89b1c7deef143619117fdc3a4e,"We describe a new data format for storing triangular, symmetric, and Hermitian matrices called Rectangular Full Packed Format (RFPF). The standard two-dimensional arrays of Fortran and C (also known as full format) that are used to represent triangular and symmetric matrices waste nearly half of the storage space but provide high performance via the use of Level 3 BLAS. Standard packed format arrays fully utilize storage (array space) but provide low performance as there is no Level 3 packed BLAS. We combine the good features of packed and full storage using RFPF to obtain high performance via using Level 3 BLAS as RFPF is a standard full-format representation. Also, RFPF requires exactly the same minimal storage as packed the format. Each LAPACK full and/or packed triangular, symmetric, and Hermitian routine becomes a single new RFPF routine based on eight possible data layouts of RFPF. This new RFPF routine usually consists of two calls to the corresponding LAPACK full-format routine and two calls to Level 3 BLAS routines. This means no new software is required. As examples, we present LAPACK routines for Cholesky factorization, Cholesky solution, and Cholesky inverse computation in RFPF to illustrate this new work and to describe its performance on several commonly used computer platforms. Performance of LAPACK full routines using RFPF versus LAPACK full routines using the standard format for both serial and SMP parallel processing is about the same while using half the storage. Performance gains are roughly one to a factor of 43 for serial and one to a factor of 97 for SMP parallel times faster using vendor LAPACK full routines with RFPF than with using vendor and/or reference packed routines. © 2010 ACM.",BLAS; Cholesky factorization and solution; Complex Hermitian matrices; LAPACK; Linear algebra libraries; Novel packed matrix data structures; Positive definite matrices; Real symmetric matrices; Rectangular Full Packed Format; Recursive algorithms,Algebra; Algorithms; Data structures; Factorization; Libraries; Shape memory effect; Standards; BLAS; Cholesky factorizations; Complex Hermitian matrices; Hermitian matrices; LAPACK; matrix; Novel packed matrix data structures; Positive-definite matrices; Recursive algorithms; Symmetric matrices; Matrix algebra
"Algorithm 902: GPOPS, A MATLAB software for solving multiple-phase optimal control problems using the gauss pseudospectral method",2010,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77951909769&doi=10.1145%2f1731022.1731032&partnerID=40&md5=ea1949a8061b48f687fb4254cd803f1a,"An algorithm is described to solve multiple-phase optimal control problems using a recently developed numerical method called the Gauss pseudospectral method. The algorithm is well suited for use in modern vectorized programming languages such as FORTRAN 95 and MATLAB. The algorithm discretizes the cost functional and the differential-algebraic equations in each phase of the optimal control problem. The phases are then connected using linkage conditions on the state and time. A large-scale nonlinear programming problem (NLP) arises from the discretization and the significant features of the NLP are described in detail. A particular reusable MATLAB implementation of the algorithm, called GPOPS, is applied to three classical optimal control problems to demonstrate its utility. The algorithm described in this article will provide researchers and engineers a useful software tool and a reference when it is desired to implement the Gauss pseudospectral method in other programming languages. © 2010 ACM.",Computational methods; Dynamic optimization; Nonlinear optimization; Nonlinear programming; Optimal control; Phases,Algorithms; Computational methods; Computer software; FORTRAN (programming language); Linguistics; MATLAB; Nonlinear programming; Numerical methods; Problem oriented languages; Spacecraft; Cost functionals; Differential algebraic equations; Discretizations; Dynamic optimization; FORTRAN 95; Gauss pseudospectral method; Matlab- software; Non-linear optimization; Nonlinear programming problem; Optimal control; Optimal control problem; Optimal controls; Programming language; Software tool; Optimization
Scaling and pivoting in an out-of-core sparse direct solver,2010,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77951921328&doi=10.1145%2f1731022.1731029&partnerID=40&md5=d74e05fbffc106ecbe52beca9aa91a42,"Out-of-core sparse direct solvers reduce the amount of main memory needed to factorize and solve large sparse linear systems of equations by holding the matrix data, the computed factors, and some of the work arrays in files on disk. The efficiency of the factorization and solution phases is dependent upon the number of entries in the factors. For a given pivot sequence, the level of fill in the factors beyond that predicted on the basis of the sparsity pattern alone depends on the number of pivots that are delayed (i.e., the number of pivots that are used later than expected because of numerical stability considerations). Our aim is to limit the number of delayed pivots, while maintaining robustness and accuracy. In this article, we consider a new out-of-core multifrontal solver HSL-MA78 from the HSL mathematical software library that is designed to solve the unsymmetric sparse linear systems that arise from finite element applications. We consider how equilibration can be built into the solver without requiring the system matrix to be held in main memory. We also examine the effects of different pivoting strategies, including threshold partial pivoting, threshold rook pivoting, and static pivoting. Numerical experiments on problems arising from a range of practical applications illustrate the importance of scaling and show that, in some cases, rook pivoting can be more efficient than partial pivoting in terms of both the factorization time and the sparsity of the computed factors. © 2010 ACM.",Element problems; Large sparse unsymmetric linear systems; Multifrontal; Out-of-core solver; Partial pivoting; Rook pivoting; Scaling,Convergence of numerical methods; Factorization; Finite Element; Large sparse linear systems; Large sparse unsymmetric linear systems; Main memory; Mathematical software; matrix; Numerical experiments; Numerical stabilities; Out-of-core; Solution phasis; Sparse direct solver; Sparse linear systems; Sparsity patterns; System matrices; Linear systems
A fast and robust mixed-precision solver for the solution of sparse symmetric linear systems,2010,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77951891411&doi=10.1145%2f1731022.1731027&partnerID=40&md5=9af6a84cc2971860be77f531e4854744,"On many current and emerging computing architectures, single-precision calculations are at least twice as fast as double-precision calculations. In addition, the use of single precision may reduce pressure on memory bandwidth. The penalty for using single precision for the solution of linear systems is a potential loss of accuracy in the computed solutions. For sparse linear systems, the use of mixed precision in which double-precision iterative methods are preconditioned by a single-precision factorization can enable the recovery of high-precision solutions more quickly and use less memory than a sparse direct solver run using double-precision arithmetic. In this article, we consider the use of single precision within direct solvers for sparse symmetric linear systems, exploiting both the reduction in memory requirements and the performance gains. We develop a practical algorithm to apply a mixed-precision approach and suggest parameters and techniques to minimize the number of solves required by the iterative recovery process. These experiments provide the basis for our new code HSL-MA79-a fast, robust, mixed-precision sparse symmetric solver that is included in the mathematical software library HSL. Numerical results for a wide range of problems from practical applications are presented. © 2010 ACM.",FGMRES; Fortran 95; Gaussian elimination; Iterative refinement; Mixed precision; Multifrontal method; Sparse symmetric linear systems,Numerical analysis; FORTRAN 95; Gaussian elimination; Iterative refinement; Mixed precision; Multifrontal methods; Linear systems
EFCOSS: An interactive environment facilitating optimal experimental design,2010,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77951927092&doi=10.1145%2f1731022.1731023&partnerID=40&md5=535cfa1e204b903cc51e2874d9177340,An interactive software environment is proposed that combines numerical simulation codes with optimization software packages in an automated and modular way. It simplifies the experimentation with varying objective functions for common optimization problems such as parameter estimation and optimal experimental design that are frequently encountered in computational science and engineering. The design philosophy takes into consideration the need for derivatives of potentially large-scale simulation codes via automatic differentiation as well as distributed computing in a heterogenous environment via CORBA. © 2010 ACM.,Automatic differentiation; Distributed computing; Optimal experimental design; Parameter estimation; Problem solving environments,Computer simulation; Computer software; Design; Metal analysis; Optimization; Parameter estimation; Philosophical aspects; Statistics; Automatic differentiations; Computational science and engineerings; Design philosophy; Distributed Computing; Estimation problem; Interactive Environments; Interactive software; Large scale simulations; Numerical simulation; Objective functions; Optimal experimental designs; Optimization problems; Optimization software package; Problem solving
Efficient controls for finitely convergent sequential algorithms,2010,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77951898452&doi=10.1145%2f1731022.1731024&partnerID=40&md5=b47c07d38e64194878b1514139e52a89,"Finding a feasible point that satisfies a set of constraints is a common task in scientific computing; examples are the linear feasibility problem and the convex feasibility problem. Finitely convergent sequential algorithms can be used for solving such problems; an example of such an algorithm is ART3, which is defined in such a way that its control is cyclic in the sense that during its execution it repeatedly cycles through the given constraints. Previously we found a variant of ART3 whose control is no longer cyclic, but which is still finitely convergent and in practice usually converges faster than ART3. In this article we propose a general methodology for automatic transformation of finitely convergent sequential algorithms in such a way that (1) finite convergence is retained, and (2) the speed of convergence is improved. The first of these properties is proven by mathematical theorems, the second is illustrated by applying the algorithms to a practical problem. © 2010 ACM.",Algebraic reconstruction technique; Cyclic subgradient projections; Feasibility problem; Finite convergence; Sequential algorithm,Algebra; Algorithms; Problem solving; Sequential switching; Algebraic reconstruction techniques; Feasibility problem; Finite convergence; Sequential algorithm; Subgradient projections; Convergence of numerical methods
PRIMME: Preconditioned iterative multimethod eigensolver-methods and software description,2010,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77951916980&doi=10.1145%2f1731022.1731031&partnerID=40&md5=eaca83aa40478999705b4cef8994155e,"This article describes the PRIMME software package for solving large, sparse Hermitian standard eigenvalue problems. The difficulty and importance of these problems have increased over the years, necessitating the use of preconditioning and near optimally converging iterative methods. However, the complexity of tuning or even using such methods has kept them outside the reach of many users. Responding to this problem, we have developed PRIMME, a comprehensive package that brings state-of-the-art methods from bleeding edge to production, with the best possible robustness, efficiency, and a flexible, yet highly usable interface that requires minimal or no tuning. We describe (1) the PRIMME multimethod framework that implements a variety of algorithms, including the near optimal methods GD+k and JDQMR; (2) a host of algorithmic innovations and implementation techniques that endow the software with its robustness and efficiency; (3) a multilayer interface that captures our experience and addresses the needs of both expert and end users. © 2010 ACM.",Block; Conjugate gradient; Davidson; Eigenvalues; Eigenvectors; Hermitian; Iterative; Jacobi-Davidson; Lanczos; Locking; Preconditioning; Software package,Conjugate gradient method; Packaging; Software packages; Tuning; Block conjugate gradients; Davidson; Eigenvalues; Eigenvectors; Hermitians; Iterative; Jacobi-Davidson; Lanczos; Eigenvalues and eigenfunctions
Efficient and formally proven reduction of large integers by small moduli,2010,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77951898997&doi=10.1145%2f1731022.1731026&partnerID=40&md5=a38ef1e4906f9cd58ff02fa69673a912,"On w-bit processors which are much faster at multiplying two w-bit integers than at dividing 2w-bit integers by w-bit integers, reductions of large integers by moduli M smaller than 2w-1 are often implemented suboptimally, leading applications to take excessive processing time. We present a modular reduction algorithm implementing division by a modulus through multiplication by a reciprocal of that modulus, a well-known method for moduli larger than 2w-1. We show that application of this method to smaller moduli makes it possible to express certain modular sums and differences without having to compensate for word overflows. By embedding the algorithm in a loop and applying a few transformations to the loop, we obtain an algorithm for reduction of large integers by moduli up to 2w-1. Implementations of this algorithm can run considerably faster than implementations of similar algorithms that allow for moduli up to 2w. This is substantiated by measurements on processors with relatively fast multiplication instructions. It is notoriously hard to specify efficient mathematical algorithms on the level of abstract machine instructions in an error-free manner. In order to eliminate the chance of errors as much as possible, we have created formal correctness proofs of our algorithms, checked by a mechanized proof assistant. © 2010 ACM.",Computer arithmetic; Machine-checked proofs; Modular reduction; Optimization,Optimization; Abstract machines; Bit integers; Computer arithmetic; Fast multiplication; Formal correctness; Machine-checked proofs; Mathematical algorithms; Modular reduction; Processing Time; Proof assistant; Algorithms
DOLFIN: Automated finite element computing,2010,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-76249096812&doi=10.1145%2f1731022.1731030&partnerID=40&md5=e766fb27ee7a60421a98a02bb0e0eab1,"We describe here a library aimed at automating the solution of partial differential equations using the finite element method. By employing novel techniques for automated code generation, the library combines a high level of expressiveness with efficient computation. Finite element variational forms may be expressed in near mathematical notation, from which low-level code is automatically generated, compiled, and seamlessly integrated with efficient implementations of computational meshes and high-performance linear algebra. Easy-to-use object-oriented interfaces to the library are provided in the form of a C++ library and a Python module. This article discusses the mathematical abstractions and methods used in the design of the library and its implementation. A number of examples are presented to demonstrate the use of the library in application code. © 2010 ACM.",Code generation; DOLFIN; FEniCS project; Form compiler,Computational efficiency; Differential equations; Network components; Program compilers; Application codes; Automated code generation; Automatically generated; C++ libraries; Code Generation; Computational mesh; Efficient computation; Efficient implementation; Finite Element; Mathematical abstraction; Mathematical notations; Novel techniques; Object-oriented interfaces; Variational form; Finite element method
"An Interoperable, Data-Structure-Neutral Component for Mesh Query and Manipulation",2010,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024251380&doi=10.1145%2f1824801.1864430&partnerID=40&md5=0e23d908a0aede5e9bfe16f15b389103,"Much of the effort required to create a new simulation code goes into developing infrastructure for mesh data manipulation, adaptive refinement, design optimization, and so forth. This infrastructure is an obvious target for code reuse, except that implementations of these functionalities are typically tied to specific data structures. In this article, we describe a software component— an abstract data model and programming interface—designed to provide low-level mesh query and manipulation support for meshing and solution algorithms. The component's data model provides a data abstraction, completely hiding all details of how mesh data is stored, while its interface defines how applications can interact with that data. Because the component has been carefully designed to be general purpose and efficient, it provides a practical platform for implementing high-level mesh operations independently of the underlying mesh data structures. After describing the data model and interface, we provide several usage examples, each of which has been used successfully with multiple implementations of the interface functionality. The overhead due to accessing mesh data through the interface rather than directly accessing the underlying mesh data is shown to be acceptably small. © 2010, ACM. All rights reserved.",Data structure independence; Design; Mesh modification; Mesh-based simulations; Performance; Software components,Abstracting; Mesh generation; Adaptive refinement; Code reuse; Data manipulations; Data structure independence; Design optimization; Mesh data; Mesh modification; Mesh-based simulation; Simulation code; Software-component; Data structures
Optimizations for quadrature representations of finite element tensors through automated code generation,2010,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-76249094925&doi=10.1145%2f1644001.1644009&partnerID=40&md5=42f560e3038d1de951b44cf587bd4008,"We examine aspects of the computation of finite element matrices and vectors that are made possible by automated code generation. Given a variational form in a syntax that resembles standard mathematical notation, the low-level computer code for building finite element tensors, typically matrices, vectors and scalars, can be generated automatically via a form compiler. In particular, the generation of code for computing finite element matrices using a quadrature approach is addressed. For quadrature representations, a number of optimization strategies which are made possible by automated code generation are presented. The relative performance of two different automatically generated representations of finite element matrices is examined, with a particular emphasis on complicated variational forms. It is shown that approaches which perform best for simple forms are not tractable for more complicated problems in terms of run-time performance, the time required to generate the code or the size of the generated code. The approach and optimizations elaborated here are effective for a range of variational forms. © 2010 ACM.",Code generation; Finite element method,Automation; Matrix algebra; Network components; Program compilers; Tensors; Automated code generation; Automatically generated; Code generation; Computer codes; Finite Element; Finite element matrices; Mathematical notations; Optimization strategy; Relative performance; Runtimes; Variational form; Finite element method
Computing correctly rounded integer powers in floating-point arithmetic,2010,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-76249108349&doi=10.1145%2f1644001.1644005&partnerID=40&md5=fbead0450e3c040218b3d6a536754694,"We introduce several algorithms for accurately evaluating powers to a positive integer in floating-point arithmetic, assuming a fused multiply-add (fma) instruction is available. For bounded, yet very large values of the exponent, we aim at obtaining correctly rounded results in round-to-nearest mode, that is, our algorithms return the floating-point number that is nearest the exact value. © 2010 ACM.",Correct rounding; Floating-point arithmetic; Integer power function,Floating point numbers; Floating-point arithmetic; Fused multiply-add; Integer power; Positive integers; Power functions; Digital arithmetic
Singularity-free evaluation of collapsed-coordinate orthogonal polynomials,2010,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-76249126422&doi=10.1145%2f1644001.1644006&partnerID=40&md5=b63540d79a5814ee7e21c17856641ede,"The L2-orthogonal polynomials used in finite and spectral element methods on nonrectangular elements may be defined in terms of collapsed coordinates, wherein the shapes are mapped to a square or cube by means of a singular change of variables. The orthogonal basis is a product of specific Jacobi polynomials in these new coordinates. Implementations of these polynomials require special handling of the coordinate singularities. We derive new recurrence relations for these polynomials on triangles and tetrahedra that work directly in the original coordinates. These relations, also applicable to pyramids and prisms, do not require any special treatment of singular points. These recurrences are seen to speed up both symbolic and numerical computation of the orthogonal polynomials. © 2010 ACM.",Nonrectangular domain; Orthogonal polynomial; Recurrence relation,Change of variables; Coordinate singularity; Jacobi polynomials; Numerical computations; Orthogonal basis; Orthogonal polynomial; Recurrence relations; Singular points; Singularity-free; Special treatments; Spectral element method; Speed-ups; Polynomials
Algorithm 898: Efficient multiplication of dense matrices over GF(2),2010,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-76249109944&doi=10.1145%2f1644001.1644010&partnerID=40&md5=510e1dcaef7424f756c380d6c24b0f64,"We describe an efficient implementation of a hierarchy of algorithms for multiplication of dense matrices over the field with two elements (F 2). In particular we present our implementationin the M4RI libraryof Strassen-Winograd matrix multiplication and the Method of the Four Russians for Multiplication (M4RM) and compare it against other available implementations. Good performance is demonstrated on AMD's Opteron processor and particulary good performance on Intel's Core 2 uo processor. The open-source M4RI library is available as a stand-alone package as well as part of the Sage mathematics system. In machine terms, addition in F2 is logical-XOR, and multiplication is logical-AND, thus a machine word of 64 bits allows one to operate on 64 elements of F2 in parallel: at most one CPU cycle for 64 parallel additions or multiplications. As such, element-wise operations over F2 are relatively cheap. In fact, in this paper, we conclude that the actual bottlenecks are memory reads and writes and issues of data locality. We present our empirical findings in relation to minimizing these and give an analysis thereof. © 2010 ACM.",GF(2); Greasing; Linear algebra; Matrix; Multiplication; Strassen,Algebra; CPU cycles; Data locality; Dense matrices; Efficient implementation; Empirical findings; Greasing; MAtrix multiplication; Open-source; Opteron; Stand -alone; Winograd; Matrix algebra
Algorithm 899: The matlab postprocessing toolkit,2010,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-76249130587&doi=10.1145%2f1644001.1644011&partnerID=40&md5=ccbf44c64033e8082041529935af4a94,Global polynomial approximation methods applied to piecewise continuous functions exhibit the well-known Gibbs phenomenon. We summarize known methods to remove the Gibbs oscillations and present a collection of Matlab programs that implement the methods. The software features a Graphical User Interface that allows easy access to the postprocessing algorithms for benchmarking and educational purposes. © 2010 ACM.,Gibbs phenomenon; Matlab; Postprocessing; Pseudospectral methods,MATLAB; Polynomial approximation; Gibbs oscillations; Gibbs phenomena; Gibbs phenomenon; MATLAB program; Piecewise continuous functions; Postprocessing algorithms; Pseudospectral methods; Software features; Graphical user interfaces
Algorithm 901: LMEFA program for the construction of linear multistep methods with exponential fitting for the numerical solution of ordinary differential equations,2010,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-76249120890&doi=10.1145%2f1644001.1644013&partnerID=40&md5=a22bab1d135d42d58ce49a2119a326d8,"LMEF is a program written in MATLAB, to calculate the coefficients of a linear multi-step method (explicit, implicit or backward differentiation formulas) with algebraic and/or exponential fitting, for the numerical solution of first order ordinary differential equations. Moreover, LMEF calculates the local truncation error and in the case of exponential fitting, the Taylor expansions of the coefficients that are necessary for the implementation of the method. © 2010 ACM.",Backward differentiation formulas; Exponential fitting; Linear multistep methods,MATLAB; Numerical analysis; Numerical methods; Backward differentiation formulae; Exponential fitting; First order ordinary differential equations; Linear multi steps; Linear multistep method; Local truncation errors; Numerical solution; Taylor expansions; Ordinary differential equations
ScaLAPACK's MRRR algorithm,2010,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-76249083264&doi=10.1145%2f1644001.1644002&partnerID=40&md5=c7fb8ccd416712b7fa6fd5c959b4ce56,"The (sequential) algorithm of Multiple Relatively Robust Representations, MRRR, is a more efficient variant of inverse iteration that does not require reorthogonalization. It solves the eigenproblem of an unreduced symmetric tridiagonal matrix T Rn × n at O(n2) cost. The computed normalized eigenvectors are numerically orthogonal in the sense that the dot product between different vectors is O (n ε), where ε refers to the relative machine precision. This article describes the design of ScaLAPACK's parallel MRRR algorithm. One emphasis is on the critical role of the representation tree in achieving both adequate accuracy and parallel scalability. A second point concerns the favorable properties of this code: subset computation, the use of static memory, and scalability. Unlike ScaLAPACK's Divide & Conquer and QR, MRRR can compute subsets of eigenpairs at reduced cost. And in contrast to inverse iterations which can fail, it is guaranteed to produce a satisfactory answer while maintaining memory scalability. ParEig, the parallel MRRR algorithm for PLAPACK, uses dynamic memory allocation. This is avoided by our code at marginal additional cost. We also use a different representation tree criterion that allows for more accurate computation of the eigenvectors but can make parallelization more difficult. © 2010 ACM.",Design; Implementation; Multiple relatively robust representations; Numerical software; Parallel computation; ScaLAPACK; Symmetric eigenvalue problem,Algorithms; Computational efficiency; Computer software; Cost reduction; Matrix algebra; Scalability; Switching systems; Design implementation; Eigenvalue problem; Numerical software; Parallel Computation; Relatively robust representations; ScaLAPACK; Eigenvalues and eigenfunctions
Certification of bounds on expressions involving rounded operators,2010,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-76249128779&doi=10.1145%2f1644001.1644003&partnerID=40&md5=1a5d78feeac57bc5d9b964bfbeb07f2d,"Gappa is a tool designed to formally verify the correctness of numerical software and hardware. It uses interval arithmetic and forward error analysis to bound mathematical expressions that involve rounded as well as exact operators. It then generates a theorem and its proof for each verified enclosure. This proof can be automatically checked with a proof assistant, such as Coq or HOL Light. It relies on a large companion library of facts that we have developed. This Coq library provides theorems dealing with addition, multiplication, division, and square root, for both fixed- and floating-point arithmetics. Gappa uses multiple-precision dyadic fractions for the endpoints of intervals and performs forward error analysis on rounded operators when necessary. When asked, Gappa reports the best bounds it is able to reach for a given expression in a given context. This feature can be used to identify where the set of facts and automatic techniques implemented in Gappa becomes insufficient. Gappa handles seamlessly additional properties expressed as interval properties or rewriting rules in order to establish more intricate bounds. Recent work showed that Gappa is suited to discharge proof obligations generated for small pieces of software. They may be produced by third-party tools and the first applications of Gappa use proof obligations written by designers or obtained from traces of execution. © 2010 ACM.",Coq; Dyadic fraction; Floating point; Forward error analysis; HOL Light; Interval arithmetic; Proof obligation; Proof system; PVS,Computer software; Digital arithmetic; Error analysis; Gene expression; Mathematical operators; Coq; Dyadic fraction; Floating points; Interval arithmetic; Proof obligations; Proof system; Theorem proving
Design patterns for multiphysics modeling in Fortran 2003 and C++,2010,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-76249132080&doi=10.1145%2f1644001.1644004&partnerID=40&md5=f9a742c4ae17fef11d237ed084619750,"We present three new object-oriented software design patterns in Fortran 2003 and C++. These patterns integrate coupled differential equations, facilitating the flexible swapping of physical and numerical software abstractions at compile-time and runtime. The Semi-Discrete pattern supports the time advancement of a dynamical system encapsulated in a single abstract data type (ADT). The Puppeteer pattern combines ADTs into a multiphysics package, mediates interabstraction communications, and enables implicit marching even when nonlinear terms couple separate ADTs with private data. The Surrogate pattern emulates C++ forward references in Fortran 2003. After code demonstrations using the Lorenz equations, we provide architectural descriptions of our use of the new patterns in extending the Rouson et al. [2008a] Navier-Stokes solver to simulate multiphysics phenomena. We also describe the relationships between the new patterns and two previously developed architectural elements: the Strategy pattern of Gamma et al. [1995] and the template emulation technique of Akin [2003]. This report demonstrates how these patterns manage complexity by providing logical separation between individual physics models and the control logic that bridges between them. Additionally, it shows how language features such as operator overloading and automated memory management enable a clear mathematical notation for model bridging and system evolution. © 2010 ACM.",Design patterns; Lorenz equations; Multiphysics modeling,Abstracting; Design; Differential equations; Dynamical systems; FORTRAN (programming language); Mathematical operators; Object oriented programming; Software design; Abstract data types; Architectural descriptions; Architectural element; Automated memory management; Compile time; Control logic; Coupled differential equations; Design Patterns; Fortran; Language features; Lorenz equation; Mathematical notations; Multi-physics; Multi-physics modeling; Multiphysics phenomena; Navier-Stokes solver; Nonlinear terms; Numerical software; Object-oriented software designs; Operator overloading; Private data; Runtimes; System evolution; Computer software
Cache-optimal algorithms for option pricing,2010,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-76249101592&doi=10.1145%2f1644001.1644008&partnerID=40&md5=b8476a5c5f09700bc2e45a69c3df1687,"Today computers have several levels of memory hierarchy. To obtain good performance on these processors it is necessary to design algorithms that minimize I/O traffic to slower memories in the hierarchy. In this article, we study the computation of option pricing using the binomial and trinomial models on processors with a multilevel memory hierarchy. We derive lower bounds on memory traffic between different levels of the hierarchy for these two models. We also develop algorithms for the binomial and trinomial models that have near-optimal memory traffic between levels. We have implemented these algorithms on an UltraSparc IIIi processor with a 4-level of memory hierarchy and demonstrated that our algorithms outperform algorithms without cache blocking by a factor of up to 5 and operate at 70% of peak performance. © 2010 ACM.",Cache blocking; Memory hierarchy,Algorithms; Economics; 4-level; Cache blocking; Lower bounds; Memory hierarchy; Multi-level memory hierarchy; Optimal algorithm; Option pricing; Peak performance; UltraSPARC; Cache memory
On the efficiency of symbolic computations combined with code generation for finite element methods,2010,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-76249129066&doi=10.1145%2f1644001.1644007&partnerID=40&md5=0d8392275dc79b263c9bd44a84c3b587,"Efficient and easy implementation of variational forms for finite element discretization can be accomplished with metaprogramming. Using a high-level language like Python and symbolic mathematics makes an abstract problem definition possible, but the use of a low-level compiled language is vital for run-time efficiency. By generating low-level C++ code based on symbolic expressions for the discrete weak form, it is possible to accomplish a high degree of abstraction in the problem definition while surpassing the run-time efficiency of traditional hand written C++ codes. We provide several examples where we demonstrate orders of magnitude in speedup. © 2010 ACM.",Automation; Code generation; Compiler; Finite element; Metaprogramming; Variational forms,Abstracting; Finite element method; High level languages; Linguistics; Network components; Automation codes; C++ codes; Code Generation; Compiled languages; Finite Element; Finite-element discretization; Meta Programming; Orders of magnitude; Problem definition; Run-time efficiency; Symbolic computation; Symbolic expression; Symbolic mathematics; Variational form; Weak form; Program compilers
Algorithm 900: A discrete time kalman filter package for large scale problems,2010,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-76249087438&doi=10.1145%2f1644001.1644012&partnerID=40&md5=80cc9cf2881c06054a22b51df98a44a3,"Data assimilation is the process of feeding a partially unknown prediction model with available information from observations, with the objective of correcting and improving the modeled results. One of the most important mathematical tools to perform data assimilation is the Kalman filter. This is essentially a predictor-corrector algorithm that is optimal in the sense of minimizing the trace of the covariance matrix of the errors. Unfortunately, the computational cost of applying the filter to large scale problems is enormous, and the programming of the filter is highly dependent on the model and the format of the data involved. The first objective of this article is to present a set of Fortran 90 modules that implement the reduced rank square root versions of the Kalman filter, adapted for the assimilation of a very large number of variables. The second objective is to present a Kalman filter implementation whose code is independent of both the model and observations and is easy to use. A detailed description of the algorithms, structure, parallelization is given along with examples of using the package to solve practical problems. © 2010 ACM.",Data assimilation; Kalman filter; Large scale problems,Covariance matrix; Data processing; Kalman filters; Mathematical models; Computational costs; Data assimilation; Discrete-time Kalman filters; Fortran 90; Large-scale problem; Mathematical tools; Model and observation; Parallelizations; Practical problems; Prediction model; Predictor-corrector algorithm; Reduced rank; Square roots; Problem solving
Using NFFT 3 - -A software library for various nonequispaced fast fourier transforms,2009,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-70349128524&doi=10.1145%2f1555386.1555388&partnerID=40&md5=76b2c5d04c459b677fcddc0b5a8127a7,"NFFT 3 is a software library that implements the nonequispaced fast Fourier transform (NFFT) and a number of related algorithms, for example, nonequispaced fast Fourier transforms on the sphere and iterative schemes for inversion. This article provides a survey on the mathematical concepts behind the NFFT and its variants, as well as a general guideline for using the library. Numerical examples for a number of applications are given. © 2009 ACM.",Approximative algorithms; Fast Fourier transforms,Computer software; Mathematical transformations; Signal receivers; Approximative algorithms; Iterative schemes; Mathematical concepts; Numerical example; Software libraries; Fast Fourier transforms
An object-oriented design for two-dimensional vortex particle methods,2009,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-70349131524&doi=10.1145%2f1555386.1555387&partnerID=40&md5=15e907a93c24b0d2a7087042a946c4a9,"Vortex methods offer a grid-free alternative to simulating incompressible, viscous, fluid flows. They require the use of fairly sophisticated algorithms and can be complicated to implement for general flows. This article describes an object-oriented design used to implement a vortex particle based flow solver in two dimensions. We provide an overview of the various abstractions that arose as a result of this design. Several of the algorithms have common components that may be abstracted and reused. We demonstrate how the design allowed us to derive the traditional benefits of OOD. In addition, we show how the design directly suggested elegant generalizations of existing algorithms. Finally, we show the benefits of using software testing techniques and building a powerful scripting layer for the library. © 2009 ACM.",Object-orientation; Particle methods; Vortex methods,Abstracting; Computational fluid dynamics; Computer software selection and evaluation; Flow of fluids; Software testing; Systems analysis; Testing; Flow solver; Fluid flow; Object-orientation; Object-oriented design; Particle methods; Software testing techniques; Two-dimension; Vortex methods; Vortex particle method; Vortex particles; Design
Accurate numerical integration of perturbed oscillatory systems in two frequencies,2009,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-70349152141&doi=10.1145%2f1555386.1555390&partnerID=40&md5=b8118df4e71e517fc94ff2cbd609026f,"Highly accurate long-term numerical integration of nearly oscillatory systems of ordinary differential equations (ODEs) is a common problem in astrodynamics. Scheifeles algorithm is one of the excellent integrators developed in the past years to take advantage of special transformations of variables such as the K-S set. It is based on using expansions in series of the so-called G-functions, and generalizes the Taylor series integrators but with the remarkable property of integrating without truncation error oscillations in one basic known frequency. A generalization of Scheifeles method capable of integrating exactly harmonic oscillations in two known frequencies is developed here, after introducing a two parametric family of analytical φ-functions. Moreover, the local error contains the perturbation parameter as a factor when the algorithm is applied to perturbed problems. The good behavior and the long-term accuracy of the new method are shown through several examples, including systems with low- and high-frequency constituents and a perturbed satellite orbit. The new methods provide significantly higher accuracy and efficiency than a selection of well-reputed general-purpose integrators and even recent symplectic or symmetric integrators, whose good behavior in the long-term integration of the Kepler problem and the other oscillatory systems is well stated in recent literature. © 2009 ACM.",Highly oscillatory solutions; Long-term integration; Numerical solutions of ODEs; Perturbed oscillators,Integration; Common problems; G-functions; Harmonic oscillation; High frequency HF; Highly oscillatory solutions; Kepler problem; Local error; Long-term integration; Numerical integrations; Numerical solutions of ODEs; Oscillatory system; Perturbation parameters; Perturbed oscillators; Perturbed problems; Satellite orbit; Symplectic; Truncation errors; Two-parametric family; Ordinary differential equations
PyMDO: An Object-Oriented framework for multidisciplinary design optimization,2009,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-70349151749&doi=10.1145%2f1555386.1555389&partnerID=40&md5=2bdef5f23cd95a167b2f0a93d72901e2,"We present pyMDO, an object-oriented framework that facilitates the usage and development of algorithms for multidisciplinary optimization (MDO). The resulting implementation of the MDO methods is efficient and portable. The main advantage of the proposed framework is that it is flexible, with a strong emphasis on object-oriented classes and operator overloading, and it is therefore useful for the rapid development and evaluation of new MDO methods. The top layer interface is programmed in Python and it allows for the layers below the interface to be programmed in C, C-temp, Fortran, and other languages. We describe an implementation of pyMDO and demonstrate that we can take advantage of object-oriented programming to obtain intuitive, easy-to-read, and easy-to-develop codes that are at the same time efficient. This allows developers to focus on the new algorithms they are developing and testing, rather than on implementation details. Examples demonstrate the user interface and the corresponding results show that the various MDO methods yield the correct solutions. © 2009 ACM.",Multidisciplinary design optimization; Object-oriented programming,Combinatorial optimization; Mathematical operators; Multi agent systems; User interfaces; Correct solution; Fortran; Layer interfaces; Multi-disciplinary optimizations; Multidisciplinary design optimization; Object-oriented class; Object-oriented frameworks; Operator overloading; Rapid development; Object oriented programming
"C-temp Bindings to External Software Libraries with Examples from BLAS, LAPACK, UMFPACK, and MUMPS",2009,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-70349160346&doi=10.1145%2f1555386.1555391&partnerID=40&md5=2f22cf2ea48da20aeec46eda9979f10f,"FORTRAN and C software packages are often used in generic C-temp software. Calling nongeneric functions in generic code is not straightforward. The bindings in this article help the C-temp programmer using external software with a small effort. The bindings provide a mechanism to keep external software interfaces and specific vector and matrix containers orthogonal. We show examples using BLAS, LAPACK, UMFPACK, and MUMPS functions and subroutines. © 2009 ACM.",Bindings; BLAS; C-temp; LAPACK; Traits,Bindings; BLAS; C-temp; LAPACK; Traits; Computer software
Anasazi software for the numerical solution of large-scale eigenvalue problems,2009,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-70349739217&doi=10.1145%2f1527286.1527287&partnerID=40&md5=d3d2a44943c3c8654a48451852fc2b19,"Anasazi is a package within the Trilinos software project that provides a framework for the iterative, numerical solution of large-scale eigenvalue problems. Anasazi is written in ANSI C++ and exploits modern software paradigms to enable the research and development of eigensolver algorithms. Furthermore, Anasazi provides implementations for some of the most recent eigensolver methods. The purpose of our article is to describe the design and development of the Anasazi framework. A performance comparison of Anasazi and the popular FORTRAN 77 code ARPACK is given. © 2009 ACM.",Eigenvalue problems; Generic programming; Large-scale scientific computing; Numerical algorithms; Object-oriented programming,Computation theory; Computer software; Large scale systems; Object oriented programming; Switching systems; Design and Development; Eigensolver algorithms; Eigensolvers; Eigenvalue problem; Eigenvalue problems; Fortran 77; Generic programming; Large-scale scientific computing; Numerical algorithms; Numerical solution; Performance comparison; Research and development; Software paradigm; Software project; Trilinos; Eigenvalues and eigenfunctions
Algorithm 896: LSA: Algorithms for large-scale optimization,2009,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-70349746948&doi=10.1145%2f1527286.1527290&partnerID=40&md5=fbaaf0bed375e1692e945803b6465175,"We present 14 basic Fortran subroutines for large-scale unconstrained and box constrained optimization and large-scale systems of nonlinear equations. Subroutines PLIS and PLIP, intended for dense general optimization problems, are based on limited-memory variable metric methods. Subroutine PNET, also intended for dense general optimization problems, is based on an inexact truncated Newton method. Subroutines PNED and PNEC, intended for sparse general optimization problems, are based on modifications of the discrete Newton method. Subroutines PSED and PSEC, intended for partially separable optimization problems, are based on partitioned variable metric updates. Subroutine PSEN, intended for nonsmooth partially separable optimization problems, is based on partitioned variable metric updates and on an aggregation of subgradients. Subroutines PGAD and PGAC, intended for sparse nonlinear least-squares problems, are based on modifications and corrections of the Gauss-Newton method. Subroutine PMAX, intended for minimization of a maximum value (minimax), is based on the primal line-search interior-point method. Subroutine PSUM, intended for minimization of a sum of absolute values, is based on the primal trust-region interior-point method. Subroutines PEQN and PEQL, intended for sparse systems of nonlinear equations, are based on the discrete Newton method and the inverse column-update quasi-Newton method, respectively. Besides the description of methods and codes, we propose computational experiments which demonstrate the efficiency of the proposed algorithms. © 2009 ACM.",Discrete Newton methods; Large-scale nonlinear least squares; Large-scale nonlinear minimax; Large-scale nonsmooth optimization; Large-scale optimization; Large-scale systems of nonlinear equations; Limited-memory methods; Partially separable problems,Algorithms; Computational efficiency; Large scale systems; Newton-Raphson method; Nonlinear equations; Subroutines; Discrete Newton methods; Large-scale nonlinear least squares; Large-scale nonlinear minimax; Large-scale nonsmooth optimization; Large-scale optimization; Large-scale systems of nonlinear equations; Limited-memory methods; Partially separable problems; Constrained optimization
Algorithm 897: VTDIRECT95: Serial and parallel codes for the global optimization algorithm direct,2009,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-70349746945&doi=10.1145%2f1527286.1527291&partnerID=40&md5=f88144b20df8ff5816ae9c8e8827813a,"VTDIRECT95 is a Fortran 95 implementation of D. R. Jones' deterministic global optimization algorithm called DIRECT, which is widely used in multidisciplinary engineering design, biological science, and physical science applications. The package includes both a serial code and a data-distributed massively parallel code for different problem scales and optimization (exploration vs. exploitation) goals. Dynamic data structures are used to organize local data, handle unpredictable memory requirements, reduce the memory usage, and share the data across multiple processors. The parallel code employs a multilevel functional and data parallelism to boost concurrency and mitigate the data dependency, thus improving the load balancing and scalability. In addition, checkpointing features are integrated into both versions to provide fault tolerance and hot restarts. Important algorithm modifications and design considerations are discussed regarding data structures, parallel schemes, error handling, and portability. Using several benchmark functions and real-world applications, the software is evaluated on different systems in terms of optimization effectiveness, data structure efficiency, parallel performance, and checkpointing overhead. The package organization and usage are also described in detail. © 2009 ACM.",Checkpointing; Data structures; DIRECT; Global optimization; Parallel schemes,Benchmarking; Biology; Data structures; Fault tolerance; Global optimization; Parallel algorithms; Quality assurance; Structural optimization; Benchmark functions; Biological science; Checkpointing; Data dependencies; Data parallelism; Design considerations; Deterministic global optimization; DIRECT; Dynamic data structure; Error handling; FORTRAN 95; Global optimization algorithm; Load-Balancing; Local data; Memory requirements; Memory usage; Multidisciplinary engineering; Multiple processors; Package-organization; Parallel code; Parallel performance; Parallel scheme; Parallel schemes; Physical science; Problem scale; Real-world application; Serial codes; Shape optimization
Programming matrix algorithms-by-blocks for thread-level parallelism,2009,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-70349755577&doi=10.1145%2f1527286.1527288&partnerID=40&md5=6fcda8fa28b0f9ef1a5e0a154139f993,"With the emergence of thread-level parallelism as the primary means for continued performance improvement, the programmability issue has reemerged as an obstacle to the use of architectural advances. We argue that evolving legacy libraries for dense and banded linear algebra is not a viable solution due to constraints imposed by early design decisions. We propose a philosophy of abstraction and separation of concerns that provides a promising solution in this problem domain. The first abstraction, FLASH, allows algorithms to express computation with matrices consisting of contiguous blocks, facilitating algorithms-by-blocks. Operand descriptions are registered for a particular operation a priori by the library implementor. A runtime system, SuperMatrix, uses this information to identify data dependencies between suboperations, allowing them to be scheduled to threads out-of-order and executed in parallel. But not all classical algorithms in linear algebra lend themselves to conversion to algorithms-by-blocks. We show how our recently proposed LU factorization with incremental pivoting and a closely related algorithm-by-blocks for the QR factorization, both originally designed for out-of-core computation, overcome this difficulty. Anecdotal evidence regarding the development of routines with a core functionality demonstrates how the methodology supports high productivity while experimental results suggest that high performance is abundantly achievable. © 2009 ACM.",High-performance; Libraries; Linear algebra; Multithreaded architectures,Abstracting; Algebra; Algorithms; Factorization; Libraries; Philosophical aspects; Anecdotal evidences; Apriori; Classical algorithms; Core functionality; Data dependencies; Design decisions; High productivity; High-performance; LU factorization; Matrix algorithms; Multithreaded architectures; Out of order; Out-of-core computation; Performance improvements; Problem domain; Programmability; QR factorizations; Runtime systems; Separation of concerns; Thread level parallelism; Viable solutions; Computation theory
Algorithm 895: A continued fractions package for special functions,2009,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-70349728419&doi=10.1145%2f1527286.1527289&partnerID=40&md5=098fdb0dca621116a2c7db01b6aa2647,"The continued fractions for special functions package (in the sequel abbreviated as CFSF package) complements a systematic study of continued fraction representations for special functions. It provides all the functionality to create continued fractions, in particular k-periodic or limit k-periodic fractions, to compute approximants, make use of continued fraction tails, perform equivalence transformations and contractions, and much more. The package, developed in Maple, includes a library of more than 200 representations of special functions, of which only 10% can be found in the 1964 NBS Handbook of Mathematical Functions with Formulas, Graphs and Mathematical Tables by M. Abramowitz and I. Stegun. © 2009 ACM.",CAS software; Continued fractions; Maple; Special functions,Computer software; Function evaluation; Approximants; CAS software; Continued fraction; Continued fractions; Equivalence transformations; Maple; Mathematical functions; Mathematical tables; Periodic fraction; Special functions; Systematic study; Functions
Adaptive Winograd's matrix multiplications,2009,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-67649276935&doi=10.1145%2f1486525.1486528&partnerID=40&md5=76b64ba6dabb553c4fc33a37df1e537c,"Modern architectures have complex memory hierarchies and increasing parallelism (e.g., multicores). These features make achieving and maintaining good performance across rapidly changing architectures increasingly difficult. Performance has become a complex tradeoff, not just a simple matter of counting cost of simple CPU operations. We present a novel, hybrid, and adaptive recursive Strassen-Winograd's matrix multiplication (MM) that uses automatically tuned linear algebra software (ATLAS) or GotoBLAS. Our algorithm applies to any size and shape matrices stored in either row or column major layout (in double precision in this work) and thus is efficiently applicable to both C and FORTRAN implementations. In addition, our algorithm divides the computation into equivalent in-complexity sub-MMs and does not require any extra computation to combine the intermediary sub-MM results. We achieve up to 22% execution-time reduction versus GotoBLAS/ATLAS alone for a single core system and up to 19% for a two dual-core processor system. Most importantly, even for small matrices such as 1500 × 1500, our approach attains already 10% execution-time reduction and, for MM of matrices larger than 3000× 3000, it delivers performance that would correspond, for a classic O(n 3) algorithm, to faster-than-processor peak performance (i.e., our algorithm delivers the equivalent of 5 GFLOPS performance on a system with 4.4 GFLOPS peak performance and where GotoBLAS achieves only 4 GFLOPS). This is a result of the savings in operations (and thus FLOPS). Therefore, our algorithm is faster than any classic MM algorithms could ever be for matrices of this size. Furthermore, we present experimental evidence based on established methodologies found in the literature that our algorithm is, for a family of matrices, as accurate as the classic algorithms.",Fast algorithms; Winograd's matrix multiplications,Algorithms; Computation theory; Method of moments; Classic algorithm; Core systems; Double precision; Dual core-processors; Experimental evidence; Fast algorithms; Fortran; MAtrix multiplication; Memory hierarchy; Modern architectures; Peak performance; Size and shape; Time reduction; Winograd; Winograd's matrix multiplications; Matrix algebra
Algorithm 894: On a block Schur - Parlett algorithm for φ-functions based on the sep-inverse estimate,2009,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-65849282797&doi=10.1145%2f1499096.1499101&partnerID=40&md5=06241bd30c50f79dbe75d55243adb99e,"FORTRAN 95 software is provided for computing the matrix values of φ-functions required in exponential integrators. The subroutines in the library accept as their argument a full, diagonal, or upper quasitriangular matrix with real or complex entries in one of four precisions. Two different algorithms are implemented, one is the scaling and squaring method, and the other is a modified block Schur - Parlett algorithm. In the latter algorithm, a recursive three-by-three blocking is applied to the argument based on an estimate of the sep-inverse function. The estimation of the sep-inverse function is carried out by Hager - Higham estimator implemented as the subroutine xLACON in LAPACK. Our modifications to the block Schur - Parlett algorithm are described together with the results of numerical experiments. © 2009 ACM.",φ-functions; Block Schur - Parlett algorithm; Exponential integrators; Matrix functions,Algorithms; Function evaluation; Subroutines; Block Schur - Parlett algorithm; Exponential integrators; FORTRAN 95; Inverse functions; matrix; Matrix functions; Numerical experiments; Scaling and squaring method; Recursive functions
An out-of-core sparse Cholesky solver,2009,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-65849282796&doi=10.1145%2f1499096.1499098&partnerID=40&md5=eb0a772caaabbdac16f3e86818330f3a,"Direct methods for solving large sparse linear systems of equations are popular because of their generality and robustness. Their main weakness is that the memory they require usually increases rapidly with problem size. We discuss the design and development of the first release of a new symmetric direct solver that aims to circumvent this limitation by allowing the system matrix, intermediate data, and the matrix factors to be stored externally. The code, which is written in Fortran and called HSL-MA77, implements a multifrontal algorithm. The first release is for positive-definite systems and performs a Cholesky factorization. Special attention is paid to the use of efficient dense linear algebra kernel codes that handle the full-matrix operations on the frontal matrix and to the input/output operations. The input/output operations are performed using a separate package that provides a virtual-memory system and allows the data to be spread over many files; for very large problems these may be held on more than one device. Numerical results are presented for a collection of 30 large real-world problems, all of which were solved successfully. © 2009 ACM.",Cholesky; Multifrontal; Out-of-core solver; Sparse symmetric linear systems,Linear systems; Cholesky; Cholesky factorizations; Dense linear algebra; Design and Development; Direct method; Direct solvers; Fortran; Frontal matrix; Input/output operations; Large sparse linear systems; matrix; Matrix operations; Memory systems; Multifrontal; Numerical results; Out-of-core; Out-of-core solver; Problem size; Real-world problem; Sparse symmetric linear systems; System matrices; Matrix algebra
"Algorithm 892: DISPMODULE, a Fortran 95 module for pretty-printing matrices",2009,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-67649264615&doi=10.1145%2f1486525.1486531&partnerID=40&md5=3e691e9faf97ff104a74f5fee27ea73a,"A standard Fortran 95 module for printing scalars, vectors, and matrices to external files is provided. The module can display variables of default kind of all intrinsic types (integer, real, complex, logical, and character), and add-on modules are provided for data of the nondefault kind. The main module is self-contained and incorporating it only requires that it be compiled and linked with a program containing a 'use dispmodule' statement. A generic interface and optional parameters are used, so that the same subroutine name, DISP, is used to display items of different data type and rank, irrespective of display options. The subroutine is quite versatile, and hopefully can improve Fortran's competitiveness against other array programming languages. The module also contains a function TOSTRING to convert numerical scalars and vectors to strings.",Array programming language; Fortran 95; Matrix pretty-printing; Matrix printing; Output utilities,C (programming language); Competition; Linguistics; Matrix algebra; Printing; Query languages; Subroutines; Array programming language; Fortran 95; Matrix pretty-printing; Matrix printing; Output utilities; FORTRAN (programming language)
Data structures and requirements for hp finite element software,2009,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-67649261504&doi=10.1145%2f1486525.1486529&partnerID=40&md5=8547870ff54b54a46796735b89cbefb7,"Finite element methods approximate solutions of partial differential equations by restricting the problem to a finite dimensional function space. In hp adaptive finite element methods, one defines these discrete spaces by choosing different polynomial degrees for the shape functions defined on a locally refined mesh. Although this basic idea is quite simple, its implementation in algorithms and data structures is challenging. It has apparently not been documented in the literature in its most general form. Rather, most existing implementations appear to be for special combinations of finite elements, or for discontinuous Galerkin methods. In this article, we discuss generic data structures and algorithms used in the implementation of hp methods for arbitrary elements, and the complications and pitfalls one encounters. As a consequence, we list the information a description of a finite element has to provide to the generic algorithms for it to be used in an hp context. We support our claim that our reference implementation is efficient using numerical examples in two dimensions and three dimensions, and demonstrate that the hp-specific parts of the program do not dominate the total computing time. This reference implementation is also made available as part of the Open Source deal.II finite element library.",Data structures; Finite element software; Hp finite element methods; Object orientation; Software design,Computer software; Data structures; Function evaluation; Galerkin methods; Partial differential equations; Software design; Adaptive finite element methods; Algorithms and data structures; Approximate solution; Basic idea; Computing time; Discontinuous Galerkin methods; Discrete spaces; Finite dimensional; Finite Element; Finite element software; Generic algorithm; Generic data structures; Hp finite element methods; Hp-finite elements; Numerical example; Object orientation; Open sources; Polynomial degree; Reference implementation; Shape functions; Three dimensions; Two-dimension; Finite element method
Algorithm 891: A Fortran virtual memory system,2009,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-65849406987&doi=10.1145%2f1486525.1486530&partnerID=40&md5=6387c576d52dda133a31e49d2624e514,Fortran-Virtual-Memory is a Fortran 95 package that provides facilities for reading from and writing to direct-access files. A buffer is used to avoid actual input/output operations whenever possible. The data may be spread over many files and for very large data sets these may be held on more than one device. We describe the design of Fortran-Virtual-Memory and comment on its use within an out-of-core sparse direct solver.,Direct-access files; Fortran; Out-of-core; Virtual memory,Direct-access files; Fortran; FORTRAN 95; Input/output operations; Out-of-core; Sparse direct solver; Very large datum; Virtual memory; Virtual memory systems; FORTRAN (programming language)
A software framework for abstract expression of coordinate-free linear algebra and optimization algorithms,2009,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-65849391522&doi=10.1145%2f1499096.1499097&partnerID=40&md5=ce0f75a75e3c8cd945a5201ffdbf4ae5,"The Rice Vector Library is a collection of C++ classes expressing core concepts (vector, function) of calculus in Hilbert space with minimal implementation dependence, and providing standardized interfaces behind which to hide application-dependent implementation details (data containers, function objects). A variety of coordinate-free algorithms from linear algebra and optimization, including Krylov subspace methods and various relatives of Newton's method for nonlinear equations and constrained and unconstrained optimization, may be expressed purely in terms of this system of classes. The resulting code may be used without alteration in a wide range of control, design, and parameter estimation applications, in serial and parallel computing environments. © 2009 ACM.",Abstract numerical algorithms; Complex simulation; Numerical optimization,Abstracting; Algebra; Algorithms; Large scale systems; Newton-Raphson method; Nonlinear equations; Parallel architectures; Parallel processing systems; Parameter estimation; Abstract numerical algorithms; Complex simulation; Data container; Estimation applications; Function objects; Krylov subspace method; Newton's methods; Numerical optimization; Optimization algorithms; Parallel-computing environment; Software frameworks; Standardized interfaces; Unconstrained optimization; Constrained optimization
KSSOLV - A MATLAB toolbox for solving the Kohn-Sham equations,2009,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-65849463189&doi=10.1145%2f1499096.1499099&partnerID=40&md5=36095e5087796424775764515d1f430e,"We describe the design and implementation of KSSOLV, a MATLAB toolbox for solving a class of nonlinear eigenvalue problems known as the Kohn-Sham equations. These types of problems arise in electronic structure calculations, which are nowadays essential for studying the microscopic quantum mechanical properties of molecules, solids, and other nanoscale materials. KSSOLV is well suited for developing new algorithms for solving the Kohn-Sham equations and is designed to enable researchers in computational and applied mathematics to investigate the convergence properties of the existing algorithms. The toolbox makes use of the object-oriented programming features available in MATLAB so that the process of setting up a physical system is straightforward and the amount of coding effort required to prototype, test, and compare new algorithms is significantly reduced. All of these features should also make this package attractive to other computational scientists and students who wish to study small- to medium-size systems. © 2009 ACM.",Density functional theory (DFT); Direct constrained minimization (DCM); Electronic structure calculation; Kohn-Sham equations; Nonlinear eigenvalue problem; Planewave discretization; Pseudopotential; Self-consistent field iteration (SCF),Convergence of numerical methods; Current voltage characteristics; Eigenvalues and eigenfunctions; Electronic properties; Electronic structure; MATLAB; Mechanical properties; Nonlinear equations; Object oriented programming; Probability density function; Switching systems; Density functional theory (DFT); Direct constrained minimization (DCM); Electronic structure calculation; Kohn-Sham equations; Nonlinear eigenvalue problem; Planewave discretization; Pseudopotential; Self-consistent field iteration (SCF); Density functional theory
SBA: A software package for generic sparse bundle adjustment,2009,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-67649243035&doi=10.1145%2f1486525.1486527&partnerID=40&md5=849575907b986902e95e7a78eedadf2d,"Bundle adjustment constitutes a large, nonlinear least-squares problem that is often solved as the last step of feature-based structure and motion estimation computer vision algorithms to obtain optimal estimates. Due to the very large number of parameters involved, a general purpose least-squares algorithm incurs high computational and memory storage costs when applied to bundle adjustment. Fortunately, the lack of interaction among certain subgroups of parameters results in the corresponding Jacobian being sparse, a fact that can be exploited to achieve considerable computational savings. This article presents sba, a publicly available C/C++ software package for realizing generic bundle adjustment with high efficiency and flexibility regarding parameterization.",Bundle adjustment; Engineering applications; Levenberg-Marquardt; Multiple-view geometry; Nonlinear least squares; Sparse Jacobian; Structure and motion estimation; Unconstrained optimization,Computation theory; Computational fluid dynamics; Computer vision; Curve fitting; Estimation; Least squares approximations; Motion estimation; Optimization; Software packages; Bundle adjustment; Engineering applications; Levenberg-Marquardt; Multiple-view geometry; Nonlinear least squares; Sparse Jacobian; Structure and motion estimation; Unconstrained optimization; Nonlinear analysis
Distributed SBP Cholesky factorization algorithms with near-optimal scheduling,2009,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-65849486487&doi=10.1145%2f1499096.1499100&partnerID=40&md5=14465fee52bb690c4440f68c4fcbf30c,"The minimal block storage Distributed Square Block Packed (DSBP) format for distributed memory computing on symmetric and triangular matrices is presented. Three algorithm variants (Basic, Static, and Dynamic) of the blocked right-looking Cholesky factorization are designed for the DSBP format, implemented, and evaluated. On our target machine, all variants outperform standard full-storage implementations while saving almost half the storage. Communication overhead is shown to be virtually eliminated by the Static and Dynamic variants, both of which take advantage of hardware parallelism to hide communication costs. The Basic variant is shown to yield comparable or slightly better performance than the full-storage ScaLAPACK routine PDPOTRF while clearly outperformed by both Static and Dynamic. Models of execution assuming zero communication costs and overhead are developed. For medium- and larger-sized problems, the Static schedule is near optimal on our target machine based on comparisons with these models and measurements of synchronization overhead. © 2009 ACM.",Cholesky factorization; Distributed square block format; Packed storage; Parallel algorithms; Parallel computing; Positive definite matrices; Real symmetric matrices,Computation theory; Electric load shedding; Factorization; Matrix algebra; Parallel architectures; Parallel processing systems; Scheduling algorithms; Cholesky factorization; Distributed square block format; Packed storage; Parallel computing; Positive definite matrices; Real symmetric matrices; Parallel algorithms
A numerical evaluation of preprocessing and ILU-type preconditioners for the solution of unsymmetric sparse linear systems using iterative methods,2009,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-67649255317&doi=10.1145%2f1486525.1486526&partnerID=40&md5=5b8f9744a743bc2787d2c37ddeaaffd5,"Recent advances in multilevel LU factorizations and novel preprocessing techniques have led to an extremely large number of possibilities for preconditioning sparse, unsymmetric linear systems for solving with iterative methods. However, not all combinations work well for all systems, so making the right choices is essential for obtaining an efficient solver. The numerical results for 256 matrices presented in this article give an indication of which approaches are suitable for which matrices (based on different criteria, such as total computation time or fill-in) and of the differences between the methods.",Incomplete LU factorization; Iterative methods; Preconditioning; Sparse linear systems,Factorization; Linear systems; Numerical methods; Incomplete LU factorization; LU factorization; Numerical evaluations; Numerical results; Preconditioners; Preconditioning; Preprocessing techniques; Sparse linear systems; Total computation time; Iterative methods
Algorithm 893: TSPACK: tension spline package for curve design and data fitting,2009,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-67649283039&doi=10.1145%2f1486525.1486532&partnerID=40&md5=22008327755ce5a9573e81b8a1aad154,"TSPACK is a curve-fitting package based on exponential tension splines with automatic selection of tension factors. It serves both as a method for data fitting with preservation of shape properties or more general constraints, and as a means of computer aided geometric design of curves in two or three dimensions. The package is based on a translation of Algorithm 716 from Fortran 77 into MATLAB. The translation includes bug corrections, vectorization where possible, and extensions, including a B-spline representation, designed to facilitate curve design as opposed to data fitting. An interactive graphical user interface, not part of the algorithm, is available from the author.",Convexity preserving; Cubic spline; Exponential spline; Interpolation; Monotonicity preserving; Parametric curve; Piecewise polynomial; Shape preserving; Smoothing; Spline under tension; Tension factor,Computer aided design; Graphical user interfaces; Interpolation; MATLAB; Splines; Convexity preserving; Cubic spline; Exponential spline; Monotonicity preserving; Parametric curve; Piecewise polynomial; Shape preserving; Smoothing; Tension factor; Curve fitting
CONTEST: A controllable test matrix toolbox for MATLAB,2009,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-61349089223&doi=10.1145%2f1462173.1462175&partnerID=40&md5=314ec46315c9aa2d3cb6849b0acb9bc6,"Large, sparse networks that describe complex interactions are a common feature across a number of disciplines, giving rise to many challenging matrix computational tasks. Several random graph models have been proposed that capture key properties of real-life networks. These models provide realistic, parametrized matrices for testing linear system and eigenvalue solvers. CONTEST (CONtrollable TEST matrices) is a random network toolbox for MATLAB that implements nine models. The models produce unweighted directed or undirected graphs; that is, symmetric or unsymmetric matrices with elements equal to zero or one. They have one or more parameters that affect features such as sparsity and characteristic pathlength and all can be of arbitrary dimension. Utility functions are supplied for rewiring, adding extra shortcuts and subsampling in order to create further classes of networks. Other utilities convert the adjacency matrices into real-valued coefficient matrices for naturally arising computational tasks that reduce to sparse linear system and eigenvalue problems. © 2009 ACM.",Clustering; Matrix computation; Preferential attachment; Random graph; Rewiring; Small-world; Sparse matrix,Eigenvalues and eigenfunctions; Large scale systems; Linear systems; MATLAB; Matrix algebra; Online searching; Clustering; Matrix computation; Preferential attachment; Random graph; Rewiring; Small-world; Sparse matrix; Graph theory
Algorithm 890: Sparco: A testing framework for sparse reconstruction,2009,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-61349195870&doi=10.1145%2f1462173.1462178&partnerID=40&md5=9f2135b744c1ab3bc04352163ab9ae60,"Sparco is a framework for testing and benchmarking algorithms for sparse reconstruction. It includes a large collection of sparse reconstruction problems drawn from the imaging, compressed sensing, and geophysics literature. Sparco is also a framework for implementing new test problems and can be used as a tool for reproducible research. Sparco is implemented entirely in Matlab, and is released as open-source software under the GNU Public License. © 2009 ACM.",Compressed sensing; Linear operators; Sparse recovery,MATLAB; Restoration; Signal reconstruction; Benchmarking algorithms; Compressed sensing; Gnu public license; Linear operators; Open-source softwares; Reproducible researches; Sparse reconstruction; Sparse recovery; Test problems; Testing frameworks; Mathematical operators
Extra-precise iterative refinement for overdetermined least squares problems,2009,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-61349142482&doi=10.1145%2f1462173.1462177&partnerID=40&md5=bb5f3cd4b9dbe1512e7d4b6e5193d704,"We present the algorithm, error bounds, and numerical results for extra-precise iterative refinement applied to overdetermined linear least squares (LLS) problems. We apply our linear system refinement algorithm to Björcks augmented linear system formulation of an LLS problem. Our algorithm reduces the forward normwise and componentwise errors to O(ε w), where ε w is the working precision, unless the system is too ill conditioned. In contrast to linear systems, we provide two separate error bounds for the solution x and the residual r. The refinement algorithm requires only limited use of extra precision and adds only O(mn) work to the O(mn 2) cost of QR factorization for problems of size m-by-n. The extra precision calculation is facilitated by the new extended-precision BLAS standard in a portable way, and the refinement algorithm will be included in a future release of LAPACK and can be extended to the other types of least squares problems. © 2009 ACM.",BLAS; Floating-point arithmetic; LAPACK; Linear algebra,Algebra; Curve fitting; Electric appliances; Least squares approximations; Linear systems; Manganese compounds; BLAS; Component wise; Error bounds; Floating-point arithmetic; Ill-conditioned; Iterative refinements; LAPACK; Least squares problems; Linear least squares; Numerical results; Q R factorizations; Refinement algorithms; System formulations; System refinements; Algorithms
Dynamic supernodes in sparse cholesky update/downdate and triangular solves,2009,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-56349122415&doi=10.1145%2f1462173.1462176&partnerID=40&md5=1106bade3a6e59d39bafc542980778ae,"The supernodal method for sparse Cholesky factorization represents the factor L as a set of supernodes, each consisting of a contiguous set of columns of L with identical nonzero pattern. A conventional supernode is stored as a dense submatrix. While this is suitable for sparse Cholesky factorization where the nonzero pattern of L does not change, it is not suitable for methods that modify a sparse Cholesky factorization after a low-rank change to A (an update/downdate, = A ± WW T). Supernodes merge and split apart during an update/downdate. Dynamic supernodes are introduced which allow a sparse Cholesky update/downdate to obtain performance competitive with conventional supernodal methods. A dynamic supernodal solver is shown to exceed the performance of the conventional (BLAS-based) supernodal method for solving triangular systems. These methods are incorporated into CHOLMOD, a sparse Cholesky factorization and update/downdate package which forms the basis of x = A\b MATLAB when A is sparse and symmetric positive definite. © 2009 ACM.",Cholesky factorization; Linear equations; Sparse matrices,MATLAB; Cholesky; Cholesky factorization; Sparse matrices; Submatrix; Super nodes; Symmetric positive definite; Triangular systems; Factorization
A standard and software for numerical metadata,2009,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-61349092305&doi=10.1145%2f1462173.1462174&partnerID=40&md5=abb90cc1dcf9e3ef9db63af2604febff,"We propose a standard for generating, manipulating, and storing metadata describing numerical problems, in particular properties of matrices and linear systems. The standard comprises: -an API for metadata generating and querying software, and -an XML format for permanent storage of metadata. The API is open-ended, allowing for other parties to define additional metadata categories to be generated and stored within this framework. Furthermore, we present two software libraries, NMD and AnaMod, that implement this standard, and that contain a number of computational modules for numerical metadata. The libraries, more than simply illustrating the use of the standard, provide considerable utility to numerical researchers. © 2009 ACM.",Design; Standardization,Linear systems; Markup languages; Metadata; Standardization; Numerical problems; Software libraries; Xml formats; Data communication systems
Algorithm 889: Jet_fitting_3: - -A generic C++ package for estimating the differential properties on sampled surfaces via polynomial fitting,2008,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-56349110019&doi=10.1145%2f1391989.1404582&partnerID=40&md5=ef72ae878acc62891632b5e29834eb15,"Surfaces of R3 are ubiquitous in science and engineering, and estimating the local differential properties of a surface discretized as a point cloud or a triangle mesh is a central building block in computer graphics, computer aided design, computational geometry, and computer vision. One strategy to perform such an estimation consists of resorting to polynomial fitting, either interpolation or approximation, but this route is difficult for several reasons: choice of the coordinate system, numerical handling of the fitting problem, and extraction of the differential properties. This article presents a generic C++ software package solving these problems. On the theoretical side and as established in a companion paper, the interpolation and approximation methods provided achieve the best asymptotic error bounds known to date. On the implementation side and following state-of-the-art coding rules in computational geometry, genericity of the package is achieved thanks to four template classes accounting for, (a) the type of the input points, (b) the internal geometric computations, (c) a conversion mechanism between these two geometries, and (d) the linear algebra operations. An instantiation within the Computational Geometry Algorithms Library (CGAL, version 3.3) and using LAPACK is also provided. © 2008 ACM.",Approximation; C++ design; Computational geometry; Differential geometry; Interpolation; Numerical linear algebra; Sampled surfaces,Algebra; Computer aided design; Computer graphics; Computer programming languages; Computer vision; Geometry; Image processing; Interpolation; Linear algebra; Polynomial approximation; Programming theory; Approximation; C++ design; Differential geometry; Numerical linear algebra; Sampled surfaces; Computational geometry
Algorithm 885: Computing the logarithm of the normal distribution,2008,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-56349169452&doi=10.1145%2f1391989.1391993&partnerID=40&md5=6da4beb86aad5e76c4a9f96ade17929c,"We present and compare three C functions to compute the logarithm of the cumulative standard normal distribution. The first is a new algorithm derived from Algorithm 304s calculation of the standard normal distribution via a series or continued fraction approximation, and it is good to the accuracy of the machine. The second is based on Algorithm 715s calculation of the standard normal distribution via rational Chebyshev approximation. This is related to, and an improvement on, the algorithm for the logarithm of the normal distribution available in the software package R. The third is a new and simple algorithm that uses the compilers implementation of the error function, and complement of the error function, to compute the log of the normal distribution. © 2008 ACM.",Error function; Logarithm of the standard normal distribution; Normal distribution; Normal integral,Algebra; Algorithms; Approximation algorithms; Chebyshev approximation; Distribution functions; Polynomial approximation; Probability density function; Standards; C functions; Continued fractions; Error function; Error functions; Logarithm of the standard normal distribution; New algorithms; Normal integral; Simple algorithms; Normal distribution
Algorithm 888: Spherical harmonic transform algorithms,2008,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-56349145687&doi=10.1145%2f1391989.1404581&partnerID=40&md5=4fdbe4326a50b16c4b650cfb2481b524,"A collection of MATLAB classes for computing and using spherical harmonic transforms is presented. Methods of these classes compute differential operators on the sphere and are used to solve simple partial differential equations in a spherical geometry. The spectral synthesis and analysis algorithms using fast Fourier transforms and Legendre transforms with the associated Legendre functions are presented in detail. A set of methods associated with a spectral_field class provides spectral approximation to the differential operators ∇ ⋯, ∇ ×, ∇, and ∇2 in spherical geometry. Laplace inversion and Helmholtz equation solvers are also methods for this class. The use of the class and methods in MATLAB is demonstrated by the solution of the barotropic vorticity equation on the sphere. A survey of alternative algorithms is given and implementations for parallel high performance computers are discussed in the context of global climate and weather models. © 2008 ACM.",Fluid dynamics; Geophysical flow; High performance computing; Spectral transform methods; Spherical,Computational geometry; Differential equations; Dynamics; Fast Fourier transforms; Fluid dynamics; Fourier transforms; Harmonic analysis; Helmholtz equation; High performance liquid chromatography; Laplace equation; Mathematical operators; MATLAB; Medical imaging; Motion compensation; Parallel algorithms; Spheres; Alternative algorithms; Analysis algorithms; Associated Legendre functions; Barotropic; Differential operators; Geophysical flow; Global climates; High performance computers; High performance computing; Laplace inversions; Legendre transforms; Spectral approximations; Spectral syntheses; Spectral transform methods; Spherical; Spherical geometries; Spherical harmonic transforms; Vorticity equations; Weather models; Mathematical transformations
CGMN revisited: Robust and efficient solution of stiff linear systems derived from elliptic partial differential equations,2008,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-56349135939&doi=10.1145%2f1391989.1391991&partnerID=40&md5=502a0b5ec70b3f466367fc1742fba57a,"Given a linear system Ax = b, one can construct a related normal equations system AATy = b, x = ATy. Björck and Elfving have shown that the SSOR algorithm, applied to the normal equations, can be accelerated by the conjugate gradient algorithm (CG). The resulting algorithm, called CGMN, is error-reducing and in theory it always converges even when the equation system is inconsistent and/or nonsquare. SSOR on the normal equations is equivalent to the Kaczmarz algorithm (KACZ), with a fixed relaxation parameter, run in a double (forward and backward) sweep on the original equations. CGMN was tested on nine well-known large and sparse linear systems obtained by central-difference discretization of elliptic convection-diffusion partial differential equations (PDEs). Eight of the PDEs were strongly convection-dominated, and these are known to produce very stiff systems with large off-diagonal elements. CGMN was compared with some of the foremost state-of-the art Krylov subspace methods: restarted GMRES, Bi-CGSTAB, and CGS. These methods were tested both with and without various preconditioners. CGMN converged in all the cases, while none of the preceding algorithm/preconditioner combinations achieved this level of robustness. Furthermore, on varying grid sizes, there was only a gradual increase in the number of iterations as the grid was refined. On the eight convection-dominated cases, the initial convergence rate of CGMN was better than all the other combinations of algorithms and preconditioners, and the residual decreased monotonically. The CGNR algorithm was also tested, and it was as robust as CGMN, but slower. © 2008 ACM.",CGMN; CGNR; Conjugate-gradient; Convection-dominated; Elliptic equations; Kaczmarz; Linear systems; Normal equations; Partial differential equations; Row projections; SOR; Sparse linear systems; SSOR; Stiff equations,Aircraft engines; Algorithms; Channel capacity; Computational fluid dynamics; Conjugate gradient method; Difference equations; Image segmentation; Linear systems; Partial differential equations; CGMN; CGNR; Conjugate-gradient; Convection-dominated; Elliptic equations; Kaczmarz; Normal equations; Row projections; SOR; Sparse linear systems; SSOR; Stiff equations; Linear equations
Algorithms and data structures for multi-adaptive time-stepping,2008,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-56349106061&doi=10.1145%2f1391989.1391990&partnerID=40&md5=78bbe911e05849176921e9d39cc1713e,"Multi-adaptive Galerkin methods are extensions of the standard continuous and discontinuous Galerkin methods for the numerical solution of initial value problems for ordinary or partial differential equations. In particular, the multi-adaptive methods allow individual and adaptive time steps to be used for different components or in different regions of space. We present algorithms for efficient multi-adaptive time-stepping, including the recursive construction of time slabs and adaptive time step selection. We also present data structures for efficient storage and interpolation of the multi-adaptive solution. The efficiency of the proposed algorithms and data structures is demonstrated for a series of benchmark problems. © 2008 ACM.",Algorithms; C++; Continuous Galerkin; Discontinuous Galerkin; DOLFIN; Implementation; Individual time steps; Local time steps; Mcgq; Mdgq; Multi-adaptivity; Multirate; ODE,Boundary value problems; Data structures; Differential equations; File organization; Galerkin methods; Initial value problems; Large scale systems; Numerical analysis; Numerical methods; Ordinary differential equations; Recursive functions; C++; Continuous Galerkin; Discontinuous Galerkin; DOLFIN; Implementation; Individual time steps; Local time steps; Mcgq; Mdgq; Multi-adaptivity; Multirate; ODE; Adaptive algorithms
Dense linear algebra over word-size prime fields: The FFLAS and FFPACK packages,2008,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-56349134473&doi=10.1145%2f1391989.1391992&partnerID=40&md5=b968a62b15a7984a12900431e80f0ce3,"In the past two decades, some major efforts have been made to reduce exact (e.g. integer, rational, polynomial) linear algebra problems to matrix multiplication in order to provide algorithms with optimal asymptotic complexity. To provide efficient implementations of such algorithms one need to be careful with the underlying arithmetic. It is well known that modular techniques such as the Chinese remainder algorithm or the p-adic lifting allow very good practical performance, especially when word size arithmetic is used. Therefore, finite field arithmetic becomes an important core for efficient exact linear algebra libraries. In this article, we study high performance implementations of basic linear algebra routines over word size prime fields: especially matrix multiplication; our goal being to provide an exact alternate to the numerical BLAS library. We show that this is made possible by a careful combination of numerical computations and asymptotically faster algorithms. Our kernel has several symbolic linear algebra applications enabled by diverse matrix multiplication reductions: symbolic triangularization, system solving, determinant, and matrix inverse implementations are thus studied. © 2008 ACM.",BLAS level 1-2-3; Determinant; Inverse; Linear algebra package; Matrix factorization; Winograds symbolic matrix multiplication; Word size prime fields,Algebra; Asymptotic analysis; Digital arithmetic; Inverse problems; Matrix algebra; BLAS level 1-2-3; Determinant; Inverse; Linear algebra package; Matrix factorization; Winograds symbolic matrix multiplication; Word size prime fields; Linear algebra
"Algorithm 887: CHOLMOD, supernodal sparse cholesky factorization and update/downdate",2008,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-56349128770&doi=10.1145%2f1391989.1391995&partnerID=40&md5=44bbaf62d3df1234c0f4c85859125b17,"CHOLMOD is a set of routines for factorizing sparse symmetric positive definite matrices of the form A or AAT, updating/downdating a sparse Cholesky factorization, solving linear systems, updating/downdating the solution to the triangular system Lx = b, and many other sparse matrix functions for both symmetric and unsymmetric matrices. Its supernodal Cholesky factorization relies on LAPACK and the Level-3 BLAS, and obtains a substantial fraction of the peak performance of the BLAS. Both real and complex matrices are supported. CHOLMOD is written in ANSI/ISO C, with both C and MATLABTM interfaces. It appears in MATLAB 7.2 as x = A\b when A is sparse symmetric positive definite, as well as in several other sparse matrix functions. © 2008 ACM.",Cholesky factorization; Linear equations; Sparse matrices,Factorization; Function evaluation; Linear systems; MATLAB; Online searching; Probability density function; Turbulent flow; Cholesky factorization; Cholesky factorizations; Level-3 blas; Peak performances; Positive definite; Sparse cholesky; Sparse matrices; Sparse matrixes; Triangular systems; Linear equations
Algorithm 886: Padua2D - -lagrange interpolation at padua points on bivariate domains,2008,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-55349106415&doi=10.1145%2f1391989.1391994&partnerID=40&md5=39e9cce0ce5307305321781de740c94b,"We present a stable and efficient Fortran implementation of polynomial interpolation at the Padua points on the square [-1, 1]2. These points are unisolvent and their Lebesgue constant has minimal order of growth (log square of the degree). The algorithm is based on the representation of the Lagrange interpolation formula in a suitable orthogonal basis, and takes advantage of a new matrix formulation together with the machine-specific optimized BLAS subroutine DGEMM for the matrix-matrix product. Extension to interpolation on rectangles, triangles and ellipses is also described. © 2008 ACM.",Bivariate Chebyshev orthogonal basis; Bivariate Lagrange interpolation; Fortran 77; Padua points,Boolean functions; Computer programming languages; FORTRAN (programming language); Lagrange multipliers; A stables; Bivariate; Bivariate Chebyshev orthogonal basis; Bivariate Lagrange interpolation; Do-mains; Fortran 77; Fortrans; Lagrange interpolations; Lebesgue constants; Matrix formulations; Matrix products; Order of growths; Orthogonal bases; Padua points; Polynomial interpolations; Interpolation
An implementation and evaluation of the AMLS method for sparse eigenvalue problems,2008,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-48249153015&doi=10.1145%2f1377596.1377600&partnerID=40&md5=d2352e32328d05dbb396890cca4c61b4,"We describe an efficient implementation and present a performance study of an automated multi-level substructuring (AMLS) method for sparse eigenvalue problems. We assess the time and memory requirements associated with the key steps of the algorithm, and compare it with the shift-and-invert Lanczos algorithm. Our eigenvalue problems come from two very different application areas: accelerator cavity design and normal-mode vibrational analysis of polyethylene particles. We show that the AMLS method, when implemented carefully, outperforms the traditional method in broad application areas when large numbers of eigenvalues are sought, with relatively low accuracy. © 2008 ACM.",Multilevel substructuring; Performance evaluation; Sparse eigenvalue problems,Boolean functions; Switching systems; AMLS method; Automated multi level substructuring (AMLS); Broad application; Cavity designs; Efficient implementation; Eigenvalue problem (EVP); Eigenvalues (of graphs); Lanczos algorithms; Memory requirements; Performance study; Polyethylene particles; Vibrational analysis; Eigenvalues and eigenfunctions
On the failure of rank-revealing QR factorization software - A case study,2008,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-48849110060&doi=10.1145%2f1377612.1377616&partnerID=40&md5=1c26424d789cbb9865922d78f43438ee,"This article reports an unexpected and rather erratic behavior of the LAPACK software implementation of the QR factorization with Businger-Golub column pivoting. It is shown that, due to finite precision arithmetic, the software implementation of the factorization can catastrophically fail to produce a properly structured triangular factor, thus leading to a potentially severe underestimate of a matrix's numerical rank. The 30-year old problem, dating back to LINPACK, has (undetectedly) badly affected many computational routines and software packages, as well as the study of rank-revealing QR factorizations. We combine computer experiments and numerical analysis to isolate, analyze, and fix the problem. Our modification of the current LAPACK xGEQP3 routine is already included in the LAPACK 3.1.0 release. The modified routine is numerically more robust and with a negligible overhead. We also provide a new, equally efficient, and provably numerically safe partial-column norm-updating strategy. © 2008 ACM.",Pivoting; QR factorization; Rank-revealing,Columns (structural); Computer software; Numerical analysis; Software packages; Case studies; Computational routines; Computer experiments; Finite-precision arithmetics; LINPACK; Numerical rank; Pivoting; QR factorization; QR factorizations; Rank-revealing; Software implementations; Updating strategy; Factorization
Hybrid differentiation strategies for simulation and analysis of applications in C++,2008,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-48849087295&doi=10.1145%2f1377603.1377604&partnerID=40&md5=ef6725f5d1fd10230e9719e7e7ad67e3,"Computationally efficient and accurate derivatives are important to the success of many different types of numerical methods. Automatic differentation (AD) approaches compute truncation-free derivatives and can be efficient in many cases. Although present AD tools can provide a convenient implementation mechanism, the computational efficiency rarely compares to analytically derived versions that have been carefully implemented. The focus of this work is to combine the strength of these methods into a hybrid strategy that attempts to achieve an optimal balance of implementation and computational efficiency by selecting the appropriate components of the target algorithms for AD and analytical derivation. Although several AD approaches can be considered, our focus is on the use of template overloading forward AD tools in C++ applications. We demonstrate this hybrid strategy for a system of partial differential equations in gas dynamics. These methods apply however to other systems of differentiable equations, including DAEs and ODEs. © 2008 ACM.",Automatic differentiation; Euler equations; Finite volume methods; Hybrid differentiation methods; Template overloading,Computer programming languages; Difference equations; Differential equations; Differentiation (calculus); Gas dynamics; Model structures; Numerical methods; Thermal spraying; AD tools; Analytical derivation; Applications.; Automatic differentiation; Computationally efficient; Euler equations; Finite volume methods; Hybrid differentiation methods; Hybrid strategies; Optimal balance; Simulation and analysis; Template overloading; Partial differential equations
Algorithm 876: Solving fredholm integral equations of the second kind in MATLAB,2008,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-48249127983&doi=10.1145%2f1377596.1377601&partnerID=40&md5=385dc918d816cc0f5d5020af5bd261f9,"We present here the algorithms and user interface of a MATLAB program, Fie, that solves numerically Fredholm integral equations of the second kind on an interval [a, b] to a specified, modest accuracy. The kernel function K(s, t) is moderately smooth on [a, b] × [a, b] except possibly across the diagonal s = t. If the interval is finite, Fie provides for kernel functions that behave in a variety of ways across the diagonal, that is, K(s, t) may be smooth, have a discontinuity in a low-order derivative, have a logarithmic singularity, or have an algebraic singularity. Fie also solves a large class of integral equations with moderately smooth kernel function on [0, ∞). © 2008 ACM.",MATLAB; Numerical solution,MATLAB; User interfaces; Fredholm integral equations; Kernel functions; Log arithmic singularity; MATLAB programs; Integral equations
Using mixed precision for sparse matrix computations to enhance the performance while achieving 64-bit accuracy,2008,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-35548992612&doi=10.1145%2f1377596.1377597&partnerID=40&md5=3dba121084fd82df1aaa49d3a00fcd23,"By using a combination of 32-bit and 64-bit floating point arithmetic, the performance of many sparse linear algebra algorithms can be significantly enhanced while maintaining the 64-bit accuracy of the resulting solution. These ideas can be applied to sparse multifrontal and supernodal direct techniques and sparse iterative techniques such as Krylov subspace methods. The approach presented here can apply not only to conventional processors but also to exotic technologies such as Field Programmable Gate Arrays (FPGA), Graphical Processing Units (GPU), and the Cell BE processor. © 2008 ACM.",Floating point; Iterative refinement; Linear systems; Precision,Algebra; Field programmable gate arrays (FPGA); Learning algorithms; Linear algebra; Matrix algebra; Field programmable gate array (FPGA); Floating point arithmetics; Graphical processing units (GPU); Iterative techniques; Krylov subspace methods; Sparse linear algebra; Sparse matrix computations; Iterative methods
Algorithm 878: Exact VARMA likelihood and its gradient for complete and incomplete data with Matlab,2008,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-48849091994&doi=10.1145%2f1377603.1377609&partnerID=40&md5=4f8c80d61f89cce64e95ec3640d4acfa,"Matlab functions for the evaluation of the exact log-likelihood of VAR and VARMA time series models are presented (vector autoregressive moving average). The functions accept incomplete data, and calculate analytical gradients, which may be used in parameter estimation with numerical likelihood maximization. Allowance is made for possible savings when estimating seasonal, structured or distributed lag models. Also provided is a function for creating simulated VARMA time series that have an accurate distribution from term one (they are spin-up free). The functions are accompanied by a a simple example driver, a program demonstrating their use for real parameter fitting, as well as a test suite for verifying their correctness and aid further development. The article concludes with description of numerical results obtained with the algorithm. © 2008 ACM.",ARMA; Exact likelihood function; Incomplete data; Missing values; VARMA; Vector autoregressive moving average model,Gaussian noise (electronic); MATLAB; Software testing; Time series; ARMA; Exact likelihood function; Incomplete data; Missing values; VARMA; Vector autoregressive moving average model; Autoregressive moving average model
High-performance implementation of the level-3 BLAS,2008,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-48849089104&doi=10.1145%2f1377603.1377607&partnerID=40&md5=4dc984abd7a50b5cf2d136d7f5925bc4,A simple but highly effective approach for transforming high-performance implementations on cache-based architectures of matrix-matrix multiplication into implementations of other commonly used matrix-matrix computations (the level-3 BLAS) is presented. Exceptional performance is demonstrated on various architectures. © 2008 ACM.,Basic linear algebra subprograms; Libraries; Linear algebra; Matrix-matrix operations,Computer software; Libraries; Linear algebra; Software engineering; Basic linear algebra subprograms; Cache-based architectures; Effective approaches; High performance implementations; Level-3 BLAS; Matrix computation; Matrix matrix multiplications; Matrix operations; Matrix algebra
Families of algorithms related to the inversion of a symmetric positive definite matrix,2008,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-48849099645&doi=10.1145%2f1377603.1377606&partnerID=40&md5=afe3b949b3fe85474bbd63cdc1aed992,"We study the high-performance implementation of the inversion of a Symmetric Positive Definite (SPD) matrix on architectures ranging from sequential processors to Symmetric MultiProcessors to distributed memory parallel computers. This inversion is traditionally accomplished in three ""sweeps"": a Cholesky factorization of the SPD matrix, the inversion of the resulting triangular matrix, and finally the multiplication of the inverted triangular matrix by its own transpose. We state different algorithms for each of these sweeps as well as algorithms that compute the result in a single sweep. One algorithm outperforms the current ScaLAPACK implementation by 20-30 percent due to improved load-balance on a distributed memory architecture. © 2008 ACM.",Inversion; Libraries; Linear algebra; Symmetric positive definite,Libraries; Linear algebra; Memory architecture; Cholesky factorizations; Distributed memory architecture; Distributed-memory parallel computers; High performance implementations; Inversion; Symmetric multi-processors; Symmetric positive definite; Symmetric positive definite matrices; Matrix algebra
Algorithm 882: Near-best fixed pole rational interpolation with applications in spectral methods,2008,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-48849097469&doi=10.1145%2f1377612.1377618&partnerID=40&md5=dac2174a0a7f84be8dd87483c1cf02d5,"We present a numerical procedure to compute the nodes and weights in rational Gauss-Chebyshev quadrature formulas. Under certain conditions on the poles, these nodes are near best for rational interpolation with prescribed poles (in the same sense that Chebyshev points are near best for polynomial interpolation). As an illustration, we use these interpolation points to solve a differential equation with an interior boundary layer using a rational spectral method. The algorithm to compute the interpolation points (and, if required, the quadrature weights) is implemented as a Matlab program. © 2008 ACM.",Quadrature; Rational interpolation,Boundary layers; Differential equations; MATLAB; Poles; Polynomials; Spectroscopy; Interpolation points; Numerical procedures; Polynomial interpolation; Quadrature; Quadrature formula; Quadrature weights; Rational interpolation; Spectral methods; Interpolation
Updating an LU factorization with pivoting,2008,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-48849086742&doi=10.1145%2f1377612.1377615&partnerID=40&md5=9fa70e52311217858b3de54f8ef85ac6,"We show how to compute an LU factorization of a matrix when the factors of a leading principle submatrix are already known. The approach incorporates pivoting akin to partial pivoting, a strategy we call incremental pivoting. An implementation using the Formal Linear Algebra Methods Environment (FLAME) application programming interface (API) is described. Experimental results demonstrate practical numerical stability and high performance on an Intel Itanium2 processor-based server. © 2008 ACM.",Linear systems; LU factorization; Pivoting; Updating,Factorization; Linear algebra; Linear systems; Formal linear algebra methods environments; Intel Itanium2; LU factorization; Pivoting; Submatrix; Updating; Application programming interfaces (API)
OpenAD/F: A modular open-source tool for automatic differentiation of fortran codes,2008,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-42149157630&doi=10.1145%2f1377596.1377598&partnerID=40&md5=0747bf112b5e03b06a92da0cf8879ea3,"The Open/ADF tool allows the evaluation of derivatives of functions defined by a Fortran program. The derivative evaluation is performed by a Fortran code resulting from the analysis and transformation of the original program that defines the function of interest. Open/ADF has been designed with a particular emphasis on modularity, flexibility, and the use of open source components. While the code transformation follows the basic principles of automatic differentiation, the tool implements new algorithmic approaches at various levels, for example, for basic block preaccumulation and call graph reversal. Unlike most other automatic differentiation tools, Open/ADF uses components provided by the Open/AD framework, which supports a comparatively easy extension of the code transformations in a language-independent fashion. It uses code analysis results implemented in the OpenAnalysis component. The interface to the language-independent transformation engine is an XML-based format, specified through an XML schema. The implemented transformation algorithms allow efficient derivative computations using locally optimized cross-country sequences of vertex, edge, and face elimination steps. Specifically, for the generation of adjoint codes, Open/ADF supports various code reversal schemes with hierarchical checkpointing at the subroutine level. As an example from geophysical fluid dynamics, a nonlinear time-dependent scalable, yet simple, barotropic ocean model is considered. OpenAD/F's reverse mode is applied to compute sensitivities of some of the model's transport properties with respect to gridded fields such as bottom topography as independent (control) variables. © 2008 ACM.",Adjoint compiler; Automatic differentiation; Source transformation,Arsenic compounds; BASIC (programming language); Codes (standards); Computer programming languages; Cosine transforms; Derivatives; Differentiation (calculus); Dynamics; Fluid dynamics; Fluid mechanics; FORTRAN (programming language); Information management; Linguistics; Markup languages; Mathematical transformations; Reusability; Sensitivity analysis; Subroutines; Transport properties; XML; Adjoint codes; Automatic differentiation (AD); Automatic differentiation (AD) tools; Barotropic; Basic blocks; Basic principles; Bottom topography; Call graphs; Check-pointing; Code analysis; Code transformations; Derivative evaluations; FORTRAN (Language); FORTRAN codes; Geophysical fluid dynamics; Ocean modelling; Open source components; Open sources; Reverse mode; Time-dependent; Transformation algorithms; Transformation engines; XML-Schema; Codes (symbols)
SNOBFIT - Stable noisy optimization by branch and fit,2008,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-48849107878&doi=10.1145%2f1377612.1377613&partnerID=40&md5=b9633e54004d57cb59fd376445679b8d,"The software package SNOBFIT for bound-constrained (and soft-constrained) noisy optimization of an expensive objective function is described. It combines global and local search by branching and local fits. The program is made robust and flexible for practical use by allowing for hidden constraints, batch function evaluations, change of search regions, etc. © 2008 ACM.",Branch-and-bound; Derivative-free; Expensive function values; Hidden constraints; Noisy function values; Parallel evaluation; Soft constraints; Surrogate model,Branch and bound method; Constrained optimization; Derivative-free; Expensive function values; Hidden constraints; Noisy function values; Parallel evaluation; Soft constraint; Surrogate model; Function evaluation
Algorithm 880: A testing infrastructure for symmetric tridiagonal eigensolvers,2008,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-48849105175&doi=10.1145%2f1377603.1377611&partnerID=40&md5=55afbbf607570a88b40e874eb35426a4,"LAPACK is often mentioned as a positive example of a software library that encapsulates complex, robust, and widely used numerical algorithms for a wide range of applications. At installation time, the user has the option of running a (limited) number of test cases to verify the integrity of the installation process. On the algorithm developer's side, however, more exhaustive tests are usually performed to study algorithm behavior on a variety of problem settings and also computer architectures. In this process, difficult test cases need to be found that reflect particular challenges of an application or push algorithms to extreme behavior. These tests are then assembled into a comprehensive collection, therefore making it possible for any new or competing algorithm to be stressed in a similar way. This article describes an infrastructure for exhaustively testing the symmetric tridiagonal eigensolvers implemented in LAPACK. It consists of two parts: a selection of carefully chosen test matrices with particular idiosyncrasies and a portable testing framework that allows for easy testing and data processing. The tester facilitates experiments with algorithmic choices, parameter and threshold studies, and performance comparisons on different architectures. © 2008 ACM.",Accuracy; Design; Eigenvalues; Eigenvectors; Implementation; LAPACK; Numerical software; Performance; Symmetric matrix; Test matrices; Testing,Application programs; Computer architecture; Data handling; Design; Eigenvalues and eigenfunctions; Matrix algebra; Testing; Accuracy; Eigenvalues; Implementation; LAPACK; Numerical software; Performance; Symmetric matrices; Test matrix; Software testing
Algorithm 881: A set of flexible GMRES routines for real and complex arithmetics on high-performance computers,2008,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-48849087656&doi=10.1145%2f1377612.1377617&partnerID=40&md5=be5bf2784ff3832f66563331c2b50e2b,"In this article we describe our implementations of the FGMRES algorithm for both real and complex, single and double precision arithmetics suitable for serial, shared-memory, and distributed-memory computers. For the sake of portability, simplicity, flexibility, and efficiency, the FGMRES solvers have been implemented in Fortran 77 using the reverse communication mechanism for the matrix-vector product, the preconditioning, and the dot-product computations. For distributed-memory computation, several orthogonalization procedures have been implemented to reduce the cost of the dot-product calculation, which is a well-known bottleneck of efficiency for Krylov methods. Furthermore, either implicit or explicit calculation of the residual at restart is possible depending on the actual cost of the matrix-vector product. Finally, the implemented stopping criterion is based on a normwise backward error. © 2008 ACM.",Distributed memory; FGMRES; Flexible Krylov methods; High-performance computing; Linear systems; Reverse communication,Efficiency; Linear systems; Memory architecture; Distributed Memory; FGMRES; Flexible Krylov methods; High performance computing; Reverse communication; Matrix algebra
Algorithm 884: A simple Matlab implementation of the Argyris element,2008,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-48849114227&doi=10.1145%2f1377612.1377620&partnerID=40&md5=70f7c661cef08f1d9ee960a4c6e4c821,"In this work we propose a new algorithm to evaluate the basis functions of the Argyris finite element and their derivatives. The main novelty here is an efficient way to calculate the matrix which gives the change of coordinates between the bases of the Argyis element for the reference and for an arbitrary triangle. This matrix is factored as the product of two rectangular matrices with a strong block structure which makes their computation very easy. We show and comment on an implementation of this algorithm in Matlab. Two numerical experiments, an interpolation of a smooth function on a triangle and the finite-element solution of the Dirichlet problem for the biLaplacian, are presented in the last section to check the performance of our implementation. © 2008 ACM.",Argyris element; Finite elements; Matlab,Finite element method; MATLAB; Software engineering; Argyris element; Basis functions; Block structures; Dirichlet problem; Finite element solution; Numerical experiments; Rectangular matrix; Smooth functions; Computer software
Evaluating exact VARMA likelihood and its gradient when data are incomplete,2008,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-48849111353&doi=10.1145%2f1377603.1377608&partnerID=40&md5=c456be73ef4ba98316bca282814a630c,A detailed description of an algorithm for the evaluation and differentiation of the likelihood function for VARMA processes in the general case of missing values is presented. The method is based on combining the Cholesky decomposition method for complete data VARMA evaluation and the Sherman-Morrison-Woodbury formula. Potential saving for pure VAR processes is discussed and formulae for the estimation of missing values and shocks are provided. A theorem on the determinant of a low rank update is proved. Matlab implementation of the algorithm is in a companion article. © 2008 ACM.,ARMA; Determinant of low rank update; Exact likelihood function; Incomplete data; Matrix derivative; Matrix differentiation; Missing values; VARMA; Vector autoregressive moving average model,Autoregressive moving average model; Function evaluation; ARMA; Determinant of low rank update; Exact likelihood function; Incomplete data; Matrix derivative; Matrix differentiation; Missing values; VARMA; Vector autoregressive moving average model; Matrix algebra
Algorithm 883: SparsePOP - A sparse semidefinite programming relaxation of polynomial optimization problems,2008,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-48849097802&doi=10.1145%2f1377612.1377619&partnerID=40&md5=32042295e9bc3dde7f5beef844abee17,"SparsePOP is a Matlab implementation of the sparse semidefinite programming (SDP) relaxation method for approximating a global optimal solution of a polynomial optimization problem (POP) proposed by Waki et al. [2006]. The sparse SDP relaxation exploits a sparse structure of polynomials in POPs when applying ""a hierarchy of LMI relaxations of increasing dimensions"" Lasserre [2006]. The efficiency of SparsePOP to approximate optimal solutions of POPs is thus increased, and larger-scale POPs can be handled. © 2008 ACM.",Global optimization; Matlab software package; Polynomial optimization problem; Semidefinite programming relaxation; Sparsity; Sums-of-squares optimization,Global optimization; MATLAB; Optimal systems; MATLAB software package; Polynomial optimization problem; Semi-definite programming relaxations; Sparsity; Sums of squares optimization; Polynomials
Algorithm 879: EIGENTEST - A test matrix generator for large-scale eigenproblems,2008,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-48849109520&doi=10.1145%2f1377603.1377610&partnerID=40&md5=3bce6788ea8d4041eb167ec18f7a46d0,"Eigentest is a package that produces real test matrices with known eigensystems. A test matrix, called an eigenmat, is generated in a factored form, in which the user can specify the eigenvalues and has some control over the condition of the eigenvalues and eigenvectors. An eigenmat A of order n requires only O(n) storage for its representation. Auxiliary programs permit the computation of (A - sI)b, (A - sI)Tb, (A - sI)-1 b, and (A - sI)-T b in O(n) operations. A special routine computes specified eigenvectors of an eigenmat and the condition of its eigenvalue. Thus eigenmats are suitable for testing algorithms based on Krylov sequences, as well as others based on matrix-vector products. This article introduces the eigenmat and describes implementations in Fortran 77, Fortran 95, C, and Matlab. © 2008 ACM.",Eigensystem; Test matrix generation,FORTRAN (programming language); MATLAB; Well testing; Eigen-problems; Eigensystem; Eigenvalues; Eigenvalues and eigenvectors; Matrix-vector products; Test matrix; Test matrix generation; Testing algorithm; Eigenvalues and eigenfunctions
Algorithm 877: A subroutine package for cylindrical functions of complex order and nonnegative argument,2008,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-48249103114&doi=10.1145%2f1377596.1377602&partnerID=40&md5=7d0cb3779c1112399fce29fca2f3efc6,"The algorithm presented provides a package of subroutines for calculating the cylindrical functions Jν(x), Nν(x), H ν(1)(x), Hν(2)(x) where the order ν is complex and the real argument x is nonnegative. The algorithm is written in Fortran 95 and calculates the functions using single, double, or quadruple precision according to the value of a parameter defined in the algorithm. The methods of calculating the functions are based on a series expansion, Debye's asymptotic expansions, Olver's asymptotic expansions, and recurrence methods (Miller's algorithms). The relative errors of the functional values computed by this algorithm using double precision are less than 2.4×10-13 in the region 0 ≤ Re ν ≤ 64, 0 ≤ Im ν ≤ 63, 0.024 ≤ x ≤ 97. © 2008 ACM.",Complex order; Cylindrical functions; Nonnegative argument; Numerical calculation,Subroutines; Cylindrical functions; Boolean functions
Efficient contouring on unstructured meshes for partial differential equations,2008,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-48249137740&doi=10.1145%2f1377596.1377599&partnerID=40&md5=2d7ad228b5c6e539cc00d9b00ab58aff,"We introduce three fast contouring algorithms for visualizing the solution of partial differential equations based on the PCI (pure cubic interpolant). The PCI is a particular piecewise bicubic polynomial interpolant defined over an unstructured mesh. Unlike standard contouring approaches, our contouring algorithms do not need a fine-structured approximation and work efficiently with the original scattered data. The basic idea is to first identify the intersection points between contour curves and the sides of each triangle and then draw smooth contour curves connecting these points. We compare these contouring algorithms with the built-in Matlab contour procedure and other contouring algorithms. We demonstrate that our algorithms are both more accurate and faster than the others. © 2008 ACM.",Contour; PDE; Scattered data; Unstructured mesh; Visualization,Algorithms; Computational fluid dynamics; Computer peripheral equipment; Difference equations; Differential equations; Differentiation (calculus); Image segmentation; Interfaces (computer); MATLAB; Mesh generation; Partial differential equations; Polynomial approximation; Standards; Basic idea; Interpolant; Intersection points; Partial differential; Scattered data; Triangle (CO); Unstructured meshes; Approximation algorithms
Benchmarking domain-specific compiler optimizations for variational forms,2008,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-48849098749&doi=10.1145%2f1377612.1377614&partnerID=40&md5=a29dc53dd73bbedac494974b58d95327,"We examine the effect of using complexity-reducing relations [Kirby et al. 2006] to generate optimized code for the evaluation of finite-element variational forms. The optimizations are implemented in a prototype code named FErari, which has been integrated as an optimizing backend to the FEniCS form compiler, FFC [Kirby and Logg 2006; 2007]. In some cases, FErari provides very little speedup, while in other cases we obtain reduced local operation counts by a factor of as much as 7.9 and speedups for the assembly of the global sparse matrix by as much as a factor of 2.8 (see Figure 9). © 2008 ACM.",Compiler; Complexity-reducing relations; FErari; FFC; Finite element method; Optimization; Variational form,Computer software; Finite element method; Optimization; Software engineering; Compiler; Compiler optimizations; Complexity-reducing relations; Domain specific; FErari; Local operations; Sparse matrices; Variational form; Program compilers
Optimal vertex elimination in single-expression-use graphs,2008,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-48849115131&doi=10.1145%2f1377603.1377605&partnerID=40&md5=7599fc13f2821c27f14ec4381f80e5fa,The source transformation tool for automatic differentiation of Fortran programs ADIFOR uses a preaccumulation technique to speed up tangent-linear codes significantly compared to the standard forward mode. Reverse mode automatic differentiation is applied to all scalar assignments to generate efficient code for the computation of local gradients. It has been well known for some time that reverse mode is not necessarily the optimal choice for the computation of these statement-level gradients as it does not minimize the number of operations required. This article presents an efficient algorithm for the solution of this combinatorial optimization problem. The corresponding software is freely available for downloading on our website. Developers of software for automatic differentiation are invited to integrate the algorithm into their tools. Gradients of scalar multivariate functions can be computed by elimination methods on the linearized computational graph. The combinatorial optimization problem that aims to minimize the number of arithmetic operations performed by the elimination algorithm is known to be NP-complete. In this article we present a polynomial algorithm for solving a relevant subclass of this problem's instances. The proposed method relies on the ability to compute vertex covers in bipartite graphs in polynomial time. A simplified version of this graph algorithm is used in a research prototype of the differentiation- enabled NAGWare Fortran compiler for the preaccumulation of local gradients of scalar assignments in the context of automatic generation of efficient tangent-linear code for numerical programs. © 2008 ACM.,Single-expression-use graph; Vertex elimination,Automatic programming; Combinatorial optimization; FORTRAN (programming language); Graph algorithms; Graph structures; Linear transformations; Mathematical transformations; Optimization; Polynomial approximation; Program compilers; Arithmetic operations; Automatic differentiations; Automatic Generation; Combinatorial optimization problems; Multivariate function; Single-expression-use graph; Source transformation; Vertex elimination; Graph theory
An event-driven method to simulate Filippov systems with accurate computing of sliding motions,2008,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-43049115072&doi=10.1145%2f1356052.1356054&partnerID=40&md5=7e1edc4b300173a596ac0ac113a6e154,"This article describes how to use smooth solvers for simulation of a class of piecewise smooth systems of ordinary differential equations, called Filippov systems, with discontinuous vector fields. In these systems constrained motion along a discontinuity surface (so-called sliding) is possible and requires special treatment numerically. The introduced algorithms are based on an extension to Filippov's method to stabilise the sliding flow together with accurate detection of the entrance and exit of sliding regions. The methods are implemented in a general way in MATLAB and sufficient details are given to enable users to modify the code to run on arbitrary examples. Here, the method is used to compute the dynamics of three example systems, a dry-friction oscillator, a relay feedback system and a model of an oil well drill-string. © 2008 ACM.",Filippov systems; Numerical simulation; Piecewise-smooth differential equations; Sliding solutions,Computer simulation; Numerical methods; Problem solving; Systems analysis; Filippov systems; Piecewise smooth differential equations; Sliding solutions; Ordinary differential equations
Algorithm 874: BACOLR-spatial and temporal error control software for PDEs based on high-order adaptive collocation,2008,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-44249097785&doi=10.1145%2f1356052.1356056&partnerID=40&md5=585de06182a5542cd59d2a2ed450fa76,"In this article we discuss a new software package, BACOLR, for the numerical solution of a general class of time-dependent 1-D PDEs. This package employs high-order adaptive methods in time and space within a method-of-lines approach and provides tolerance control of the spatial and temporal errors. The DAEs resulting from the spatial discretization (based on B-spline collocation) are handled by a substantially modified version of the Runge-Kutta solver, RADAU5. For each time step, the RADAU5 code computes an estimate of the temporal error and requires it to satisfy the user tolerance. After each time step BACOLR then computes a high-order estimate of the spatial error and requires this error estimate to satisfy the user tolerance. BACOLR was developed through a substantial modification of the adaptive method-of-lines package, BACOL. In this article we introduce the BACOLR package and present numerical results to show that the performance of BACOLR is comparable to and in some cases significantly superior to that of BACOL, which was shown in previous work to be more efficient, reliable and robust than other existing codes, especially for problems with solutions exhibiting narrow spikes or boundary layers. © 2008 ACM.",1-D PDEs; Adaptive method-of-lines; Differential-algebraic equations; Numerical software; Runge-Kutta methods; Spatial error control,Differential equations; Error analysis; Runge Kutta methods; Software packages; Differential algebraic equations; Numerical software; Substantial modification; Adaptive algorithms
Anatomy of high-performance matrix multiplication,2008,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-44249094647&doi=10.1145%2f1356052.1356053&partnerID=40&md5=fb88c2d225f3c136b820164ec272f6e9,We present the basic principles that underlie the high-performance implementation of the matrix-matrix multiplication that is part of the widely used GotoBLAS library. Design decisions are justified by successively refining a model of architectures with multilevel memories. A simple but effective algorithm for executing this operation results. Implementations on a broad selection of architectures are shown to achieve near-peak performance. © 2008 ACM.,Basic linear algebra subprogrms; Linear algebra; Matrix multiplication,Computer architecture; Data storage equipment; Digital libraries; Mathematical models; Basic linear algebra subprogrms; Matrix multiplication; Matrix algebra
Algorithm 875: DSDP5-software for semidefinite programming,2008,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-44249088324&doi=10.1145%2f1356052.1356057&partnerID=40&md5=33ee148166cdd141feb21d94c1d64aa4,"DSDP implements the dual-scaling algorithm for semidefinite programming. The source code for this interior-point algorithm, written entirely in ANSI C, is freely available under an open source license. The solver can be used as a subroutine library, as a function within the Matlab environment, or as an executable that reads and writes to data files. Initiated in 1997, DSDP has developed into an efficient and robust general-purpose solver for semidefinite programming. Its features include a convergence proof with polynomially bounded worst-case complexity, primal and dual feasible solutions when they exist, certificates of infeasibility when solutions do not exist, initial points that can be feasible or infeasible, relatively low memory requirements for an interior-point method, sparse and low-rank data structures, extensibility that allows applications to customize the solver and improve its performance, a subroutine library that enables it to be linked to larger applications, scalable performance for large problems on parallel architectures, and a well-documented interface and examples of its use. The package has been used in many applications and tested for efficiency, robustness, and ease of use. © 2008 ACM.",Conic programming; Dual-scaling algorithm; Interior-point methods; Linear matrix inequalities; Semidefinite programming,Computational complexity; Digital libraries; Linear matrix inequalities; MATLAB; Robustness (control systems); Conic programming; Dual scaling algorithms; Interior-point methods; Semidefinite programming; Computer software
Cache efficient bidiagonalization using BLAS 2.5 operators,2008,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-44249127493&doi=10.1145%2f1356052.1356055&partnerID=40&md5=705eb063895622a11a0becd21204f820,"On cache based computer architectures using current standard algorithms, Householder bidiagonalization requires a significant portion of the execution time for computing matrix singular values and vectors. In this paper we reorganize the sequence of operations for Householder bidiagonalization of a general m × n matrix, so that two (_GEMV) vector-matrix multiplications can be done with one pass of the unreduced trailing part of the matrix through cache. Two new BLAS operations approximately cut in half the transfer of data from main memory to cache, reducing execution times by up to 25 per cent. We give detailed algorithm descriptions and compare timings with the current LAPACK bidiagonalization algorithm. © 2008 ACM.",Bidiagonalization; BLAS 2.5; Cache-efficient; Householder reflections; Matrix factorization; Singular values; SVD,Approximation theory; Computer architecture; Mathematical operators; Matrix algebra; Vectors; Bidiagonalization; Cache efficient; Householder reflections; Matrix factorization; Singular values; Buffer storage
Parallel unsymmetric-pattern multifrontal sparse LU with column preordering,2008,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-41149119930&doi=10.1145%2f1326548.1326550&partnerID=40&md5=4f3cfac7812a1f1256231a68885c6fa0,"We present a new parallel sparse LU factorization algorithm and code. The algorithm uses a column-preordering partial-pivoting unsymmetric-pattern multifrontal approach. Our baseline sequential algorithm is based on UMFPACK 4, but is somewhat simpler and is often somewhat faster than UMFPACK version 4.0. Our parallel algorithm is designed for shared-memory machines with a small or moderate number of processors (we tested it on up to 32 processors). We experimentally compare our algorithm with SuperLU_MT, an existing shared-memory sparse LU factorization with partial pivoting. SuperLU_MT scales better than our new algorithm, but our algorithm is more reliable and is usually faster. More specifically, on matrices that are costly to factor, our algorithm is usually faster on up to 4 processors, and is usually faster on 8 and 16. We were not able to run SuperLU_MT on 32. The main contribution of this article is showing that the column-preordering partial-pivoting unsymmetric-pattern multifrontal approach, developed as a sequential algorithm by Davis in several recent versions of UMFPACK, can be effectively parallelized. © 2008 ACM.",Gaussian elimination; Multifrontal; Unsymmetric,Distributed computer systems; Factorization; Matrix algebra; LU factorization; UMFPACK 4; Unsymmetric-pattern multifrontal approach; Parallel algorithms
PyTrilinos: High-performance distributed-memory solvers for Python,2008,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-41149168657&doi=10.1145%2f1326548.1326549&partnerID=40&md5=643047bf750f5f1a5a422f0c66829006,"PyTrilinos is a collection of Python modules that are useful for serial and parallel scientific computing. This collection contains modules that cover serial and parallel dense linear algebra, serial and parallel sparse linear algebra, direct and iterative linear solution techniques, domain decomposition and multilevel preconditioners, nonlinear solvers, and continuation algorithms. Also included are a variety of related utility functions and classes, including distributed I/O, coloring algorithms, and matrix generation. PyTrilinos vector objects are integrated with the popular NumPy Python module, gathering together a variety of high-level distributed computing operations with serial vector operations. PyTrilinos is a set of interfaces to existing, compiled libraries. This hybrid framework uses Python as front-end, and efficient precompiled libraries for all computationally expensive tasks. Thus, we take advantage of both the flexibility and ease of use of Python, and the efficiency of the underlying C++, C, and FORTRAN numerical kernels. Out numerical results show that, for many important problem classes, the overhead required by the Python interpreter is negligible. To run in parallel, PyTrilinos simply requires a standard Python interpreter. The fundamental MPI calls are encapsulated under an abstract layer that manages all interprocessor communications. This makes serial and parallel scripts using PyTrilinos virtually identical. © 2008 ACM.",Direct solvers; Multilevel preconditioners; Nonlinear solvers; Object-oriented programming; Script languages,Distributed computer systems; FORTRAN (programming language); Linear algebra; Natural sciences computing; Parallel processing systems; Direct solvers; Multilevel preconditioners; Nonlinear solvers; Script languages; Object oriented programming
On the design of interfaces to sparse direct solvers,2008,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-41149101595&doi=10.1145%2f1326548.1326551&partnerID=40&md5=397fd485aa1abd5ed4c311f5c94e519b,"We discuss the design of general, flexible, consistent, reusable, and efficient interfaces to software libraries for the direct solution of systems of linear equations on both serial and distributed memory architectures. We introduce a set of abstract classes to access the linear system matrix elements and their distribution, access vector elements, and control the solution of the linear system. We describe a concrete implementation of the proposed interfaces, and report examples of applications and numerical results showing that the overhead induced by the object-oriented design is negligible under typical conditions of usage. We include examples of applications, and we comment on the advantages and limitations of the design. © 2008 ACM.",Direct solver libraries; Distributed linear algebra; Object-oriented design,Digital libraries; Linear systems; Object oriented programming; Problem solving; Direct solver libraries; Distributed linear algebra; Object-oriented design; Interfaces (computer)
Scalable parallelization of FLAME code via the workqueuing model,2008,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-41149134895&doi=10.1145%2f1326548.1326552&partnerID=40&md5=60a93ca48710b2d42aa8df79a3270595,"We discuss the OpenMP parallelization of linear algebra algorithms that are coded using the Formal Linear Algebra Methods Environment (FLAME) API. This API expresses algorithms at a higher level of abstraction, avoids the use loop and array indices, and represents these algorithms as they are formally derived and presented. We report on two implementations of the workqueuing model, neither of which requires the use of explicit indices to specify parallelism. The first implementation uses the experimental taskq pragma, which may influence the adoption of a similar construct into OpenMP 3.0. The second workqueuing implementation is domain-specific to FLAME but allows us to illustrate the benefits of sorting tasks according to their computational cost prior to parallel execution. In addition, we discuss how scalable parallelization of dense linear algebra algorithms via OpenMP will require a two-dimensional partitioning of operands much like a 2D data distribution is needed on distributed memory architectures. We illustrate the issues and solutions by discussing the parallelization of the symmetric rank-k update and report impressive performance on an SGI system with 14 Itanium2 processors. © 2008 ACM.",FLAME; OpenMP; Parallel; Scalability; SMP; Workqueuing,Abstracting; Algorithms; Computational methods; Mathematical models; Program processors; Array indices; FLAME code; Workqueuing model; Linear algebra
Algorithm 873:: LSTRS: MATLAB software for large-scale trust-region subproblems and regularization,2008,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-41149176023&doi=10.1145%2f1326548.1326553&partnerID=40&md5=47c0add6cbb1fc78af492cadadc233a7,"A MATLAB 6.0 implementation of the LSTRS method is presented. LSTRS was described in Rojas et al. [2000]. LSTRS is designed for large-scale quadratic problems with one norm constraint. The method is based on a reformulation of the trust-region subproblem as a parameterized eigenvalue problem, and consists of an iterative procedure that finds the optimal value for the parameter. The adjustment of the parameter requires the solution of a large-scale eigenvalue problem at each step. LSTRS relies on matrix-vector products only and has low and fixed storage requirements, features that make it suitable for large-scale computations. In the MATLAB implementation, the Hessian matrix of the quadratic objective function can be specified either explicitly, or in the form of a matrix-vector multiplication routine. Therefore, the implementation preserves the matrix-free nature of the method. A description of the LSTRS method and of the MATLAB software, version 1.2, is presented. Comparisons with other techniques and applications of the method are also included. A guide for using the software and examples are provided. © 2008 ACM.",ARPACK; Constrained quadratic optimization; Lanczos method; MATLAB; Regularization; Trust-region,Computer software; Constrained optimization; Eigenvalues and eigenfunctions; MATLAB; Problem solving; Constrained quadratic optimization; Hessian matrix; Lanczos method; Trust-region; Algorithms
A grid-free abstraction of the Navier-Stokes equations in Fortran 95/2003,2008,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-38749104305&doi=10.1145%2f1322436.1322438&partnerID=40&md5=b21ffabbe3f60b73ce72d4fe14bc514f,"Computational complexity theory inspires a grid-free abstraction of the Navier-Stokes equations in Fortran 95/2003. A novel complexity analysis estimates that structured programming time grows at least quadratically with the number of program lines. Further analysis demonstrates how an object-oriented strategy focused on mathematical objects renders the quadratic estimate scaleinvariant, so the time required for the limiting factor in program development (debugging) no longer grows as the code grows. Compared to the coordinate-free C++ programming of Grant et al. [2000], grid-free Fortran programming eliminates a layer of procedure calls, eliminates a related need for the C++ template construct, and offers a shorter migration path for Fortran programmers. The grid-free strategy is demonstrated by constructing a physical-space driver for a Fourier-space Navier-Stokes solver. Separating the expression of the continuous mathematical model from the discrete numerics clarifies issues that are otherwise easily conflated. A run-time profile suggests that grid-free design substantially reduces the fraction of the procedures that significantly impact runtime, freeing more code to be structured in ways that reduce development time. Applying Amdahl's law to the total solution time (development time plus run time) leads to a strategy that negligibly impacts development time but achieves 58% of the maximum possible speedup. © 2008 ACM.",Complexity; Computational fluid dynamics; Coordinate-free programming; Fortran; Grid-free programming; Navier-Stokes equations; Scientific computing,Computational complexity; Computational fluid dynamics; Fourier analysis; Mathematical models; Natural sciences computing; Object oriented programming; Coordinate-free programming; Grid-free abstraction; Grid-free programming; Navier Stokes equations
Algorithm 870: A static geometric Medial Axis domain decomposition in 2D Euclidean space,2008,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-38749117575&doi=10.1145%2f1322436.1322440&partnerID=40&md5=a7033666c9b1d2f8418e568604af3e9d,"We present a geometric domain decomposition method and its implementation, which produces good domain decompositions in terms of three basic criteria: (1) The boundary of the subdomains create good angles, that is, angles no smaller than a given tolerance 0, where the value of 0 is determined by the application which will use the domain decomposition. (2) The size of the separator should be relatively small compared to the area of the subdomains. (3) The maximum area of the subdomains should be close to the average subdomain area. The domain decomposition method uses an approximation of a Medial Axis as an auxiliary structure for constructing the boundary of the subdomains (separators). The N-way decomposition is based on the divide and conquer algorithmic paradigm and on a smoothing procedure that eliminates the creation of any new artifacts in the subdomains. This approach produces well shaped uniform and graded domain decompositions, which are suitable for parallel mesh generation. © 2008 ACM.",Delaunay triangulation; Domain decomposition; Parallel mesh generation,Angle measurement; Approximation theory; Boundary conditions; Parallel algorithms; Delaunay triangulation; Domain decomposition; Parallel mesh generation; Computational geometry
Algorithm 872: Parallel 2D constrained Delaunay mesh generation,2008,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-38749143720&doi=10.1145%2f1322436.1322442&partnerID=40&md5=e2517671627ed42e43d4e78cb60790cd,"Delaunay refinement is a widely used method for the construction of guaranteed quality triangular and tetrahedral meshes. We present an algorithm and a software for the parallel constrained Delaunay mesh generation in two dimensions. Our approach is based on the decomposition of the original mesh generation problem into N smaller subproblems which are meshed in parallel. The parallel algorithm is asynchronous with small messages which can be aggregated and exhibits low communication costs. On a heterogeneous cluster of more than 100 processors our implementation can generate over one billion triangles in less than 3 minutes, while the single-node performance is comparable to that of the fastest to our knowledge sequential guaranteed quality Delaunay meshing library (the Triangle). © 2008 ACM.",Delaunay triangulation; Mesh generation; Parallel refinement,Constrained optimization; Mesh generation; Problem solving; Sequential switching; Delaunay triangulation; Parallel refinement; Single-node performances; Parallel algorithms
Algorithm 871: A C/C++ precompiler for autogeneration of multiprecision programs,2008,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-38749117934&doi=10.1145%2f1322436.1322441&partnerID=40&md5=a1e53dbaeda997c300a19c440b9c6804,"In the past decade a number of libraries for multiprecision floating-point arithmetic have been developed. We describe an easy to use, generic C/C++ transcription program or precompiler for the conversion of C or C++ source code into new code that uses a C++ multiprecision library of choice. The precompiler can convert any type in the input source code to another type in the output source code. The input source can be either C or C++ , while the output code generated by the precompiler and using the new types, is C++. The type conversion is based on a simple XML configuration file which is provided by either the developer of the multiprecision library or by the user of the precompiler. The precompiler can also convert to data types with additional features, which are not supported in the types of the source code. Applicability of the precompiler is shown with the successful conversion of large subsets of the GNU Scientific Library and Numerical Recipes. © 2008 ACM.",Conversion; Floating-point; Multiprecision; Precompiler,Algorithms; C (programming language); Digital arithmetic; Object oriented programming; Set theory; XML; Conversion; Floating-points; Multiprecision; Precompilers; Source codes; Program compilers
Computing sparse Hessians with automatic differentiation,2008,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-38749101647&doi=10.1145%2f1322436.1322439&partnerID=40&md5=b49ab0bc87c2b09cba4400cdd7e433a9,"A new approach for computing a sparsity pattern for a Hessian is presented: nonlinearity information is propagated through the function evaluation yielding the nonzero structure. A complexity analysis of the proposed algorithm is given. Once the sparsity pattern is available, coloring algorithms can be applied to compute a seed matrix. To evaluate the product of the Hessian and the seed matrix, a vector version for evaluating second order adjoints is analysed. New drivers of ADOL-C are provided implementing the presented algorithms. Runtime analyses are given for some problems of the CUTE collection. © 2008 ACM.",Automatic differentiation; Second order derivatives; Sparsity pattern,Algorithms; Computational complexity; Control nonlinearities; Matrix algebra; Automatic differentiation; Coloring algorithms; Runtime analysis; Second order derivatives; Sparsity pattern; Differentiation (calculus)
Block variants of Hammarling's method for solving Lyapunov equations,2008,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-38749139292&doi=10.1145%2f1322436.1322437&partnerID=40&md5=9566de1f85a22051e1d1c44b51674154,"This article is concerned with the efficient numerical solution of the Lyapunov equation AT X + XA -C with a stable matrix A and a symmetric positive semidefinite matrix C of possibly small rank. We discuss the efficient implementation of Hammarling's method and propose among other algorithmic improvements a block variant, which is demonstrated to perform significantly better than existing implementations. An extension to the discrete-time Lyapunov equation ATXA - X = -C is also described. © 2008 ACM.",Block algorithm; Lyapunov equation; Numerical solution,Algorithms; Discrete time control systems; Matrix algebra; Numerical methods; Problem solving; Block algorithm; Numerical solution; Lyapunov functions
Algorithm 867: QUADLOG - A package of routines for generating Gauss-related quadrature for two classes of logarithmic weight functions,2007,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34548795959&doi=10.1145%2f1268769.1268774&partnerID=40&md5=b66ee0619b0fabd33d9559bff68951ff,"A collection of subroutines and examples of their uses are described for the quadrature method developed in the companion article. These allow the exact evaluation (up to computer truncation and rounding errors) of integrals of polynomials with two general types of logarithmic weights, and also with the corresponding nonlogarithmic weights. The recurrence coefficients for the related nonclassical orthogonal polynomials with logarithmic weight functions can also be obtained. Tests of accuracy on various platforms are presented. The routines are usable from Fortran, C, and C++ programs conforming to any of at least six international programming-language standards. © 2007 ACM.",EISPACK pythag function; Gamma-function testing; Gauss-Chebyshev quadrature; Gauss-Jacobi quadrature; Gauss-Laguerre quadrature; Gauss-Legendre quadrature; Gauss-type quadrature; Logarithmic integrals; Maple symbolic algebra system; Mehler quadrature,Algorithms; Chebyshev approximation; FORTRAN (programming language); Function evaluation; Integral equations; Jacobian matrices; EISPACK pythag functions; Gamma-function testing; Gauss-Chebyshev quadrature; Gauss-Jacobi quadrature; Gauss-Laguerre quadrature; Gauss-Legendre quadrature; Logarithmic integrals; Maple symbolic algebra systems; Mehler quadrature; Software packages
Efficient Gauss-related quadrature for two classes of logarithmic weight functions,2007,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34548735367&doi=10.1145%2f1268769.1268773&partnerID=40&md5=3976a84bc58e3a44935343e8183387c9,"Integrals with logarithmic singularities are often difficult to evaluate by numerical methods. In this work, a quadrature method is developed that allows the exact evaluation (up to machine accuracy) of integrals of polynomials with two general types of logarithmic weights. The total work for the determination of N nodes and points of the quadrature method is O(N2). Subsequently, integrals can be evaluated with O(N) operations and function evaluations, so the quadrature is efficient. This quadrature method can then be used to generate the nonclassical orthogonal polynomials for weight functions containing logarithms and obtain Gauss and Gauss-related quadratures for these weights. Two algorithms for each of the two types of logarithmic weights that incorporate these methods are given in this paper. © 2007 ACM.",Gauss-Chebyshev quadrature; Gauss-Jacobi quadrature; Gauss-Laguerre quadrature; Gauss-Legendre quadrature; Gauss-related quadrature; Logarithmic integrals; Mehler quadrature; Orthogonal polynomials,Algorithms; Function evaluation; Integral equations; Numerical methods; Polynomials; Gauss-Chebyshev quadrature; Gauss-Jacobi quadrature; Gauss-Laguerre quadrature; Gauss-Legendre quadrature; Gauss-related quadrature; Logarithmic integrals; Mehler quadrature; Orthogonal polynomials; Mathematical programming
An accurate nonuniform fourier transform for SPRITE magnetic resonance imaging data,2007,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34548723951&doi=10.1145%2f1268769.1268770&partnerID=40&md5=2e8e7a584eb952e57f7c193b2f2039a7,"A new algorithm is proposed for computing the discrete Fourier Transform (DFT) of purely phase encoded data acquired during Magnetic Resonance Imaging (MRI) experiments. These experiments use the SPRITE (Single Point Ramped Imaging with T1 Enhancement) method and multiple-point acquisition, sampling data in a nonuniform manner that prohibits reconstruction by fast Fourier transform. The chirp z-transform algorithm of Rabiner, Schafer, and Rader can be combined with phase corrections to compute the DFT of this data to extremely high accuracy. This algorithm outperforms the interpolation methods that are traditionally used to process nonuniform data, both in terms of execution time and in terms of accuracy as compared to the DFT. © 2007 ACM.",Chirp z-transform; Discrete Fourier transform; MRI; Nonuniform Fourier transform; SPRITE,Algorithms; Discrete Fourier transforms; Interpolation; Magnetic resonance imaging; Z transforms; Chirp z-transform; Multiple-point acquisition; Nonuniform Fourier transforms; Image enhancement
Experiences of sparse direct symmetric solvers,2007,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34548731368&doi=10.1145%2f1268769.1268772&partnerID=40&md5=c65db390f15867ff7aa00088839806c1,"We recently carried out an extensive comparison of the performance of state-of-the-art sparse direct solvers for the numerical solution of symmetric linear systems of equations. Some of these solvers were written primarily as research codes while others have been developed for commercial use. Our experiences of using the different packages to solve a wide range of problems arising from real applications were mixed. In this paper, we highlight some of these experiences with the aim of providing advice to both software developers and users of sparse direct solvers. We discuss key features that a direct solver should offer and conclude that while performance is an essential factor to consider when choosing a code, there are other features that a user should also consider looking for that vary significantly between packages. © 2007 ACM.",Direct solvers; Software development; Sparse matrices; Symmetric linear systems,Codes (symbols); Information use; Matrix algebra; Problem solving; Software engineering; Direct solvers; Research codes; Sparse matrices; Symmetric linear systems; Linear systems
Efficient compilation of a class of variational forms,2007,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34548709708&doi=10.1145%2f1268769.1268771&partnerID=40&md5=d6de58b6da7872041d9234103afee060,"We investigate the compilation of general multilinear variational forms over affines simplices and prove a representation theorem for the representation of the element tensor (element stiffness matrix) as the contraction of a constant reference tensor and a geometry tensor that accounts for geometry and variable coefficients. Based on this representation theorem, we design an algorithm for efficient pretabulation of the reference tensor. The new algorithm has been implemented in the FEniCS Form Compiler (FFC) and improves on a previous loop-based implementation by several orders of magnitude, thus shortening compile-times and development cycles for users of FFC. © 2007 ACM.",Automation; BLAS; Compiler; Finite element; Granularity; Loop hoisting; Monomials; Variational form,Algorithms; Automation; Computational geometry; Finite element method; Program compilers; Tensors; Granularity; Loop hoisting; Monomials; Multilinear variational forms; Variational techniques
Algorithm 868: Globally doubly adaptive quadrature - Reliable matlab codes,2007,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34548742922&doi=10.1145%2f1268769.1268775&partnerID=40&md5=01a6fbb2d079bc4f4d9c52a0b5a04c7b,"We discuss how to modify a recently published Matlab code, coteglob, so that the excellent performance this code demonstrates for low and intermediate accuracy requests is retained while the performance is improved for high accuracy requests. coteglob is a globally adaptive code using a 5 and 9 point pair of Newton-Cotes rules. Combining an extended sequence of rules using 5, 9, 17 and 33 points with a doubly adaptive bisection strategy is the main focus of the paper. We also discuss local versus global adaptivity and conclude that globally adaptive codes are to be preferred. Based on this we develop several new globally adaptive codes that all compare favorably both with coteglob, with Matlab's best currently available quadrature software quadl and the general purpose QUADPACK codes dqk15 and dqk21. We include the results from extensive testing using both a Lyness-Kaganove testing technique and a battery test. © 2007 ACM.",Error estimation; Matlab; Newton-Cotes rules; Software; Testing,Adaptive systems; Algorithms; Codes (symbols); Error analysis; Software testing; Adaptive codes; Lyness-Kaganove testing techniques; Newton-Cotes rules; QUADPACK codes; MATLAB
Remark on algorithm 644,2007,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34548294155&doi=10.1145%2f1268776.1268783&partnerID=40&md5=6ee388379483e1ab4e8605c05cee15f3,This remark details correction for errors in the functions which compute the modified Bessel function of the second kind and the log of the gamma function. In both cases these errors cause a loss of precision for a small range of values of the ν argument. These routines are used in the calculation of a number of other functions within the package whose accuracy is thus similarly affected. © 2007 ACM.,Cylindrical functions for nonnegative order and complex argument; Numerical calculation,Bessel functions; Computational methods; Error analysis; Number theory; Numerical methods; Cylindrical functions for nonnegative order and complex argument; Loss of precisions; Numerical calculation; Algorithms
Deal.II - -A general-purpose object-oriented finite element library,2007,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34548277425&doi=10.1145%2f1268776.1268779&partnerID=40&md5=69e2a3b2384b2c14c138b2b9c6744a8a,"An overview of the software design and data abstraction decisions chosen for deal.II, a general purpose finite element library written in C++, is given. The library uses advanced object-oriented and data encapsulation techniques to break finite element implementations into smaller blocks that can be arranged to fit users requirements. Through this approach, deal.II supports a large number of different applications covering a wide range of scientific areas, programming methodologies, and application-specific algorithms, without imposing a rigid framework into which they have to fit. A judicious use of programming techniques allows us to avoid the computational costs frequently associated with abstract object-oriented class libraries. The paper presents a detailed description of the abstractions chosen for defining geometric information of meshes and the handling of degrees of freedom associated with finite element spaces, as well as of linear algebra, input/output capabilities and of interfaces to other software, such as visualization tools. Finally, some results obtained with applications built atop deal.II are shown to demonstrate the powerful capabilities of this toolbox. © 2007 ACM.",Object-orientation; Software design,Algorithms; Computer aided design; Digital libraries; Encapsulation; Linear algebra; User interfaces; Data encapsulation; Software designs; Users requirements; Object oriented programming
A parallel symmetric block-tridiagonal divide-and-conquer algorithm,2007,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34548283447&doi=10.1145%2f1268776.1268780&partnerID=40&md5=67e6a4fe3d45b7aa5f9ff6224fc65db2,"We present a parallel implementation of the block-tridiagonal divide-and-conquer algorithm that computes eigensolutions of symmetric block-tridiagonal matrices to reduced accuracy. In our implementation, we use mixed data/task parallelism to achieve data distribution and workload balance. Numerical tests show that our implementation is efficient, scalable and computes eigenpairs to prescribed accuracy. We compare the performance of our parallel eigensolver with that of the ScaLAPACK divide-and-conquer eigensolver on block-tridiagonal matrices. © 2007 ACM.",Approximate eigenvalues; Block divide-and-conquer; Block-tridiagonal matrix; Mixed data/task parallel,Data reduction; Distribution functions; Eigenvalues and eigenfunctions; Matrix algebra; Parallel algorithms; Approximate eigenvalues; Block divide-and-conquers; Block-tridiagonal matrix; Mixed data/task parallel; Parallel processing systems
Accurate numerical derivatives in MATLAB,2007,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34548210335&doi=10.1145%2f1268776.1268781&partnerID=40&md5=1ef78b703fc54eb5176ccff4055cd6d1,"Complex step differentiation (CSD) is a technique for computing very accurate numerical derivatives in languages that support complex arithmetic. We describe here the development of a CSD package in MATLAB called PMAD. We have extended work done in other languages for scalars to the arrays that are fundamental to MATLAB. This extension raises questions that we have been able to resolve in a satisfactory way. Our goal has been to make it as easy as possible to compute approximate Jacobians in MATLAB that are all but exact. Although PMAD has a fast option for the expert that implements CSD as in previous work, the default is an object-oriented implementation that asks very little of the user. © 2007 ACM.",AD; Complex step differentiation; MATLAB,Approximation algorithms; Differentiation (calculus); Jacobian matrices; Numerical methods; Object oriented programming; Approximate Jacobians; Complex step differentiation; Complex step differentiation (CSD); Numerical derivatives; MATLAB
TestU01: A C library for empirical testing of random number generators,2007,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34548237327&doi=10.1145%2f1268776.1268777&partnerID=40&md5=baf171703a4bcc6b39cd40342d7e6154,"We introduce TestU01, a software library implemented in the ANSI C language, and offering a collection of utilities for the empirical statistical testing of uniform random number generators (RNGs). It provides general implementations of the classical statistical tests for RNGs, as well as several others tests proposed in the literature, and some original ones. Predefined tests suites for sequences of uniform random numbers over the interval (0, 1) and for bit sequences are available. Tools are also offered to perform systematic studies of the interaction between a specific test and the structure of the point sets produced by a given family of RNGs. That is, for a given kind of test and a given class of RNGs, to determine how large should be the sample size of the test, as a function of the generator's period length, before the generator starts to fail the test systematically. Finally, the library provides various types of generators implemented in generic form, as well as many specific generators proposed in the literature or found in widely used software. The tests can be applied to instances of the generators predefined in the library, or to user-defined generators, or to streams of random numbers produced by any kind of device or stored in files. Besides introducing TestU01, the article provides a survey and a classification of statistical tests for RNGs. It also applies batteries of tests to a long list of widely used RNGs. © 2007 ACM.",Random number generators; Random number tests; Statistical software; Statistical test,C (programming language); Computer software; Function evaluation; Problem solving; Statistical tests; Random number generators; Random number tests; Statistical softwares; Random number generation
HpGEM - -A software framework for discontinuous Galerkin finite element methods,2007,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34548268014&doi=10.1145%2f1268776.1268778&partnerID=40&md5=aeb3ad8d023f663c67f76bd47e5fb00c,"hpGEM, a novel framework for the implementation of discontinuous Galerkin finite element methods (FEMs), is described. We present data structures and methods that are common for many (discontinuous) FEMs and show how we have implemented the components as an object-oriented framework. This framework facilitates and accelerates the implementation of finite element programs, the assessment of algorithms, and their application to real-world problems. The article documents the status of the framework, exemplifies aspects of its philosophy and design, and demonstrates the feasibility of the approach with several application examples. © 2007 ACM.",Discontinuous Galerkin methods; Object-oriented programming; PDE; Unstructured mesh,Data structures; Finite element method; Galerkin methods; Object oriented programming; Partial differential equations; Discontinuous Galerkin methods; Finite element programs; Unstructured mesh; Computer software
Algorithm 869: ODRPACK95: A weighted orthogonal distance regression code with bound constraints,2007,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34548232138&doi=10.1145%2f1268776.1268782&partnerID=40&md5=89c0d469c8d231a32cd22ca16b2b5670,"ODRPACK (TOMS Algorithm 676) has provided a complete package for weighted orthogonal distance regression for many years. The code is complete with user selectable reporting facilities, numerical and analytic derivatives, derivative checking, and many more features. The foundation for the algorithm is a stable and efficient trust region Levenberg-Marquardt minimizer that exploits the structure of the orthogonal distance regression problem. ODRPACK95 was created to extend the functionality and usability of ODRPACK. ODRPACK95 adds bound constraints, uses the newer Fortran 95 language, and simplifies the interface to the user called subroutine. © 2007 ACM.",Errors in variables; Fortran 95; Measurement error models; Nonlinear least squares; Orthogonal distance regression; Simple bounds,Error analysis; FORTRAN (programming language); Least squares approximations; Measurement theory; Nonlinear analysis; Regression analysis; Errors in variables; Measurement error models; Nonlinear least squares; Orthogonal distance regressions; Simple bounds; Algorithms
"Algorithm 866: IFISS, a Matlab toolbox for modelling incompressible flow",2007,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34547488514&doi=10.1145%2f1236463.1236469&partnerID=40&md5=13b8be5f445e032b98b9d25a831bc538,"IFISS is a graphical Matlab package for the interactive numerical study of incompressible flow problems. It includes algorithms for discretization by mixed finite element methods and a posteriori error estimation of the computed solutions. The package can also be used as a computational laboratory for experimenting with state-of-the-art preconditioned iterative solvers for the discrete linear equation systems that arise in incompressible flow modelling. A unique feature of the package is its comprehensive nature; for each problem addressed, it enables the study of both discretization and iterative solution algorithms as well as the interaction between the two and the resulting effect on overall efficiency. © 2007 ACM.",Finite elements; Incompressible flow; Iterative solvers; Matlab; Stabilization,Algorithms; Computational methods; Linear equations; Mathematical models; Iterative solution algorithms; Iterative solvers; Incompressible flow
A numerical evaluation of sparse direct solvers for the solution of large sparse symmetric linear systems of equations,2007,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34547483006&doi=10.1145%2f1236463.1236465&partnerID=40&md5=ee2d8fcbe1e510e7e667f3293ca56c01,"In recent years a number of solvers for the direct solution of large sparse symmetric linear systems of equations have been developed. These include solvers that are designed for the solution of positive definite systems as well as those that are principally intended for solving indefinite problems. In this study, we use performance profiles as a tool for evaluating and comparing the performance of serial sparse direct solvers on an extensive set of symmetric test problems taken from a range of practical applications. © 2007 ACM.",Direct solvers; Gaussian elimination; Software; Sparse matrices; Symmetric linear systems,Linear equations; Linear systems; Problem solving; Set theory; Direct solvers; Gaussian elimination; Sparse matrices; Numerical methods
Erratum: Remark on algorithm 515: Generation of a vector from the lexicographical index combinations (ACM Trnasactions on Mathematical Software),2007,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34547430583&doi=10.1145%2f1236463.1236470&partnerID=40&md5=11902edd520108e6c4aaa89269b9b39c,[No abstract available],Algorithms,
Using the GA and TAO toolkits for solving large-scale optimization problems on parallel computers,2007,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34547463131&doi=10.1145%2f1236463.1236466&partnerID=40&md5=563ae31914a7454adb337f66d024da73,"Challenges in the scalable solution of large-scale optimization problems include the development of innovative algorithms and efficient tools for parallel data manipulation. This article discusses two complementary toolkits from the collection of Advanced CompuTational Software (ACTS), namely, Global Arrays (GA) for parallel data management and the Toolkit for Advanced Optimization (TAO), which have been integrated to support large-scale scientific applications of unconstrained and bound constrained minimization problems. Most likely to benefit are minimization problems arising in classical molecular dynamics, free energy simulations, and other applications where the coupling among variables requires dense data structures. TAO uses abstractions for vectors and matrices so that its optimization algorithms can easily interface to distributed data management and linear algebra capabilities implemented in the GA library. The GA/TAO interfaces are available both in the traditional library mode and as components compliant with the Common Component Architecture (CCA). We highlight the design of each toolkit, describe the interfaces between them, and demonstrate their use. © 2007 ACM.",Distributed data structures; Lennard-Jones potential; Load balancing; Molecular dynamics; Numerical optimization; One-sided communication,Data structures; Molecular dynamics; Optimization; Problem solving; Distributed data structures; Lennard-Jones potential; Numerical optimization; Parallel processing systems
OPT++: An object-oriented toolkit for nonlinear optimization,2007,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34547421904&doi=10.1145%2f1236463.1236467&partnerID=40&md5=d84c1c6a598ca4a41d9e5807f9dc2434,"Object-oriented programming is a relatively new tool in the development of optimization software. The code extensibility and the rapid algorithm prototyping capability enabled by this programming paradigm promise to enhance the reliability, utility, and ease of use of optimization software. While the use of object-oriented programming is growing, there are still few examples of general purpose codes written in this manner, and a common approach is far from obvious. This paper describes OPT++, a C++ class library for nonlinear optimization. The design is predicated on the concept of distinguishing between an algorithm-independent class hierarchy for nonlinear optimization problems and a class hierarchy for nonlinear optimization methods that is based on common algorithmic traits. The interface is designed for ease of use while being general enough so that new optimization algorithms can be added easily to the existing framework. A number of nonlinear optimization algorithms have been implemented in OPT++ and are accessible through this interface. Furthermore, example applications demonstrate the simplicity of the interface as well as the advantages of a common interface in comparing multiple algorithms. © 2007 ACM.",Object oriented programming; Parallel optimization; Scientific computing,Natural sciences computing; Optimization; Parallel processing systems; Optimization software; Parallel optimization; Object oriented programming
MPFR: A multiple-precision binary floating-point library with correct rounding,2007,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34547422405&doi=10.1145%2f1236463.1236468&partnerID=40&md5=6b0465e568d71b8ce0478bbedb904b9f,"This article presents a multiple-precision binary floating-point library, written in the ISO C language, and based on the GNU MP library. Its particularity is to extend to arbitrary-precision, ideas from the IEEE 754 standard, by providing correct rounding and exceptions. We demonstrate how these strong semantics are achieved - -with no significant slowdown with respect to other arbitrary-precision tools - -and discuss a few applications where such a library can be useful. © 2007 ACM.",Correct rounding; Elementary function; Floating-point arithmetic; IEEE 754 standard; Multiple-precision arithmetic; Portable software,Computer software; Correct rounding; Elementary function; IEEE 754 standard; Multiple-precision arithmetic; Standards
SIPs: Shift-and-invert parallel spectral transformations,2007,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34547490767&doi=10.1145%2f1236463.1236464&partnerID=40&md5=1583443dc9366995e7a2ab887aed8ca2,"SIPs is a new efficient and robust software package implementing multiple shift-and-invert spectral transformations on parallel computers. Built on top of SLEPc and PETSc, it can compute very large numbers of eigenpairs for sparse symmetric generalized eigenvalue problems. The development of SIPs is motivated by applications in nanoscale materials modeling, in which the growing size of the matrices and the pathological eigenvalue distribution challenge the efficiency and robustness of the solver. In this article, we present a parallel eigenvalue algorithm based on distributed spectrum slicing. We describe the object-oriented design and implementation techniques in SIPs, and demonstrate its numerical performance on an advanced distributed computer. © 2007 ACM.",Parallelism; Sparse eigenvalue computation; Spectral transformation,Eigenvalues and eigenfunctions; Parallel algorithms; Parallel processing systems; Software packages; Sparse eigenvalue computation; Spectral transformation; Distributed computer systems
Algorithm 864: General and robot-packable variants of the three-dimensional bin packing problem,2007,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33947711857&doi=10.1145%2f1206040.1206047&partnerID=40&md5=2cdba0d0c7a0d046382f0cdb3b194aa9,We consider the problem of orthogonally packing a given set of rectangular-shaped boxes into the minimum number of three-dimensional rectangular bins. The problem is NP-hard in the strong sense and extremely difficult to solve in practice. We characterize relevant subclasses of packing and present an algorithm which is able to solve moderately large instances to optimality. Extensive computational experiments compare the algorithm for the three-dimensional bin packing when solving general orthogonal packings and when restricted to robot packings. © 2007 ACM.,Bin packing; Branch-and-bound; Constraint programming; Exact algorithms,Computational complexity; Computer software; Constraint theory; Problem solving; Robotics; Bin packing; Branch-and-bound; Exact algorithms; Algorithms
Algorithm 865: Fortran 95 subroutines for Cholesky factorization in block hybrid format,2007,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33947667914&doi=10.1145%2f1206040.1206048&partnerID=40&md5=82a8b6356efeefac394c020052b1fcf1,"We present subroutines for the Cholesky factorization of a positive-definite symmetric matrix and for solving corresponding sets of linear equations. They exploit cache memory by using the block hybrid format proposed by the authors in a companion article. The matrix is packed into n(n + 1)/2 real variables, and the speed is usually better than that of the LAPACK algorithm that uses full storage (n2 variables). Included are subroutines for rearranging a matrix whose upper or lower-triangular part is packed by columns to this format and for the inverse rearrangement. Also included is a kernel subroutine that is used for the Cholesky factorization of the diagonal blocks since it is suitable for any positive-definite symmetric matrix that is small enough to be held in cache. We provide a comprehensive test program and simple example programs. © 2007 ACM.",BLAS; Cholesky factorization and solution; Linear systems of equations; Novel packed matrix data structures; Positive definite matrices; Real symmetric matrices; Recursive algorithms,Algorithms; Buffer storage; Computational complexity; FORTRAN (programming language); Linear equations; Matrix algebra; BLAS; Cholesky factorization; Linear systems of equations; Novel packed matrix data structures; Positive definite matrices; Real symmetric matrices; Recursive algorithms; Subroutines
Robust and reliable defect control for Runge-Kutta methods,2007,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33947702166&doi=10.1145%2f1206040.1206041&partnerID=40&md5=bebc0dbdbf03c3e62571b50dbaac58f7,"The quest for reliable integration of initial value problems (IVPs) for ordinary differential equations (ODEs) is a long-standing problem in numerical analysis. At one end of the reliability spectrum are fixed stepsize methods implemented using standard floating point, where the onus lies entirely with the user to ensure the stepsize chosen is adequate for the desired accuracy. At the other end of the reliability spectrum are rigorous interval-based methods, that can provide provably correct bounds on the error of a numerical solution. This rigour comes at a price, however: interval methods are generally two to three orders of magnitude more expensive than fixed stepsize floating-point methods. Along the spectrum between these two extremes lie various methods of different expense that estimate and control some measure of the local errors and adjust the stepsize accordingly. In this article, we continue previous investigations into a class of interpolants for use in Runge-Kutta methods that have a defect function whose qualitative behavior is asymptotically independent of the problem being integrated. In particular the point, in a step, where the maximum defect occurs as h → 0 is known a priori. This property allows the defect to be monitored and controlled in an efficient and robust manner even for modestly large stepsizes. Our interpolants also have a defect with the highest possible order given the constraints imposed by the order of the underlying discrete formula. We demonstrate the approach on three Runge-Kutta methods of orders 5, 6, and 8, and provide Fortran and preliminary Matlab interfaces to these three new integrators. We also consider how sensitive such methods are to roundoff errors. Numerical results for four problems on a range of accuracy requests are presented. © 2007 ACM.",Initial value problems; Interpolation; Ordinary differential equations; Runge-Kutta methods,Asymptotic stability; Error analysis; Initial value problems; Interpolation; MATLAB; Ordinary differential equations; Problem solving; Floating-point methods; Interval-based methods; Reliability spectrum; Runge Kutta methods
"FILTRANE, a Fortran 95 filter-trust-region package for solving nonlinear least-squares and nonlinear feasibility problems",2007,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33947710109&doi=10.1145%2f1206040.1206043&partnerID=40&md5=3becb0e99a156ccc4dc42f243eb36f55,"FILTRANE, a new Fortran 95 package for finding vectors satisfying general sets of nonlinear equations and/or inequalities, is presented. Several algorithmic variants are discussed and extensively compared on a set of CUTEr test problems, indicating that the default variant is both reliable and efficient. This discussion provides a first experimental study of the parameters inherent in filter algorithms. © 2007 ACM.",Filter methods; Nonlinear feasibility; Nonlinear least-squares; Nonlinear systems,Algorithms; Least squares approximations; Nonlinear systems; Parameter estimation; Problem solving; Filter methods; Nonlinear feasibility; Nonlinear least-squares; FORTRAN (programming language)
Using dense storage to solve small sparse linear systems,2007,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33947628789&doi=10.1145%2f1206040.1206045&partnerID=40&md5=64a0ac66072f14f1d69fcd33a2d96c7e,"A data structure is used to build a linear solver specialized for relatively small sparse systems. The proposed solver, optimized for run-time performance at the expense of memory footprint, outperforms widely used direct and sparse solvers for systems with between 100 and 3000 equations. A multithreaded version of the solver is shown to give some speedups for problems with medium fill-in, while it does not give any benefit for very sparse problems. © 2007 ACM.",Linear equations; Sparse unsymmetric matrices,Linear equations; Linear systems; Matrix algebra; Optimization; Problem solving; Dense storage; Sparse systems; Sparse unsymmetric matrices; Data structures
Complex standard functions and their implementation in the CoStLy library,2007,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33947653841&doi=10.1145%2f1206040.1206042&partnerID=40&md5=8fb2a37d40d025ffc533555245540383,"The practical calculation of range bounds for some complex standard functions is addressed in this article. The functions under consideration are root and power functions, the exponential, trigonometric and hyperbolic functions, and their inverse functions. For such a function f and a given rectangular complex interval z, some interval w is computed that contains all function values of f in z. This is done by expressing the real and the imaginary part of f as compositions of real standard functions and then estimating the ranges of these compositions. In many cases, the inclusions are optimal, such that w is the smallest rectangular interval containing the range of f. The algorithms presented in this article have been implemented in a C++ class library called CoStLy (Complex Standard Functions License), which is distributed under the conditions of the GNU General Public License. © 2007 ACM.",Complex standard functions,Algorithms; Computational complexity; Computer programming languages; Computer software; Inverse problems; CoStLy library; Hyperbolic functions; Inverse functions; Functions
EXPINT - A MATLAB package for exponential integrators,2007,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33947657481&doi=10.1145%2f1206040.1206044&partnerID=40&md5=b0ae340fba57684cb6f4d29ed41cb7c6,"Recently, a great deal of attention has been focused on the construction of exponential integrators for semilinear problems. In this article we describe a MATLAB1 package which aims to facilitate the quick deployment and testing of exponential integrators, of Runge - Kutta, multistep, and general linear type. A large number of integrators are included in this package along with several well-known examples. The so-called φ functions and their evaluation is crucial for accuracy, stability, and efficiency of exponential integrators, and the approach taken here is through a modification of the scaling and squaring technique, the most common approach used for computing the matrix exponential. © 2007 ACM.",Exponential integrators for semilinear problems; MATLAB; Multistep methods; Runge-Kutta methods; Scaling and squaring technique,Computer software; Problem solving; Runge Kutta methods; Scalability; Software engineering; Exponential integrators for semilinear problems; Multistep methods; Scaling and squaring technique; MATLAB
"Algorithm 863: L2WPMA, a Fortran 77 package for weighted least-squares piecewise monotonic data approximation",2007,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33947651739&doi=10.1145%2f1206040.1206046&partnerID=40&md5=16a14bcc11026e520bfec99165016158,"Fortran software is developed that calculates a best piecewise monotonic approximation to n univariate data contaminated by random errors. The underlying method minimizes the weighted sum of the squares of the errors by requiring k - 1 sign changes in the first divided differences of the approximation, where k is a given positive integer. Hence, the piecewise linear interpolant to the fit consists of k monotonic sections, alternately increasing and decreasing. This calculation can have about O(nk) local minima, because the positions of the turning points of the fit are integer variables of the problem. The method, however, by employing a dynamic programming technique divides the data into at most k disjoint sets of adjacent data and solves a k = 1 problem (monotonic fit or isotonic regression) for each set. So it calculates efficiently a global solution in only O(nσ + kσ2) computer operations when k 3, where σ is the number of local minima of the data, always bounded by n/2. This complexity reduces to only O(n) when k = 1 or k = 2 (unimodal case). At the end of the calculation a spline representation of the solution and the corresponding Lagrange multipliers are provided. The software package has been tested on a variety of data sets showing a performance that does provide in practice shorter computation times than the complexity indicates in theory. An application of the method on identifying turning points and monotonic trends of data from 1947 - 1996 on the U.K. pound over the U.S. dollar exchange rate is presented. Generally, the method may have useful applications as, for example, in estimating the turning points of a function from some noisy measurements of its values, or in image and signal processing, or in providing a preliminary or complementary smoothing phase to further analyses of the data. © 2007 ACM.",Approximation; Data smoothing; Divided difference; Dynamic programming; Fitting; Histogram; Image processing; Isotonic regression; Lagrange multipliers; Peak finding; Piecewise monotonic; Pound/dollar exchange rate; Signal processing; Spline; Turning point,Approximation theory; Dynamic programming; Image processing; Lagrange multipliers; Least squares approximations; Signal processing; Data smoothing; Divided difference; Histograms; Isotonic regression; Peak finding; Piecewise monotonic; Turning points; Algorithms
Algorithm 862: MATLAB tensor classes for fast algorithm prototyping,2006,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33845525246&doi=10.1145%2f1186785.1186794&partnerID=40&md5=f2b0021921409465916499e4713f0495,"Tensors (also known as multidimensional arrays or N-way arrays) are used in a variety of applications ranging from chemometrics to psychometrics. We describe four MATLAB classes for tensor manipulations that can be used for fast algorithm prototyping. The tensor class extends the functionality of MATLAB's multidimensional arrays by supporting additional operations such as tensor multiplication. The tensor_as_matrix class supports the matricization of a tensor, that is, the conversion of a tensor to a matrix (and vice versa), a commonly used operation in many algorithms. Two additional classes represent tensors stored in decomposed formats: cp_tensor and tucker_tensor. We describe all of these classes and then demonstrate their use by showing how to implement several tensor algorithms that have appeared in the literature. © 2006 ACM.",Higher-order tensors; MATLAB; Multilinear algebra; N-way arrays,Algorithms; Linear algebra; Psychology computing; Rapid prototyping; Chemometrics; Psychometrics; Tensor algorithms; Tensor multiplication; Tensors
The design and implementation of the MRRR algorithm,2006,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33845548230&doi=10.1145%2f1186785.1186788&partnerID=40&md5=6ae9cd94debc0dd1bc8a585d2a26aa4b,"In the 1990's, Dhillon and Parlett devised the algorithm of multiple relatively robust representations (MRRR) for computing numerically orthogonal eigenvectors of a symmetric tridiagonal matrix T with O(n 2) cost. While previous publications related to MRRR focused on theoretical aspects of the algorithm, a documentation of software issues has been missing. In this article, we discuss the design and implementation of the new MRRR version STEGR that will be included in the next LAPACK release. By giving an algorithmic description of MRRR and identifying governing parameters, we hope to make STEGR more easily accessible and suitable for future performance tuning. Furthermore, this should help users understand design choices and tradeoffs when using the code. © 2006 ACM.",Design; Eigenvalues; Eigenvectors; Implementation; LAPACK; Multiple relatively robust representations; Numerical software; Symmetric matrix,Computer software; Eigenvalues and eigenfunctions; Matrix algebra; Optimization; Numerical software; Symmetric matrix; Algorithms
Algorithm 858: Computing infinite range integrals of an arbitrary product of bessel functions,2006,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33750038958&doi=10.1145%2f1186785.1186790&partnerID=40&md5=0333d88922538e320054e200718b0470,"We present an algorithm to compute integrals of the form ∫ ∞ 0 x m ∏ k i = 1J νi(a ix)dx with J νi(x) the Bessel function of the first kind and (real) order ν i. The parameter m is a real number such that ∑ i ν i + m > -1 and the coefficients a i are strictly positive real numbers. The main ingredients in this algorithm are the well-known asymptotic expansion for J νi(x) and the observation that the infinite part of the integral can be approximated using the incomplete Gamma function (a,z). Accurate error estimates are included in the algorithm, which is implemented as a MATLAB program. © 2006 ACM.",Bessel functions; Quadrature,Asymptotic stability; Bessel functions; Error analysis; Integration; Number theory; Parameter estimation; Gamma functions; Quadrature; Real numbers; Algorithms
Algorithm 860: SimpleS - An extension of Freudenthal's simplex subdivision,2006,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33845543731&doi=10.1145%2f1186785.1186792&partnerID=40&md5=967de225d85ac6616848ce6f5672d927,"This article presents a simple efficient algorithm for the subdivision of a d-dimensional simplex in k d simplices, where k is any positive integer number. The algorithm is an extension of Freudenthal's subdivision method. The proposed algorithm deals with the more general case of k d subdivision, and is considerably simpler than the RedRefinementND algorithm for implementation of Freudenthal's strategy. The proposed simplex subdivision algorithm is motivated by a problem in the field of robust control theory: the computation of a tight upper bound of a dynamical system performance index by means of a branch-and-bound algorithm. © 2006 ACM.",Mesh generation; Simplex subdivision,Computational methods; Control theory; Number theory; Robustness (control systems); Freudenthal's subdivision methods; Mesh generation; System performance index; Algorithms
Algorithm 859: BABDCR - A Fortran 90 package for the solution of bordered ABD linear systems,2006,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33845533427&doi=10.1145%2f1186785.1186791&partnerID=40&md5=cef6ce5b1fb2f6006b0b32c8b22576e6,"BABDCR is a package of Fortran 90 subroutines for the solution of linear systems with bordered almost block diagonal coefficient matrices. It is designed to handle matrices with blocks of the same size, that is, having a block upper bidiagonal structure with an additional block in the right upper corner. The algorithm implemented in the package performs cyclic reduction of the coefficient matrix in order to reduce the fill-in due to the corner block. © 2006 ACM.",Bordered almost block diagonal matrices; Cyclic reduction; Linear systems; Numerical solution,Algorithms; Computational geometry; Linear systems; Matrix algebra; Subroutines; Cyclic reduction; Diagonal matrices; FORTRAN (programming language)
Algorithm 857: POLSYS_GLP - A parallel general linear product homotopy code for solving polynomial systems of equations,2006,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33746454263&doi=10.1145%2f1186785.1186789&partnerID=40&md5=13c8595ba74c221203a865c5af1ad346,"Globally convergent, probability-one homotopy methods have proven to be very effective for finding all the isolated solutions to polynomial systems of equations. After many years of development, homotopy path trackers based on probability-one homotopy methods are reliable and fast. Now, theoretical advances reducing the number of homotopy paths that must be tracked and handling singular solutions have made probability-one homotopy methods even more practical. POLSYS_GLP consists of Fortran 95 modules for finding all isolated solutions of a complex coefficient polynomial system of equations. The package is intended to be used on a distributed memory multiprocessor in conjunction with HOMPACK90 (Algorithm 777), and makes extensive use of Fortran 95-derived data types and MPI to support a general linear product (GLP) polynomial system structure. GLP structure is intermediate between the partitioned linear product structure used by POLSYS_PLP (Algorithm 801) and the BKK-based structure used by PHCPACK. The code requires a GLP structure as input, and although finding the optimal GLP structure is a difficult combinatorial problem, generally physical or engineering intuition about a problem yields a very good GLP structure. POLSYS_GLP employs a sophisticated power series end game for handling singular solutions, and provides support for problem definition both at a high level and via hand-crafted code. Different GLP structures and their corresponding Bezout numbers can be systematically explored before committing to root finding. © 2006 ACM.",Chow-Yorke algorithm; Curve tracking; Fixed point; Fortran 95; General linear product; Globally convergent; Homotopy methods; Linear product decomposition; Probability-one; Zero,Codes (symbols); Convergence of numerical methods; Data storage equipment; Multiprocessing systems; Polynomials; Probability; Problem solving; Bezout numbers; Chow Yorke algorithm; Curve tracking; Homotopy methods; Algorithms
Inverting the symmetrical beta distribution,2006,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33845531552&doi=10.1145%2f1186785.1186786&partnerID=40&md5=23bb5be301766526bfbb5a6446589b1a,"We propose a fast algorithm for computing the inverse symmetrical beta distribution. Four series (two around x = 0 and two around x = 1/2) are used to approximate the distribution function, and its inverse is found via Newton's method. This algorithm can be used to generate beta random variates by inversion and is much faster than currently available general inversion methods for the beta distribution. It turns out to be very useful for generating gamma processes efficiently via bridge sampling. © 2006 ACM.",Inversion method; Quantiles; Random variate generation; Symmetrical beta distribution,Approximation theory; Functions; Numerical methods; Statistical methods; Beta distribution; Inversion method; Newton's method; Quantiles; Random variate generation; Algorithms
Block algorithms for reordering standard and generalized schur forms,2006,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33845523724&doi=10.1145%2f1186785.1186787&partnerID=40&md5=ce3c3d06ac5967ed7d23e3256856cba5,"Block algorithms for reordering a selected set of eigenvalues in a standard or generalized Schur form are proposed. Efficiency is achieved by delaying orthogonal transformations and (optionally) making use of level 3 BLAS operations. Numerical experiments demonstrate that existing algorithms, as currently implemented in LAPACK, are outperformed by up to a factor of four. © 2006 ACM.",Deflating subspace; Invariant subspace; Reordering; Schur form,Eigenvalues and eigenfunctions; Mathematical transformations; Deflating subspace; Invariant subspace; Schur form; Algorithms
Algorithm 861: Fortran 90 subroutines for computing the expansion coefficients of Mathieu functions using Blanch's algorithm,2006,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33845546775&doi=10.1145%2f1186785.1186793&partnerID=40&md5=dedef07e3de77d8259319ff4fe3aa387,"A translation to Fortran 90 of Gertrude Blanch's algorithm for computing the expansion coefficients of the series that represent Mathieu functions is presented. Its advantages are portability, higher precision, practicality of use, and extended documentation. In addition, numerical validations and comparisons with other existing methods are presented. © 2006 ACM.",Computation; Mathieu function; Special function; Validation,Algorithms; Computational methods; Computer software portability; Functions; Precision engineering; Subroutines; Blanch's algorithms; Mathieu functions; FORTRAN (programming language)
An out-of-core sparse symmetric-indefinite factorization method,2006,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33749334130&doi=10.1145%2f1163641.1163645&partnerID=40&md5=f73830d388e724990138055c5d6abee5,"We present a new out-of-core sparse symmetric-indefinite factorization algorithm. The most significant innovation of the new algorithm is a dynamic partitioning method for the sparse factor. This partitioning method results in very low I/O traffic and allows the algorithm to run at high computational rates, even though the factor is stored on a slow disk. Our implementation of the new code compares well with both high-performance in-core sparse symmetric-indefinite codes and a high-performance out-of-core sparse Cholesky code. © 2006 ACM.",Out-of-Core; Symmetric-indefinite,Algorithms; Computational complexity; Computational methods; Telecommunication traffic; Computational rates; Dynamic partitioning; Factorization method; Mathematical programming
An object-oriented framework for the development of scalable parallel multilevel preconditioners,2006,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33749351444&doi=10.1145%2f1163641.1163643&partnerID=40&md5=7e2a8b8121ebb795a4d5a687bdbdeb08,"We present the design of a high-performance object-oriented framework that enables the rapid development and usage of efficient, scalable, and portable implementations of multilevel preconditioners for distributed sparse real matrices, in both serial and (massively) parallel environments. The main feature of the proposed framework is the use of several programming paradigms for the different implementation layers, with a strong emphasis on object-oriented classes and operator overloading for the top layer, and optimized FORTRAN and C code for the layers underneath. We describe an implementation of the proposed framework that is based on the ML library, the algebraic multilevel preconditioning package of Trilinos, which supports state-of-the-art parallel smoothed aggregation methods, and can be used to define general algebraic and geometric multilevel and multigrid preconditioners and solvers. The article demonstrates that we can take advantage of object-oriented programming and operator overloading to obtain intuitive, easy-to-read, and easy-to-develop codes that are at the same time efficient and scalable. Several numerical experiments obtained on serial and parallel computers show that the overhead required by the object-oriented layer is very modest, therefore allowing developers to focus on the new algorithms they are developing and testing, rather than on implementation details. © 2006 ACM.",Multilevel preconditiones; Object-oriented,Algorithms; Geometry; Object oriented programming; Optimization; Product design; Software engineering; Multigrid preconditioners; Multilevel preconditiones; Numerical experiments; Parallel processing systems
Algorithm 855: Subroutines for the computation of Mathieu characteristic numbers and their general orders,2006,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33749317260&doi=10.1145%2f1163641.1163646&partnerID=40&md5=136cdc56847e63c92f0bacd3703ca141,"A continued fraction function algorithm is developed to evaluate general-order Mathieu characteristic numbers, and a new technique is presented for evaluating the Mathieu determinant which can be used to compute the order directly. Approximate expressions are developed to estimate the orders and Mathieu characteristic numbers for the root, finding algorithms. The algorithms, with minor modifications, were used for computing Mathieu coefficients of general order. The algorithms can deal with a large range of Mathieu characteristic number c, real and complex order v, and parameter h. © 2006 ACM.",Elliptic coordinates; General-order Mathieu functions; Hill's determinant; Mathieu characteristic determinant; Mathieu characteristic numbers; Mathieu coefficients; Mathieu determinant; Mathieu differential equation; Mathieu functions; Wave equation,Algorithms; Approximation theory; Computational complexity; Differential equations; Wave equations; Elliptic coordinates; General-order Mathieu functions; Hill's determinant; Mathieu characteristic determinant; Mathieu characteristic numbers; Mathieu coefficients; Mathieu determinant; Mathieu differential equations; Mathieu functions; Number theory
Algorithm 856: APPSPACK 4.0: Asynchronous parallel pattern search for derivative-free optimization,2006,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33749349565&doi=10.1145%2f1163641.1163647&partnerID=40&md5=33ec9045683f967be2e425700926043b,"APPSPACK is software for solving unconstrained and bound-constrained optimization problems. It implements an asynchronous parallel pattern search method that has been specifically designed for problems characterized by expensive function evaluations. Using APPSPACK to solve optimization problems has several advantages: No derivative information is needed; the procedure for evaluating the objective function can be executed via a separate program or script; the code can be run serially or in parallel, regardless of whether the function evaluation itself is parallel; and the software is freely available. We describe the underlying algorithm, data structures, and features of APPSPACK version 4.0, as well as how to use and customize the software. © 2006 ACM.",Parallel derivative-free optimization; Pattern search,Algorithms; Boundary value problems; Computer software; Data structures; Optimization; Pattern recognition; Objective function; Parallel derivative-free optimization; Pattern search; Parallel processing systems
A compiler for variational forms,2006,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33749341848&doi=10.1145%2f1163641.1163644&partnerID=40&md5=240621d62b09353285094956cfc9977f,"As a key step towards a complete automation of the finite element method, we present a new algorithm for automatic and efficient evaluation of multilinear variational forms. The algorithm has been implemented in the form of a compiler, the FEniCS Form Compiler (FFC). We present benchmark results for a series of standard variational forms, including the incompressible Navier-Stokes equations and linear elasticity. The speedup compared to the standard quadrature-based approach is impressive; in some cases the speedup is as large as a factor of 1000. © 2006 ACM.",Automation; Compiler; Finite element; Variational form,Algorithms; Automation; Elasticity; Finite element method; Navier Stokes equations; Finite elements; Quadrature; Variational form; Variational forms; Program compilers
N-body simulations: The performance of some integrators,2006,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33749317983&doi=10.1145%2f1163641.1163642&partnerID=40&md5=7e85bdcbc6bb0cbf13a6bdca260a4226,"We describe four challenging N-body test problems involving the Sun and planets and use them to compare the performance of nine nonsymplectic and two symplectic integrators. Each problem has a long interval of integration and two have non-Newtonian gravitational interactions. The emphasis in our comparison is on the accuracy of the solution, including the phase information produced by nonsympletic methods; the symplectic methods have been included to provide a contrast. Long intervals of integration necessitate small local error tolerances for the nonsymplectic integrators. Among variable-stepsize integrators, RKHINT requires the least CPU time on the two problems with Newtonian interactions and DIVA the least CPU time on the other two problems for the intervals of integration we used. We find that the error growth on some integrations is noticeably slower than predicted by asymptotic analysis of the truncation and round-off error. Our comparisons suggest that the numerical solutions near the end of a billion year simulation in double precision with variable-stepsize nonsymplectic methods would have poor accuracy. © 2006 ACM.",Accurate integration; N-body test problems; Solar system,Asymptotic stability; Computer simulation; Error analysis; Mathematical models; Numerical analysis; Solar system; Accurate integration; Asymptotic analysis; N-body test problems; Software engineering
"Accumulating householder transformations, revisited",2006,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33746075581&doi=10.1145%2f1141885.1141886&partnerID=40&md5=40069468836a7ca6813093b865d77c88,"A theorem related to the accumulation of Householder transformations into a single orthogonal transformation known as the compact WY transform is presented. It provides a simple characterization of the computation of this transformation and suggests an alternative algorithm for computing it. It also suggests an alternative transformation, the UT transform, with the same utility as the compact WY Transform which requires less computation and has similar stability properties. That alternative transformation was first published over a decade ago but has gone unnoticed by the community. © 2006 ACM.",Compact WY transform; Householder transformation; Linear algebra; QR factorization,Algorithms; Computation theory; Linear algebra; Theorem proving; Compact WY transform; Householder transformations; QR factorization; Mathematical transformations
Error bounds from extra-precise iterative refinement,2006,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33746084108&doi=10.1145%2f1141885.1141894&partnerID=40&md5=1ad984246d7b5b609460e09595d64328,"We present the design and testing of an algorithm for iterative refinement of the solution of linear equations where the residual is computed with extra precision. This algorithm was originally proposed in 1948 and analyzed in the 1960s as a means to compute very accurate solutions to all but the most ill-conditioned linear systems. However, two obstacles have until now prevented its adoption in standard subroutine libraries like LAPACK: (1) There was no standard way to access the higher precision arithmetic needed to compute residuals, and (2) it was unclear how to compute a reliable error bound for the computed solution. The completion of the new BLAS Technical Forum Standard has essentially removed the first obstacle. To overcome the second obstacle, we show how the application of iterative refinement can be used to compute an error bound in any norm at small cost and use this to compute both an error bound in the usual infinity norm, and a componentwise relative error bound. We report extensive test results on over 6.2 million matrices of dimensions 5,10,100, and 1000. As long as a normwise (componentwise) condition number computed by the algorithm is less than 1/max{10, √n}εw, the computed normwise (componentwise) error bound is at most 2 max{10, √n}·εw, and indeed bounds the true error. Here, n is the matrix dimension and εw = 2-24 is the working precision. Residuals were computed in double precision (53 bits of precision). In other words, the algorithm always computed a tiny error at negligible extra cost for most linear systems. For worse conditioned problems (which we can detect using condition estimation), we obtained small correct error bounds in over 90% of cases. © 2006 ACM.",BLAS; Floating-point arithmetic; LAPACK; Linear algebra,Computation theory; Digital arithmetic; Digital libraries; Error analysis; Estimation; Linear equations; Linear systems; Error bounds; Iterative refinement; Precision arithmetic; Iterative methods
An efficient overloaded implementation of forward mode automatic differentiation in MATLAB,2006,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33746038542&doi=10.1145%2f1141885.1141888&partnerID=40&md5=543a908fe864d7c0bc68c46f2f874ed5,"The MAD package described here facilitates the evaluation of first derivatives of multidimensional functions that are defined by computer codes written in MATLAB. The underlying algorithm is the well-known forward mode of automatic differentiation implemented via operator overloading on variables of the class fmad. The main distinguishing feature of this MATLAB implementation is the separation of the linear combination of derivative vectors into a separate derivative vector class derivvec. This allows for the straightforward performance optimization of the overall package. Additionally, by internally using a matrix (two-dimensional) representation of arbitrary dimension directional derivatives, we may utilize MATLAB's sparse matrix class to propagate sparse directional derivatives for MATLAB code which uses arbitrary dimension arrays. On several examples, the package is shown to be more efficient than Verma's ADMAT package [Verma 1998a]. © 2006 ACM.",Efficient computation of jacobians; MATLAB,Algorithms; Codes (symbols); Difference equations; Matrix algebra; Optimization; Vectors; Dimension arrays; Efficient computation of jacobians; MATLAB; Sparse matrix; Computer software
Algorithm 854: Fortran 77 subroutines for computing the eigenvalues of hamiltonian matrices II,2006,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33746265928&doi=10.1145%2f1141885.1141895&partnerID=40&md5=030ad2b10fb2e0e4ed6a518dda2a7e41,"This article describes Fortran 77 subroutines for computing eigenvalues and invariant subspaces of Hamiltonian and skew-Hamiltonian matrices. The implemented algorithms are based on orthogonal symplectic decompositions, implying numerical backward stability as well as symmetry preservation for the computed eigenvalues. These algorithms are supplemented with balancing and block algorithms which can lead to considerable accuracy and performance improvements. As a by-product, an efficient implementation for computing symplectic QR decompositions is provided. We demonstrate the usefulness of the subroutines for several, practically relevant examples. © 2006 ACM.",Algebraic riccati equation; Eigenvalues; Hamiltonian matrix; Invariant subspaces; Skew-hamiltonian matrix; Symplectio qr decomposition,Algorithms; Computation theory; Mathematical models; Matrix algebra; Algebraic riccati equations; Eigenvalues; Hamiltonian matrix; Invariant subspaces; Skew-hamiltonian matrix; Symplectio qr decomposition; Hamiltonians
"A matlab package for automatically generating runge-kutta trees, order conditions, and truncation error coefficients",2006,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33746081271&doi=10.1145%2f1141885.1141892&partnerID=40&md5=5ba4114d786de506a2e34bce5c9dad27,"In designing parts of Runge-Kutta methods, order conditions and truncation error coefficients (TECs) are needed. Order conditions and TECs are typically presented as a set of trees combined with rules for producing algebraic expressions from the trees. The tree sets are defined recursively and can be generated by hand only for low orders. This article describes a package of Matlab routines for automatically generating Runge-Kutta trees, order conditions, and TECs. The routines are capable of generating Maple code, Matlab code, or LATEX expressions for ODEs or DAEs of index 1 and 2. In producing the package, two theoretical problems are tackled: (a) avoiding the repeated generation of the same tree and (b) the efficient storage of TECs. © 2006 ACM.",Automatic code generation; Order conditions; Runge-kutta methods; Truncation error coefficients,Codes (symbols); Error analysis; Linear algebra; Problem solving; Runge Kutta methods; Automatic code generation; Order conditions; Truncation error coefficients (TEC); Trees (mathematics)
Improving the performance of reduction to hessenberg form,2006,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33746058651&doi=10.1145%2f1141885.1141887&partnerID=40&md5=9db1d971a464210f9b44420f0a607348,"In this article, a modification of the blocked algorithm for reduction to Hessenberg form is presented that improves performance by shifting more computation from less efficient matrix-vector operations to highly efficient matrix-matrix operations. Significant performance improvements are reported relative to the performance achieved by the current LAPACK implementation. © 2006 ACM.",Eigenvalue problems; Linear algebra; Reduction to condensed form,Computation theory; Eigenvalues and eigenfunctions; Matrix algebra; Vectors; Eigenvalue problems; Hessenberg form; Matrix-matrix operations; Reduction to condensed form; Algorithms
Optimizing FIAT with level 3 BLAS,2006,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33746042186&doi=10.1145%2f1141885.1141889&partnerID=40&md5=4407ce9077adbdfee64989892d11b86b,"Our previous work on FIAT (Finite Element Automatic Tabulator) developed a ""computational representation theory"" that allowed us to construct arbitrary order instances of a wide range of finite elements, many of which are infrequently used owing to their associated code complexity. In our present work, we further hone this theory by rephrasing most of the internal operations as linear transformations over finite-dimensional Banach spaces. This additional insight has led to increased code granularity and allowed the use of level 3 BLAS operations. This is both a conceptual and a practical development; as the run-time performance of FIAT has been improved multiple orders of magnitude. © 2006 ACM.",Finite element; Hilbert space; Numerical linear algebra,Codes (symbols); Computation theory; Linear algebra; Mathematical transformations; Numerical methods; Banach spaces; Finite elements; Hilbert space; Numerical linear algebra; Finite element method
Computing machine-efficient polynomial approximations,2006,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33746101079&doi=10.1145%2f1141885.1141890&partnerID=40&md5=b1fb76cbf7533d64ec06cec730f55601,"Polynomial approximations are almost always used when implementing functions on a computing system. In most cases, the polynomial that best approximates (for a given distance and in a given interval) a function has coefficients that are not exactly representable with a finite number of bits. And yet, the polynomial approximations that are actually implemented do have coefficients that are represented with a finite - and sometimes small - number of bits. This is due to the finiteness of the floating-point representations (for software implementations), and to the need to have small, hence fast and/ or inexpensive, multipliers (for hardware implementations). We then have to consider polynomial approximations for which the degree-i coefficient has at most m i fractional bits; in other words, it is a rational number with denominator 2 mi. We provide a general and efficient method for finding the best polynomial approximation under this constraint. Moreover, our method also applies if some other constraints (such as requiring some coefficients to be equal to some predefined constants or minimizing relative error instead of absolute error) are required. © 2006 ACM.",Chebyshev polynomials; Floating-point arithmetic; Linear programming; Minimax approximation; Polynomial approximation; Polytopes,Computation theory; Constraint theory; Digital arithmetic; Function evaluation; Linear programming; Chebyshev polynomials; Floating-point arithmetic; Minimax approximation; Polytopes; Polynomial approximation
Sequential reservoir sampling with a nonuniform distribution,2006,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33746031434&doi=10.1145%2f1141885.1141891&partnerID=40&md5=5fdf7d4acd59fd61ddadd16c699b0344,"We present a simple algorithm that allows sampling from a stream of data items without knowing the number of items in advance and without having to store all items in main memory. The sampling distribution may be general, that is, the probability of selecting a data item i may depend on the individual item. The main advantage of the algorithms is that they have to pass through the data items only once to produce a sample of arbitrary size n. We give different variants of the algorithm for sampling with and without replacement and analyze their complexity. We generalize earlier results of Knuth on reservoir sampling with a uniform sampling distribution. The general distribution considered here allows us to sample an item with a probability equal to the relative weight (or fitness) of the data item within the whole set of items. Applications include heuristic optimization procedures such as genetic algorithms where solutions are sampled from a population with probability proportional to their fitness. © 2006 ACM.",Genetic algorithms; Proportional selection; Random sampling; Reservoir sampling; Roulette wheel simulation; Sequential sampling,Data reduction; Genetic algorithms; Heuristic methods; Optimization; Probability distributions; Set theory; Proportional selection; Random sampling; Reservoir sampling; Roulette wheel simulation; Sequential sampling; Sampling
"FILIB++, A fast interval library supporting containment computations",2006,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33746039708&doi=10.1145%2f1141885.1141893&partnerID=40&md5=fe573872ed34d75029e8c1ab27b45241,"filib++ is an extension of the interval library filib originally developed at the University of Karlsruhe. The most important aim of filib is the fast computation of guaranteed bounds for interval versions of a comprehensive set of elementary functions. filib++ extends this library in two aspects. First, it adds a second mode, the extended mode, that extends the exception-free computation mode (using special values to represent infinities and NaNs known from the IEEE floating-point standard 754) to intervals. In this mode, the so-called containment sets are computed to enclose the topological closure of a range of a function over an interval. Second, our new design uses templates and traits classes to obtain an efficient, easily extendable, and portable C++ library. © 2006 ACM.",C++ class library; Containment computations; Containment sets; Exception free computations; Filib++; Guaranteed numerical results; Interval arithmetic; Interval computations; Templates; Traits class; Validated numerics; Validation,Computation theory; Digital arithmetic; Functions; Numerical methods; Set theory; C++ class library; Containment computations; Containment sets; Exception free computations; Filib++; Interval arithmetic; Interval computations; Templates; Traits class; Validated numerics; Digital libraries
Remark on algorithm 815: FORTRAN subroutines for computing approximate solutions of feedback set problems using GRASP,2006,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745284303&doi=10.1145%2f1132973.1132982&partnerID=40&md5=8d7bd870ca33bf1d581fd64695ec0ee6,We show that the Fortran source code for Algorithm 815 contains an error and we propose a correction. The error may cause the algorithm to generate incorrect results. We also show that the performance of the corrected algorithm can be improved by a minor adjustment in the code. © 2006 ACM.,Feedback set problems; Fortran subroutines; GRASP; Performance,Approximation theory; Computation theory; Computer programming languages; Feedback set problems; Fortran subroutines; GRASP; Algorithms
Constructing memory-minimizing schedules for multifrontal methods,2006,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745263729&doi=10.1145%2f1132973.1132975&partnerID=40&md5=bedcaa0254fb718d73e7ab279fa4fd95,"We are interested in the memory usage of multifrontal methods. Starting from the algorithms introduced by Liu, we propose new schedules to allocate and process tasks that improve memory usage. This generalizes two existing factorization and memory-allocation schedules by allowing a more flexible task allocation together with a specific tree traversal. We present optimal algorithms for this new class of schedules, and demonstrate experimentally their benefit for some real-world matrices from sparse matrix collections where either the active memory or the total memory is minimized. © 2006 ACM.",Liu's algorithm; Memory; Multifrontal methods; Scheduling; Sparse matrices,Algorithms; Matrix algebra; Resource allocation; Memory-allocation schedules; Multifrontal methods; Real-world matrices; Scheduling
"Algorithm 851: CG_descent, a conjugate gradient method with guaranteed descent",2006,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745245573&doi=10.1145%2f1132973.1132979&partnerID=40&md5=867ea0b5eeae23200c1cd498e278eff9,"Recently, a new nonlinear conjugate gradient scheme was developed which satisfies the descent condition g k T dk ≤ - 7/8||gk|| 2 and which is globally convergent whenever the line search fulfills the Wolfe conditions. This article studies the convergence behavior of the algorithm; extensive numerical tests and comparisons with other methods for large-scale unconstrained optimization are given. © 2006 ACM.",Conjugate gradient method; Convergence; Line search; Unconstrained optimization; Wolfe conditions,Algorithms; Convergence of numerical methods; Optimization; Conjugate gradient method; Convergence; Line search; Unconstrained optimizatio; Wolfe conditions; Nonlinear equations
Improved long-period generators based on linear recurrences modulo 2,2006,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745244126&doi=10.1145%2f1132973.1132974&partnerID=40&md5=140147842d1cf772afdfba353247ea51,"Fast uniform random number generators with extremely long periods have been defined and implemented based on linear recurrences modulo 2. The twisted GFSR and the Mersenne twister are famous recent examples. Besides the period length, the statistical quality of these generators is usually assessed via their equidistribution properties. The huge-period generators proposed so far are not quite optimal in this respect. In this article, we propose new generators of that form with better equidistribution and ""bit-mixing"" properties for equivalent period length and speed. The state of our new generators evolves in a more chaotic way than for the Mersenne twister. We illustrate how this can reduce the impact of persistent dependencies among successive output values, which can be observed in certain parts of the period of gigantic generators such as the Mersenne twister. © 2006 ACM.",GFSR linear recurrence modulo 2; Linear feedback shift register; Mersenne twister; Random number generation,Equivalence classes; Optimization; Statistical methods; GFSR linear recurrence modulo 2; Linear feedback shift register; Mersenne twister; Random number generation
"Computing the real parabolic cylinder functions U(a,x), V(a,x)",2006,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745280081&doi=10.1145%2f1132973.1132977&partnerID=40&md5=ec8f9cfe516a39cab91aa46eff1880a4,"Methods for the computation of real parabolic cylinder functions U(a, x) and V(a, x) and their derivatives are described. We give details on power series, asymptotic series, recursion and quadrature. A combination of these methods can be used for computing parabolic cylinder functions for unrestricted values of the order a and the variable x except for the overflow/underflow limitations. By factoring the dominant exponential factor, scaled functions can be computed without practical overflow/underflow limitations. In an accompanying article we describe the precise domains for these methods and we present the Fortran 90 codes for the computation of these functions. © 2006 ACM.",Asymptotic expansions; Nonoscillating integral representations; Numerical quadrature; Parabolic cylinder functions,Asymptotic stability; Codes (symbols); Computational methods; Asymptotic expansions; Nonoscillating integral representations; Numerical quadrature; Parabolic cylinder functions; Function evaluation
Nonorthogonal decomposition of binary matrices for bounded-error data compression and analysis,2006,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745052298&doi=10.1145%2f1132973.1132976&partnerID=40&md5=95a2317f0f50cd9c60b7565fc80b7cad,"This article presents the design and implementation of a software tool, PROXIMUS, for error-bounded approximation of high-dimensional binary attributed datasets based on nonorthogonal decomposition of binary matrices. This tool can be used for analyzing data arising in a variety of domains ranging from commercial to scientific applications. Using a combination of innovative algorithms, novel data structures, and efficient implementation, PROXIMUS demonstrates excellent accuracy, performance, and scalability to large datasets. We experimentally demonstrate these on diverse applications in association rule mining and DNA microarray analysis. In limited beta release, PROXIMUS currently has over 300 installations in over 10 countries. © 2006 ACM.",Compressing binary-valued vectors; Nonorthogonal matrix decompositions; Semidiscrete decomposition,Algorithms; Approximation theory; Data reduction; Error analysis; Matrix algebra; Compressing binary-valued vectors; Nonorthogonal matrix decompositions; Semidiscrete decomposition; Data structures
Algorithm 852: RealPaver: An interval solver using constraint satisfaction techniques,2006,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745226980&doi=10.1145%2f1132973.1132980&partnerID=40&md5=8baafb138a07257ac9b514fe81a32089,"RealPaver is an interval software for modeling and solving nonlinear systems. Reliable approximations of continuous or discrete solution sets are computed using Cartesian products of intervals. Systems are given by sets of equations or inequality constraints over integer and real variables. Moreover, they may have different natures, being square or nonsquare, sparse or dense, linear, polynomial, or involving transcendental functions. The modeling language permits stating constraint models and tuning parameters of solving algorithms which efficiently combine interval methods and constraint satisfaction techniques. Several consistency techniques (box, hull, and 3B) are implemented. The distribution includes C sources, executables for different machine architectures, documentation, and benchmarks. The portability is ensured by the GNU C compiler. © 2006 ACM.",Constraint satisfaction; Interval arithmetic; Interval Newton; Local consistency; Nonlinear system,Computer software; Constraint theory; Nonlinear systems; Parameter estimation; Polynomial approximation; Constraint satisfaction; Interval arithmetic; Interval Newton; Local consistency; Problem solving
Algorithm 853: An efficient algorithm for solving rank-deficient least squares problems,2006,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745232326&doi=10.1145%2f1132973.1132981&partnerID=40&md5=d4be125ab1e11a00d4300c551aaee099,"Existing routines, such as xGELSY or xGELSD in LAPACK, for solving rank-deficient least squares problems require O(mn 2) operations to solve min ∥b - Ax ∥ where A is an m by re matrix. We present a modification of the LAPACK routine xGELSY that requires O(mnk) operations where k is the effective numerical rank of the matrix A. For low rank matrices the modification is an order of magnitude faster than the LAPACK code. © 2006 ACM.",Least squares; QR factorization; Rank-deficient,Algorithms; Codes (symbols); Least squares approximations; Matrix algebra; Numerical methods; Least squares; QR factorization; Rank-deficient; Problem solving
"Algorithm 850: Real parabolic cylinder functions U(a, x), V(a, x)",2006,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745236646&doi=10.1145%2f1132973.1132978&partnerID=40&md5=5538c72cd0410b1ad05a378aaac2f785,"Fortran 90 programs for the computation of real parabolic cylinder functions are presented. The code computes the functions U(a, x), V(a, x) and their derivatives for real a and x (x ≥ 0). The code also computes scaled functions. The range of computation for scaled PCFs is practically unrestricted. The aimed relative accuracy for scaled functions is better than 510 -14. Exceptions to this accuracy are the evaluation of the functions near their zeros and the error caused by the evaluation of trigonometric functions of large arguments when |a| ≫ x. The routines always give values for which the Wronskian relation for scaled functions is verified with a relative accuracy better than 510 -14. The accuracy of the unsealed functions is also better than 510 -14 for moderate values of x and a (except close to the zeros), while for large x and a the error is dominated by exponential and trigonometric function evaluations. For IEEE standard double precision arithmetic, the accuracy is better than 510 -13 in the computable range of unsealed PCFs (except close to the zeros). © 2006 ACM.",Asymptotic expansions; Numerical quadrature; Parabolic cylinder functions,Algorithms; Asymptotic stability; Computational methods; Error analysis; Asymptotic expansions; Numerical quadrature; Parabolic cylinder functions; Functional assessment
SUNDIALS: Suite of nonlinear and differential/algebraic equation solvers,2005,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745217791&doi=10.1145%2f1089014.1089020&partnerID=40&md5=6458b5f89d991fac2da67003f007525d,"SUNDIALS is a suite of advanced computational codes for solving large-scale problems that can be modeled as a system of nonlinear algebraic equations, or as initial-value problems in ordinary differential or differential-algebraic equations. The basic versions of these codes are called KINSOL, CVODE, and IDA, respectively. The codes are written in ANSI standard C and are suitable for either serial or parallel machine environments. Common and notable features of these codes include inexact Newton-Krylov methods for solving large-scale nonlinear systems; linear multistep methods for time-dependent problems; a highly modular structure to allow incorporation of different preconditioning and/or linear solver methods; and clear interfaces allowing for users to provide their own data structures underneath the solvers. We describe the current capabilities of the codes, along with some of the algorithms and heuristics used to achieve efficiency and robustness. We also describe how the codes stem from previous and widely used Fortran 77 solvers, and how the codes have been augmented with forward and adjoint methods for carrying out first-order sensitivity analysis with respect to model parameters or initial conditions. © 2005 ACM.",Daes; Nonlinear systems; Odes; Sensitivity analysis,Algebra; Codes (symbols); Nonlinear systems; Parallel processing systems; Problem solving; Sensitivity analysis; Daes; Fortran 77 solvers; Newton-Krylov methods; Odes; Nonlinear equations
Algorithm 844: Computing sparse reduced-rank approximations to sparse matrices,2005,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-30544452185&doi=10.1145%2f1067967.1067972&partnerID=40&md5=b9ec0f262be8ccca1154c97c4a0510ab,"In many applications - latent semantic indexing, for example-it is required to obtain a reduced rank approximation to a sparse matrix A. Unfortunately, the approximations based on traditional decompositions, like the singular value and QR decompositions, are not in general sparse. Stewart [(1999), 313-323] has shown how to use a variant of the classical Gram-Schmidt algorithm, called the quasi-Gram-Schmidt-algorithm, to obtain two kinds of low-rank approximations. The first, the SPQR, approximation, is a pivoted, Q-less QR approximation of the form (XR 11 -1)(R11́ R 12), where X consists of columns of A. The second, the SCR approximation, is of the form the form A ≅ XTY T, where X and Y consist of columns and rows A and T, is small. In this article we treat the computational details of these algorithms and describe a MATLAB implementation. © 2005 ACM.",Additional Key Words and Phrases: Sparse approximations; Gram-Schmidt algorithm; MATLAB,Algorithms; Approximation theory; Computational methods; Gram-Schmidt algorithms; MATLAB; Rank approximation; Matrix algebra
Pursuing scalability for hypre's conceptual interfaces,2005,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745210525&doi=10.1145%2f1089014.1089018&partnerID=40&md5=2976b44aa658db038ea87ae1b40a99c7,"The software library hypre provides high-performance preconditioned and solvers for the solution of large, sparse linear systems on massively parallel computers as well as conceptual interfaces that allow users to access the library in the way they naturally think about their problems. These interfaces include a stencil-based structured interface (Struct); a semistructured interface (semiStruct), which is appropriate for applications that are mostly structured, for example, block structured grids, composite grids in structured adaptive mesh refinement applications, and overset grids; and a finite element interface (FEI) for unstructured problems, as well as a conventional linear-algebraic interface (IJ). It is extremely important to provide an efficient, scalable implementation of these interfaces in order to support the scalable solvers of the library, especially when using tens of thousands of processors. This article describes the data structures, parallel implementation, and resulting performance of the IJ, Struct and semiStruct interfaces. It investigates their scalability, presents successes as well as pitfalls of some of the approaches and suggests ways of dealing with them. © 2005 ACM.",Parallel programming; Scalability; User interfaces,Finite element method; Linear algebra; Linear systems; Parallel processing systems; Problem solving; Parallel programming; Scalability; SemiStruct interfaces; User interfaces
ACM Transactions on Mathematical Software: Introduction,2005,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745185972&doi=10.1145%2f1089014.1089015&partnerID=40&md5=41c4ee9692ebac13fd9bd682964d0547,[No abstract available],,
Algorithm 845: EIGIFP: A MATLAB program for solving large symmetric generalized eigenvalue problems,2005,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-30544449884&doi=10.1145%2f1067967.1067973&partnerID=40&md5=5c0bb842dc191348373359b1b8256eb8,eigif p is a MATLAB program for computing a few extreme eigenvalues and eigenvectors of the large symmetric generalized eigenvalue problem Ax = λBx. It is a black-box implementation of an inverse free preconditioned Krylov subspace projection method developed by Golub and Ye [2002]. It has important features that allow it to solve some difficult problems without any input from users. It is particularly suitable for problems where preconditioning by the standard shift-and-invert transformation is not feasible. © 2005 ACM.,Additional Key Words and Phrases: Krylov subspace methods; Eigenvalue; Generalized eigenvalue problem; Preconditioning,Algorithms; Problem solving; Generalized eigenvalue problem; MATLAB program; Preconditioning; Eigenvalues and eigenfunctions
Algorithm 843: Improvements to the Schwarz-Christoffel toolbox for MATLAB,2005,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-30544453141&doi=10.1145%2f1067967.1067971&partnerID=40&md5=fa1d9426d6c68a239bd30986b4b6bcfd,"The Schwarz-Christoffel Toolbox (SC Toolbox) for MATLAB, first released in 1994, made possible the interactive creation and visualization of conformal maps to regions bounded by polygons. The most recent release supports new features, including an object-oriented command-line interface model, new algorithms for multiply elongated and multiple-sheeted regions, and a module for solving Laplace's equation on a polygon with Dirichlet and homogeneous Neumann conditions. Brief examples are given to demonstrate the new capabilities. © 2005 ACM.",Additional Key Words and Phrases: MATLAB; Conformal mapping; Laplace's equation; Polygons; Schwarz-Christoffel,Conformal mapping; Laplace transforms; Object oriented programming; Neumann conditions; Polygons; Algorithms
A fully portable high performance minimal storage hybrid format cholesky algorithm,2005,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-30544437857&doi=10.1145%2f1067967.1067969&partnerID=40&md5=d9a425c909d6a952ff5caa9c4bd4d9e9,"We consider the efficient implementation of the Cholesky solution of symmetric positive-definite dense linear systems of equations using packed storage. We take the same starting point as that of LINPACK and LAPACK, with the upper (or lower) triangular part of the matrix stored by columns. Following LINPACK and LAPACK, we overwrite the given matrix by its Cholesky factor. We consider the use of a hybrid format in which blocks of the matrices are held contiguously and compare this to the present LAPACK code. Code based on this format has the storage advantages of the present code but substantially outperforms it. Furthermore, it compares favorably to using conventional full format (LAPACK) and using the recursive format of Andersen et al. [2001]. © 2005 ACM.",Additional Key Words and Phrases: BLAS; Cholesky factorization and solution; Complex Hermitian matrices; Linear systems of equations; Novel packed-matrix data structures; Positive-definite matrices; Real symmetric matrices; Recursive algorithms,Algorithms; Matrix algebra; Linear system of equations; Real symmetric matrices; Recursive algorithms; Linear systems
SLEPC: A scalable and flexible toolkit for the solution of eigenvalue problems,2005,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745196196&doi=10.1145%2f1089014.1089019&partnerID=40&md5=1a761e22430ff65df5451ccf685e6bb4,"The Scalable Library for Eigenvalue Problem Computations (SLEPc) is a software library for computing a few eigenvalues and associated eigenvectors of a large sparse matrix or matrix pencil. It has been developed on top of PETScand enforces the same programming paradigm. The emphasis of the software is on methods and techniques appropriate for problems in which the associated matrices are sparse, for example, those arising after the discretization of partial differential equations. Therefore, most of the methods offered by the library are projection methods such as Arnoldi or Lanczos, or other methods with similar properties. SLEPc provides basic methods as well as more sophisticated algorithms. It also provides built-in support for spectral transformations such as the shift-and-invert technique. SLEPc is a general library in the sense that it covers standard and generalized eigenvalue problems, both Hermitian and non-Hermitian, with either real or complex arithmetic. SLEPc can be easily applied to real world problems. To illustrate this, several case studies arising from real applications are presented and solved with SLEPc with little programming effort. The addressed problems include a matrix-free standard problem, a complex generalized problem, and a singular value decomposition. The implemented codes exhibit good properties regarding flexibility as well as parallel performance. © 2005 ACM.",Eigenvalue computation; singular values; spectral transform,Codes (symbols); Eigenvalues and eigenfunctions; Matrix algebra; Parallel processing systems; Partial differential equations; Eigenvalue computation; Singular values; Sparse matrices; Spectral transform; Problem solving
An overview of the Advanced CompuTational Software (ACTS) collection,2005,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745197727&doi=10.1145%2f1089014.1089016&partnerID=40&md5=0f51ff4133a055c7d7da6b8ce9150e10,"The ACTS Collection brings together a number of general-purpose computational tools that were developed by independent research projects mostly funded and supported by the U.S. Department of Energy. These tools tackle a number of common computational issues found in many applications, mainly implementation of numerical algorithms, and support for code development, execution, and optimization. In this article, we introduce the numerical tools in the collection and their functionalities, present a model for developing more complex computational applications on top of ACTS tools, and summarize applications that use these tools. Last, we present a vision of the ACTS project for deployment of the ACTS Collection by the computational sciences community. © 2005 ACM.",Computational sciences; High-performance computing,Algorithms; Computation theory; Natural sciences computing; Optimization; Computational sciences; Computational tools; High-performance computing; Computer software
"An overview of SuperLU: Algorithms, implementation, and user interface",2005,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-24344434086&doi=10.1145%2f1089014.1089017&partnerID=40&md5=37671fa6d7217eb44c0d26393ef036dc,"We give an overview of the algorithms, design philosophy, and implementation techniques in the software SuperLU, for solving sparse unsymmetric linear systems. In particular, we highlight the differences between the sequential SuperLU (including its multithreaded extension) and parallel SuperLU_DIST. These include the numerical pivoting strategy, the ordering strategy for preserving sparsity, the ordering in which the updating tasks are performed, the numerical kernel, and the parallelization strategy. Because of the scalability concern, the parallel code is drastically different from the sequential one. We describe the user interfaces of the libraries, and illustrate how to use the libraries most efficiently depending on some matrix characteristics. Finally, we give some examples of how the solver has been used in large-scale scientific applications, and the performance. © 2005 ACM.",Distributed-memory computers; Parallelism; Scalability; Sparse direct solver; Supernodal factorization,Algorithms; Codes (symbols); Distributed computer systems; Parallel processing systems; Problem solving; User interfaces; Distributed-memory computers; Parallelism; Scalability; Sparse direct solvers; Supernodal factorization; Computer software
An overview of the trilinos project,2005,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-20044393250&doi=10.1145%2f1089014.1089021&partnerID=40&md5=5c9eaf9954d4ce6913562df8248188f4,"The Trilinos Project is an effort to facilitate the design, development, integration, and ongoing support of mathematical software libraries within an object-oriented framework for the solution of large-scale, complex multiphysics engineering and scientific problems. Trilinos addresses two fundamental issues of developing software for these problems: (i) providing a streamlined process and set of tools for development of new algorithmic implementations and (ii) promoting interoperability of independently developed software. Trilinos uses a two-level software structure designed around collections of packages. A Trilinos package is an integral unit usually developed by a small team of experts in a particular algorithms area such as algebraic preconditioners, nonlinear solvers, etc. Packages exist underneath the Trilinos top level, which provides a common look-and-feel, including configuration, documentation, licensing, and bug-tracking. Here we present the overall Trilinos design, describing our use of abstract interfaces and default concrete implementations. We discuss the services that Trilinos provides to a prospective package and how these services are used by various packages. We also illustrate how packages can be combined to rapidly develop new algorithms. Finally, we discuss how Trilinos facilitates high-quality software engineering practices that are increasingly required from simulation software. © 2005 ACM.",Interfaces; Software framework; Software quality engineering,Algorithms; Computer simulation; Information retrieval systems; Interfaces (computer); Problem solving; Software engineering; Nonlinear solvers; Software framework; Software quality engineering; Trilinos project; Project management
Implementation of hierarchical bases in FEMLAB for simplicial elements,2005,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-30544436873&doi=10.1145%2f1067967.1067968&partnerID=40&md5=483b1e9b026cad9782d363ebfcd78121,"We present the implementation of well-conditioned hierarchical bases for one-dimensional, triangular and tetrahedral elements in finite element FEMLAB software. Using the domain mesh information provided by FEMLAB, we found an easy way to maintain the continuity of solution across the interelement boundaries. The conditionings of the global stiffness matrices of several standard problems are compared with the Lagrange bases and are smaller for all cases. © 2005 ACM.",Additional Key Words and Phrases: FEMLAB; Condition number; Finite element methods; Hierarchical bases,Computer software; Lagrange multipliers; Matrix algebra; Condition number; Hierarchical bases; Lagrange bases; Finite element method
Algorithm 842: A set of GMRES routines for real and complex arithmetics on high performance computers,2005,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-30544434868&doi=10.1145%2f1067967.1067970&partnerID=40&md5=ac5180677098f1d8987dca616963fdcf,"In this article we describe our implementations of the GMRES algorithm for both real and complex, single and double precision arithmetics suitable for serial, shared memory and distributed memory computers. For the sake of portability, simplicity, flexibility and efficiency the GMRES solvers have been implemented in Fortran 77 using the reverse communication mechanism for the matrixvector product, the preconditioning and the dot product computations. For distributed memory computation, several orthogonalization procedures have been implemented to reduce the cost of the dot product calculation, which is a well-known bottleneck of efficiency for the Krylov methods. Either implicit or explicit calculation of the residual at restart are possible depending on the actual cost of the matrix-vector product. Finally the implemented stopping criterion is based on a normwise backward error. © 2005 ACM.",Additional Key Words and Phrases: Linear systems; Distributed memory; GMRES; Krylov methods; Reverse communication,Computer programming languages; Matrix algebra; Vectors; GMRES; Krylov methods; Reverse communication; Algorithms
SlideCont: An Auto97 driver for bifurcation analysis of Filippov systems,2005,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-17644403511&doi=10.1145%2f1055531.1055536&partnerID=40&md5=8bde274418c211c272d768b982f20c1a,"SLIDECONT, an AuTo97 driver for sliding bifurcation analysis of discontinuous piecewise-smooth autonomous systems, known as Filippov systems, is described in detail. Sliding bifurcations are those in which some sliding on the discontinuity boundary is critically involved. The software allows for detection and continuation of codimension-1 sliding bifurcations as well as detection of some codimension-2 singularities, with special attention to planar systems (n = 2). Some bifurcations are also supported for n-dimensional systems. This article gives a brief introduction to Filippov systems, describes the structure of SLIDECONT and all computations supported by SLIDECONT 2.0. Several examples, which are distributed together with the source code of SLIDECONT, are presented. © 2005 ACM.",AUTO97; Filippov systems; Numerical continuation; Piecewise-smooth differential equations; Sliding bifurcations,Algorithms; Bifurcation (mathematics); Boundary conditions; Chaos theory; Differential equations; Ordinary differential equations; Filippov systems; Numerical continuation; Piecewise smooth differential equations; Sliding bifurcations; Computer software
SFCGen: A framework for efficient generation of multi-dimensional space-filling curves by recursion,2005,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-17644387739&doi=10.1145%2f1055531.1055537&partnerID=40&md5=f17eea9f76375b83888075a448a0207f,"Because they are continuous and self-similar, space-filling curves have been widely used in mathematics to transform multi-dimensional problems into one-dimensional forms. For scientific applications, reordering computation by certain space-filling curves can significantly improve data reuse because of the locality properties of these curves. However, when space-filling curves are used in programs for reordering data, traversal or indexing of the curves must be efficient. To address this problem, we present the table-driven framework SFCGen to efficiently generate multi-dimensional space-filling curves on the fly. The framework is general and easy enough to be used in any application that can be partitioned recursively in multiple dimensions. We describe a movement specification table, a universal turtle algorithm to enumerate points along a space-filling curve, a table-based indexing algorithm to transform coordinates of a point into its position along the curve and an algorithm to pregenerate the table automatically. As examples, we show how high-dimensional Hilbert, Morton, and Peano curves and a two-dimensional Sierpiński curve can be generated with our algorithms. We present performance results for Hilbert, Morton, and Peano curves and compare the efficiency of our curve generation algorithm with the most recent work on generating Hilbert curves. Our experimental results on three modern microprocessor-based platforms show that SFCGen performs up to 63% faster than the most recent recursive algorithm on 2D curve generation and up to a factor of 132 faster than two previous byte-oriented non-recursive implementations. On curve indexing, SFCGen performs as much as a factor of three faster than the byte-oriented implementation. Our results on 4D space-filling curves also show that SFCGen scales very well with curve level for higher dimensional spaces. © 2005 ACM.",Space-filling curve,Algorithms; Computer graphics; Computer programming languages; Database systems; Microprocessor chips; Parallel processing systems; Curve generation algorithms; High dimensional spaces; Multidimensional space filling curves; Table based indexing algorithms; Computer software
The science of deriving dense linear algebra algorithms,2005,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-17644412337&doi=10.1145%2f1055531.1055532&partnerID=40&md5=deff84461279511107498674eb9f0f89,"In this article we present a systematic approach to the derivation of families of high-performance algorithms for a large set of frequently encountered dense linear algebra operations. As part of the derivation a constructive proof of the correctness of the algorithm is generated. The article is structured so that it can be used as a tutorial for novices. However, the method has been shown to yield new high-performance algorithms for well-studied linear algebra operations and should also be of interest to those who wish to produce best-in-class high-performance codes. © 2005 ACM.",Formal derivation; High-performance computing; Libraries; Linear algebra,Algorithms; Codes (symbols); Computational methods; Computer aided analysis; Computer programming languages; Formal languages; Software engineering; Formal derivation; High performance algorithms; High performance codes; High performance computing; Linear algebra
Parallel out-of-core computation and updating of the QR factorization,2005,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-17644368925&doi=10.1145%2f1055531.1055534&partnerID=40&md5=02d88da4c7fcef9fe0f483714113408d,"This article discusses the high-performance parallel implementation of the computation and updating of QR factorizations of dense matrices, including problems large enough to require out-of-core computation, where the matrix is stored on disk. The algorithms presented here are scalable both in problem size and as the number of processors increases. Implementation using the Parallel Linear Algebra Package (PLAPACK) and the Parallel Out-of-Core Linear Algebra Package (POOCLAPACK) is discussed. The methods are shown to attain excellent performance, in some cases attaining roughly 80% of the ""realizable"" peak of the architectures on which the experiments were performed. © 2005 ACM.",Dense systems; Linear algebra; Linear least squares,Algorithms; Computational methods; Computer software; Linear algebra; Matrix algebra; Problem solving; Dense systems; High performance computers; Linear least squares; Matrix matrix multiplications; Parallel processing systems
"Algorithm 840: Computation of grid points, quadrature weights and derivatives for spectral element methods using prolate spheroidal wave functions - Prolate elements",2005,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-17644389543&doi=10.1145%2f1055531.1055538&partnerID=40&md5=2144e0e715f1f48990a75b649a898e83,"High order domain decomposition methods using a basis of Legendre polynomials, known variously as ""spectral elements"" or ""p-type finite elements,"" have become very popular. Recent studies suggest that accuracy and efficiency can be improved by replacing Legendre polynomials by prolate spheroidal wave functions of zeroth order. In this article, we explain the practicalities of computing all the numbers needed to switch bases: the grid points x j, the quadrature weights w j, and the values of the prolate functions and their derivatives at the grid points. The prolate functions themselves are computed by a Legendre-Galerkin discretization of the prolate differential equation; this yields a symmetric tridiagonal matrix. The prolate functions are then defined by Legendre series whose coefficients are the eigenfunctions of the matrix eigenproblem. The grid points and weights are found simultaneously through a Newton iteration. For large N and c, the iteration diverges from a first guess of the Legendre-Lobatto points and weights. Fortunately, the variations of the x j and w j with c are well-approximated by a symmetric parabola over the whole range of interest. This makes it possible to bypass the continuation procedures of earlier authors. © 2005 ACM.",P-finite elements; Prolate element method; Prolate spheroidal wavefunctions; Spectral elements,Algorithms; Approximation theory; Differential equations; Eigenvalues and eigenfunctions; Matrix algebra; Polynomials; Legendre Galerkin discretization; Prolate element methods; Prolate spheroidal wavefunctions; Spectral elements; Computer software
Representing linear algebra algorithms in code: The FLAME application program interfaces,2005,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-17644370328&doi=10.1145%2f1055531.1055533&partnerID=40&md5=6a8c5b1d8624e40c75aa68602239ccec,"In this article, we present a number of Application Program Interfaces (APIs) for coding linear algebra algorithms. On the surface, these APIs for the MATLAB M-script and C programming languages appear to be simple, almost trivial, extensions of those languages. Yet with them, the task of programming and maintaining families of algorithms for a broad spectrum of linear algebra operations is greatly simplified. In combination with our Formal Linear Algebra Methods Environment (FLAME) approach to deriving such families of algorithms, dozens of algorithms for a single linear algebra operation can be derived, verified to be correct, implemented, and tested, often in a matter of minutes per algorithm. Since the algorithms are expressed in code much like they are explained in a classroom setting, these APIs become not just a tool for implementing libraries, but also a valuable tool for teaching the algorithms that are incorporated in the libraries. In combination with an extension of the Parallel Linear Algebra Package (PLAPACK) API, the approach presents a migratory path from algorithm to MATLAB implementation to high-performance sequential implementation to parallel implementation. Finally, the APIs are being used to create a repository of algorithms and implementations for linear algebra operations, the FLAME Interface REpository (FIRE), which already features hundreds of algorithms for dozens of commonly encountered linear algebra operations. © 2005 ACM.",Application program interfaces; Formal derivation; High-performance libraries; Linear algebra,Algorithms; C (programming language); Computer software; Linear algebra; Parallel processing systems; User interfaces; Application program interfaces (API); Formal linear algebra methods environment (FLAME); High performance libraries; Linear algebra algorithms; Codes (symbols)
Using AD to solve BVPs in MATLAB,2005,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-17644371666&doi=10.1145%2f1055531.1055535&partnerID=40&md5=d28e6863cd0796fcded8828a9d47402a,"The MATLAB program bvp4c solves two-point boundary value problems (BVPs) of considerable generality. The numerical method requires partial derivatives of several kinds. To make solving BVPs as easy as possible, the default in bvp4c is to approximate these derivatives with finite differences. The solver is more robust and efficient if analytical derivatives are supplied. In this article we investigate how to use automatic differentiation (AD) to obtain the advantages of analytical derivatives without giving up the convenience of finite differences. In bvp4cAD we have approached this ideal by a careful use of the MAD AD tool and some modification of bvp4c. © 2005 ACM.",AD; BVP; Matlab,Approximation theory; Boundary conditions; Boundary value problems; Differentiation (calculus); Finite difference method; Ordinary differential equations; Problem solving; Automatic differentiation (AD); Nonlinear algebric equations; Partial derivatives; Vectorization; Computer software
MATSLISE: A MATLAB package for the numerical solution of Sturm-Liouville and Schrödinger equations,2005,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745223828&doi=10.1145%2f1114268.1114273&partnerID=40&md5=a662678e61bfb61bad3ef9e50e62fe58,"MATSLISE is a graphical MATLAB software package for the interactive numerical study of regular Sturm-Liouville problems, one-dimensional Schrödinger equations, and radial Schrödinger equations with a distorted Coulomb potential. It allows the fast and accurate computation of the eigenvalues and the visualization of the corresponding eigenfunctions. This is realized by making use of the power of high-order piece wise constant perturbation methods, a technique described by Ixaru. For a well-outlined class of problems, the implemented algorithms are more efficient than the well-established SL-solvers SL02F, SLEDGE, SLEIGN, and SLEIGN2, which are included by Pryce in the SLDRIVER code that has been built on top of SLTSTPAK. © 2005 ACM.",Software package; Sohrödinger equations; Sturm-Liouville problems,Algorithms; Computation theory; Eigenvalues and eigenfunctions; Perturbation techniques; Wave equations; Coulomb potential; Schrödinger equations; Software package; Sturm-Liouville problems; Computer software
Algorithm 847: Spinterp: Piecewise multilinear hierarchical sparse grid interpolation in MATLAB,2005,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745213726&doi=10.1145%2f1114268.1114275&partnerID=40&md5=0da39c5b25ed3bc0628d44d379c1160c,"To recover or approximate smooth multivariate functions, sparse grids are superior to full grids due to a significant reduction of the required support nodes. The order of the convergence rate in the maximum norm is preserved up to a logarithmic factor. We describe three possible piecewise multilinear hierarchical interpolation schemes in detail and conduct a numerical comparison. Furthermore, we document the features of our sparse grid interpolation software package spintarp for MATLAB. © 2005 ACM.",Multivariate interpolation; Smolyak algorithm; Sparse grids,Algorithms; Approximation theory; Convergence of numerical methods; Distributed computer systems; Hierarchical systems; Multivariate interpolation; Smolyak algorithm; Sparse grids; Interpolation
DFTI - A new interface for fast fourier transform libraries,2005,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745213973&doi=10.1145%2f1114268.1114271&partnerID=40&md5=f09bf6cce3228a3545e8f191d3bc0d0f,"The Fast Fourier Transform (FFT) algorithm that calculates the Discrete Fourier Transform (DFT) is one of the major breakthroughs in scientific computing and is now an indispensable tool in a vast number of fields. Unfortunately, software applications that provide fast computation of DFT via FFT differ vastly in functionality and lack uniformity. A widely accepted Applications Programmer Interface (API) for DFT would advance the field of scientific computing significantly. In this article, we present the specification of DFTI, a new interface that combines functionality with ease of use. This API is our strawman proposal toward a common interface for DFT calculations. © 2005 ACM.",API; FFT; Interface; Software,Computer software; Natural sciences computing; User interfaces; API; Interface; Software; Software applications; Fast Fourier transforms
Algorithm 841: BHESS: Gaussian reduction to a similar banded Hessenberg form,2005,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-17644420697&doi=10.1145%2f1055531.1055539&partnerID=40&md5=5ee7bb94c3eac140cbc63e2ec6196a46,"BHESS uses Gaussian similarity transformations to reduce a general real square matrix to similar upper Hessenberg form. Multipliers are bounded in root mean square by a user-supplied parameter. If the input matrix is not highly nonnormal and the user-supplied tolerance on multipliers is of a size greater than ten, the returned matrix usually has small upper bandwidth. In such a case, eigenvalues of the returned matrix can be determined by the bulge-chasing BR iteration or by Rayleigh quotient iteration. BHESS followed by BR iteration determines a complete spectrum in about one-fifth the time required for orthogonal reduction to Hessenberg form followed by QR iterations. The FORTRAN 77 code provided for BHESS runs efficiently on a cache-based architecture. © 2005 ACM.",Cache-efficient; Gaussian similarity transformations; Hessenberg form; Matrix eigenvalues; Spectra; Sylvester equation,Codes (symbols); Eigenvalues and eigenfunctions; FORTRAN (programming language); Iterative methods; Mathematical transformations; Matrix algebra; Problem solving; Cache efficient; Gaussian similarity transformations; Hessenberg forms; Matrix eigenvalues; Sylvester equations; Algorithms
A differentiation-enabled Fortran 95 compiler,2005,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745204829&doi=10.1145%2f1114268.1114270&partnerID=40&md5=2ffdceef847b2b49fe887990db0f83e0,"The availability of first derivatives of vector functions is crucial for the robustness and efficiency of a large number of numerical algorithms. An upcoming new version of the differentiation-enabled NAGWare Fortran 95 compiler is described that uses programming language extensions and a semantic code transformation known as automatic differentiation to provide Jacobians of numerical programs with machine accuracy. We describe a new user interface as well as the relevant algorithmic details. In particular, we focus on the source transformation approach that generates locally optimal gradient code for single assignments by vertex elimination in the linearized computational graph. Extensive tests show the superiority of this method over the current overloading-based approach. The robustness and convenience of the new compiler-feature is illustrated by various case studies. © 2005 ACM.",Preaccumulation; Source transformation,Algorithms; Functions; Graph theory; Robustness (control systems); Semantics; Vectors; Code transformation; Computational graph; Preaccumulation; Source transformation; Program compilers
Algorithm 848: A recursive fixed-point algorithm for the infinity-norm case,2005,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745219145&doi=10.1145%2f1114268.1114276&partnerID=40&md5=c9bc47dc9640bad1aae179082596f0ec,"We present the PFix algorithm for approximating a fixed point of a function f that has arbitrary dimensionality, is defined on a rectangular domain, and is Lipschitz continuous with respect to the infinity norm with constant 1. PFix has applications in economics, game theory, and the solution of partial differential equations. PFix computes an approximation that satisfies the residual error criterion, and can also compute an approximation satisfying the absolute error criterion when the Lipschitz constant is less than 1. For functions defined on all rectangular domains, the worst-case complexity of PFix has order equal to the logarithm of the reciprocal of the tolerance, raised to the power of the dimension. Dividing this order expression by the factorial of the dimension yields the order of the worst-case bound for the case of the unit hypercube. PFix is a recursive algorithm, in that it uses solutions to a d-dimensional problem to compute a solution to a (d + 1)-dimensional problem. A full analysis of PFix may be found in Shellman and Sikorski [2003b], and a C implementation is available through ACM ToMS. © 2005 ACM.",Economics; Fixed points; Fixed-points algorithms; Game theory; Nonlinear partial differential equations,Algorithms; Approximation theory; Computation theory; Economics; Game theory; Partial differential equations; Fixed points; Fixed-points algorithms; Nonlinear partial differential equations; Recursive functions
Algorithm 846: MixedVol: A software package for mixed-volume computation,2005,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745183751&doi=10.1016%2fj.jcrysgro.2005.09.034&partnerID=40&md5=607a1dc10d7adbba3fea2977c1dd801f,MixedVol is a C++ software package that computes the mixed volume of n finite subsets of ℤn or the support of a system of n polynomials in n variables. The software produces the mixed volume as well as the mixed cells. The mixed cells are crucial for solving polynomial systems by the polyhedral homotopy continuation method. The software leads existing codes for mixed-volume computation in speed by a substantial margin and its memory requirement is very low. © 2005 ACM.,Mixed volume; Polynomial system; Semimixed structure; Support,Algorithms; C (programming language); Computation theory; Polynomials; Mixed volume; Polynomial system; Semimixed structure; Support; Computer software
PDE.mart: A network-based problem-solving environment for PDEs,2005,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745207521&doi=10.1145%2f1114268.1114272&partnerID=40&md5=451f4a1ec39acecb934f092c2716ae91,"PDE.Mart is a network-based problem-solving environment (PSE) for solving partial differential equations (PDEs) in numerical simulations and academic research, as well as in educational settings. The client-server protocol consists of a Web-browser-enabled graphical user interface, PDE-GUI, that runs on client machines to manage the server connection, geometric and model specifications, computational method selection, and postprocessing; a server system, PDE-Server, to build computational engines and provide PDE solution services on the host machine; and a library, PDE-LIB, that contains building blocks for developing network-based and PDE-oriented PSEs. © 2005 ACM.",Network and grid computing; Partial differential equation; Problem solving environment; Software integration,Client server computer systems; Computational methods; Engineering research; Graphical user interfaces; Problem solving; Web browsers; Computational engines; Network and grid computing; Problem solving environment; Software integration; Partial differential equations
FEMSTER: An object-oriented class library of high-order discrete differential forms,2005,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745194320&doi=10.1145%2f1114268.1114269&partnerID=40&md5=d90e587422854f76e6208de19f7527cc,"FEMSTER is a modular finite element class library for solving three-dimensional problems arising in electromagnetism. The library was designed using a modern geometrical approach based on differential forms (or p-forms) and can be used for high-order spatial discretizations of well-known ℋ(div)- and ℋ(curl)-conforming finite element methods. The software consists of a set of abstract interfaces and concrete classes, providing a framework in which the user is able to add new schemes by reusing the existing classes or by incorporating new user-defined data types. © 2005 ACM.",Computational electromagnetism; High-order finite element; Object-oriented programming; ℋ(div)- and ℋ(curl)-conforming finite element methods,Computational geometry; Digital libraries; Electromagnetism; Object oriented programming; Problem solving; User interfaces; Abstract interfaces; Computational electromagnetism; High-order finite element; ℋ(div)- and ℋ(curl)-conforming finite element methods; Finite element method
Algorithm 849: A concise sparse Cholesky factorization package,2005,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745213090&doi=10.1145%2f1114268.1114277&partnerID=40&md5=7bf9a6dc8cdd1adecba4ff1f69ffda06,"The LDL software package is a set of short, concise routines for factorizing symmetric positive-definite sparse matrices, with some applicability to symmetric indefinite matrices. Its primary purpose is to illustrate much of the basic theory of sparse matrix algorithms in as concise a code as possible, including an elegant method of sparse symmetric factorization that computes the factorization row-by-row but stores it column-by-column. The entire symbolic and numeric factorization consists of less than 50 executable lines of code. The package is written in C, and includes a MATLAB interface. © 2005 ACM.",Cholesky factorization; Linear equations; Sparse matrices,Algorithms; Linear equations; Matrix algebra; Set theory; Cholesky factorization; Sparse matrices; Sparse matrix algorithms; Symmetric factorization; Computer software
Algorithm 838: Airy functions,2004,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-12744271502&doi=10.1145%2f1039813.1039819&partnerID=40&md5=24f281e97a476de0b7ec7e496770553e,"We present a Fortran 90 module, which computes the solutions and their derivatives of Airy's differential equation, both on the real line and in the complex plane. The module also computes the zeros and associated values of the solutions and their derivatives, and the modulus and phase functions on the negative real axis. The computational methods are numerical integration of the differential equation and summation of asymptotic expansions for large argument. These methods were chosen because they are simple, adaptable to any precision, and amenable to rigorous error analysis. The module can be used to validate other codes or as a component in programs that require Airy functions.",Algorithms for special functions; Asymptotic expansions; Differential equations; Error analysis; Real and complex Airy functions; Zeros,Algorithms; Asymptotic stability; Computational methods; Differential equations; Error analysis; FORTRAN (programming language); Laplace transforms; Numerical analysis; Algorithms for special functions; Asymptotic expansions; Complex airy functions; Real airy functions; Zeros; Functions
"Algorithm 839: FIAT, a new paradigm for computing finite element basis functions",2004,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-12744252825&doi=10.1145%2f1039813.1039820&partnerID=40&md5=9e49864fbaba9a242b4bac56ac350987,"Much of finite element computation is constrained by the difficulty of evaluating high-order nodal basis functions. While most codes rely on explicit formulae for these basis functions, we present a new approach that allows us to construct a general class of finite element basis functions from orthonormal polynomials and evaluate and differentiate them at any points. This approach relies on fundamental ideas from linear algebra and is implemented in Python using several object-oriented and functional programming techniques.",Finite elements; High order methods; Linear algebra; Python,Algorithms; Finite element method; Linear algebra; Object oriented programming; Polynomials; Reliability; Vectors; Finite element basis functions; High order methods; Python; Run-time libraries; Functions
BACOL: B-spline adaptive COLlocation software for 1-D parabolic PDEs,2004,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-12744258013&doi=10.1145%2f1039813.1039817&partnerID=40&md5=69d4633cab211f44de437d5ed461ed53,"BACOL is a new, high quality, robust software package in Fortran 77 for solving one-dimensional parabolic PDEs, which has been shown to be significantly more efficient than any other widely available software package of the same class (to our knowledge), especially for problems with solutions exhibiting rapid spatial variation. A novel feature of this package is that it employs high order, adaptive methods in both time and space, controlling and balancing both spatial and temporal error estimates. The software implements a spline collocation method at Gaussian points, with a B-spline basis, for the spatial discretization. The time integration is performed using a modification of the popular DAE solver, DASSL. Based on the computation of a second, higher order, global solution, a high quality a posteriori spatial error estimate is obtained after each successful time step. The spatial error is controlled by a sophisticated new mesh selection algorithm based on an equidistribution principle. In this article we describe the overall structure of the BACOL package, and in particular the modifications to the DASSL package that improve its performance within BACOL. An example is provided in the online Appendix to illustrate the use of the package.",1-D Parabolic PDEs; B-splines; Collocation; Equidistribution principle; High-order; Mesh selection,Computational methods; Computer software; Digital libraries; Numerical analysis; Polynomials; Problem solving; Runge Kutta methods; 1-D parabolic PDEs; B-splines; Collocation; Equidistribution principle; High-order; Mesh selection; Partial differential equations
Efficient scaling for complex division,2004,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-12744258012&doi=10.1145%2f1039813.1039814&partnerID=40&md5=7ea9713841612331c971c7e0ccbde8f4,"We develop a simple method for scaling to avoid overflow and harmful underflow in complex division. The method guarantees that no overflow will occur unless at least one component of the quotient must overflow, otherwise the normwise error in the computed result is at most a few units in the last place. Moreover, the scaling requires only four floating point multiplications and a small amount of integer arithmetic to compute the scale factor. Thus, on many modern CPUs, our method is both safer and faster than Smith's widely used algorithm.",Complex division; Overflow; Underflow,Algorithms; Computer programming languages; Error statistics; Functions; Robustness (control systems); Standards; Complex division; Floating point divisions; Overflow; Underflow; Mathematical techniques
Analysis and applications of Priest's distillation,2004,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-12744250264&doi=10.1145%2f1039813.1039815&partnerID=40&md5=3fa3de21b5a4354c621fead7b94d8b9b,"Correcting an infinite loop in Douglas M. Priest's renormalization algorithm, the theory proved here supports streamlined algorithms to resolve the tablemaker's dilemma for the floating-point computation of real and complex sums and dot-products, properly rounded to the ultimate digit. Applications include computations of areas, volumes, and intersections.",Complex arithmetic; Floating-point arithmetic; Fused multiply-add instruction; Interval arithmetic; Matrix arithmetic; Provable accuracy; Rounding error,Algorithms; Computational methods; Digital arithmetic; Formal logic; Mathematical operators; Photogrammetry; Standardization; Complex arithmetic; Floating-point arithmetic; Fused multiply-add instruction; Interval arithmetic; Matrix arithmetic; Priest distillation; Provable accuracy; Rounding error; Mathematical techniques
Computation of complex airy functions and their zeros using asymptotics and the differential equation,2004,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-12744276032&doi=10.1145%2f1039813.1039818&partnerID=40&md5=582ea9053800dcd5f83dd4f61daafb28,"We describe a method by which one can compute the solutions of Airy's differential equation, and their derivatives, both on the real line and in the complex plane. The computational methods are numerical integration of the differential equation and summation of asymptotic expansions for large argument. We give details involved in obtaining all of the parameter values, and we control the truncation errors rigorously. Using the same computational methods, we describe an algorithm that computes the zeros and associated values of the Airy functions and their derivatives, and the modulus and phase functions on the negative real axis.",Algorithms for special functions; Asymptotic expansions; Differential equations; Error analysis; Real and complex Airy functions; Zeros,Algorithms; Approximation theory; Asymptotic stability; Computational methods; Differential equations; Error analysis; FORTRAN (programming language); Algorithms for special functions; Asymptotic expansions; Complex airy functions; Real airy functions; Zeros; Functions
Automating the implementation of Kalman filter algorithms,2004,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-12744267748&doi=10.1145%2f1039813.1039816&partnerID=40&md5=d2661a93188d5e58f193a4f38d74bb1f,"AUTOFILTER is a tool that generates implementations that solve state estimation problems using Kalman filters. From a high-level, mathematics-based description of a state estimation problem, AUTOFILTER automatically generates code that computes a statistically optimal estimate using one or more of a number of well-known variants of the Kalman filter algorithm. The problem description may be given in terms of continuous or discrete, linear or nonlinear process and measurement dynamics. From this description, AUTOFILTER automates many common solution methods (e.g., linearization, discretization) and generates C or Matlab code fully automatically. AUTOFILTER surpasses toolkit-based programming approaches for Kalman filters because it requires no low-level programming skills (e.g., to ""glue"" together library function calls). AUTOFILTER raises the level of discourse to the mathematics of the problem at hand rather than the details of what algorithms, data structures, optimizations and so on are required to implement it. An overview of AUTOFILTER is given along with an example of its practical application to deep space attitude estimation.",Automatic programming; Code generation; Kalman filters; State estimation,Algorithms; Automatic programming; Automation; Data structures; Decision theory; Differential equations; Encoding (symbols); Functions; Libraries; Mathematical models; State estimation; Code generation; Deep space attitude estimation; Kalman filter algorithms; Library functions; Kalman filtering
A column approximate minimum degree ordering algorithm,2004,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-8344231475&doi=10.1145%2f1024074.1024079&partnerID=40&md5=e53264a638f87487a97a5b7e7ca3e2a4,"Sparse Gaussian elimination with partial pivoting computes the factorization PAQ = LU of a sparse matrix A, where the row ordering P is selected during factorization using standard partial pivoting with row interchanges. The goal is to select a column preordering, Q, based solely on the nonzero pattern of A, that limits the worst-case number of nonzeros in the factorization. The fill-in also depends on P, but Q is selected to reduce an upper bound on the fill-in for any subsequent choice of P. The choice of Q can have a dramatic impact on the number of nonzeros in L and U. One scheme for determining a good column ordering for A is to compute a symmetric ordering that reduces fill-in in the Cholesky factorization of A TA. A conventional minimum degree ordering algorithm would require the sparsity structure of A TA to be computed, which can be expensive both in terms of space and time since A TA may be much denser than A. An alternative is to compute Q directly from the sparsity structure of A; this strategy is used by MATLAB's COLMMD preordering algorithm. A new ordering algorithm, COLAMD, is presented. It is based on the same strategy but uses a better ordering heuristic. COLAMD is faster and computes better orderings, with fewer nonzeros in the factors of the matrix.",Linear equations; Ordering methods; Sparse nonsymmetric matrices,Algorithms; Approximation theory; Computational complexity; Matrix algebra; Optimization; Column appriximation; Factorization; Minimum degree ordering algorithm; Partial pivoting; Pattern recognition
"Algorithm 837: AMD, an approximate minimum degree ordering algorithm",2004,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-2942699190&doi=10.1145%2f1024074.1024081&partnerID=40&md5=e2d43b25491dbaae9e48d4f198ef22fb,AMD is a set of routines that implements the approximate minimum degree ordering algorithm to permute sparse matrices prior to numerical factorization. There are versions written in both C and Fortran 77. A MATLAB interface is included.,Algorithms; Experimentation; Performance,Algorithms; Approximation theory; Interfaces (computer); Iterative methods; Linear equations; Matrix algebra; Mathematical software; Minimum degree; Ordering methods; Sparse matrices; Computer software
"Algorithm 836: COLAMD, a column approximate minimum degree ordering algorithm",2004,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-2942647144&doi=10.1145%2f1024074.1024080&partnerID=40&md5=fc903f70327e8718d290a73c0e45a083,"Two codes are discussed, COLAMD and SYMAMD, that compute approximate minimum degree orderings for sparse matrices in two contexts: (1) sparse partial pivoting, which requires a sparsity preserving column pre-ordering prior to numerical factorization, and (2) sparse Cholesky factorization, which requires a symmetric permutation of both the rows and columns of the matrix being factorized. These orderings are computed by COLAMD and SYMAMD, respectively. The ordering from COLAMD is also suitable for sparse QR factorization, and the factorization of matrices of the form A TA and AA T, such as those that arise in least-squares problems and interior point methods for linear programming problems. The two routines are available both in MATLAB and C-callable forms. They appear as built-in routines in MATLAB Version 6.0.",Linear equations; Ordering methods; Sparse nonsymmetric matrices,Algorithms; Approximation theory; Computational complexity; Iterative methods; Matrix algebra; Problem solving; Built-in routines; Mathematical software; Minimum degree ordering algorithm; Sparse matrices; Linear programming
"A numerical evaluation of HSL packages for the direct solution of large sparse, symmetric linear systems of equations",2004,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-8344277455&doi=10.1145%2f1024074.1024077&partnerID=40&md5=00b40103b8346064e81b1f211b51928a,"In recent years, a number of new direct solvers for the solution of large sparse, symmetric linear systems of equations have been added to the mathematical software library HSL. These include solvers that are designed for the solution of positive-definite systems as well as solvers that are principally intended for solving indefinite problems. The available choice can make it difficult for users to know which solver is the most appropriate for their use. In this study, we use performance profiles as a tool for evaluating and comparing the performance of the HSL solvers on an extensive set of test problems taken from a range of practical applications.",Direct solvers; Gaussian elimination; Software; Sparse matrices; Symmetric linear systems,Computational complexity; Computer software; Information retrieval systems; Matrix algebra; Probability; Problem solving; Direct solvers; Gaussian elimination; Sparse matrices; Symmetric linear systems; Linear systems
Jacobian code generated by source transformation and vertex elimination can be as efficient as hand-coding,2004,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-8344224448&doi=10.1145%2f1024074.1024076&partnerID=40&md5=206f2d748515e6203ff88031896cc458,"This article presents the first extended set of results from ELIAD, a source-transformation implementation of the vertex-elimination Automatic Differentiation approach to calculating the Jacobians of functions defined by Fortran code (Griewank and Reese, Automatic Differentiation of Algorithms: Theory, Implementation, and Application, 1991, pp. 126-135). We introduce the necessary theory in terms of well known algorithms of numerical linear algebra applied to the linear, extended Jacobian system that prescribes the relationship between the derivatives of all variables in the function code. Using an example, we highlight the potential for numerical instability in vertex-elimination. We describe the source transformation implementation of our tool ELIAD and present results from five test cases, four of which are taken from the MINPACK-2 collection (Averick et al, Report ANL/MCS-TM-150, 1992) and for which hand-coded Jacobian codes are available. On five computer/compiler platforms, we show that the Jacobian code obtained by ELIAD is as efficient as hand-coded Jacobian code. It is also between 2 to 20 times more efficient than that produced by current, state of the art, Automatic Differentiation tools even when such tools make use of sophisticated techniques such as sparse Jacobian compression. We demonstrate the effectiveness of reverse-ordered pre-elimination from the (successively updated) extended Jacobian system of all intermediate variables used once. Thereafter, the monotonic forward/reverse ordered eliminations of all other intermediates is shown to be very efficient. On only one test case were orderings determined by the Markowitz or related VLR heuristics found superior. A re-ordering of the statements of the Jacobian code, with the aim of reducing reads and writes of data from cache to registers, was found to have mixed effects but could be very beneficial.",Jacobian; Source transformation; Vertex elimination,Algorithms; Data acquisition; Data compression; Differentiation (calculus); Linear algebra; Program compilers; Hand-coding; Jacobian codes; Source transformation; Vertex elimination; Codes (symbols)
"Block tridiagonalization of ""effectively"" sparse symmetric matrices",2004,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-8344266771&doi=10.1145%2f1024074.1024078&partnerID=40&md5=22326910bed23dbd4ee749073610c2c8,"A block tridiagonalization algorithm is proposed for transforming a sparse (or ""effectively"" sparse) symmetric matrix into a related block tridiagonal matrix, such that the eigenvalue error remains bounded by some prescribed accuracy tolerance. It is based on a heuristic for imposing a block tridiagonal structure on matrices with a large percentage of zero or ""effectively zero"" (with respect to the given accuracy tolerance) elements. In the light of a recently developed block tridiagonal divide-and-conquer eigensolver [Gansterer, Ward, Muller, and Goddard, III, SIAM J. Sci. Comput. 25 (2003), pp. 65-85], for which block tridiagonalization may be needed as a preprocessing step, the algorithm also provides an option for attempting to produce at least a few very small diagonal blocks in the block tridiagonal matrix. This leads to low time complexity of the last merging operation in the block divide-and-conquer method. Numerical experiments are presented and various block tridiagonalization strategies are compared.",Approximations; Block divide-and-conquer; Block-tridiagonal matrix,Abstracting; Approximation theory; Computational complexity; Eigenvalues and eigenfunctions; Heuristic methods; Matrix algebra; Block divide-and-conquer; Block-tridiagonal matrix; Sparse symmetric matrices; Tridiagonalization; Phase transitions
"ProtoMol, an object-oriented framework for prototyping novel algorithms for molecular dynamics",2004,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-8344261362&doi=10.1145%2f1024074.1024075&partnerID=40&md5=0aab79a2ee746289e44ae2977f6777a2,"PROTOMOL is a high-performance framework in C++ for rapid prototyping of novel algorithms for molecular dynamics and related applications. Its flexibility is achieved primarily through the use of inheritance and design patterns (object-oriented programming). Performance is obtained by using templates that enable generation of efficient code for sections critical to performance (generic programming). The framework encapsulates important optimizations that can be used by developers, such as parallelism in the force computation. Its design is based on domain analysis of numerical integrators for molecular dynamics (MD) and of fast solvers for the force computation, particularly due to electrostatic interactions. Several new and efficient algorithms are implemented in PROTOMOL. Finally, it is shown that PROTOMOL'S sequential performance is excellent when compared to a leading MD program, and that it scales well for moderate number of processors. Binaries and source codes for Windows, Linux, Solaris, IRIX, HP-UX, and AIX platforms are available under open source license at http://protomol.sourceforge.net.",Fast electrostatic methods; Incremental parallelism; Molecular dynamics; Multigrid; Multiple time-stepping integration; Object-oriented framework,Algorithms; Molecular dynamics; Optimization; Pattern recognition; Program processors; Software engineering; Software prototyping; Fast electrostatic methods; Incremental parallelism; Mathematical software; Multivariate methods; Object oriented programming
Algorithm 835: MULTROOT - A matlab package for computing polynomial roots and multiplicities,2004,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-2942692216&doi=10.1145%2f992200.992209&partnerID=40&md5=4a60ed5e5edbf28ed6733829e2b870f4,"MULTROOT is a collection of Matlab modules for accurate computation of polynomial roots, especially roots with non-trivial multiplicities. As a blackbox-type software, MULTROOT requires the polynomial coefficients as the only input, and outputs the computed roots, multiplicities, backward error, estimated forward error, and the structure-preserving condition number. The most significant features of MULTROOT are the multiplicity identification capability and high accuracy on multiple roots without using multiprecision arithmetic, even if the polynomial coefficients are inexact. A comprehensive test suite of polynomials that are collected from the literature is included for numerical experiments and performance comparison.",Multiple root; Multiple zero; Multiplicity identification; Polynomial; Root-finding,Computer software; Error analysis; Iterative methods; Nonlinear equations; Polynomials; Multiple roots; Multiple zero; Multiplicity identification; Root-finding; Algorithms
Computing solutions of the modified bessel differential equation for imaginary orders and positive arguments,2004,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-2942692217&doi=10.1145%2f992200.992203&partnerID=40&md5=1b07a01fa402a04946007cfaf22ba071,"We describe a variety of methods to compute the functions K ia(x), Lia(x) and their derivatives for real a and positive x. These functions are numerically satisfactory independent solutions of the differential equation x2w″ + xw′ + (a2 - x2)w = 0. In the accompanying paper [Gil et al. 2004], we describe the implementation of these methods in Fortran 77 codes.",Asymptotic expansions; Bessel functions; Numerical quadrature,Asymptotic stability; Bessel functions; Codes (symbols); Computational methods; Problem solving; Asymptotic expansions; Imaginary orders; Numerical quadrature; Differential equations
Algorithm 834: Glsurf - An interactive surface plotting program using OpenGL,2004,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-2942633657&doi=10.1145%2f992200.992208&partnerID=40&md5=a06ebc30c2f76c7e8378cd5b6b35533d,"We describe an interactive surface visualization tool implemented in C, OpenGL, and GLUT. The surface is represented by a set of triangles in Euclidean 3-space, thus allowing for unrestricted topology. Capabilities include color-filled contour plots (for the graph of a bivariate function) and surface perspective plots with lighting and smooth shading. Interactive zooms and axis rotations are executed with a single keypress or mouse motion. The advantage of this code over the many alternatives is that it is small, simple, portable, easy to install and use, and the source code is available if the user wishes to change defaults, add light sources, or whatever.",Contour plot; OpenGL; Surface plot,Codes (symbols); Computer hardware; Functions; Interactive computer systems; Rotation; Standards; Contour plots; OpenGL; Surface plots; Algorithms
Algorithm 832: UMFPACK V4.3 - An unsymmetric-pattern multifrontal method,2004,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-2942653250&doi=10.1145%2f992200.992206&partnerID=40&md5=ea22c93c354abc44544c135ecb1fec72,"An ANSI C code for sparse LU factorization is presented that combines a column pre-ordering strategy with a right-looking unsymmetric-pattern multifrontal numerical factorization. The preordering and symbolic analysis phase computes an upper bound on fill-in, work, and memory usage during the subsequent numerical factorization. User-callable routines are provided for ordering and analyzing a sparse matrix, computing the numerical factorization, solving a system with the LU factors, transposing and permuting a sparse matrix, and converting between sparse matrix representations. The simple user interface shields the user from the details of the complex sparse factorization data structures by returning simple handles to opaque objects. Additional user-callable routines are provided for printing and extracting the contents of these opaque objects. An even simpler way to use the package is through its MATLAB interface. UMFPACK is incorporated as a built-in operator in MATLAB 6.5 as x = A\b when A is sparse and unsymmetric.",Linear equations; Multifrontal method; Ordering methods; Sparse nonsymmetric matrices,Codes (symbols); Data structures; Iterative methods; Linear equations; Matrix algebra; User interfaces; Multifrontal methods; Ordering methods; Sparse nonsymmetric matrices; Algorithms
A parallel direct solver for large sparse highly unsymmetric linear systems,2004,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-2942670738&doi=10.1145%2f992200.992201&partnerID=40&md5=295aa068d425723045f4c6e328d7558d,"The need to solve large sparse linear systems of equations efficiently lies at the heart of many applications in computational science and engineering. For very large systems when using direct factorization methods of solution, it can be beneficial and sometimes necessary to use multiple processors, because of increased memory availability as well as reduced factorization time. We report on the development of a new parallel code that is designed to solve linear systems with a highly unsymmetric sparsity structure using a modest number of processors (typically up to about 16). The problem is first subdivided into a number of loosely connected subproblems and a variant of sparse Gaussian elimination is then applied to each of the subproblems in parallel. An interface problem in the variables on the boundaries of the subproblems must also be factorized. We discuss how our software is designed to achieve the goals of portability, ease of use, efficiency, and flexibility, and illustrate its performance on an SGI Origin 2000, a Cray T3E, and a 2-processor Compaq DS20, using problems arising from real applications.",Gaussian elimination; Highly unsymmetric linear systems; Parallel processing; Sparse matrices,Codes (symbols); Computational methods; Computer simulation; Computer software; Computer workstations; Matrix algebra; Parallel processing systems; Problem solving; Gaussian elimination; Highly unasymmetric linear systems; Sparse matrices; Linear systems
Algorithm 831: Modified bessel functions of imaginary order and positive argument,2004,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-2942662161&doi=10.1145%2f992200.992204&partnerID=40&md5=7ee8b7788fc7f329285787292fdd8b4c,"Fortran 77 programs for the computation of modified Bessel functions of purely imaginary order are presented. The codes compute the functions K ia(x), Lia(x) and their derivatives for real a and positive x; these functions are independent solutions of the differential equation x2w″ + xw′ + (a2 - x2)w = 0. The code also computes exponentially scaled functions. The range of computation is (x, a) ε (0, 1500] × [-1500, 1500] when scaled functions are considered and it is larger than (0, 500] × [-400, 400] for standard IEEE double precision arithmetic. The relative accuracy is better than 10-13 in the range (0, 200] × [-200, 200] and close to 10 -12 in (0, 1500] × [-1500, 1500].",Asymptotic expansions; Bessel functions; Numerical quadrature,Algorithms; Codes (symbols); Computational methods; Computer software; Convergence of numerical methods; Error analysis; Set theory; Asymptotic expansions; Imaginary orders; Numerical quadrature; Bessel functions
A column pre-ordering strategy for the unsymmetric-pattern multifrontal method,2004,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-2942655475&doi=10.1145%2f992200.992205&partnerID=40&md5=e66a94327b2cab0a6cb19ed09c462b97,"A new method for sparse LU factorization is presented that combines a column pre-ordering strategy with a right-looking unsymmetric-pattern multifrontal numerical factorization. The column ordering is selected to give a good a priori upper bound on fill-in and then refined during numerical factorization (while preserving the bound). Pivot rows are selected to maintain numerical stability and to preserve sparsity. The method analyzes the matrix and automatically selects one of three pre-ordering and pivoting strategies. The number of nonzeros in the LU factors computed by the method is typically less than or equal to those found by a wide range of unsymmetric sparse LU factorization methods, including left-looking methods and prior multifrontal methods.",Linear equations; Multifrontal method; Ordering methods; Sparse nonsymmetrices matrices,Algorithms; Computer software; Iterative methods; Linear algebra; Linear systems; Matrix algebra; Multifrontal methods; Ordering methods; Sparse nonsymmetric matrices; Linear equations
MA57 - A code for the solution of sparse symmetric definite and indefinite systems,2004,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-2942690049&doi=10.1145%2f992200.992202&partnerID=40&md5=60161fb93db935209fd1efcb215ce860,"We introduce a new code for the direct solution of sparse symmetric linear equations that solves indefinite systems with 2 × 2 pivoting for stability. This code, called MA57, is in HSL 2002 and supersedes the well used HSL code MA27. We describe some of the implementation details and emphasize the novel features of MA57. These include restart facilities, matrix modification, partial solution for matrix factors, solution of multiple right-hand sides, and iterative refinement and error analysis. The code is written in Fortran 77, but there are additional facilities within a Fortran 90 implementation that include the ability to identify and change pivots. Several of these facilities have been developed particularly to support optimization applications, and we illustrate the performance of the code on problems arising therefrom.",Augmented systems; Direct sparse factorization; Multifrontal method; Numerical optimization; Sparse definite and indefinite systems.,Algorithms; Error analysis; Iterative methods; Linear equations; Linear systems; Matrix algebra; Optimization; Reliability; Augmented systems; Direct sparse factorization; Multifrontal methods; Numerical optimization; Sparse definite and indefinite systems; Codes (symbols)
Algorithm 833: CSRFPACK - Interpolation of scattered data with a C 1 convexity-preserving surface,2004,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-2942690048&doi=10.1145%2f992200.992207&partnerID=40&md5=2877df661a48693229520ca45c0a7073,"We describe a Fortran-77 software package for constructing a C 1 convex surface that interpolates a convex data set consisting of data values at arbitrarily distributed points in the plane (nodes) such that there exists a triangulation of the nodes for which the triangle-based piecewise linear interpolant is convex. The method consists of constructing this data-dependent triangulation, computing a set of nodal gradients for which there exists a convex piecewise linear Hermite interpolant H of the nodal values and gradients, and applying convolution smoothing to H.",Convexity-preserving interpolation; Data-dependent triangulation; Scattered data fitting,Convolution; Data reduction; Functions; Gradient methods; Interpolation; Set theory; Convexity-preserving interpolation; Data-dependent triangulation; Scattered data fitting; Algorithms
The design and implementation of a new out-of-core sparse cholesky factorization method,2004,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-2442521453&doi=10.1145%2f974781.974783&partnerID=40&md5=a8a4d084ec24083fe26adcc3ed8e18a4,"We describe a new out-of-core sparse Cholesky factorization method. The new method uses the elimination tree to partition the matrix, an advanced subtree-scheduling algorithm, and both right-looking and left-looking updates. The implementation of the new method is efficient and robust. On a 2 GHz personal computer with 768 MB of main memory, the code can easily factor matrices with factors of up to 48 GB, usually at rates above 1 Gflop/s. For example, the code can factor AUDIKW, currenly the largest matrix in any matrix collection (factor size over 10 GB), in a little over an hour, and can factor a matrix whose graph is a 140-by-140-by-140 mesh in about 12 hours (factor size around 27 GB).",Out-of-core,Algorithms; Combinatorial mathematics; Cost effectiveness; Iterative methods; Matrix algebra; Performance; Personal computers; Scheduling; Trees (mathematics); Data structure; Out-of-core; Sparse cholesky factorization method; Subtree-scheduling algorithm; Linear systems
SIPAMPL: Semi-infinite programming with AMPL,2004,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-2442460123&doi=10.1145%2f974781.974784&partnerID=40&md5=1cda41653132f36035d3c141edee52ca,"SIPAMPL is an environment for coding semi-infinite programming (SIP) problems. This environment includes a database containing a set of SIP problems that have been collected from the literature and a set of routines. It allows users to code their own SIP problems in AMPL, to use any problem already in the database, and to develop and test any SIP solver. The SIPAMPL routines support the interface between a potential SIP solver and test problems coded in AMPL. SIPAMPL also provides a tool that allows the selection of problems from the database with specified characteristics. As a concept demonstration, we show how MATLAB can use SIPAMPL to solve the problems in the database. The Linux and Microsoft Windows versions together with the database of coded problems are freely available via the web.",Evaluation tools; Semi-infinite programming; Test problems,Computer programming languages; Database systems; Encoding (symbols); Functions; Linear equations; Nonlinear equations; Optimization; Problem solving; Set theory; Evaluation tools; Mathematical programming problems; Semi-infinite programming (SIP); Test problems; Computer programming
Vector reduction/transformation operators,2004,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-2442480827&doi=10.1145%2f974781.974785&partnerID=40&md5=ca0ce124f63b07efee08986bc6918839,"Development of flexible linear algebra interfaces is an increasingly critical issue. Efficient and expressive interfaces are well established for some linear algebra abstractions, but not for vectors. Vectors differ from other abstractions in the diversity of necessary operations, sometimes requiring dozens for a given algorithm (e.g. interior-point methods for optimization). We discuss a new approach based on operator objects that are transported to the underlying data by the linear algebra library implementation, allowing developers of abstract numerical algorithms to easily extend the functionality regardless of computer architecture, application or data locality/organization. Numerical experiments demonstrate efficient implementation.",Interfaces; Object-orientation; Optimization; Vectors,Algorithms; Computational methods; Computer architecture; Mathematical operators; Mathematical transformations; Optimization; Performance; Standardization; Vectors; Abstract numerical algorithm (ANA); Data organization; Distributed memory computer; Interfaces; Object oriented programming
Two-step error estimators for implicit Runge-Kutta methods applied to stiff systems,2004,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-2442526799&doi=10.1145%2f974781.974782&partnerID=40&md5=5074caca5f9409c802d4e5e9292ca30f,"This paper is concerned with local error estimation in the numerical integration of stiff systems of ordinary differential equations by means of Runge-Kutta methods. With implicit Runge-Kutta methods it is often difficult to embed a local error estimate with the appropriate order and stability properties. In this paper local error estimation based on the information from the last two integration steps (that are supposed to have the same steplength) is proposed. It is shown that this technique, applied to Radau IIA methods, lets us get estimators with proper order and stability properties. Numerical examples showing that the proposed estimate improves the efficiency of the integration codes are presented.",Error estimation; Implicit Runge-Kutta methods; Runge-Kutta; Stiff initial value problems,Approximation theory; Error analysis; Estimation; Initial value problems; Integration; Ordinary differential equations; Error estimation; Implicit runge-kutta methods; Stiff initial value problems; Two-step estimate (TS); Runge Kutta methods
Algorithm 830: Another visit with standard and modified givens transformations and a remark on algorithm 539,2004,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-2442537185&doi=10.1145%2f974781.974786&partnerID=40&md5=805c99da9bd22b4fd31ce8c06114c100,"First we report on a correction and improvement to the Level 1 BLAS routine srotmg for computing the Modified Givens Transformation (MG). We then, in the light of the performance of the code on modern compiler/hardware combinations, reconsider the strategy of supplying separate routines to compute and apply the transformation. Finally, we show that the apparent savings in multiplies obtained by using MG rather than the Standard Givens Transformation (SG) do not always translate into reductions in execution time.",BLAS; Givens rotation; Linear algebra,Algorithms; Benchmarking; Cache memory; Combinatorial mathematics; Least squares approximations; Linear algebra; Mathematical transformations; Optimization; BLAS; Givens rotation; Modified givens transformation (MG); Computational methods
"GALAHAD, a library of thread-safe fortran 90 packages for large-scale nonlinear optimization",2003,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-2442619389&doi=10.1145%2f962437.962438&partnerID=40&md5=bb5970a394ed122cf8ed2a972636a835,"We describe the design of version 1.0 of GALAHAD, a library of Fortran 90 packages for large-scale nonlinear optimization. The library particularly addresses quadratic programming problems, containing both interior point and active set algorithms, as well as tools for preprocessing problems prior to solution. It also contains an updated version of the venerable nonlinear programming package, LANCELOT.",Fortran 90; GALAHAD; LANCELOT; Large-scale nonlinear optimization; Large-scale quadratic programming,Algorithms; Codes (standards); Computer programming; Computer software; Lagrange multipliers; Optimization; Problem solving; Quadratic programming; Fortran 90; GALAHAD; LANCELOT; Large-scale nonlinear optimization; Large-scale quadratic programming; Computer programming languages
Parallel frontal solvers for large sparse linear systems,2003,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-2442566928&doi=10.1145%2f962437.962440&partnerID=40&md5=15177ea5a1b91caa570de4aac2020aad,"Many applications in science and engineering give rise to large sparse linear systems of equations that need to be solved as efficiently as possible. As the size of the problems of interest increases, it can become necessary to consider exploiting multiprocessors to solve these systems. We report on the design and development of parallel frontal solvers for the numerical solution of large sparse linear systems. Three codes have been developed for the mathematical software library HSL (www.cse.clrc.ac.uk/Activity/HSL). The first is for unsymmetric finite-element problems; the second is for symmetric positive definite finite-element problems; and the third is for highly unsymmetric linear systems such as those that arise in chemical process engineering. In each case, the problem is subdivided into a small number of loosely connected subproblems and a frontal method is then applied to each of the subproblems in parallel. We discuss how our software is designed to achieve the goals of portability, ease of use, efficiency, and flexibility, and illustrate the performance using problems arising from real applications.",Finite-elements; Frontal method; Linear systems; Parallel processing; Sparse matrices,Algorithms; Computational complexity; Computer simulation; Computer software; Finite element method; Matrix algebra; Parallel processing systems; Problem solving; Frontal method; Fully summed; Sparse matrices; System matrix; Linear systems
Modeling the performance of interface contraction,2003,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-2442426479&doi=10.1145%2f962437.962442&partnerID=40&md5=11d25c062c5e56ac77f7da89c7368eba,"Automatic differentiation is a technique used to transform a computer code implementing some mathematical function into another program capable of evaluating the function and its derivatives. Compared to numerical differentiation, the derivatives obtained from applying automatic differentiation are free from truncation error, and their computation often requires less time. To increase the efficiency of a black box approach of automatic differentiation, a technique called interface contraction may be used. Interface contraction exploits the local structure of a code to temporarily reduce the global number of derivatives propagated through the code. Two performance models are introduced to predict the potential improvement in the execution time of a program making use of interface contraction compared to a program generated by a black box approach of automatic differentiation. The performance models are validated by numerical experiments carried out on different computing platforms. The computer codes used in the experiments stem from the application areas of neutron scattering and biostatistics.",Automatic differentiation; Forward mode; Interface contraction; Local preaccumulation; Performance modeling,Algorithms; Codes (symbols); Computer aided design; Computer programming; Differentiation (calculus); Mathematical models; Automatic differentiation; Forward mode; Interface contraction; Local preaccumulation; Performance modeling; Computer software
Algorithm 828: DNSPLIN1: Discrete nonlinear spline interpolation,2003,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-2442486949&doi=10.1145%2f962437.962443&partnerID=40&md5=77bc312698154ab784de90da6ab2c8a2,"We describe a new method and a Fortran-77 code for constructing discrete approximations to nonparametric interpolating nonlinear spline curves. Our approach consists of minimizing the discretized strain energy by a descent method with a Sobolev gradient in place of the standard gradient. It serves as a demonstration of the Sobolev gradient method, which is much more generally applicable. The effectiveness of the method in rapidly producing smooth interpolatory curves is demonstrated by test results for several challenging data sets.",Interpolation; Minimum energy curve; Nonlinear spline; Sobolev gradient,Approximation theory; Codes (standards); Computational complexity; Interpolation; Optimization; Polynomials; Problem solving; Minimum energy curve; Nonlinear splines; Sobolev gradient; Nonlinear programming
Algorithm 829: Software for generation of classes of test functions with known local and global minima for global optimization,2003,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-2442621528&doi=10.1145%2f962437.962444&partnerID=40&md5=f292bd3ce6a8e521d43f1879665377b0,"A procedure for generating non-differentiable, continuously differentiate, and twice continuously differentiable classes of test functions for multiextremal multidimensional box-constrained global optimization is presented. Each test class consists of 100 functions. Test functions are generated by defining a convex quadratic function systematically distorted by polynomials in order to introduce local minima. To determine a class, the user defines the following parameters: (i) problem dimension, (ii) number of local minima, (iii) value of the global minimum, (iv) radius of the attraction region of the global minimizer, (v) distance from the global minimizer to the vertex of the quadratic function. Then, all other necessary parameters are generated randomly for all 100 functions of the class. Full information about each test function including locations and values of all local minima is supplied to the user. Partial derivatives are also generated where possible.",Global optimization; Known local minima; Test problems generation,Algorithms; Computational complexity; Global optimization; Nonlinear programming; Polynomials; Problem solving; Known local minima; Test problem generation; Computer software
"CUTEr and sifdec: A constrained and unconstrained testing environment, revisited",2003,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-2442598318&doi=10.1145%2f962437.962439&partnerID=40&md5=5b073319d52d61fb4daddb22bb24471a,"The initial release of CUTE, a widely used testing environment for optimization software, was described by Bongartz, et al. [1995]. A new version, now known as CUTEr, is presented. Features include reorganisation of the environment to allow simultaneous multi-platform installation, new tools for, and interfaces to, optimization packages, and a considerably simplified and entirely automated installation procedure for UNIX systems. The environment is fully backward compatible with its predecessor, and offers support for Fortran 90/95 and a general C/C++ Application Programming Interface. The SIF decoder, formerly a part of CUTE, has become a separate tool, easily callable by various packages. It features simple extensions to the SIF test problem format and the generation of files suited to automatic differentiation packages.",Heterogeneous environment; Nonlinearly constrained optimization; Shared filesystems; SIF format; Testing environment,Algorithms; C (programming language); Decoding; Installation; Optimization; Problem solving; Program documentation; UNIX; World Wide Web; Hetergeneous environment; Nonlinearly constrained optimization; Shared filesystems; SIF format; Testing environment; Computer software
Fast contouring of solutions to partial differential equations,2003,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-2442600455&doi=10.1145%2f962437.962441&partnerID=40&md5=c2eb7f51f0cdbe2bdd13e675fd08c33a,"The application of Differential Equation Interpolants (DEIs) to the visualization of the solutions to Partial Differential Equations (PDEs) is investigated. In particular, we describe how a DEI can be used to generate a fine mesh approximation from a coarse mesh approximation; this fine mesh approximation can then be used by a standard contouring function to render an accurate contour plot of the surface. However, the standard approach has a time complexity equivalent to that of rendering a surface plot, O(fm 2) for each element of the coarse mesh, (where fm is the ratio of the width of the coarse mesh to the fine mesh). To address this concern three fast contouring algorithms are proposed that compute accurate contour lines directly from the DEI, and have time complexity at most O(fm) for each coarse mesh element.",Contour; Interpolation; PDEs; Rendering; Visualization,Algorithms; Approximation theory; Computational complexity; Computational methods; Computer simulation; Interpolation; Problem solving; Visualization; Contour; FORTRAN; Rendering; Partial differential equations
ACETAF: A software package for computing validated bounds for Taylor coefficients of analytic functions,2003,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-2442566927&doi=10.1145%2f838250.838252&partnerID=40&md5=8e3c178decac5c23aaa93799de79a748,This article presents methods for practical computation of verified bounds for Taylor coefficients of analytic functions. These bounds are constructed from Cauchy's estimate and from some of its modifications. Interval arithmetic is used to obtain rigorous results.,Analytic functions; Bounds for Taylor coefficients,Algorithms; Approximation theory; Computer software; Error analysis; Estimation; Functions; Mathematical models; Analytic functions; Bounds for Taylor coefficients; Cauchy's estimate; Verification; Computational methods
Algorithm 824: CUBPACK: A package for automatic cubature; framework description,2003,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-2442486944&doi=10.1145%2f838250.838253&partnerID=40&md5=9baa6c5729e3ed68ba350192cb33f581,"CUBPACK aims to offer a collection of re-usable code for automatic re-dimensional (n ≥ 1) numerical integration of functions over a collection of regions, i.e., quadrature and cubature. The current version allows this region to consist of a union of n-simplices and n-parellellepids. The framework of CUBPACK is described as well as its user interface. The functionality of several well known routines is embedded. New features include integration algorithms using the ε-algorithm for extrapolation for regions other than triangles and the implementation of a new type of subdivision for 3-cubes.",Automatic integration; Cubature; Quadrature,Computational methods; Data structures; Error statistics; Extrapolation; FORTRAN (programming language); Integration; Vectors; Automatic integration; Cubature; CUBPACK; Quadrature; Adaptive algorithms
An adaptive numerical cubature algorithm for simplices,2003,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-2442630589&doi=10.1145%2f838250.838254&partnerID=40&md5=8c66c931a366fe10e7ff1e538299451a,"A globally adaptive algorithm for numerical cubature of a vector of functions over a collection of n-dimensional simplices is described. The algorithm is based on a subdivision strategy that chooses for subdivision at each stage the subregion (of the input simplices) with the largest estimated error. This subregion is divided into two, three or four equal volume subregions by cutting selected edges. These edges are selected using information about the smoothness of the integrands in the edge directions. The algorithm allows a choice from several embedded cubature rule sequences for approximate integration and error estimation. A Fortran 95 implementation as a part of CUBPACK is also discussed. Testing of the algorithm is described.",Adaptive integration; Cubature; Multidimensional integration; Simplex; Software,Approximation theory; Computer software; Error analysis; Extrapolation; FORTRAN (programming language); Functions; Integration; Vectors; Cubature; Multidimensional integration; Simplex; Adaptive algorithms
Algorithm 826: A parallel eigenvalue routine for complex Hessenberg matrices,2003,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-2442441158&doi=10.1145%2f838250.838256&partnerID=40&md5=c6fc4997d1b0c566c339a3aed32552b4,"A code for computing the eigenvalues of a complex Hessenberg matrix is presented. This code computes the Schur decomposition of a complex Hessenberg matrix. Together with existing ScaLAPACK routines, the eigenvalues of dense complex matrices can be directly computed using a parallel QR algorithm. This parallel complex Schur decomposition routine was developed to fill a void in the ScaLAPACK library and was based on the parallel real Schur decomposition routine already in ScaLAPACK. The real-arithmetic version was appropriately modified to make it work with complex arithmetic and implement a complex multiple bulge QR algorithm. This also required the development of new auxiliary routines that perform essential operations for the complex Schur decomposition, and that will provide additional linear algebra computation capability to the parallel numerical library community.",Complex matrices; Eigenvalue; Parallel programming; Parallel QR algorithm; ScaLAPACK; Schur decomposition,Approximation theory; Computation theory; Computer programming; Eigenvalues and eigenfunctions; Matrix algebra; Complex matrices; Parallel programming; Parallel QR algorithm; ScaLAPACK; Schur decomposition; Algorithms
Remark on algorithm 769: Fortran subroutines for approximate solution of sparse quadratic assignment problems using GRASP,2003,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-2442533095&doi=10.1145%2f838250.838258&partnerID=40&md5=99740c49d144ca29618e9daab46fe394,A number of corrections and improvements to algorithm 769 were presented. Fortran subroutines for approximate solution of sparse quadratic assignment problems were studied using GRASP. In the original code the generation of random integers in a given range were implemented using the seed value to schrage's portable generator. The declaration statements for rand and xrand and the three executable statements were also elaborated.,Algorithms; Performance; Reliability,Approximation theory; Combinatorial mathematics; Computer software; FORTRAN (programming language); Program compilers; Reliability; Subroutines; Actual arguments; Dummy arguments; GRASP package; Schrage's portable; Algorithms
Algorithm 825: A deep-cut bisection envelope algorithm for fixed points,2003,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0347383275&doi=10.1145%2f838250.838255&partnerID=40&md5=f9d34b8104df910017d8b17054a7463b,"We present the BEDFix (Bisection Envelope Deep-cut Fixed point) algorithm for the problem of ap-1 proximating a fixed point of a function of two variables. The function must be Lipschitz continuous with constant 1 with respect to the infinity norm; such functions are commonly found in economics and game theory. The computed approximation satisfies a residual criterion given a specified error tolerance. The BEDFix algorithm improves the BEFix algorithm presented in Shellman and Sikorski [2002] by utilizing ""deep cuts,"" that is, eliminating additional segments of the feasible domain which cannot contain a fixed point. The upper bound on the number of required function evaluations is the same for BEDFix and BEFix, but our numerical tests indicate that BEDFix significantly improves the average-case performance. In addition, we show how BEDFix may be used to solve the absolute criterion fixed point problem with significantly better performance than the simple iteration method, when the Lipschitz constant is less than but close to 1. BEDFix is highly efficient when used to compute residual solutions for bivariate functions, having a bound on function evaluations that is twice the logarithm of the reciprocal of the tolerance. In the tests described in this article, the number of evaluations performed by the method averaged 31 percent of this worst-case bound. BEDFix works for nonsmooth continuous functions, unlike methods that require gradient information; also, it handles functions with minimum Lipschitz constants equal to 1, whereas the complexity of simple iteration approaches infinity as the minimum Lipschitz constant approaches 1. When BEDFix is used to compute absolute criterion solutions, the worst-case complexity depends on the logarithm of the reciprocal of 1-q, where q is the Lipschitz constant, as well as on the logarithm of the reciprocal of the tolerance.",Economics; Fixed points; Game theory; Nonlinear partial differential equations,Approximation theory; Computational methods; Functions; Game theory; Iterative methods; Numerical analysis; Partial differential equations; BEDFix algorithm; BEFix algorithm; Fixed points; Lipschitz complexity; Algorithms
The complex-step derivative approximation,2003,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-2442449533&doi=10.1145%2f838250.838251&partnerID=40&md5=12008a906c60fe9c04426be15d308e68,"The complex-step derivative approximation and its application to numerical algorithms are presented. Improvements to the basic method are suggested that further increase its accuracy and robustness and unveil the connection to algorithmic differentiation theory. A general procedure for the implementation of the complex-step method is described in detail and a script is developed that automates its implementation. Automatic implementations of the complex-step method for Fortran and C/C++ are presented and compared to existing algorithmic differentiation tools. The complex-step method is tested in two large multidisciplinary solvers and the resulting sensitivities are compared to results given by finite differences. The resulting sensitivities are shown to be as accurate as the analyses. Accuracy, robustness, ease of implementation and maintainability make these complex-step derivative approximation tools very attractive options for sensitivity analysis.",Automatic differentiation; Complex-step derivative approximation; Forward mode; Gradients; Overloading; Sensitivities,Approximation theory; Computational fluid dynamics; Computational methods; Differentiation (calculus); Finite difference method; FORTRAN (programming language); Optimization; Robustness (control systems); Automatic differentiation; Complex-step derivative approximation; Forward mode; Gradients; Sensitivities; Sensitivity analysis
Algorithm 827: Irbleigs: A MATLAB program for computing a few eigenpairs of a large sparse Hermitian matrix,2003,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-2442576081&doi=10.1145%2f838250.838257&partnerID=40&md5=a425785bf7558526ecb8008ae55cf0f1,"Irbleigs is a MATLAB program for computing a few eigenvalues and associated eigenvectors of a sparse Hermitian matrix of large order n. The matrix is accessed only through the evaluation of matrix-vector products. Working space of only a few n-vectors is required. The program implements a restarted block-Lanczos method. Judicious choices of acceleration polynomials make it possible to compute approximations of a few of the largest eigenvalues, a few of the smallest eigenvalues, or a few eigenvalues in the vicinity of a user-specified point on the real axis. irbleigs also can be applied to certain large generalized eigenproblems as well as to the computation of a few nearby singular values and associated right and left singular vectors of a large general matrix.",Block Lanczos method; Eigenvalue computation; Generalized eigenproblem; Polynomial acceleration; Singular values,Approximation theory; Computation theory; Eigenvalues and eigenfunctions; Linear equations; Linear systems; Matrix algebra; Vectors; Block Lanczos method; Eigenvalue computation; Generalized Eigenproblem; Polynomial acceleration; Singular values; Algorithms
MATCONT: A MATLAB package for numerical bifurcation analysis of ODEs,2003,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-8744230577&doi=10.1145%2f779359.779362&partnerID=40&md5=c657dee76170f88bffa5ddd063f9cf1a,"MATCONT is a graphical MATLAB software package for the interactive numerical study of dynamical systems. It allows one to compute curves of equilibria, limit, points, Hopf points, limit cycles, period doubling bifurcation points of limit cycles, and fold bifurcation points of limit cycles. All curves are computed by the same function that implements a prediction-correction continuation algorithm based on the Moore-Penrose matrix pseudo-inverse. The continuation of bifurcation points of equilibria and limit cycles is based on bordering methods and minimally extended systems. Hence no additional unknowns such as singular vectors and eigenvectors are used and no artificial sparsity in the systems is created. The sparsity of the discretized systems for the computation of limit cycles and their bifurcation points is exploited by using the standard Matlab sparse matrix methods. The MATLAB environment makes the standard MATLAB Ordinary Differential Equations (ODE) Suite interactively available and provides computational and visualization tools; it also eliminates the compilation stage and so makes installation straightforward. Compared to other packages such as AUTO and CONTENT, adding a new type of curves is easy in the MATLAB environment. We illustrate this by a detailed description of the limit point curve type.",Bifurcation; Dynamical system; Numerical continuation,Algorithms; Bifurcation (mathematics); Computational methods; Computer graphics; Eigenvalues and eigenfunctions; Ordinary differential equations; Vectors; Dynamical systems; Hopf points; MATLAB software package; Numerical continuation; Computer software
GloptiPoly: Global Optimization over Polynomials with Matlab and SeDuMi,2003,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-2542587541&doi=10.1145%2f779359.779363&partnerID=40&md5=367eec7cc6027964c33d3907eea4a3a6,"GloptiPoly is a Matlab/SeDuMi add-on to build and solve convex linear matrix inequality relaxations of the (generally nonconvex) global optimization problem of minimizing a multivariable polynomial function subject to polynomial inequality, equality, or integer constraints. It generates a series of lower bounds monotonically converging to the global optimum without any problem splitting. Global optimality is detected and isolated optimal solutions are extracted automatically. Numerical experiments show that for most of the small-scale problems described in the literature, the global optimum is reached at low computational cost.",Linear matrix inequality; Matlab; Polynomial programming; SeDuMi; Semidefinite programming,Computational methods; Constraint theory; Convergence of numerical methods; Global optimization; Integer programming; Polynomials; Problem solving; User interfaces; Linear matrix inequality (LMI); Matlab; Polynomial programming; SeDuMi; Semidefinite programming (SDP); Computer software
The spectral signal processing suite,2003,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-10844258009&doi=10.1145%2f779359.779364&partnerID=40&md5=4380a048e9bd195174f3ccaf9247b69c,"A software suite written, in the Java programming language for the postprocessing of Chebyshev approximations to discontinuous functions is presented. It is demonstrated how to use the package to remove the effects of the Gibbs-Wilbraham phenomenon from Chebyshev approximations of discontinuous functions. Additionally, the package is used to postprocess Chebyshev collocation and Chebyshev super spectral viscosity approximations of hyperbolic partial differential equations. The postprocessing method is the Gegenbauer reconstruction procedure. The Spectral Signal Processing Suite is the first publicly available package that implements the procedure. State-of-the-art techniques are used to implement the algorithms with efficiency while reducing round-off error.",Chebyshev; Edge detection; Gegenbauer postprocessing; Gibbs-Wilbraham phenomenon; Java,Algorithms; Chebyshev approximation; Edge detection; Java programming language; Polynomials; Signal processing; Chebyshev collocation; Gegenbauer postprocessing; Gibbs-Wilbraham phenomenon; Java; Computer software
Algorithm 823: Implementing scrambled digital sequences,2003,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-1842651631&doi=10.1145%2f779359.779360&partnerID=40&md5=586ba8ac6d8f1d6eede03fcda80d3c30,"Random scrambling of deterministic (t, m, s)-nets and (t, s)-sequences eliminates their inherent bias while retaining their low-discrepancy properties. This article describes an implementation of two types of random scrambling, one proposed by Owen and another proposed by Faure and Tezuka. The four different constructions of digital sequences implemented are those proposed by Sobol', Faure, Niederreiter, and Niederreiter and Xing. Because the random scrambling involves manipulating all digits of each point, the code must be written carefully to minimize the execution time. Computed root mean square discrepancies of the scrambled sequences are compared to known theoretical results. Furthermore, the performances of these sequences on various test problems are discussed.",Digital net; Scrambling,Algorithms; Convergence of numerical methods; Error analysis; Error detection; Mathematical models; Monte Carlo methods; Convergence rates; Digital nets; Digital sequences; Scrambling; Random processes
Formal derivation of algorithms: The triangular sylvester equation,2003,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-10844267383&doi=10.1145%2f779359.779365&partnerID=40&md5=c2f7b4d7ebff8dcd9bd8a521940f8c7b,"In this paper we apply a formal approach for the derivation of dense linear algebra algorithms to the triangular Sylvester equation. The result is a large family of provably correct algorithms. By using a coding style that reflects the algorithms as they are naturally presented, the correctness of the algorithms carries through to the correctness of the implementations. Analytically motivated heuristics are used to subsequently choose members from the family that can be expected to yield high performance. Finally, we report performance on the Intel (R) Pentium (R) III processor that is competitive with that of recursive algorithms reported previously in the literature for this operation.",Control theory; Formal derivation; Libraries; Linear algebra; Sylvester equations,Algorithms; Control theory; Digital libraries; Heuristic methods; Program processors; Software engineering; Formal derivation; Linear algebra algorithms; Recursive algorithms; Sylvester equations; Linear algebra
SuperLU_DIST: A scalable distributed-memory sparse direct solver for unsymmetric linear systems,2003,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0038621899&doi=10.1145%2f779359.779361&partnerID=40&md5=f51726f4e7e88879d406fd300805f215,"We present the main algorithmic features in the software package SuperLU_DIST, a distributed-memory sparse direct solver for large sets of linear equations. We give in detail our parallelization strategies, with a focus on scalability issues, and demonstrate the software's parallel performance and scalability on current machines. The solver is based on sparse Gaussian elimination, with an innovative static pivoting strategy proposed earlier by the authors. The main advantage of static pivoting over classical partial pivoting is that it permits a priori determination of data structures and communication patterns, which lets us exploit techniques used in parallel sparse Cholesky algorithms to better parallelize both LU decomposition and triangular solution on large-scale distributed machines.",Distributed-memory computers; Parallelism; Scalability; Sparse direct solver; Supernodal factorization,Algorithms; Data storage equipment; Distributed computer systems; Linear equations; Parallel processing systems; Software engineering; Distributed-memory computers; Parallelism; Scalability; Sparse direct solver; Supernodal factorization; Linear systems
Digital filters in adaptive time-stepping,2003,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-3042616322&doi=10.1145%2f641876.641877&partnerID=40&md5=aeb648317bb0ecfef3aff85294956a31,"Adaptive time-stepping based on linear digital control theory has several advantages: the algorithms can be analyzed in terms of stability and adaptivity, and they can be designed to produce smoother stepsize sequences resulting in significantly improved regularity and computational stability. Here, we extend this approach by viewing the closed-loop transfer map H φ̂ : log φ̂ → log h as a digital filter, processing the signal log φ̂ (the principal error function) in the frequency domain, in order to produce a smooth stepsize sequence log h. The theory covers all previously considered control structures and offers new possibilities to construct stepsize selection algorithms in the asymptotic stepsize-error regime. Without incurring extra computational costs, the controllers can be designed for special purposes such as higher order of adaptivity (for smooth ODE problems) or a stronger ability to suppress high-frequency error components (nonsmooth problems, stochastic ODEs). Simulations verify the controllers' ability to produce stepsize sequences resulting in improved regularity and computational stability.",Adaptivity; Algorithm analysis; Control theory; Digital filters; Error control; Mathematical software; Stepsize control,Algorithms; Asymptotic stability; Closed loop control systems; Computer simulation; Computer software; Control equipment; Control theory; Difference equations; Error analysis; Frequency domain analysis; Mathematical models; Ordinary differential equations; Z transforms; Adaptivity; Algorithm analysis; Error control; Mathematical software; Stepsize control; Digital filters
Remark on Algorithm 659: Implementing Sobol's quasirandom sequence generator,2003,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-10844292762&doi=10.1145%2f641876.641879&partnerID=40&md5=cff8e4f0e0440bcd753e5d0394a77162,"An algorithm to generate Sobol' sequences to approximate integrals in up to 40 dimensions has been previously given by Bratley and Fox in Algorithm 659. Here, we provide more primitive polynomials and ""direction numbers"" so as to allow the generation of Sobol' sequences to approximate integrals in up to 1111 dimensions. The direction numbers given generate Sobol' sequences that satisfy Sobol's so-called Property A.",Low-discrepancy sequences; Primitive polynomials; Quasirandom sequences; Sobol' sequences,Approximation theory; Computer software; Differentiation (calculus); FORTRAN (programming language); Polynomials; Low-discrepancy sequences; Primitive polynomials; Quasirandom sequences; Sobol's sequences; Algorithms
Scalar fused multiply-add instructions produce floating-point matrix arithmetic provably accurate to the penultimate digit,2003,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-10844263523&doi=10.1145%2f641876.641878&partnerID=40&md5=57e251e4e62d93ddece55fe409597f97,"Combined with doubly compensated summation, scalar fused multiply-add instructions redefine the concept of floating-point arithmetic, because they allow for the computation of sums of real or complex matrix products accurate to the penultimate digit. Particular cases include complex arithmetic, dot products, cross products, residuals of linear systems, determinants of small matrices, discriminants of quadratic, cubic, or quartic equations, and polynomials.",Doubly compensated summation; Floating-point arithmetic; Fused multiply-add instruction; Matrix arithmetic; Provable accuracy; Rounding error,Algorithms; Computer graphics; Error analysis; High level languages; Linear systems; Matrix algebra; Polynomials; Standardization; Doubly compensated summation; Fused multiply-add instruction; Matrix arithmetic; Provable accuracy; Rounding errors; Digital arithmetic
Induced well-distributed sets in Riemannian spaces,2003,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-10844277400&doi=10.1145%2f641876.641881&partnerID=40&md5=caa728412435ca71696d33f2cf5f00e1,"The concept of Riemannian geometries is used to construct induced homogeneous point sets on manifolds that are based on well-distributed point sets in unit cubes of an appropriately chosen Euclidean space. These well-distributed point sets in unit cubes are based on standard low-discrepancy sequences. The approach is algorithmic, that is, the methods developed in this article have been implemented and tested. Applications in image processing, graph theory and measurement-based exploration are presented.",Image processing; Low-discrepancy sequences; Riemannian geometry; Well-distributed point sets,Algorithms; Fiber optics; Graph theory; Image processing; Probability distributions; Problem solving; Euclidean spaces; Low-discrepancy sequences; Riemannian geometry; Well-distributed point sets; Computational geometry
Object-oriented software for quadratic programming,2003,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-10844238213&doi=10.1145%2f641876.641880&partnerID=40&md5=f50299140c17bb81df4a0ddfc27b00f2,"The object-oriented software package OOQP for solving convex quadratic programming problems (QP) is described. The primal-dual interior point algorithms supplied by OOQP are implemented in a way that is largely independent of the problem structure. Users may exploit problem structure by supplying linear algebra, problem data, and variable classes that are customized to their particular applications. The OOQP distribution contains default implementations that solve several important QP problem types, including general sparse and dense QPs, bound-constrained QPs, and QPs arising from support vector machines and Huber regression. The implementations supplied with the OOQP distribution are based on such well known linear algebra packages as MA27/57, LAPACK, and PETSc. OOQP demonstrates the usefulness of object-oriented design in optimization software development, and establishes standards that can be followed in the design of software packages for other classes of optimization problems. A number of the classes in OOQP may also be reusable directly in other codes.",Interior-Point Methods; Object-Oriented Software; Quadratic Programming,Algorithms; Codes (symbols); Linear algebra; Object oriented programming; Problem solving; Quadratic programming; Regression analysis; Huber regression; Interior-point methods; Object-oriented software; Support vector machines; Computer software
"Algorithm 819: AIZ, BIZ: Two Fortran 77 routines for the computation of complex airy functions",2002,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0040620160&doi=10.1145%2f569147.569150&partnerID=40&md5=bf01258981032b6a34965c1b7939901b,"Two Fortran 77 routines for the evaluation of Airy functions of complex arguments Ai(z), Bi(z) and their first derivatives are presented. The routines are based on the use of Gaussian quadrature, Maclaurin series and asymptotic expansions. Comparison with a previous code by D. E. Amos [1986] is provided.",Airy functions; Algorithms; Complex values; G.4 [Mathematics of Computing]: Mathematical software; Gauss quadrature,
A comment on the presentation and testing of CALGO codes and a remark on algorithm 639: To integrate some infinite oscillating tails,2002,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0040620161&doi=10.1145%2f569147.569148&partnerID=40&md5=9b3e1632c23d24e4a1ac24000bab58f6,"We report on a number of coding problems that occur frequently in published CALGO software and are still appearing in new algorithm submissions. Using Algorithm 639 as an extended example, we describe how these types of faults may be almost entirely eliminated using available commercial compilers and software tools. We consider the levels of testing required to instil confidence that code performs reliably. Finally, we look at how the source code may be re-engineered, and thus made more maintainable, by taking account of advances in hardware and language development.","Algorithms; D.2.5 [Software Engineering]: Testing and Debugging; D.2.7 [Software Engineering]: Distribution, Maintenance, and Enhancement; Debugging; Fortran; G.4 [Mathematics of Computing]: Mathematical Software; Software tools; Testing",
Implementing Hager's exchange methods for matrix profile reduction,2002,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0040673834&doi=10.1145%2f592843.592844&partnerID=40&md5=6373348791ef021371207e05b9b49b97,"Hager recently introduced down and up exchange methods for reducing the profile of a sparse matrix with a symmetric sparsity pattern. The methods are particularly useful for refining orderings that have been obtained using a standard profile reduction algorithm, such as the Sloan method. The running times for the exchange algorithms reported by Hager suggested their cost could be prohibitive for practical applications. We examine how to implement the exchange algorithms efficiently. For a range of real test problems, it is shown that the cost of running our new implementation does not add a prohibitive overhead to the cost of the original reordering.","Algorithms; Exchange method; G.1.0 [Numerical Analysis]: General - numerical algorithms; G.1.3 [Numerical Analysis]: Numerical Linear Algebra - sparse, structured, and very large systems; Matrix profile; Performance; Sparse symmetric matrices",
Algorithm 820: A flexible implementation of matching pursuit for Gabor functions on the interval,2002,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0040620156&doi=10.1145%2f569147.569151&partnerID=40&md5=1fe5524bcf23de018f1ba14c801fc09b,"In digital signal processing it is often advantageous to analyze a given signal using an adaptive method. The signal is approximated or represented as a superposition of ""basic"" waveforms chosen from a dictionary of such waveforms so as to best match the signal. The matching pursuit algorithm of Mallat and Zhang is such a method and is discussed in the context of discretized Gabor functions on an interval. We describe two software implementations based on these dictionaries. Both implementations rely on functions denned on an interval to avoid edge effects. One implementation allows for users to have great flexibility in the Gabor dictionary to be used. This is a useful improvement over other implementations, which only allow for a fixed dictionary. The other implementation takes advantage of the FFT algorithm and is faster. These implementations are written in C++, and can be used in practical applications.",Adaptive signal processing; Algorithms; G.1.2 [Numerical Analysis]: Approximations; G.4 [Mathematics of Computing]: Mathematical Software; Gabor analysis; I.4.5 [Image Processing]: Reconstruction,
Recursive blocked algorithms for solving triangular systems - Part I: one-sided and coupled Sylvester-type matrix equations,2002,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-19044380439&doi=10.1145%2f592843.592845&partnerID=40&md5=274a61131aa10fd10639c795b8ba2fb8,"Triangular matrix equations appear naturally in estimating the condition numbers of matrix equations and different eigenspace computations, including block-diagonalization of matrices and matrix pairs and computation of functions of matrices. To solve a triangular matrix equation is also a major step in the classical Bartels-Stewart method for solving the standard continuous-time Sylvester equation (AX - XB = C). We present novel recursive blocked algorithms for solving one-sided triangular matrix equations, including the continuous-time Sylvester and Lyapunov equations and a generalized coupled Sylvester equation. The main parts of the computations are performed as level-3 general matrix multiply and add (GEMM) operations. In contrast to explicit standard blocking techniques, our recursive approach leads to an automatic variable blocking that has the potential of matching the memory hierarchies of today's HPC systems. Different implementation issues are discussed, including when to terminate the recursion, the design of new optimized superscalar kernels for solving leaf-node triangular matrix equations efficiently, and how parallelism is utilized in our implementations. Uniprocessor and SMP parallel performance results of our recursive blocked algorithms and corresponding routines in the state-of-the-art libraries LAPACK and SLICOT are presented. The performance improvements of our recursive algorithms are remarkable, including 10-fold speedups compared to standard algorithms.","F.2.1 [Analysis of Algorithms and Problem Complexity]: Numerical Algorithms and Problems - computations on matrices; G.1.3 [Numerical Analysis]: Numerical Linear Algebra - conditioning, linear systems; G.4 [Mathematical Software]: Algorithm design",
"Algorithm 822: GIZ, HIZ: Two Fortran 77 routines for the computation of complex scorer functions",2002,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0040080223&doi=10.1145%2f592843.592847&partnerID=40&md5=72c64ac5a5f8f3c9d1260560e47b927d,"Two Fortran 77 routines for the evaluation of Scorer functions of complex arguments Gi(z), Hi(z), and their derivatives are presented. The routines are based on the use of quadrature, Maclaurin series, and asymptotic expansions. For real z, comparison with a previous code by A. J. MacLeod [1994] is provided.",Airy functions; Algorithms; G.4 [Mathematics of Computing]: Mathematical software; Numerical quadrature; Scorer functions,
A software package for sparse orthogonal factorization and updating,2002,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0039488522&doi=10.1145%2f592843.592848&partnerID=40&md5=62258eaa618bc51a30223b8f762ebcbb,"Although there is good software for sparse QR factorization, there is little support for updating and downdating, something that is absolutely essential in some linear programming algorithms, for example. This article describes an implementation of sparse LQ factorization, including block triangularization, approximate minimum degree ordering, symbolic factorization, multifrontal factorization, and updating and downdating. The factor Q is not retained. The updating algorithm expands the nonzero pattern of the factor L, which is reflected in dynamic representation of L. The block triangularization is used as an ""ordering for sparsity"" rather than as a prerequisite for block backward substitution. In symbolic factorization, something called ""element counters"" is introduced to reduce the overestimation of the number of nonzeros that the commonly used methods do. Both the approximate minimum degree ordering and the symbolic factorization are done without explicitly forming the nonzero pattern of the symmetric matrix in the corresponding normal equations. Tests show that the average time used for a single update or downdate is essentially the same as the time used for a single forward or backward substitution. Other parts of the implementation show the same range of performance as existing code, but cannot be replaced because of the special character of the systems that are solved.","Algorithms; Downdating; G.1.3 [Numerical Analysis]: Numerical Linear Algebra - sparse, structured, and very large systems; G.4 [Mathematics of Computing]: Mathematical Software; Orthogonal factorization; Software; Sparse matrix; Updating",
Recent advances in direct methods for solving unsymmetric sparse systems of linear equations,2002,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0013137287&doi=10.1145%2f569147.569149&partnerID=40&md5=abf0691404b9fa640e6626ca69bc1d4f,"During the past few years, algorithmic improvements alone have reduced the time required for the direct solution of unsymmetric sparse systems of linear equations by almost an order of magnitude. This paper compares the performance of some well-known software packages for solving general sparse sytems. In particular, it demonstrates the consistently high level of performance achieved by WSMP-the most recent of such solvers. It compares the various algorithmic components of to solvers and discusses their impact on solver performance. Our experiments show that the algorithmic choices made in WSMP enable it to run more than twice as fast as the best among similar solvers and that WSMP can factor some of the largest sparse matrices available from real applications in only a few seconds on a 4-CPU workstation. Thus, the combination of advances in hardware and algorithms makes it possible to solve those general sparse linear systems quickly and easily that might have been considered too large until recently.",Algorithms; G.1.3 [Mathematics of Computing]: Numerical Linear Algebra; Multifrontal Method; Parallel Sparse Solvers; Performance; Sparse LU Decomposition; Sparse Matrix Factorization,
Recursive blocked algorithms for solving triangular systems - Part II: Two-sided and generalized Sylvester and Lyapunov matrix equations,2002,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-19044400922&doi=10.1145%2f592843.592846&partnerID=40&md5=7e18599c92c1a5772dee51d8b85543f9,"We continue our study of high-performance algorithms for solving triangular matrix equations. They appear naturally in different condition estimation problems for matrix equations and various eigenspace computations, and as reduced systems in standard algorithms. Building on our successful recursive approach applied to one-sided matrix equations (Part I), we now present novel recursive blocked algorithms for two-sided matrix equations, which include matrix product terms such as AXBT. Examples are the discrete-time standard and generalized Sylvester and Lyapunov equations. The means for achieving high performance is the recursive variable blocking, which has the potential of matching the memory hierarchies of today's high-performance computing systems, and level-3 computations which mainly are performed as GEMM operations. Different implementation issues are discussed, including the design of efficient new algorithms for two-sided matrix products. We present uniprocessor and SMP parallel performance results of recursive blocked algorithms and routines in the state-of-the-art SLICOT library. Although our recursive algorithms with optimized kernels for the two-sided matrix equations perform more operations, the performance improvements are remarkable, including 10-fold speedups or more, compared to standard algorithms.","F.2.1 [Analysis of Algorithms and Problem Complexity]: Numerical Algorithms and Problems - computations on matrices; G.1.3 [Numerical Analysis]: Numerical Linear Algebra - conditioning, linear systems; G.4 [Mathematical Software]: Algorithm design",
Algorithm 821: A Fortran interface to POSIX threads,2002,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0038842248&doi=10.1145%2f569147.569152&partnerID=40&md5=0774b6e9ae0ca3ab8bef3f4f27cf13d7,"Pthreads is the library of POSIX standard functions for concurrent, multithreaded programming. The POSIX standard only defines an application programming interface (API) to the C programming language, not to Fortran. Many scientific and engineering applications are written in Fortran. Also, many of these applications exhibit functional, or task-level, concurrency. They would benefit from multithreading, especially on symmetric multiprocessors (SMP). We present here an interface to that part of the Pthreads library that is compatible with standard Fortran. The contribution consists of two primary source files: a Fortran module and a collection of C wrappers to Pthreads functions. The Fortran module defines the data structures, interface and initialization routines used to manage threads. The stability and portability of the Fortran API to Pthreads has been demonstrated using common mathematical computations on several SMP systems.",Algorithms; D.1.3 [Concurrent Programming]: Parallel Programming; G.4 [Mathematical Software]: Parallel and vector implementations; Languages; Mathematical software; Performance; POSIX Threads; Scientific computing; Standardization; Symmetric multi-processor,
Remark on algorithm 705: A Fortran-77 software package for solving the Sylvester matrix equation AXBT + CXDT = E,2002,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0038842245&doi=10.1145%2f569147.569153&partnerID=40&md5=602138f7e3e6b615fbc468d893f64dca,We present a number of corrections to Algorithm 705 [Gardiner et al. 1992].,Algorithms; G.1.3 [Numerical Analysis]: Numerical LinearAlgebra-Linear systems (direct and iterative methods); G.4 [Mathematics of Computing]: Mathematical Software - Reliability and robustness; Performance; Reliability,
On Computing Givens Rotations Reliably and Efficiently,2002,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0041413300&doi=10.1145%2f567806.567809&partnerID=40&md5=9acf4626ccdbeaeda49ea70bbf622c63,"We consider the efficient and accurate computation of Givens rotations. When f and g are positive real numbers, this simply amounts to computing the values of c = f / √f2 + g2, s = g/ √f2 + g2, and r = √f2 + g2. This apparently trivial computation merits closer consideration for the following three reasons. First, while the definitions of c, s and r seem obvious in the case of two nonnegative arguments f and g, there is enough freedom of choice when one or more of f and g are negative, zero or complex that LAPACK auxiliary routines SLARTG, CLARTG, SLARGV and CLARGV can compute rather different values of c, s and r for mathematically identical values of f and g. To eliminate this unnecessary ambiguity, the BLAS Technical Forum chose a single consistent definition of Givens rotations that we will justify here. Second, computing accurate values of c, s and r as efficiently as possible and reliably despite over/underflow is surprisingly complicated. For complex Givens rotations, the most efficient formulas require only one real square root and one real divide (as well as several much cheaper additions and multiplications), but a reliable implementation using only working precision has a number of cases. On a Sun Ultra-10, the new implementation is slightly faster than the previous LAPACK implementation in the most common case, and 2.7 to 4.6 times faster than the corresponding vendor, reference or ATLAS routines. It is also more reliable; all previous codes occasionally suffer from large inaccuracies due to over/underflow. For real Givens rotations, there are also improvements in speed and accuracy, though not as striking. Third, the design process that led to this reliable implementation is quite systematic, and could be applied to the design of similarly reliable subroutines.",G.4 [Mathematical Software]: Efficiency; reliability and robustness,
"Design, Implementation and Testing of Extended and Mixed Precision BLAS",2002,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-19044370033&doi=10.1145%2f567806.567808&partnerID=40&md5=309f7be29884bd36ea1a9178c3f90aa0,"This article describes the design rationale, a C implementation, and conformance testing of a subset of the new Standard for the BLAS (Basic Linear Algebra Subroutines): Extended and Mixed Precision BLAS. Permitting higher internal precision and mixed input/output types and precisions allows us to implement some algorithms that are simpler, more accurate, and sometimes faster than possible without these features. The new BLAS are challenging to implement and test because there are many more subroutines than in the existing Standard, and because we must be able to assess whether a higher precision is used for internal computations than is used for either input or output variables. We have therefore developed an automated process of generating and systematically testing these routines. Our methodology is applicable to languages besides C. In particular, our algorithms used in the testing code will be valuable to all other BLAS implementors. Our extra precision routines achieve excellent performance - close to half of the machine peak Megaflop rate even for the Level 2 BLAS, when the data access is stride one.","Algorithms; BLAS; Double-double arithmetic; Extended and mixed precision; G.4 [Mathematical Software]: Algorithm design and analysis, reliability and robustness; Performance; Reliability; Standardization",
An Overview of the Sparse Basic Linear Algebra Subprograms: The New Standard from the BLAS Technical Forum,2002,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0042415671&doi=10.1145%2f567806.567810&partnerID=40&md5=d3b621931f1e2bdb456712577b64600a,"We discuss the interface design for the Sparse Basic Linear Algebra Subprograms (BLAS), the kernels in the recent standard from the BLAS Technical Forum that are concerned with unstructured sparse matrices. The motivation for such a standard is to encourage portable programming while allowing for library-specific optimizations. In particular, we show how this interface can shield one from concern over the specific storage scheme for the sparse matrix. This design makes it easy to add further functionality to the sparse BLAS in the future. We illustrate the use of the Sparse BLAS with examples in the three supported programming languages, Fortran 95, Fortran 77, and C.","Algorithms; Computational kernels; Design; G.1.3 [Numerical Analysis]: Numerical Linear Algebra -sprase, structured, and very large systems (direct and iterative methods); G.4 [Mathematical Software]: user interfaces; Software; Sparse BLAS; Standardization",
Preface to the Special Issue on the Basic Linear Algebra Subprograms (BLAS),2002,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013632915&doi=10.1145%2f567806.567812&partnerID=40&md5=173cd285f23c3bb7329ddfde0087bd00,[No abstract available],,
Algorithm 818: A Reference Model Implementation of the Sparse BLAS in Fortran 95,2002,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-19044373611&doi=10.1145%2f567806.567811&partnerID=40&md5=270164145ae366492b1322c9cc06c360,"The Basic Linear Algebra Subprograms for sparse matrices (Sparse BLAS) as defined by the BLAS Technical Forum are a set of routines providing basic operations for sparse matrices and vectors. A principal goal of the Sparse BLAS standard is to aid in the development of iterative solvers for large sparse linear systems by specifying on the one hand interfaces for a high-level description of vector and matrix operations for the algorithm developer and on the other hand leaving enough freedom for vendors to provide the most efficient implementation of the underlying algorithms for their specific architectures. The Sparse BLAS standard defines interfaces and bindings for the three target languages: C, Fortran 77 and Fortran 95. We describe here our Fortran 95 implementation intended as a reference model for the Sparse BLAS. We identify the underlying complex issues of the representation and the handling of sparse matrices and give suggestions to other implementors of how to address them.",Algorithms; BLAS; Design; G.1.3 [Numerical Analysis]: Numerical Linear Algebra; Iterative linear solvers; Programming standard; Sparse BLAS; Sparse data structures; Standardization; Unstructured sparse matrices,
An Updated Set of Basic Linear Algebra Subprograms (BLAS),2002,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-19044386208&doi=10.1145%2f567806.567807&partnerID=40&md5=98c1fceb497553b115857e5a2b38d23b,[No abstract available],Algorithms; BLAS; G.1.3 [Numerical Analysis]: Numerical Linear Algebra; G.4 [Mathematical Software]; Linear algebra; Standardization; Standards,
An extension of the divide-and-conquer method for a class of symmetric block-tridiagonal eigenproblems,2002,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0040668385&doi=10.1145%2f513001.513004&partnerID=40&md5=bf006d5d39ecf683bbb35374c0fa62a6,A divide-and-conquer method for computing eigenvalues and eigenvectors of a block-tridiagonal matrix with rank-one off-diagonal blocks is presented. The implications of unbalanced merging operations due to unequal block sizes are analyzed and illustrated with numerical examples. It is shown that an unfavorable order for merging blocks in the synthesis phase of the algorithm may lead to a significant increase of the arithmetic complexity. A strategy to determine a good merging order that is at least close to optimal in all cases is given. The method has been implemented and applied to test problems from a quantum chemistry application.,G.1.3 [Numerical Analysis]: Numerical Linear Algebra - Eigenvalues and eigenvectors (direct and iterative methods); G.4 [Mathematical Software]: - Algorithm design and analysis,
Efficient and automatic implementation of the adjoint state method,2002,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0040668355&doi=10.1145%2f513001.513003&partnerID=40&md5=1816ab82003f18c88751c42671a6977c,"Combination of object-oriented programming with automatic differentiation techniques facilitates the solution of data fitting, control, and design problems driven by explicit time stepping schemes for initial-boundary value problems. The C++ class f dtd takes a complete specification of a single step, along with some associated code, and assembles from it a complete simulator, along with the linearized and adjoint simulations. The result is a (nonlinear) operator in the sense of the Hubert Class Library (HCL), a C++ software package for optimization. The HCL operator so produced links directly with any of the HCL optimization algorithms. Moreover the performance of simulators constructed in this way is equivalent to that of optimized Fortran implementations.","Algorithms; Design; G.1.8 [Numerical Analysis]: Partial Differential Equations - finite difference methods; G.4 [Mathematical Software]: - efficiency, user interfaces; Object-oriented design; Optimization; Performance; Simulation",
Algorithm 817 P2MESH: Generic object-oriented interface between 2-D unstructured meshes and FEM/FVM-based PDE solvers,2002,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0039483051&doi=10.1145%2f513001.513007&partnerID=40&md5=93f4588c2c4259a70fa25d82234e541f,"The software interface P2MESH is a collection of C++ class templates suitable for developing prototypes of high-performance PDE solvers on unstructured 2-D meshes. P2MESH supports several discretization methods on triangles and quadrilaterals, such as finite volume or finite element. The design philosophy of P2MESH does not consider specific model problems or built-in approximation algorithms. The software package is general purpose and it may also be used as a building block in the implementation of numerical codes both for engineering applications and mathematical problems.","D.1.5 [Programming Techniques]: Object-oriented programming; D.3.3 [Programming Languages]: Language constructs and features - Classes and objects, data types and structures",
Algorithm 816: r2d2lri: An algorithm for automatic two-dimensional cubature,2002,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0039483058&doi=10.1145%2f513001.513006&partnerID=40&md5=7a2feb5f10a9877c24c5050f8f89b269,"r2d2lri is a non-adaptive algorithm implemented in C++ for performing automatic cubature over a wide variety of finite and non-finite two-dimensional domains. The core integrator uses a sixth-order Sidi transformation applied to a sequence of embedded lattice rules in such a fashion as to incur virtually no computational overhead. Even for integrals over non-finite domains, for which several non-finite to finite transformations may be attempted, the algorithm remains very fast. Performance data are presented which demonstrate both the effectiveness and efficiency of r2d2lri, taking into account the number of function evaluations needed and the execution speed.",Algorithms; Automatic integration; Efficiency; G.1.4 [Numerical Analysis]: Quadrature and numerical differentiation - Multidimensional (multiple) quadrature; G.4 [Mathematical Software]: - Algorithm design and analysis; Reliability and robustness,
Renovating the collected algorithms from ACM,2002,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0040668354&doi=10.1145%2f513001.513005&partnerID=40&md5=7d0bff949061ba54a1b43e2da8bda76f,"Since 1960 the Association for Computing Machinery has published a series of refereed algorithm implementations known as the Collected Algorithms of the ACM (CALGO). Most of those published since 1975 are mathematical algorithms, and many of them remain useful today. In this paper we describe measures that have been taken to bring some 300 of these latter codes to an up-to-date and consistent state.","Algorithms; D.2.7 [Software Engineering]: Distribution, maintenance, and enhancement - Portability; restructuring, reverse engineering, and reengineering; G.4 [Mathematical Software]: - Certification and testing; reliability and robustness",
Numerical bifurcation analysis of delay differential equations using DDE-BIFTOOL,2002,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0039483059&doi=10.1145%2f513001.513002&partnerID=40&md5=aa0f122c6db8a97eac66f9e923cd7524,"We describe DDE-BIFTOOL, a Matlab package for numerical bifurcation analysis of systems of delay differential equations with several fixed, discrete delays. The package implements continuation of steady state solutions and periodic solutions and their stability analysis. It also computes and continues steady state fold and Hopf bifurcations and, from the latter, it can switch to the emanating branch of periodic solutions. We describe the numerical methods upon which the package is based and illustrate its usage and capabilities through analysing three examples: two models of coupled neurons with delayed feedback and a model of two oscillators coupled with delay.",G.1.0 [Numerical analysis]: General - numerical algorithms; G.1.4 [Numerical analysis]: Quadrature and Numerical Differentiation; G.1.7 [Numerical analysis]: Ordinary Differential Equations - multistep and multivalue methods,
Algorithm 814: Fortran 90 software for floating-point multiple precision arithmetic gamma and related functions,2001,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0038068331&doi=10.1145%2f504210.504211&partnerID=40&md5=a3b9c4866c20fdda9ebb0320eefefad3,A collection of Fortran-90 routines for evaluating the gamma function and related functions using the FM multiple-precision arithmetic package.,G.1.0 [Numerical Analysis]: General - Computer arithmetic; G.1.2 [Numerical Analysis]: Approximation - Special function approximations,
FLAME: Formal linear algebra methods environment,2001,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0039435412&doi=10.1145%2f504210.504213&partnerID=40&md5=30ad602957ef129bdf75b790f5604559,"Since the advent of high-performance distributed-memory parallel computing, the need for intelligible code has become ever greater. The development and maintenance of libraries for these architectures is simply too complex to be amenable to conventional approaches to implementation. Attempts to employ traditional methodology have led, in our opinion, to the production of an abundance of anfractuous code that is difficult to maintain and almost impossible to upgrade. Having struggled with these issues for more than a decade, we have concluded that a solution is to apply a technique from theoretical computer science, formal derivation, to the development of high-performance linear algebra libraries. We think the resulting approach results in aesthetically pleasing, coherent code that greatly facilitates intelligent modularity and high performance while enhancing confidence in its correctness. Since the technique is language-independent, it lends itself equally well to a wide spectrum of programming languages (and paradigms) ranging from C and Fortran to C++ and Java. In this paper, we illustrate our observations by looking at the Formal Linear Algebra Methods Environment (FLAME), a framework that facilitates the derivation and implementation of linear algebra algorithms on sequential architectures. This environment demonstrates that lessons learned in the distributed-memory world can guide us toward better approaches even in the sequential world. We present performance experiments on the Intel (R) Pentium (R) III processor that demonstrate that high performance can be attained by coding at a high level of abstraction.",D.2.11 [Software Engineering]: Software architectures -domain specific architectures; D.2.2 [Software Engineering]: Design tools and techniques -software libraries; G.4 [Mathematical Software]: - Algorithm design and analysis,
Algorithm 815: FORTRAN subroutines for computing approximate solutions of feedback set problems using GRASP,2001,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0040027544&doi=10.1145%2f504210.504214&partnerID=40&md5=3c8f93f28eacdd191c85f37f8777bf4c,We propose FORTRAN subroutines for approximately solving the feedback vertex and arc set problems on directed graphs using a Greedy Randomized Adaptive Search Procedure (GRASP). Implementation and usage of the package is outlined and computational experiments are reported illustrating solution quality as a function of running time.,Algorithms; Combinatorial optimization; G.1.6 [Numerical Analysis]: Optimization - Integer programming; G.2.1 [Discrete Mathematics]: Combinatorics - Combinatorial algorithms; G.M [Mathematics of Computing]: Miscellaneous; Performance,
Analysis and comparison of two general sparse solvers for distributed memory computers,2001,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0013092252&doi=10.1145%2f504210.504212&partnerID=40&md5=4dea4d7f94b1015bc17d1285110b437c,"This paper provides a comprehensive study and comparison of two state-of-the-art direct solvers for large sparse sets of linear equations on large-scale distributed-memory computers. One is a multifrontal solver called MUMPS, the other is a supernodal solver called SuperLU. We describe the main algorithmic features of the two solvers and compare their performance characteristics with respect to uniprocessor speed, interprocessor communication, and memory requirements. For both solvers, preorderings for numerical stability and sparsity play an important role in achieving high parallel efficiency. We analyse the results with various ordering algorithms. Our performance analysis is based on data obtained from runs on a 512-processor Cray T3E using a set of matrices from real applications. We also use regular 3D grid problems to study the scalability of the two solvers.","Algorithms; G.1.3 [Numerical Analysis]: Numerical linear algebra -sparse, structured, and very large systems (direct and iterative methods); G.4 [Mathematics of Computing]: Mathematical software - Parallel and vector implementations; Performance",
A Case Study in the Performance and Scalability of Optimization Algorithms,2001,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0011791902&doi=10.1145%2f502800.502805&partnerID=40&md5=6af1cd6d0267fd7e082cd007356fdbbe,"We analyze the performance and scalabilty of algorithms for the solution of large optimization problems on high-performance parallel architectures. Our case study uses the GPCG (gradient projection, conjugate gradient) algorithm for solving bound-constrained convex quadratic problems. Our implementation of the GPCG algorithm within the Toolkit for Advanced Optimization (TAO) is available for a wide range of high-performance architectures and has been tested on problems with over 2.5 million variables. We analyze the performance as a function of the number of variables, the number of free variables, and the preconditioner. In addition, we discuss how the software design facilitates algorithmic comparisons.",Algorithms; D.2.13 [Software Engineering]: Reusable Software - reusable libraries; G.1.6 [Numerical Analysis]: Optimization - quadratic programming methods; G.4 [Mathematical Software]: Parallel and vector implementations; Performance,
An automatic continuation strategy for the solution of singularly perturbed nonlinear boundary value problems,2001,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0037855750&doi=10.1145%2f383738.383742&partnerID=40&md5=37b94c503436f31b145e6e05cfafbff2,"In a recent paper, the present authors derived an automatic continuation algorithm for the solution of linear singular perturbation problems. The algorithm was incorporated into two general-purpose codes for solving boundary value problems, and it was shown to deal effectively with a large test set of linear problems. The present paper describes how the continuation algorithm for linear problems can be extended to deal with the nonlinear case. The results of extensive numerical testing on a set of nonlinear singular perturbation problems are given, and these clearly demonstrate the efficacy of continuation for solving such problems.",Algorithms; Automatic continuation; G.1.7 [Numerical Analysis]: Ordinary Differential Equations - Boundary value problems; G.4 [Mathematics of Computing]: Mathematical Software; Mesh selection; Nonlinear boundary value problems; Performance; Theory,
A BVP Solver Based on Residual Control and the MATLAB PSE,2001,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0012027011&doi=10.1145%2f502800.502801&partnerID=40&md5=d6eb7c17d0fc28c0e493bcd2da625606,"Our goal was to make it as easy as possible to solve a large class of boundary value problems (BVPs) for ordinary differential equations in the MATLAB problem solving environment (PSE). We present here theoretical and software developments resulting in bvp4c, a capable BVP solver that is exceptionally easy to use.",Collocation method; G.1.7 [Numerical Analysis]: Ordinary Differential Equations - boundary value problems; G.4 [Mathematics of Computing]: Mathematical Software; MATLAB; Residual control,
Algorithm 811: NDA: Algorithms for nondifferentiable optimization,2001,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-18044402795&doi=10.1145%2f383738.383740&partnerID=40&md5=b80d068c17633dfc46d5ccf829443630,"We present four basic Fortran subroutines for nondifferentiable optimization with simple bounds and general linear constraints. Subroutine PMIN, intended for minimax optimization, is based on a sequential quadratic programming variable metric algorithm. Subroutines PBUN and PNEW, intended for general nonsmooth problems, are based on bundle-type methods. Subroutine PVAR is based on special nonsmooth variable metric methods. Besides the description of methods and codes, we propose computational experiments which demonstrate the efficiency of this approach.",Algorithms; D.3.2 [Programming Languages]: Language Classifications - FORTRAN 77; Discrete Chebyshev approximation; G.1.6 [Numerical Analysis]: Optimization; G.4 [Mathematics of Computing]: Mathematical Software; Minimax optimization,
A Revised Simplex Method with Integer Q-Matrices,2001,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0345877011&doi=10.1145%2f502800.502804&partnerID=40&md5=e6ebb95788102074961122d232275db2,We describe a modification of the simplex formulas in which Q-matrices are used to implement exact computations with an integer multiprecision library. Our motivation comes from the need for efficient and exact incremental solvers in the implementation of constraint solving languages such as PROLOG. We explain how to reformulate the problem and the different steps of the simplex algorithm. We compare some measurements obtained with integer and rational computations.,G.1.3 [Numerical Analysis]: Numerical Linear Algebra; Integer calculations; Linear systems; Performance; Q-matrices; Simplex method,
Algorithm 810: The SLEIGN2 Sturm-Liouville code,2001,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0038594418&doi=10.1145%2f383738.383739&partnerID=40&md5=53aeac99d775ab74444bcfe25d83ad2e,"The SLEIGN2 code is based on the ideas and methods of the original SLEIGN code of 1979. The main purpose of the SLEIGN2 code is to compute eigenvalues and eigenfunctions of regular and singular self-adjoint Sturm-Liouville problems, with both separated and coupled boundary conditions, and to approximate the continuous spectrum in the singular case. The code uses some new algorithms, which we describe, and has a driver program that offers a user-friendly interface. In this paper the algorithms and their implementation are discussed, and the class of problems to which each algorithm applies is identified.",Algorithms; D.3.2 [Programming Languages]: Language Classifications - FORTRAN 77; G.1.7 [Numerical Analysis]: Ordinary Differential Equations - Boundary value problems; G.4 [Mathematics of Computing]: Mathematical Software; Sturm-Liouville,
Compression of Particle Data from Hierarchical Approximate Methods,2001,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0347768279&doi=10.1145%2f502800.502802&partnerID=40&md5=b8e3f9b293e5aaa28e26a0e6c2b6a54b,"This article presents an analytical and computational framework for the compression of particle data resulting from hierarchical approximate treecodes such as the Barnes-Hut and Fast Multipole Methods. Due to approximations introduced by hierarchical methods, various parameters (such as position, velocity, acceleration, potential) associated with a particle can be bounded by distortion radii. Using this distortion radii, we develop storage schemes that guarantee error bounds while maximizing compression. Our schemes make extensive use of spatial and temporal coherence of particle behavior and yield compression ratios higher than 12:1 over raw data, and 6:1 over gzipped (LZ) raw data for selected simulations instances. We demonstrate that for uniform distributions with 2M particles, storage requirements can be reduced from 24 MB to about 1.8 MB (about 7 bits per particle per timestep) for storing particle positions. This is significant because it enables faster storage/retrieval, better temporal resolution, and improved analysis. Our results are shown to scale from small systems (2K particles) to much larger systems (over 2M particles). The associated algorithm is asymptotically optimal in computation time (O(n)) with a small constant. Our implementations are demonstrated to run extremely fast - much faster than the time it takes to compute a single time-step advance. In addition, our compression framework relies on a natural hierarchical representation upon which other analysis tasks such as segmented and window retrieval can be built.",E.2 [Data]: Data Storage Representations; E.4 [Data]: Coding and Information Theory - data compaction and compression; G.1.0 [Numerical Analysis]: General - error analysis; G.1.10 [Numerical Analysis]: Applications,
A recursive formulation of Cholesky factorization of a matrix in packed storage,2001,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-18044400448&doi=10.1145%2f383738.383741&partnerID=40&md5=f492930f2c93d12e8b37d769f0daec65,"A new compact way to store a symmetric or triangular matrix called RPF for Recursive Packed Format is fully described. Novel ways to transform RPF to and from standard packed format are included. A new algorithm, called RPC for Recursive Packed Cholesky, that operates on the RPF format is presented. Algorithm RPC is based on level-3 BLAS and requires variants of algorithms TRSM and SYRK that work on RPF. We call these RP_TRSM and RP_SYRK and find that they do most of their work by calling GEMM. It follows that most of the execution time of RPC lies in GEMM. The advantage of this storage scheme compared to traditional packed and full storage is demonstrated. First, the RPC storage format uses the minimal amount of storage for the symmetric or triangular matrix. Second, RPC gives a level-3 implementation of Cholesky factorization whereas standard packed implementations are only level 2. Hence, the performance of our RPC implementation is decidedly superior. Third, unlike fixed block size algorithms, RPC requires no block size tuning parameter. We present performance measurements on several current architectures that demonstrate improvements over the traditional packed routines. Also SMP parallel computations on the IBM SMP computer are made. The graphs that are attached in Section 7 show that the RPC algorithms are superior by a factor between 1.6 and 7.4 for order around 1000, and between 1.9 and 10.3 for order around 3000 over the traditional packed algorithms. For some architectures, the RPC performance results are almost the same or even better than the traditional full-storage algorithm results.",G. 1.3 [Numerical Analysis]: Numerical Linear Algebra - Linear systems (direct and iterative methods); G.4 [Mathematics of Computing]: Mathematical Software,
Algorithm 812: BPOLY: An object-oriented library of numerical algorithms for polynomials in bernstein form,2001,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0038375748&doi=10.1145%2f383738.383743&partnerID=40&md5=2096933c82e19acda7e2cfd4070a326e,"The design, implementation, and testing of a C++ software library for univariate polynomials in Bernstein form is described. By invoking the class environment and operator overloading, each polynomial in an expression is interpreted as an object compatible with the arithmetic operations and other common functions (subdvision, degree elevation, differentiation and integration, composition, greatest common divisor, real-root solving, etc.) for polynomials in Bernstein form. The library allows compact and intuitive implementation of lengthy manipulations of Bernstein-form polynomials, which often arise in computer graphics and computeraided design and manufacturing applications. A series of empirical tests indicates that the library functions are typically very accurate and reliable, even for polynomials of surprisingly high degree.",Algorithms; Bernstein basis; G.1.0 [Numerical Analysis]: General; G.1.5 [Numerical Analysis]: Roots of Nonlinear Equations; G.4 [Mathematics of Computing]: Mathematical Software-Algorithm design and analysis; Performance; Theory,
A precision- and range-independent tool for testing floating-point arithmetic II: Conversions,2001,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0012062683&doi=10.1145%2f382043.382405&partnerID=40&md5=5d06afaec20f8870974366aa700594ed,"The IEEE 754 and 854 standards for floating-point arithmetic are essentially a specification of a programming environment, encompassing aspects from computer hardware, operating systems, and compilers to programming languages (see especially Section 8). Parts I and II of this paper together describe a tool to test floating-point implementations of arbitrary precision and exponent range (hardware as well as software) for compliance with the principles outlined in the IEEE standards. The tool consists of a driver program, together with a very large set of test vectors encoded in a precision-independent syntax. In Part I we have covered the testing of the basic operations +, -, ×, / and of the square root and remainder functions. In Part II we describe the extension of the test tool to deal with conversions between floating-point formats, conversions between floating-point and integer formats, the rounding of floating-point numbers to integral values, and binary-decimal conversions. Conversions can now be tested from a floating-point format of arbitrary precision and exponent range to another arbitrary smaller (larger) floating-point format as well as to and from fixed hardware integer formats. Conversions between the bases 2 and 10 can be tested for a number of precisions ranging from single (24 bits), double (53 bits), long double or extended (64 bits) to quadruple (113 bits) precision and a proper multiprecision (240 bits) format. We conclude Part II with some applications of our test tool and report on the results of testing various floating-point implementations, meaning various language-compiler-hardware combinations as well as multiprecision libraries.",Arithmetic; D.3.0 [Programming Languages]: General - Standards; D.3.4 [Programming Languages]: Processors - Compilers; Floating-point; G.1.0 [Numerical Analysis]: General - Computer arithmetic; IEEE floating-point standard; Multiprecision; Verification,
Algorithm 813: SPG - Software for Convex-Constrained Optimization,2001,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-18044400238&doi=10.1145%2f502800.502803&partnerID=40&md5=e9787d36e0f82ee1cc9431a7cbefad36,"Fortran 77 software implementing the SPG method is introduced. SPG is a nonmonotone projected gradient algorithm for solving large-scale convex-constrained optimization problems. It combines the classical projected gradient method with the spectral gradient choice of steplength and a non-monotone line-search strategy. The user provides objective function and gradient values, and projections onto the feasible set. Some recent numerical tests are reported on very large location problems, indicating that SPG is substantially more efficient than existing general-purpose software on problems for which projections can be computed efficiently.",Algorithms; Bound constrained problems; D.3.2 [Programming Languages]: Language Classifications; G.1.6 [Numerical Analysis]: Optimization - gradient methods; G.4 [Mathematics of Computing]: Mathematical Software; Large-scale problems,
"A precision- and range-independent tool for testing floating-point arithmetic I: Basic operations, square root, and remainder",2001,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0012096424&doi=10.1145%2f382043.382404&partnerID=40&md5=ff21f16bcff985ff37f4c5e5a127c4f0,"This paper introduces a precision- and range-independent tool for testing the compliance of hardware or software implementations of (multiprecision) floating-point arithmetic with the principles of the IEEE standards 754 and 854. The tool consists of a driver program, offering many options to test only specific aspects of the IEEE standards, and a large set of test vectors, encoded in a precision-independent syntax to allow the testing of basic and extended hardware formats as well as multiprecision floating-point implementations. The suite of test vectors stems on one hand from the integration and fully precision- and range-independent generalization of existing hardware test sets, and on the other hand from the systematic testing of exact rounding for all combinations of round and sticky bits that can occur. The former constitutes only 50% of the resulting test set. In the latter we especially focus on hard-to-round cases. In addition, the test suite implicitly tests properties of floating-point operations, following the idea of Paranoia, and it reports which of the three IEEE-compliant underflow mechanisms is used by the floating-point implementation under consideration. We also check whether that underflow mechanism is used consistently. The tool is backward compatible with the UCBTEST package and with Coonen's test syntax.",Arithmetic; D.3.0 [programming languages]: General - Standards; Floating-point; G.1.0 [numerical analysis]: General - Computer arithmetic; IEEE floating-point standard; Multiprecision; Validation; Verification,
Algorithm 809: PREQN: Fortran 77 subroutines for preconditioning the conjugate gradient method,2001,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0011630080&doi=10.1145%2f382043.382343&partnerID=40&md5=b7ed9498c589cbc4662cd0ddea8be1f8,"PREQN is a package of Fortran 77 subroutines for automatically generating preconditioners for the conjugate gradient method. It is designed for solving a sequence of linear systems Aix = bi, i = 1, . . . , t, where the coefficient matrices Ai are symmetric and positive definite and vary slowly. Problems of this type arise, for example, in nonlinear optimization. The preconditioners are based on limited-memory quasi-Newton updating and are recommended for problems in which (i) the coefficient matrices are not explicitly known and only matrix-vector products of the form Aiv can be computed; or (ii) the coefficient matrices are not sparse. PREQN is written so that a single call from a conjugate gradient routine performs the preconditioning operation and stores information needed for the generation of a new preconditioner.",G.1.3 [Numerical Analysis]: Numerical linear algebra - Linear systems (direct and iterative methods); G.1.6 [Numerical Analysis]: Optimization - Gradient methods; Unconstrained optimization,
Solving systems of partial differential equations using object-oriented programming techniques with coupled heat and fluid flow as example,2001,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0012439715&doi=10.1145%2f382043.382045&partnerID=40&md5=69b3f64976e621631f15a1250ac198db,"This paper exploits object-oriented implementation techniques to facilitate the development of computer codes for solving systems of coupled partial differential equations. We show how to build a simulator for equation systems by merging independent solvers for each equation that enters the system. The main goal is to obtain a rapid, robust, and reliable software development process with extensive reuse of implemented code. Coupled heat and fluid flow in pipes is used as example for illustrating the implementation techniques. We also present some results for the particular case of temperature-dependent generalized Newtonian fluid flow between two nonconcentric cylinders. The general applicability of the approach is discussed.",D.1.5 [Programming Techniques]: Object-Oriented Programming; Design; G.1.8 [Numerical Analysis]: Partial Differential Equations - Finite-element methods; Object-oriented programming; Software development; Systems of partial differential equations,
Estimation of parameters and eigenmodes of multivariate autoregressive models,2001,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0002537923&doi=10.1145%2f382043.382304&partnerID=40&md5=f1d1a45baa7ee66e2d118961a488de14,"Dynamical characteristics of a complex system can often be inferred from analyses of a stochastic time series model fitted to observations of the system. Oscillations in geophysical systems, for example, are sometimes characterized by principal oscillation patterns, eigenmodes of estimated autoregressive (AR) models of first order. This paper describes the estimation of eigenmodes of AR models of arbitrary order. AR processes of any order can be decomposed into eigenmodes with characteristic oscillation periods, damping times, and excitations. Estimated eigenmodes and confidence intervals for the eigenmodes and their oscillation periods and damping times can be computed from estimated model parameters. As a computationally efficient method of estimating the parameters of AR models from high-dimensional data, a stepwise least squares algorithm is proposed. This algorithm computes model coefficients and evaluates criteria for the selection of the model order stepwise for AR models of successively decreasing order. Numerical simulations indicate that, with the least squares algorithm, the AR model coefficients and the eigenmodes derived from the coefficients are estimated reliably and that the approximate 95% confidence intervals for the coefficients and eigenmodes are rough approximations of the confidence intervals inferred from the simulations.",G.3 [Mathematics of Computing]: Probability and Statistics - Markov processes; I.6.4 [Simulation and Modeling]: Model Validation and Analysis; Multivariate statistics; Statistical computing; Stochastic processes; Time series analysis,
Algorithm 808: ARFIT - A Matlab package for the estimation of parameters and eigenmodes of multivariate autoregressive models,2001,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0002537922&doi=10.1145%2f382043.382316&partnerID=40&md5=7698a2da9bf9eb8d7b53feeecf5a299c,"ARFIT is a collection of Matlab modules for modeling and analyzing multivariate time series with autoregressive (AR) models. ARFIT contains modules for fitting AR models to given time series data, for analyzing cigenmodes of a fitted model, and for simulating AR processes. ARFIT estimates the parameters of AR models from given time series data with a stepwise least squares algorithm that is computationally efficient, in particular when the data are high-dimensional. ARFIT modules construct approximate confidence intervals for the estimated parameters and compute statistics with which the adequacy of a fitted model can be assessed. Dynamical characteristics of the modeled time series can be examined by means of a decomposition of a fitted AR model into eigenmodes and associated oscillation periods, damping times, and excitations. The ARFIT module that performs the eigendecomposition of a fitted model also constructs approximate confidence intervals for the eigenmodes and their oscillation periods and damping times.",G.3 [Mathematics of Computing]: Probability and Statistics - Markov processes; G.4 [Mathematics of Computing]: Mathematical Software - Documentation; Multivariate statistics; Statistical software; Stochastic processes; Time series analysis,
A simple universal generator for continuous and discrete univariate T-concave distributions,2001,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0038076396&doi=10.1145%2f382043.382322&partnerID=40&md5=dd64312ab1a23924ba9d4ec5ccc0c653,"We use inequalities to design short universal algorithms that can be used to generate random variates from large classes of univariate continuous or discrete distributions (including all log-concave distributions). The expected time is uniformly bounded over all these distributions. The algorithms can be implemented in a few lines of high-level language code. In opposition to other black-box algorithms hardly any setup step is required, and thus it is superior in the changing-parameter case.",Algorithms; Continuous distributions; Discrete distributions; G.3 [Mathematics of Computing]: Probability and statistics - Random number generation; Nonuniform random variates; Ratio-of-uniforms method; Transformed density rejection; Universal method,
Remark on Algorithm 723: Fresnel Integrals,2000,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024254027&doi=10.1145%2f365723.365737&partnerID=40&md5=c27f2bf0d38083ae85a9f03f334e2dd0,"Permission to make digital /hard copy of part or all of this work for personal or classroom use is granted without fee provided that the copies are not made or distributed for profit or commercial advantage, the copyright notice, the title of the publication, and its date appear, and notice is given that copying is by permission of the ACM, Inc. To copy otherwise, to republish, to post on servers, or to redistribute to lists, requires prior specific permission and/or a fee. © 2000, ACM. All rights reserved.",,
Remark on algorithm 761: Scattered-data surface fitting that has the accuracy of a cubic polynomial,2000,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0041037782&doi=10.1145%2f347837.349795&partnerID=40&md5=1800be12061819b8a982f3f6cf9234ac,"Several improvements to the estimation of partial derivatives in Algorithm 761 are presented. The problems corrected are (1) in the calculation of the probability weight in subroutine SDPD3P which may result in overflow, (2) in the calculation of final weight in subroutine SDPD3P which may result in overflow, (3) in the computation of a determinant in subroutine SDLEQN which is not necessary, and (4) in the computation of the condition number of a matrix in subroutine SDLEQN which generates very different results for matrices that differ only in row order.",Algorithms; Bivariate interpolation; Interpolation; Local interpolation,
An observation on bisection software for the symmetric tridiagonal eigenvalue problem,2000,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0041114071&doi=10.1145%2f365723.365728&partnerID=40&md5=5366307dbf6a0f696251ed5d0fd0c583,"In this article we discuss a small modification of the bisection routines in EISPACK and LAPACK for finding a few of the eigenvalues of a symmetric tridiagonal matrix A. When the principal minors of the matrix A yield good approximations to the desired eigenvalues, these modifications can yield about 30% reduction in the computation times.",Bisection; Eigenvalues,
Editorial: Special Issue in Honor of John Rice's 65th Birthday,2000,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025424076&doi=10.1145%2f353474.354094&partnerID=40&md5=a0072122d0533b434397c8c964b6e053,[No abstract available],,
Algorithm 803: A Simpler Macro Processor,2000,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0346484434&doi=10.1145%2f353474.353484&partnerID=40&md5=7e76e43dbd0ed40d2988bd76542203d9,"Macro processors have been in the computing tool chest since the late 1950's. Their use, though perhaps not what it was in the heyday of assembly language programming, is still widespread. In the past, producing a full-featured macro processor has required significant effort, similar to that required to implement the front-end to a compiler augmented by appropriate text substitution capabilities. The tool described here adopts a different approach. The text containing macro definitions and substitutions is, in a sense, ""compiled"" to produce a program, and this program must then be executed to produce the final output.",Algorithms; Awk; Portable; Simple,
A simple method for generating gamma variables,2000,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0040519931&doi=10.1145%2f358407.358414&partnerID=40&md5=5c6a39971a9d1c653a26957233a3f99a,"We offer a procedure for generating a gamma variate as the cube of a suitably scaled normal variate. It is fast and simple, assuming one has a fast way to generate normal variables. In brief: generate a normal variate x and a uniform variate U until In(U) < 0.5x2 + d - dv + dln(u), then return dv. Here, the gamma parameter is α ≥ 1, and v = (1 + x/√9d)3, with d = α - 1/3. The efficiency is high, exceeding 0.951, 0.981, 0.992, 0.996 at α = 1, 2, 4, 8. The procedure can be made to run faster by means of a simple squeeze that avoids the two logarithms most of the time: return dv if U < 1 - 0.0331x4. We give a short C program for any α ≥ 1, and show how to boost an α < 1 into an α > 1. The gamma procedure is particularly fast for C implementation if the normal variate is generated in-line, via the #define feature. We include such an inline version, based on our ziggurat method. With it, and an inline uniform generator, gamma variates can be produced in 400MHz CPUs at better than 1.3 million per second, with the parameter a changing from call to call. Categories and Subject Descriptors: G.4 [Mathematics of Computing]: Mathematical Software; 1.6 [Computing Methodologies]: Simulation and Modeling.",Algorithms; Gamma distribution; Performance; Random number generation; Ziggurat method,
JOHN R. RICE Biographical and Professional Notes,2000,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025409016&doi=10.1145%2f353474.354105&partnerID=40&md5=8ea1cbc148b727940fd7e23135cdca4b,[No abstract available],,
Corrigendum: Algorithm 806: Sprng: A Scalable Library for Pseudorandom Number Generation,2000,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77955963209&doi=10.1145%2f365723.365738&partnerID=40&md5=3b0f247bfa8c77cc11c63bd850508be0,"In this article we present background, rationale, and a description of the Scalable Parallel Random Number Generators (SPRNG) library. We begin by presenting some methods for parallel pseudorandom number generation. We will focus on methods based on parameterization, meaning that we will not consider splitting methods such as the leap-frog or blocking methods. We describe, in detail, parameterized versions of the following pseudorandom number generators: (i) linear congruential generators, (ii) shift-register generators, and (iii) lagged-Fibonacci generators. We briefly describe the methods, detail some advantages and disadvantages of each method, and recount results from number theory that impact our understanding of their quality in parallel applications. SPRNG was designed around the uniform implementation of different families of parameterized random number generators. We then present a short description of SPRNG. The description contained within this document is meant only to outline the rationale behind and the capabilities of SPRNG. Much more information, including examples and detailed documentation aimed at helping users with putting and using SPRNG on scalable systems is available at http://sprng.cs.fsu.edu. In this description of SPRNG we discuss the random-number generator library as well as the suite of tests of randomness that is an integral part of SPRNG. Random-number tools for parallel Monte Carlo applications must be subjected to classical as well as new types of empirical tests of randomness to eliminate generators that show defects when used in scalable environments. © 2000, ACM. All rights reserved.",,
How to Vectorize the Algebraic Multilevel Iteration,2000,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0345853510&doi=10.1145%2f353474.353483&partnerID=40&md5=07c1c5ad216410861be920cf0aec0b1d,"We consider the algebraic multilevel iteration (AMLI) for the solution of systems of linear equations as they arise from a finite-difference discretization on a rectangular grid. Key operation is the matrix-vector product, which can efficiently be executed on vector and parallel-vector computer architectures if the nonzero entries of the matrix are concentrated in a few diagonals. In order to maintain this structure for all matrices on all levels coarsening in alternating directions is used. In some cases it is necessary to introduce additional dummy grid hyperplanes. The data movements in the restriction and prolongation are crucial, as they produce massive memory conflicts on vector architectures. By using a simple performance model the best of the possible vectorization strategies is automatically selected at runtime. Examples show that on a Fujitsu VPP300 the presented implementation of AMLI reaches about 85% of the useful performance, and scalability with respect to computing time can be achieved.",Algorithms; Large linear systems; Multigrid method; Numerical software; Parallel processing; Performance; Preconditioned iterative solver; Vector computer,
Accurate Approximate Solution of Partial Differential Equations at Off-Mesh Points,2000,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0141691306&doi=10.1145%2f353474.353482&partnerID=40&md5=5768435304c67220490409caf20529c3,"Numerical methods for partial differential equations often determine approximations that are more accurate at the set of discrete meshpoints than they are at the ""off-mesh"" points in the domain of interest. These methods are generally most effective if they are allowed to adjust the location of the mesh points to match the local behavior of the solution. Different methods will typically generate their respective approximations on incompatible, unstructured meshes, and it can be difficult to evaluate the quality of a particular solution, or to visualize important properties of a solution. In this paper we will introduce a generic approach which can be used to generate approximate solution values at arbitrary points in the domain of interest for any method that determines approximations to the solution and low-order derivatives at meshpoints. This approach is based on associating a set of ""collocation"" points with each mesh element and requiring that the local approximation interpolate the meshpoint data and almost satisfy the partial differential equation at the collocation points. The accuracy associated with this interpolation/collocation approach is consistent with the ""meshpoint accuracy"" of the underlying method. The approach that we develop applies to a large class of methods and problems. It uses local information only and is therefore particularly suitable for implementation in a parallel or network computing environment. Numerical examples are given for some second-order problems in two and three dimensions.",Collocation; Rendering; Scientific visualization,
Mining and Visualizing Recommendation Spaces for Elliptic PDEs with Continuous Attributes,2000,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0002491405&doi=10.1145%2f353474.353481&partnerID=40&md5=8b87eab003250783f2cc92f201e7513d,"In this paper we extend previous work in mining recommendation spaces based on symbolic problem features to PDE problems with continuous-valued attributes. We identify the research issues in mining such spaces, present a dynamic programming algorithm from the data-mining literature, and describe how a priori domain metaknowledge can be used to control the complexity of induction. A visualization aid for continuous-valued recommendation spaces is also outlined. Two case studies are presented to illustrate our approach and tools: (i) a comparison of an iterative and a direct linear system solver on nearly singular problems, and (ii) a comparison of two iterative solvers on problems posed on nonrectangular domains. Both case studies involve continuously varying problem and method parameters which strongly influence the choice of best algorithm in particular cases. By mining the results from thousands of PDE solves, we can gain valuable insight into the relative performance of these methods on similar problems.",Associations; Data mining; Performance evaluation; Recommender systems,
A framework for symmetric band reduction,2000,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0039699635&doi=10.1145%2f365723.365735&partnerID=40&md5=e2269d35fb581e42a1b18d451757743d,"We develop an algorithmic framework for reducing the bandwidth of symmetric matrices via orthogonal similarity transformations. This framework includes the reduction of full matrices to banded or tridiagonal form and the reduction of banded matrices to narrower banded or tridiagonal form, possibly in multiple steps. Our framework leads to algorithms that require fewer floating-point operations than do standard algorithms, if only the eigenvalues are required. In addition, it allows for space-time tradeoffs and enables or increases the use of blocked transformations.",Blocked house-holder transformations; Symmetric matrices; Tridiagonalization,
Algorithm 801: POLSYS_PLP: A partitioned linear product homotopy code for solving polynomial systems of equations,2000,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0002299927&doi=10.1145%2f347837.347885&partnerID=40&md5=4b529bf217427a342620a90ba6a2de39,"Globally convergent, probability-one homotopy methods have proven to be very effective for finding all the isolated solutions to polynomial systems of equations. After many years of development, homotopy path trackers based on probability-one homotopy methods are reliable and fast. Now, theoretical advances reducing the number of homotopy paths that must be tracked, and in the handling of singular solutions, have made probability-one homotopy methods even more practical. POLSYS_PLP consists of Fortran 90 modules for finding all isolated solutions of a complex coefficient polynomial system of equations. The package is intended to be used in conjunction with HOMPACK90 (Algorithm 777), and makes extensive use of Fortran 90 derived data types to support a partitioned linear product (PLP) polynomial system structure. PLP structure is a generalization of m-homogeneous structure, whereby each component of the system can have a different m-homogeneous structure. The code requires a PLP structure as input, and although finding the optimal PLP structure is a difficult combinatorial problem, generally physical or engineering intuition about a problem yields a very good PLP structure. POLSYS_PLP employs a sophisticated power series end game for handling singular solutions, and provides support for problem definition both at a high level and via hand-crafted code. Different PLP structures and their corresponding Bezout numbers can be systematically explored before committing to root finding.",Algorithms; Chow-Yorke algorithm; Curve tracking; Fixed point; Globally convergent; Homotopy methods; m-homogeneous; Partitioned linear product; Probability-one; Zero,
Automatic sampling with the ratio-of-uniforms method,2000,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0003011474&doi=10.1145%2f347837.347863&partnerID=40&md5=add383696c1eb434769d1b570b7e81ff,"Applying the ratio-of-uniforms method for generating random variates results in very efficient, fast, and easy-to-implement algorithms. However parameters for every particular type of density must be precalculated analytically. In this article we show, that the ratio-of-uniforms method is also useful for the design of a black-box algorithm suitable for a large class of distributions, including all with log-concave densities. Using polygonal envelopes and squeezes results in an algorithm that is extremely fast. In opposition to any other ratio-of-uniforms algorithm the expected number of uniform random numbers is less than two. Furthermore, we show that this method is in some sense equivalent to transformed density rejection.",Adaptive method; Algorithms; Log-concave; Nonuniform; Random-number generation; Ratio of uniforms; Rejection method; T-concave; Universal method,
Band reduction algorithms revisited,2000,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0038424356&doi=10.1145%2f365723.365733&partnerID=40&md5=a06189c4cf243beb3cd5085c5736837a,"In this paper we explain some of the changes that have been incorporated in the latest version of the LAPACK subroutine for reducing a symmetric banded matrix to tridiagonal form. These modifications improve the performance for larger-bandwidth problems and reduce the number of operations when accumulating the transformations onto the identity matrix, by taking advantage of the structure of the initial matrix. We show that similar modifications can be made to the LAPACK subroutines for reducing a symmetric positive definite generalized eigenvalue problem to a standard symmetric banded eigenvalue problem and for reducing a general banded matrix to bidiagonal form to facilitate the computation of the singular values of the matrix.",Banded matrix; Eigenvalues,
"Remark on algorithm 746: New features of PCOMP, a fortran code for automatic differentiation",2000,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0039334753&doi=10.1145%2f358407.358412&partnerID=40&md5=8787064280f5b112c2d075fdfcc56142,"The software system PCOMP uses automatic differentiation to calculate derivatives of functions that are defined by the user in a modeling language similar to Fortran. This symbolical representation is converted into an intermediate code, which can be interpreted to calculate function and derivative values at run-time within machine accuracy. Furthermore, it is possible to generate Fortran code for function and gradient evaluation, which has to be compiled and linked separately. The first version of PCOMP was introduced in Dobmann et al. [1995]. In this article, we describe a series of extensions and additional features that have been implemented in the meantime. Categories and Subject Descriptors: D.1.2 [Programming Techniques]: Automatic Programming; D.3.2 [Programming Languages]: Language Classifications-Fortran 77; D.3.4 [Programming Languages]: Processors - Code generation; G.1.4 [Numerical Analysis]: Quadrature and Numerical Differentiation; G.4 [Mathematics of Computing]: Mathematical Software.",Algorithms; Automatic differentiation; Forward accumulation; Reverse accumulation,
NEOS and condor: Solving optimization problems over the internet,2000,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0002518120&doi=10.1145%2f347837.347842&partnerID=40&md5=cceb8ddd700364f7f135db064eb7d8cf,"We discuss the use of Condor, a distributed resource management system, as a provider of computational resources for NEOS, an environment for solving optimization problems over the Internet. We also describe how problems are submitted and processed by NEOS, and then scheduled and solved by Condor on available (idle) workstations.",Algorithms; Automatic differentiation; Comp; Design; Languages; Performance; Reliability,
Algorithm 805: Computation and uses of the semidiscrete matrix decomposition,2000,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0039334745&doi=10.1145%2f358407.358424&partnerID=40&md5=d36a294a6be2ae869bb0be7e00b0584d,"We present algorithms for computing a semidiscrete approximation to a matrix in a weighted norm, with the Frobenius norm as a special case. The approximation is formed as a weighted sum of outer products of vectors whose elements are ±1 or 0, so the storage required by the approximation is quite small. We also present a related algorithm for approximation of a tensor. Applications of the algorithms are presented to data compression, filtering, and information retrieval; software is provided in C and in Matlab. Categories and Subject Descriptors: D.3.2 [Programming Languages]: Language Classifications - C; Matlab; G.1.2 [Numerical Analysis]: Approximation; H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval.",Algorithms; Compression; Latent semantic indexing; Matrix decomposition; Semidiscrete decomposition; Singular value decomposition,
Note on generalization in experimental algorithmics,2000,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0012458953&doi=10.1145%2f365723.365734&partnerID=40&md5=212c4cb36c715d8e1510dfa67f00e62b,"A recurring theme in mathematical software evaluation is the generalization of rankings of algorithms on test problems to build knowledge-based recommender systems for algorithm selection. A key issue is to profile algorithms in terms of the qualitative characteristics of benchmark problems. In this methodological note, we adapt a novel all-pairs algorithm for the profiling task; given performance rankings for m algorithms on n problem instances, each described with p features, identify a (minimal) subset of p that is useful for assessing the selective superiority of an algorithm over another, for all pairs of m algorithms. We show how techniques presented in the mathematical software literature are inadequate for such profiling purposes. In conclusion, we also address various statistical issues underlying the effective application of this technique.",Benchmark studies; Experimental algorithmics; Performance evaluation; Profiling,
Algorithm 800: Fortran 77 subroutines for computing the eigenvalues of Hamiltonian matrices I: The square-reduced method,2000,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0003000033&doi=10.1145%2f347837.347852&partnerID=40&md5=373d1ef338bcb1a3d582632ddc92959e,"This article describes LAPACK-based Fortran 77 subroutines for the reduction of a Hamiltonian matrix to square-reduced form and the approximation of all its eigenvalues using the implicit version of Van Loan's method. The transformation of the Hamiltonian matrix to a square-reduced form transforms a Hamiltonian eigenvalue problem of order 2n to a Hessenberg eigenvalue problem of order n. The eigenvalues of the Hamiltonian matrix are the square roots of those of the Hessenberg matrix. Symplectic scaling and norm scaling are provided, which, in some cases, improve the accuracy of the computed eigenvalues. We demonstrate the performance of the subroutines for several examples and show how they can be used to solve some control-theoretic problems.",,
Algorithm 804: Subroutines for the computation of Mathieu functions of integer orders,2000,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0008369914&doi=10.1145%2f358407.358422&partnerID=40&md5=dfcaab486f56225ebf294818111ad5d5,Computer subroutines in C++ for computing Mathieu functions of integer orders are described. The routines can handle a large range of the order n and the parameter h. Sample test results and graphs are given. Categories and Subject Descriptors: D.3.2 [Programming Languages]: Language Classifications - C++; G.1 [Mathematics of Computing]: Numerical Analysis.,Algorithms; Elliptic coordinates; Hill's determinant; Mathieu characteristic determinant; Mathieu characteristic numbers; Mathieu coefficients; Mathieu differential equation; Mathieu functions; Wave equation,
PSBLAS: A library for parallel linear algebra computation on sparse matrices,2000,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0039927058&doi=10.1145%2f365723.365732&partnerID=40&md5=0e96e228be59aeb732070e36c35d35c7,"Many computationally intensive problems in engineering and science give rise to the solution of large, sparse, linear systems of equations. Fast and efficient methods for their solution are very important because these systems usually occur in the innermost loop of the computational scheme. Parallelization is often necessary to achieve an acceptable level of performance. This paper presents the design, implementation, and interface of a library of Basic Linear Algebra Subroutines for sparse matrices (PSBLAS) which is specifically tailored to distributed-memory computers. PSBLAS enables easy, efficient, and portable implementations of parallel iterative solvers for linear systems. The interface keeps in view a Single Program Multiple Data programming model on distributed-memory machines. However, the architecture of the library does not exclude an implementation in different paradigms, such as those based on the shared-memory model.",Basic linear algebra subprograms,
HPFBench: A High Performance Fortran benchmark suite,2000,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0005875647&doi=10.1145%2f347837.347872&partnerID=40&md5=0c40e95d1c522771577e34272774eb26,"The High Performance Fortran (HPF) benchmark suite HPFBench is designed for evaluating the HPF language and compilers on scalable architectures. The functionality of the bench-marks covers scientific software library functions and application kernels that reflect the computational structure and communication patterns in fluid dynamic simulations, fundamental physics, and molecular studies in chemistry and biology. The benchmarks are characterized in terms of FLOP count, memory usage, communication pattern, local memory accesses, array allocation mechanism, as well as operation and communication counts per iteration. The benchmarks output performance evaluation metrics in the form of elapsed times, FLOP rates, and communication time breakdowns. We also provide a benchmark guide to aid the choice of subsets of the benchmarks for evaluating particular aspects of an HPF compiler. Furthermore, we report an evaluation of an industry-leading HPF compiler from the Portland Group Inc. using the HPFBench benchmarks on the distributed-memory IBM SP2.",,
Algorithm 799: Revolve: An implementation of checkpointing for the reverse or adjoint mode of computational differentiation,2000,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0010666451&doi=10.1145%2f347837.347846&partnerID=40&md5=c42de9063a0ddeed502b06db49b615d4,"In its basic form, the reverse mode of computational differentiation yields the gradient of a scalar-valued function at a cost that is a small multiple of the computational work needed to evaluate the function itself. However, the corresponding memory requirement is proportional to the run-time of the evaluation program. Therefore, the practical applicability of the reverse mode in its original formulation is limited despite the availability of ever larger memory systems. This observation leads to the development of checkpointing schedules to reduce the storage requirements. This article presents the function revolve, which generates checkpointing schedules that are provably optimal with regard to a primary and a secondary criterion. This routine is intended to be used as an explicit ""controller"" for running a time-dependent applications program.",Adjoint mode; Algorithms; Checkpointing; Computational differentiation; Reverse mode,
Algorithm 802: An automatic generator for bivariate log-concave distributions,2000,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0040443837&doi=10.1145%2f347837.347908&partnerID=40&md5=48dd8b80d9907a9a0b44ea45d1db8409,"Different automatic (also called universal or black-box) methods have been suggested to sample from univariate log-concave distributions. Our new automatic algorithm for bivariate log-concave distributions is based on the method of transformed density rejection. In order to construct a hat function for a rejection algorithm the bivariate density is transformed by the logarithm into a concave function. Then it is possible to construct a dominating function by taking the minimum of several tangent planes, which are by exponentiation transformed back into the original scale. The choice of the points of contact is automated using adaptive rejection sampling. This means that points that are rejected by the rejection algorithm can be used as additional points of contact. The article describes the details how this main idea can be used to construct Algorithm ALC2D that can generate random pairs from all bivariate log-concave distributions with known domain, computable density, and computable partial derivatives.",Algorithms; Automatic generator; Bivariate log-concave distributions; Rejection method; Universal generator,
ADMIT-1: Automatic differentiation and MATLAB interface toolbox,2000,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0040443838&doi=10.1145%2f347837.347879&partnerID=40&md5=452d5162b01253bbdff46a1223d9ce46,"ADMIT-1 enables the computation of sparse Jacobian and Hessian matrices, using automatic differentiation technology, from a MATLAB environment. Given a function to be differentiated, ADMIT-1 will exploit sparsity if present to yield sparse derivative matrices (in sparse MATLAB form). A generic automatic differentiation tool, subject to some functionality requirements, can be plugged into ADMIT-1; examples include ADOL-C (C/C++ target functions) and ADMAT (MATLAB target functions). ADMIT-1 also allows for the calculation of gradients and has several other related functions. This article provides an introduction to the design and usage of ADMIT-1.",Algorithms; Automatic differentiation; Computational differentiation; Efficient computation of gradient; Graph coloring; Jacobians and Hessians; User interface,
Algorithm 807: The SBR toolbox - Software for successive band reduction,2000,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0012881041&doi=10.1145%2f365723.365736&partnerID=40&md5=580921318ab755b80a16fa94ea549978,"We present a software toolbox for symmetric band reduction via orthogonal transformations, together with a testing and timing program. The toolbox contains drivers and computational routines for the reduction of full symmetric matrices to banded form and the reduction of banded matrices to narrower banded or tridiagonal form, with optional accumulation of the orthogonal transformations, as well as repacking routines for storage rearrangement. The functionality and the calling sequences of the routines are described, with a detailed discussion of the ""control"" parameters that allow adaptation of the codes to particular machine and matrix characteristics. We also briefly describe the testing and timing program included in the toolbox.",Blocked house-holder transformations; Symmetric matrices; Tridiagonalization,
Superconvergent interpolants for collocation methods applied to mixed-order BVODEs,2000,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0039927060&doi=10.1145%2f358407.358410&partnerID=40&md5=8e5784e65ce166d9193d084e6c947e1f,"Continuous approximations to boundary value problems in ordinary differential equations (BVODEs), constructed using collocation at Gauss points, are more accurate at the mesh points than at off-mesh points. From these approximations, it is possible to construct improved continuous approximations by extending the high accuracy that is available at the mesh points to off-mesh points. One possibility is the bootstrap approach, which improves the accuracy of the approximate solution at the off-mesh points in a sequence of steps until the accuracy at the mesh points and off-mesh points is consistent. A bootstrap approach for systems of mixed-order BVODEs is developed to improve approximate solutions produced by COLNEW, a Gauss-collocation-based software package. An implementation of this approach is discussed and numerical results presented which confirm that the improved approximations satisfy the predicted error bounds and are relatively inexpensive to construct. Categories and Subject Descriptors: G.1.0 [Numerical Analysis]: General - Error analysis; G.1.1 [Numerical Analysis]: Interpolation - Spline and piecewise polynomial interpolation; G.1.2 [Numerical Analysis]: Approximation - Spline and piecewise polynomial approximation; G.1.7 [Numerical Analysis]: Ordinary Differential Equations - Boundary value problems; G.A [Mathematics of Computing]: Mathematical Software.",Algorithms; Bootstrap; Collocation; Hermite-Birkhoff interpolation; Performance; Theory,
On stopping criteria in verified nonlinear systems or optimization algorithms,2000,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0039927062&doi=10.1145%2f358407.358418&partnerID=40&md5=74e7d212f1ec155f13192fcf84b50092,"Traditionally, iterative methods for nonlinear systems use heuristic domain and range stopping criteria to determine when accuracy tolerances have been met. However, such heuristics can cause stopping at points far from actual solutions, and can be unreliable due to the effects of round-off error or inaccuracies in data. In verified computations, rigorous determination of when a set of bounds has met a tolerance can be done analogously to the traditional approximate setting. Nonetheless, the range tolerance possibly cannot be met. If the criteria are used to determine when to stop subdivision of n-dimensional bounds into subregions, then failure of a range tolerance results in excessive, unnecessary subdivision, and could make the algorithm impractical. On the other hand, interval techniques can detect when inaccuracies or round-off will not permit residual bounds to be narrowed. These techniques can be incorporated into range thickness stopping criteria that complement the range stopping criteria. In this note, the issue is first introduced and illustrated with a simple example. The thickness stopping criterion is then formally introduced and analyzed. Third, inclusion of the criterion within a general verified global optimization algorithm is studied. An industrial example is presented. Finally, consequences and implications are discussed. Categories and Subject Descriptors: G.1.5 [Numerical Analysis]: Roots of Nonlinear Equations - Error analysis; Iterative methods; Systems of equations; G.1.6 [Numerical Analysis]: Optimization - Unconstrained optimization; Constrained optimization; Global optimization.",Algorithms; GlobSol; Performance; Reliability; Stopping criteria; Verification; Verified computations,
Algorithms for the computation of all Mathieu functions of integer orders,2000,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0000545468&doi=10.1145%2f358407.358420&partnerID=40&md5=e9d716eceddf1a445292ab3296dcb241,"The article presents methods for the computation of all Mathieu functions of integer order, which cover a large range of n and h; previous algorithms were limited to small values of n. The algorithms are given in sufficient detail to enable straightforward implementation. The algorithms can handle a large range of the order n (0-200) and the parameter h (0-4 n). Categories and Subject Descriptors: D.3.2 [Programming Languages]: Language Classifications - C++; G.1 [Mathematics of Computing]: Numerical Analysis.",Algorithms; Elliptic coordinates; Hill's determinant; Mathieu characteristic determinant; Mathieu characteristic numbers; Mathieu coefficients; Mathieu differential equation; Mathieu functions; Wave equation,
A MATLAB differentiation matrix suite,2000,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0000603224&doi=10.1145%2f365723.365727&partnerID=40&md5=3b47c71fb734b949364ee420ebaa16a5,"A software suite consisting of 17 MATLAB functions for solving differential equations by the spectral collocation (i.e., pseudospectral) method is presented. It includes functions for computing derivatives of arbitrary order corresponding to Chebyshev, Hermite, Laguerre, Fourier, and sine interpolants. Auxiliary functions are included for incorporating boundary conditions, performing interpolation using barycentric formulas, and computing roots of orthogonal polynomials. It is demonstrated how to use the package for solving eigenvalue, boundary value, and initial value problems arising in the fields of special functions, quantum mechanics, nonlinear waves, and hydrodynamic stability.",Differentiation matrices; MATLAB; Pseudospectral methods; Spectral collocation methods,
PYTHIA-II: A Knowledge/Database System for Managing Performance Data and Recommending Scientific Software,2000,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0001576756&doi=10.1145%2f353474.353475&partnerID=40&md5=c5a5b215cb2f08a7bba9e61541673ffc,"Often scientists need to locate appropriate software for their problems and then select from among many alternatives. We have previously proposed an approach for dealing with this task by processing performance data of the targeted software. This approach has been tested using a customized implementation referred to as PYTHIA. This experience made us realize the complexity of the algorithmic discovery of knowledge from performance data and of the management of these data together with the discovered knowledge. To address this issue, we created PYTHIA-II - a modular framework and system which combines a general knowledge discovery in databases (KDD) methodology and recommender system technologies to provide advice about scientific software/hardware artifacts. The functionality and effectiveness of the system is demonstrated for two existing performance studies using sets of software for solving partial differential equations. From the end-user perspective, PYTHIA-II allows users to specify the problem to be solved and their computational objectives. In turn, PYTHIA-II (i) selects the software available for the user's problem, (ii) suggests parameter values, and (iii) assesses the recommendation provided. PYTHIA-II provides all the necessary facilities to set up database schemas for testing suites and associated performance data in order to test sets of software. Moreover, it allows easy interfacing of alternative data mining and recommendation facilities. PYTHIA-II is an open-ended system implemented on public domain software and has been used for performance evaluation in several different problem domains.",Data mining; Inductive logic programming; Knowledge discovery in databases; Knowledge-based systems; Performance evaluation; Recommender systems; Scientific software,
Sprng: A scalable library for pseudorandom number generation,2000,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0001290917&doi=10.1145%2f358407.358427&partnerID=40&md5=a7624e060516e528677c1c04ad9d631a,"In this article we present background, rationale, and a description of the Scalable Parallel Random Number Generators (SPRNG) library. We begin by presenting some methods for parallel pseudorandom number generation. We will focus on methods based on parameterization, meaning that we will not consider splitting methods such as the leap-frog or blocking methods. We describe, in detail, parameterized versions of the following pseudorandom number generators: (i) linear congruential generators, (ii) shift-register generators, and (iii) lagged-Fibonacci generators. We briefly describe the methods, detail some advantages and disadvantages of each method, and recount results from number theory that impact our understanding of their quality in parallel applications. SPRNG was designed around the uniform implementation of different families of parameterized random number generators. We then present a short description of SPRNG. The description contained within this document is meant only to outline the rationale behind and the capabilities of SPRNG. Much more information, including examples and detailed documentation aimed at helping users with putting and using SPRNG on scalable systems is available at http://sprng.cs.fsu.edu. In this description of SPRNG we discuss the random-number generator library as well as the suite of tests of randomness that is an integral part of SPRNG. Random-number tools for parallel Monte Carlo applications must be subjected to classical as well as new types of empirical tests of randomness to eliminate generators that show defects when used in scalableenvironments. Categories and Subject Descriptors: D.3.2 [Programming Languages] : Language Classifications- C; C++; Fortran; G.4 [MathematicsofComputingl]: Mathematical Software-Algorithm design and analysis; Documentation; Efficiency; Parallel and vector implementations; Reliability and robustness.",Algorithms; Design; Documentation; Experimentation; Lagged-fibonacci generator; Linear congruential generator; Parallel random-number generators; Performance; Random-number software; Random-number tests; Reliability; Standardization,
Remark on Algorithm 716,1999,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0040364590&doi=10.1145%2f305658.287656&partnerID=40&md5=f3d4e4f31d7022b25e574865f9ccb5d2,"The curve-fitting package TSPACK has been converted to double precision. Also, portability has been improved by eliminating some potential errors.",Algorithms; Convexity preserving; Cubic spline; Exponential spline; G.1.1 [Numerical Analysis]: Interpolation; G.1.2 [Numerical Analysis]: Approximation; G.4 [Mathematics of Computing]: Mathematical Software; Interpolation; Monotonicity preserving,
Algorithm 789: SLTSTPAK: A test package for Sturm-Liouville solvers,1999,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0040364595&doi=10.1145%2f305658.287652&partnerID=40&md5=7b6375bdcbddb258867d4bcfdc50b4d8,"We give technical details of the Sturm-Liouville test package SLTSTPAK, complementing the companion article (this issue) on its design. SLTSTPAK comprises the following: a specification of how to write a routine TSTSET containing a set of Sturm-Liouville problems; a number of routines that act as a harness between a TSTSET, written to this specification, and a driver program. A set of 60 standard problems is provided, but it is simple to replace this by another one.",D.2.5 [Software Engineering]: Testing and Debugging; D.3.2 [Programming Languages]: Language Classifications - Fortran 77; Fortran 90; G.1.7 [Numerical Analysis]: Ordinary Differential Equations - boundary value problems,
An implementation of a Fourier series method for the numerical inversion of the Laplace transform,1999,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0039374302&doi=10.1145%2f326147.326148&partnerID=40&md5=062dd640c7a84e980b0b0b399fb7741a,"Our method is based on the numerical evaluation of the integral which occurs in the Riemann Inversion formula. The trapezoidal rule approximation to this integral reduces to a Fourier series. We analyze the corresponding discretization error and demostrate how this expression can be used in the development of an automatic routine, one in which the user needs to specify only the required accuracy. Categories and Subject Descriptors: D.3.2 [Programming Languages]: Language Classifications-Fortran 77; G.1.0 [Numerical Analysis]: General - Numerical algorithms; G.1.9 [Numerical Analysis]: Integral Equations; G.1.2 [Numerical Analysis]: Approximation - Nonlinear approximation General Terms: Algorithms.",Automatic stopping criterion; Fourier series methods; Laplace transform inversion,
Algorithm 794: Numerical Hankel transform by the Fortran program HANKEL,1999,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0039032796&doi=10.1145%2f317275.317284&partnerID=40&md5=5be194ac8e56d4adaaf41481717cc127,"The numerical evaluation of the Hankel transform poses the problems of both infinite integration and Bessel function calculation. Using the corresponding numerical program routines from the literature, a Fortran program has been written to perform the Hankel transform for real functions, given either in analytical form as subroutines or in discrete form as tabulated data.","Algorithms; D.3.2 [programming languages]: language classifications - FORTRAN 77; F.2.1 [analysis of algorithms and problem complexity]: numerical algorithms and problems - computation of transforms (e.g., fast fourier transfor)",
The RISC BLAS: A blocked implementation of level 3 BLAS for RISC processors,1999,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0039625011&doi=10.1145%2f326147.326150&partnerID=40&md5=7b03bfe4b646da592a0c77e82f7c126b,"We describe a version of the Level 3 BLAS which is designed to be efficient on RISC processors. This is an extension of previous studies by the authors and colleagues on a similar approach for efficient serial and parallel implementations on virtual-memory and shared-memory multiprocessors. All our codes are written in Fortran and use loop-unrolling, blocking, and copying to improve the performance. A blocking technique is used to express the BLAS in terms of operations involving triangular blocks and calls to the matrix-matrix multiplication kernel (GEMM). No manufacturer-supplied or assembler code is used. This blocked implementation uses the same blocking ideas as in our implementation for vector machines except that the ordering of loops is designed for efficient reuse of data held in cache and not necessarily for parallelization. All the codes are specifically tuned for RISC processors. The software also includes a tuned version of GEMM. A parameter which controls the blocking allows efficient exploitation of the memory hierarchy on the various target computers. We present results on a range of RISC-based workstations and multiprocessors: CRAY T3D, DEC 8400 5/300, HP 715/64, IBM SP2, MEIKO CS2-HA, SGI Power Challenge 10000, and SUN UltraSPARC-1 model 140. This implementation of the Level 3 BLAS is available on anonymous FTP, and we welcome input from users to improve and extend our BLAS implementation. Categories and Subject Descriptors: G.4 [Mathematics of Computing]: Mathematical Software; F.2.1 [Analysis of Algorithms and Problem Complexity]: Numerical Algorithms and Problems - Computations on matrices; G.1.0 [Numerical Analysis]: General - Numerical algorithms; G.1.3 [Numerical Analysis]: Numerical Linear Algebra - Linear systems (direct and iterative methods) General Terms: Algorithms, Measurement, Performance.",Blocking; Level 3 BLAS; Loop-unrolling; Matrix-matrix kernels; RISC processors,
Blocked algorithms and software for reduction of a regular matrix pair to generalized Schur form,1999,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0010976738&doi=10.1145%2f332242.332244&partnerID=40&md5=8422ffca222688a9cc714a8ea0020dd8,"A two-stage blocked algorithm for reduction of a regular matrix pair (A, B) to upper Hessenberg-triangular form is presented. In stage 1 (A, B) is reduced to block upper Hessenberg-triangular form using mainly level 3 (matrix-matrix) operations that permit data reuse in the higher levels of a memory hierarchy. In the second stage all but one of the r subdiagonals of the block Hessenberg A-part are set to zero using Givens rotations. The algorithm proceeds in a sequence of supersweeps, each reducing m columns. The updates with respect to row and column rotations are organized to reference consecutive columns of A and B. To further improve the data locality, all rotations produced in a supersweep are stored to enable a left-looking reference pattern, i.e., all updates are delayed until they are required for the continuation of the supersweep. Moreover, we present a blocked variant of the single-diagonal double-shift QZ method for computing the generalized Schur form of (A, B) in upper Hessenberg-triangular form. The blocking for improved data locality is done similarly, now by restructuring the reference pattern of the updates associated with the bulge chasing in the QZ iteration Timing results show that our new blocked variants outperform the current LAPACK routines, including drivers for the generalized eigenvalue problem, by a factor 2-5 for sufficiently large problems. Categories and Subject Descriptors: F.2.1 [Analysis of Algorithms and Problem Complexity]: Numerical Algorithms and Problems-Computations on matrices; G. 1.3 [Numerical Analysis]: Numerical Linear Algebra - Eigenvalues and eigenvectors (direct and iterative methods); G.4 [Mathematics of Computing]: Mathematical Software - Certification and testing; Efficiency; Portability; Reliability and robustness.",F.2.1 [Analysis of Algorithms and Problem Complexity]: Numerical Algorithms and Problems-Computations on matrices; G. 1.3 [Numerical Analysis]: Numerical Linear Algebra - Eigenvalues and eigenvectors (direct and iterative methods),
Remark on Algorithm 751,1999,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0039179988&doi=10.1145%2f305658.305726&partnerID=40&md5=3c81f0950dcddaff2e285a520b48c6a0,"The triangulation package TRIPACK has been revised to run more efficiently and to eliminate some potential errors. Also, a portable triangulation plotting routine was added.",Algorithms; Constrained Delaunay triangulation; Dirichlet tessellation; G.1.1 [Numerical Analysis]: Interpolation; G.1.2 [Numerical Analysis]: Approximation; G.4 [Mathematics of Computing]: Mathematical Software; Interpolation; Thiessen regions,
C++ classes for linking optimization with complex simulations,1999,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0012614704&doi=10.1145%2f317275.317280&partnerID=40&md5=ab95276d46ea9ada8aad7417d25008df,"The object-oriented programming paradigm can be used to overcome the incompatibilities between off-the-shelf optimization software and application software. The Hilbert Class Library (HCL) defines the fundamental mathematical objects arising in optimization problems, such as vectors, linear operators, and so forth, as C++ classes, making it possible to write optimization code in a natural fashion, while allowing application software such as simulators to use the most convenient data structures and programming style. In spite of the poor reputation C++ has for runtime performance, the use of mixed-language programming allows performance equal to that achieved by standard Fortran packages, as comparisons with the popular code LBFGS and ARPACK demonstrate.",Algorithms; D. 1.5 [programming techniques]: object-oriented programming; Languages; Object-oriented design; Optimization; Performance; Simulation,
Remark on Algorithm 752,1999,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0040958765&doi=10.1145%2f305658.305731&partnerID=40&md5=911d042e6669ec9068734f0f39df523a,"The triangulation-based scattered-data fitting package SRFPACK was updated for (a) compatibility with a revised interface to the triangulation package TRIPACK, (b) the elimination of potential errors in the treatment of tension factors and in the extrapolation procedure, and (c) the addition of a more accurate local gradient-estimation procedure and a simple but portable contour-plotting capability.",Algorithms; G.1.1 [Numerical Analysis]: Interpolation; G.1.2 [Numerical Analysis]: Approximation; G.4 [Mathematics of Computing]: Mathematical Software; Interpolation; Scattered-data fitting; Smoothing; Surface under tension,
Complex fans: A representation for vectors in polar form with interval attributes,1999,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0040216904&doi=10.1145%2f317275.317277&partnerID=40&md5=d507a9408a52f238abc352cb55d64269,"If we allow the magnitude and angle of a complex number (expressed in polar form) to range over an interval, it describes a semicircular region, similar to a fan; these regions are what we call complex fans. Complex numbers are a special case of complex fans, where the magnitude and angle are point intervals. Operations (especially addition) with complex numbers in polar form are complicated. What most applications do is to convert them to rectangular form, perform operations, and return the result to polar form. However, if the complex number is a Complex Fan, that transformation increases ambiguity in the result. That is, the resulting Fan is not the smallest Fan that contains all possible results. The need for minimal results took us to develop algorithms to perform the basic arithmetic operations with complex fans, ensuring the result will always be the smallest possible complex fan. We have developed the arithmetic operations of addition, negation, subtraction, product, and division of complex fans. The algorithms presented in this article are written in pseudocode, and the programs in Common Lisp, making use of CLOS (Common Lisp Object System). Translation to any other high-level programming language should be straightforward.",G.4 [mathematics of computing]: mathematical software - algorithm design and analysis; I.2.4 [artificial intelligence]: knowledge representation formalisms and methods; J.2 [computer applications]: physical sciences and engineering - engineering,
Characteristic spectra of the curvature functional: A numerical study in bifurcation,1999,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0039032785&doi=10.1145%2f332242.332245&partnerID=40&md5=253ca8413ade4815b372992e78ad5835,"A method is described for the eigenanalysis of piecewise smooth C2 extremum-energy curves. Typical interpolants are investigated within the framework of their eigensystems, and conclusions are presented concerning their natural modes of vibration, stability state, and limits of existence. In the present discussion the word ""spline"" means exclusively an interpolating elastica.",Acoustics; Buckling; Catast; Eigenvalues and eigenvectors (direct and iterative methods); G.1.1 [Numerical Analysis]: Interpolation - Spline and piecewise polynomial interpolation; G.1.3 [Numerical Analysis]: Numerical Linear Algebra -Determinants,
Algorithm 792: Accuracy tests of ACM algorithms for interpolation of scattered data in the plane,1999,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0008792285&doi=10.1145%2f305658.305745&partnerID=40&md5=211a11e4a73332aad6911b835a4e4f59,"We present results of accuracy tests on scattered-data fitting methods that have been published as ACM algorithms. The algorithms include seven triangulation-based methods and three modified Shepard methods, two of which are new algorithms. Our purpose is twofold: to guide potential users in the selection of an appropriate algorithm and to provide a test suite for assessing the accuracy of new methods (or existing methods that are not included in this survey). Our test suite consists of five sets of nodes, with node counts ranging from 25 to 100, and 10 test functions. These are made available in the form of three Fortran subroutines: TESTDT returns one of the node sets; TSTFN1 returns a value and, optionally, a gradient value, of one of the test functions; and TSTFN2 returns a value, first partials, and second partial derivatives of one of the test functions.",D.3.2 [Programming Languages]: Language Classifications - Fortran 77; G.1.1 [Numerical Analysis]: Interpolation; G.1.2 [Numerical Analysis]: Approximation; G.4 [Mathematics of Computing]: Mathematical Software General Terms: Algorithms,
Self-adapting fortran 77 machine constants: Comment on Algorithm 528,1999,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0040016438&doi=10.1145%2f305658.305711&partnerID=40&md5=57f4d7ecb8e4db628057d5da2e8f9753,"This note discusses user dissatisfaction with the need to uncomment data statements in Algorithm 528, comments on alternative approaches tried by the community, and proposes a solution that is both automatic and safe.",Algorithms; D.3.2 [Programming Languages]: Language Classifications - Fortran 77; d1mach; G.1.0 [Numerical Analysis]: General - computer arithmetic; Languages; Machine environment parameters,
Algorithm 796: A fortran software package for the numerical inversion of the Laplace Transform based on a Fourier series method,1999,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0038006166&doi=10.1145%2f326147.326149&partnerID=40&md5=3d3a103e9b154027cb32277fb2498b7b,"A software package for the numerical inversion of a Laplace Transform function is described. Besides function values of F(z) for complex and real z, the user has only to provide the numerical value of the Laplace convergence abscissa σ0 or, failing this, an upper bound to this quantity, and the accuracy he or she requires in the computed value of the inverse Transform. The method implemented is based on a Fourier series expansion of the inverse transform, and it is especially suitable when such inverse Laplace Transform is sectionally continuous. Categories and Subject Descriptors: D.3.2 [Programming Languages]: Language Classifications-FORTRAN 77; G.1.9 [Numerical Analysis]: Integral Equations; G.1.2 [Numerical Analysis]: Approximation - Nonlinear approximation General Terms: Algorithms.",Automatic stopping criterion; Fourier series methods; Laplace transform inversion,
A note on the recursive calculation of incomplete gamma functions,1999,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0039107841&doi=10.1145%2f305658.305717&partnerID=40&md5=de9df05181f81335dc0f14e008a3f58a,"It is known that the recurrence relation for incomplete gamma functions {γ(a + n, x)}, 0 ≤ a < 1, n = 0,1,2, . . . , when x is positive, is unstable - more so the larger x. Nevertheless, the recursion can be used in the range 0 ≤ n ≤ x practically without error growth, and in larger ranges 0 ≤ n ≤ N with a loss of accuracy that can be controlled by suitably limiting N.",Algorithms; G.1.0 [Numerical Analysis]: General - stability (and instability); G.1.2 [Numerical Analysis]: Approximation; Incomplete gamma functions; Recursive calculation; Reliability,
A test package for Sturm-Liouville solvers,1999,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0039179991&doi=10.1145%2f305658.287651&partnerID=40&md5=303075740d35ceb10903f3e5615dbbfb,"The author and colleagues have produced a collection of 60 test problems which offer a realistic performance test of the currently available automatic codes for eigenvalues of the classical Sturm-Liouville problem. We describe a Fortran implementation and the considerations that went into its design. A novel feature is that (almost) all the code defining one problem is textually contiguous in the Fortran text, unlike for example the DETEST package for ODE initial-value solvers where the definition of a problem is spread over several routines. The described implementation forms the infrastructure of the SLDRIVER interactive package which supports exploration of a set of Sturm-Liouville problems with the four SL-solvers SLEIGN, SLEDGE, SL02F, and SLEIGN2. A ""standard"" set of 60 problems is provided, but it is simple to replace this by another one.",D.2.5 [Software Engineering]: Testing and Debugging; D.3.2 [Programming Languages]: Language Classifications-Fortran 77; Fortran 90; G.1.7 [Numerical Analysis]: Ordinary Differential Equations - boundary value problems,
C++ implementations of numerical methods for solving differential-algebraic equations: Design and optimization considerations,1999,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0000253859&doi=10.1145%2f332242.334001&partnerID=40&md5=5591db0d48427272dc3dbdc99b0a26a5,"Object-oriented programming can produce improved implementations of complex numerical methods, but it can also introduce a performance penalty. Since computational simulation often requires intricate and highly efficient codes, the performance penalty of high-level techniques must always be weighed against the improvements they enable. These issues are addressed in a general object-oriented (OO) toolkit for the numerical solution of differential-algebraic equations (DAEs). The toolkit can be configured in several different ways to solve DAE initial-value problems with an adaptive multistep method. It contains a wrapped version of the Fortran 77 code DASPK and a translation of this code to C++. Two C++ constructs for assembling the tools are provided, as are two implementations of an important DAE test problem. Multiple configurations of the toolkit for DAE test problems are compared in order to assess the performance penalties of C++. The mathematical methods and implementation techniques are discussed in detail in order to provide heuristics for efficient OO scientific programming and to demonstrate the effectiveness of OO techniques in managing complexity and producing better code. The codes were tested on a variety of problems using publicly available Fortran 77 and C++ compilers. Extensive efficiency comparisons are presented in order to isolate computationally inefficient OO techniques. Techniques that caused difficulty in implementation and maintenance are also highlighted. The comparisons demonstrate that the majority of C++'s built-in support for OO programming has a negligible effect on performance, when used at sufficiently high levels, and provides flexible and extensible software for numerical methods. Categories and Subject Descriptors: D.3.2 [Programming Languages]: Language Classifications - C++; Fortran 77 General Terms: Algorithms, Design, Experimentation, Languages, Performance.",Algorithms; D.3.2 [Programming Languages]: Language classifications - C++; Design; Differential-algebraic equations; Experimentation; Fortran 77; Languages; Performance,
A combined unifrontal/multifrontal method for unsymmetric sparse matrices,1999,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0001946784&doi=10.1145%2f305658.287640&partnerID=40&md5=69437aeb357f6373c22298d1c530ad4a,"We discuss the organization of frontal matrices in multifrontal methods for the solution of large sparse sets of unsymmetric linear equations. In the multifrontal method, work on a frontal matrix can be suspended, the frontal matrix can be stored for later reuse, and a new frontal matrix can be generated. There are thus several frontal matrices stored during the factorization, and one or more of these are assembled (summed) when creating a new frontal matrix. Although this means that arbitrary sparsity patterns can be handled efficiently, extra work is required to sum the frontal matrices together and can be costly because indirect addressing is required. The (uni)frontal method avoids this extra work by factorizing the matrix with a single frontal matrix. Rows and columns are added to the frontal matrix, and pivot rows and columns are removed. Data movement is simpler, but higher fill-in can result if the matrix cannot be permuted into a variable-band form with small profile. We consider a combined unifrontal/multifrontal algorithm to enable general fill-in reduction orderings to be applied without the data movement of previous multifrontal approaches. We discuss this technique in the context of a code designed for the solution of sparse systems with unsymmetric pattern.","G.1.3 [Numerical Analysis]: Numerical Linear Algebra - linear systems (direct and iterative methods); Sparse, structured and very large systems (direct and iterative methods)",
Algorithm 797: Fortran subroutines for approximate solution of graph planarization problems using GRASP,1999,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0001725090&doi=10.1145%2f326147.326153&partnerID=40&md5=d9d4e292fa5e2feba61c348733da48fc,"We describe Fortran subroutines for finding approximate solutions of the maximum planar subgraph problem (graph planarization) using a Greedy Randomized Adaptive Search Procedure (GRASP). The design and implementation of the code are described in detail. Computational results with the subroutines illustrate the quality of solutions found as a function of number of GRASP iterations. Categories and Subject Descriptors: D.3.2 [Programming Languages]: Language Classifications - Fortran 77; G.2.1 [Discrete Mathematics]: Combinatorics - Combinatorial algorithms; G.m [Mathematics of Computing]: Miscellaneous General Terms: Algorithms, Performance.",Automatic graph drawing; Combinatorial optimization; Graph planarization; GRASP; Local search,
Algorithm 798: High-dimensional interpolation using the modified Shepard method,1999,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0005996981&doi=10.1145%2f326147.326154&partnerID=40&md5=089fab051c386a808823544063b28135,"A new implementation of the Modified Quadratic Shepard Method for the interpolation of scattered data is presented. QSHEP5D is a C++ translation of the original Fortran 77 program QSHEP3D developed by Renka (for 2D and 3D interpolation) which has been upgraded for 5D interpolation. This software development was motivated by the need for interpolated 5D hypervolumes of environmental response variables produced by forest growth and production models. Categories and Subject Descriptors: G.4 [Mathematics of Computing]: Mathematical Software - Algorithm design and analysis; G.1.1 [Numerical Analysis]: Interpolation General Terms: Algorithms, Measurement, Performance.",C++ implementation; Modified Shepard method; Multivariate interpolation; NetCDF file format,
An interface between optimization and application for the numerical solution of optimal control problems,1999,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0000218149&doi=10.1145%2f317275.317278&partnerID=40&md5=b69234a9b13206ed319c39b630f2adca,"An interface between the application problem and the nonlinear optimization algorithm is proposed for the numerical solution of distributed optimal control problems. By using this interface, numerical optimization algorithms can be designed to take advantage of inherent problem features like the splitting of the variables into states and controls and the scaling inherited from the functional scalar products. Further, the interface allows the optimization algorithm to make efficient use of user-provided function evaluations and derivative calculations.",Algorithms; Design; G.4 [mathematics of computing]: mathematical software; Optimal control; Optimization; Simulation,
Algorithm 795: PHCpack: A general-purpose solver for polynomial systems by homotopy continuation,1999,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0001656792&doi=10.1145%2f317275.317286&partnerID=40&md5=f078fa23f6b7142cd4b5b547ecc7f6b5,"Polynomial systems occur in a wide variety of application domains. Homotopy continuation methods are reliable and powerful methods to compute numerically approximations to all isolated complex solutions. During the last decade considerable progress has been accomplished on exploiting structure in a polynomial system, in particular its sparsity. In this article the structure and design of the software package PHC is described. The main program operates in several modes, is menu driven, and is file oriented. This package features a great variety of root-counting methods among its tools. The outline of one black-box solver is sketched, and a report is given on its performance on a large database of test problems. The software has been developed on four different machine architectures. Its portability is ensured by the gnu-ada compiler.","D.3.2 [programming languages]: language classifications - ada; G.1.5 [numerical analysis]: roots of nonlinear equations - systems of equations; G.2.1 [discrete mathematics]: combinatorics - counting problems; Polynomials, methods for",
Beware of linear congruential generators with multipliers of the form a = ±2q ±2r,1999,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0001238643&doi=10.1145%2f326147.326156&partnerID=40&md5=7f147cc9f4d4b9c5d181d16747f7f80d,"Linear congruential random-number generators with Mersenne prime modulus and multipliers of the form a = ±2q ±2r have been proposed recently. Their main advantage is the availability of a simple and fast implementation algorithm for such multipliers. This note generalizes this algorithm, points out statistical weaknesses of these multipliers when used in a straightforward manner, and suggests in what context they could be used safely. Categories and Subject Descriptors: G.4 [Mathematics of Computing]: Mathematical Software - Algorithm design and analysis; 1.6 [Computing Methodologies]: Simulation and Modeling General Terms: Algorithms, Performance.",Correlation test; Linear congruential generators; Random number generation,
Algorithm 790: CSHEP2D: Cubic Shepard method for bivariate interpolation of scattered data,1999,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0002359421&doi=10.1145%2f305658.305737&partnerID=40&md5=a4be45c313fcd8d26137b3b79589e319,We describe a new algorithm for scattered data interpolation. The method is similar to that of Algorithm 660 but achieves cubic precision and C2 continuity at very little additional cost. An accompanying article presents test results that show the method to be among the most accurate available.,D.3.2 [Programming Languages]: Language Classifications - Fortran 77; G.1.1 [Numerical Analysis]: Interpolation; G.1.2 [Numerical Analysis]: Approximation; G.4 [Mathematics of Computing]: Mathematical Software General Terms: Algorithms; Interpolation,
Algorithm 793: GQRAT - Gauss quadrature for rational functions,1999,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0003806191&doi=10.1145%2f317275.317282&partnerID=40&md5=b4eff067a2f8dd790b94247419a5a30d,"The concern here is with Gauss-type quadrature rules that are exact for a mixture of polynomials and rational functions, the latter being selected so as to simulate poles that may be present in the integrand. The underlying theory is presented as well as methods for constructing such rational Gauss formulae. Relevant computer routines are provided and applied to a number of examples, including Fermi-Dirac and Bose-Einstein integrals of interest in solid state physics.",Algorithms; Construction of quadrature rules; D.3.2 [programming languages]: language classifications - FORTRAN 77; G. 1.4 [numerical analysis]: quadrature and numerical differentiation; Gaussian quadrature exact for rational functions,
Algorithm 791: TSHEP2D: Cosine series Shepard method for bivariate interpolation of scattered data,1999,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0004925728&doi=10.1145%2f305658.305754&partnerID=40&md5=b5ca74e8852f93b67e0ff48f2017ca77,"We describe a new algorithm for scattered data interpolation. It is based on a modified Shepard method similar to that of Algorithm 660 but uses 10-parameter cosine series nodal functions in place of quadratic polynomials. Also, the interpolant has continuous second partial derivatives. An accompanying survey article presents test results that show the method to be more accurate than polynomial-based methods in terms of reproducing test functions with large variations and steep gradients.",D.3.2 [Programming Languages]: Language Classifications - Fortran 77; G.1.1 [Numerical Analysis]: Interpolation; G.1.2 [Numerical Analysis]: Approximation; G.4 [Mathematics of Computing]: Mathematical Software General Terms: Algorithms; Interpolation,
A frontal code for the solution of sparse positive-definite symmetric systems arising from finite-element applications,1999,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0000740207&doi=10.1145%2f332242.332243&partnerID=40&md5=165d94c36001bcbc4089bd8547ca742f,"We describe the design, implementation, and performance of a frontal code for the solution of large sparse symmetric systems of linear finite-element equations. The code is intended primarily for positive-definite systems, since numerical pivoting is not performed. The resulting software package, MA62, will be included in the Harwell Subroutine Library. We illustrate the performance of our new code on a range of problems arising from real engineering and industrial applications. The performance of the code is compared with that of the Harwell Subroutine Library general frontal solver MA42 and with other positive-definite codes from the Harwell Subroutine Library. Categories and Subject Descriptors: G.1.0 [Numerical Analysis]: General - Numerical algorithms; G.1.3 [Numerical Analysis]: Numerical Linear Algebra - Sparse, structured and very large systems (direct and iterative methods).","Algorithms; Finite-element equations; G.1.0 [Numerical Analysis]: General - Numerical algorithms; G.1.3 [Numerical Analysis]: Numerical Linear Algebra - Sparse, structured and very large systems (direct and iterative methods); Gaussian; Performance",
Remark on Algorithm 702 - The updated truncated Newton minimization package,1999,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0000743314&doi=10.1145%2f305658.305698&partnerID=40&md5=e35939d11e2db7da4c7189def8c28d2e,"A truncated Newton minimization package, TNPACK, was described in ACM Transactions on Mathematical Software 14, 1 (Mar. 1992), pp. 46-111. Modifications to enhance performance, especially for large-scale minimization of molecular potential functions, are described here. They involve three program segments of TNPACK: negative curvature test, modified Cholesky factorization, and line-search stopping rule.",Algorithms; G.1.6 [Numerical Analysis]: Optimization - nonlinear programming; G.4 [Mathematics of Computing]: Mathematical Software; Indefinite preconditioner; J.3 [Computer Applications]: Life and Medical Sciences; Performance,
Remark on algorithm 726: Orthpol—A Package of Routines for Generating Orthogonal Polynomials and Gauss-Type Quadrature Rules,1998,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962317122&doi=10.1145%2f292395.292467&partnerID=40&md5=668d2947ba4b9717b5306993d787bbd4,[No abstract available],Algorithms; Gauss-type rules; Orthogonal polynomials,
Certification of Algorithm 734: A Fortran 90 Code for Unconstrained Nonlinear Minimization,1998,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0032155011&doi=10.1145%2f292395.292460&partnerID=40&md5=a5b27667d8dcc0869283e951408d57a9,"Algorithm 734: A Fortran 90 Code for Unconstrained Nonlinear Minimization (ACM Trans. Math. Softw. 20, 3 (Sept. 1994), pages 354-372; CALGO Supplement 131) was ported to a number of compiler-platform combinations. The necessary changes to the code are given along with some comparative timings.",Fortran 90; Language classifications - Fortran 90; Mathematical software - certification and testing; Nonlinear optimization; Optimization - gradient methods,Algorithms; Computer software selection and evaluation; Optimization; Program compilers; FORTRAN 90; Nonlinear minimization; FORTRAN (programming language)
Computing Rank-Revealing QR Factorizations of Dense Matrices,1998,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0032083715&doi=10.1145%2f290200.287637&partnerID=40&md5=1e020b6e5b9843c53d44bb9c0079f374,"We develop algorithms and implementations for computing rank-revealing QR (RRQR) factorizations of dense matrices. First, we develop an efficient block algorithm for approximating an RRQR factorization, employing a windowed version of the commonly used Golub pivoting strategy, aided by incremental condition estimation. Second, we develop efficiently implementable variants of guaranteed reliable RRQR algorithms for triangular matrices originally suggested by Chandrasekaran and Ipsen and by Pan and Tang. We suggest algorithmic improvements with respect to condition estimation, termination criteria, and Givens updating. By combining the block algorithm with one of the triangular postprocessing steps, we arrive at an efficient and reliable algorithm for computing an RRQR factorization of a dense matrix. Experimental results on IBM RS/6000 and SGI R8000 platforms show that this approach performs up to three times faster than the less reliable QR factorization with column pivoting as it is currently implemented in LAPACK, and comes within 15% of the performance of the LAPACK block algorithm for computing a QR factorization without any column exchanges. Thus, we expect this routine to be useful in many circumstances where numerical rank deficiency cannot be ruled out, but currently has been ignored because of the computational cost of dealing with it.",Algorithms; G.1.3 [Numerical Analysis]: Numerical Linear Algebra; G.4 [Mathematics of Computing]: Mathematical Software; Performance,Algorithms; Computer software; Efficiency; Linear algebra; Matrix algebra; Performance; Factorization; Computational methods
Remark on Algorithm 622: A Simple Macroprocessor,1998,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0032154280&doi=10.1145%2f292395.292448&partnerID=40&md5=4b2fd4c9896d7b94033ddaa7e7a22def,A number of updates to the macroprocessor are described that bring the code into line with the Fortran 77 standard. This is followed by an outline of how the macroprocessor was used for the rapid porting of geophysical software from a 64-bit supercomputer environment to a number of different Unix workstations. Finally a number of deficiencies remaining in the macroprocessor are noted and workarounds suggested where possible.,Algorithms; Language classifications - macro and assembly languages; Fortran 77; Portable software tool; Processors - preprocessors; Fortran; Simple,Algorithms; Computer software portability; Error correction; FORTRAN (programming language); Macroprocessor; Program processors
Remark on Algorithm 761,1998,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0032274069&doi=10.1145%2f293686.293689&partnerID=40&md5=767a9c76c61cebbcb4b8b6c807083dd8,"Several improvements to Algorithm 761 are presented. The problems corrected consist of (1) the computation of a determinant in subroutine SDLEQN may result in overflow, (2) subroutine SDTRCH may fail to correctly determine the set of boundary edges, (3) subroutine SDTRTT, which removes long thin triangles from the boundary in order to improve the accuracy of extrapolation, may remove too many triangles, and (4) the nodal derivative estimation procedure, SDPD3P, often fails to achieve cubic precision.",Algorithms; D.3.2 [Programming Languages]: Language Classifications - Fortran; G.1.1 [Numerical Analysis]: Interpolation; G.4 [Mathematical Software]; Scattered data fitting; Triangle-based interpolations,Algorithms; Computational geometry; Data reduction; Extrapolation; FORTRAN (programming language); Interpolation; Scattered data fitting; Triangle-based interpolation; Computer software
Stiffness Detection and Estimation of Dominant Spectra with Explicit Runge-Kutta Methods,1998,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0032284785&doi=10.1145%2f293686.287641&partnerID=40&md5=afda6715390beb662e6a830af943ea1c,"A new stiffness detection scheme based on explicit Runge-Kutta methods is proposed. It uses a Krylov subspace approximation to estimate the eigenvalues of the Jacobian of the differential system. The numerical examples indicate that this technique is a worthwhile alternative to other known stiffness detection schemes, especially when the systems are large and when it is desirable to know more about the spectrum of the Jacobian than just the spectral radius.",Algorithms; Explicit Runge-Kutta methods; G.1.7 [Mathematics of Computing]: Ordinary Differential Equations - initial value problems; one-step (single-step) methods; stiff equations; Stiffness detection,Approximation theory; Eigenvalues and eigenfunctions; Initial value problems; Ordinary differential equations; Stiffness; Krylov subspace approximation; Stiffness detection; Runge Kutta methods
SPRINT2D: Adaptive Software for PDEs,1998,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0032269310&doi=10.1145%2f293686.293696&partnerID=40&md5=a3a5f4e74e0d6cf4dbbd0b9b763f7dd1,"SPRINT2D is a set of software tools for solving both steady and unsteady partial differential equations in two-space variables. The software consists of a set of coupled modules for mesh generation, spatial discretization, time integration, nonlinear equations, linear algebra, spatial adaptivity, and visualization. The software uses unstructured triangular meshes and adaptive local error control in both space and time. The class of problems solved includes systems of parabolic, elliptic, and hyperbolic equations; for the latter by use of Riemann-solver-based methods. This article describes the software and shows how the adaptive techniques may be used to increase the reliability of the solution procedure for a Burgers' equations problem, an electrostatics problem from elastohydrodynamic lubrication, and a challenging gas jet problem.",Adaptivity; Algorithms; Error control; Finite-volume methods; G.1.8 [Numerical Analysis]: Partial Differential Equations - method of lines; hyperbolic equations; elliptic equations; G.4 [Mathematics of Computing]: Mathematical Software,Computer software; Error analysis; Finite volume method; Integration; Nonlinear equations; Partial differential equations; Adaptive error control; Burger's equation; Computer aided software engineering
Algorithm 781: Generating Hilbert's Space-Filling Curve by Recursion,1998,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0032084028&doi=10.1145%2f290200.290219&partnerID=40&md5=8415e7907f86e2c0daa57fa2eab37c54,"An efficient algorithm for the generation of Hilbert's space-filling curve is given. The algorithm implements a recursive procedure that involves simple integer operations and quickly converges to the set of points that make the Hilbert curve. The algorithm is elegant, short, and considerably easier to implement than previous recursive and nonrecursive algorithms and can be efficiently implemented in all programming languages that have integer operations and allow recursion. The fundamental Hilbert shape (a line joining the four corners of a square) is represented by two variables with values of either 0 or 1. This coding technique could be successfully applied to the generation of other regular space-filling curves, such as the Peano curve.",D.3.2 [Programming Languages]: Language Classifications-C; I.3.3 [Computer Graphics]: Picture/Image Generation-line and curve generation; Recursion,Algorithms; Computer graphics; Curve fitting; Efficiency; Image processing; Recursive functions; Mathematical spaces; Computer programming languages
Expokit: A Software Package for Computing Matrix Exponentials,1998,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0032010619&doi=10.1145%2f285861.285868&partnerID=40&md5=b5e55d406d8e229a418cddee968f7654,"Expokit provides a set of routines aimed at computing matrix exponentials. More precisely, it computes either a small matrix exponential in full, the action of a large sparse matrix exponential on an operand vector, or the solution of a system of linear ODEs with constant inhomogeneity. The backbone of the sparse routines consists of matrix-free Krylov subspace projection methods (Arnoldi and Lanczos processes), and that is why the toolkit is capable of coping with sparse matrices of large dimension. The software handles real and complex matrices and provides specific routines for symmetric and Hermitian matrices. The computation of matrix exponentials is a numerical issue of critical importance in the area of Markov chains and furthermore, the computed solution is subject to probabilistic constraints. In addition to addressing general matrix exponentials, a distinct attention is assigned to the computation of transient states of Markov chains.",Algorithms; G.1.3 [Numerical Analysis]: Numerical Linear Algebra; G.1.7 [Numerical Analysis]: Ordinary Differential Equations - initial value problems; G.4 [Mathematics of Computing]: Mathematical Software; Krylov methods; Markov chains; Matrix exponential,Computer software; Differential equations; Markov processes; Matrix algebra; Numerical methods; Krylov methods; Linear algebra
Algorithm 786: Multiple-Precision Complex Arithmetic and Functions,1998,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0032269566&doi=10.1145%2f293686.293687&partnerID=40&md5=7f870eb2c6f572545ca5b405dae02159,"This article describes a collection of Fortran routines for multiple-precision complex arithmetic and elementary functions. The package provides good exception handling, flexible input and output, trace features, and results that are almost always correctly rounded. For best efficiency on different machines, the user can change the arithmetic type used to represent the multiple-precision numbers.","G. 1.0 [Numerical Analysis]: General - computer arithmetic; G. 1.2 [Numerical Analysis]: Approximation - elementary function approximation; G.4 [Mathematics of Computing]: Mathematical Software - algorithm analysis; efficiency, portability",Algorithms; Computer systems programming; Digital arithmetic; FORTRAN (programming language); Function evaluation; Response time (computer systems); Multiple precision numbers; Computer software
Recipes for Adjoint Code Construction,1998,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0032269567&doi=10.1145%2f293686.293695&partnerID=40&md5=53160dedee1cfb3807df5e1f164c5cb5,"Adjoint models are increasingly being developed for use in meteorology and oceanography. Typical applications are data assimilation, model tuning, sensitivity analysis, and determination of singular vectors. The adjoint model computes the gradient of a cost function with respect to control variables. Generation of adjoint code may be seen as the special case of differentiation of algorithms in reverse mode, where the dependent function is a scalar. The described method for adjoint code generation is based on a few basic principles, which permits the establishment of simple construction rules for adjoint statements and complete adjoint subprograms. These rules are presented and illustrated with some examples. Conflicts that occur due to loops and redefinition of variables are also discussed. Direct coding of the adjoint of a more sophisticated model is extremely time consuming and subject to errors. Hence, automatic generation of adjoint code represents a distinct advantage. An implementation of the method, described in this article, is the tangent linear and adjoint model compiler (TAMC).",D.3.4 [Programming Languages]: Processors - preprocessors; G.1.4 [Numerical Analysis]: Quadrature and Numerical Differentiation - automatic differentiation; G.1.6 [Numerical Analysis]: Optimization - gradient methods,Algorithms; Computer software; Data reduction; Mathematical models; Optimization; Program compilers; Sensitivity analysis; Vectors; Data assimilation; Tangent linear and adjoint model compiler (TAMC); Computer aided software engineering
Algorithm 785: A Software Package for Computing Schwarz-Christoffel Conformal Transformation for Doubly Connected Polygonal Regions,1998,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0032155998&doi=10.1145%2f292395.291204&partnerID=40&md5=fe1807d98c7940ede45dfd94686e51bb,"A software package implementing Schwarz-Christoffel Conformal transformation (or mapping) of doubly connected polygonal regions is fully described in this article from mathematical, numerical, and practical perspectives. The package solves the so-called accessory parameter problem associated with the mapping function as well as evaluates forward and inverse maps. The robustness of the package is reflected by the flexibility in choosing the accuracy of the parameters to be computed, the speed of computation, the ability of mapping ""difficult"" regions (to be specified in Section 2), and being user friendly. Several examples are presented to demonstrate the capabilities of the package.",Accessory parameters; Algorithms; Doubly connected region; Mathematical Software - Efficiency; reliability; robustness; Miscellaneous; Numerical conformal mapping; Schwarz-Christoffel conformal transformation; System of nonlinear equations,Algorithms; Computational methods; Conformal mapping; Efficiency; Nonlinear equations; Reliability; Robustness (control systems); Schwarz-Christoffel conformal transformation; Computer software
An Object-Oriented Framework for Block Preconditioning,1998,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0032083702&doi=10.1145%2f290200.287639&partnerID=40&md5=e7fa4696b355186592b3461baf1ad513,"General software for preconditioning the iterative solution of linear systems is greatly lagging behind the literature. This is partly because specific problems need specific matrix and preconditioner data structures in order to be solved efficiently, i.e., multiple implementations of a preconditioner with specialized data structures are required. This article presents a framework to support preconditioning with various, possibly user-defined, data structures for matrices that are partitioned into blocks. The main idea is to define data structures for the blocks, and an upper layer of software which uses these blocks transparently of their data structure. This transparency can be accomplished by using an object-oriented language. Thus, various preconditioners, such as block relaxations and block-incomplete factorizations, only need to be defined once and will work with any block type. In addition, it is possible to transparently interchange various approximate or exact techniques for inverting pivot blocks, or solving systems whose coefficient matrices are diagonal blocks. This leads to a rich variety of preconditioners that can be selected. Operations with the blocks are performed with optimized libraries or fundamental data types. Comparisons with an optimized Fortran 77 code on both workstations and Cray supercomputers show that this framework can approach the efficiency of Fortran 77, as long as suitable block sizes and block types are chosen.",Block matrices; D.1.5 [Programming Techniques]: Object-Oriented Programming; G.1.3 [Numerical Analysis]: Numerical Linear Algebra-sparse and very large systems; G.4 [Mathematics of Computing]: Mathematical Software; Preconditioners,Computer software; Data structures; Design; Linear algebra; Matrix algebra; Preconditioners; Object oriented programming
Algorithm 783: Pcp2Nurb - Smooth Free-Form Surfacing with Linearly Trimmed Bicubic B-Splines,1998,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0032156050&doi=10.1145%2f292395.292399&partnerID=40&md5=85b929fa5006d59108e05c7821e3afb6,"Unrestricted control polyhedra facilitate modeling free-form surfaces of arbitrary topology and local patch-layout by allowing n-sided, possibly nonplanar, facets and m-valent vertices. By cutting off edges and corners, the smoothing of an unrestricted control polyhedron can be reduced to the smoothing of a planar-cut polyhedron. A planar-cut polyhedron is a generalization of the well-known tensor-product control structure. The routine Pcp2Nurb in turn translates planar-cut polyhedra to a collection of four-sided linearly trimmed bicubic B-splines and untrimmed biquadratic B-splines. The routine can thus serve as central building block for overcoming topological constraints in the mathematical modeling of smooth surfaces that are stored, transmitted, and rendered using only the standard representation in industry. Specifically, on input of a nine-point subnet of a planar-cut polyhedron, the routine outputs a trimmed bicubic NURBS patch. If the subnet does not have geometrically redundant edges, this patch joins smoothly with patches from adjacent subnets as a four-sided piece of a regular C1 surface. The patch integrates smoothly with untrimmed biquadratic tensor-product surfaces derived from subnets with tensor-product structure. Sharp features can be retained in this representation by using geometrically redundant edges in the planar-cut polyhedron. The resulting surface follows the outlines of the planar-cut polyhedron in the manner traditional tensor-product splines follow the outline of their rectilinear control polyhedron. In particular, it stays in the local convex hull of the planar-cut polyhedron.",Algorithms; Arbitrary patch layout; Arbitrary surface topology; Boundary Representations; Language Classifications; Spline and Piecewise Polynomial Approximation; Spline and Piecewise Polynomial Interpolation; Surface Representations,Algorithms; Approximation theory; Computational geometry; Interpolation; Object recognition; Polynomials; Boundary representation; Surface representation; Trimmed bicubic B-splines; Subroutines
Algorithm 787: Fortran Subroutines for Approximate Solution of Maximum Independent Set Problems Using GRASP,1998,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0032266182&doi=10.1145%2f293686.293690&partnerID=40&md5=015824129b22036481ebf216057c4bbd,"Let G = (V, E) be an undirected graph, where V and E are the sets of vertices and edges of G, respectively. A subset of the vertices S ⊆ V is independent if all of its members are pairwise nonadjacent, i.e., have no edge between them. A solution to the NP-hard maximum independent set problem is an independent set of maximum cardinality. This article describes gmis, a set of Fortran subroutines to find an approximate solution of a maximum independent set problem. A greedy randomized adaptive search procedure (GRASP) is used to produce the solutions. The algorithm is described in detail. Implementation and usage of the package is outlined, and computational experiments are reported, illustrating solution quality as a function of running time.",D.3.2 [Programming Languages]: Language classifications - Fortran; G.1.6 [Numerical Analysis]: Optimization - integer programming; G.2.1 [Discrete Mathematics]: Combinatorics - combinatorial algorithms,Algorithms; Computational complexity; Computer systems programming; FORTRAN (programming language); Graph theory; Optimization; Random processes; Response time (computer systems); Greedy randomized adaptive search procedure (GRASP); Subroutines
Remark on Algorithm 706: DCUTRI - An Algorithm for Adaptive Cubature over a Collection of Triangles,1998,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0346614690&doi=10.1145%2f292395.291205&partnerID=40&md5=12499b80b7c8d08f114d4d036cb0e9eb,"We present corrections to Algorithm 706 (ACM Trans. Math. Softw. 18, 3, Sept. 1992, pages 329-342; CALGO Supplement 123).",Algorithms; Automatic integration; Cubature; Cubature rules; Error estimation; Mathematical Software - Efficiency; reliability and robustness; Null rules; Quadrature and Numerical Differentiation - Adaptive quadrature; multiple quadrature; Reliability,
The Automatic Generation of Sparse Primitives,1998,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0032083929&doi=10.1145%2f290200.287636&partnerID=40&md5=724f803da24732315935246d2222ae67,"Primitives in mathematical software are usually written and optimized by hand. With the implementation of a ́sparse compileŕ that is capable of automatically converting a dense program into sparse code, however, a completely different approach to the generation of sparse primitives can be taken. A dense implementation of a particular primitive is supplied to the sparse compiler, after which it can be converted into many different sparse versions of this primitive. Each version is specifically tailored to a class of sparse matrices having a specific nonzero structure. In this article, we discuss some of our experiences with this new approach.",Compilers; D.1.2 [Programming Techniques]: Automatic Programming; D.3.4 [Programming Languages]; Data structure transformations; E.2 [Data]: Data Storage Representations; G.1.3 [Numerical Analysis]: Numerical Linear Algebra; Processors - compilers,Algorithms; Computer programming languages; Data storage equipment; Data structures; Linear algebra; Matrix algebra; Performance; Program compilers; Sparse matrix; Computer programming
PELLPACK: A Problem-Solving Environment for PDE-Based Applications on Multicomputer Platforms,1998,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0032010496&doi=10.1145%2f285861.285864&partnerID=40&md5=4188a456b0502420b9ce10332144c6f5,"This article presents the software architecture and implementation of the problem-solving environment (PSE) PELLPACK for modeling physical objects described by partial differential equations (PDEs). The scope of this PSE is broad, as PELLPACK incorporates many PDE solving systems, and some of these, in turn, include several specific PDE solving methods. Its coverage for 1D, 2D, and 3D elliptic or parabolic problems is quite broad, and it handles some hyperbolic problems. Since a PSE should provide complete support for the problem-solving process, PELLPACK also contains a large amount of code to support graphical user interfaces, analytic tools, user help, domain or mesh partitioning, machine and data selection, visualization, and various other tasks. Its total size is well over 1 million lines of code. Its open-ended software architecture consists of several software layers. The top layer is an interactive graphical interface for specifying the PDE model and its solution framework. This interface saves the results of the user specification in the form of a very high level PDE language which is an alternative interface to the PELLPACK system. This language also allows a user to specify the PDE problem and its solution framework textually in a natural form. The PELLPACK language preprocessor generates a Fortran control program with the interfaces, calls to specified components and libraries of the PDE solution framework, and functions defining the PDE problem. The PELLPACK program execution is supported by a high-level tool where the virtual parallel system is defined, where the execution mode, file system, and hardware resources are selected, and where the compilation, loading, and execution are controlled. Finally, the PELLPACK PSE integrates several PDE libraries and PDE systems available in the public domain. The system employs several parallel reuse methodologies based on the decomposition of discrete geometric data to map sparse PDE computations to parallel machines. An instance of the system is available as a Web server (WebPELLPACK) for public use at http://pellpack.cs.purdue.edu.",C.3 [Computer Systems Organization]: Special-Purpose and Application-Based Systems; D.2.6 [Software Engineering]: Programming Environments - graphical environments; Integrated environments; Interactive environments,Computer graphics; Computer software; Differential equations; FORTRAN (programming language); Parallel processing systems; Multicomputer platforms; Problem solving environment; Software architecture; Computer simulation
Algorithm 788: Automatic Boundary Integral Equation Programs for the Planar Laplace Equation,1998,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0032258901&doi=10.1145%2f293686.293692&partnerID=40&md5=a3eda74a596ccb55042f8f13cc70f2a3,"Algorithms with automatic error control are described for the solution of Laplace's equation on both interior and exterior regions, with both Dirichlet and Neumann boundary conditions. The algorithms are based on standard reformulations of each boundary value problem as a boundary integral equation of the second kind. The Nyström method is used to solve the integral equations, and convergence of arbitrary high order is observed when the boundary data are analytic. The Kelvin transformation is introduced to allow a simple conversion between internal and external problems. Two Fortran program implementations, DRCHLT and NEUMAN, are defined, analyzed, and illustrated.",D.3.2 [Programming Languages]: Language Classifications - Fortran 77; G.1.8 [Mathematics of Computing]: Partial Differential Equations,Algorithms; Boundary conditions; Boundary value problems; Convergence of numerical methods; Integral equations; Mathematical transformations; Partial differential equations; Laplace equation; Computer software
Algorithm 780: Exponential Pseudorandom Distribution,1998,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0032010530&doi=10.1145%2f285861.285866&partnerID=40&md5=790961fe25b80a259e1d7847f59e4ae2,"An algorithm is presented for the calculation of exponentially distributed random numbers. It is based on mathematics that was published by Ahrens and Dieter, but some errors have been corrected.",D.3.2 [Programming Languages]: Language Classifications - Fortran 90; G.3 [Mathematics of Computing]: Probability and Statistics - random number generation; G.4 [Mathematics of Computing]: Mathematical Software - certification and testing,Algorithms; Mathematical programming; Mathematical techniques; Pseudorandom numbers; Random number generation
"The Design, Implementation, and Evaluation of a Symmetric Banded Linear Solver for Distributed-Memory Parallel Computers",1998,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0032010531&doi=10.1145%2f285861.285865&partnerID=40&md5=0840ad22fb3ee4c17330e3bdf88e642b,"This article describes the design, implementation, and evaluation of a parallel algorithm for the Cholesky factorization of symmetric banded matrices. The algorithm is part of IBM's Parallel Engineering and Scientific Subroutine Library version 1.2 and is compatible with ScaLAPACK's banded solver. Analysis, as well as experiments on an IBM SP2 distributed-memory parallel computer, shows that the algorithm efficiently factors banded matrices with wide bandwidth. For example, a 31-node SP2 factors a large matrix more than 16 times faster than a single node would factor it using the best sequential algorithm, and more than 20 times faster than a single node would using LAPACK's DPBTRF. The algorithm uses novel ideas in the area of distributed dense-matrix computations that include the use of a dynamic schedule for a blocked systolic-like algorithm and the separation of the input and output data layouts from the layout the algorithm uses internally. The algorithm also uses known techniques such as blocking to improve its communication-to-computation ratio and its data-cache behavior.",Algorithms; Banded matrices; Cholesky factorization; Distributed memory; Efficiency; G.1.3 [Numerical Analysis]: Numerical Linear Algebra - linear systems; G.4 [Mathematics of Computing]: Mathematical Software - algorithm analysis; Performance,Matrix algebra; Parallel algorithms; Storage allocation (computer); Banded matrices; Distributed memory; Parallel processing systems
"Algorithm 779: Fermi-Dirac Functions of Order -1/2, 1/2, 3/2, 5/2",1998,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0032010653&doi=10.1145%2f285861.285862&partnerID=40&md5=bff78a7db260254040e2c50377db770e,"The computation of Fermi-Dirac integrals ℱk is discussed for the values k = -1, 1/2, 3/2, 5/2. We derive Chebyshev polynomial expansions which allow the computation of these functions to double precision IEEE accuracy.",Algorithms; Chebyshev polynomials; Collocation; D.3.2 [Programming Languages]: Language Classifications - Fortran 77; F.2.1 [Analysis of Algorithms and Problem Complexity]: Numerical Algorithms and Problems; G.1.2 [Numerical Analysis]: Approximation,Algorithms; Chebyshev approximation; Digital arithmetic; Polynomials; Chebyshev polynomial expansions; Fermi-Dirac functions; Precision; Natural sciences computing
The Monty Python Method for Generating Random Variables,1998,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0032155866&doi=10.1145%2f292395.292453&partnerID=40&md5=ebbe72bb222df82e71e26debbc0aa260,"We suggest an interesting and fast method for generating normal, exponential, t, von Mises, and certain other important random variables used in Monte Carlo studies. The right half of a symmetric density is cut into pieces, then, using simple area-preserving transformations, reassembled into a rectangle from which the x-coordinate - or a linear function of the x-coordinate - of a random point provides the required variate. To illustrate the speed and simplicity of the Monty Python method, we provide a small C program, self-contained, for rapid generation of normal (Gaussian) variables. It is self-contained in the sense that required uniform variates are generated in-line, as pairs of 16-bit integers by means of the remarkable new multiply-with-carry method.",Algorithms; Monty Python method; Normal variates; Probability and statistics; Simulation theory; t variates; Theory; Von Mises variates,Algorithms; Probability; Random processes; Subroutines; Monty Python method; Random number generation
The Computation of Spectral Density Functions for Singular Sturm-Liouville Problems Involving Simple Continuous Spectra,1998,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0032010657&doi=10.1145%2f285861.285867&partnerID=40&md5=2cf06628129f25a517142823dde302f4,The software package SLEDGE has as one of its options the estimation of spectral density functions ρ(t) for a wide class of singular Sturm-Liouville problems. In this article the underlying theory and implementation issues are discussed. Several examples exhibiting quite varied asymptotic behavior in ρ are presented.,Algorithms; Continuous spectrum; Eigenfunction norm; Eigenvalue; G.1.7 [Numerical Analysis]: Ordinary Differential Equations - boundary value problems; G.4 [Mathematics of Computing]: Mathematical Software - algorithm analysis; Performance,Algorithms; Computer software; Eigenvalues and eigenfunctions; Mathematical programming; Sturm-Liouville problem; Probability density function
Implementation of Hopf and Double-Hopf Continuation Using Bordering Methods,1998,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0032255516&doi=10.1145%2f293686.293693&partnerID=40&md5=ae0d6da3690fe6c46034e74f2f056c21,"We discuss the computational study of curves of Hopf and double-Hopf points in the software package CONTENT developed at CWI, Amsterdam. These are important points in the numerical study of dynamical systems characterized by the occurrence of one or two conjugate pairs of pure imaginary eigenvalues in the spectrum of the Jacobian matrix. The bialternate product of matrices is extensively used in three codes for the numerical continuation of curves of Hopf points and in one for the continuation of curves of double-Hopf points. In the double-Hopf and two of the single-Hopf cases this is combined with a bordered matrix method. We use this software to find special points on a Hopf curve in a model of chemical oscillations and by computing a Hopf and a double-Hopf curve in a realistic model of a neuron.",Algorithms; Bialternate product; CONTENT; D.2.6 [Software Engineering]: Programming Environments; Design; Dynamical system; G.1.7 [Numerical Analysis]: Ordinary Differential Equations; G.4 [Mathematics of Computing]: Mathematical Software,Algorithms; Computer software; Eigenvalues and eigenfunctions; Matrix algebra; Software engineering; Bialternate product; Hopf points; Jacobian matrix; Software package CONTENT; Computer systems programming
Algorithm 784: GEMM-Based Level 3 BLAS: Portability and Optimization Issues,1998,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0032155342&doi=10.1145%2f292395.292426&partnerID=40&md5=e870f0ef717b75608b0beb581058cdc2,"This companion article discusses portability and optimization issues of the GEMM-based level 3 BLAS model implementations and the performance evaluation benchmark. All software comes in all four data types (single- and double-precision, real and complex) and are designed to be easy to implement and use on different platforms. Each of the GEMM-based routines has a few machine-dependent parameters that specify internal block sizes, cache characteristics, and branch points for alternative code sections. These parameters provide means for adjustment to the characteristics of a memory hierarchy.",Language Classifications - Fortran 77; Numerical Algorithms and Problems - computation on matrices; Numerical Linear Algebra - linear systems (direct and iterative methods),Algorithms; Benchmarking; Computer software portability; FORTRAN (programming language); Matrix algebra; Optimization; Performance; GEMM-based level 3 BLAS; Memory hierarchy; Subroutines
GEMM-Based Level 3 BLAS: High-Performance Model Implementations and Performance Evaluation Benchmark,1998,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0032155271&doi=10.1145%2f292395.292412&partnerID=40&md5=e24c8643442d8cebad8bb957f171d39a,"The level 3 Basic Linear Algebra Subprograms (BLAS) are designed to perform various matrix multiply and triangular system solving computations. Due to the complex hardware organization of advanced computer architectures the development of optimal level 3 BLAS code is costly and time consuming. However, it is possible to develop a portable and high-performance level 3 BLAS library mainly relying on a highly optimized GEMM, the routine for the general matrix multiply and add operation. With suitable partitioning, all the other level 3 BLAS can be defined in terms of GEMM and a small amount of level 1 and level 2 computations. Our contribution is twofold. First, the model implementations in Fortran 77 of the GEMM-based level 3 BLAS are structured to reduce effectively data traffic in a memory hierarchy. Second, the GEMM-based level 3 BLAS performance evaluation benchmark is a tool for evaluating and comparing different implementations of the level 3 BLAS with the GEMM-based model implementations.",Language classifications - Fortran 77; Mathematical Software - benchmarking; Numerical Algorithms and Problems - computation on matrices; Numerical Linear Algebra - linear systems (direct and iterative methods),Benchmarking; Computational methods; Computer software portability; Efficiency; FORTRAN (programming language); Matrix algebra; GEMM-based level 3 BLAS; Subroutines
Generation of High-Order Interpolants for Explicit Runge-Kutta Pairs,1998,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0032010599&doi=10.1145%2f285861.285863&partnerID=40&md5=06c35987e970566711eb9eff62578ed6,"Explicit Runge-Kutta pairs can be enhanced by providing them with interpolants. Enhancements include the ability to estimate and control the defect, to produce dense output, and to calculate past values in delay differential equations. The coefficients of an interpolant are easily generated by bootstrapping on the order conditions. However, the generation of high-order interpolants requires a large number of arithmetic operations. We describe an efficient algorithm for the generation of high-order interpolants and illustrate the use of the algorithm with three applications.",Algorithms; Explicit; G.1.7 [Numerical Analysis]: Ordinary Differential Equations; Generation; High order; Interpolants; Pairs; Runge-Kutta,Algorithms; Computational methods; Interpolation; Explicit Runge-Kutta pairs; Interpolants; Differential equations
Algorithm 782: Codes for Rank-Revealing QR Factorizations of Dense Matrices,1998,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0032083879&doi=10.1145%2f290200.287638&partnerID=40&md5=0d86c9916f67a0c892a7836b6aa4b5f8,"This article describes a suite of codes as well as associated testing and timing drivers for computing rank-revealing QR (RRQR) factorizations of dense matrices. The main contribution is an efficient block algorithm for approximating an RRQR factorization, employing a windowed version of the commonly used Golub pivoting strategy and improved versions of the RRQR algorithms for triangular matrices originally suggested by Chandrasekaran and Ipsen and by Pan and Tang, respectively. We highlight usage and features of these codes.",Algorithms; Block algorithm; D.3.2 [Programming Languages]: Language Classifications - Fortran 77; G.1.3 [Numerical Analysis]: Numerical Linear Algebra; G.4 [Mathematics of Computing]: Mathematical Software; Numerical rank; Performance,Algorithms; Computer software; Efficiency; FORTRAN (programming language); Linear algebra; Matrix algebra; Performance; Factorization; Computer programming languages
Algorithm 773: SSRFPACK: Interpolation of Scattered Data on the Surface of a Sphere with a Surface under Tension,1997,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031221316&doi=10.1145%2f275323.275330&partnerID=40&md5=97b8a616e4f29a4b950d7eb2061662b5,SSRFPACK is a Fortran 77 software package that constructs a smooth interpolatory or approximating surface to data values associated with arbitrarily distributed points on the surface of a sphere. It employs automatically selected tension factors to preserve shape properties of the data and avoid overshoot and undershoot associated with steep gradients.,Algorithms; G.1.1 [Numerical Analysis]: Interpolation; G.1.2 [Numerical Analysis]: Approximation; G.4 [Mathematics of Computing]: Mathematical Software; Scattered data fitting; Smoothing; Surface under tension; Triangle-based interpolation,Algorithms; Approximation theory; FORTRAN (programming language); Interpolation; Spheres; Surface tension; Software package SSRFPACK; Computer software
Algorithm 764: Cubpack++: A C++ package for automatic two-dimensional cubature,1997,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031099533&doi=10.1145%2f244768.244770&partnerID=40&md5=e2784efd58d9b6f32c2d339f1af55c76,"In this article, software for the numerical approximation of double integrals over a variety of regions is described. The software was written in C++. Classes for a large number of shapes are provided. A global adaptive integration algorithm is used based on transformations and subdivisions of regions.",,Adaptive algorithms; Approximation theory; C (programming language); Computational complexity; Integration; Mathematical transformations; Automatic integration; Cubature; Computer software
Algorithm 777: HOMPACK90: A Suite of Fortran 90 Codes for Globally Convergent Homotopy Algorithms,1997,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031345669&doi=10.1145%2f279232.279235&partnerID=40&md5=c91e58d97e4e558eef11096b49d62870,"HOMPACK90 is a Fortran 90 version of the Fortran 77 package HOMPACK (Algorithm 652), a collection of codes for finding zeros or fixed points of nonlinear systems using globally convergent probability-one homotopy algorithms. Three qualitatively different algorithms -ordinary differential equation based, normal flow, quasi-Newton augmented Jacobian matrix - are provided for tracking homotopy zero curves, as well as separate routines for dense and sparse Jacobian matrices. A high level driver for the special case of polynomial systems is also provided. Changes to HOMPACK include numerous minor improvements, simpler and more elegant interfaces, use of modules, new end games, support for several sparse matrix data structures, and new iterative algorithms for large sparse Jacobian matrices.",Algorithms; Chow-Yorke algorithm; Curve tracking; D.3.2 [Programming Languages]: Language Classifications - Fortran 90; G.1.5 [Numerical Analysis]: Roots of Nonlinear Equations - systems of equations; G.4 [Mathematics of Computing]: Mathematical Software,Algorithms; Convergence of numerical methods; Data structures; Differential equations; FORTRAN (programming language); Iterative methods; Matrix algebra; Nonlinear systems; Polynomials; Software package HOMPACK; Computer software
Remark on Algorithm 745,1997,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85023136663&doi=10.1145%2f264029.643581&partnerID=40&md5=4f99c2bd90a5b7a9cebeff38507b2174,[No abstract available],,
Algorithm 778: L-BFGS-B: Fortran Subroutines for Large-Scale Bound-Constrained Optimization,1997,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031345518&doi=10.1145%2f279232.279236&partnerID=40&md5=a434c6ae0baba172db092bd3ed6101c5,"L-BFGS-B is a limited-memory algorithm for solving large nonlinear optimization problems subject to simple bounds on the variables. It is intended for problems in which information on the Hessian matrix is difficult to obtain, or for large dense problems. L-BFGS-B can also be used for unconstrained problems and in this case performs similarly to its predecessor, algorithm L-BFGS (Harwell routine VA15). The algorithm is implemented in Fortran 77.",Algorithms; D.3.2 [Programming Languages]: Language Classifications - Fortran 77; G.1.6 [Numerical Analysis]: Optimization - Gradient methods; G.4 [Mathematics of Computing]: Mathematical Software; Large-scale optimization; Limited-memory method,Algorithms; Data storage equipment; FORTRAN (programming language); Matrix algebra; Optimization; Problem solving; Limited memory methods; Variable metric methods; Subroutines
High-Precision Division and Square Root,1997,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031360906&doi=10.1145%2f279232.279237&partnerID=40&md5=eeab778c7e42398b31ea8c3932bdad52,"We present division and square root algorithms for calculations with more bits than are handled by the floating-point hardware. These algorithms avoid the need to multiply two high-precision numbers, speeding up the last iteration by as much as a factor of 10. We also show how to produce the floating-point number closest to the exact result with relatively few additional operations.",Algorithms; Division; G.1.0 [Numerical Analysis]: General - Computer arithmetic; G.4 [Mathematics of Computing]: Mathematical Software - Algorithm analysis; Performance; Quad precision; Square root,Algorithms; Computational methods; Computer hardware; Iterative methods; Floating point hardware; Digital arithmetic
Algorithm 774: Fortran Subroutines for Generating Box-Constrained Optimization Problems,1997,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031224746&doi=10.1145%2f275323.275332&partnerID=40&md5=439187b8b0f37899c7c2b0ef133eb421,"We describe a set of Fortran routines for generating box-constrained nonlinear programming test problems. The technique, as described by Facchinei et al. (this issue), allows the user to control relevant properties of the generated problems.",Algorithms; Certification and testing; D.3.2 [Programming Languages]: Language Classifications - Fortran 77; G.1.6 [Numerical Analysis]: Optimization; G.4 [Mathematics of Computing]: Mathematical Software - algorithm analysis; Performance; Verification,Algorithms; Constraint theory; FORTRAN (programming language); Nonlinear programming; Box constrained nonlinear programming test problems; Subroutines
Local error estimates and regularity tests for the implementation of double adaptive quadrature,1997,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031098807&doi=10.1145%2f244768.244772&partnerID=40&md5=94284f14e7e72ce62d3693e30c4b3e1b,"This article presents a device which is suitable for a practical and efficient implementation of Double Adaptive Quadrature. The device includes local error estimates and attempts to detect the presence of numerical difficulties in the integrand function. If a family of rules with suitable properties is chosen, then this can be achieved without affecting the overall computational cost. Extensive numerical testing has been performed on a comprehensive set of functions showing the effectiveness of the device and its efficiency.",,Computational complexity; Error detection; Function evaluation; Integral equations; Integration; Automatic integration; Double adaptive quadrature; Software engineering
Algorithm 776: SRRIT: A Fortran Subroutine to Calculate the Dominant Invariant Subspace of a Nonsymmetric Matrix,1997,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031346306&doi=10.1145%2f279232.279234&partnerID=40&md5=eed27afb6b9fefc3408e2c0fadd50d4d,"SRRIT is a Fortran program to calculate an approximate orthonormal basis for a dominant invariant subspace of a real matrix A by the method of simultaneous iteration. Specifically, given an integer m, SRRIT computes a matrix Q with m orthonormal columns and real quasi-triangular matrix T of order m such that the equation AQ = QT is satisfied up to a tolerance specified by the user. The eigenvalues of T are approximations to the m eigenvalues of largest absolute magnitude of A, and the columns of Q span the invariant subspace corresponding to those eigenvalues. SRRIT references A only through a user-provided subroutine to form the product AQ; hence it is suitable for large sparse problems.",D.3.2 [Programming Languages]: Language Classifications - Fortran; F.2.1 [Analysis of Algorithms and Problem Complexity]: Numerical Algorithms and Problems - computations on matrices; G.1.3 [Numerical Analysis]: Numerical Linear Algebra - eigenvalues,Algorithms; Computational methods; Eigenvalues and eigenfunctions; FORTRAN (programming language); Iterative methods; Matrix algebra; Problem solving; Dominant invariant subspaces; Software package SRRIT; Subroutines
Algorithm 768: TENSOLVE: A software package for solving systems of nonlinear equations and nonlinear least-squares problems using tensor methods,1997,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031152378&doi=10.1145%2f264029.264032&partnerID=40&md5=2de5d1eb84b604fc547dbf060b0bfef8,"This article describes a modular software package for solving systems of nonlinear equations and nonlinear least-squares problems, using a new class of methods called tensor methods. It is intended for small- to medium-sized problems, say with up to 100 equations and unknowns, in cases where it is reasonable to calculate the Jacobian matrix or to approximate it by finite differences at each iteration. The software allows the user to choose between a tensor method and a standard method based on a linear model. The tensor method approximates F(x) by a quadratic model, where the second-order term is chosen so that the model is hardly more expensive to form, store, or solve than the standard linear model. Moreover, the software provides two different global strategies: a line search approach and a two-dimensional trust region approach. Test results indicate that, in general, tensor methods are significantly more efficient and robust than standard methods on small- and medium-sized problems in iterations and function evaluations.",,Algorithms; Finite difference method; Function evaluation; Iterative methods; Least squares approximations; Mathematical models; Matrix algebra; Nonlinear equations; Problem solving; Tensors; Jacobian matrix; Software package TENSOLVE; Computer software
Algorithm 772: STRIPACK: Delaunay Triangulation and Voronoi Diagram on the Surface of a Sphere,1997,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031221068&doi=10.1145%2f275323.275329&partnerID=40&md5=0fc1d8a73f99ad54ecbd400faa51df63,"STRIPACK is a Fortran 77 software package that employs an incremental algorithm to construct a Delaunay triangulation and, optionally, a Voronoi diagram of a set of points (nodes) on the surface of the unit sphere. The triangulation covers the convex hull of the nodes which need not be the entire surface, while the Voronoi diagram covers the entire surface The package provides a wide range of capabilities including an efficient means of updating the triangulation with nodal additions or deletions. For N nodes, the storage requirement for the triangulation is 13N integer storage locations in addition to 3N nodal coordinates. Using an off-line algorithm and work space of size 3N, the triangulation can be constructed with time complexity O(NlogN).",Algorithms; Delaunay triangulation; Dirichlet tessellation; G.1.1 [Numerical Analysis]: Interpolation; G.1.2 [Numerical Analysis]: Approximation; G.4 [Mathematics of Computing]: Mathematical Software; Sphere; Thiessen regions; Voronoi diagram,Algorithms; Computational complexity; Computational geometry; FORTRAN (programming language); Interpolation; Spheres; Delaunay triangulation; Dirichlet tessellation; Software package STRIPACK; Thiessen regions; Voronoi diagram; Computer software
Compiler Blockability of Dense Matrix Factorizations,1997,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031223129&doi=10.1145%2f275323.275325&partnerID=40&md5=133d2a3e32a5a7bae2399355bffa82c6,"The goal of the LAPACK project is to provide efficient and portable software for dense numerical linear algebra computations. By recasting many of the fundamental dense matrix computations in terms of calls to an efficient implementation of the BLAS (Basic Linear Algebra Subprograms), the LAPACK project has, in large part, achieved its goal. Unfortunately, the efficient implementation of the BLAS results often in machine-specific code that is not portable across multiple architectures without a significant loss in performance or a significant effort to reoptimize them. This article examines whether most of the hand optimizations performed on matrix factorization codes are unnecessary because they can (and should) be performed by the compiler. We believe that it is better for the programmer to express algorithms in a machine-independent form and allow the compiler to handle the machine-dependent details. This gives the algorithms portability across architectures and removes the error-prone, expensive, and tedious process of hand optimization. Although there currently exist no production compilers that can perform all the loop transformations discussed in this article, a description of current research in compiler technology is provided that will prove beneficial to the numerical linear algebra community. We show that the Cholesky and optimized automatically by a compiler to be as efficient as the same hand-optimized version found in LAPACK. We also show that the QR factorization may be optimized by the compiler to perform comparably with the hand-optimized LAPACK version on modest matrix sizes. Our approach allows us to conclude that with the advent of the compiler optimizations discussed in this article, matrix factorizations may be efficiently implemented in a BLAS-less form.",D.3.4 [Programming Languages]: Processors - compilers; F.2.1 [Analysis of Algorithms and Problem Complexity]: Numerical Algorithms and Problems - computations on matrices; Optimization,Algorithms; Codes (symbols); Matrix algebra; Optimization; Basic linear algebra subprograms (BLAS); Dense matrix factorizations; Program compilers
Algorithm 771: Rksuite_90: Fortran 90 Software for Ordinary Differential Equation Initial-Value Problems,1997,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031221067&doi=10.1145%2f275323.275328&partnerID=40&md5=e2a0a04d278b78d135e70d5b513949d4,"We present Fortran 90 software for the initial-value problem in ordinary differential equations, including the interfaces and how Fortran 90 language features afford the opportumty both to address different types and structures of variables and to provide additional functionality. A novel feature of this software is the availability of Unix scripts which enable presentation of the software for multiple problem types.",Algorithms; Complex; D.3.2 [Progamming Languages]: Language Classifications - Fortran 90; G.1.7 [Numerical Analysis]: Ordinary Differential Equations - initial value problems; G.4 [Mathematics of Computing]: Mathematical Software; Recursion,Algorithms; Differential equations; FORTRAN (programming language); Recursive functions; UNIX; Initial value problems; Computer software
Level 3 Basic Linear Algebra Subprograms for Sparse Matrices: A User-Level Interface,1997,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031223114&doi=10.1145%2f275323.275327&partnerID=40&md5=f1f77be8589f57ede99630742a158b1b,"This article proposes a set of Level 3 Basic Linear Algebra Subprograms and associated kernels for sparse matrices. A major goal is to design and develop a common framework to enable efficient, and portable, implementations of iterative algorithms for sparse matrices on high-performance computers. We have designed the routines to shield the developer of mathematical software from most of the complexities of the various data structures used for sparse matrices. We have kept the interface and suite of codes as simple as possible while at the same time including sufficient functionality to cover most of the requirements of iterative solvers and sufficient flexibility to cover most sparse matrix data structures. An important aspect of our framework is that it can be easily extended to incorporate new kernels if the need arises. We discuss the design, implementation, and use of subprograms for the multiplication of a full matrix by a sparse one and for the solution of sparse triangular systems with one or more (full) right-hand sides. We include a routine for checking the input data, generating a new sparse data structure from that input, and scaling a sparse matrix. The new data structure for the transformation can be specified by the user or can be chosen automatically by vendors to be efficient on their machines. We also include a routine for permuting the columns of a sparse matrix and one for permuting the rows of a full matrix.",D.2.2 [Software Engineering]: Tools and Techniques - user interfaces; D.2.3 [Software Engineering]: Coding - standards; F.2.1 [Analysis of Algorithms and Problem Complexity]: Numerical Algorithms and Problems - computations on matrices,Algorithms; Codes (symbols); Data structures; Iterative methods; Matrix algebra; User interfaces; Basic linear algebra subprograms (BLAS); Computer aided software engineering
Implementing the Complex Arcsine and Arccosine Functions Using Exception Handling,1997,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031224550&doi=10.1145%2f275323.275324&partnerID=40&md5=c97c39ab917d7fbc6b680f472076c5c6,"We develop effcient algorithms for reliable and accurate evaluations of the complex arcsine and arccosine functions. A tight error bound is derived for each algorithm; the results are valid for all machine-representable points in the complex plane. The algorithms are presented in a pseudocode that has a convenient exception-handling facility. Corresponding Fortran 77 programs for an IEEE environment have also been developed to illustrate the practicality of the algorithms, and these programs have been tested very carefully to help confirm the correctness of the algorithms and their error bounds. The results of these tests are included in the article, but the Fortran 77 programs are not (these programs are available from Fairgrieve). Tests of other widely available programs fail at many points in the complex plane, and otherwise are slower and produce ,ush less accurate results.",G.1.0 [Numerical Analysis]: General - error analysis; G.1.2 [Numerical Analysis]: Approximation - elementary function approximation; G.4 [Mathematics of Computing]: Mathematical Software - algorithm anallysis; Numerical algorithms,Algorithms; Approximation theory; Error analysis; FORTRAN (programming language); Functions; Complex arcsine/arccosine functions; Exception handling; Software engineering
Algorithm 775: The Code SLEUTH for Solving Fourth-Order Sturm-Liouville Problems,1997,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031376840&doi=10.1145%2f279232.279231&partnerID=40&md5=3878e91d9e4eb5f5cf364663b6c772b1,"We describe a new code (SLEUTH) for numerical solution of regular two-point fourth-order Sturm-Liouville eigenvalue problems. Eigenvalues are computed according to index: the user specifies an integer k ≥ 0, and the code computes an approximation to the kth eigenvalue. Eigenfunctions are also available through an auxiliary routine, called after the eigenvalue has been determined. The code will be made available through netlib.",Algorithms; D.3.2 [Programming Languages]: Language Classifications - Fortran 77; G.1.7 [Numerical Analysis]: Ordinary Differential Equations - boundary value problems; G.4 [Mathematics of Computing]: Mathematical Software; SLEUTH,Algorithms; Approximation theory; Codes (symbols); Differential equations; Eigenvalues and eigenfunctions; Problem solving; Software package SLEUTH; Two-point fourth order Sturm-Liouville eigenvalue problems; Computer software
Generating Box-Constrained Optimization Problems,1997,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0346444941&doi=10.1145%2f275323.275331&partnerID=40&md5=b2a3b6c161db420c3592c8aac85e39ab,We present a method for generating box-constrained nonlinear programming test problems. The technique allows the user to control some properties of the generated test problems that are known to influence the behavior of algorithms for their solution. A corresponding set of Fortran 77 routines is described in a companion algorithm (774).,Algorithms; G.1.6 [Numerical Analysis]: Optimization; G.4 [Mathematics of Computing]: Mathematical Software - certification and testing; Nonlinear programming test problems; Optimization; Performance; Problems generation; Test; Verification,Algorithms; Constraint theory; FORTRAN (programming language); Box constrained nonlinear programming; Nonlinear programming
Efficient Householder QR Factorization for Superscalar Processors,1997,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031224551&doi=10.1145%2f275323.275326&partnerID=40&md5=af6edabb19858a728a4f483296dd9767,"To extract the potential promised by superscalar processors, algorithm designers must streamline memory references and allow for efficient data reuse throughout the memory hierarchy. Two parameterized Householder QR factorization algorithms are presented that take into account the caches and registers typical of such processors. Guidelines are developed for choosing parameter values that obtain near-optimal cache and register utilization. The new algorithms are implemented and performance-tuned on an Intel Pentium Pro system, a single thin POWER2 node of the IBM Scalable Parallel System 2 (SP2), and a single R8000 processor of a Silicon Graphics POWER Challenge XL.",B.3.2 [Memory Structures]: Design Styles - cache memories; F.2.1 [Analysis of Algorithms and Problem Complexity]: Numerical Algorithms and Problems - computations on matrices,Algorithms; Computer simulation; Parallel processing systems; Storage allocation (computer); Superscalar processors; Buffer storage
Algorithm 766: Experiments with a Weakly Stable Algorithm for Computing PadéHermite and Simultaneous Padé Approximants,1997,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031096146&doi=10.1145%2f244768.244790&partnerID=40&md5=af2c46312edcae0578e2d337ad117b3a,"In a recent paper, Cabay, Jones, and Labahn develop a fast, iterative, lookahead algorithm for numerically computing Padé-Hermite systems and simultaneous Padé systems along a diagonal of the associated Padé tables. Included in their work is a detailed error analysis showing that the algorithm is weakly stable. In this article, we describe a Fortran implementation, VECTOR_PADE, of this algorithm together with a number of numerical experiments. These experiments show that the theoretical error bounds obtained by Cabay, Jones, and Labahn reflect the general behavior of the actual error, but that in practice these bounds are large overestimates.",D.3.2 [Programming Languages]: Language Classifications - Fortran; G.1 [Mathematics of Computing]: Numerical Analysis; G.1.2 [Numerical Analysis]: Approximation - rational approximation,Approximation theory; Computational methods; Convergence of numerical methods; Error analysis; FORTRAN (programming language); Iterative methods; Matrix algebra; Hankel matrix; Pade-Hermite approximants; Computer software
Algorithm 769: Fortran Subroutines for Approximate Solution of Sparse Quadratic Assignment Problems Using GRASP,1997,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031152332&doi=10.1145%2f264029.264038&partnerID=40&md5=9869dce141d5747b5af265ea3f066620,"We describe Fortran subroutines for finding approximate solutions of sparse instances of the Quadratic Assignment Problem (QAP) using a Greedy Randomized Adaptive Search Procedure (GRASP). The design and implementation of the code are described in detail. Computational results comparing the new subroutines with a dense version of the code (Algorithm 754, ACM TOMS) show that the speedup increases with the sparsity of the data.",Algorithms; Combinatorial optimization; G.1.6 [Numerical Analysis]: Optimization - integer programming; G.2.1 [Discrete Mathematics]: Combinatorics - combinatorial algorithms; G.m [Mathematics of Computing]: Miscellaneous; Performance,Algorithms; Codes (symbols); FORTRAN (programming language); Subroutines; Greedy randomized adaptive search procedure (GRASP); Quadratic assignment problem (QAP); Integer programming
Distributed Nested Decomposition of Staircase Linear Programs,1997,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031153423&doi=10.1145%2f264029.264031&partnerID=40&md5=da34cc30caf94e9d612f877642ceb1da,"This article considers the application of a primal nested-decomposition method to solve staircase linear programs (SLPs) on distributed-memory, multiple-instruction-multiple-data computers. Due to the coupling that exists among the stages of an SLP, a standard parallel-decomposition algorithm for these problems would allow only a subset of the subproblem processes to overlap with one another at any give time. We propose algorithms that seek to increase the amount of overlap among the processes as well as utilize idle time beneficially. Computational results testing the effectiveness of our algorithms are reported, using a standard set of test problems.","C.1.2 [Processor Architectures]: Multiple Data Stream Architectures - multiple-instruction-stream, multiple-data-stream processors; G.1.0 [Numerical Analysis]: General - parallel algorithms",Computer architecture; Data storage equipment; Distributed computer systems; Parallel algorithms; Set theory; Multiple instruction multiple data computers; Primal nested decomposition method; Staircase linear programs (SLP); Linear programming
Fortran 90: An Entry to Object-Oriented Programming for the Solution of Partial Differential Equations,1997,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031094878&doi=10.1145%2f244768.244774&partnerID=40&md5=52f39b0247910187c45e526534f966e5,"The aim of this work is to set up a programming model suitable for numerical computing while taking benefit of Fortran 90's features. The use of concepts from object-oriented programming avoids the weaknesses of the traditional global data programming model of Fortran 77. This work supports the view that object-oriented concepts are not in contradiction with good Fortran 77 programming practices but complement them. These concepts can be embodied in a module-based programming style and result in an efficient and easy-to-maintain code (maintainability means code clarity, scope for further enhancements, and ease of debugging). After introducing the terminology associated with object-oriented programming, it is shown how these concepts are implemented in the framework of Fortran 90. Then, we present an object-oriented implementation of a spectral element solver for a Poisson equation.",D.1.5 [Programming Techniques]: Object-Oriented Programming; D.3.2 [Programming Languages]: Language Classifications - Fortran 90; G.1.8 [Numerical Analysis]: Partial Differential Equations; Spectral element method,Computational methods; FORTRAN (programming language); Partial differential equations; Poisson equations; Spectral element method; Object oriented programming
Boundary-Valued Shape-Preserving Interpolating Splines,1997,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031153379&doi=10.1145%2f264029.264050&partnerID=40&md5=93c6be4737da7248136c09d60532acbc,This article describes a general-purpose method for computing interpolating polynomial splines with arbitrary constraints on their shape and satisfying separable or nonseparable boundary conditions. Examples of applications of the related Fortran code are periodic shape-preserving spline interpolation and the construction of visually pleasing closed curves.,Algorithms; Bernstein-bézier polynomials; Dynamic programming; G.1.1 [Numerical Analysis]: Interpolation; Performance; Spline interpolation,Algorithms; Boundary conditions; Codes (symbols); Constraint theory; Dynamic programming; FORTRAN (programming language); Interpolation; Bernstein Bezier polynomials; Interpolating polynomial splines; Polynomials
Practical Experience in the Numerical Dangers of Heterogeneous Computing,1997,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031152270&doi=10.1145%2f264029.264030&partnerID=40&md5=d8bed6ce3ad79db5400379fbaab6ba89,"Special challenges exist in writing reliable numerical library software for heterogeneous computing environments. Although a lot of software for distributed-memory parallel computers has been written, porting this software to a network of workstations requires careful consideration. The symptoms of heterogeneous computing failures can range from erroneous results without warning to deadlock. Some of the problems are straightforward to solve, but for others the solutions are not so obvious, or incur an unacceptable overhead. Making software robust on heterogeneous systems often requires additional communication. We describe and illustrate the problems encountered during the development of ScaLAPACK and the NAG Numerical PVM Library. Where possible, we suggest ways to avoid potential pitfalls, or if that is not possible, we recommend that the software not be used on heterogeneous networks.",Algorithms; D.1.3 [Programming Techniques]: Concurrent Programming - distributed programming; Distributed-memory systems; Floating-point arithmetic; G.1.0 [Numerical Analysis]: General - computer arithmetic; Parallel algorithms; Reliability,Computer networks; Computer software; Computer system recovery; Data storage equipment; Digital arithmetic; Distributed database systems; Parallel algorithms; Parallel processing systems; Concurrent programming; Software package ScaLAPACK; Computer programming
Object-Oriented Design of Preconditioned Iterative Methods in Diffpack,1997,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031097391&doi=10.1145%2f244768.244776&partnerID=40&md5=7102f5fafe040767e9963cba052fde37,"As modern programming methodologies migrate from computer science to scientific computing, developers of numerical software are faced with new possibilities and challenges. Based on experiences from an ongoing project that develops C++ software for the solution of partial differential equations, this article has its focus on object-oriented design of iterative solvers for linear systems of equations. Special attention is paid to possible conflicts that have to be resolved in order to achieve a very flexible, yet efficient, code.",D.2.2 [Software Engineering]: Tools and Techniques - software libraries; D.3.2 [Programming Languages]: Language Classifications - C++; G.1.3 [Numerical Analysis]: Numerical Linear Algebra - linear systems; Sparse and very large systems,C (programming language); Iterative methods; Linear algebra; Object oriented programming; Partial differential equations; Numerical software; Software engineering
Algorithm 767: A Fortran 77 Package for Column Reduction of Polynomial Matrices,1997,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031097390&doi=10.1145%2f244768.244791&partnerID=40&md5=3f24baf19559bcbc935117e0eb796a7b,"A polynomial matrix is called column reduced if its column degrees are as low as possible in some sense. Two polynomial matrices P and R are called unimodularly equivalent if there exists a unimodular polynomial matrix U such that PU = R. Every polynomial matrix is unimodularly equivalent to a column-reduced polynomial matrix. In this article a subroutine is described that takes a polynomial matrix P as input and yields on output a unimodular matrix U and a column-reduced matrix R such that PU = R; actually PU - R is near zero. The subroutine is based on an algorithm, described in a paper by Neven and Praagman. The subroutine has been tested with a number of examples on different computers, with comparable results. The performance of the subroutine on every example tried is satisfactory in the sense that the magnitude of the elements of the residual matrix PU - R is about ∥P∥ ∥U∥EPS, where EPS is the machine precision. To obtain these results a tolerance, used to determine the rank of some (sub)matrices, has to be set properly. The influence of this tolerance on the performance of the algorithm is discussed, from which a guideline for the usage of the subroutine is derived.",Computations on polynomials; D.3.2 [Programming Languages]: Language Classifications - Fortran; F.2.1 [Analysis of Algorithms and Problem Complexity]: Numerical Algorithms and Problems - computations on matrices,FORTRAN (programming language); Matrix algebra; Polynomials; Column reduction; Polynomial matrix; Subroutines
"Multiplicative, Congruential Random-Number Generators with Multiplier ± 2k1 ± 2k2 and Modulus 2p - 1",1997,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031152251&doi=10.1145%2f264029.264056&partnerID=40&md5=6035c24ecabe4b1336761d9918a68dbe,"The demand for random numbers in scientific applications is increasing. However, the most widely used multiplicative, congruential random-number generators with modulus 231 - 1 have a cycle length of about 2.1 × 109. Moreover, developing portable and efficient generators with a larger modulus such as 261 - 1 is more difficult than those with modulus 231 - 1. This article presents the development of multiplicative, congruential generators with modulus m = 2p - 1 and four forms of multipliers: 2k1 - 2k2, 2k1 + 2k2, m - 2k1 + 2k2, and m - 2k1 - 2k2, k1 > k2. The multipliers for modulus 231 - 1 and 261 - 1 are measured by spectral tests, and the best ones are presented. The generators with these multipliers are portable and very fast. They have also passed several empirical tests, including the frequency test, the run test, and the maximum-of-t test.",Algorithms; Cycle length; Efficiency; G.3 [Mathematics of Computing]: Probability and Statistics-random number generation; Multiplicative congruential random-number generators; Performance; Portability; Spectral test,Algorithms; Computational complexity; Probability; Statistical methods; Cycle length; Random number generation
Algorithm 770: BVSPIS - A Package for Computing Boundary-Valued Shape-Preserving Interpolating Splines,1997,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031153346&doi=10.1145%2f264029.264059&partnerID=40&md5=78c484cc00a9eb40200e72fc4bd36efd,This article describes a software package for computing interpolating polynomial splines with arbitrary constraints on their shape and satisfying separable or nonseparable boundary conditions.,Algorithms; Bernstein-bézier polynomials; D.3.2 [Programming Languages]: Language Classifications - Fortran; Dynamic programming; G.1.1 [Numerical Analysis]: Interpolation; G.4 [Mathematics of Computing]: Mathematical Software; Spline interpolation,Algorithms; Boundary conditions; Constraint theory; Dynamic programming; Interpolation; Polynomials; Bernstein Bezier polynomials; Interpolating polynomial splines; Computer software
Computational Investigations of Low-Discrepancy Sequences,1997,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031152363&doi=10.1145%2f264029.264064&partnerID=40&md5=1d67ae02996f3af7fb1c6db3c6b36488,"The Halton, Sobol, and Faure sequences and the Braaten-Weller construction of the generalized Halton sequence are studied in order to assess their applicability for the quasi Monte Carlo integration with large number of variates. A modification of the Halton sequence (the Halton sequence leaped) and a new construction of the generalized Halton sequence are suggested for unrestricted number of dimensions and are shown to improve considerably on the original Halton sequence. Problems associated with estimation of the error in quasi Monte Carlo integration and with the selection of test functions are identified. Then an estimate of the maximum error of the quasi Monte Carlo integration of nine test functions is computed for up to 400 dimensions and is used to evaluate the known generators mentioned above and the two new generators. An empirical formula for the error of the quasi Monte Carlo integration is suggested.",Algorithms; Discrepancy; Error of numerical integration; Faure sequence; G.1.4 [Numerical Analysis]: Quadrature and Numerical Differentiation; Generalized Halton sequence; Halton sequence; I.6 [Computing Methodologies]: Simulation and Modeling; Performance,Algorithms; Computer simulation; Integration; Mathematical models; Monte Carlo methods; Faure sequence; Generalized Halton sequence; Low discrepancy sequence; Sobol sequence; Computation theory
Enhanced Simulated Annealing for Globally Minimizing Functions of Many-Continuous Variables,1997,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031153042&doi=10.1145%2f264029.264043&partnerID=40&md5=42bbeccfde53588959ad767381521433,"A new global optimization algorithm for functions of many continuous variables is presented, derived from the basic Simulated Annealing method. Our main contribution lies in dealing with high-dimensionality minimization problems, which are often difficult to solve by all known minimization methods with or without gradient. In this article we take a special interest in the variables discretization issue. We also develop and implement several complementary stopping criteria. The original Metropolis iterative random search, which takes place in a Euclidean space ℝn, is replaced by another similar exploration, performed within a succession of Euclidean spaces ℝp, with p ≪ n. This Enhanced Simulated Annealing (ESA) algorithm was validated first on classical highly multimodal functions of 2 to 100 variables. We obtained significant reductions in the number of function evaluations compared to six other global optimization algorithms, selected according to previously published computational results for the same set of test functions. In most cases, ESA was able to closely approximate known global optima. The reduced ESA computational cost helped us to refine further the obtained global results, through the use of some local search. We have used this new minimizing procedure to solve complex circuit design problems, for which the objective function evaluation can be exceedingly costly.",Algorithms; G.1.6 [Numerical Analysis]: Optimization - nonlinear programming; G.3 [Mathematics of Computing]: Probability and Statistics - probabilistic algorithms; G.4 [Mathematics of Computing]: Mathematical Software - certification and testing,Algorithms; Function evaluation; Iterative methods; Probability; Problem solving; Random processes; Simulated annealing; Enhanced simulated annealing (ESA); Nonlinear programming
"Algorithm 765: STENMIN: a Software Package for Large, Sparse Unconstrained Optimization Using Tensor Methods",1997,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031094820&doi=10.1145%2f244768.244788&partnerID=40&md5=7c29a3bbb2d169b47326c2e23aeee700,"We describe a new package for minimizing an unconstrained nonlinear function where the Hessian is large and sparse. The software allows the user to select between a tensor method and a standard method based upon a quadratic model. The tensor method models the objective function by a fourth-order model, where the third- and fourth-order terms are chosen such that the extra cost of forming and solving the model is small. The new contribution of this package consists of the incorporation of an entirely new way of minimizing the tensor model that makes it suitable for solving large, sparse optimization problems efficiently. The test results indicate that, in general, the tensor method is often more efficient and more reliable than the standard Newton method for solving large, sparse unconstrained optimization problems.",D.3.2 [Programming Languages]: Language Classifications - Fortran; G.1.3 [Numerical Analysis]: Numerical Linear Algebra - sparse and very large systems; G.1.6 [Numerical Analysis]: Optimization - unconstrained optimization,Algorithms; Computational methods; Function evaluation; Mathematical models; Optimization; Tensors; Quadratic model; Software package STENMIN; Unconstrained nonlinear function; Computer software
Remark on Algorithm 715,1996,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34548706253&doi=10.1145%2f229473.236186&partnerID=40&md5=444f7b382bb19f31fefa4400ae7ed844,[No abstract available],,
Using the SLEDGE Package on Sturm-Liouville Problems Having Nonempty Essential Spectra,1996,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030420255&doi=10.1145%2f235815.235819&partnerID=40&md5=15cb1f785b5426c6d0c3208f608978e4,We describe the performance of the Sturm-Liouville software package SLEDGE on a variety of problems having continuous spectra. The code's output is shown to be in good accord with a wide range of known theoretical results.,Algorithms; G.1.7 [Numerical Analysis]: Ordinary Differential Equations - boundary value problems; G.4 [Mathematics of Computing]: Mathematical Software - algorithm analysis; Performance; Spectral density functions; Sturm-Liouville problems,Boundary value problems; Computer software; Digital arithmetic; Natural sciences computing; Numerical analysis; Software package SLEDGE; Spectral density function; Sturm Liouville problems; Algorithms
"A Remark on Bartels and Conn's Linearly Constrained, Discrete l1 Problems",1996,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030420005&doi=10.1145%2f235815.235823&partnerID=40&md5=b9670b21ac24cc4679b84f7517cca75a,Two modifications of Bartels and Conn's algorithm for solving linearly constrained discrete l1 problems are described. The modifications are designed to improve performance of the algorithm under conditions of degeneracy.,Algorithms; Degeneracy; G.1.6 [Numerical Analysis]: Optimization - constrained optimization; gradient methods; linear programming; G.3 [Mathematics of Computing]: Probability and Statistics - statistical computing; Performance; Reliability; Theory,Constraint theory; Linear programming; Numerical analysis; Reliability theory; Bartels and Conn's algorithm; Degeneracy; Discrete approximation; Linearly constrained approximation; Algorithms
DESI Methods for Stiff Initial-Value Problems,1996,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030411747&doi=10.1145%2f235815.235818&partnerID=40&md5=309d6ece219a246dbbe8b7a1005e1a30,"Recently, the so-called DESI (diagonally extended singly implicit) Runge-Kutta methods were introduced to overcome some of the limitations of singly implicit methods. Preliminary experiments have shown that these methods are usually more efficient than the standard singly implicit Runge-Kutta (SIRK) methods and, in many cases, are competitive with backward differentiation formulae (BDF). This article presents an algorithm for determining the full coefficient matrix from the stability function, which is already chosen to make the method A-stable. Because of their unconventional nature, DESI methods have to be implemented in a special way. In particular, the effectiveness of these methods depends heavily on how starting values are chosen for the stage iterations. These and other implementation questions are discussed in detail, and the design choices we have made form the basis of an experimental code for the solution of stiff problems by DESI methods. We present here a small subset of the numerical results obtained with our code. Many of these results are quite satisfactory and suggest that DESI methods have a useful role in the solution of this type of problem.",G.1 [Mathematics of Computing]: Numerical Analysis; G.1.7 [Mathematics of Computing]: Ordinary Differential Equations - initial value problems; stiff equations,Differential equations; Matrix algebra; Numerical analysis; Problem solving; Backward differentiation formulae (BDF); Diagonally extended singly implicit (DESI) Runge Kutta methods; Initial value problem; Singly implicit Runge Kutta (SIRK) methods; Stiff equations; Algorithms
A Modified Schur-Complement Method for Handling Dense Columns in Interior-Point Methods for Linear Programming,1996,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030247572&doi=10.1145%2f232826.232937&partnerID=40&md5=d1c11ee247fd786c48dd8c27c93fde25,"The main computational work in interior-point methods for linear programming (LP) is to solve a least-squares problem. The normal equations are often used, but if the LP constraint matrix contains a nearly dense column the normal-equations matrix will be nearly dense. Assuming that the nondense part of the constraint matrix is of full rank, the Schur complement can be used to handle dense columns. In this article we propose a modified Schur-complement method that relaxes this assumption. Encouraging numerical results are presented.",Algorithms; Cholesky factorization; G.1.3 [numerical analysis]: Numerical linear algebra - Linear systems (direct and iterative methods); G.1.6 [numerical analysis]: Optimization - Linear programming; Interior-point methods; Performance,Algorithms; Iterative methods; Least squares approximations; Linear programming; Matrix algebra; Performance; Cholesky factorization; Interior point methods; Modified Schur complement method; Linear algebra
Algorithm 759: VLUGR3: a Vectorizable Adaptive-Grid Solver for PDEs in 3D - Part II. Code Description,1996,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030247571&doi=10.1145%2f232826.232853&partnerID=40&md5=db2c5fce6cf2c4907b234cd415bbd0f8,"This article describes an ANSI Fortran 77 code, VLUGR3, autovectorizable on the Cray Y-MP, that is based on an adaptive-grid finite-difference method to solve time-dependent three-dimensional systems of partial differential equations.",Adaptive-grid methods; Algorithms; D.3.2 [programming languages]: Language classifications - Fortran; G.1.8 [numerical analysis]: Partial differential equations; G.4 mathematical software; Iterative solvers; Method of lines,Algorithms; Computer software; Finite difference method; Iterative methods; Partial differential equations; Vectors; Adaptive grid methods; Method of lines; Nonsymmetric sparse linear systems; Vectorization; FORTRAN (programming language)
Algorithm 756: A MATLAB Toolbox for Schwarz-Christoffel Mapping,1996,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030172363&doi=10.1145%2f229473.229475&partnerID=40&md5=2568662f9d9e8422882a66fa569e6096,"The Schwarz-Christoffel transformation and its variations yield formulas for conformai maps from standard regions to the interiors or exteriors of possibly unbounded polygons. Computations involving these maps generally require a computer, and although the numerical aspects of these transformations have been studied, there are few software implementations that are widely available and suited for general use. The Schwarz-Christoffel Toolbox for MATLAB is a new implementation of Schwarz-Christoffel formulas for maps from the disk, half-plane, strip, and rectangle domains to polygon interiors, and from the disk to polygon exteriors. The toolbox, written entirely in the MATLAB script language, exploits the high-level functions, interactive environment, visualization tools, and graphical user interface elements supplied by current versions of MATLAB, and is suitable for use both as a standalone tool and as a library for applications written in MATLAB, Fortran, or C. Several examples and simple applications are presented to demonstrate the toolbox's capabilities.",Algorithms; G.1.m [Numerical Analysis]: Miscellaneous; G.4 [Mathematics of Computing]: Mathematical Software - MATLAB; J.2 [Computer Applications]: Physical Sciences and Engineering; Numerical conformal mapping; Schwarz-Christoffel transformation,C (programming language); Computational geometry; Computational methods; Computer programming languages; Computer software; Conformal mapping; FORTRAN (programming language); Functions; Graphical user interfaces; High level languages; Mathematical transformations; High level functions; Mathematical software; MATLAB script language; Schwarz-Christoffel mapping; Unbounded polygons; Algorithmic languages
The Mathematical Basis and a Prototype Implementation of a New Polynomial Rootfinder with Quadratic Convergence,1996,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030244864&doi=10.1145%2f232826.232830&partnerID=40&md5=81b3417eb0684f3188fa683d55522cfc,"Formulas developed originally by Weierstrass have been used since the 1960s by many others for the simultaneous determination of all the roots of a polynomial. Convergence to simple roots is quadratic, but individual approximations to a multiple root converge only linearly. However, it is shown here that the mean of such individual approximations converges quadratically to that root. This result, along with some detail about the behavior of such approximations in the neighborhood of the multiple root, suggests a new approach to the design of polynomial rootfinders. It should also be noted that the technique is well suited to take advantage of a parallel environment. This article first provides the relevant mathematical results: a short derivation of the formulas, convergence proofs, an indication of the behavior near a multiple root, and some error bounds. It then provides the outline of an algorithm based on these results, along with some graphical and numerical results to illustrate the major theoretical points. Finally, a new program based on this algorithm, but with a more efficient way of choosing starting values, is described and then compared with corresponding programs from IMSL and NAG with good results. This program is available from Mathon (combin@cs.utoronto.ca).",Error analysis; G.1.0 [numerical analysis]: General - error analysis; G.1.5 [numerical analysis]: Roots of nonlinear equations - convergence; Iterative methods; Methods for polynomials; Numerical algorithms,Algorithms; Approximation theory; Computer software; Convergence of numerical methods; Error analysis; Iterative methods; Nonlinear equations; Polynomial rootfinder; Polynomial zerofinder; Quadratic convergence; Polynomials
Algorithm 760: Rectangular-Grid-Data Surface Fitting that Has the Accuracy of a Bicubic Polynomial,1996,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030245388&doi=10.1145%2f232826.232854&partnerID=40&md5=1e5806ea70ee242a874951b3f4957e97,A local algorithm for smooth surface fitting for rectangular-grid data has been presented. It has the accuracy of a bicubic polynomial.,Algorithm; Bivariate interpolation; Cubic polynomial; D.3.2 [programming languages]: language classifications - fortran; G.1.1 [numerical analysis]: interpolation; G.4 [mathematics of computing]: mathematical software; Interpolation; Local interpolation,Computer software; Curve fitting; Estimation; FORTRAN (programming language); Interpolation; Polynomials; Bicubic polynomial; Bivariate interpolation; Local interpolation; Rectangular grid data surface fitting; Algorithms
The Design of MA48: A Code for the Direct Solution of Sparse Unsymmetric Linear Systems of Equations,1996,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030164754&doi=10.1145%2f229473.229476&partnerID=40&md5=9496415db8b63e305b2fcd519359df30,"We describe the design of a new code for the direct solution of sparse unsymmetric linear systems of equations. The new code utilizes a novel restructuring of the symbolic and numerical phases, which increases speed and saves storage without sacrifice of numerical stability. Other features include switching to full-matrix processing in all phases of the computation enabling the use of all three levels of BLAS, treatment of rectangular or rank-deficient matrices, partial factorization, and integrated facilities for iterative refinement and error estimation.",Algorithms; BLAS; Block triangular form; Error estimation; G.1.3 [Mathematics of Computing]: Numerical Linear Algebra - linear systems (direct methods); Gaussian elimination; Performance; Sparse and very large systems; Sparse unsymmetric matrices,Algorithms; Codes (symbols); Computational methods; FORTRAN (programming language); Iterative methods; Linear algebra; Matrix algebra; Parallel processing systems; Problem solving; User interfaces; Error estimation; Full matrix processing; Numerical phases; Partial factorization; Rank deficient matrices; Rectangular matrices; Restructuring; Sparse unsymmetric linear systems of equations; Symbolic phases; Computer software
Exploiting Zeros on the Diagonal in the Direct Solution of Indefinite Sparse Symmetric Linear Systems,1996,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030170479&doi=10.1145%2f229473.229480&partnerID=40&md5=1eaf335b49da6245f7b8b77fbfbb4c35,"We describe the design of a new code for the solution of sparse indefinite symmetric linear systems of equations. The principal difference between this new code and earlier work lies in the exploitation of the additional sparsity available when the matrix has a significant number of zero diagonal entries. Other new features have been included to enhance the execution speed, particularly on vector and parallel machines.",2 x 2 pivots; Algorithms; Augmented systems; BLAS; G.1.3 [Numerical Analysis]: Numerical Linear Algebra - linear systems (direct methods); Gaussian elimination; Indefinite symmetric matrices; Performance; Sparse; Sparse and very large systems,Algorithms; Codes (symbols); Constraint theory; FORTRAN (programming language); Least squares approximations; Linear algebra; Matrix algebra; Parallel processing systems; Vectors; Additional sparsity; Augmented systems; Gaussian elimination; Sparse indefinite symmetric linear systems; Zero diagonal entries; Computer software
"The Design of a New Frontal Code for Solving Sparse, Unsymmetric Systems",1996,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030092662&doi=10.1145%2f225545.225550&partnerID=40&md5=f1e261b8473a135a7e9fa6ec3b7cdd10,"We describe the design, implementation, and performance of a frontal code for the solution of large, sparse, unsymmetric systems of linear equations. The resulting software package, MA42, is included in Release 11 of the Harwell Subroutine Library and is intended to supersede the earlier MA32 package. We discuss in detail the extensive use of higher-level BLAS kernels within MA42 and illustrate the performance on a range of practical problems on a CRAY Y-MP, an IBM 3090, and an IBM RISC System/6000. We examine extending the frontal solution scheme to use multiple fronts to allow MA42 to be run in parallel. We indicate some directions for future development.",Algorithms; Frontal method; G.1.0 [Numerical Analysis]: General - numerical algorithms; G.1.3 [Numerical Analysis]: Numerical Linear Algebra - linear systems; Gaussian elimination; Large sparse matrices; Level 3 BLAS; Real unsymmetric matrices,Algebra; Computer software; Matrix algebra; Numerical analysis; Frontal method; Gaussian elimination; Level 3 BLAS; MA42; Sparse unsymmetric systems; Algorithms
Algorithm 763: Interval_arithmetic: A Fortran 90 Module for an Interval Data Type,1996,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030392826&doi=10.1145%2f235815.235816&partnerID=40&md5=a72012cb926c69ccb0fb3ce86a84ec80,"Interval arithmetic is useful in automatically verified computations, that is, in computations in which the algorithm itself rigorously proves that the answer must lie within certain bounds. In addition to rigor, interval arithmetic also provides a simple and sometimes sharp method of bounding ranges of functions for global optimization and other tasks. Convenient use of interval arithmetic requires an interval data type in the programming language. Although various packages supply such a data type, previous ones are machine specific, obsolete, and unsupported, for languages other than Fortran, or commercial. The Fortran 90 module INTERVAL_ARITHMETIC provides a portable interval data type in Fortran 90. This data type is based on two double-precision real Fortran storage units. Module INTERVAL_ARITHMETIC uses the Fortran 77 library INTLIB (ACM TOMS Algorithm 737) as a supporting library. The module has been employed extensively in the author's own research.",Algorithms; D.3.2 [Programming Languages]: Language Classifications - Fortran 90; G.1.0 [Numerical Analysis]: General - computer arithmetic; error analysis; numerical algorithms; Interval arithmetic; Languages; Operator overloading; Portability,Algorithmic languages; Algorithms; Computer software portability; Digital arithmetic; Error analysis; Natural sciences computing; Algorithm 723; Data type; Interval arithmetic; Operator overloading; FORTRAN (programming language)
Algorithm 755: ADOL-C: A Package for the Automatic Differentiation of Algorithms Written in C/C++,1996,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030168036&doi=10.1145%2f229473.229474&partnerID=40&md5=a244a9d152b6705564092c9c4737c213,"The C++ package ADOL-C described here facilitates the evaluation of first and higher derivatives of vector functions that are defined by computer programs written in C or C++. The resulting derivative evaluation routines may be called from C/C++, Fortran, or any other language that can be linked with C. The numerical values of derivative vectors are obtained free of truncation errors at a small multiple of the run-time and randomly accessed memory of the given function evaluation program. Derivative matrices are obtained by columns or rows. For solution curves defined by ordinary differential equations, special routines are provided - that evaluate the Taylor coefficient vectors and their Jacobians with respect to the current state vector. The derivative calculations involve a possibly substantial (but always predictable) amount of data that are accessed strictly sequentially and are therefore automatically paged out to external files.",Algorithms; Automatic differentiation; G.1.4 [Numerical Analysis]: Quadrature and Numerical Differentiation - computational differentiation; I.1.2 [Algebraic Manipulation]: Algorithms - analysis of algorithms; Performance,Algorithms; C (programming language); Calculations; Differential equations; FORTRAN (programming language); Functions; Matrix algebra; Random access storage; Vectors; Automatic differentiation; Chain rule; Derivative evaluation; Derivative vectors; Forward mode; Hessians; Jacobians; Taylor coefficient vectors; Truncation errors; Algorithmic languages
Algorithm 761: Scattered-Data Surface Fitting that Has the Accuracy of a Cubic Polynomial,1996,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030246781&doi=10.1145%2f232826.232856&partnerID=40&md5=d532499d781fdcb3605a820ed3644537,"An algorithm for smooth surface fitting for scattered data has been presented. It has the accuracy of a cubic polynomial in most cases and is a local, triangle-based algorithm.",Algorithm; Bivariate interpolation; D.3.2 [programming languages]: language classifications - fortran; G.1.1 [numerical analysis]: interpolation; G.4 [mathematics of computing]: mathematical software; Interpolation; Local interpolation,Computer software; Curve fitting; Estimation; FORTRAN (programming language); Geometry; Interpolation; Polynomials; Bivariate interpolation; Cubic polynomial; Local interpolation; Scattered data surface fitting; Algorithms
A Composition-Alias Method for Generating Gamma Variates with Shape Parameter Greater than 1,1996,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030379993&doi=10.1145%2f235815.235822&partnerID=40&md5=84593a524f1d6149b6dd3e508192bdb7,"In this article the author describes a procedure for generating gamma variates with shape parameter >1. Given a supply of ""good"" uniform (0,1) variates, the procedure makes use of composition method, squeeze method, and aliasing to generate gamma variates. Comparison with existing methods shows that the author's method is faster in terms of computer time and uses a smaller number of uniform (0,1) variates. The method is also statistically exact.",Acceptance-rejection; Algorithms; Aliasing; G.3 [Mathematics of Computing]: Probability and Statistics; I.6.1 [Simulation and Modeling]: Simulation Theory; Squeeze method; Theory,Computation theory; Digital arithmetic; Probability; Simulation; Acceptance rejection; Aliasing; Composition method; Gamma variates; Squeeze method; Algorithms
"Algorithm 757: MISCFUN, a Software Package to Compute Uncommon Special Functions",1996,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030244925&doi=10.1145%2f232826.232846&partnerID=40&md5=6d897506ddfed3f0c7a669276289ecaf,"MISCFUN (Miscellaneous FUNctions) is a Fortran package for the evaluation of several special functions, which are not used often enough to have been included in the standard libraries or packages. The package uses Chebyshev expansions as the underlying method of approximation, with the Chebyshev coefficients given to 20D. A wide variety of functions are included, and the package is designed so that other functions can be added in a standard manner.",Algorithms; Chebyshev polynomials; D.3.2 [programming languages]: Language classifications - fortran; G.1.2 [numerical analysis]: Approximation - chebyshev approximation and theory; G.4 [mathematical software]: Certification and testing; Special functions,Algorithms; Chebyshev approximation; FORTRAN (programming language); Polynomials; Program compilers; Chebyshev polynomials; Uncommon special functions; Computer software
The Computation of Elementary Unitary Matrices,1996,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030392610&doi=10.1145%2f235815.235817&partnerID=40&md5=63ac54e0c5e1d0bc49082efec197923f,"The construction of elementary unitary matrices that transform a complex vector to a multiple of e1, the first column of the identity matrix, is studied. We present four variants and their software implementation, including a discussion on the LAPACK subroutine CLARFG. Comparisons are also given.",F.2. [Analysis of Algorithms and Problem Complexity]: Numerical Algorithms and Problems - computations on matrices; G.1.3 [Numerical Analysis]: Numerical Linear Algebra; G.4 [Mathematics of Computing]: Mathematical Software - algorithm analysis,Computational complexity; Mathematical transformations; Matrix algebra; Numerical analysis; Software engineering; Subroutines; Elementary matrices; Hermitian matrix; Householder reflectors; Software package CLARFG; Algorithms
"Remark on ""Fast Floating-Point Processing in Common Lisp""",1996,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030380918&doi=10.1145%2f235815.235824&partnerID=40&md5=85600cf2b9803bea08da3824a191f367,We explain why we feel that the comparison between Common Lisp and Fortran in a recent article by Fateman et al. in this journal is not entirely fair.,"Common Lisp; D.3.0 [Programming Languages]: General - standards; D.3.3 [Programming Languages]: Language Constructs - control structures; modules, packages; Floating-point computation; Fortran; Fortran 90; Languages; Standardization",FORTRAN (programming language); LISP (programming language); Standards; Common Lisp language; Control structures; Floating point computation; Digital arithmetic
Algorithm 758: VLUGR2: a Vectorizable Adaptive-Grid Solver for PDEs in 2D,1996,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030245980&doi=10.1145%2f232826.232850&partnerID=40&md5=ac338f56220a2c7fd6950de466c534b3,"This article deals with an adaptive-grid finite-difference solver for time-dependent two-dimensional systems of partial differential equations. It describes the ANSI Fortran 77 code, VLUGR2, autovectorizable on the Cray Y-MP, that is based on this method. The robustness and the efficiency of the solver, both for vector and scalar processors, are illustrated by the application of the code to two example problems arising from a groundwater-flow model.",Adaptive-grid methods; Algorithms; D.3.2 [programming languages]: Language classifications - Fortran; G.1.8 [numerical analysis]: Partial differential equations; G.4 [mathematical software]; Iterative solvers,Computer software; Finite difference method; FORTRAN (programming language); Iterative methods; Mathematical models; Partial differential equations; Vectors; Adaptive grid methods; Method of lines; Nonsymmetric sparse linear systems; Vectorization; Algorithms
PYTHIA: A Knowledge-Based System to Select Scientific Algorithms,1996,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030381078&doi=10.1145%2f235815.235820&partnerID=40&md5=bc78ad2fc6773877610dbf68902e0053,"Problem-solving Environments (PSEs) interact with the user in a language ""natural"" to the associated discipline, and they provide a high-level abstraction of the underlying, computationally complex model. The knowledge-based system PYTHIA addresses the problem of (parameter, algorithm) pair selection within a scientific computing domain assuming some minimum user-specified computational objectives and some characteristics of the given problem. PYTHIA's framework and methodology are general and applicable to any class of scientific problems and solvers. PYTHIA is applied in the context of Parallel ELLPACK where there are many alternatives for the numerical solution of elliptic partial differential equations (PDEs). PYTHIA matches the characteristics of the given problem with those of PDEs in an existing problem population and then uses performance profiles of the various solvers to select the appropriate method given user-specified error and solution time bounds. The profiles are automatically generated for each solver of the Parallel ELLPACK library.",Algorithms; Computational intelligence; G.1.8 [Numerical Analysis]: Partial Differential Equations; I.2.1 [Artificial Intelligence]: Applications and Expert Systems; Knowledge-based systems; Partial differential equations; Performance,Algorithms; Artificial intelligence; Computational complexity; Digital arithmetic; Natural sciences computing; Numerical analysis; Parameter estimation; Partial differential equations; Problem solving environments (PSEs); PYTHIA knowledge based systems; Expert systems
"Algorithm 762: LLDRLF, Log-Likelihood and Some Derivatives for Log-F Models",1996,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030246464&doi=10.1145%2f232826.232858&partnerID=40&md5=1aa0629a1dd6bf075d912651b48fb0bb,Two flexible statistical models incorporating the log-F distribution are little used because of numeric difficulties. We describe a method for calculating the log-likelihood and two derivatives with respect to the data argument. Fortran subroutines incorporating these calculations are provided.,Accelerated failure; Algorithms; D.3.2 [programming languages]: language classifications - fortran; G.1.2 [numerical analysis]: approximation; Log-F distribution; Log-likelihood,Approximation theory; FORTRAN (programming language); Mathematical models; Statistics; Subroutines; Accelerated failure; Log F distribution; Log likelihood; Algorithms
The Quickhull Algorithm for Convex Hulls,1996,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030381077&doi=10.1145%2f235815.235821&partnerID=40&md5=a2466a3dedc9a282f160dfc04d3385af,"The convex hull of a set of points is the smallest convex set that contains the points. This article presents a practical convex hull algorithm that combines the two-dimensional Quickhull Algorithm with the general-dimension Beneath-Beyond Algorithm. It is similar to the randomized, incremental algorithms for convex hull and Delaunay triangulation. We provide empirical evidence that the algorithm runs faster when the input contains nonextreme points and that it uses less memory. Computational geometry algorithms have traditionally assumed that input sets are well behaved. When an algorithm is implemented with floating-point arithmetic, this assumption can lead to serious errors. We briefly describe a solution to this problem when computing the convex hull in two, three, or four dimensions. The output is a set of ""thick"" facets that contain all possible exact convex hulls of the input. A variation is effective in five or more dimensions.","Algorithms; Convex hull; Delaunay triangulation; Halfspace intersection; I.3.5 [Computer Graphics]: Computational Geometry and Object Modeling - geometric algorithms, languages, and systems; Reliability; Voronoi diagram",Algorithmic languages; Computer graphics; Digital arithmetic; Numerical analysis; Beneath beyond algorithm; Convex hulls; Delaunay triangulation; Halfspace intersection; Quickhull algorithm; Voronoi diagram; Computational geometry
Note on the End Game in Homotopy Zero Curve Tracking,1996,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030244841&doi=10.1145%2f232826.232843&partnerID=40&md5=d7516ad03f6a52aab750cfd217bbd60c,"Homotopy algorithms to solve a nonlinear system of equations f(x) = 0 involve tracking the zero curve of a homotopy map ρ(a, λ, x) from λ = 0 until λ = 1. When the algorithm nears or crosses the hyperplane λ = 1, an ""end game"" phase is begun to compute the solution x̄, satisfying ρ(a, λ, x̄) = f(x̄) = 0. This note compares several end game strategies, including the one implemented in the normal flow code FIXPNF in the homotopy software package HOMPACK.",Algorithms; Curve tracking; Fixed point; G.1.5 [numerical analysis]: Roots of nonlinear equations - systems of equations; G.4 [mathematics of computing]: Mathematical software; Globally convergent; Homotopy methods; Polynomial systems; Zero,Computer software; Convergence of numerical methods; Nonlinear equations; Poles and zeros; Polynomials; End game; Fixed point; Globally convergent; Homotopy zero curve tracking; Polynomial systems; Algorithms
Remark oh Algorithm 723: Fresnel Integrals,1996,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030378476&doi=10.1145%2f235815.235825&partnerID=40&md5=03781471ccc794d76ca0f68b215627c2,"""Algorithm 723: Fresnel Integrals"" has been improved to provide more precise results for x ≫ 0.",Algorithms; D.3.2 [Programming Languages]: Language Classifications - Fortran; G.1.2 [Numerical Analysis]: Approximation - rational approximation; G.4 [Mathematics of Computing]: Mathematical Software - certification and testing; Performance,Algorithms; Digital arithmetic; FORTRAN (programming language); Numerical analysis; Algorithm 723; Fresnel integrals; Algorithmic languages
Algorithm 754: Fortran Subroutines for Approximate Solution of Dense Quadratic Assignment Problems Using GRASP,1996,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030092646&doi=10.1145%2f225545.225553&partnerID=40&md5=31b67b58f5515539789d484241038226,"In the NP-complete quadratic assignment problem (QAP), n facilities are to be assigned to n sites at minimum cost. The contribution of assigning facility i to site k and facility j to site l to the total cost is fij · dkl, where fij is the flow between facilities j and j, and dkl is the distance between sites k and l. Only very small (n ≤ 20) instances of the QAP have been solved exactly, and heuristics are therefore used to produce approximate solutions. This article describes a set of Fortran subroutines to find approximate solutions to dense quadratic assignment problems, having at least one symmetric flow or distance matrix. A greedy, randomized, adaptive search procedure (GRASP) is used to produce the solutions. The design and implementation of the code are described in detail, and extensive computational experiments are reported, illustrating solution quality as a function of running time.",Algorithms; Combinatorial optimization; D.3.2 [Programming Languages]: Language Classifications - Fortran; G.1.6 [Numerical Analysis]: Optimization - Integer programming; G.2.1 [Discrete Mathematics]: Combinatorics - Combinatorial algorithms; Performance,Combinatorial mathematics; Computer software; FORTRAN (programming language); Integer programming; Numerical analysis; Optimization; GRASP; Local search; Quadratic assignment problem; Algorithms
QMRPACK: A Package of QMR Algorithms,1996,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030092645&doi=10.1145%2f225545.225551&partnerID=40&md5=ecd1f561cb7d85fcae27404b26fd0331,"The quasi-minimal residual (QMR) algorithm is a Krylov-subspace method for the iterative solution of large non-Hermitian linear systems. QMR is based on the look-ahead Lanczos algorithm that, by itself, can also be used to obtain approximate eigenvalues of large non-Hermitian matrices. QMRPACK is a software package with Fortran 77 implementations of the QMR algorithm and variants thereof, and of the three-term and coupled two-term look-ahead Lanczos algorithms. In this article, we discuss some of the features of the algorithms in the package, with emphasis on the issues related to using the codes. We describe in some detail two routines from the package, one for the solution of linear systems and the other for the computation of eigenvalue approximations. We present some numerical examples from applications where QMRPACK was used.",D.3.2 [Programming Languages]: Language Classifications - Fortran 77; F.2.1 [Analysis of Algorithms and Problem Complexity]: Numerical Algorithms and Problems - computations on matrices,Algebra; Computer software; Eigenvalues and eigenfunctions; FORTRAN (programming language); Iterative methods; Matrix algebra; Numerical analysis; Reliability; Transfer functions; Krylov subspace; Lanczos method; Non-Hermitian matrices; QMRPACK package; Algorithms
LAPACK-Style Algorithms and Software for Solving the Generalized Sylvester Equation and Estimating the Separation between Regular Matrix Pairs,1996,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030092417&doi=10.1145%2f225545.225552&partnerID=40&md5=a279577e019c7af86f397117420e9302,"Robust and fast software to solve the generalized Sylvester equation (AR - LB = C, DR - LE = F) for unknowns R and L is presented. This special linear system of equations, and its transpose, arises in computing error bounds for computed eigenvalues and eigenspaces of the generalized eigenvalue problem S - λT, in computing deflating subspaces of the same problem, and in computing certain decompositions of transfer matrices arising in control theory. Our contributions are twofold. First, we reorganize the standard algorithm for this problem to use Level 3 BLAS operations, like matrix multiplication, in its inner loop. This speeds up the algorithm by a factor of 9 on an IBM RS6000. Second, we develop and compare several condition estimation algorithms, which inexpensively but accurately estimate the sensitivity of the solution of this linear system.",F.2.1 [Analysis of Algorithms and Problem Complexity]: Numerical Algorithms and Problems - computation on matrices; G.1.3 [Numerical Analysis]: Numerical Linear Algebra - conditioning; eigenvalues; linear system,Algebra; Computer software; Eigenvalues and eigenfunctions; Matrix algebra; Numerical analysis; Condition numbers; Dif-estimators; Generalized Sylvester equation; LAPACK; Algorithms
Erratum: Corrigendum: Computing selected eigenvalues of sparse unsymmetric matrices using subspace iteration (ACM Transactions on Mathematical Software (TOMS)),1995,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-62549129851&doi=10.1145%2f212066.215254&partnerID=40&md5=2ded65acbd981ce06e426a32acbd2c68,[No abstract available],Algorithms; Design,
Fast Pseudorandom Generators for Normal and Exponential Variates,1996,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030092450&doi=10.1145%2f225545.225554&partnerID=40&md5=71796df0665fd4225e137a8ecf021bfd,"Fast algorithms for generating pseudorandom numbers from the unit-normal and unit-exponential distributions are described. The methods are unusual in that they do not rely on a source of uniform random numbers, but generate the target distributions directly by using their maximal-entropy properties. The algorithms are fast. The normal generator is faster than the commonly used Unix library uniform generator ""random"" when the latter is used to yield real values. Their statistical properties seem satisfactory, but only a limited suite of tests has been conducted. They are written in C and as written assume 32-bit integer arithmetic. The code is publicly available as C source and can easily be adopted for longer word lengths and/or vector processing.",Algorithms; Design; Exponential distribution; G.3 [Mathematics of Computing]: Probability and Statistics - random number generation; statistical computing; Gaussian distribution; Normal distribution; Performance; Pseudorandom numbers; Random numbers,C (programming language); Computer software; Design; Probability; Random number generation; Statistics; Exponential distribution; Gaussian distribution; Normal distribution; Algorithms
Efficient Vector and Parallel Manipulation of Tensor Products,1996,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030092454&doi=10.1145%2f225545.225548&partnerID=40&md5=59ddcc122a64344885e31344b340fc23,We present efficient vector and parallel methods for manipulating tensor products of matrices. We consider both computing the matrix-vector product (A1 ⊗ ⋯ ⊗ AK)x and solving the system of linear equations (A1 ⊗ ⋯ ⊗ AK)x = b. The methods described are independent of K. We accompany this article with a companion algorithm which describes an implementation of a complete set of tensor product routines based on LAPACK and the Level 2 and 3 Basic Linear Algebra Subprograms (BLAS) which provide vectorization and parallelization.,Algorithms; D.1.3 [Programming Techniques]: Concurrent Programming; G.1.3 [Numerical Analysis]: Numerical Linear Algebra; G.4 [Mathematics of Computing]: Mathematical Software; LAPACK; Parallel; Performance; Tensor product; Vector,Algebra; Computer programming; Computer software; Numerical analysis; Tensors; Vectors; LAPACK; Parallelization; Tensor product; Vectorization; Algorithms
Algorithm 752: SRFPACK: Software for Scattered Data Fitting with a Constrained Surface under Tension,1996,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030092461&doi=10.1145%2f225545.225547&partnerID=40&md5=9116911161b27f0400a770e92792e98c,"SRFPACK is a Fortran 77 software package that constructs a smooth interpolatory or approximating surface to data values associated with arbitrarily distributed points in the plane. It employs automatically selected tension factors to preserve shape properties of the data and to avoid overshoot and undershoot associated with steep gradients. The domain of the fitting function may be nonconvex or multiply connected, and the surface may be constrained to have discontinuous value or derivative across a user-specified curve representing, for example, a geological fault line. Although triangle based, the method provides a means of avoiding the inaccuracy associated with long thin triangles on the boundary of the convex hull of the data abscissae.",Algorithms; D.3.2 [Programming Languages]: Language Classifications - Fortran; G.1.1 [Numerical Analysis]: Interpolation; G.1.2 [Numerical Analysis]: Approximation; G.4 [Mathematics of Computing]: Mathematical Software; Interpolation,Computer software; FORTRAN (programming language); Interpolation; Mathematical techniques; Numerical analysis; Surfaces; Tensile strength; Approximation; Scattered data fitting; Smoothing; SRFPACK; Algorithms
Algorithm 751: TRIPACK: A Constrained Two-Dimensional Delaunay Triangulation Package,1996,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030092479&doi=10.1145%2f225545.225546&partnerID=40&md5=70b479d915aa0692d66a0a0971462e38,"TRIPACK is a Fortran 77 software package that employs an incremental algorithm to construct a constrained Delaunay triangulation of a set of points in the plane (nodes). The triangulation covers the convex hull of the nodes but may include polygonal constraint regions whose triangles are distinguishable from those in the remainder of the triangulation. This effectively allows for a nonconvex or multiply connected triangulation (the complement of the union of constraint regions) while retaining the efficiency of searching and updating a convex triangulation. The package provides a wide range of capabilities including an efficient means of updating the triangulation with nodal additions or deletions. For N nodes, the storage requirement is 13N integer storage locations in addition to the 2N nodal coordinates.",Algorithms; Constrained Delaunay triangulation; D.3.2 [Programming Languages]: Language Classifications - Fortran; G.1.1 [Numerical Analysis]: Interpolation; G.1.2 [Numerical Analysis]: Approximation; G.4 [Mathematics of Computing]: Mathematical Software,Computer software; FORTRAN (programming language); Interpolation; Mathematical techniques; Numerical analysis; Approximation; Constrained Delaunay triangulation; TRIPACK; Algorithms
Algorithm 753: TENPACK: A LAPACK-Based Library for the Computer Manipulation of Tensor Products,1996,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030092363&doi=10.1145%2f225545.225549&partnerID=40&md5=dad60b0abdb9b065df6042468047cf16,This article presents the interface of an implementation of methods to manipulate equations of the form A1 ⊗ ⋯ ⊗Am where the Ai are matrices. The methods described are independent of m. The code is based on LAPACK and the BLAS and supports virtually all of the matrix formats supported by those packages. Timings of the implementation on several machines are also given.,Algorithms; D.1.3 [Programming Techniques]: Concurrent Programming; D.3.2 [Programming Languages]: Language Classifications - Fortran; G.1.3 [Numerical Analysis]: Numerical Linear Algebra; G.4 [Mathematics of Computing]: Mathematical Software; Performance,Algebra; Computer programming; Computer software; FORTRAN (programming language); Numerical analysis; Tensors; Vectors; LAPACK; TENPACK; Tensor product; Algorithms
Corrigendum: algorithm 729 FORTRAN subroutines for general Toeplitz systems,1994,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0028385588&doi=10.1145%2f174603.174407&partnerID=40&md5=5b3e8539f1112fa62acb5d3c9ecaa52c,"This paper presents FORTRAN 77 implementations of the lookahead Levinson algorithm of Chan and Hansen for solving symmetric indefinite and general Toeplitz systems. The algorithms are numerically stable for all Toeplitz matrices that do not have many consecutive ill-conditioned leading principal submatrices, and also produce estimates of the algorithm and matrix condition numbers.",,FORTRAN (programming language); Linear programming; Matrix algebra; Numerical analysis; Condition estimation; Levinsons algorithm; Toeplitz system; Algorithms
Corrigendum: algorithms 730 an implementation of a divide and conquer algorithm for the unitary eigenproblem,1994,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0028386763&doi=10.1145%2f174603.174406&partnerID=40&md5=a665848abc75276e73d8f95ddf8b14c2,"We present a FORTRAN implementation of a divide-and-conquer method for computing the spectral resolution of a unitary upper Hessenberg matrix H. Any such matrix H of order n, normalized so that its subdiagonal elements are nonnegative, can be written as a product of n - 1 Givens matrices and a diagonal matrix.",,Linear programming; Numerical analysis; Software engineering; Divide and conquer algorithm; Fast algorithm; Unitary eigenvalue problem; Algorithms
Algorithm 702: TNPACK - a truncated Newton minimization package for large-scale problems: I. Algorithm and usage,1992,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0026870308&doi=10.1145%2f146847.146921&partnerID=40&md5=9648e2819d7cbc77e7dea4779b57acc6,"We present a FORTRAN package of subprograms for minimizing multivariate functions without constraints by a truncated Newton algorithm. The algorithm is especially suited for problems involving a large number of variables. Truncated Newton methods allow approximate, rather than exact, solutions to the Newton equations. Truncation is accomplished in the present version by using the preconditioned Conjugate Gradient algorithm (PCG) to solve approximately the Newton equations. The preconditioner M is factored in PCG using a sparse modified Cholesky factorization based on the Yale Sparse Matrix Package. In this paper we briefly describe the method and provide details for program usage.",,Algorithms; Computational methods; Computer software; FORTRAN (programming language); Linear algebra; Nonlinear programming; Subroutines; Algorithm 702; Multivariate functions; Preconditioned conjugate gradient (PCG) algorithm; Software package TNPACK; Software package Yale Sparse Matrix; Sparse modified Cholesky factorization; Truncated Newton minimization algorithm; Numerical analysis
Algorithm 697: Univariate interpolation that has the accuracy of a third-degree polynomial,1991,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0026216308&doi=10.1145%2f114697.116808&partnerID=40&md5=cd237f13d5e9b5c5a3484542c3086b2c,"This algorithm describes the UVIP3P subroutine subprogram that implements the method of univariate interpolation and smooth curve fitting [1]. The subroutine is written in ANSI Standard FORTRAN [2]. Possible use of a higher degree polynomial is also implemented as the user's option. Unless the option of the use of a higher degree polynomial is exercised by the user, the method has the accuracy of a third-degree (cubic) polynomial even when the input data points are given at unequal intervals.",,Computational methods; Curve fitting; FORTRAN (programming language); Interpolation; Piecewise linear techniques; Polynomials; Subroutines; ANSI standard FORTRAN; Third degree polynomial; Univariate interpolation; UVIP3P subroutine subprogram; Algorithms
Evaluation of the complex error function,1990,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0025403983&doi=10.1145%2f77626.77630&partnerID=40&md5=002cfa42ff8eb7794e33470a1a7d748a,"Given a complex number z, in the first quadrant of the complex plane, WOFZ(z) computes the value of the Faddeeva-function w(z) = exp(-z2) erfc(-iz) with an accuracy of 14 significant digits. While the body of the algorithm is the same as that of Algorithm 363, the initialization part is largely changed so as to improve both the accuracy and the speed of the algorithm. The major distinction between Algorithm 363 and Algorithm 680 lies initially in the choice of the variable QRHO and in the fact that NU isn't a constant if (QRHO.GE.10) but decreases with increasing |z|. Secondly, in the neighborhood of the origin, a different approximation for the Faddeeva function is used.",,Mathematical Techniques--Error Analysis; Complex Error Function; Error Functions; Mathematical Software; Recursive Computation; Computer Programming
ON COMPUTATIONAL EFFICIENCY OF THE ITERATIVE METHODS FOR THE SIMULTANEOUS APPROXIMATION OF POLYNOMIAL ZEROS.,1986,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0023011965&doi=10.1145%2f22721.8932&partnerID=40&md5=9de7cbbe4ccd3e3474035439f60ca9bc,"A measure of efficiency of simultaneous methods for determination of polynomial zeros, defined by the coefficient of efficiency, is considered. This coefficient takes into consideration (1) the R-order of convergence in the sense of the definition introduced by J. M. Ortega and W. C. Rheinboldt (Iterative Solution of Nonlinear Equations in Several Variables. Academic Press, New York, 1970) and (2) the number of basic arithmetic operations per iteration, taken with certain weights depending on a processor time. The introduced definition of computational efficiency was used for comparison of the simultaneous methods with various structures.",,COMPUTER SOFTWARE; COMPUTATIONAL EFFICIENCY; ITERATIVE METHODS; MATHEMATICAL SOFTWARE; POLYNOMIAL ZEROS; SIMULTANEOUS APPROXIMATION; MATHEMATICAL TECHNIQUES
COMPUTING CONTOURS BY SUCCESSIVE SOLUTION OF QUINTIC POLYNOMIAL EQUATIONS.,1984,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0021602710&partnerID=40&md5=a707fddbef39530cf4953ed694a11bac,"By a method known from finite-element theory, bivariate quintic interpolation polynomials can be determined over triangles (18-degree-of-freedom element with value, slope, and curvature parameters in the nodes). The connections to neighboring polynomials are continuous and smooth. A new method is discussed for computing contour lines directly from the polynomial coefficients found for each triangle by solving a nonlinear equation for every point of a line. The lines are computed triangle by triangle so that, in principle, true parallel operation on all triangles is possible. The method is superior in methodical and operational aspects over the traditional way where values are computed over a rectangular grid prior to contour plotting.",,COMPUTER PROGRAMMING - Algorithms; BIVARIATE INTERPOLATION; COMPUTATIONAL GEOMETRY; CONTOUR PLOTTING; FINITE ELEMENTS; QUINTIC POLYNOMIAL EQUATIONS; MATHEMATICAL TECHNIQUES
ASYNCHRONIC PROCESSES - 1. DEFINITION AND INTERPRETATION.,1981,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0019679707&partnerID=40&md5=a51d77953e98c869b1e3e306406e9870,A formalized concept of an asynchronic process is introduced. An interpretation of finite automata and Petri nets is given in the framework of the model proposed. An interpretation is presented also for a number of other mathematical models of representation of the operation of discrete devices. A description of the mechanism of renewal of an asynchronic process is formalized. An operation of reduction of asynchronic processes is proposed. It is pointed out that the models treated can be used when one is describing joint matched functioning of discrete devices.,,Automata theory
E1.,1978,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0017982276&doi=10.1145%2f355780.355787&partnerID=40&md5=86ba50ce12802fc2abc9431c0c22e4bb,"This algorithm describes describes the IDBBVIP/IDSFFT subprogram package that implements the method of bivariate interpolation and smooth surface fitting for irregularly distributed data points. The package is written in ANSI Standard Fortran. The package consists of nine subprograms, i. e. eight subroutines and a function. Two subroutines, IDBVIP and IDSFFT, are the master subroutines of the package, and each interfaces with the user. The IDBVIP subroutine performs bivariate interpolation; it estimates the z values at the specified points in the x-y plane. The IDSFFT subroutine performs smooth surface fitting; it estimates the z values at the specified rectangular grid points in the x-y plane and generates a doubly-dimensioned array containing these estimated values.",,Computer programming
PARALLEL ALGORITHMS FOR ADAPATIVE QUADRATURE - 3. PROGRAM CORRECTNESS.,1976,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0016928796&doi=10.1145%2f355666.355667&partnerID=40&md5=84911c9d8bfa06089ed60a75398f1f99,"The primary aim is to study the rate of convergence achieved by parallel algorithms. It is proved that a specific algorithm (computer program) achieves the optimal rate of convergence which has been established by the previous papers, and that this program terminates with a quadrature estimate within the prescribed input accuracy requirement. This estimate is computed with a number of integrand evaluations of a specified order as the accuracy requirement goes to zero.",,MATHEMATICAL TECHNIQUES - Convergence of Numerical Methods; Computer programming
C5.,1976,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0016931444&doi=10.1145%2f355666.355675&partnerID=40&md5=1b5955c9075e4fa886b8db08e10d30ae,"The Fortran algorithm DERPAR is a subroutine suitable for evaluation of the dependence of the solution of a system of equations on a parameter. The modified method of Davidenko, which applies the implicit function theorem, is used in combination with the Newton method and Adams integration formulas.",,MATHEMATICAL TECHNIQUES - Nonlinear Equations; Computer programming
Automatic Differentiation of C++ Codes on Emerging Manycore Architectures with Sacado,2022,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151560223&doi=10.1145%2f3560262&partnerID=40&md5=1ebf1ad27a47c020864ee91a9d85e469,"Automatic differentiation (AD) is a well-known technique for evaluating analytic derivatives of calculations implemented on a computer, with numerous software tools available for incorporating AD technology into complex applications. However, a growing challenge for AD is the efficient differentiation of parallel computations implemented on emerging manycore computing architectures such as multicore CPUs, GPUs, and accelerators as these devices become more pervasive. In this work, we explore forward mode, operator overloading-based differentiation of C++ codes on these architectures using the widely available Sacado AD software package. In particular, we leverage Kokkos, a C++ tool providing APIs for implementing parallel computations that is portable to a wide variety of emerging architectures. We describe the challenges that arise when differentiating code for these architectures using Kokkos, and two approaches for overcoming them that ensure optimal memory access patterns as well as expose additional dimensions of fine-grained parallelism in the derivative calculation. We describe the results of several computational experiments that demonstrate the performance of the approach on a few contemporary CPU and GPU architectures. We then conclude with applications of these techniques to the simulation of discretized systems of partial differential equations. © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Automatic differentiation; CUDA; GPU; manycore; multicore; OpenMP; threads,Application programming interfaces (API); Application programs; C++ (programming language); Codes (symbols); Computer aided software engineering; Memory architecture; Optimal systems; Parallel architectures; Program processors; Automatic differentiations; C++ codes; CUDA; Many-core; Many-core architecture; Multi-cores; Openmp; Parallel Computation; Software-tools; Thread; Graphics processing unit
DIRECTGO: A New DIRECT-Type MATLAB Toolbox for Derivative-Free Global Optimization,2022,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138510611&doi=10.1145%2f3559755&partnerID=40&md5=155ae2c794e0218f2604b9d71e3561a6,"In this work, we introduce DIRECTGO, a new MATLAB toolbox for derivative-free global optimization. DIRECTGO collects various deterministic derivative-free DIRECT-type algorithms for box-constrained, generally constrained, and problems with hidden constraints. Each sequential algorithm is implemented in two ways: using static and dynamic data structures for more efficient information storage and organization. Furthermore, parallel schemes are applied to some promising algorithms within DIRECTGO. The toolbox is equipped with a graphical user interface (GUI), ensuring the user-friendly use of all functionalities available in DIRECTGO. Available features are demonstrated in detailed computational studies using a comprehensive DIRECTGOLib v1.0 library of global optimization test problems. Additionally, 11 classical engineering design problems illustrate the potential of DIRECTGO to solve challenging real-world problems. Finally, the appendix gives examples of accompanying MATLAB programs and provides a synopsis of its use on the test problems with box and general constraints. © 2022 Association for Computing Machinery.",benchmarking; derivative-free optimization; DIRECT-type algorithms; Global optimization; MATLAB; optimization software; TOMLAB,Digital storage; Graphical user interfaces; MATLAB; Derivative-free; Derivative-free optimization; Deterministics; DIRECT-type algorithm; Global optimisation; Hidden constraints; Optimization software; Sequential algorithm; TOMLAB; Two ways; Global optimization
Remark on Algorithm 1010: Boosting Efficiency in Solving Quartic Equations with No Compromise in Accuracy,2022,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151501161&doi=10.1145%2f3564270&partnerID=40&md5=3757451c8a0be4ff384fcb557833ad99,We present a correction and an improvement to Algorithm 1010 [A. Orellana and C. De Michele 2020]. © 2022 Copyright held by the owner/author(s).,factorization into quadratics; Newton-Raphson scheme; numerical solver design; performance; Quartic equation,Boosting efficiency; Factorization into quadratic; Newton Raphson Scheme; Numerical solv design; Numerical solvers; Performance; Quartic equation
Waveform Relaxation with Asynchronous Time-integration,2022,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151556116&doi=10.1145%2f3569578&partnerID=40&md5=e263e1d7cd3e950933d36a627bea9418,"We consider Waveform Relaxation (WR) methods for parallel and partitioned time-integration of surface-coupled multiphysics problems. WR allows independent time-discretizations on independent and adaptive time-grids, while maintaining high time-integration orders. Classical WR methods such as Jacobi or Gauss-Seidel WR are typically either parallel or converge quickly.We present a novel parallel WR method utilizing asynchronous communication techniques to get both properties. Classical WR methods exchange discrete functions after time-integration of a subproblem. We instead asynchronously exchange time-point solutions during time-integration and directly incorporate all new information in the interpolants. We show both continuous and time-discrete convergence in a framework that generalizes existing linear WR convergence theory. An algorithm for choosing optimal relaxation in our new WR method is presented. Convergence is demonstrated in two conjugate heat transfer examples. Our new method shows an improved performance over classical WR methods. In one example, we show a partitioned coupling of the compressible Euler equations with a nonlinear heat equation, with subproblems implemented using the open source libraries DUNE and FEniCS. © 2022 Association for Computing Machinery.",Asynchronous iteration; coupled problems; dynamic iteration; thermal fluid-structure interaction; waveform relaxation,Computation theory; Fluid structure interaction; Heat transfer; Integration; Nonlinear equations; Asynchronous iterations; Coupled problems; Dynamic iteration; Multiphysics problems; Sub-problems; Thermal fluid structure interaction; Time discretization; Time-integration; Waveform relaxation; Waveform relaxation method; Iterative methods
Cache-oblivious Hilbert Curve-based Blocking Scheme for Matrix Transposition,2022,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151550089&doi=10.1145%2f3555353&partnerID=40&md5=232d12548a999c23de7c45199e184f0f,"This article presents a fast SIMD Hilbert space-filling curve generator, which supports a new cache-oblivious blocking-scheme technique applied to the out-of-place transposition of general matrices. Matrix operations found in high performance computing libraries are usually parameterized based on host microprocessor specifications to minimize data movement within the different levels of memory hierarchy. The performance of cache-oblivious algorithms does not rely on such parameterizations. This type of algorithm provides an elegant and portable solution to address the lack of standardization in modern-day processors. Our solution consists in an iterative blocking scheme that takes advantage of the locality-preserving properties of Hilbert space-filling curves to minimize data movement in any memory hierarchy. This scheme traverses the input matrix, in O(nm) time and space, improving the behavior of matrix algorithms that inherently present poor memory locality. The application of this technique to the problem of out-of-place matrix transposition achieved competitive results when compared to state-of-the-art approaches. The performance of our solution surpassed Intel MKL version after employing standard software prefetching techniques. © 2022 Copyright held by the owner/author(s).",Cache-oblivious algorithms; dense matrix; Hilbert space-filling curve; out-of-place transposition,Cache memory; Filling; Hilbert spaces; Matrix algebra; Memory architecture; Parameter estimation; Parameterization; Vector spaces; Blockings; Cache-oblivious; Cache-oblivious algorithms; Data movements; Dense matrix; Hilbert space filling curves; Matrix transposition; Memory hierarchy; Out-of-place transposition; Performance; Iterative methods
"pylspack: Parallel Algorithms and Data Structures for Sketching, Column Subset Selection, Regression, and Leverage Scores",2022,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151489918&doi=10.1145%2f3555370&partnerID=40&md5=8230fe50af0fd636bcbd1c67a22ba633,"We present parallel algorithms and data structures for three fundamental operations in Numerical Linear Algebra: (i) Gaussian and CountSketch random projections and their combination, (ii) computation of the Gram matrix, and (iii) computation of the squared row norms of the product of two matrices, with a special focus on ""tall-and-skinny""matrices, which arise in many applications. We provide a detailed analysis of the ubiquitous CountSketch transform and its combination with Gaussian random projections, accounting for memory requirements, computational complexity and workload balancing. We also demonstrate how these results can be applied to column subset selection, least squares regression and leverage scores computation. These tools have been implemented in pylspack, a publicly available Python package1 whose core is written in C++ and parallelized with OpenMP and that is compatible with standard matrix data structures of SciPy and NumPy. Extensive numerical experiments indicate that the proposed algorithms scale well and significantly outperform existing libraries for tall-and-skinny matrices. © 2022 Association for Computing Machinery.",column subset selection; Parallel algorithms; preconditioning; regression; sketching; sparse data structures; statistical leverage scores,Application programming interfaces (API); C++ (programming language); Data structures; Genetic algorithms; Parallel algorithms; Python; Regression analysis; Algorithms and data structures; Column subset selection; Gaussians; matrix; Preconditioning; Regression; Sketchings; Sparse data structures; Statistical leverage score; Subset selection; Matrix algebra
Algorithm 1030: SC-SR1: MATLAB Software for Limited-memory SR1 Trust-region Methods,2022,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147667581&doi=10.1145%2f3550269&partnerID=40&md5=fff3e2fd62553ac51f415416a382b8e8,"We present a MATLAB implementation of the symmetric rank-one (SC-SR1) method that solves trust-region subproblems when a limited-memory symmetric rank-one (L-SR1) matrix is used in place of the true Hessian matrix, which can be used for large-scale optimization. The method takes advantage of two shape-changing norms [Burdakov and Yuan 2002; Burdakov et al. 2017] to decompose the trust-region subproblem into two separate problems. Using one of the proposed norms, the resulting subproblems have closed-form solutions. Meanwhile, using the other proposed norm, one of the resulting subproblems has a closed-form solution while the other is easily solvable using techniques that exploit the structure of L-SR1 matrices. Numerical results suggest that the SC-SR1 method is able to solve trust-region subproblems to high accuracy even in the so-called ""hard case.""When integrated into a trust-region algorithm, extensive numerical experiments suggest that the proposed algorithms perform well, when compared with widely used solvers, such as truncated conjugate-gradients. © 2022 Association for Computing Machinery.",Large-scale unconstrained optimization; limited-memory quasi-Newton methods; shape-changing norm; symmetric rank-one update; trust-region methods,Matrix algebra; Newton-Raphson method; Numerical methods; Shape optimization; Large scale unconstrained optimizations; Limited memory; Limited memory quasi newtons; Limited-memory quasi-newton method; Quasi-Newton methods; Shape-changing norm; Symmetric rank-one update; Symmetrics; Trust region subproblem; Trust-region methods; MATLAB
CS-TSSOS: Correlative and Term Sparsity for Large-Scale Polynomial Optimization,2022,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85148278874&doi=10.1145%2f3569709&partnerID=40&md5=3bf3b32c094afbe1ba97c3d6222cf570,"This work proposes a new moment-SOS hierarchy, called CS-TSSOS, for solving large-scale sparse polynomial optimization problems. Its novelty is to exploit simultaneously correlative sparsity and term sparsity by combining advantages of two existing frameworks for sparse polynomial optimization. The former is due to Waki et al. [40] while the latter was initially proposed by Wang et al. [42] and later exploited in the TSSOS hierarchy [46, 47]. In doing so we obtain CS-TSSOS - a two-level hierarchy of semidefinite programming relaxations with (i) the crucial property to involve blocks of SDP matrices and (ii) the guarantee of convergence to the global optimum under certain conditions. We demonstrate its efficiency and scalability on several large-scale instances of the celebrated Max-Cut problem and the important industrial optimal power flow problem, involving up to six thousand variables and tens of thousands of constraints. © 2022 Association for Computing Machinery.",correlative sparsity; large-scale polynomial optimization; Lasserre's hierarchy; Moment-SOS hierarchy; optimal power flow; term sparsity; TSSOS,Acoustic generators; Electric load flow; Optimization; Correlative sparsity; Large-scale polynomial optimization; Large-scales; Lasserre hierarchy; Moment-SOS hierarchy; Optimal power flows; Polynomial optimization; Sparse polynomials; Term sparsity; TSSOS; Polynomials
A Normal Form Algorithm for Tensor Rank Decomposition,2022,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151538984&doi=10.1145%2f3555369&partnerID=40&md5=d00f761edae6879714286a60ec53e38f,"We propose a new numerical algorithm for computing the tensor rank decomposition or canonical polyadic decomposition of higher-order tensors subject to a rank and genericity constraint. Reformulating this computational problem as a system of polynomial equations allows us to leverage recent numerical linear algebra tools from computational algebraic geometry. We characterize the complexity of our algorithm in terms of an algebraic property of this polynomial system - the multigraded regularity. We prove effective bounds for many tensor formats and ranks, which are of independent interest for overconstrained polynomial system solving. Moreover, we conjecture a general formula for the multigraded regularity, yielding a (parameterized) polynomial time complexity for the tensor rank decomposition problem in the considered setting. Our numerical experiments show that our algorithm can outperform state-of-the-art numerical algorithms by an order of magnitude in terms of accuracy, computation time, and memory consumption. © 2022 Association for Computing Machinery.",canonical polyadic decomposition; normal form algorithms; polynomial systems; Tensor rank decomposition,Computational complexity; Computational geometry; Polynomial approximation; Canonical polyadic decompositions; Computational problem; Genericity; Higher-order tensor; Normal form; Normal form algorithm; Numerical algorithms; Polynomial systems; Tensor rank decomposition; Tensor ranks; Tensors
Exploiting Constant Trace Property in Large-scale Polynomial Optimization,2022,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149667687&doi=10.1145%2f3555309&partnerID=40&md5=c28ba9cbf895d1f0aa1feee2159cf7c3,"We prove that every semidefinite moment relaxation of a polynomial optimization problem (POP) with a ball constraint can be reformulated as a semidefinite program involving a matrix with constant trace property (CTP). As a result, such moment relaxations can be solved efficiently by first-order methods that exploit CTP, e.g., the conditional gradient-based augmented Lagrangian method. We also extend this CTP-exploiting framework to large-scale POPs with different sparsity structures. The efficiency and scalability of our framework are illustrated on some moment relaxations for various randomly generated POPs, especially second-order moment relaxations for quadratically constrained quadratic programs. © 2022 Association for Computing Machinery.",conditional gradient-based augmented Lagrangian; constant trace property; moment-SOS hierarchy; Polynomial optimization,Constrained optimization; Lagrange multipliers; Polynomials; Augmented Lagrangians; Conditional gradient; Conditional gradient-based augmented lagrangian; Constant trace property; Gradient based; Large-scales; Moment-SOS hierarchy; Polynomial optimization; Polynomial optimization problem; Property; Quadratic programming
QPPAL: A Two-phase Proximal Augmented Lagrangian Method for High-dimensional Convex Quadratic Programming Problems,2022,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142429146&doi=10.1145%2f3476571&partnerID=40&md5=af0411e77c5c03a3b8e625b26dd6b9b4,"In this article, we aim to solve high-dimensional convex quadratic programming (QP) problems with a large number of quadratic terms, linear equality, and inequality constraints. To solve the targeted QP problem to a desired accuracy efficiently, we consider the restricted-Wolfe dual problem and develop a two-phase Proximal Augmented Lagrangian method (QPPAL), with Phase I to generate a reasonably good initial point to warm start Phase II to obtain an accurate solution efficiently. More specifically, in Phase I, based on the recently developed symmetric Gauss-Seidel (sGS) decomposition technique, we design a novel sGS-based semi-proximal augmented Lagrangian method for the purpose of finding a solution of low to medium accuracy. Then, in Phase II, a proximal augmented Lagrangian algorithm is proposed to obtain a more accurate solution efficiently. Extensive numerical results evaluating the performance of QPPAL against existing state-of-the-art solvers Gurobi, OSQP, and QPALM are presented to demonstrate the high efficiency and robustness of our proposed algorithm for solving various classes of large-scale convex QP problems. The MATLAB implementation of the software package QPPAL is available at https://blog.nus.edu.sg/mattohkc/softwares/qppal/.  © 2022 Association for Computing Machinery.",augmented Lagrangian method; Convex quadratic programming; restricted-Wolfe dual; symmetric Gauss-Seidel,Constrained optimization; Lagrange multipliers; Quadratic programming; Augmented Lagrangian methods; Convex quadratic programming; Gauss-Seidel; High-dimensional; Higher-dimensional; Quadratic programming problems; Restricted-wolfe dual; Symmetric gauss-seidel; Symmetrics; Two phase; MATLAB
Toward Accurate and Fast Summation,2022,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142417081&doi=10.1145%2f3544488&partnerID=40&md5=06508ab58dd6db2f7ea86b5ce1b3d7b1,"We introduce a new accurate summation algorithm based on the error-free summation into floating-point buckets. Our algorithm exploits ideas from Zhu and Hayes' OnlineExactSum, but it uses a significantly smaller number of accumulators and has a better instruction-level parallelism. In the default setting, our implementation aaaSum returns a faithfully rounded floating-point approximation of the true sum. We also discuss possible modifications for the computation of reproducible, correctly rounded, and multiple precision floating-point approximations. The computational overhead for any of these modifications is kept comparably small. Numerical tests demonstrate that aaaSum performs well for very small to large problem sizes, independent of the condition number of the problem. We compare our algorithm with other accurate and high-precision summation approaches.  © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Floating-point summation; rounding error,Number theory; Accurate summation; Computational overheads; Default setting; Fast summation; Floating points; Floating-point summation; Instruction level parallelism; Precision floating point; Rounding errors; Summation algorithm; Digital arithmetic
The Linear Algebra Mapping Problem. Current State of Linear Algebra Languages and Libraries,2022,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138262428&doi=10.1145%2f3549935&partnerID=40&md5=1366c3ead8a54e8ace0ec6b9b28756e0,"We observe a disconnect between developers and end-users of linear algebra libraries. On the one hand, developers invest significant effort in creating sophisticated numerical kernels. On the other hand, end-users are progressively less likely to go through the time consuming process of directly using said kernels; instead, languages and libraries, which offer a higher level of abstraction, are becoming increasingly popular. These languages offer mechanisms that internally map the input program to lower level kernels. Unfortunately, our experience suggests that, in terms of performance, this translation is typically suboptimal.In this paper, we define the problem of mapping a linear algebra expression to a set of available building blocks as the ""Linear Algebra Mapping Problem""(LAMP); we discuss its NP-complete nature, and investigate how effectively a benchmark of test problems is solved by popular high-level programming languages and libraries. Specifically, we consider Matlab, Octave, Julia, R, Armadillo (C++), Eigen (C++), and NumPy (Python); the benchmark is meant to test both compiler optimizations, as well as linear algebra specific optimizations, such as the optimal parenthesization of matrix products. The aim of this study is to facilitate the development of languages and libraries that support linear algebra computations.  © 2022 Association for Computing Machinery.",compilers; domain specific languages; LAMP; linear algebra; linear algebra mapping problem,"C++ (programming language); Libraries; Linear algebra; Mapping; MATLAB; Problem oriented languages; Program compilers; 'current; Compiler; Domains specific languages; End-users; High level of abstraction; Linear algebra libraries; Linear algebra mapping problem; Linear algebra mapping problem""; Mapping problem; Optimization"
Parallel QR Factorization of Block Low-rank Matrices,2022,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142418732&doi=10.1145%2f3538647&partnerID=40&md5=5d33eaa13db28b33b161ad8d99c7ed68,"We present two new algorithms for Householder QR factorization of Block Low-Rank (BLR) matrices: one that performs block-column-wise QR and another that is based on tiled QR. We show how the block-column-wise algorithm exploits BLR structure to achieve arithmetic complexity of (mn), while the tiled BLR-QR exhibits (mn1.5 complexity. However, the tiled BLR-QR has finer task granularity that allows parallel task-based execution on shared memory systems. We compare the block-column-wise BLR-QR using fork-join parallelism with tiled BLR-QR using task-based parallelism. We also compare these two implementations of Householder BLR-QR with a block-column-wise Modified Gram-Schmidt (MGS) BLR-QR using fork-join parallelism and a state-of-the-art vendor-optimized dense Householder QR in Intel MKL. For a matrix of size 131k × 65k, all BLR methods are more than an order of magnitude faster than the dense QR in MKL. Our methods are also robust to ill conditioning and produce better orthogonal factors than the existing MGS-based method. On a CPU with 64 cores, our parallel tiled Householder and block-column-wise Householder algorithms show a speedup of 50 and 37 times, respectively.  © 2022 Association for Computing Machinery.",Block low-rank matrix; householder reflections; QR factorization; task-based execution,Factorization; Block columns; Block low-rank matrix; Column-wise algorithms; Gram-schmidt; Householder reflections; Low-rank matrices; QR factorizations; Rank structure; Task-based; Task-based execution; Matrix algebra
Enabling New Flexibility in the SUNDIALS Suite of Nonlinear and Differential/Algebraic Equation Solvers,2022,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142194361&doi=10.1145%2f3539801&partnerID=40&md5=98fe5bf862da053032bf8311c3932d09,"In recent years, the SUite of Nonlinear and DIfferential/ALgebraic equation Solvers (SUNDIALS) has been redesigned to better enable the use of application-specific and third-party algebraic solvers and data structures. Throughout this work, we have adhered to specific guiding principles that minimized the impact to current users while providing maximum flexibility for later evolution of solvers and data structures. The redesign was done through the addition of new linear and nonlinear solvers classes, enhancements to the vector class, and the creation of modern Fortran interfaces. The vast majority of this work has been performed ""behind-the-scenes,""with minimal changes to the user interface and no reduction in solver capabilities or performance. These changes allow SUNDIALS users to more easily utilize external solver libraries and create highly customized solvers, enabling greater flexibility on extreme-scale, heterogeneous computational architectures.  © 2022 Association for Computing Machinery.",high-performance computing; nonlinear solvers; Numerical software; object-oriented design; time integration,Data structures; Differential equations; Nonlinear equations; Object oriented programming; Application specific; Differential/algebraic equations; Equation solvers; High-performance computing; Non-linear solver; Nonlinear algebraic equations; Numerical software; Object-oriented design; Performance computing; Time-integration; User interfaces
HIFIR: Hybrid Incomplete Factorization with Iterative Refinement for Preconditioning Ill-Conditioned and Singular Systems,2022,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142434435&doi=10.1145%2f3536165&partnerID=40&md5=5582d462be68018a6a84f4d096ee24d9,"We introduce a software package called Hybrid Incomplete Factorization with Iterative Refinement (HIFIR) for preconditioning sparse, unsymmetric, ill-conditioned, and potentially singular systems. HIFIR computes a hybrid incomplete factorization (HIF), which combines multilevel incomplete LU factorization with a truncated, rank-revealing QR (RRQR) factorization on the final Schur complement. This novel hybridization is based on the new theory of μ-accurate approximate generalized inverse (AGI). It enables near-optimal preconditioners for consistent systems and enables flexible GMRES to solve inconsistent systems when coupled with iterative refinement. In this article, we focus on some practical algorithmic and software issues of HIFIR. In particular, we introduce a new inverse-based rook pivoting (IBRP) into ILU, which improves the robustness and the overall efficiency for some ill-conditioned systems by significantly reducing the size of the final Schur complement for some systems. We also describe the software design of HIFIR in terms of its efficient data structures for supporting rook pivoting in a multilevel setting, its template-based generic programming interfaces for mixed-precision real and complex values in C++, and its user-friendly high-level interfaces in MATLAB and Python. We demonstrate the effectiveness of HIFIR for ill-conditioned or singular systems arising from several applications, including the Helmholtz equation, linear elasticity, stationary incompressible Navier-Stokes (INS) equations, and time-dependent advection-diffusion equation.  © 2022 Association for Computing Machinery.",approximate generalized inverse; hybrid incomplete factorization; iterative refinement; multilevel ILU factorization; Preconditioning; rank-revealing factorization; singular systems,Advection; C++ (programming language); Iterative methods; Lower-upper decomposition; MATLAB; Navier Stokes equations; Python; Software design; Approximate generalized inverse; Generalized inverse; Hybrid incomplete factorization; ILU factorization; Incomplete factorization; Iterative refinement; Multilevel ILU factorization; Multilevels; Preconditioning; Rank-revealing; Rank-revealing factorization; Singular system; Factorization
Robust level-3 BLAS Inverse Iteration from the Hessenberg Matrix,2022,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142424944&doi=10.1145%2f3544789&partnerID=40&md5=8dd281e6a95a11021a34ba5822361441,"Inverse iteration is known to be an effective method for computing eigenvectors corresponding to simple and well-separated eigenvalues. In the non-symmetric case, the solution of shifted Hessenberg systems is a central step. Existing inverse iteration solvers approach the solution of the shifted Hessenberg systems with either RQ or LU factorizations and, once factored, solve the corresponding systems. This approach has limited level-3 BLAS potential since distinct shifts have distinct factorizations. This paper rearranges the RQ approach such that data shared between distinct shifts can be exploited. Thereby the backward substitution with the triangular R factor can be expressed mostly with matrix-matrix multiplications (level-3 BLAS). The resulting algorithm computes eigenvectors in a tiled, overflow-free, and task-parallel fashion. The numerical experiments show that the new algorithm outperforms existing inverse iteration solvers for the computation of both real and complex eigenvectors.  © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Inverse iteration; overflow-free computation; shifted Hessenberg systems,Factorization; Inverse problems; Iterative methods; Lower-upper decomposition; Eigen-value; Hessenberg matrix; Hessenberg systems; Inverse iteration; Level-3 BLAS; LU factorization; Overflow-free computation; Shifted hessenberg system; Simple++; Symmetric case; Eigenvalues and eigenfunctions
Algorithm 1028: VTMOP: Solver for Blackbox Multiobjective Optimization Problems,2022,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132139556&doi=10.1145%2f3529258&partnerID=40&md5=34fb9315e1497eb4cfe56682885fba3b,"VTMOP is a Fortran 2008 software package containing two Fortran modules for solving computationally expensive bound-constrained blackbox multiobjective optimization problems. VTMOP implements the algorithm of [32], which handles two or more objectives, does not require any derivatives, and produces well-distributed points over the Pareto front. The first module contains a general framework for solving multiobjective optimization problems by combining response surface methodology, trust region methodology, and an adaptive weighting scheme. The second module features a driver subroutine that implements this framework when the objective functions can be wrapped as a Fortran subroutine. Support is provided for both serial and parallel execution paradigms, and VTMOP is demonstrated on several test problems as well as one real-world problem in the area of particle accelerator optimization.  © 2022 Association for Computing Machinery.",adaptive weighting schemes; blackbox optimization; Multiobjective optimization; response surface methodology; trust region methods,Constrained optimization; FORTRAN (programming language); Surface properties; Adaptive weighting; Adaptive weighting scheme; Black boxes; Black-box optimization; Multi-objectives optimization; Multiobjective optimization problems; Pareto front; Response-surface methodology; Trust-region methods; Weighting scheme; Multiobjective optimization
Configurable Open-source Data Structure for Distributed Conforming Unstructured Homogeneous Meshes with GPU Support,2022,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142412558&doi=10.1145%2f3536164&partnerID=40&md5=d5ecfe8c3d17707c86eb0953c1eb2499,"A general multi-purpose data structure for an efficient representation of conforming unstructured homogeneous meshes for scientific computations on CPU and GPU-based systems is presented. The data structure is provided as open-source software as part of the TNL library (https://tnl-project.org/). The abstract representation supports almost any cell shape and common 2D quadrilateral, 3D hexahedron and arbitrarily dimensional simplex shapes are currently built into the library. The implementation is highly configurable via templates of the C++ language, which allows avoiding the storage of unnecessary dynamic data. The internal memory layout is based on state-of-the-art sparse matrix storage formats, which are optimized for different hardware architectures in order to provide high-performance computations. The proposed data structure is also suitable for meshes decomposed into several subdomains and distributed computing using the Message Passing Interface (MPI). The efficiency of the implemented data structure on CPU and GPU hardware architectures is demonstrated on several benchmark problems and a comparison with another library. Its applicability to advanced numerical methods is demonstrated with an example problem of two-phase flow in porous media using a numerical scheme based on the mixed-hybrid finite element method (MHFEM). We show GPU speed-ups that rise above 20 in 2D and 50 in 3D when compared to sequential CPU computations, and above 2 in 2D and 9 in 3D when compared to 12-threaded CPU computations.  © 2022 Copyright held by the owner/author(s).",data structure; GPGPU; MPI; performance evaluation; Unstructured mesh,Application programming interfaces (API); C++ (programming language); Digital storage; Graphics processing unit; Memory architecture; Message passing; Numerical methods; Open source software; Open systems; Porous materials; Program processors; Two phase flow; GPGPU; Hardware architecture; Message passing interface; Message-passing; Multi-purpose; Open source datum; Open-source softwares; Performances evaluation; Scientific computation; Unstructured meshes; Data structures
Algorithm 1027: NOMAD Version 4: Nonlinear Optimization with the MADS Algorithm,2022,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139510058&doi=10.1145%2f3544489&partnerID=40&md5=3604bbf4b564b7701eab5aa0f4aecb69,"NOMADis a state-of-the-art software package for optimizing blackbox problems. In continuous development since 2001, it constantly evolved with the integration of new algorithmic features published in scientific publications. These features are motivated by real applications encountered by industrial partners. The latest major release of NOMAD, version 3, dates to 2008. Minor releases are produced as new features are incorporated. The present work describes NOMAD 4, a complete redesign of the previous version, with a new architecture providing more flexible code, added functionalities, and reusable code. We introduce algorithmic components, which are building blocks for more complex algorithms and can initiate other components, launch nested algorithms, or perform specialized tasks. They facilitate the implementation of new ideas, including the MegaSearchPoll component, warm and hot restarts, and a revised version of the PsdMads algorithm. Another main improvement of NOMAD 4 is the usage of parallelism, to simultaneously compute multiple blackbox evaluations and to maximize usage of available cores. Running different algorithms, tuning their parameters, and comparing their performance for optimization are simpler than before, while overall optimization performance is maintained between versions 3 and 4. NOMAD is freely available at www.gerad.ca/nomad and the whole project is visible at github.com/bbopt/nomad.  © 2022 Association for Computing Machinery.",blackbox optimization; derivative-free optimization; mesh adaptive direct search; Optimization software,Codes (symbols); Computer programming; Algorithmics; Black boxes; Black-box optimization; Derivative-free optimization; Mesh adaptive direct search; Non-linear optimization; Optimisations; Optimization software; Performance; State of the art; Nonlinear programming
Algorithm 1026: Concurrent Alternating Least Squares for Multiple Simultaneous Canonical Polyadic Decompositions,2022,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140090648&doi=10.1145%2f3519383&partnerID=40&md5=d4fb3c317217caab82ea25a244546d41,"Tensor decompositions, such as CANDECOMP/PARAFAC (CP), are widely used in a variety of applications, such as chemometrics, signal processing, and machine learning. A broadly used method for computing such decompositions relies on the Alternating Least Squares (ALS) algorithm. When the number of components is small, regardless of its implementation, ALS exhibits low arithmetic intensity, which severely hinders its performance and makes GPU offloading ineffective. We observe that, in practice, experts often have to compute multiple decompositions of the same tensor, each with a small number of components (typically fewer than 20), to ultimately find the best ones to use for the application at hand. In this article, we illustrate how multiple decompositions of the same tensor can be fused together at the algorithmic level to increase the arithmetic intensity. Therefore, it becomes possible to make efficient use of GPUs for further speedups; at the same time, the technique is compatible with many enhancements typically used in ALS, such as line search, extrapolation, and non-negativity constraints. We introduce the Concurrent ALS algorithm and library, which offers an interface to MATLAB, and a mechanism to effectively deal with the issue that decompositions complete at different times. Experimental results on artificial and real datasets demonstrate a shorter time to completion due to increased arithmetic intensity.  © 2022 Association for Computing Machinery.",CP; decomposition; high-performance; PARAFAC; Tensor,Least squares approximations; MATLAB; Program processors; Signal processing; Alternating least squares; CANDECOMP/PARAFAC; Canonical polyadic decompositions; High-performance; LeastSquare algorithm; Multiple decomposition; Number of components; PARAFAC; Performance; Tensor decomposition; Tensors
Parallel Weighted Random Sampling,2022,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142432628&doi=10.1145%2f3549934&partnerID=40&md5=b771039424d7de8718b8c0dac15d4eeb,"Data structures for efficient sampling from a set of weighted items are an important building block of many applications. However, few parallel solutions are known. We close many of these gaps. We give efficient, fast, and practicable parallel and distributed algorithms for building data structures that support sampling single items (alias tables, compressed data structures). This also yields a simplified and more space-efficient sequential algorithm for alias table construction. Our approaches to sampling k out of n items with/without replacement and to subset (Poisson) sampling are output-sensitive, i.e., the sampling algorithms use work linear in the number of different samples. This is also interesting in the sequential case. Weighted random permutation can be done by sorting appropriate random deviates. We show that this is possible with linear work. Finally, we give a communication-efficient, highly scalable approach to (weighted and unweighted) reservoir sampling. This algorithm is based on a fully distributed model of streaming algorithms that might be of independent interest. Experiments for alias tables and sampling with replacement show near linear speedups using up to 158 threads of shared-memory machines. An experimental evaluation of distributed weighted reservoir sampling on up to 5,120 cores also shows good speedups.  © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",alias method; Categorical distribution; communication efficient algorithm; multinoulli distribution; parallel algorithm; Poisson sampling; PRAM; reservoir sampling,Clustering algorithms; Data structures; Poisson distribution; Sampling; Alias method; Building blockes; Categorical distribution; Communication efficient algorithm; Efficient sampling; Multinoulli distribution; Poisson sampling; PRAM; Random sampling; Reservoir samplings; Parallel algorithms
A Provably Robust Algorithm for Triangle-triangle Intersections in Floating-point Arithmetic,2022,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133680145&doi=10.1145%2f3513264&partnerID=40&md5=9ec3105b818f37c631869dca22743cab,"Motivated by the unexpected failure of the triangle intersection component of the Projection Algorithm for Nonmatching Grids (PANG), this article provides a robust version with proof of backward stability. The new triangle intersection algorithm ensures consistency and parsimony across three types of calculations. The set of intersections produced by the algorithm, called representations, is shown to match the set of geometric intersections, called models. The article concludes with a comparison between the old and new intersection algorithms for PANG using an example found to reliably generate failures in the former.  © 2022 held by the owner/author(s).",advancing front algorithms; floating-point arithmetic; Mesh intersection; non-matching grids; polygon clipping; robustness,Interactive computer graphics; Advancing front; Advancing front algorithm; Floating-point arithmetic; Intersection algorithms; Mesh intersection; Non-matching grid; Polygon Clipping; Projection algorithms; Robust algorithm; Robustness; Digital arithmetic
Algorithm 1023: Restoration of Function by Integrals with Cubic Integral Smoothing Spline in R,2022,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132431813&doi=10.1145%2f3519384&partnerID=40&md5=74239470cff0e724e86aca9c5896294f,"In this paper, a cubic integral smoothing spline with roughness penalty for restoring a function by integrals is described. A mathematical method for building such a spline is described in detail. The method is based on cubic integral spline with a penalty function, which minimizes the sum of squares of the difference between the observed integrals of the unknown function and the integrals of the spline being constructed, plus an additional penalty for the nonlinearity (roughness) of the spline. This method has a matrix form, and this paper shows in detail how to fill in each matrix. The parameter governs the desired smoothness of the restored function. Spline knots can be chosen independently of observations, and a weight can be defined for each observation for more control over the resulting spline shape. An implementation in the R language as function int-spline is given. The function int-spline is easy to use, with all arguments completely described and corresponding examples given. An example of the application of the method in rare event analysis and forecasting is given.  © 2022 Association for Computing Machinery.",capacity method; int-spline; integral spline; nonlinearity penalty; R; rare events; Recovery; regression; smoothing spline; spline; spline collocation,Integral equations; Splines; Capacity method; Int-spline; Integral spline; Nonlinearity penalty; R; Rare event; Regression; Roughness penalty; Smoothing spline; Spline collocation; Restoration
"Algorithm 1021: SPEX Left LU, Exactly Solving Sparse Linear Systems via a Sparse Left-looking Integer-preserving LU Factorization",2022,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134878996&doi=10.1145%2f3519024&partnerID=40&md5=1b94402c9475f209834bc1572b2206cb,"SPEX Left LU is a software package for exactly solving unsymmetric sparse linear systems. As a component of the sparse exact (SPEX) software package, SPEX Left LU can be applied to any input matrix, A, whose entries are integral, rational, or decimal, and provides a solution to the system , which is either exact or accurate to user-specified precision. SPEX Left LU preorders the matrix A with a user-specified fill-reducing ordering and computes a left-looking LU factorization with the special property that each operation used to compute the L and U matrices is integral. Notable additional applications of this package include benchmarking the stability and accuracy of state-of-the-art linear solvers and determining whether singular-to-double-precision matrices are indeed singular. Computationally, this article evaluates the impact of several novel pivoting schemes in exact arithmetic, benchmarks the exact iterative solvers within Linbox, and benchmarks the accuracy of MATLAB sparse backslash. Most importantly, it is shown that SPEX Left LU outperforms the exact iterative solvers in run time on easy instances and in stability as the iterative solver fails on a sizeable subset of the tested (both easy and hard) instances. The SPEX Left LU package is written in ANSI C, comes with a MATLAB interface, and is distributed via GitHub, as a component of the SPEX software package, and as a component of SuiteSparse.  © 2022 Association for Computing Machinery.",exact matrix factorization; Exactly solving linear systems; roundoff errors; sparse linear systems; sparse matrix algorithms,Benchmarking; C (programming language); Factorization; Iterative methods; Lower-upper decomposition; MATLAB; Exact matrix factorization; Exactly solving linear system; Iterative solvers; LU factorization; Matrix algorithms; Matrix factorizations; Round-off errors; Sparse linear systems; Sparse matrices; Sparse matrix algorithm; Linear systems
Algorithm 1022: Efficient Algorithms for Computing a Rank-Revealing UTV Factorization on Parallel Computing Architectures,2022,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134877794&doi=10.1145%2f3507466&partnerID=40&md5=b3dc0f60ac10346d7118091d2f866ec3,"Randomized singular value decomposition (RSVD) is by now a well-established technique for efficiently computing an approximate singular value decomposition of a matrix. Building on the ideas that underpin RSVD, the recently proposed algorithm ""randUTV""computes a full factorization of a given matrix that provides low-rank approximations with near-optimal error. Because the bulk of randUTV is cast in terms of communication-efficient operations such as matrix-matrix multiplication and unpivoted QR factorizations, it is faster than competing rank-revealing factorization methods such as column-pivoted QR in most high-performance computational settings. In this article, optimized randUTV implementations are presented for both shared-memory and distributed-memory computing environments. For shared memory, randUTV is redesigned in terms of an algorithm-by-blocks that, together with a runtime task scheduler, eliminates bottlenecks from data synchronization points to achieve acceleration over the standard blocked algorithmbased on a purely fork-join approach. The distributed-memory implementation is based on the ScaLAPACK library. The performance of our new codes compares favorably with competing factorizations available on both shared-memory and distributed-memory architectures.  © 2022 Association for Computing Machinery.",algorithm-by-blocks; block algorithm; high performance; Numerical linear algebra; randomized methods; randomized SVD; rank-revealing matrix factorization; singular value decomposition,Approximation algorithms; Approximation theory; Factorization; Memory architecture; Numerical methods; Parallel architectures; Algorithm-by-block; Block algorithm; High performance; Matrix factorizations; Numerical Linear Algebra; Performance; Randomized method; Randomized SVD; Rank-revealing; Rank-revealing matrix factorization; Singular value decomposition
A Safe Computational Framework for Integer Programming Applied to Chvátal's Conjecture,2022,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134874207&doi=10.1145%2f3485630&partnerID=40&md5=3d3d0663b6fdc69604832c3ce42d06c6,"We describe a general and safe computational framework that provides integer programming results with the degree of certainty that is required for machine-assisted proofs of mathematical theorems. At its core, the framework relies on a rational branch-and-bound certificate produced by an exact integer programming solver, SCIP, in order to circumvent floating-point round-off errors present in most state-of-the-art solvers for mixed-integer programs. The resulting certificates are self-contained and checker software exists that can verify their correctness independently of the integer programming solver used to produce the certificate. This acts as a safeguard against programming errors that may be present in complex solver software. The viability of this approach is tested by applying it to finite cases of Chvátal's conjecture, a long-standing open question in extremal combinatorics. We take particular care to verify also the correctness of the input for this specific problem, using the Coq formal proof assistant. As a result, we are able to provide the first machine-assisted proof that Chvátal's conjecture holds for all downsets whose union of sets contains seven elements or less.  © 2022 held by the owner/author(s).",Exact rational integer programming; extremal combinatorics; verification,Combinatorial mathematics; Digital arithmetic; Theorem proving; Branch and bounds; Computational framework; Degree of certainty; Exact rational integer programming; Extremal combinatorics; Floating points; Integer Program- ming; Rational integers; Round-off errors; State of the art; Integer programming
On Memory Traffic and Optimisations for Low-order Finite Element Assembly Algorithms on Multi-core CPUs,2022,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134878895&doi=10.1145%2f3503925&partnerID=40&md5=4e3877e171f23e129d81b35781636989,"Motivated by the wish to understand the achievable performance of finite element assembly on unstructured computational meshes, we dissect the standard cellwise assembly algorithm into four kernels, two of which are dominated by irregular memory traffic. Several optimisation schemes are studied together with associated lower and upper bounds on the estimated memory traffic volume. Apart from properly reordering the mesh entities, the two most significant optimisations include adopting a lookup table in adding element matrices or vectors to their global counterparts, and using a row-wise assembly algorithm for multi-threaded parallelisation. Rigorous benchmarking shows that, due to the various optimisations, the actual volumes of memory traffic are in many cases very close to the estimated lower bounds. These results confirm the effectiveness of the optimisations, while also providing a recipe for developing efficient software for finite element assembly.  © 2022 held by the owner/author(s).",AMD Epyc; assembly; Cavium TX2; Finite element methods; Intel Xeon; multi-core,Program processors; Table lookup; Achievable performance; AMD epyc; Assembly algorithm; Cavium TX2; Computational mesh; Intel xeon; Low-order finite elements; Multi-cores; Optimisations; Optimization scheme; Finite element method
Construction of Arbitrary Order Finite Element Degree-of-Freedom Maps on Polygonal and Polyhedral Cell Meshes,2022,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131942356&doi=10.1145%2f3524456&partnerID=40&md5=e36e6c79b190c29961f398a46e95796a,"We develop a method for generating degree-of-freedom maps for arbitrary order Ciarlet-type finite element spaces for any cell shape. The approach is based on the composition of permutations and transformations by cell sub-entity. Current approaches to generating degree-of-freedom maps for arbitrary order problems typically rely on a consistent orientation of cell entities that permits the definition of a common local coordinate system on shared edges and faces. However, while orientation of a mesh is straightforward for simplex cells and is a local operation, it is not a strictly local operation for quadrilateral cells and, in the case of hexahedral cells, not all meshes are orientable. The permutation and transformation approach is developed for a range of element types, including arbitrary degree Lagrange, serendipity, and divergence- and curl-conforming elements, and for a range of cell shapes. The approach is local and can be applied to cells of any shape, including general polytopes and meshes with mixed cell types. A number of examples are presented and the developed approach has been implemented in open-source libraries.  © 2022 Association for Computing Machinery.",degrees-of-freedom; Finite element methods; polyhedral cells,Cells; Cytology; Degrees of freedom (mechanics); Interactive computer graphics; 'current; Arbitrary order; Cell shapes; Consistent orientations; Degree-of-freedom; Finite element space; Local coordinate system; Local operations; Order problems; Polyhedral cells; Finite element method
Algorithm 1024: Spherical Triangle Algorithm: A Fast Oracle for Convex Hull Membership Queries,2022,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134875543&doi=10.1145%2f3516520&partnerID=40&md5=9ee4e0fbe877725632f71225d46dc97e,"The Convex Hull Membership (CHM) tests whether , where p and the n points of S lie in . CHM finds applications in Linear Programming, Computational Geometry, and Machine Learning. The Triangle Algorithm (TA), previously developed, in iterations computes , either an -approximate solution, or a witness certifying . We first prove the equivalence of exact and approximate versions of CHM and Spherical-CHM, where and for each v in S. If for some every non-witness with admits satisfying , we prove the number of iterations improves to and always holds. Equivalence of CHM and Spherical-CHM implies Minimum Enclosing Ball (MEB) algorithms can be modified to solve CHM. However, we prove -approximation in MEB is -approximation in Spherical-CHM. Thus, even iteration MEB algorithms are not superior to Spherical-TA. Similar weakness is proved for MEB core sets. Spherical-TA also results a variant of the All Vertex Triangle Algorithm (AVTA) for computing all vertices of . Substantial computations on distinct problems demonstrate that TA and Spherical-TA generally achieve superior efficiency over algorithms such as Frank-Wolfe, MEB, and LP-Solver.  © 2022 Association for Computing Machinery.",Convex Hull Membership; data reduction; Machine Learning; minimum enclosing ball; Triangle Algorithm,Computational geometry; Iterative methods; Linear programming; Spheres; Approximate solution; Convex hull; Convex hull membership; Linear-programming; Machine-learning; Membership query; Minimum enclosing ball; Number of iterations; Spherical triangles; Triangle algorithm; Machine learning
BiqBin: A Parallel Branch-and-bound Solver for Binary Quadratic Problems with Linear Constraints,2022,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134874616&doi=10.1145%2f3514039&partnerID=40&md5=7d2f5d2950005df10ec87cd32e279bd7,"We present BiqBin, an exact solver for linearly constrained binary quadratic problems. Our approach is based on an exact penalty method to first efficiently transform the original problem into an instance of Max-Cut, and then to solve the Max-Cut problem by a branch-and-bound algorithm. All the main ingredients are carefully developed using new semidefinite programming relaxations obtained by strengthening the existing relaxations with a set of hypermetric inequalities, applying the bundle method as the bounding routine and using new strategies for exploring the branch-and-bound tree.Furthermore, an efficient C implementation of a sequential and a parallel branch-and-bound algorithm is presented. The latter is based on a load coordinator-worker scheme using MPI for multi-node parallelization and is evaluated on a high-performance computer.The new solver is benchmarked against BiqCrunch, GUROBI, and SCIP on four families of (linearly constrained) binary quadratic problems. Numerical results demonstrate that BiqBin is a highly competitive solver. The serial version outperforms the other three solvers on the majority of the benchmark instances. We also evaluate the parallel solver and show that it has good scaling properties. The general audience can use it as an on-line service available at http://www.biqbin.eu.  © 2022 Association for Computing Machinery.",binary quadratic programming; parallel algorithms; semidefinite programming; Solvers,Benchmarking; Branch and bound method; Constrained optimization; Parallel algorithms; Binary quadratic programming; Branch-and-bound algorithms; Exact penalty methods; Linear constraints; MAX CUT; Max-cut; Parallel branch and bounds; Quadratic problem; Semi-definite programming; Solver; Quadratic programming
Kummer versus Montgomery Face-off over Prime Order Fields,2022,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134881669&doi=10.1145%2f3503536&partnerID=40&md5=b31a3e707175f79ed10c8a8ae5b208ee,"This paper makes a comprehensive comparison of the efficiencies of vectorized implementations of Kummer lines and Montgomery curves at various security levels. For the comparison, nine Kummer lines are considered, out of which eight are new, and new assembly implementations of all nine Kummer lines have been made. Seven previously proposed Montgomery curves are considered and new vectorized assembly implementations have been made for three of them. Our comparisons show that for all security levels, Kummer lines are consistently faster than Montgomery curves, though the speed-up gap is not much.  © 2022 Association for Computing Machinery.",4-way vectorization; Diffie-Hellman; Kummer line; Montgomery curve; SIMD,4-way vectorization; Assembly implementation; Diffie Hellman; Kumm line; Montgomery; Montgomery curve; Prime orders; Security level; SIMD; Vectorization
Fast and Accurate Proper Orthogonal Decomposition using Efficient Sampling and Iterative Techniques for Singular Value Decomposition,2022,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134874584&doi=10.1145%2f3506691&partnerID=40&md5=52ccae87c88759fffa5d74fdca7d50a8,"In this article, we propose a computationally efficient iterative algorithm for proper orthogonal decomposition (POD) using random sampling based techniques. In this algorithm, additional rows and columns are sampled and a merging technique is used to update the dominant POD modes in each iteration. We derive bounds for the spectral norm of the error introduced by a series of merging operations. We use an existing theorem to get an approximate measure of the quality of subspaces obtained on convergence of the iteration. Results on various datasets indicate that the POD modes and/or the subspaces are approximated with excellent accuracy with a significant runtime improvement over computing the truncated SVD. We also propose a method to compute the POD modes of large matrices that do not fit in the RAM using this iterative sampling and merging algorithms.  © 2022 Association for Computing Machinery.",fast and accurate POD; Iterative and incremental sampling,Computation theory; Iterative methods; Merging; Principal component analysis; Computationally efficient; Decomposition modes; Efficient sampling; Fast and accurate proper orthogonal decomposition; Incremental samplings; Iterative and incremental sampling; Iterative technique; Orthogonal decomposition; Proper Orthogonal; Sampling technique; Singular value decomposition
Algorithm 1025: PARyOpt: A Software for Parallel Asynchronous Remote Bayesian Optimization,2022,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134882483&doi=10.1145%2f3529517&partnerID=40&md5=5eee28611edaac7a0205317871045143,"PARyOpt is a Python based implementation of the Bayesian optimization routine designed for remote and asynchronous function evaluations. Bayesian optimization is especially attractive for computational optimization due to its low cost function footprint as well as the ability to account for uncertainties in data. A key challenge to efficiently deploy any optimization strategy on distributed computing systems is the synchronization step, where data from multiple function calls is assimilated to identify the next campaign of function calls. Bayesian optimization provides an elegant approach to overcome this issue via asynchronous updates. We formulate, develop and implement a parallel, asynchronous variant of Bayesian optimization. The framework is robust and resilient to external failures. We show how such asynchronous evaluations help reduce the total optimization wall clock time for a suite of test problems. Additionally, we show how the software design of the framework allows easy extension to response surface reconstruction (Kriging), providing a high performance software for autonomous exploration. The software is available on PyPI, with examples and documentation.  © 2022 held by the owner/author(s).",Bayesian optimization; parallel optimization; python software; remote evaluation,Computer software; Cost functions; Function evaluation; High level languages; Software design; Bayesian optimization; Computational optimization; Cost-function; Function calls; Low-costs; Optimization routine; Parallel optimization; Python software; Remote evaluation; Uncertainty; Python
Remark on Algorithm 982: Explicit Solutions of Triangular Systems of First-order Linear Initial-value Ordinary Differential Equations with Constant Coefficients,2022,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125182271&doi=10.1145%2f3479429&partnerID=40&md5=03446f9375c02c463b8e9968d0052fac,"Algorithm 982: Explicit solutions of triangular systems of first-order linear initial-value ordinary differential equations with constant coefficients provides an explicit solution for an homogeneous system, and a brief description of how to compute a solution for the inhomogeneous case. The method described is not directly useful if the coefficient matrix is singular. This remark explains more completely how to compute the solution for the inhomogeneous case and for the singular coefficient matrix case.  © 2021 Copyright held by the owner/author(s).",constant coefficient; explicit solution; first order; initial value problem; Ordinary differential equations; triangular system,Initial value problems; Matrix algebra; Coefficient matrix; Constant coefficients; Explicit solutions; First order; Homogeneous system; Initial-value problem; Matrix case; Triangular system; Ordinary differential equations
GraphBLAST: A High-Performance Linear Algebra-based Graph Framework on the GPU,2022,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125207018&doi=10.1145%2f3466795&partnerID=40&md5=1235a336e99f19639246f2fea23a2105,"High-performance implementations of graph algorithms are challenging to implement on new parallel hardware such as GPUs because of three challenges: (1) the difficulty of coming up with graph building blocks, (2) load imbalance on parallel hardware, and (3) graph problems having low arithmetic intensity. To address some of these challenges, GraphBLAS is an innovative, on-going effort by the graph analytics community to propose building blocks based on sparse linear algebra, which allow graph algorithms to be expressed in a performant, succinct, composable, and portable manner. In this paper, we examine the performance challenges of a linear-algebra-based approach to building graph frameworks and describe new design principles for overcoming these bottlenecks. Among the new design principles is exploiting input sparsity, which allows users to write graph algorithms without specifying push and pull direction. Exploiting output sparsity allows users to tell the backend which values of the output in a single vectorized computation they do not want computed. Load-balancing is an important feature for balancing work amongst parallel workers. We describe the important load-balancing features for handling graphs with different characteristics. The design principles described in this paper have been implemented in ""GraphBLAST"", the first high-performance linear algebra-based graph framework on NVIDIA GPUs that is open-source. The results show that on a single GPU, GraphBLAST has on average at least an order of magnitude speedup over previous GraphBLAS implementations SuiteSparse and GBTL, comparable performance to the fastest GPU hardwired primitives and shared-memory graph frameworks Ligra and Gunrock, and better performance than any other GPU graph framework, while offering a simpler and more concise programming model.  © 2022 Copyright held by the owner/author(s).",GPU; Graph algorithm; matrix multiply; sparse linear algebra,Computer hardware; Graph theory; Linear algebra; Open source software; Program processors; Building blockes; Design Principles; Graph algorithms; Graph framework; High performance implementations; Load-Balancing; Matrix multiply; Parallel hardware; Performance; Sparse linear algebra; Graphics processing unit
Source-to-Source Automatic Differentiation of OpenMP Parallel Loops,2022,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125181623&doi=10.1145%2f3472796&partnerID=40&md5=6de44a070cdfd05dff1e17b3d1abed23,"This article presents our work toward correct and efficient automatic differentiation of OpenMP parallel worksharing loops in forward and reverse mode. Automatic differentiation is a method to obtain gradients of numerical programs, which are crucial in optimization, uncertainty quantification, and machine learning. The computational cost to compute gradients is a common bottleneck in practice. For applications that are parallelized for multicore CPUs or GPUs using OpenMP, one also wishes to compute the gradients in parallel. We propose a framework to reason about the correctness of the generated derivative code, from which we justify our OpenMP extension to the differentiation model. We implement this model in the automatic differentiation tool Tapenade and present test cases that are differentiated following our extended differentiation procedure. Performance of the generated derivative programs in forward and reverse mode is better than sequential, although our reverse mode often scales worse than the input programs.  © 2022 Association for Computing Machinery.",Automatic differentiation; multicore; OpenMP; shared-memory parallel,Application programming interfaces (API); Multicore programming; Numerical methods; Automatic differentiations; Forward mode; Multi-cores; Numerical programs; Openmp; Optimisations; Parallel loops; Reverse mode; Shared-memory parallels; Work-sharing; Program processors
"Formalization of Double-Word Arithmetic, and Comments on ""Tight and Rigorous Error Bounds for Basic Building Blocks of Double-Word Arithmetic",2022,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125198074&doi=10.1145%2f3484514&partnerID=40&md5=8ff69e3f5d12888b222deebe56740fca,"Recently, a complete set of algorithms for manipulating double-word numbers (some classical, some new) was analyzed [16]. We have formally proven all the theorems given in that article, using the Coq proof assistant. The formal proof work led us to: (i) locate mistakes in some of the original paper proofs (mistakes that, however, do not hinder the validity of the algorithms), (ii) significantly improve some error bounds, and (iii) generalize some results by showing that they are still valid if we slightly change the rounding mode. The consequence is that the algorithms presented in [16] can be used with high confidence, and that some of them are even more accurate than what was believed before. This illustrates what formal proof can bring to computer arithmetic: beyond mere (yet extremely useful) verification, correction, and consolidation of already known results, it can help to find new properties. All our formal proofs are freely available.  © 2022 Association for Computing Machinery.",Coq; double-double arithmetic; double-word arithmetic; Floating-point arithmetic; formalization; proof assistant,Error analysis; Theorem proving; Basic building block; Complete sets; Coq; Double-double arithmetic; Double-word arithmetic; Error bound; Floating-point arithmetic; Formal proofs; Formalisation; Proof assistant; Digital arithmetic
A Computational Study of Using Black-box QR Solvers for Large-scale Sparse-dense Linear Least Squares Problems,2022,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125196454&doi=10.1145%2f3494527&partnerID=40&md5=75bbc6c1c6eaa8f462a76f37aa8d6dbb,"Large-scale overdetermined linear least squares problems arise in many practical applications. One popular solution method is based on the backward stable QR factorization of the system matrix A. This article focuses on sparse-dense least squares problems in which A is sparse except from a small number of rows that are considered dense. For large-scale problems, the direct application of a QR solver either fails because of insufficient memory or is unacceptably slow. We study several solution approaches based on using a sparse QR solver without modification, focussing on the case that the sparse part of A is rank deficient. We discuss partial matrix stretching and regularization and propose extending the augmented system formulation with iterative refinement for sparse problems to sparse-dense problems, optionally incorporating multi-precision arithmetic. In summary, our computational study shows that, before applying a black-box QR factorization, a check should be made for rows that are classified as dense and, if such rows are identified, then A should be split into sparse and dense blocks; a number of ways to use a black-box QR factorization to exploit this splitting are possible, with no single method found to be the best in all cases.  © 2022 Copyright held by the owner/author(s).",dense rows; linear least-squares problems; QR factorization; Sparse matrices,Factorization; Least squares approximations; Matrix algebra; Black boxes; Computational studies; Dense row; Large-scales; Least square problems; Linear least-square problems; QR factorizations; Solution methods; Sparse matrices; System matrix; Iterative methods
Reproduced Computational Results Report for “Ginkgo: A Modern Linear Operator Algebra Framework for High Performance Computing,2022,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125169246&doi=10.1145%2f3480936&partnerID=40&md5=14d365c9540163f6b52d3431739a1c87,"The article titled ""Ginkgo: A Modern Linear Operator Algebra Framework for High Performance Computing""by Anzt et al. presents a modern, linear operator centric, C++ library for sparse linear algebra. Experimental results in the article demonstrate that Ginkgo is a flexible and user-friendly framework capable of achieving high-performance on state-of-the-art GPU architectures.In this report, the Ginkgo library is installed and a subset of the experimental results are reproduced. Specifically, the experiment that shows the achieved memory bandwidth of the Ginkgo Krylov linear solvers on NVIDIA A100 and AMD MI100 GPUs is redone and the results are compared to what presented in the published article. Upon completion of the comparison, the published results are deemed reproducible.  © 2022 Association for Computing Machinery.",healthy software lifecycle; High performance computing; multicore and manycore architectures,C++ (programming language); Life cycle; Linear algebra; Program processors; C++ libraries; Computational results; Healthy software lifecycle; High performance computing; Linear operators; Many-core architecture; Multicore architectures; Operator algebras; Performance computing; Software life cycles; Mathematical operators
Algorithm 1020: Computation of Multi-Degree Tchebycheffian B-Splines,2022,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125207109&doi=10.1145%2f3478686&partnerID=40&md5=ae0b243a8f799506f162c3357b737145,"Multi-degree Tchebycheffian splines are splines with pieces drawn from extended (complete) Tchebycheff spaces, which may differ from interval to interval, and possibly of different dimensions. These are a natural extension of multi-degree polynomial splines. Under quite mild assumptions, they can be represented in terms of a so-called multi-degree Tchebycheffian B-spline (MDTB-spline) basis; such basis possesses all the characterizing properties of the classical polynomial B-spline basis. We present a practical framework to compute MDTB-splines, and provide an object-oriented implementation in Matlab. The implementation supports the construction, differentiation, and visualization of MDTB-splines whose pieces belong to Tchebycheff spaces that are null-spaces of constant-coefficient linear differential operators. The construction relies on an extraction operator that maps local Tchebycheffian Bernstein functions to the MDTB-spline basis of interest.  © 2022 Association for Computing Machinery.",B-splines; constant-coefficient linear differential operators; extraction operator; multi-degree splines; Tchebycheffian splines,Differential equations; Extraction; Mathematical operators; Polynomial approximation; Splines; B splines; B-spline basis; Constant coefficients; Constant-coefficient linear differential operator; Extraction operator; Linear differential operators; Multi-degree spline; Natural extension; Polynomial splines; Tchebycheffian spline; Interpolation
Bringing Trimmed Serendipity Methods to Computational Practice in Firedrake,2022,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125201807&doi=10.1145%2f3490485&partnerID=40&md5=df592a53fab59f7642460458e36cc609,"We present an implementation of the trimmed serendipity finite element family, using the open-source finite element package Firedrake. The new elements can be used seamlessly within the software suite for problems requiring H1, H(curl), or H(div)-conforming elements on meshes of squares or cubes. To test how well trimmed serendipity elements perform in comparison to traditional tensor product elements, we perform a sequence of numerical experiments including the primal Poisson, mixed Poisson, and Maxwell cavity eigenvalue problems. Overall, we find that the trimmed serendipity elements converge, as expected, at the same rate as the respective tensor product elements, while being able to offer significant savings in the time or memory required to solve certain problems.  © 2022 Copyright held by the owner/author(s).",FEM; finite element method; firedrake; PDEs; trimmed serendipity,Eigenvalues and eigenfunctions; Open source software; Tensors; Conforming elements; Finite element packages; Firedrake; Numerical experiments; Open-source; PDE; Product elements; Software suite; Tensor products; Trimmed serendipity; Finite element method
Algorithm 1019: A Task-based Multi-shift QR/QZ Algorithm with Aggressive Early Deflation,2022,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125191396&doi=10.1145%2f3495005&partnerID=40&md5=e89cc106043e995422133e09e0a32323,"The QR algorithm is one of the three phases in the process of computing the eigenvalues and the eigenvectors of a dense nonsymmetric matrix. This paper describes a task-based QR algorithm for reducing an upper Hessenberg matrix to real Schur form. The task-based algorithm also supports generalized eigenvalue problems (QZ algorithm) but this paper concentrates on the standard case. The task-based algorithm adopts previous algorithmic improvements, such as tightly-coupled multi-shifts and Aggressive Early Deflation (AED), and also incorporates several new ideas that significantly improve the performance. This includes, but is not limited to, the elimination of several synchronization points, the dynamic merging of previously separate computational steps, the shortening and the prioritization of the critical path, and experimental GPU support. The task-based implementation is demonstrated to be multiple times faster than multi-threaded LAPACK and ScaLAPACK in both single-node and multi-node configurations on two different machines based on Intel and AMD CPUs. The implementation is built on top of the StarPU runtime system and is part of the open-source StarNEig library.  © 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM.",aggressive early deflation; distributed memory; Eigenvalue problem; GPU; MPI; multi-shift; QR algorithm; QZ algorithm; real Schur form; shared memory; StarPU; task-based,Graphics processing unit; Matrix algebra; Memory architecture; Open systems; Program processors; Aggressive early deflation; Distributed Memory; Eigenvalue problem; MPI; Multi-shift; QR algorithms; QZ algorithms; Real schur form; Schur form; Shared memory; Starpu; Task-based; Eigenvalues and eigenfunctions
Ginkgo: A Modern Linear Operator Algebra Framework for High Performance Computing,2022,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125185298&doi=10.1145%2f3480935&partnerID=40&md5=e646deb9fcdb37ce6e8ef870d820691f,"In this article, we present Ginkgo, a modern C++ math library for scientific high performance computing. While classical linear algebra libraries act on matrix and vector objects, Ginkgo's design principle abstracts all functionality as ""linear operators,""motivating the notation of a ""linear operator algebra library.""Ginkgo's current focus is oriented toward providing sparse linear algebra functionality for high performance graphics processing unit (GPU) architectures, but given the library design, this focus can be easily extended to accommodate other algorithms and hardware architectures. We introduce this sophisticated software architecture that separates core algorithms from architecture-specific backends and provide details on extensibility and sustainability measures. We also demonstrate Ginkgo's usability by providing examples on how to use its functionality inside the MFEM and deal.ii finite element ecosystems. Finally, we offer a practical demonstration of Ginkgo's high performance on state-of-the-art GPU architectures.  © 2022 Association for Computing Machinery.",healthy software lifecycle; High performance computing; multi-core and manycore architectures,C++ (programming language); Computer graphics; Computer graphics equipment; Graphics processing unit; Life cycle; Linear algebra; Program processors; Healthy software lifecycle; High performance computing; Linear operators; Many-core architecture; Math library; Multicore architectures; Operator algebras; Performance; Performance computing; Software life cycles; Mathematical operators
An Algorithm for the Complete Solution of the Quartic Eigenvalue Problem,2022,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125191057&doi=10.1145%2f3494528&partnerID=40&md5=3a741d435742146a4868654484e8ede2,"The quartic eigenvalue problem (λ4A+λ3B+λ2C+λD+E)x = 0 naturally arises in a plethora of applications, such as when solving the Orr-Sommerfeld equation in the stability analysis of the Poiseuille flow, in theoretical analysis and experimental design of locally resonant phononic plates, modeling a robot with electric motors in the joints, calibration of catadioptric vision system, or, for example, computation of the guided and leaky modes of a planar waveguide. This article proposes a new numerical method for the full solution (all eigenvalues and all left and right eigenvectors) that, starting with a suitable linearization, uses an initial, structure-preserving reduction designed to reveal and deflate a certain number of zero and infinite eigenvalues before the final linearization is forwarded to the QZ algorithm. The backward error in the reduction phase is bounded column wise in each coefficient matrix, which is advantageous if the coefficient matrices are graded. Numerical examples show that the proposed algorithm is capable of computing the eigenpairs with small residuals, and that it is competitive with the available state-of-the-art methods.  © 2022 Association for Computing Machinery.",Eigenvalues; eigenvectors; infinite eigenvalues; linearization; nonlinear eigenvalue problem; Orr-Sommerfeld equation; quadratification; quartic eigenvalue problem; QZ algorithm,Computer aided design; Linearization; Machine design; Matrix algebra; Nonlinear equations; Numerical methods; Eigen-value; Eigenvalue problem; Infinite eigenvalue; Linearisation; Nonlinear eigenvalue problem; Orr-Sommerfeld equations; Quadratification; Quartic eigenvalue problem; QZ algorithms; Eigenvalues and eigenfunctions
Exploiting Problem Structure in Derivative Free Optimization,2022,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125179007&doi=10.1145%2f3474054&partnerID=40&md5=0e36f1efd5ce655e938897d660a07f29,"A structured version of derivative-free random pattern search optimization algorithms is introduced, which is able to exploit coordinate partially separable structure (typically associated with sparsity) often present in unconstrained and bound-constrained optimization problems. This technique improves performance by orders of magnitude and makes it possible to solve large problems that otherwise are totally intractable by other derivative-free methods. A library of interpolation-based modelling tools is also described, which can be associated with the structured or unstructured versions of the initial pattern search algorithm. The use of the library further enhances performance, especially when associated with structure. The significant gains in performance associated with these two techniques are illustrated using a new freely-available release of the Brute Force Optimizer (BFO) package firstly introduced in [Porcelli and Toint 2017], which incorporates them. An interesting conclusion of the numerical results presented is that providing global structural information on a problem can result in significantly less evaluations of the objective function than attempting to building local Taylor-like models.  © 2022 Association for Computing Machinery.",Derivative-free optimization; direct-search methods; interpolation models; structured problems,Constrained optimization; Shape optimization; Structural optimization; Bound constrained optimization; Derivative-free; Derivative-free optimization; Direct search methods; Interpolation model; Pattern search optimizations; Performance; Problem structure; Random pattern; Structured problems; Interpolation
Irksome: Automating Runge Kuta Time-stepping for Finite Element Methods,2021,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116445594&doi=10.1145%2f3466168&partnerID=40&md5=caae71c2fe44b34ff335ff2efe354323,"While implicit Runge-Kutta (RK) methods possess high order accuracy and important stability properties, implementation difficulties and the high expense of solving the coupled algebraic system at each time step are frequently cited as impediments. We present Irksome, a high-level library for manipulating UFL (Unified Form Language) expressions of semidiscrete variational forms to obtain UFL expressions for the coupled Runge-Kutta stage equations at each time step. Irksome works with the Firedrake package to enable the efficient solution of the resulting coupled algebraic systems. Numerical examples confirm the efficacy of the software and our solver techniques for various problems. © 2021 Association for Computing Machinery.",automation; Finite element methods; Runge-Kutta methods,Algebra; High level languages; Numerical methods; Runge Kutta methods; Stability; Algebraic system; Higher order accuracy; Implementation difficulties; Implicit Runge-Kutta method; Stability properties; Time step; Time-stepping; Unified form; Variational form; Finite element method
"Hyper.deal: An Efficient, Matrix-free Finite-element Library for High-dimensional Partial Differential Equations",2021,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112338608&doi=10.1145%2f3469720&partnerID=40&md5=d36c7e12532405b36ae0e340c1ec7a5f,"This work presents the efficient, matrix-free finite-element library hyper.deal for solving partial differential equations in two up to six dimensions with high-order discontinuous Galerkin methods. It builds upon the low-dimensional finite-element library deal.II to create complex low-dimensional meshes and to operate on them individually. These meshes are combined via a tensor product on the fly, and the library provides new special-purpose highly optimized matrix-free functions exploiting domain decomposition as well as shared memory via MPI-3.0 features. Both node-level performance analyses and strong/weak-scaling studies on up to 147,456 CPU cores confirm the efficiency of the implementation. Results obtained with the library hyper.deal are reported for high-dimensional advection problems and for the solution of the Vlasov-Poisson equation in up to six-dimensional phase space. © 2021 Association for Computing Machinery.",discontinuous Galerkin methods; high-dimensional; high-order; Matrix-free operator evaluation; MPI-3.0 shared memory; Vlasov-Poisson equation,Domain decomposition methods; Finite element method; Memory architecture; Phase space methods; Poisson equation; Vlasov equation; Discontinous Galerkin methods; High-dimensional; High-order; Higher-dimensional; Higher-order; Matrix free; Matrix-free operator evaluation; MPI-3.0 shared memory; Operator Evaluation; Shared memory; Vlasov-Poisson equations; Galerkin methods
Propagating Geometry Information to Finite Element Computations,2021,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112299608&doi=10.1145%2f3468428&partnerID=40&md5=9e9138cf8a6d55b40237dbfb8bb2221a,"The traditional workflow in continuum mechanics simulations is that a geometry description - for example obtained using Constructive Solid Geometry (CSG) or Computer Aided Design (CAD) tools - forms the input for a mesh generator. The mesh is then used as the sole input for the finite element, finite volume, and finite difference solver, which at this point no longer has access to the original, ""underlying""geometry. However, many modern techniques - for example, adaptive mesh refinement and the use of higher order geometry approximation methods - really do need information about the underlying geometry to realize their full potential. We have undertaken an exhaustive study of where typical finite element codes use geometry information, with the goal of determining what information geometry tools would have to provide. Our study shows that nearly all geometry-related needs inside the simulators can be satisfied by just two ""primitives"": elementary queries posed by the simulation software to the geometry description. We then show that it is possible to provide these primitives in all of the frequently used ways in which geometries are described in common industrial workflows, and illustrate our solutions using a number of examples. © 2021 Association for Computing Machinery.",computer aided design; Finite element meshes; geometry description,Computer aided design; Computer software; Continuum mechanics; Finite element method; Metal working tools; Computer aided design tools; Computer-aided design; Constructive solid geometry; Finite element computations; Finite element meshes; Geometry description; Geometry information; Mechanics simulations; Mesh generators; Work-flows; Mesh generation
Scrambled Linear Pseudorandom Number Generators,2021,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116491919&doi=10.1145%2f3460772&partnerID=40&md5=7074a0a4e9a0a51d0dff49a49c6a1f7c,"F2-linear pseudorandom number generators are very popular due to their high speed, to the ease with which generators with a sizable state space can be created, and to their provable theoretical properties. However, they suffer from linear artifacts that show as failures in linearity-related statistical tests such as the binary-rank and the linear-complexity test. In this article, we give two new contributions. First, we introduce two new F2-linear transformations that have been handcrafted to have good statistical properties and at the same time to be programmable very efficiently on superscalar processors, or even directly in hardware. Then, we describe some scramblers, that is, nonlinear functions applied to the state array that reduce or delete the linear artifacts, and propose combinations of linear transformations and scramblers that give extremely fast pseudorandom number generators of high quality. A novelty in our approach is that we use ideas from the theory of filtered linear-feedback shift registers to prove some properties of our scramblers, rather than relying purely on heuristics. In the end, we provide simple, extremely fast generators that use a few hundred bits of memory, have provable properties, and pass strong statistical tests. © 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Pseudorandom number generators,Linear transformations; Number theory; Random number generation; Shift registers; High quality; High Speed; Linear complexity; Linear feedback shift registers; Nonlinear functions; Property; Pseudorandom number generators; State-space; Statistical properties; Superscalar Processor; Statistical tests
Remark on Algorithm 992: An OpenGL- And C++-based Function Library for Curve and Surface Modeling in a Large Class of Extended Chebyshev Spaces,2021,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116504962&doi=10.1145%2f3461643&partnerID=40&md5=20a9009654954f320be1186455546d47,We provide a number of corrections to the software component that accompanied this Algorithm submission [3]. An updated version of the code is available from the ACM Collected Algorithms site [1]. © 2021 Copyright held by the owner/author(s).,B-curve/surface modeling; basis transformation; constant-coefficient homogeneous linear differential equations; control-point-based exact description; Extended Chebyshev spaces; normalized B-basis; OpenGL; OpenMP; order (dimension) elevation; subdivision (B-algorithm),C++ (programming language); Curve fitting; Differential equations; Linear transformations; Three dimensional computer graphics; B-bases; B-basis; B-curve/surface modeling; Base transformation; Chebyshev; Constant coefficients; Constant-coefficient homogeneous linear differential equation; Control point; Control-point-based exact description; Curve/surface modeling; Extended chebyshev space; Linear differential equation; Normalized B-base; Opengl; Openmp; Order (dimension) elevation; Point-based; Subdivision (B-algorithm); Application programming interfaces (API)
Exactly Computing the Tail of the Poisson-Binomial Distribution,2021,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116410104&doi=10.1145%2f3460774&partnerID=40&md5=b3c4c8373c27a2c9dde6739403c1612b,"We present ShiftConvolvePoibin, a fast exact method to compute the tail of a Poisson-binomial distribution (PBD). Our method employs an exponential shift to retain its accuracy when computing a tail probability, and in practice we find that it is immune to the significant relative errors that other methods, exact or approximate, can suffer from when computing very small tail probabilities of the PBD. The accompanying R package is also competitive with the fastest implementations for computing the entire PBD. © 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM.",exact method; exponential shift; FFT; Poisson-binomial,Budget control; Poisson equation; Exact methods; Exponential shift; Exponentials; Fast implementation; Poisson binomial distributions; Poisson-binomial; Relative errors; Tail probability; Poisson distribution
PySPH: A Python-based Framework for Smoothed Particle Hydrodynamics,2021,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104919717&doi=10.1145%2f3460773&partnerID=40&md5=ccdaf9442f8e2ff5b70620422e611884,"PySPH is an open-source, Python-based, framework for particle methods in general and Smoothed Particle Hydrodynamics (SPH) in particular. PySPH allows a user to define a complete SPH simulation using pure Python. High-performance code is generated from this high-level Python code and executed on either multiple cores, or on GPUs, seamlessly. It also supports distributed execution using MPI. PySPH supports a wide variety of SPH schemes and formulations. These include, incompressible and compressible fluid flow, elastic dynamics, rigid body dynamics, shallow water equations, and other problems. PySPH supports a variety of boundary conditions including mirror, periodic, solid wall, and inlet/outlet boundary conditions. The package is written to facilitate reuse and reproducibility. This article discusses the overall design of PySPH and demonstrates many of its features. Several example results are shown to demonstrate the range of features that PySPH provides. © 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM.",CPU; GPU; open source; PySPH; Python; smoothed particle hydrodynamics,Boundary conditions; Equations of motion; Graphics processing unit; High level languages; Hydrodynamics; Incompressible flow; Open source software; Program processors; Compressible fluid flow; CPU; High performance codes; Hydrodynamic simulation; Incompressible fluid flow; Open-source; Particle methods; PySPH; Python code; Smoothed particle hydrodynamics; Python
Abstractions and Automated Algorithms for Mixed Domain Finite Element Methods,2021,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116417765&doi=10.1145%2f3471138&partnerID=40&md5=e82d821a3ddb693e8264a3d12305c9ed,"Mixed dimensional partial differential equations (PDEs) are equations coupling unknown fields defined over domains of differing topological dimension. Such equations naturally arise in a wide range of scientific fields including geology, physiology, biology, and fracture mechanics. Mixed dimensional PDEs are also commonly encountered when imposing non-standard conditions over a subspace of lower dimension, e.g., through a Lagrange multiplier. In this article, we present general abstractions and algorithms for finite element discretizations of mixed domain and mixed dimensional PDEs of codimension up to one (i.e., nD-mD with |n-m| ≤ 1). We introduce high-level mathematical software abstractions together with lower-level algorithms for expressing and efficiently solving such coupled systems. The concepts introduced here have also been implemented in the context of the FEniCS finite element software. We illustrate the new features through a range of examples, including a constrained Poisson problem, a set of Stokes-type flow models, and a model for ionic electrodiffusion. © 2021 Association for Computing Machinery.",FEniCS project; mixed dimensional; mixed domains; mixed finite elements,Abstracting; Finite element method; Fracture mechanics; Partial differential equations; Abstraction algorithms; Automated algorithms; FEniCS project; Fractures mechanics; Mixed dimensional; Mixed domains; Mixed finite elements; Scientific fields; Standard conditions; Topological dimensions; Lagrange multipliers
Algorithm 1018: FaVeST-Fast Vector Spherical Harmonic Transforms,2021,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111237599&doi=10.1145%2f3458470&partnerID=40&md5=f2e429baa86861323c5e32d52baaaad9,"Vector spherical harmonics on the unit sphere of R3 have broad applications in geophysics, quantum mechanics, and astrophysics. In the representation of a tangent vector field, one needs to evaluate the expansion and the Fourier coefficients of vector spherical harmonics. In this article, we develop fast algorithms (FaVeST) for vector spherical harmonic transforms on these evaluations. The forward FaVeST evaluates the Fourier coefficients and has a computational cost proportional to N log ĝN for N number of evaluation points. The adjoint FaVeST, which evaluates a linear combination of vector spherical harmonics with a degree up to ĝM for M evaluation points, has cost proportional to M log ĝM. Numerical examples of simulated tangent fields illustrate the accuracy, efficiency, and stability of FaVeST. © 2021 Association for Computing Machinery.",FFT; tangent vector fields; Vector spherical harmonics,Astrophysics; Harmonic analysis; Quantum theory; Spheres; Vectors; Adjoints; Broad application; Computational costs; Fast algorithms; Spherical harmonic transforms; Tangent vector field; Tangent vectors; Unit spheres; Vector fields; Vector spherical harmonics; Fourier analysis
Erratum: Remark on Algorithm 723: Fresnel Integrals (ACM Trans. Math. Softw. (1996) 47:4 (498-500) DOI: 10.1145/3452336),2021,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116470247&doi=10.1145%2f235815.235825&partnerID=40&md5=0a9df5459099f8ddfbd1b0a45ca0fc7b,"There are mistakes and typographical errors in Remark on Algorithm 723: Fresnel Integrals, which appeared in ACM Transactions on Mathematical Software 22, 4 (December 1996). This remark corrects those errors. The software provided to Collected Algorithms of the ACM was correct. © 2021 Copyright held by the owner/author(s).",,
Erratum: Remark on Algorithm 723: Fresnel Integrals (ACM Transaction on Mathematical Software (1996) 22:4 (498–500) DOI: 10.1145/365723.365737),2021,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144933435&doi=10.1145%2f3452336&partnerID=40&md5=a3c65f9a5b70f96216534e39c3f8d1eb,"There are mistakes and typographical errors in Remark on Algorithm 723: Fresnel Integrals, which appeared in ACM Transactions on Mathematical Software 22, 4 (December 1996). This remark corrects those errors. The software provided to Collected Algorithms of the ACM was correct. © 2021 Copyright held by the owner/author(s).",,
HyperNOMAD: Hyperparameter Optimization of Deep Neural Networks Using Mesh Adaptive Direct Search,2021,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108907582&doi=10.1145%2f3450975&partnerID=40&md5=60601178bb536a0c7d3576869b6983ac,"The performance of deep neural networks is highly sensitive to the choice of the hyperparameters that define the structure of the network and the learning process. When facing a new application, tuning a deep neural network is a tedious and time-consuming process that is often described as a ""dark art.""This explains the necessity of automating the calibration of these hyperparameters. Derivative-free optimization is a field that develops methods designed to optimize time-consuming functions without relying on derivatives. This work introduces the HyperNOMAD package, an extension of the NOMAD software that applies the MADS algorithm [7] to simultaneously tune the hyperparameters responsible for both the architecture and the learning process of a deep neural network (DNN). This generic approach allows for an important flexibility in the exploration of the search space by taking advantage of categorical variables. HyperNOMAD is tested on the MNIST, Fashion-MNIST, and CIFAR-10 datasets and achieves results comparable to the current state of the art.  © 2021 ACM.",blackbox optimization; categorical variables.; Deep neural networks; derivative-free optimization; hyperparameter optimization; mesh adaptive direct search; neural architecture search,Arts computing; Deep learning; Deep neural networks; Learning algorithms; Learning systems; MESH networking; Optimization; Categorical variables; Consuming functions; Derivative-free optimization; Generic approach; Hyper-parameter optimizations; Learning process; Mesh adaptive direct search; New applications; Neural networks
Linnea: Automatic Generation of Efficient Linear Algebra Programs,2021,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108915765&doi=10.1145%2f3446632&partnerID=40&md5=38d483f583168078aeaa0e54f5290647,"The translation of linear algebra computations into efficient sequences of library calls is a non-trivial task that requires expertise in both linear algebra and high-performance computing. Almost all high-level languages and libraries for matrix computations (e.g., Matlab, Eigen) internally use optimized kernels such as those provided by BLAS and LAPACK; however, their translation algorithms are often too simplistic and thus lead to a suboptimal use of said kernels, resulting in significant performance losses. To combine the productivity offered by high-level languages, and the performance of low-level kernels, we are developing Linnea, a code generator for linear algebra problems. As input, Linnea takes a high-level description of a linear algebra problem; as output, it returns an efficient sequence of calls to high-performance kernels. Linnea uses a custom best-first search algorithm to find a first solution in less than a second, and increasingly better solutions when given more time. In 125 test problems, the code generated by Linnea almost always outperforms Matlab, Julia, Eigen, and Armadillo, with speedups up to and exceeding 10×.  © 2021 ACM.",code generation; Linear algebra,MATLAB; Matrix algebra; Translation (languages); Automatic Generation; Best-first-search algorithm; High level description; High performance computing; Linear algebra problems; Matrix computation; Non-trivial tasks; Translation algorithms; High level languages
FAME: Fast Algorithms for Maxwell’s Equations for Three-dimensional Photonic Crystals,2021,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108908648&doi=10.1145%2f3446329&partnerID=40&md5=a0750fd65ae451a898f999195ad61be5,"In this article, we propose the Fast Algorithms for Maxwell's Equations (FAME) package for solving Maxwell's equations for modeling three-dimensional photonic crystals. FAME combines the null-space free method with fast Fourier transform (FFT)-based matrix-vector multiplications to solve the generalized eigenvalue problems (GEPs) arising from Yee's discretization. The GEPs are transformed into a null-space free standard eigenvalue problem with a Hermitian positive-definite coefficient matrix. The computation times for FFT-based matrix-vector multiplications with matrices of dimension 7 million are only 0.33 and 3.6 × 10 - 3 seconds using MATLAB with an Intel Xeon CPU and CUDA C++ programming with a single NVIDIA Tesla P100 GPU, respectively. Such multiplications significantly reduce the computational costs of the conjugate gradient method for solving linear systems. We successfully use FAME on a single P100 GPU to solve a set of GEPs with matrices of dimension more than 19 million, in 127 to 191 seconds per problem. These results demonstrate the potential of our proposed package to enable large-scale numerical simulations for novel physical discoveries and engineering applications of photonic crystals.  © 2021 ACM.",,C++ (programming language); Conjugate gradient method; Eigenvalues and eigenfunctions; Fast Fourier transforms; Linear systems; MATLAB; Matrix algebra; Maxwell equations; Three dimensional computer graphics; Vector spaces; Coefficient matrix; Computational costs; Eigenvalue problem; Engineering applications; Generalized eigenvalue problems; Matrix vector multiplication; Positive definite; Three dimensional photonic crystals; Photonic crystals
A Set of Batched Basic Linear Algebra Subprograms and LAPACK Routines,2021,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108906597&doi=10.1145%2f3431921&partnerID=40&md5=599aad90671ff3e74596d80caf882ac3,"This article describes a standard API for a set of Batched Basic Linear Algebra Subprograms (Batched BLAS or BBLAS). The focus is on many independent BLAS operations on small matrices that are grouped together and processed by a single routine, called a Batched BLAS routine. The matrices are grouped together in uniformly sized groups, with just one group if all the matrices are of equal size. The aim is to provide more efficient, but portable, implementations of algorithms on high-performance many-core platforms. These include multicore and many-core CPU processors, GPUs and coprocessors, and other hardware accelerators with floating-point compute facility. As well as the standard types of single and double precision, we also include half and quadruple precision in the standard. In particular, half precision is used in many very large scale applications, such as those associated with machine learning.  © 2021 ACM.",batched BLAS; BLAS,Digital arithmetic; Program processors; Basic linear algebra subprograms; Double precision; Floating points; Hardware accelerators; Large-scale applications; Multi core and many cores; Quadruple precision; Standard type; Matrix algebra
NEP: A Module for the Parallel Solution of Nonlinear Eigenvalue Problems in SLEPc,2021,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108910965&doi=10.1145%2f3447544&partnerID=40&md5=9f88df07333cec9f9f766f58527ad2cb,"SLEPc is a parallel library for the solution of various types of large-scale eigenvalue problems. Over the past few years, we have been developing a module within SLEPc, called NEP, that is intended for solving nonlinear eigenvalue problems. These problems can be defined by means of a matrix-valued function that depends nonlinearly on a single scalar parameter. We do not consider the particular case of polynomial eigenvalue problems (which are implemented in a different module in SLEPc) and focus here on rational eigenvalue problems and other general nonlinear eigenproblems involving square roots or any other nonlinear function. The article discusses how the NEP module has been designed to fit the needs of applications and provides a description of the available solvers, including some implementation details such as parallelization. Several test problems coming from real applications are used to evaluate the performance and reliability of the solvers.  © 2021 ACM.",Eigenvalue computations; message-passing parallelization; nonlinear eigenvalue problem; SLEPc,Rational functions; Large-scale eigenvalue problems; Matrix-valued functions; Nonlinear eigenvalue problem; Nonlinear functions; Parallel solutions; Performance and reliabilities; Polynomial eigenvalue problems; Rational eigenvalue problems; Eigenvalues and eigenfunctions
Medusa: A C++ Library for Solving PDEs Using Strong Form Mesh-free Methods,2021,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108907264&doi=10.1145%2f3450966&partnerID=40&md5=cbfee938977f40d47c6cbc0c7e60beae,"Medusa, a novel library for implementation of non-particle strong form mesh-free methods, such as GFDM or RBF-FD, is described. We identify and present common parts and patterns among many such methods reported in the literature, such as node positioning, stencil selection, and stencil weight computation. Many different algorithms exist for each part and the possible combinations offer a plethora of possibilities for improvements of solution procedures that are far from fully understood. As a consequence there are still many unanswered questions in the mesh-free community resulting in vivid ongoing research in the field. Medusa implements the core mesh-free elements as independent blocks, which offers users great flexibility in experimenting with the method they are developing, as well as easily comparing it with other existing methods. The article describes the chosen abstractions and their usage, illustrates aspects of the philosophy and design, offers some executions time benchmarks and demonstrates the application of the library on cases from linear elasticity and fluid flow in irregular 2D and 3D domains.  © 2021 ACM.",meshless methods; object-oridented programming; PDE; RBF-FD; Strong form mesh-free methods,Benchmarking; Flow of fluids; C++ libraries; Linear elasticity; Mesh free methods; Meshfree; Solution procedure; Stencil selection; Strong form; Time benchmark; Mesh generation
PLANC: Parallel Low-rank Approximation with Nonnegativity Constraints,2021,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108913656&doi=10.1145%2f3432185&partnerID=40&md5=10971ed0babdf25c9ba2820e4abc174e,"We consider the problem of low-rank approximation of massive dense nonnegative tensor data, for example, to discover latent patterns in video and imaging applications. As the size of data sets grows, single workstations are hitting bottlenecks in both computation time and available memory. We propose a distributed-memory parallel computing solution to handle massive data sets, loading the input data across the memories of multiple nodes, and performing efficient and scalable parallel algorithms to compute the low-rank approximation. We present a software package called Parallel Low-rank Approximation with Nonnegativity Constraints, which implements our solution and allows for extension in terms of data (dense or sparse, matrices or tensors of any order), algorithm (e.g., from multiplicative updating techniques to alternating direction method of multipliers), and architecture (we exploit GPUs to accelerate the computation in this work). We describe our parallel distributions and algorithms, which are careful to avoid unnecessary communication and computation, show how to extend the software to include new algorithms and/or constraints, and report efficiency and scalability results for both synthetic and real-world data sets.  © 2021 ACM.",communication-avoiding algorithms; nonnegative least squares; Tensor factorization,Approximation algorithms; Computational efficiency; Memory architecture; Program processors; Tensors; Alternating direction method of multipliers; Distributed Memory; Imaging applications; Low rank approximations; Non-negativity constraints; Parallel com- puting; Parallel distribution; Updating techniques; Approximation theory
Fast Matching Pursuit with Multi-Gabor Dictionaries,2021,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108903847&doi=10.1145%2f3447958&partnerID=40&md5=6712ea699130efc7c296b422d08a17fb,"Finding the best K-sparse approximation of a signal in a redundant dictionary is an NP-hard problem. Suboptimal greedy matching pursuit algorithms are generally used for this task. In this work, we present an acceleration technique and an implementation of the matching pursuit algorithm acting on a multi-Gabor dictionary, i.e., a concatenation of several Gabor-type time-frequency dictionaries, each of which consists of translations and modulations of a possibly different window and time and frequency shift parameters. The technique is based on pre-computing and thresholding inner products between atoms and on updating the residual directly in the coefficient domain, i.e., without the round-trip to the signal domain. Since the proposed acceleration technique involves an approximate update step, we provide theoretical and experimental results illustrating the convergence of the resulting algorithm. The implementation is written in C (compatible with C99 and C++11), and we also provide Matlab and GNU Octave interfaces. For some settings, the implementation is up to 70 times faster than the standard Matching Pursuit Toolkit.  © 2021 ACM.",Gabor dictionary; Greedy approximation; matching pursuit; short-time Fourier transform; time-frequency,MATLAB; Open source software; Acceleration technique; Inner product; Matching pursuit; Matching pursuit algorithms; Redundant dictionaries; Sparse approximations; Time and frequencies; Time frequency; NP-hard
PCPATCH: Software for the Topological Construction of Multigrid Relaxation Methods,2021,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106393728&doi=10.1145%2f3445791&partnerID=40&md5=e1c24ad063955368c27e8a702cc055cd,"Effective relaxation methods are necessary for good multigrid convergence. For many equations, standard Jacobi and Gauß-Seidel are inadequate, and more sophisticated space decompositions are required; examples include problems with semidefinite terms or saddle point structure. In this article, we present a unifying software abstraction, PCPATCH, for the topological construction of space decompositions for multigrid relaxation methods. Space decompositions are specified by collecting topological entities in a mesh (such as all vertices or faces) and applying a construction rule (such as taking all degrees of freedom in the cells around each entity). The software is implemented in PETSc and facilitates the elegant expression of a wide range of schemes merely by varying solver options at runtime. In turn, this allows for the very rapid development of fast solvers for difficult problems.  © 2021 ACM.",finite elements -2pt; Multigrid; parameter-robust preconditioning; relaxation; subspace correction,Degrees of freedom (mechanics); Construction rules; Fast solvers; Multigrid convergences; Relaxation methods; Software abstractions; Space decomposition; Topological constructions; Topological entity; Topology
Algorithm 1017: Fuzzyreg: An R Package for Fitting Fuzzy Regression Models,2021,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103601641&doi=10.1145%2f3451389&partnerID=40&md5=5764756802ea4df0149346c9a08fd648,"Fuzzy regression provides an alternative to statistical regression when the model is indefinite, the relationships between model parameters are vague, the sample size is low, or the data are hierarchically structured. Such cases allow to consider the choice of a regression model based on the fuzzy set theory. In fuzzyreg, we implement fuzzy linear regression methods that differ in the expectations of observational data types, outlier handling, and parameter estimation method. We provide a wrapper function that prepares data for fitting fuzzy linear models with the respective methods from a syntax established in R for fitting regression models. The function fuzzylm thus provides a novel functionality for R through standardized operations with fuzzy numbers. Additional functions allow for conversion of real-value variables to be fuzzy numbers, printing, summarizing, model plotting, and calculation of model predictions from new data using supporting functions that perform arithmetic operations with triangular fuzzy numbers. Goodness of fit and total error of the fit measures allow model comparisons. The package contains a dataset named bats with measurements of temperatures of hibernating bats and the mean annual surface temperature reflecting the climate at the sampling sites. The predictions from fuzzy linear models fitted to this dataset correspond well to the observed biological phenomenon. Fuzzy linear regression has great potential in predictive modeling where the data structure prevents statistical analysis and the modeled process exhibits inherent fuzziness.  © 2021 ACM.",Fuzzy regression; fuzzy set; possibilistic-based fuzzy regression; R; statistics-based fuzzy regression,Fuzzy rules; Fuzzy set theory; Regression analysis; Arithmetic operations; Biological phenomena; Fuzzy linear regression; Fuzzy linear regression method; Fuzzy regression models; Parameter estimation method; Statistical regression; Triangular fuzzy numbers; Predictive analytics
Algorithm 1016: PyMGRIT: A Python Package for the Parallel-in-time Method MGRIT,2021,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105010534&doi=10.1145%2f3446979&partnerID=40&md5=7ca7b364f47b2c0571a072565a136eb1,"In this article, we introduce the Python framework PyMGRIT, which implements the multigrid-reduction-in-time (MGRIT) algorithm for solving (non-)linear systems arising from the discretization of time-dependent problems. The MGRIT algorithm is a reduction-based iterative method that allows parallel-in-time simulations, i.e., calculating multiple time steps simultaneously in a simulation, using a time-grid hierarchy. The PyMGRIT framework includes many different variants of the MGRIT algorithm, ranging from different multigrid cycle types and relaxation schemes, various coarsening strategies, including time-only and space-time coarsening, and the ability to utilize different time integrators on different levels in the multigrid hierachy. The comprehensive documentation with tutorials and many examples and the fully documented code allow an easy start into the work with the package. The functionality of the code is ensured by automated serial and parallel tests using continuous integration. PyMGRIT supports serial runs suitable for prototyping and testing of new approaches, as well as parallel runs using the Message Passing Interface (MPI). In this manuscript, we describe the implementation of the MGRIT algorithm in PyMGRIT and present the usage from both a user and a developer point of view. Three examples illustrate different aspects of the package itself, especially running tests with pure time parallelism, as well as space-time parallelism through the coupling of PyMGRIT with PETSc or Firedrake.  © 2021 ACM.",Multigrid-reduction-in-time (MGRIT); parallel-in-time integration,Coarsening; High level languages; Linear systems; Message passing; Ostwald ripening; Well testing; Algorithm for solving; Comprehensive documentation; Continuous integrations; Message passing interface; Multiple time step; Relaxation schemes; Time integrators; Time-dependent problem; Iterative methods
Algorithm 1015 A Fast Scalable Solver for the Dense Linear (Sum) Assignment Problem,2021,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105004995&doi=10.1145%2f3442348&partnerID=40&md5=d4cf246d5e9efeb5c1809130c97e81ee,"We present a new algorithm for solving the dense linear (sum) assignment problem and an efficient, parallel implementation that is based on the successive shortest path algorithm. More specifically, we introduce the well-known epsilon scaling approach used in the Auction algorithm to approximate the dual variables of the successive shortest path algorithm prior to solving the assignment problem to limit the complexity of the path search. This improves the runtime by several orders of magnitude for hard-to-solve real-world problems, making the runtime virtually independent of how hard the assignment is to find. In addition, our approach allows for using accelerators and/or external compute resources to calculate individual rows of the cost matrix. This enables us to solve problems that are larger than what has been reported in the past, including the ability to efficiently solve problems whose cost matrix exceeds the available systems memory. To our knowledge, this is the first implementation that is able to solve problems with more than one trillion arcs in less than 100 hours on a single machine.  © 2021 ACM.",epsilon scaling; parallel processing; Successive shortest path algorithm,Combinatorial optimization; Graph theory; Algorithm for solving; Assignment problems; Auction algorithms; Compute resources; Orders of magnitude; Parallel implementations; Real-world problem; Shortest path algorithms; Matrix algebra
"Replicated Computational Results (RCR) Report for ""Adaptive Precision Block-Jacobi for High Performance Preconditioning in the Ginkgo Linear Algebra Software""",2021,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104960560&doi=10.1145%2f3446000&partnerID=40&md5=c49d7c3e1125a1a48868556a3d2e56a2,"The article by Flegar et al. titled ""Adaptive Precision Block-Jacobi for High Performance Preconditioning in the Ginkgo Linear Algebra Software""presents a novel, practical implementation of an adaptive precision block-Jacobi preconditioner. Performance results using state-of-the-art GPU architectures for the block-Jacobi preconditioner generation and application demonstrate the practical usability of the method, compared to a traditional full-precision block-Jacobi preconditioner. A production-ready implementation is provided in the Ginkgo numerical linear algebra library. In this report, the Ginkgo library is reinstalled and performance results are generated to perform a comparison to the original results when using Ginkgo's Conjugate Gradient solver with either the full or the adaptive precision block-Jacobi preconditioner for a suite of test problems on an NVIDIA GPU accelerator. After completing this process, the published results are deemed reproducible.  © 2021 Public Domain.",adaptive precision; block-Jacobi preconditioning; GPU; Krylov subspace methods; replicated computational results,Computer software; Software engineering; Computational results; Conjugate-gradient solvers; Jacobi preconditioner; Numerical Linear Algebra; Nvidia gpu; Precision block; State of the art; Test problem; Linear algebra
Improved Arithmetic of Complex Fans,2021,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104938196&doi=10.1145%2f3434400&partnerID=40&md5=949804cee0693df825c8f0f331f2cbfa,"Complex fans are sets of complex numbers whose magnitudes and angles range in closed intervals. The fact that the sum of two fans is a disordered shape gives rise to the need for computational methods to find the minimal enclosing fan. Cases where the sum of two fans contains the origin of the complex plane as a boundary point are of special interest. The result of the addition is then enclosed by circles in current methods, but under certain circumstances this turns out to be an overestimate. The focus of this article is the diagnosis and treatment of such cases.  © 2021 ACM.",complex sets; Interval arithmetic; polar form,Software engineering; Boundary points; Complex number; Complex planes; Computer software
Confidence Intervals for Stochastic Arithmetic,2021,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104964409&doi=10.1145%2f3432184&partnerID=40&md5=f0c62a4c0e04b6cea07228c44d76421f,"Quantifying errors and losses due to the use of Floating-point (FP) calculations in industrial scientific computing codes is an important part of the Verification, Validation, and Uncertainty Quantification process. Stochastic Arithmetic is one way to model and estimate FP losses of accuracy, which scales well to large, industrial codes. It exists in different flavors, such as CESTAC or MCA, implemented in various tools such as CADNA, Verificarlo, or Verrou. These methodologies and tools are based on the idea that FP losses of accuracy can be modeled via randomness. Therefore, they share the same need to perform a statistical analysis of programs results to estimate the significance of the results. In this article, we propose a framework to perform a solid statistical analysis of Stochastic Arithmetic. This framework unifies all existing definitions of the number of significant digits (CESTAC and MCA), and also proposes a new quantity of interest: the number of digits contributing to the accuracy of the results. Sound confidence intervals are provided for all estimators, both in the case of normally distributed results, and in the general case. The use of this framework is demonstrated by two case studies of industrial codes: Europlexus and code_aster.  © 2021 ACM.",confidence intervals; Monte Carlo Arithmetic; numerical analysis; Stochastic arithmetic,Digital arithmetic; Normal distribution; Statistical methods; Stochastic models; Case-studies; Confidence interval; Floating-point calculations; Industrial codes; Quantity of interest; Significant digits; Stochastic arithmetic; Uncertainty quantifications; Stochastic systems
Supporting Mixed-domain Mixed-precision Matrix Multiplication within the BLIS Framework,2021,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104982235&doi=10.1145%2f340222&partnerID=40&md5=c0b94e8594b47946db9e565a16a7dde9,"We approach the problem of implementi ixed-datatype support within the general matrix multiplication (gemm) operation of the BLAS-like Library Instantiation Software framework, whereby each matrix operand A, B, and C may be stored as single- or double-precision real or complex values. Another factor of complexity, whereby the matrix product and accumulation are allowed to take place in a precision different from the storage precisions of either A or B, is also discussed. We first break the problem into orthogonal dimensions, considering the mixing of domains separately from mixing precisions. Support for all combinations of matrix operands stored in either the real or complex domain is mapped out by enumerating the cases and describing an implementation approach for each. Supporting all combinations of storage and computation precisions is handled by typecasting the matrices at key stages of the computation - during packing and/or accumulation, as needed. Several optional optimizations are also documented. Performance results gathered on a 56-core Marvell ThunderX2 and a 52-core Intel Xeon Platinum demonstrate that high performance is mostly preserved, with modest slowdowns incurred from unavoidable typecast instructions. The mixed-datatype implementation confirms that combinatorial intractability is avoided, with the framework relying on only two assembly microkernels to implement 128 datatype combinations.  © 2021 ACM.",BLAS; BLIS; complex; datatype; Dense; DLA; domain; framework; high-performance; libraries; linear algebra; matrix; microkernel; mixed; multiplication; precision; real; type,Computer programming; Mixing; Complex domains; Computation precision; Double precision; Implementation approach; MAtrix multiplication; Matrix products; Mixed precision; Software frameworks; Matrix algebra
fenicsR13: A Tensorial Mixed Finite Element Solver for the Linear R13 Equations Using the FEniCS Computing Platform,2021,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104979070&doi=10.1145%2f3442378&partnerID=40&md5=727a9487b00fbcc810449b117aac31e7,"We present a mixed finite element solver for the linearized regularized 13-moment equations of non-equilibrium gas dynamics. The Python implementation builds upon the software tools provided by the FEniCS computing platform. We describe a new tensorial approach utilizing the extension capabilities of FEniCS' Unified Form Language to define required differential operators for tensors above second degree. The presented solver serves as an example for implementing tensorial variational formulations in FEniCS, for which the documentation and literature seem to be very sparse. Using the software abstraction levels provided by the Unified Form Language allows an almost one-to-one correspondence between the underlying mathematics and the resulting source code. Test cases support the correctness of the proposed method using validation with exact solutions. To justify the usage of extended gas flow models, we discuss typical application cases involving rarefaction effects. We provide the documented and validated solver publicly.  © 2021 ACM.",continuous interior penalty; FEniCS project; R13 equations; Tensorial mixed finite element method,Computer software; Finite element method; Flow of gases; Gas dynamics; Python; Computing platform; Continuous interior penalty; FEniCS project; Finite element solver; Interior penalties; Mixed finite element methods; Mixed finite elements; R13 equation; Tensorial mixed finite element method; Unified form; Mathematical operators
HIPPYlib: An Extensible Software Framework for Large-Scale Inverse Problems Governed by PDEs: Part I: Deterministic Inversion and Linearized Bayesian Inference,2021,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104960511&doi=10.1145%2f3428447&partnerID=40&md5=3e2407c0ce562d0f07178c09f76971af,"We present an extensible software framework, hIPPYlib, for solution of large-scale deterministic and Bayesian inverse problems governed by partial differential equations (PDEs) with (possibly) infinite-dimensional parameter fields (which are high-dimensional after discretization). hIPPYlib overcomes the prohibitively expensive nature of Bayesian inversion for this class of problems by implementing state-of-the-art scalable algorithms for PDE-based inverse problems that exploit the structure of the underlying operators, notably the Hessian of the log-posterior. The key property of the algorithms implemented in hIPPYlib is that the solution of the inverse problem is computed at a cost, measured in linearized forward PDE solves, that is independent of the parameter dimension. The mean of the posterior is approximated by the MAP point, which is found by minimizing the negative log-posterior with an inexact matrix-free Newton-CG method. The posterior covariance is approximated by the inverse of the Hessian of the negative log posterior evaluated at the MAP point. The construction of the posterior covariance is made tractable by invoking a low-rank approximation of the Hessian of the log-likelihood. Scalable tools for sample generation are also discussed. hIPPYlib makes all of these advanced algorithms easily accessible to domain scientists and provides an environment that expedites the development of new algorithms.  © 2021 ACM.",adjoint-based methods; Bayesian inference; generic PDE toolkit; inexact Newton-CG method; Infinite-dimensional inverse problems; low-rank approximation; sampling; uncertainty quantification,Bayesian networks; Computer programming; Differential equations; Inference engines; Linearization; Bayesian inference; Bayesian inversion; Infinite dimensional; Low rank approximations; Partial Differential Equations (PDEs); Sample generations; Scalable algorithms; Software frameworks; Inverse problems
Recycling Krylov Subspaces and Truncating Deflation Subspaces for Solving Sequence of Linear Systems,2021,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104968480&doi=10.1145%2f3439746&partnerID=40&md5=3ac4da07f154ef2d00041dee6ecce017,"This article presents deflation strategies related to recycling Krylov subspace methods for solving one or a sequence of linear systems of equations. Besides well-known strategies of deflation, Ritz-, and harmonic Ritz-based deflation, we introduce an Singular Value Decomposition based deflation technique. We consider the recycling in two contexts: recycling the Krylov subspace between the restart cycles and recycling a deflation subspace when the matrix changes in a sequence of linear systems. Numerical experiments on real-life reservoir simulation demonstrate the impact of our proposed strategy.  © 2021 ACM.",deflation; Krylov subspace; recycling; sequence of linear systems; SVD,Linear systems; Singular value decomposition; Krylov sub spaces; Krylov subspace method; Linear systems of equations; Numerical experiments; Reservoir simulation; Recycling
Adaptive Precision Block-Jacobi for High Performance Preconditioning in the Ginkgo Linear Algebra Software,2021,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102751784&doi=10.1145%2f3441850&partnerID=40&md5=d39eeccc74972df08ad6cd96d6f78cef,"The use of mixed precision in numerical algorithms is a promising strategy for accelerating scientific applications. In particular, the adoption of specialized hardware and data formats for low-precision arithmetic in high-end GPUs (graphics processing units) has motivated numerous efforts aiming at carefully reducing the working precision in order to speed up the computations. For algorithms whose performance is bound by the memory bandwidth, the idea of compressing its data before (and after) memory accesses has received considerable attention. One idea is to store an approximate operator-like a preconditioner-in lower than working precision hopefully without impacting the algorithm output. We realize the first high-performance implementation of an adaptive precision block-Jacobi preconditioner which selects the precision format used to store the preconditioner data on-the-fly, taking into account the numerical properties of the individual preconditioner blocks. We implement the adaptive block-Jacobi preconditioner as production-ready functionality in the Ginkgo linear algebra library, considering not only the precision formats that are part of the IEEE standard, but also customized formats which optimize the length of the exponent and significand to the characteristics of the preconditioner blocks. Experiments run on a state-of-the-art GPU accelerator show that our implementation offers attractive runtime savings.  © 2021 ACM.",adaptive precision; block-Jacobi; GPU; Krylov solvers; preconditioning; Sparse linear algebra,Computer graphics; Graphics processing unit; IEEE Standards; Program processors; High performance implementations; Jacobi preconditioner; Linear algebra libraries; Numerical algorithms; Numerical properties; Precision arithmetic; Scientific applications; Specialized hardware; Linear algebra
An Enhancement of the Bisection Method Average Performance Preserving Minmax Optimality,2021,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099366700&doi=10.1145%2f3423597&partnerID=40&md5=c127cd05a4672870feeaf2ac4f3232b3,"We identify a class of root-searching methods that surprisingly outperform the bisection method on the average performance while retaining minmax optimality. The improvement on the average applies for any continuous distributional hypothesis. We also pinpoint one specific method within the class and show that under mild initial conditions it can attain an order of convergence of up to 1.618, i.e., the same as the secant method. Hence, we attain both an improved average performance and an improved order of convergence with no cost on the minmax optimality of the bisection method. Numerical experiments show that, on regular functions, the proposed method requires a number of function evaluations similar to current state-of-the-art methods, about 24% to 37% of the evaluations required by the bisection procedure. In problems with non-regular functions, the proposed method performs significantly better than the state-of-the-art, requiring on average 82% of the total evaluations required for the bisection method, while the other methods were outperformed by bisection. In the worst case, while current state-of-the-art commercial solvers required two to three times the number of function evaluations of bisection, our proposed method remained within the minmax bounds of the bisection method. © 2020 ACM.",average performance; Bisection method; minmax; order of convergence; regula-falsi method; root searching; worst-case performance; zero finding,Function evaluation; Commercial solvers; Initial conditions; Numerical experiments; Order of convergence; Regular function; Searching methods; State of the art; State-of-the-art methods; Numerical methods
Algorithm 1013: An R Implementation of a Continuous Spectral Algorithm for Simulating Vector Gaussian Random Fields in Euclidean Spaces,2021,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099372262&doi=10.1145%2f3421316&partnerID=40&md5=be016d97362e486bc52f5a7b939eb094,"A continuous spectral algorithm and computer routines in the R programming environment that enable the simulation of second-order stationary and intrinsic (i.e., with second-order stationary increments or generalized increments) vector Gaussian random fields in Euclidean spaces are presented. The simulation is obtained by computing a weighted sum of cosine and sine waves, with weights that depend on the matrix-valued spectral density associated with the spatial correlation structure ofthe random field to simulate. The computational cost is proportional to the number of locations targeted for simulation, below that of sequential, matrix decomposition and discrete spectral algorithms. Also, the implementation is versatile, as there is no restriction on the number of vector components, workspace dimension, number and geometrical configuration of the target locations. The computer routines are illustrated with synthetic examples and statistical testing is proposed to check the normality of the distribution of the simulated random field or of its generalized increments. A by-product of this work is a spectral representation of spherical, cubic, penta, Askey, J-Bessel, Cauchy, Laguerre, hypergeometric, iterated exponential, gamma, and stable covariance models in the d-dimensional Euclidean space. © 2020 ACM.",direct and cross-covariances; generalized covariances; intrinsic random field of order k; matrix-valued spectral density; Stationary random field,Computer programming; Gaussian distribution; Geometry; Matrix algebra; Spectral density; Computational costs; Gaussian random fields; Geometrical configurations; Matrix decomposition; Spatial correlation structures; Spectral representations; Stationary increments; Statistical testing; Vector spaces
Automatic Code Generation for High-performance Discontinuous Galerkin Methods on Modern Architectures,2021,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099381790&doi=10.1145%2f3424144&partnerID=40&md5=5874536447c19b00417f639177527f26,"SIMD vectorization has lately become a key challenge in high-performance computing. However, hand-written explicitly vectorized code often poses a threat to the software's sustainability. In this publication, we solve this sustainability and performance portability issue by enriching the simulation framework dune-pdelab with a code generation approach. The approach is based on the well-known domain-specific language UFL but combines it with loopy, a more powerful intermediate representation for the computational kernel. Given this flexible tool, we present and implement a new class of vectorization strategies for the assembly of Discontinuous Galerkin methods on hexahedral meshesexploiting the finite element's tensor product structure. The performance-optimal variant from thisclass is chosen by the code generator through an auto-tuning approach. The implementation is done within the open source PDE software framework Dune and the discretization module dune-pdelab. The strength of the proposed approach is illustrated with performance measurements for DG schemes for a scalar diffusion reaction equation and the Stokes equation. In our measurements, we utilize both theAVX2 and the AVX512 instruction set, achieving 30% to 40% of the machine's theoretical peak performance for one matrix-free application of the operator. © 2020 Owner/Author.",Code generation; Galerkin methods,Automatic programming; Computer aided software engineering; Open source software; Open systems; Optimal systems; Problem oriented languages; Sustainable development; Automatic code generations; Discontinuous Galerkin methods; Domain specific languages; High performance computing; Intermediate representations; Performance measurements; Performance portability; Tensor product structures; Galerkin methods
H2Pack: High-performance H2 Matrix Package for Kernel Matrices Using the Proxy Point Method,2021,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099373720&doi=10.1145%2f3412850&partnerID=40&md5=c673af21903402e9a8c2c8541716515f,"Dense kernel matrices represented in H2 matrix format typically require less storage and have faster matrix-vector multiplications than when these matrices are represented in the standard dense format. In this article, we present H2Pack, a high-performance, shared-memory library for constructing and operating with H2 matrix representations for kernel matrices definedby non-oscillatory, translationally invariant kernel functions. Using a hybrid analytic-algebraic compression method called the proxy point method, H2Pack can efficiently construct an H2 matrix representation with linear computational complexity. Storage and matrix-vector multiplication also have linear complexity. H2Pack also introduces the concept of ""partially admissible blocks""for H2 matrices to make H2 matrix-vector multiplication mathematically identical to the fast multipole method (FMM) if analytic expansions are used. We optimize H2Pack from both the algorithm and software perspectives. Compared to existing FMM libraries, H2Pack generally has much faster H2 matrix-vector multiplications, since the proxy point method ismore effective at producing block low-rank approximations than the analytic methods used in FMM. Asa tradeoff, H2 matrix construction in H2Pack is typically more expensive than the setup cost in FMM libraries. Thus, H2Pack is ideal for applications that need a large number of matrix-vector multiplications for a given configuration of data points. © 2020 ACM.",fast multipole method; H2 matrix; high-performance computing; N-body problem; proxy point method; Rank-structured matrix,Approximation theory; Digital storage; Libraries; Vectors; Analytic method; Compression methods; Fast multipole method; Kernel matrices; Linear complexity; Low rank approximations; Matrix representation; Matrix vector multiplication; Matrix algebra
Strengths and Limitations of Stretching for Least-squares Problems with Some Dense Rows,2021,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099365064&doi=10.1145%2f3412559&partnerID=40&md5=7f70192487cf7f930ddded9a676db6be,"We recently introduced a sparse stretching strategy for handling dense rows that can arise in large-scale linear least-squares problems and make such problems challenging to solve. Sparse stretching is designed to limit the amount of fill within the stretched normal matrix and hence within the subsequent Cholesky factorization. While preliminary results demonstrated that sparse stretchingperforms significantly better than standard stretching, it has a number of limitations. In this article, we discuss and illustrate these limitations and propose new strategies that are designed to overcome them. Numerical experiments on problems arising from practical applications are used to demonstrate the effectiveness of these new ideas. We consider both direct and preconditioned iterativesolvers. © 2020 Owner/Author.",Cholesky factorization; dense rows; direct solvers; iterative solvers; linear least-squares problems; matrix stretching; normal matrix; Schur complement; Sparse matrices,Software engineering; Cholesky factorizations; Least squares problems; Linear least squares problems; Normal matrices; Numerical experiments; Computer software
"A Flexible, Parallel, Adaptive Geometric Multigrid Method for FEM",2021,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099332934&doi=10.1145%2f3425193&partnerID=40&md5=47680d6633f852c8e2e1778561a1bd81,We present the design and implementation details of a geometric multigrid method on adaptivelyrefined meshes for massively parallel computations. The method uses local smoothing on the refined part of the mesh. Partitioning is achieved by using a space filling curve for the leaf mesh and distributing ancestors in the hierarchy based on the leaves. We present a model of the efficiency of mesh hierarchy distribution and compare its predictions to runtime measurements. The algorithm is implemented as part of the deal.II finite-element library and as such available to the public. ©2020 ACM.,finite-element methods; message passing; Multigrid,Finite element method; Design and implementations; Geometric multigrid methods; Local smoothing; Massively parallels; Run-time measurement; Space-filling curve; Mesh generation
ArborX: A Performance Portable Geometric Search Library,2021,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099363014&doi=10.1145%2f3412558&partnerID=40&md5=71bd175ee80ed9b8cb49436594442c21,"Searching for geometric objects that are close in space is a fundamental component of many applications. The performance of search algorithms comes to the forefront as the size of a problem increases both in terms of total object count as well as in the total number of search queries performed. Scientific applications requiring modern leadership-class supercomputers also pose an additional requirement of performance portability, i.e., being able to efficiently utilize a variety of hardware architectures. In this article, we introduce a new open-source C++ search library, ArborX, which we have designed for modern supercomputing architectures. We examine scalable search algorithms with a focus on performance, including a highly efficient parallel bounding volume hierarchy implementation, and propose a flexible interface making it easy to integrate with existing applications. We demonstrate the performance portability of ArborX on multi-core CPUs and GPUs and compare it to the state-of-the-art libraries such as Boost.Geometry.Index and nanoflann. © 2020 ACM.",Bounding volume hierarchy; geometric search; performance portable algorithm,Learning algorithms; Program processors; Supercomputers; Bounding volume hierarchies; Flexible interfaces; Fundamental component; Geometric objects; Hardware architecture; Performance portability; Scientific applications; Search Algorithms; C++ (programming language)
GetFEM: Automated FE Modeling of Multiphysics Problems Based on a GenericWeak Form Language,2021,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096658007&doi=10.1145%2f3412849&partnerID=40&md5=013a5aa8e440960b025907033b6946d6,"This article presents the major mathematical and implementation features of a weak form language (GWFL) for an automated finite-element (FE) solution of partial differential equation systems. The language is implemented in the GetFEM framework and strategic modeling and software architecturechoices both for the language and the framework are presented in detail. Moreover, conceptual similarities and differences to existing high-level FE frameworks are discussed. Special attention is given to the concept of a generic transformation mechanism that contributes to the high expressive power of GWFL, allowing to interconnect multiple computational domains or parts of the same domain. Finally, the capabilities of the language for expressing strongly coupled multiphysics problems in acompact and readable form are shown by means of modeling examples. © 2020 ACM.",Automated FEM; coupled PDEs; symbolic differentiation; weak form language,Finite element method; Computational domains; Expressive power; FE model; Generic transformations; Multiphysics problems; Partial differential; Strategic modeling; Weak form; Modeling languages
"Algorithm 1014: An Improved Algorithm for hypot(x,y)",2021,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099362352&doi=10.1145%2f3428446&partnerID=40&md5=2260cc3a42301d2c22fdc0b62f5aa502,"We develop fast and accurate algorithms for evaluating 2+y2 for two floating-point numbers x and y. Library functions that perform this computation are generally named hypot(x,y). We compare five approaches that we will develop in this article to the current resident library function that is delivered with Julia 1.1 and to the code that has been distributed with the C math library for decades. We will investigate the accuracy of our algorithms by simulation. © 2020 Public Domain.",floating-point; fused multiply-add; hypot(); IEEE 754,Digital arithmetic; Fast and accurate algorithms; Floating point numbers; Library functions; Math library; Functions
Yet Another Tensor Toolbox for Discontinuous Galerkin Methods and Other Applications,2020,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096829450&doi=10.1145%2f3406835&partnerID=40&md5=c4671d15a0eef5d6c90f42d73de867bf,"The numerical solution of partial differential equations is at the heart of many grand challenges in supercomputing. Solvers based on high-order discontinuous Galerkin (DG) discretisation have been shown to scale on large supercomputers with excellent performance and efficiency if the implementation exploits all levels of parallelism and is tailored to the specific architecture. However, every year new supercomputers emerge and the list of hardware-specific considerations grows simultaneously with the list of desired features in a DG code. Thus, we believe that a sustainable DG code needs an abstraction layer to implement the numerical scheme in a suitable language. We explore the possibility to abstract the numerical scheme as small tensor operations, describe them in a domain-specific language (DSL) resembling the Einstein notation, and to map them to small General Matrix-Matrix Multiplication routines. The compiler for our DSL implements classic optimisations that are used for large tensor contractions, and we present novel optimisation techniques such as equivalent sparsity patterns and optimal index permutations for temporary tensors. Our application examples, which include the earthquake simulation software SeisSol, show that the generated kernels achieve over 50% peak performance of a recent 48-core Skylake system while the DSL considerably simplifies the implementation.  © 2020 Owner/Author.",ADER-DG; finite element method; high-performance computing; Tensor operations,Application programs; Codes (symbols); Computer aided software engineering; Digital subscriber lines; Earthquake engineering; Problem oriented languages; Supercomputers; Tensors; Application examples; Discontinuous galerkin; Discontinuous Galerkin methods; Domain specific languages; Earthquake simulation; Matrix matrix multiplications; Numerical solution of partial differential equations; Optimisation techniques; Galerkin methods
A Feature-complete SPIKE Dense Banded Solver,2020,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096881458&doi=10.1145%2f3410153&partnerID=40&md5=3e41064b2b2f0f3ccf76ba5d88a95759,"This article presents a parallel, effective, and feature-complete recursive SPIKE algorithm that achieves near feature-parity with the standard linear algebra package banded linear system solver. First, we present a flexible parallel implementation of the recursive SPIKE scheme that aims at removing its original limitation that the number of cores/processors be restricted to powers of two. A new transpose solve option for SPIKE is then developed to satisfy a standard requirement of most numerical solver libraries. Finally, a pivoting recursive SPIKE strategy is presented as an alternative to the non-pivoting scheme to improve numerical stability. All these new enhancements lead to the release of a new black-box feature-complete SPIKE-OpenMP package that significantly improves upon the performance and scalability obtained with other state-of-the-art banded solvers.  © 2020 ACM.",banded matrices; linear system solver; SPIKE,Application programming interfaces (API); Linear systems; Black boxes; Linear algebra package; Linear system solver; Numerical solvers; Parallel implementations; Performance and scalabilities; Standard requirements; State of the art; Linear algebra
A Shift Selection Strategy for Parallel Shift-invert Spectrum Slicing in Symmetric Self-consistent Eigenvalue Computation,2020,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096823903&doi=10.1145%2f3409571&partnerID=40&md5=58abaa3b624d843b0671e065f5b546eb,"The central importance of large-scale eigenvalue problems in scientific computation necessitates the development of massively parallel algorithms for their solution. Recent advances in dense numerical linear algebra have enabled the routine treatment of eigenvalue problems with dimensions on the order of hundreds of thousands on the world's largest supercomputers. In cases where dense treatments are not feasible, Krylov subspace methods offer an attractive alternative due to the fact that they do not require storage of the problem matrices. However, demonstration of scalability of either of these classes of eigenvalue algorithms on computing architectures capable of expressing massive parallelism is non-trivial due to communication requirements and serial bottlenecks, respectively. In this work, we introduce the SISLICE method: a parallel shift-invert algorithm for the solution of the symmetric self-consistent field (SCF) eigenvalue problem. The SISLICE method drastically reduces the communication requirement of current parallel shift-invert eigenvalue algorithms through various shift selection and migration techniques based on density of states estimation and k-means clustering, respectively. This work demonstrates the robustness and parallel performance of the SISLICE method on a representative set of SCF eigenvalue problems and outlines research directions that will be explored in future work.  © 2020 ACM.",Eigenvalues; parallel eigenvalue algorithms; self-consistent field; shift-invert spectrum slicing,K-means clustering; Supercomputers; Computing architecture; Eigenvalue algorithms; Eigenvalue computation; Krylov subspace method; Large-scale eigenvalue problems; Numerical Linear Algebra; Scientific computation; Self-consistent field; Eigenvalues and eigenfunctions
Variable Step-Size Control Based on Two-Steps for Radau IIA Methods,2020,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096870672&doi=10.1145%2f3408892&partnerID=40&md5=aea2a260144deacac1c9c5de94ce602b,"Two-step embedded methods of order s based on s-stage Radau IIA formulas are considered for the variable step-size integration of stiff differential equations. These embedded methods are aimed at local error control and are computed through a linear combination of the internal stages of the underlying method in the last two steps. Particular embedded methods for 2 ≤ s ≤ 7 internal stages with good stability properties and damping for the stiff components are constructed. Furthermore, a new formula for step-size change is proposed, having the advantage that it can be applied to any s-stage Radau IIA method. It is shown through numerical testing on some representative stiff problems that the RADAU5 code by Hairer and Wanner with the new strategy performs as well or even better as with the standard one, which is only feasible for an odd number of stages. Numerical experiments support the efficiency and flexibility of the new step-size change strategy.  © 2020 ACM.",differential algebraic equations. AMS classification: 65L06; 65L05; embedded Runge-Kutta methods; local error estimates; Radau IIA formulas; RADAU5; stiff problems; variable step-size implementation,Differential equations; Stability; Good stability; Linear combinations; Local error; Numerical experiments; Numerical testing; Stiff differential equations; Stiff problem; Variable step size; Well testing
Parallel Tree Algorithms for AMR and Non-Standard Data Access,2020,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096851825&doi=10.1145%2f3401990&partnerID=40&md5=d69bd1263d6ae23f38bda93531413932,"We introduce several parallel algorithms operating on a distributed forest of adaptive quadtrees/octrees. They are targeted at large-scale applications relying on data layouts that are more complex than required for standard finite elements, such as hp-adaptive Galerkin methods, particle tracking and semi-Lagrangian schemes, and in-situ post-processing and visualization. Specifically, we design algorithms to derive an adapted worker forest based on sparse data, to identify owner processes in a top-down search of remote objects, and to allow for variable process counts and per-element data sizes in partitioning and parallel file I/O. We demonstrate the algorithms' usability and performance in the context of a particle tracking example that we scale to 21e9 particles and 64Ki MPI processes on the Juqueen supercomputer, and we describe the construction of a parallel assembly of variably sized spheres in space creating up to 768e9 elements on the Juwels supercomputer.  © 2020 ACM.",Adaptive mesh refinement; forest of octrees; particle tracking,Data visualization; Forestry; Galerkin methods; Supercomputers; Large-scale applications; Particle tracking; Post processing; Remote object; Semi-Lagrangian scheme; Standard finite element; Top-down searches; Tree algorithms; In situ processing
Algorithm 1012: DELAUNAYSPARSE: Interpolation via a Sparse Subset of the Delaunay Triangulation in Medium to High Dimensions,2020,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096863286&doi=10.1145%2f3422818&partnerID=40&md5=afa99b50077096156fc6c8d96da18e3e,"DELAUNAYSPARSE contains both serial and parallel codes written in Fortran 2003 (with OpenMP) for performing medium- to high-dimensional interpolation via the Delaunay triangulation. To accommodate the exponential growth in the size of the Delaunay triangulation in high dimensions, DELAUNAYSPARSE computes only a sparse subset of the complete Delaunay triangulation, as necessary for performing interpolation at the user specified points. This article includes algorithm and implementation details, complexity and sensitivity analyses, usage information, and a brief performance study.  © 2020 ACM.",Delaunay triangulation; high-dimensional data; interpolation; multivariate approximation,Application programming interfaces (API); Interpolation; Sensitivity analysis; Algorithm and implementation; Delau-nay triangulations; Exponential growth; High dimensions; High-dimensional; Parallel code; Performance study; Triangulation
Error Analysis and Improving the Accuracy of Winograd Convolution for Deep Neural Networks,2020,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096823313&doi=10.1145%2f3412380&partnerID=40&md5=96642da6f358f2e99dc2bf6930abb899,"Popular deep neural networks (DNNs) spend the majority of their execution time computing convolutions. The Winograd family of algorithms can greatly reduce the number of arithmetic operations required and is used in many DNN software frameworks. However, the performance gain is at the expense of a reduction in floating point (FP) numerical accuracy. In this article, we analyse the worst-case FP error and derive an estimation of the norm and conditioning of the algorithm. We show that the bound grows exponentially with the size of the convolution. Further, the error bound of the modified algorithm is slightly lower but still exponential. We propose several methods for reducing FP error. We propose a canonical evaluation ordering based on Huffman coding that reduces summation error. We study the selection of sampling ""points""experimentally and find empirically good points for the most important sizes. We identify the main factors associated with good points. In addition, we explore other methods to reduce FP error, including mixed-precision convolution, and pairwise summation across DNN channels. Using our methods, we can significantly reduce FP error for a given block size, which allows larger block sizes to be used and reduced computation.  © 2020 ACM.",convolution; deep neural network; Floating point error; numerical analysis; Toom-Cook algorithm; Winograd algorithm,Convolution; Deep neural networks; Digital arithmetic; Errors; Importance sampling; Arithmetic operations; Evaluation order; Floating points; Modified algorithms; Numerical accuracy; Pairwise summations; Performance Gain; Software frameworks; Neural networks
"PHIST: A Pipelined, Hybrid-Parallel Iterative Solver Toolkit",2020,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096837577&doi=10.1145%2f3402227&partnerID=40&md5=d2a7bdd0683d8297ccb98024eba2cbc7,"The increasing complexity of hardware and software environments in high-performance computing poses big challenges on the development of sustainable and hardware-efficient numerical software. This article addresses these challenges in the context of sparse solvers. Existing solutions typically target sustainability, flexibility, or performance, but rarely all of them. Our new library PHIST provides implementations of solvers for sparse linear systems and eigenvalue problems. It is a productivity platform for performance-aware developers of algorithms and application software with abstractions that do not obscure the view on hardware-software interaction. The PHIST software architecture and the PHIST development process were designed to overcome shortcomings of existing packages. An interface layer for basic sparse linear algebra functionality that can be provided by multiple backends ensures sustainability, and PHIST supports common techniques for improving scalability and performance of algorithms such as blocking and kernel fusion. We showcase these concepts using the PHIST implementation of a block Jacobi-Davidson solver for non-Hermitian and generalized eigenproblems. We study its performance on a multi-core CPU, a GPU, and a large-scale many-core system. Furthermore, we show how an existing implementation of a block Krylov-Schur method in the Trilinos package Anasazi can benefit from the performance engineering techniques used in PHIST.  © 2020 ACM.",GPU computing; hybrid parallel computing; Jacobi-Davidson; performance engineering; Sparse eigenvalue solvers,Application programs; Eigenvalues and eigenfunctions; Linear systems; Sustainable development; Generalized eigenproblems; Hardware and software; Hardware-software interactions; High performance computing; Parallel iterative solvers; Performance engineering; Scalability and performance; Sparse linear systems; Iterative methods
New Numerical Algorithm for Deflation of Infinite and Zero Eigenvalues and Full Solution of Quadratic Eigenvalue Problems,2020,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096875805&doi=10.1145%2f3401831&partnerID=40&md5=a305bd0a9580f2acda334047d8c6c9ae,"This article presents a new method for computing all eigenvalues and eigenvectors of quadratic matrix pencil Q(λ)=λ2 M + λ C + K. It is an upgrade of the quadeig algorithm by Hammarlinget al., which attempts to reveal and remove by deflation a certain number of zero and infinite eigenvalues before QZ iterations. Proposed modifications of the quadeig framework are designed to enhance backward stability and to make the process of deflating infinite and zero eigenvalues more numerically robust. In particular, careful preprocessing allows scaling invariant/component-wise backward error and thus a better condition number. Further, using an upper triangular version of the Kronecker canonical form enables deflating additional infinite eigenvalues, in addition to those inferred from the rank of M. Theoretical analysis and empirical evidence from thorough testing of the software implementation confirm superior numerical performances of the proposed method.  © 2020 ACM.",Backward error; infinite eigenvalues,Number theory; Numerical methods; Software testing; Backward stabilities; Eigenvalues and eigenvectors; Kronecker canonical form; Numerical algorithms; Numerical performance; Numerically robust; Quadratic eigenvalue problems; Software implementation; Eigenvalues and eigenfunctions
Faithfully Rounded Floating-point Computations,2020,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092335707&doi=10.1145%2f3290955&partnerID=40&md5=af6290c5a9a3db8625a718e19c101a98,"We present a pair arithmetic for the four basic operations and square root. It can be regarded as a simplified, more-efficient double-double arithmetic. The central assumption on the underlying arithmetic is the first standard model for error analysis for operations on a discrete set of real numbers. Neither do we require a floating-point grid nor a rounding to nearest property. Based on that, we define a relative rounding error unit u and prove rigorous error bounds for the computed result of an arbitrary arithmetic expression depending on u, the size of the expression, and possibly a condition measure. In the second part of this note, we extend the error analysis by examining requirements to ensure faithfully rounded outputs and apply our results to IEEE 754 standard conform floating-point systems. For a class of mathematical expressions, using an IEEE 754 standard conform arithmetic with base β, the result is proved to be faithfully rounded for up to 1 / √βu-2 operations. Our findings cover a number of previously published algorithms to compute faithfully rounded results, among them Horner's scheme, products, sums, dot products, or Euclidean norm. Beyond that, several other problems can be analyzed, such as polynomial interpolation, orientation problems, Householder transformations, or the smallest singular value of Hilbert matrices of large size.  © 2020 ACM.",Double-double; inaccurate cancellation; rigorous error bounds,Error analysis; IEEE Standards; Linear transformations; Mathematical transformations; Arithmetic expression; Basic operation; Floating-point computation; Householder transformation; IEEE-754 standard; Mathematical expressions; Polynomial interpolation; Rounding errors; Digital arithmetic
CGPOPS: A C++ Software for Solving Multiple-Phase Optimal Control Problems Using Adaptive Gaussian Quadrature Collocation and Sparse Nonlinear Programming,2020,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092403640&doi=10.1145%2f3390463&partnerID=40&md5=9881eeb1e5964bb3ac8bbc55299b9ea3,"A general-purpose C++ software program called CGPOPS is described for solving multiple-phase optimal control problems using adaptive direct orthogonal collocation methods. The software employs a Legendre-Gauss-Radau direct orthogonal collocation method to transcribe the continuous optimal control problem into a large sparse nonlinear programming problem (NLP). A class of hp mesh refinement methods are implemented that determine the number of mesh intervals and the degree of the approximating polynomial within each mesh interval to achieve a specified accuracy tolerance. The software is interfaced with the open source Newton NLP solver IPOPT. All derivatives required by the NLP solver are computed via central finite differencing, bicomplex-step derivative approximations, hyper-dual derivative approximations, or automatic differentiation. The key components of the software are described in detail, and the utility of the software is demonstrated on five optimal control problems of varying complexity. The software described in this article provides researchers a transitional platform to solve a wide variety of complex constrained optimal control problems.  © 2020 Owner/Author.",applied mathematics; C++; direct orthogonal collocation; Gaussian quadrature; hp methods; numerical methods; Optimal control; scientific computation,Constrained optimization; Mesh generation; Nonlinear programming; Open source software; Open systems; Optimal control systems; Approximating polynomials; Automatic differentiations; Constrained optimal control problems; Finite differencing; Gaussian quadratures; Optimal control problem; Orthogonal collocation methods; Software program; C++ (programming language)
Algorithm 1011: Improved Invariant Polytope Algorithm and Applications,2020,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092360774&doi=10.1145%2f3408891&partnerID=40&md5=d5a65e40ca89b83d8715bedfb3e89a49,"In several papers of 2013-2016, Guglielmi and Protasov made a breakthrough in the problem of the joint spectral radius computation, developing the invariant polytope algorithm that for most matrix families finds the exact value of the joint spectral radius. This algorithm found many applications in problems of functional analysis, approximation theory, combinatorics, and so on. In this article, we propose a modification of the invariant polytope algorithm making it roughly 3 times faster (single threaded), suitable for higher dimensions, and parallelise it. The modified version works for most matrix families of dimensions up to 25, for non-negative matrices up to 3,000. In addition, we introduce a new, fast algorithm, called modified Gripenberg algorithm, for computing good lower bounds for the joint spectral radius. The corresponding examples and statistics of numerical results are provided. Several applications of our algorithms are presented. In particular, we find the exact values of the regularity exponents of Daubechies wavelets up to order 42 and the capacities of codes that avoid certain difference patterns.  © 2020 ACM.",capacity of codes; daubechies wavelets; invariant polytope algorithm; Joint spectral radius; norm estimation; parallelization,Approximation algorithms; Computation theory; Discrete wavelet transforms; Daubechies Wavelet; Difference patterns; Fast algorithms; Higher dimensions; Joint spectral radius; Non-negative matrix; Numerical results; Polytope algorithms; Matrix algebra
SODECL: An Open-Source Library for Calculating Multiple Orbits of a System of Stochastic Differential Equations in Parallel,2020,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092371213&doi=10.1145%2f3385076&partnerID=40&md5=07abb9e5de97a61be89fdbb4804fa9f6,"Stochastic differential equations (SDEs) are widely used to model systems affected by random processes. In general, the analysis of an SDE model requires numerical solutions to be generated many times over multiple parameter combinations. However, this process often requires considerable computational resources to be practicable. Due to the embarrassingly parallel nature of the task, devices such as multi-core processors and graphics processing units (GPUs) can be employed for acceleration. Here, we present SODECL (https://github.com/avramidis/sodecl), a software library that utilizes such devices to calculate multiple orbits of an SDE model. To evaluate the acceleration provided by SODECL, we compared the time required to calculate multiple orbits of an exemplar stochastic model when one CPU core is used, to the time required when using all CPU cores or a GPU. In addition, to assess scalability, we investigated how model size affected execution time on different parallel compute devices. Our results show that when using all 32 CPU cores of a high-end high-performance computing node, the task is accelerated by a factor of up to ≈6.7, compared to when using a single CPU core. Executing the task on a high-end GPU yielded accelerations of up to ≈4.5, compared to a single CPU core.  © 2020 ACM.",computational biology; CPU; GPU; HPC; Kuramoto model; OpenCL; optimisation; Stochastic differential equations,Computer graphics; Differential equations; Graphics processing unit; Open source software; Open systems; Program processors; Random processes; Stochastic systems; Computational resources; High performance computing; Multi-core processor; Multiple parameters; Numerical solution; Open-source libraries; Software libraries; Stochastic differential equations; Stochastic models
Algorithms for Efficient Reproducible Floating Point Summation,2020,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092332877&doi=10.1145%2f3389360&partnerID=40&md5=d085d294cf052d08641cfa6f2a76983f,"We define ""reproducibility""as getting bitwise identical results from multiple runs of the same program, perhaps with different hardware resources or other changes that should not affect the answer. Many users depend on reproducibility for debugging or correctness. However, dynamic scheduling of parallel computing resources, combined with nonassociative floating point addition, makes reproducibility challenging even for summation, or operations like the BLAS. We describe a ""reproducible accumulator""data structure (the ""binned number"") and associated algorithms to reproducibly sum binary floating point numbers, independent of summation order. We use a subset of the IEEE Floating Point Standard 754-2008 and bitwise operations on the standard representations in memory. Our approach requires only one read-only pass over the data, and one reduction in parallel, using a 6-word reproducible accumulator (more words can be used for higher accuracy), enabling standard tiling optimization techniques. Summing n words with a 6-word reproducible accumulator requires approximately 9n floating point operations (arithmetic, comparison, and absolute value) and approximately 3n bitwise operations. The final error bound with a 6-word reproducible accumulator and our default settings can be up to 229 times smaller than the error bound for conventional (recursive) summation on ill-conditioned double-precision inputs.  © 2020 ACM.",binned number; binned summation; computer arithmetic; floating point number; floating point summation; parallel; reproducibility; Reproducible summation; summation,Computer software; Software engineering; Floating point numbers; Floating point operations; Floating-point addition; Floating-point summation; Hardware resources; IEEE floating point; Optimization techniques; Parallel com- puting; Digital arithmetic
Analysis and Performance Evaluation of Adjoint-guided Adaptive Mesh Refinement for Linear Hyperbolic PDEs Using Clawpack,2020,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092379072&doi=10.1145%2f3392775&partnerID=40&md5=b483c86388bcabb4405a948da42537a6,"Adaptive mesh refinement (AMR) is often used when solving time-dependent partial differential equations using numerical methods. It enables time-varying regions of much higher resolution, which can selectively refine areas to track discontinuities in the solution. The open source Clawpack software implements block-structured AMR to refine around propagating waves in the AMRClaw package. For problems where the solution must be computed over a large domain but is only of interest in a small area, this approach often refines waves that will not impact the target area. We seek a method that enables the identification and refinement of only the waves that will influence the target area. Here we show that solving the time-dependent adjoint equation and using a suitable inner product allows for a more precise refinement of the relevant waves. We present the adjoint methodology in general and give details on the implementation of this method in AMRClaw. Examples and a computational performance analysis for linear acoustics equations are presented. The adjoint method is compared to AMR methods already available in AMRClaw, and the advantages and disadvantages are discussed. The approach presented here is implemented in Clawpack, in Version 5.6.1, and code for all examples presented is archived on Github.  © 2020 ACM.",adaptive mesh refinement; Adjoint problem; AMRClaw; Clawpack; finite volume method; hyperbolic equations,Mesh generation; Numerical analysis; Open source software; Open systems; Adaptive mesh refinement; Adjoint equations; Block structured; Computational performance; Higher resolution; Propagating waves; Software implement; Time-dependent partial differential equations; Numerical methods
"Polynomial Evaluation on Superscalar Architecture, Applied to the Elementary Function ex",2020,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092418140&doi=10.1145%2f3408893&partnerID=40&md5=35e348c265940115c27de7b6c7195f68,"The evaluation of small degree polynomials is critical for the computation of elementary functions. It has been extensively studied and is well documented. In this article, we evaluate existing methods for polynomial evaluation on superscalar architecture. In addition, we have completed this work with a factorization method, which is surprisingly neglected in the literature. This work focuses on out-of-order Intel processors, amongst others, of which computational units are available. Moreover, we applied our work on the elementary function ex that requires, in the current implementation, an evaluation of a polynomial of degree 10 for a satisfying precision and performance. Our results show that the factorization scheme is the fastest in benchmarks, and that latency and throughput are intrinsically dependent on each other on superscalar architecture.  © 2020 Owner/Author.",compute units; elementary function; Polynomial evaluation; superscalar architecture,Architecture; Factorization; Polynomials; Computational units; Elementary function; Factorization methods; Intel processors; Out of order; Polynomial evaluation; Superscalar architecture; Function evaluation
MultiZ: A Library for Computation of High-order Derivatives Using Multicomplex or Multidual Numbers,2020,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083812729&doi=10.1145%2f3378538&partnerID=40&md5=a8b24ba282eab6489d34fe6e96dca860,"Multicomplex and multidual numbers are two generalizations of complex numbers with multiple imaginary axes, useful for numerical computation of derivatives with machine precision. The similarities between multicomplex and multidual algebras allowed us to create a unified library to use either one for sensitivity analysis. This library can be used to compute arbitrary order derivates of functions of a single variable or multiple variables. The storage of matrix representations of multicomplex and multidual numbers is avoided using a combination of one-dimensional resizable arrays and an indexation method based on binary bitwise operations. To provide high computational efficiency and low memory usage, the multiplication of hypercomplex numbers up to sixth order is carried out using a hard-coded algorithm. For higher hypercomplex orders, the library uses by default a multiplication method based on binary bitwise operations. The computation of algebraic and transcendental functions is achieved using a Taylor series approximation. Fortran and Python versions were developed, and extensions to other languages are self-evident.  © 2020 ACM.",Commutative hypercomplex; high order derivatives; hyperdual; multicomplex; multidual,Algebra; One dimensional; Sensitivity analysis; Bitwise operations; High order derivatives; Hypercomplex number; Machine precision; Matrix representation; Numerical computations; Taylor series approximation; Transcendental functions; Computational efficiency
Automating the Formulation and Resolution of Convex Variational Problems,2020,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091487611&doi=10.1145%2f3393881&partnerID=40&md5=762dd404bb1d2643ae475518be270de0,"Convex variational problems arise in many fields ranging from image processing to fluid and solid mechanics communities. Interesting applications usually involve non-smooth terms, which require well-designed optimization algorithms for their resolution. The present manuscript presents the Python package called fenics-optim built on top of the FEniCS finite element software, which enables one to automate the formulation and resolution of various convex variational problems. Formulating such a problem relies on FEniCS domain-specific language and the representation of convex functions, in particular, non-smooth ones, in the conic programming framework. The discrete formulation of the corresponding optimization problems hinges on the finite element discretization capabilities offered by FEniCS, while their numerical resolution is carried out by the interior-point solver Mosek. Through various illustrative examples, we show that convex optimization problems can be formulated using only a few lines of code, discretized in a very simple manner, and solved extremely efficiently.  © 2020 ACM.",conic programming; Convex optimization; FEniCS; finite element method,Computer software; Convex optimization; Functions; Image processing; Problem oriented languages; Convex optimization problems; Discrete formulations; Domain specific languages; Finite element software; Finite-element discretization; Numerical resolution; Optimization algorithms; Optimization problems; Variational techniques
Error Analysis of Some Operations Involved in the Cooley-Tukey Fast Fourier Transform,2020,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086462632&doi=10.1145%2f3368619&partnerID=40&md5=2bd117e8400b3f547a0497410eab2039,"We are interested in obtaining error bounds for the classical Cooley-Tukey fast Fourier transform algorithm in floating-point arithmetic, for the 2-norm as well as for the infinity norm. For that purpose, we also give some results on the relative error of the complex multiplication by a root of unity, and on the largest value that can take the real or imaginary part of one term of the fast Fourier transform of a vector x, assuming that all terms of x have real and imaginary parts less than some value b. © 2020 ACM.",fast Fourier transform; Floating-point arithmetic; rounding error analysis,Digital arithmetic; Error analysis; Complex multiplication; Cooley-Tukey; Error bound; Fast Fourier transform algorithm; Imaginary parts; Infinity norm; Real and imaginary; Relative errors; Fast Fourier transforms
The BLAS API of BLASFEO: Optimizing Performance for Small Matrices,2020,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086463231&doi=10.1145%2f3378671&partnerID=40&md5=9a01d668e63b00b30ce9d0b00025099f,"Basic Linear Algebra Subroutines For Embedded Optimization (BLASFEO) is a dense linear algebra library providing high-performance implementations of BLAS-and LAPACK-like routines for use in embedded optimization and other applications targeting relatively small matrices. BLASFEO defines an application programming interface (API) which uses a packed matrix format as its native format. This format is analogous to the internal memory buffers of optimized BLAS, but it is exposed to the user and it removes the packing cost from the routine call. For matrices fitting in cache, BLASFEO outperforms optimized BLAS implementations, both open source and proprietary. This article investigates the addition of a standard BLAS API to the BLASFEO framework, and proposes an implementation switching between two or more algorithms optimized for different matrix sizes. Thanks to the modular assembly framework in BLASFEO, tailored linear algebra kernels with mixed column-and panel-major arguments are easily developed. This BLAS API has lower performance than the BLASFEO API, but it nonetheless outperforms optimized BLAS and especially LAPACK libraries for matrices fitting in cache. Therefore, it can boost a wide range of applications, where standard BLAS and LAPACK libraries are employed and the matrix size is moderate. In particular, this article investigates the benefits in scientific programming languages such as Octave, SciPy, and Julia. © 2020 ACM.",BLAS; high-performance; libraries; Linear algebra; matrix,Application programming interfaces (API); Libraries; Open source software; Basic linear algebra subroutines; Dense linear algebra; Embedded optimizations; High performance implementations; Internal memory; Modular assembly; Optimizing performance; Scientific programming; Matrix algebra
Algorithm 1010: Boosting Efficiency in Solving Quartic Equations with No Compromise in Accuracy,2020,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086429362&doi=10.1145%2f3386241&partnerID=40&md5=8c7b1eb74d029abf4f80e11be14a7352,"Aiming to provide a very accurate, efficient, and robust quartic equation solver for physical applications, we have proposed an algorithm that builds on the previous works of P. Strobach and S. L. Shmakov. It is based on the decomposition of the quartic polynomial into two quadratics, whose coefficients are first accurately estimated by handling carefully numerical errors and afterward refined through the use of the Newton-Raphson method. Our algorithm is very accurate in comparison with other state-of-the-art solvers that can be found in the literature, but (most importantly) it turns out to be very efficient according to our timing tests. A crucial issue for us is the robustness of the algorithm, i.e., its ability to cope with the detrimental effect of round-off errors, no matter what set of quartic coefficients is provided in a practical application. In this respect, we extensively tested our algorithm in comparison to other quartic equation solvers both by considering specific extreme cases and by carrying out a statistical analysis over a very large set of quartics. Our algorithm has also been heavily tested in a physical application, i.e., simulations of hard cylinders, where it proved its absolute reliability as well as its efficiency. © 2020 ACM.",factorization into quadratics; Newton-Raphson scheme; numerical solver design; performance; Quartic equation,Newton-Raphson method; Numerical methods; Boosting efficiency; Its efficiencies; Numerical errors; Physical application; Quartic coefficients; Quartic polynomial; Round-off errors; State of the art; Efficiency
Algorithm 1009: MieSolver-An Object-Oriented Mie Series Software forWave Scattering by Cylinders,2020,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086442313&doi=10.1145%2f3381537&partnerID=40&md5=85ffcce564df67b2fb3922410897fd39,"MieSolver provides an efficient solver for the problem of wave propagation through a heterogeneous configuration of nonidentical circular cylinders. MieSolver allows great flexibility in the physical properties of each cylinder, and the cylinders may have opaque or penetrable cores, as well as an arbitrary number of penetrable layers. The wave propagation is governed by the two-dimensional Helmholtz equation and models electromagnetic, acoustic, and elastic waves. The solver is based on the Mie series solution for scattering by a single circular cylinder and hence is numerically stable and highly accurate. We demonstrate the accuracy of our software with extensive numerical experiments over a wide range of frequencies (about five orders of magnitude) and up to 60 cylinders. © 2020 ACM.",Helmholtz equation; Mie series,Acoustic wave propagation; Elastic waves; Object oriented programming; Arbitrary number; Efficient solvers; Highly accurate; Non-identical; Numerical experiments; Object oriented; Orders of magnitude; Scattering by cylinders; Circular cylinders
Bidiagonal SVD Computation via an Associated Tridiagonal Eigenproblem,2020,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086436562&doi=10.1145%2f3361746&partnerID=40&md5=c7ca54bafc16e07c48e87345a508e09d,"The Singular Value Decomposition (SVD) is widely used in numerical analysis and scientific computing applications, including dimensionality reduction, data compression and clustering, and computation of pseudo-inverses. In many cases, a crucial part of the SVD of a general matrix is to find the SVD of an associated bidiagonal matrix. This article discusses an algorithm to compute the SVD of a bidiagonal matrix through the eigenpairs of an associated symmetric tridiagonal matrix. The algorithm enables the computation of only a subset of singular values and corresponding vectors, with potential performance gains. The article focuses on a sequential version of the algorithm, and discusses special cases and implementation details. The implementation, called BDSVDX, has been included in the LAPACK library. We use a large set of bidiagonal matrices to assess the accuracy of the implementation, both in single and double precision, as well as to identify potential shortcomings. The results show that BDSVDX can be up to three orders of magnitude faster than existing algorithms, which are limited to the computation of a full SVD. We also show comparisons of an implementation that uses BDSVDX as a building block for the computation of the SVD of general matrices. © 2020 ACM.",design; eigenvalues; eigenvectors; implementation; LAPACK; numerical software; Singular value decomposition,Dimensionality reduction; Bidiagonal matrixes; Building blockes; Double precision; Performance Gain; Scientific computing applications; Singular values; Symmetric tridiagonal matrix; Three orders of magnitude; Singular value decomposition
H-Revolve: A Framework for Adjoint Computation on Synchronous Hierarchical Platforms,2020,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086450970&doi=10.1145%2f3378672&partnerID=40&md5=ea0ce36250af5dcbe4c69b114042e526,"We study the problem of checkpointing strategies for adjoint computation on synchronous hierarchical platforms, specifically computational platforms with several levels of storage with different writing and reading costs. When reversing a large adjoint chain, choosing which data to checkpoint and where is a critical decision for the overall performance of the computation. We introduce H-Revolve, an optimal algorithm for this problem. We make it available in a public Python library along with the implementation of several state-of-the-art algorithms for the variant of the problem with two levels of storage. We provide a detailed description of how one can use this library in an adjoint computation software in the field of automatic differentiation or backpropagation. Finally, we evaluate the performance of H-Revolve and other checkpointing heuristics though an extensive campaign of simulation. © 2020 ACM.",Adjoint computation; automatic differentiation; deep learning; hierarchical memory; revolve,Computer software; Software engineering; Adjoints; Automatic differentiations; Check pointing; Computation software; Computational platforms; Hierarchical platforms; Optimal algorithm; State-of-the-art algorithms; Digital storage
JGraphT-A Java Library for Graph Data Structures and Algorithms,2020,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086435849&doi=10.1145%2f3381449&partnerID=40&md5=70d0c6bbd90a93a7e6d461cd3ec57e67,"Mathematical software and graph-theoretical algorithmic packages to efficiently model, analyze, and query graphs are crucial in an era where large-scale spatial, societal, and economic network data are abundantly available. One such package is JGraphT, a programming library that contains very efficient and generic graph data structures along with a large collection of state-of-the-art algorithms. The library is written in Java with stability, interoperability, and performance in mind. A distinctive feature of this library is its ability to model vertices and edges as arbitrary objects, thereby permitting natural representations of many common networks, including transportation, social, and biological networks. Besides classic graph algorithms such as shortest-paths and spanning-tree algorithms, the library contains numerous advanced algorithms: graph and subgraph isomorphism, matching and flow problems, approximation algorithms for NP-hard problems such as independent set and the traveling salesman problem, and several more exotic algorithms such as Berge graph detection. Due to its versatility and generic design, JGraphT is currently used in large-scale commercial products, as well as noncommercial and academic research projects. In this work, we describe in detail the design and underlying structure of the library, and discuss its most important features and algorithms. A computational study is conducted to evaluate the performance of JGraphT versus several similar libraries. Experiments on a large number of graphs over a variety of popular algorithms show that JGraphT is highly competitive with other established libraries such as NetworkX or the BGL. © 2020 ACM.",Algorithmic library; data structures; graph theory; network analysis,Approximation algorithms; Flow graphs; Graph structures; Java programming language; NP-hard; Product design; Traveling salesman problem; Trees (mathematics); Approximation algorithms for NP-hard problems; Commercial products; Computational studies; Mathematical software; Natural representation; Spanning tree algorithms; State-of-the-art algorithms; Subgraph isomorphism; Graph algorithms
Algorithm 1007: QNSTOP-Quasi-Newton Algorithm for Stochastic Optimization,2020,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086469760&doi=10.1145%2f3374219&partnerID=40&md5=e4da93f7455943963af7b18ce6225e22,"QNSTOP consists of serial and parallel (OpenMP) Fortran 2003 codes for the quasi-Newton stochastic optimization method of Castle and Trosset for stochastic search problems. A complete description of QNSTOP for both local search with stochastic objective and global search with ""noisy"" deterministic objective is given here, to the best of our knowledge, for the first time. For stochastic search problems, some convergence theory exists for particular algorithmic choices and parameter values. Both the parallel driver subroutine, which offers several parallel decomposition strategies, and the serial driver subroutine can be used for local stochastic search or global deterministic search, based on an input switch. Some performance data for computational systems biology problems is given. © 2020 ACM.",Derivative-free; deterministic global optimization; quasi-Newton; response surface methodology; stochastic search,Application programming interfaces (API); Computation theory; Stochastic systems; Computational Systems Biology; Convergence theory; Decomposition strategy; Local stochastic searches; Quasi-newton algorithm; Stochastic optimization methods; Stochastic optimizations; Stochastic search; Optimization
TuckerMPI: A Parallel C++/MPI Software Package for Large-scale Data Compression via the Tucker Tensor Decomposition,2020,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086467473&doi=10.1145%2f3378445&partnerID=40&md5=70723fb3724c1bfdb068e1b8851f584f,"Our goal is compression of massive-scale grid-structured data, such as the multi-terabyte output of a high-fidelity computational simulation. For such data sets, we have developed a new software package called TuckerMPI, a parallel C++/MPI software package for compressing distributed data. The approach is based on treating the data as a tensor, i.e., a multidimensional array, and computing its truncated Tucker decomposition, a higher-order analogue to the truncated singular value decomposition of a matrix. The result is a low-rank approximation of the original tensor-structured data. Compression efficiency is achieved by detecting latent global structure within the data, which we contrast to most compression methods that are focused on local structure. In this work, we describe TuckerMPI, our implementation of the truncated Tucker decomposition, including details of the data distribution and in-memory layouts, the parallel and serial implementations of the key kernels, and analysis of the storage, communication, and computational costs. We test the software on 4.5 and 6.7 terabyte data sets distributed across 100 s of nodes (1,000 s of MPI processes), achieving compression ratios between 100 and 200,000×, which equates to 99-99.999% compression (depending on the desired accuracy) in substantially less time than it would take to even read the same dataset from a parallel file system. Moreover, we show that our method also allows for reconstruction of partial or down-sampled data on a single node, without a parallel computer so long as the reconstructed portion is small enough to fit on a single machine, e.g., in the instance of reconstructing/visualizing a single down-sampled time step or computing summary statistics. The code is available at https://gitlab.com/tensors/TuckerMPI. © 2020 ACM.",higher-order singular value decomposition (HOSVD); tensor decomposition; Tucker decomposition,Approximation theory; C++ (programming language); Data compression ratio; Digital storage; File organization; Singular value decomposition; Software packages; Software testing; Statistical tests; Tensors; Compression efficiency; Computational simulation; Low rank approximations; Multidimensional arrays; Parallel file system; Tensor decomposition; Truncated singular value decomposition; Tucker decompositions; Distributed database systems
"Algorithm 1008: Multicomplex Number Class for Matlab, with a Focus on the Accurate Calculation of Small Imaginary Terms for Multicomplex Step Sensitivity Calculations",2020,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086428629&doi=10.1145%2f3378542&partnerID=40&md5=3ee92d1697615f08c79fe1797a50307a,"A Matlab class for multicomplex numbers was developed with particular attention paid to the robust and accurate handling of small imaginary components. This is primarily to allow the class to be used to obtain n-order derivative information using the multicomplex step method for, among other applications, gradient-based optimization and optimum control problems. The algebra of multicomplex numbers is described, as is its accurate computational implementation, considering small term approximations and the identification of principal values. The implementation of the method in Matlab is studied, and a class definition is constructed. This new class definition enables Matlab to handle n-order multicomplex numbers and perform arithmetic functions. It was found that with this method, the step size could be arbitrarily decreased toward machine precision. Use of the method to obtain up to the seventh derivative of functions is presented, as is timing data to demonstrate the efficiency of the class implementation. © 2020 ACM.",MATLAB class; mulitcomplex step; Multicomplex numbers,Computer software; Software engineering; Accurate calculations; Arithmetic functions; Computational implementations; Derivative information; Gradient-based optimization; Machine precision; Optimum control; Step method; Number theory
Algorithm 1005: Fortran subroutines for reverse mode algorithmic differentiation of BLAS matrix operations,2020,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084763721&doi=10.1145%2f3382191&partnerID=40&md5=3045f83473bc6277f7000b84403a4cec,"A set of Fortran subroutines for reverse mode algorithmic (or automatic) differentiation of the basic linear algebra subprograms (BLAS) is presented. This is preceded by a description of the mathematical tools used to obtain the formulae of these derivatives, with emphasis on special matrices supported by the BLAS: triangular, symmetric, and band. All single and double precision BLAS derivatives have been implemented, together with the Cholesky factorization from Linear Algebra Package (LAPACK). The subroutines are written in Fortran 2003 with a Fortran 77 interface to allow use from C and C++, as well as dynamic languages such as R, Python, Matlab, and Octave. The subroutines are all implemented by calling BLAS, thereby attaining fast runtime. Timing results show derivative runtimes that are about twice those of the corresponding BLAS, in line with theory. The emphasis is on reverse mode because it is more important for the main application that we have in mind, numerical optimization. Two examples are presented, one dealing with the least squares modeling of groundwater, and the other dealing with the maximum likelihood estimation of the parameters of a vector autoregressive time series. The article contains comprehensive tables of formulae for the BLAS derivatives as well as for several non-BLAS matrix operations commonly used in optimization. © 2020 Owner/Author.",AD; algorithmic differentiation; automatic differentiation; matrix operations; Numerical linear algebra; reverse mode; RMAD,C++ (programming language); FORTRAN (programming language); Groundwater; MATLAB; Maximum likelihood estimation; Optimization; Algorithmic differentiations; Autoregressive time series; Basic linear algebra subprograms; Cholesky factorizations; Fortran subroutine; Least squares modeling; Linear algebra package; Numerical optimizations; Matrix algebra
Product algebras for galerkin discretisations of boundary integral operators and their applications,2020,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084749154&doi=10.1145%2f3368618&partnerID=40&md5=bb1f6e30d468fbf745eef7f835a18eb2,"Operator products occur naturally in a range of regularised boundary integral equation formulations. However, while a Galerkin discretisation only depends on the domain space and the test (or dual) space of the operator, products require a notion of the range. In the boundary element software package Bempp, we have implemented a complete operator algebra that depends on knowledge of the domain, range, and test space. The aim was to develop a way of working with Galerkin operators in boundary element software that is as close to working with the strong form on paper as possible, while hiding the complexities of Galerkin discretisations. In this article, we demonstrate the implementation of this operator algebra and show, using various Laplace and Helmholtz example problems, how it significantly simplifies the definition and solution of a wide range of typical boundary integral equation problems. © 2020 ACM.",boundary element software; Boundary integral equations; operator preconditioning,Algebra; Boundary integral equations; Software testing; Boundary integral operators; Discretisation; Helmholtz; Operator algebras; Strong form; Test space; Galerkin methods
"Algorithm 1003: Mongoose, a Graph Coarsening and Partitioning Library",2020,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084754792&doi=10.1145%2f3337792&partnerID=40&md5=e439608848c509aea154febf65068c60,"Partitioning graphs is a common and useful operation in many areas, from parallel computing to VLSI design to sparse matrix algorithms. In this article, we introduce Mongoose, a multilevel hybrid graph partitioning algorithm and library. Building on previous work in multilevel partitioning frameworks and combinatoric approaches, we introduce novel stall-reducing and stall-free coarsening strategies, as well as an efficient hybrid algorithm leveraging (1) traditional combinatoric methods and (2) continuous quadratic programming formulations. We demonstrate how this new hybrid algorithm outperforms either strategy in isolation, and we also compare Mongoose to METIS and demonstrate its effectiveness on large and social networking (power law) graphs. © 2020 ACM.",graph coarsening; Graph partitioning; vertex matching,Coarsening; Graph theory; Ostwald ripening; Quadratic programming; Graph coarsening; Hybrid algorithms; Hybrid graphs; Multi-level partitioning; Partitioning graphs; Power-law; Sparse matrices; VLSI design; Graph algorithms
A software platform for adaptive high order multistep methods,2020,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084748673&doi=10.1145%2f3372159&partnerID=40&md5=0075bc6f253a086aca1889ace5d746ed,"We present a software package, Modes, offering h-adaptive and p-adaptive linear multistep methods for first order initial value problems in ordinary differential equations. The implementation is based on a new parametric, grid-independent representation of multistep methods [Arévalo and Söderlind 2017]. Parameters are supplied for over 60 methods. For nonstiff problems, all maximal order methods (p=k for explicit and p=k+1 for implicit methods) are supported. For stiff computation, implicit methods of order p=k are included. A collection of step-size controllers based on digital filters is provided, generating smooth step-size sequences offering improved computational stability. Controllers may be selected to match method and problem classes. A new system for automatic order control is also provided for designated families of multistep methods, offering simultaneous h- and p-adaptivity. Implemented as a Matlab toolbox, the software covers high order computations with linear multistep methods within a unified, generic framework. Computational experiments show that the new software is competitive and offers qualitative improvements. Modes is available for downloading and is primarily intended as a platform for developing a new generation of state-of-the-art multistep solvers, as well as for true ceteris paribus evaluation of algorithmic components. This also enables method comparisons within a single implementation environment. © 2020 ACM.",multistep methods; ordinary differential equations; Solver; variable order; variable step size,Digital filters; Initial value problems; Ordinary differential equations; Petroleum reservoir evaluation; Computational experiment; Computational stability; Generic frameworks; High-order computations; Linear multistep method; Method comparison; Multi step methods; Software platforms; MATLAB
High-order numerical quadratures in a tetrahedron with an implicitly defined curved interface,2020,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084442189&doi=10.1145%2f3372144&partnerID=40&md5=b4da21c642331a65dffef68421883c39,"Given a shape regular tetrahedron and a curved surface that is defined implicitly by a nonlinear level set function and divides the tetrahedron into two sub-domains, a general-purpose, robust, and high-order numerical algorithm is proposed in this article for computing both volume integrals in the sub-domains and surface integrals on their common boundary. The algorithm uses a direct approach that decomposes 3D volume integrals or 2D surface integrals into multiple 1D integrals and computes the 1D integrals with Gaussian quadratures. It only requires finding roots of univariate nonlinear functions in given intervals and evaluating the integrand, the level set function, and the gradient of the level set function at given points. It can achieve arbitrarily high accuracy by increasing the orders of Gaussian quadratures, and it does not need extra a priori knowledge about the integrand and the level set function. The code for the algorithm is freely available in the open-source finite element toolbox Parallel Hierarchical Grid (PHG) and can serve as a basic building block for implementing 3D high-order numerical algorithms involving implicit interfaces or boundaries. © 2020 ACM.",curved surface; extended finite element; high order; Quadrature; tetrahedral mesh,Geometry; Open systems; Rotational flow; Basic building block; Finite element toolbox; Gaussian quadratures; Level set functions; Nonlinear functions; Numerical algorithms; Numerical quadrature; Regular tetrahedron; Set theory
PETSc DMNetwork: A library for scalable network PDE-Based multiphysics simulations,2020,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084766899&doi=10.1145%2f3344587&partnerID=40&md5=33efb9331ac2b99ffb24b2b339ebebe6,"We present DMNetwork, a high-level package included in the PETSc library for the simulation of multiphysics phenomena over large-scale networked systems. The library aims at applications that have networked structures such as those in electrical, gas, and water distribution systems. DMNetwork provides data and topology management, parallelization for multiphysics systems over a network, and hierarchical and composable solvers to exploit the problem structure. DMNetwork eases the simulation development cycle by providing the necessary infrastructure through simple abstractions to define and query the network components. This article presents the design of DMNetwork, illustrates its user interface, and demonstrates its ability to solve multiphysics systems, such as an electric circuit, a network of power grid and water subnetworks, and transient hydraulic systems over large networks with more than 2 billion variables on extreme-scale computers using up to 30,000 processors. © 2020 ACM.",extreme-scale computing; multiphysics; Networks,Electric power transmission networks; Hierarchical systems; Hydraulic equipment; Phase interfaces; User interfaces; Water distribution systems; Multiphysics phenomena; Multiphysics simulations; Multiphysics systems; Networked structures; Problem structure; Simulation development; Topology management; Transient hydraulics; Information management
Strassen's algorithm reloaded on GPUs,2020,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084736423&doi=10.1145%2f3372419&partnerID=40&md5=ec478c749847da433f75a81ac6ac3a4e,"Conventional Graphics Processing Unit (GPU) implementations of Strassen's algorithm (Strassen) rely on the existing high-performance matrix multiplication (gemm), trading space for time. As a result, such approaches can only achieve practical speedup for relatively large, ""squarish"" matrices due to the extra memory overhead, and their usages are limited due to the considerable workspace. We present novel Strassen primitives for GPUs that can be composed to generate a family of Strassen algorithms. Our algorithms utilize both the memory and thread hierarchies on GPUs, reusing shared memory and register files inherited from gemm, fusing additional operations, and avoiding extra workspace. We further exploit intra- and inter-kernel parallelism by batching, streaming, and employing atomic operations. We develop a performance model for NVIDIA Volta GPUs to select the appropriate blocking parameters and predict the performance for gemm and Strassen. Overall, our 1-level Strassen can achieve up to 1.11× speedup with a crossover point as small as 1,536 compared to cublasSgemm on a NVIDIA Tesla V100 GPU. With additional workspace, our 2-level Strassen can achieve 1.19× speedup with a crossover point at 7,680. © 2020 ACM.",GEMM; GPU; high-performance computing; linear algebra; matrix multiplication; performance optimization; Strassen; Volta,Computer graphics; Computer graphics equipment; Program processors; Atomic operation; Crossover points; Memory overheads; Nvidia teslas; Performance matrices; Performance Model; Register files; Strassen's algorithm; Graphics processing unit
Algorithm 1006: Fast and accurate evaluation of a generalized incomplete gamma function,2020,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084748904&doi=10.1145%2f3365983&partnerID=40&md5=eb92cffa202a070e927146a8ead1188f,"We present a computational procedure to evaluate the integral ĝ«yx sp-1 e-μs ds for 0 ≤ x < y ≤ +∞,μ = ±1, p> 0, which generalizes the lower (x=0) and upper (y=+∞) incomplete gamma functions. To allow for large values of x, y, and p while avoiding under/overflow issues in the standard double precision floating point arithmetic, we use an explicit normalization that is much more efficient than the classical ratio with the complete gamma function. The generalized incomplete gamma function is estimated with continued fractions, with integrations by parts, or, when x ≈ y, with the Romberg numerical integration algorithm. We show that the accuracy reached by our algorithm improves a recent state-of-the-art method by two orders of magnitude, and it is essentially optimal considering the limitations imposed by floating point arithmetic. Moreover, the admissible parameter range of our algorithm (0 ≤ p,x,y ≤ 1015) is much larger than competing algorithms, and its robustness is assessed through massive usage in an image processing application. © 2020 ACM.",continued fraction; Incomplete gamma function; incomplete gamma integral; numerical cancellation; Romberg's method,Digital arithmetic; Image processing; Integral equations; Admissible parameters; Competing algorithms; Complete gamma function; Computational procedures; Image processing applications; Incomplete gamma functions; Numerical-integration algorithms; Orders of magnitude; Function evaluation
Algorithm 1004: The iisignature library: Efficient calculation of iterated-integral signatures and log signatures,2020,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084747788&doi=10.1145%2f3371237&partnerID=40&md5=e97cad4672d8b0b1a6884809a5be5719,"Iterated-integral signatures and log signatures are sequences calculated from a path that characterizes its shape. They originate from the work of K. T. Chen and have become important through Terry Lyons's theory of differential equations driven by rough paths, which is an important developing area of stochastic analysis. They have applications in statistics and machine learning, where there can be a need to calculate finite parts of them quickly for many paths. We introduce the signature and the most basic information (displacement and signed areas) that it contains. We present algorithms for efficiently calculating these signatures. For log signatures this requires consideration of the structure of free Lie algebras. We benchmark the performance of the algorithms. The methods are implemented in C++ and released as a Python extension package, which also supports differentiation. In combination with a machine learning library (Tensorflow, PyTorch, or Theano), this allows end-to-end learning of neural networks involving signatures. © 2020 ACM.",free Lie algebras; Iterated-integral signatures; machine learning; Python library; rough paths,Benchmarking; Differential equations; Machine learning; Stochastic systems; End to end; Finite part; Iterated integrals; Lie Algebra; Stochastic analysis; Cryptography
"Architecture and performance of devito, a system for automated stencil computation",2020,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084748249&doi=10.1145%2f3374916&partnerID=40&md5=43bba48fbd96e8da545b38acf93becef,"Stencil computations are a key part of many high-performance computing applications, such as image processing, convolutional neural networks, and finite-difference solvers for partial differential equations. Devito is a framework capable of generating highly optimized code given symbolic equations expressed in Python, specialized in, but not limited to, affine (stencil) codes. The lowering process - from mathematical equations down to C++ code - is performed by the Devito compiler through a series of intermediate representations. Several performance optimizations are introduced, including advanced common sub-expressions elimination, tiling, and parallelization. Some of these are obtained through well-established stencil optimizers, integrated in the backend of the Devito compiler. The architecture of the Devito compiler, as well as the performance optimizations that are applied when generating code, are presented. The effectiveness of such performance optimizations is demonstrated using operators drawn from seismic imaging applications. © 2020 Owner/Author.",compiler; domain-specific language; Finite-difference method; performance optimization; stencil; structured grid; symbolic processing,Convolutional neural networks; Image processing; Network architecture; Program compilers; Architecture and performance; High-performance computing applications; Intermediate representations; Mathematical equations; Parallelizations; Performance optimizations; Stencil computations; Symbolic equations; C++ (programming language)
Algorithm 1002: Graph coloring based parallel push-relabel algorithm for the maximum flow problem,2019,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076707765&doi=10.1145%2f3330481&partnerID=40&md5=45975ec6e35e4de64dff45761b887b9a,"The maximum flow problem is one of the most common network flow problems. This problem involves finding the maximum possible amount of flow between two designated nodes on a network with arcs having flow capacities. The push-relabel algorithm is one of the fastest algorithms to solve this problem. We present a shared memory parallel push-relabel algorithm. Graph coloring is used to avoid collisions between threads for concurrent push and relabel operations. In addition, excess values of target nodes are updated using atomic instructions to prevent race conditions. The experiments show that our algorithm is competitive for wide graphs with low diameters. Results from three different data sets are included, computer vision problems, DIMACS challenge problems, and KaHIP partitioning problems. These are compared with existing pushrelabel and pseudoflow implementations. We show that high speedup rates are possible using our coloring based parallelization technique on sparse networks. However, we also observe that the pseudoflow algorithm runs faster than the push-relabel algorithm on dense and long networks. © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Minimum cut problem; Preflow-push algorithm; Vertex coloring,Computer software; Software engineering; Computer vision problems; Maximum flow problems; Minimum cut; Parallelization techniques; Partitioning problem; Preflow-push algorithms; Shared-memory parallels; Vertex coloring; Flow graphs
High-performance derivative computations using CoDiPack,2019,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076710395&doi=10.1145%2f3356900&partnerID=40&md5=ec0bf8c926e9ee3cb2d1fce2908f85f3,"There are several AD tools available that all implement different strategies for the reverse mode of AD. The most common strategies are primal value taping (implemented e.g. by ADOL-C) and Jacobian taping (implemented e.g. by Adept and dco/c++). Particulary for Jacobian taping, recent advances using expression templates make it very attractive for large scale software. However, the current implementations are either closed source or miss essential features and flexibility. Therefore, we present the new AD tool CoDiPack (Code Differentiation Package) in this paper. It is specifically designed for minimal memory consumption and optimal runtime, such that it can be used for the differentiation of large scale software. An essential part of the design of CoDiPack is the modular layout and the recursive data structures which not only allow the efficient implementation of the Jacobian taping approach but will also enable other approaches like the primal value taping or new research ideas. We will finally present the performance values of CoDiPack on a generic PDE example and on the SU2 code. © 2019 Association for Computing Machinery.",Algorithmic differentiation; Efficient implementation; Expression templates; Maintainable implementation; Recursive data structures,Data structures; Algorithmic differentiations; Efficient implementation; Expression templates; Maintainable implementation; Recursive data structures; Codes (symbols)
Code generation for generally mapped finite elements,2019,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077742442&doi=10.1145%2f3361745&partnerID=40&md5=fbeeec3a7fbea1ff7b63ee1cad5520bc,"Many classical finite elements such as the Argyris and Bell elements have long been absent from high-level PDE software. Building on recent theoretical work, we describe how to implement very general finite-element transformations in FInAT and hence into the Firedrake finite-element system. Numerical results evaluate the new elements, comparing them to existing methods for classical problems. For a second-order model problem, we find that new elements give smooth solutions at a mild increase in cost over standard Lagrange elements. For fourth-order problems, however, the newly enabled methods significantly outperform interior penalty formulations. We also give some advanced use cases, solving the nonlinear Cahn-Hilliard equation and some biharmonic eigenvalue problems (including Chladni plates) using C1 discretizations. © 2019 Copyright held by the owner/author(s).",Mapped finite elements,Eigenvalues and eigenfunctions; Finite element method; Nonlinear equations; Biharmonic eigenvalue problem; Cahn-Hilliard equation; Classical problems; Element transformations; Finite element system; Fourth order problems; Interior penalties; Second-order models; Numerical methods
On Kummer lines with full rational 2-torsion and their usage in cryptography,2019,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076712395&doi=10.1145%2f3361680&partnerID=40&md5=2568c305a480f16938b86a5b39931d65,"A paper by Karati and Sarkar at Asiacrypt'17 has pointed out the potential for Kummer lines in genus 1, by observing that their SIMD-friendly arithmetic is competitive with the status quo. A more recent preprint explores the connection with (twisted) Edwards curves. In this article, we extend this work and significantly simplify the treatment of Karati and Sarkar. We show that their Kummer line is the x-line of a Montgomery curve translated by a point of order two, and exhibit a natural isomorphism to the y-line of a twisted Edwards curve. Moreover, we show that the Kummer line presented by Gaudry and Lubicz can be obtained via the action of a point of order two on the y-line of an Edwards curve. The maps connecting these curves and lines are all very simple. As a result, a cryptographic implementation can use the arithmetic that is optimal for its instruction set at negligible cost. © 2019 Association for Computing Machinery.",Digital signatures; Edwards curves; Kummer lines; Montgomery curves; Montgomery ladder,Computer software; Electronic document identification systems; Software engineering; Cryptographic implementation; Edwards curves; Instruction set; Kummer lines; Montgomery; Status quo; Cryptography
Algorithm 1000: SuiteSparse: GraphBLAS: Graph algorithms in the language of sparse linear Algebra,2019,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070405697&doi=10.1145%2f3322125&partnerID=40&md5=52f24d2077d2240ae14ff5ff4c2cd306,"SuiteSparse:GraphBLAS is a full implementation of the GraphBLAS standard, which defines a set of sparse matrix operations on an extended algebra of semirings using an almost unlimited variety of operators and types. When applied to sparse adjacency matrices, these algebraic operations are equivalent to computations on graphs. GraphBLAS provides a powerful and expressive framework for creating graph algorithms based on the elegant mathematics of sparse matrix operations on a semiring. An overview of the GraphBLAS specification is given, followed by a description of the key features and performance of its implementation in the SuiteSparse:GraphBLAS package. © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Graph algorithms; GraphBLAS; Sparse matrices,Graph theory; Adjacency matrices; Algebraic operations; Graph algorithms; GraphBLAS; Key feature; Semi-ring; Semirings; Sparse matrices; Matrix algebra
FloatX: A C++ library for customized floating-point arithmetic,2019,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076678288&doi=10.1145%2f3368086&partnerID=40&md5=ca5ba4230611b580c0c88d7a7f5a5988,"We present FloatX (Float eXtended), a C++ framework to investigate the effect of leveraging customized floating-point formats in numerical applications. FloatX formats are based on binary IEEE 754 with smaller significand and exponent bit counts specified by the user. Among other properties, FloatX facilitates an incremental transformation of the code, relies on hardware-supported floating-point types as back-end to preserve efficiency, and incurs no storage overhead. The article discusses in detail the design principles, programming interface, and datatype casting rules behind FloatX. Furthermore, it demonstrates FloatX's usage and benefits via several case studies from well-known numerical dense linear algebra libraries, such as BLAS and LAPACK; the Ginkgo library for sparse linear systems; and two neural network applications related with image processing and text recognition. © 2019 Association for Computing Machinery.",ACM proceedings; LATEX; Text tagging,C++ (programming language); Character recognition; Image processing; Latexes; Linear algebra; Linear systems; ACM proceedings; Dense linear algebra; Incremental transformation; Neural network application; Numerical applications; Programming interface; Sparse linear systems; Text tagging; Digital arithmetic
A wolfe line search algorithm for vector optimization,2019,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074669015&doi=10.1145%2f3342104&partnerID=40&md5=ac710d27e7a24954472f897ee772b553,"In a recent article, Lucambio Pérez and Prudente extended the Wolfe conditions for the vector-valued optimization. Here, we propose a line search algorithm for finding a step size satisfying the strong Wolfe conditions in the vector optimization setting. Well definedness and finite termination results are provided. We discuss practical aspects related to the algorithm and present some numerical experiments illustrating its applicability. Codes supporting this article are written in Fortran 90 and are freely available for download. © 2019 Association for Computing Machinery.",Line search algorithm; Vector optimization; Wolfe conditions,Learning algorithms; Multiobjective optimization; Particle separators; Finite termination; Fortran 90; Line search algorithm; Numerical experiments; Vector optimizations; Vector valued; Wolfe conditions; Wolfe line searches; Vectors
"Replicated computational results (RCR) Report for ""Code generation for generally mapped finite elements""",2019,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076706874&doi=10.1145%2f3360984&partnerID=40&md5=9bbd005d1277aa567c140d0b824df82f,"""Code Generation for Generally Mapped Finite Elements"" includes performance results for the finite element methods discussed in that manuscript. The authors provided a Zenodo archive with the Firedrake components and dependencies used, as well as the scripts that generated the results. The software was installed on two similar platforms; then, new results were gathered and compared to the original results. After completing this process, the results have been deemed replicable by the reviewer. © 2019 Association for Computing Machinery.",Finite element; Partial differential equations; Replicated computational results,Computer software; Partial differential equations; Software engineering; Code Generation; Computational results; New results; Finite element method
Algorithm 1001: IPscatt - A MATLAB toolbox for the inverse medium problem in scatering,2019,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076704173&doi=10.1145%2f3328525&partnerID=40&md5=36210822ff4e30009d2dea53a0296828,"IPscatt is a free, open-source MATLAB toolbox facilitating the solution for time-independent scattering (also known as time-harmonic scattering) in two- and three-dimensional settings. The toolbox has three main application cases: simulation of the scattered field for a given transmitter-receiver geometry; the generation of simulated data as well as the handling of the real-world data from Institute Fresnel; and the reconstruction of the contrast from several measured, scattered fields. In each case, a variety of options tailored to the needs of practitioners is provided. For example, the toolbox allows the simulation of the scattered near field as well as of the far field. Also, it provides methods for the modeling of the incident field as point sources as well as plane waves. Finally, many common geometries of transmitters and receivers are included out of the box. Regarding the reconstruction, the provided functions implement the regularization scheme that relies on a primal-dual algorithm and was introduced by F. Bürgel, K. S. Kazimierski, and A. Lechleiter [Journal of Computational Physics 339 (2017), 1-30]. This article provides a survey of the mathematical concepts in scattering, connects them with the provided implementation, gives an overview of the software framework as well as its application areas, and compares it with existing software packages solving the same problem. © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Denoising; Helmholtz equation; Inverse scattering problem; MATLAB toolbox; Parameter identification; Primal-dual algorithm; Sparsity regularization; Total variation regularization,Application programs; Computer programming; Helmholtz equation; Identification (control systems); MATLAB; Open source software; Transmitters; De-noising; Inverse scattering problems; Matlab toolboxes; Primal dual algorithms; Sparsity regularizations; Total variation regularization; Inverse problems
Algorithm 999: Computation of multi-degree B-splines,2019,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072869739&doi=10.1145%2f3321514&partnerID=40&md5=3a48165e012c3a346794508c99d3887a,Multi-degree splines are smooth piecewise-polynomial functions where the pieces can have different degrees. We describe a simple algorithmic construction of a set of basis functions for the space of multi-degree splines with similar properties to standard B-splines. These basis functions are called multi-degree B-splines (or MDB-splines). The construction relies on an extraction operator that represents all MDB-splines as linear combinations of local B-splines of different degrees. This enables the use of existing efficient algorithms for B-spline evaluations and refinements in the context of multi-degree splines. A Matlab implementation is provided to illustrate the computation and use of MDB-splines. © 2019 Association for Computing Machinery.,B-splines; Extraction operator; MDB-splines; Multi-degree splines,Extraction; Functions; Algorithmic construction; B splines; Basis functions; Linear combinations; Piecewise polynomial functions; Interpolation
Algorithm 997: PysDC—prototyping spectral deferred corrections,2019,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071194780&doi=10.1145%2f3310410&partnerID=40&md5=e82bb8c290081c881483ccc937682527,"In this article, we present the Python framework pySDC for solving collocation problems with spectral deferred correction (SDC) methods and their time-parallel variant PFASST, the parallel full approximation scheme in space and time. pySDC features many implementations of SDC and PFASST, from simple implicit timestepping to high-order implicit-explicit or multi-implicit splitting and multilevel SDCs. The software package comes with many different, preimplemented examples and has seven tutorials to help new users with their first steps. Time parallelism is implemented either in an emulated way for debugging and prototyping or using MPI for benchmarking. The code is fully documented and tested using continuous integration, including most results of previous publications. Here, we describe the structure of the code by taking two different perspectives: those of the user and those of the developer. The first sheds light on the front-end, the examples, and the tutorials, and the second is used to describe the underlying implementation and the data structures. We show three different examples to highlight various aspects of the implementation, the capabilities, and the usage of pySDC. In addition, couplings to the FEniCS framework and PETSc, the latter including spatial parallelism with MPI, are described. © 2019 Copyright held by the owner/author(s).",Multigrid; Parallel-in-time integration; PFASST; Spectral deferred corrections,Software engineering; Continuous integrations; Deferred correction; Full approximation schemes; Implicit-explicit; Multi-grid; PFASST; Spatial parallelism; Time integration; Computer software
Tree partitioning reduction: A new parallel partition method for solving tridiagonal systems,2019,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071184186&doi=10.1145%2f3328731&partnerID=40&md5=91ad71fd5a6e60171e018de3d6b71933,"Solving tridiagonal linear-equation systems is a fundamental computing kernel in a wide range of scientific and engineering applications, and its computation can be modeled with parallel algorithms. These parallel solvers are typically designed to compute problems whose data fit in a common shared-memory space where all the cores taking part in the computation have access. However, when the problem size is large, data cannot be entirely stored in the common shared-memory space, and a high number of high-latency communications are performed. One alternative is to partition the problem among different memory spaces. At this point, conventional parallel algorithms do not facilitate the partition of computation in independent tiles, since each reduction depends on equations that May be in different tiles. This article proposes an algorithm based on a tree reduction, called the Tree Partitioning Reduction (TPR) method, which partitions the problem into independent slices that can be partially computed in parallel within different common shared-memory spaces. The TPR method can be implemented for any parallel and distributed programming paradigm. Furthermore, in this work, TPR is efficiently implemented for CUDA GPUs to solve large size problems, providing highly competitive performance results with respect to existing packages, being, on average, 22.03× faster than CUSPARSE. © 2019 Copyright held by the owner/author(s).",CUDA; CUSPARSE; GPU; Tridiagonal systems; Tuning,Forestry; Graphics processing unit; Linear equations; Memory architecture; Parallel algorithms; Program processors; Tuning; Competitive performance; CUDA; CUSPARSE; Linear equation system; Parallel and distributed programming; Partition methods; Scientific and engineering applications; Tridiagonal systems; Trees (mathematics)
Adjoint code design patterns,2019,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071159689&doi=10.1145%2f3326162&partnerID=40&md5=21b136f96e6202702ae4fd4133c6fbef,"Adjoint methods have become fundamental ingredients of the scientific computing toolbox over the past decades. Large-scale parameter sensitivity analysis, uncertainty quantification, and nonlinear optimization would otherwise turn out computationally infeasible. The symbolic derivation of adjoint mathematical models for relevant problems in science and engineering and their implementation in consistency with the implementation of the underlying primal model frequently proves highly challenging. Hence, an increased interest in algorithmic adjoints can be observed. The algorithmic derivation of adjoint numerical simulation programs shifts some of the problems faced from functional and numerical analysis to computer science. It becomes a highly complex software engineering task requiring expertise in software analysis, transformation, and optimization. Despite rather mature software tool support for algorithmic differentiation, substantial user intervention is typically required when targeting nontrivial numerical programs. A large number of patterns shared by numerous application codes results in repeated duplication of development effort. The adjoint code design patterns introduced in this article aim to reduce this problem through improved formalization from the software engineering perspective. Fully functional reference implementations are provided through github. © 2019 Association for Computing Machinery.",Adjoint; Checkpointing; Design pattern; Implicit function; Preaccumulation,Codes (symbols); Computer aided software engineering; Nonlinear programming; Sensitivity analysis; Adjoints; Check pointing; Design Patterns; Implicit function; Preaccumulation; Uncertainty analysis
Enclosing Chebyshev expansions in linear time,2019,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071175470&doi=10.1145%2f3319395&partnerID=40&md5=59840c76a55763955028120312d53113,"We consider the problem of computing rigorous enclosures for polynomials represented in the Chebyshev basis. Our aim is to compare and develop algorithms with a linear complexity in terms of the polynomial degree. A first category of methods relies on a direct interval evaluation of the given Chebyshev expansion in which Chebyshev polynomials are bounded, e.g., with a divide-and-conquer strategy. Our main category of methods that are based on the Clenshaw recurrence includes interval Clenshaw with defect correction (ICDC), and the spectral transformation of Clenshaw recurrence rewritten as a discrete dynamical system. An extension of the barycentric representation to interval arithmetic is also considered that has a log-linear complexity as it takes advantage of a verified discrete cosine transform. We compare different methods and provide illustrative numerical experiments. In particular, our eigenvalue-based methods are interesting for bounding the range of high-degree interval polynomials. Some of the methods rigorously compute narrow enclosures for high-degree Chebyshev expansions at thousands of points in a few seconds on an average computer. We also illustrate how to employ our methods as an automatic a posteriori forward error analysis tool to monitor the accuracy of the Chebfun feval command. © 2019 Association for Computing Machinery.",Chebfun; Chebyshev polynomials; Clenshaw algorithm; INTLAB; Iterative refinement; Rigorous computing,Discrete cosine transforms; Dynamical systems; Eigenvalues and eigenfunctions; Enclosures; Iterative methods; Numerical methods; Chebfun; Chebyshev polynomials; INTLAB; Iterative refinement; Rigorous computing; Polynomials
High-performance implementation of elliptic curve cryptography using vector instructions,2019,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071175387&doi=10.1145%2f3309759&partnerID=40&md5=467b5b8f885ee46c55860f61c7054aad,"Elliptic curve cryptosystems are considered an efficient alternative to conventional systems such as DSA and RSA. Recently, Montgomery and Edwards elliptic curves have been used to implement cryptosystems. In particular, the elliptic curves Curve25519 and Curve448 were used for instantiating Diffie-Hellman protocols named X25519 and X448. Mapping these curves to twisted Edwards curves allowed deriving two new signature instances, called Ed25519 and Ed448, of the Edwards Digital Signature Algorithm. In this work, we focus on the secure and efficient software implementation of these algorithms using SIMD parallel processing. We present software techniques that target the Intel AVX2 vector instruction set for accelerating prime field arithmetic and elliptic curve operations. Our contributions result in a high-performance software library for AVX2-ready processors. For example, our library computes digital signatures 19% (for Ed25519) and 29% (for Ed448) faster than previous optimized implementations. Also, our library improves by 10% and 20% the execution time of X25519 and X448, respectively. © 2019 Copyright held by the owner/author(s).",AVX2 vector instructions; Diffie-hellman protocol; Ed25519; Edwards digital signature algorithm; Elliptic curve cryptography; Secure software,Authentication; Digital libraries; Public key cryptography; AVX2 vector instructions; Diffie-Hellman protocol; Digital signature algorithms; Ed25519; Elliptic curve cryptography; Secure software; Geometry
"Algorithm 996: BBCpop: A sparse doubly nonnegative relaxation of polynomial optimization problems with binary, box, and complementarity constraints",2019,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071177304&doi=10.1145%2f3309988&partnerID=40&md5=e7bc4c041653212d2356a70629b5355f,"The software package BBCPOP is a MATLAB implementation of a hierarchy of sparse doubly nonnegative relaxations of a class of polynomial optimization (minimization) problems (POPs) with binary, box, and complementarity (BBC) constraints. Given a POP in the class and a relaxation order, BBCPOP constructs a simple conic optimization problem (COP), which serves as a doubly nonnegative relaxation of the POP, and then solves the COP by applying the bisection and projection method. The COP is expressed with a linear objective function and constraints described as a single hyperplane and two cones, which are the Cartesian product of positive semidefinite cones and a polyhedral cone induced from the BBC constraints. BBCPOP aims to compute a tight lower bound for the optimal value of a large-scale POP in the class that is beyond the comfort zone of existing software packages. The robustness, reliability, and efficiency of BBCPOP are demonstrated in comparison to the state-of-the-art software SDP package SDPNAL+ on randomly generated sparse POPs of degree 2 and 3 with up to a few thousands variables, and ones of degree from 5 to 8 with up to a few hundred variables. Numerical results on BBC-constrained POPs that arise from quadratic assignment problems are also reported. The software package BBCPOP is available at https://sites.google.com/site/bbcpop1/. © 2019 Association for Computing Machinery.",Bisection and projection methods; Box and complementarity constraints; Efficiency; Hierarchy of doubly nonnegative relaxations; High-degree polynomial optimization problems with binary; MATLAB software package; Sparsity; Tight lower bounds,Combinatorial optimization; Efficiency; Optimization; Polynomials; Software reliability; Complementarity constraint; Lower bounds; MATLAB software package; Non negatives; Polynomial optimization problem; Projection method; Sparsity; MATLAB
Computing hypergeometric functions rigorously,2019,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071162500&doi=10.1145%2f3328732&partnerID=40&md5=324681616e039aeffd7146fc2bf50c1f,"We present an efficient implementation of hypergeometric functions in arbitrary-precision interval arithmetic. The functions 0F11F12F1, and 2F0 (or the Kummer U-function) are supported for unrestricted complex parameters and argument, and, by extension, we cover exponential and trigonometric integrals, error functions, Fresnel integrals, incomplete gamma and beta functions, Bessel functions, Airy functions, Legendre functions, Jacobi polynomials, complete elliptic integrals, and other special functions. The output can be used directly for interval computations or to generate provably correct floating-point approximations in any format. Performance is competitive with earlier arbitrary-precision software and sometimes orders of magnitude faster. We also partially cover the generalized hypergeometric function pFq and computation of high-order parameter derivatives. © 2019 Copyright held by the owner/author(s).",Arbitrary-precision arithmetic; Automatic differentiation; Bessel functions; Hypergeometric functions; Interval arithmetic; Orthogonal polynomials,Bessel functions; Digital arithmetic; Polynomials; Arbitrary precision; Automatic differentiations; Hypergeometric functions; Interval arithmetic; Orthogonal polynomial; Orthogonal functions
Improving the flexibility and robustness of model-based derivative-free optimization solvers,2019,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071147525&doi=10.1145%2f3338517&partnerID=40&md5=c67086ce66d77561e8bacd23e03f1ba3,"We present two software packages for derivative-free optimization (DFO): DFO-LS for nonlinear least-squares problems and Py-BOBYQA for general objectives, both with optional bound constraints. Inspired by the Gauss-Newton method, DFO-LS constructs simplified linear regression models for the residuals and allows flexible initialization for expensive problems, whereby it can begin making progress after as few as two objective evaluations. Numerical results show DFO-LS can gain reasonable progress on some medium-scale problems with fewer objective evaluations than is needed for one gradient evaluation. DFO-LS has improved robustness to noise, allowing sample averaging, regression-based model construction, and multiple restart strategies with an auto-detection mechanism. Our extensive numerical experimentation shows that restarting the solver when stagnation is detected is a cheap and effective mechanism for achieving robustness, with superior performance over sampling and regression techniques. The package Py-BOBYQA is a Python implementation of BOBYQA (Powell 2009), with novel features such as the implementation of robustness to noise strategies. Our numerical experiments show that Py-BOBYQA is comparable to or better than existing general DFO solvers for noisy problems. In our comparisons, we introduce an adaptive accuracy measure for data profiles of noisy functions, striking a balance between measuring the true and the noisy objective improvement. © 2019 Association for Computing Machinery.",Derivative-free optimization; Least-squares; Mathematical software; Performance evaluation; Stochastic optimization; Trust region methods,Image registration; Least squares approximations; Newton-Raphson method; Regression analysis; Derivative-free optimization; Least Square; Mathematical software; Performance evaluation; Stochastic optimizations; Trust-region methods; Optimization
The implementation of the colored abstract simplicial complex and its application to mesh generation,2019,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071190110&doi=10.1145%2f3321515&partnerID=40&md5=32b0a858a35f46038de48a69be1c43eb,"We introduce the Colored Abstract Simplicial Complex library (CASC): a new, modern, and header-only C++ library that provides a data structure to represent arbitrary dimension abstract simplicial complexes (ASC) with user-defined classes stored directly on the simplices at each dimension. This is accomplished by using the latest C++ language features including variadic template parameters introduced in C++11 and automatic function return type deduction from C++14. Effectively, CASC decouples the representation of the topology from the interactions of user data. We present the innovations and design principles of the data structure and related algorithms. This includes a metadata-aware decimation algorithm, which is general for collapsing simplices of any dimension. We also present an example application of this library to represent an orientable surface mesh. © 2019 Association for Computing Machinery.",Abstract simplicial complexes; C++ library; Mesh decimation; Mesh generation; Molecular modeling; Variadic templates,C++ (programming language); Data structures; Molecular modeling; Arbitrary dimension; C++ libraries; Design Principles; Mesh decimation; Orientable surfaces; Related algorithms; Simplicial complex; Variadic; Mesh generation
Algorithm 995: An efficient parallel anisotropic delaunay mesh generator for two-dimensional finite element analysis,2019,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069432178&doi=10.1145%2f3301321&partnerID=40&md5=46eb4f9040225d459b5639910b290f48,"A bottom-up approach to parallel anisotropic mesh generation is presented by building a mesh generator starting from the basic operations of vertex insertion and Delaunay triangles. Applications focusing on high-lift design or dynamic stall, or numerical methods and modeling test cases, still focus on two-dimensional domains. This automated parallel mesh generation approach can generate high-fidelity unstructured meshes with anisotropic boundary layers for use in the computational fluid dynamics field. The anisotropy requirement adds a level of complexity to a parallel meshing algorithm by making computation depend on the local alignment of elements, which in turn is dictated by geometric boundaries and the density functions—one-dimensional spacing functions generated from an exponential distribution. This approach yields computational savings in mesh generation and flow solution through well-shaped anisotropic triangles instead of isotropic triangles. The validity of the meshes is shown through solution characteristic comparisons to verified reference solutions. A 79% parallel weak scaling efficiency on 1,024 distributed memory nodes, and a 72% parallel efficiency over the fastest sequential isotropic mesh generator on 512 distributed memory nodes, is shown through numerical experiments. © 2019 Association for Computing Machinery.",Anisotropic mesh generation; Boundary layer; Computational geometry; Finite element analysis; Parallel algorithms,Anisotropy; Boundary layers; Computational fluid dynamics; Computational geometry; Efficiency; Finite element method; Memory architecture; Numerical methods; One dimensional; Parallel algorithms; Anisotropic boundary layers; Anisotropic mesh generation; Computational savings; Exponential distributions; Numerical experiments; Solution characteristics; Two-dimensional domain; Two-dimensional finite element analysis; Mesh generation
Fast matrix-free evaluation of discontinuous Galerkin finite element operators,2019,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065799641&doi=10.1145%2f3325864&partnerID=40&md5=b90462b52904cf01899dfe8abeb6f535,"We present an algorithmic framework for matrix-free evaluation of discontinuous Galerkin finite element operators. It relies on fast quadrature with sum factorization on quadrilateral and hexahedral meshes, targeting general weak forms of linear and nonlinear partial differential equations. Different algorithms and data structures are compared in an in-depth performance analysis. The implementations of the local integrals are optimized by vectorization over several cells and faces and an even-odd decomposition of the one-dimensional interpolations. Up to 60% of the arithmetic peak on Intel Haswell, Broadwell, and Knights Landing processors is reached when running from caches and up to 40% of peak when also considering the access to vectors from main memory. On 2 × 14 Broadwell cores, the throughput is up to 2.2 billion unknowns per second for the 3D Laplacian and up to 4 billion unknowns per second for the 3D advection on affine geometries, close to a simple copy operation at 4.7 billion unknowns per second. Our experiments show that MPI ghost exchange has a considerable impact on performance and we present strategies to mitigate this effect. Finally, various options for evaluating geometry terms and their performance are discussed. Our implementations are publicly available through the deal.II finite element library. © 2019 Association for Computing Machinery.",Discontinuous galerkin method; Finite element method; Matrix free method; Parallelization; Sum factorization; Vectorization,Factorization; Finite element method; Galerkin methods; Nonlinear equations; Partial differential equations; Algorithmic framework; Algorithms and data structures; Discontinuous galerkin; Discontinuous Galerkin methods; Matrix-free methods; Nonlinear partial differential equations; Parallelizations; Vectorization; Matrix algebra
Algorithm 998: The robust LMI parser—A toolbox to construct LMI conditions for uncertain systems,2019,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071151132&doi=10.1145%2f3323925&partnerID=40&md5=2f85409ac116a5420feb2c66b94a2367,"The ROLMIP (Robust LMI Parser) is a toolbox specialized in control theory for uncertain linear systems, built to work under MATLAB jointly with YALMIP, to ease the programming of sufficient Linear Matrix Inequality (LMI) conditions that, if feasible, assure the validity of parameter-dependent LMIs in the entire set of uncertainty considered. This article presents the new version of the ROLMIP toolbox, which was completely remodeled to provide a high-level user-friendly interface to cope with distinct uncertain domains (hypercube and multi-simplex) and to treat time-varying parameters in discrete- and continuous-time. By means of simple commands, the user is able to define polynomial matrices as well as to describe the desired parameter-dependent LMIs in an easy way, considerably reducing the programming time to end up with implementable LMI conditions. Therefore, ROLMIP helps the popularization of the state-of-the-art robust control methods for uncertain systems based on LMIs among graduate students, researchers, and engineers in control systems. © 2019 Association for Computing Machinery.",Computational package; LMIs; Parser; Robust control,Computation theory; Continuous time systems; Linear systems; MATLAB; Robust control; Students; Uncertain systems; Uncertainty analysis; Computational package; LMIs; Parameter dependents; Parser; Robust control methods; Time varying parameter; Uncertain linear system; User friendly interface; Linear matrix inequalities
Algorithm 994: Fast implementations of the Brouwer-Zimmermann algorithm for the computation of the minimum distance of a random linear code,2019,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075661730&doi=10.1145%2f3302389&partnerID=40&md5=c7d0cc6c70b6b28929fc67cbe2186d21,"The minimum distance of an error-correcting code is an important concept in information theory. Hence, computing the minimum distance of a code with a minimum computational cost is crucial to many problems in this area. In this article, we present and assess a family of implementations of both the brute-force algorithm and the Brouwer-Zimmermann algorithm for computing the minimum distance of a random linear code over F2 that are faster than current implementations, both in the commercial and public domain. In addition to the basic sequential implementations, we present parallel and vectorized implementations that produce high performances on modern architectures. The attained performance results show the benefits of the developed optimized algorithms, which obtain remarkable improvements compared with state-of-the-art implementations widely used nowadays. © 2019 ACM.",Information theory; Linear codes; Minimum distance,Codes (symbols); Information theory; Network coding; Brute force algorithms; Error correcting code; Fast implementation; Linear codes; Minimum distance; Modern architectures; Optimized algorithms; Sequential implementation; Computation theory
Remark on “Algorithm 680: Evaluation of the complex error function”: Cause and remedy for the loss of accuracy near the real axis,2019,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065775080&doi=10.1145%2f3309681&partnerID=40&md5=545bc04e9b8047dd70248ac626f8cf9c,"In this remark, we identify the cause of the loss of accuracy in the computation of the Faddeyeva function, w(z), near the real axis when using Algorithm 680. We provide a simple correction to this problem that allows us to restore this code as one of the important reference routines for accuracy comparisons. Categories and Subject Descriptors: G.1.0 [Numerical Analysis]: General-Computer Arithmetic; Numerical Algorithms; Multiple Precision Arithmetic; G.1.2 [Numerical Analysis]: Approximation-Special Functions Approximations; G.4 [Mathematical Software]: Algorithm Design and Analysis; Efficiency. © 2019 Copyright held by the owner/author(s).",Dawson function; Error function; Faddeyeva function; Fresnel integrals; Imaginary error function; Special functions evaluation,Approximation algorithms; Errors; Functions; Number theory; Algorithm design and analysis; Complex error functions; Error function; Faddeyeva functions; Fresnel integrals; Mathematical software; Multiple precision arithmetic; Special functions; Function evaluation
Chase: Chebyshev accelerated subspace iteration eigensolver for sequences of Hermitian eigenvalue problems,2019,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065709560&doi=10.1145%2f3313828&partnerID=40&md5=c4b0d4c46db13fd1d33c2af7f358d44c,"Solving dense Hermitian eigenproblems arranged in a sequence with direct solvers fails to take advantage of those spectral properties that are pertinent to the entire sequence and not just to the single problem. When such features take the form of correlations between the eigenvectors of consecutive problems, as is the case in many real-world applications, the potential benefit of exploiting them can be substantial. We present the Chebyshev Accelerated Subspace iteration Eigensolver (ChASE), a modern algorithm and library based on subspace iteration with polynomial acceleration. Novel to ChASE is the computation of the spectral estimates that enter in the filter and an optimization of the polynomial degree that further reduces the necessary floating-point operations. ChASE is written in C++ using the modern software engineering concepts that favor a simple integration in application codes and a straightforward portability over heterogeneous platforms. When solving sequences of Hermitian eigenproblems for a portion of their extremal spectrum, ChASE greatly benefits from the sequence's spectral properties and outperforms direct solvers in many scenarios. The library ships with two distinct parallelization schemes, supports execution over distributed GPUs, and is easily extensible to other parallel computing architectures. © 2019 Association for Computing Machinery.",Eigenvector correlation; Elemental library; Optimized polynomial degree; Spectral density; Subspace iteration,Application programs; C++ (programming language); Computer software portability; Digital arithmetic; Iterative methods; Parallel architectures; Polynomials; Program processors; Spectral density; Eigenvalue problem; Floating point operations; Heterogeneous platforms; Parallel computing architecture; Polynomial degree; Potential benefits; Spectral properties; Subspace iterations; Eigenvalues and eigenfunctions
Algorithm 993: Efficient computation with Kronecker products,2019,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065739644&doi=10.1145%2f3291041&partnerID=40&md5=d887f4b199f2f5c81059d747ea1a476f,"An algorithm for multiplying a chain of Kronecker products by a matrix is described. The algorithm does not require that the Kronecker chain actually be computed and the main computational work is a series of matrix-matrix multiplications. Use of the algorithm can lead to substantial savings in both memory requirements and computational speed. Although similar algorithms have been described before, this article makes two novel contributions. First, it shows how shuffling of data can be (largely) avoided. Second, it provides a simple method to determine the optimal ordering of the workflow. © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Kronecker products; Tensor products,Matrix algebra; Computational speed; Computational work; Efficient computation; Kronecker product; Matrix matrix multiplications; Memory requirements; Optimal ordering; Tensor products; Computational efficiency
Batched triangular dense linear algebra kernels for very small matrix sizes on GPUs,2019,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065711898&doi=10.1145%2f3267101&partnerID=40&md5=445322ab18a757f871bfedb4cafd59ea,"Batched dense linear algebra kernels are becoming ubiquitous in scientific applications, ranging from tensor contractions in deep learning to data compression in hierarchical low-rank matrix approximation. Within a single API call, these kernels are capable of simultaneously launching up to thousands of similar matrix computations, removing the expensive overhead of multiple API calls while increasing the occupancy of the underlying hardware. A challenge is that for the existing hardware landscape (x86, GPUs, etc.), only a subset of the required batched operations is implemented by the vendors, with limited support for very small problem sizes. We describe the design and performance of a new class of batched triangular dense linear algebra kernels on very small data sizes (up to 256) using single and multiple GPUs. By deploying recursive formulations, stressing the register usage, maintaining data locality, reducing threads synchronization, and fusing successive kernel calls, the new batched kernels outperform existing state-of-the-art implementations. © 2019 Association for Computing Machinery.",Batched BLAS kernels; Dense linear algebra; Hardware accelerators; KBLAS; Recursive formulation,Deep learning; Program processors; Batched BLAS kernels; Dense linear algebra; Hardware accelerators; KBLAS; Recursive formulation; Matrix algebra
Plasma: Parallel linear algebra software for multicore using OpenMP,2019,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065709656&doi=10.1145%2f3264491&partnerID=40&md5=94b9b3a194bd770195e3fa7b726677f9,"The recent version of the Parallel Linear Algebra Software for Multicore Architectures (PLASMA) library is based on tasks with dependencies from the OpenMP standard. The main functionality of the library is presented. Extensive benchmarks are targeted on three recent multicore and manycore architectures, namely, an Intel Xeon, Intel Xeon Phi, and IBM POWER 8 processors. © 2019 Copyright held by the owner/author(s).",Multicore processors; Numerical linear algebra libraries; OpenMP; PLASMA; Task-based programming; Tile algorithms,Application programming interfaces (API); Computer architecture; Linear algebra; Parallel processing systems; Plasmas; Software architecture; Many-core architecture; Multi core; Multi-core processor; Multicore architectures; Numerical Linear Algebra; OpenMP; Parallel linear algebras; Task-based; Multicore programming
A QDWH-based SVD software framework on distributed-memory manycore systems,2019,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065714052&doi=10.1145%2f3309548&partnerID=40&md5=c1b7c13a0ccdbbbe3ddec8c88d492074,"This article presents a high-performance software framework for computing a dense SVD on distributed-memory manycore systems. Originally introduced by Nakatsukasa et al. (2010) and Nakatsukasa and Higham (2013), the SVD solver relies on the polar decomposition using the QR Dynamically Weighted Halley algorithm (QDWH). Although the QDWH-based SVD algorithm performs a significant amount of extra floating-point operations compared to the traditional SVD with the one-stage bidiagonal reduction, the inherent high level of concurrency associated with Level 3 BLAS compute-bound kernels ultimately compensates for the arithmetic complexity overhead. Using the ScaLAPACK two-dimensional block cyclic data distribution with a rectangular processor topology, the resulting QDWH-SVD further reduces excessive communications during the panel factorization, while increasing the degree of parallelism during the update of the trailing submatrix, as opposed to relying on the default square processor grid. After detailing the algorithmic complexity and the memory footprint of the algorithm, we conduct a thorough performance analysis and study the impact of the grid topology on the performance by looking at the communication and computation profiling trade-offs. We report performance results against state-of-the-art existing QDWH software implementations (e.g., Elemental) and their SVD extensions on large-scale distributed-memory manycore systems based on commodity Intel x86 Haswell processors and Knights Landing (KNL) architecture. The QDWH-SVD framework achieves up to 3/8-fold speedups on the Haswell/KNL-based platforms, respectively, against ScaLAPACK PDGESVD and turns out to be a competitive alternative for well- and ill-conditioned matrices. We finally come up herein with a performance model based on these empirical results. Our QDWH-based polar decomposition and its SVD extension are freely available at https://github.com/ecrc/qdwh.git and https://github.com/ecrc/ksvd.git, respectively, and have been integrated into the Cray Scientific numerical library LibSci v17.11.1. © 2019 Association for Computing Machinery.",Dense SVD solver; Distributed-memory manycore systems; Performance analysis; Polar decomposition; QDWH,Computational complexity; Computer programming; Digital arithmetic; Economic and social effects; Electronic trading; HTTP; Parallel processing systems; Topology; Dense SVD solver; Manycore systems; Performance analysis; Polar decompositions; QDWH; Memory architecture
Automated tiling of unstructured mesh computations with application to seismological modeling,2019,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065709924&doi=10.1145%2f3302256&partnerID=40&md5=44a2e1341eafa0dd41527b53fea279e4,"Sparse tiling is a technique to fuse loops that access common data, thus increasing data locality. Unlike traditional loop fusion or blocking, the loops may have different iteration spaces and access shared datasets through indirect memory accesses, such as A[map[i]]-hence the name “sparse.” One notable example of such loops arises in discontinuous-Galerkin finite element methods, because of the computation of numerical integrals over different domains (e.g., cells, facets). The major challenge with sparse tiling is implementation-not only is it cumbersome to understand and synthesize, but it is also onerous to maintain and generalize, as it requires a complete rewrite of the bulk of the numerical computation. In this article, we propose an approach to extend the applicability of sparse tiling based on raising the level of abstraction. Through a sequence of compiler passes, the mathematical specification of a problem is progressively lowered, and eventually sparse-tiled C for-loops are generated. Besides automation, we advance the state-of-the-art by introducing a revisited, more efficient sparse tiling algorithm; support for distributed-memory parallelism; a range of fine-grained optimizations for increased runtime performance; implementation in a publicly available library, SLOPE; and an in-depth study of the performance impact in Seigen, a real-world elastic wave equation solver for seismological problems, which shows speed-ups up to 1.28× on a platform consisting of 896 Intel Broadwell cores. © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Compiler; Finite element method; Loop fusion; Loop tiling; Performance optimization; Sparse tiling; Unstructured mesh,C (programming language); Cache memory; Digital libraries; Elastic waves; Galerkin methods; Iterative methods; Mesh generation; Numerical methods; Program compilers; Seismology; Compiler; Loop fusion; Loop tiling; Performance optimizations; Sparse tiling; Unstructured meshes; Finite element method
Client-side computational optimization,2019,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065704506&doi=10.1145%2f3309549&partnerID=40&md5=f44b790f08b76c9f16b64096db8df352,"Mobile platforms have matured to a point where they can provide the infrastructure required to support sophisticated optimization codes. This opens the possibility to envisage new interest for distributed application codes and the opportunity to intensify research on optimization algorithms requiring limited computational resources, as provided by mobile platforms. In this article, we report on some exploratory experience in this area. We illustrate some practical, real-world cases where running optimization programs on mobile or embedded devices can be useful, with particular emphasis on matheuristics approaches. Then, we discuss a practical use case involving the feasibility version of the generalized assignment problem (GAP). We present a JavaScript implementation of a GAP solver that can be executed inside an ordinary browser supporting ECMAScript. We tested the code on different smartphones of varying age and power, as well as on desktop PCs and other embedded devices. Our experiments confirm the viability of mobile devices for computational intensive tasks. © 2019 Association for Computing Machinery.",Client-side computing; Combinatorial optimization; Matheuristics,Combinatorial optimization; Drilling platforms; Client sides; Computational optimization; Computational resources; Distributed applications; Generalized assignment problem; Matheuristics; Optimization algorithms; Optimization programs; Codes (symbols)
Mathematics and speed for interval arithmetic: A complement to IEEE 1788,2019,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065707540&doi=10.1145%2f3264448&partnerID=40&md5=f1c094ad3222ee03ae0b96492aa15e6e,"After a short introduction, the article begins with an axiomatic definition of rounded arithmetic. The concepts of rounding and of rounded arithmetic operations are defined in an axiomatic manner fully independent of special data formats and encodings. Basic properties of floating-point and interval arithmetic can directly be derived from this abstract mathematical model. Interval operations are defined as set operations for elements of the set IR of closed and connected sets of real numbers. As such, they form an algebraically closed subset of the powerset of the real numbers. This property leads to explicit formulas for the arithmetic operations of floating-point intervals of IF, which are executable on the computer. Arithmetic for intervals of IF forms an exception free calculus, i.e., arithmetic operations for intervals of IF always lead to intervals of IF again. Later sections are concerned with programming support and hardware for interval arithmetic. Both are a must and absolutely necessary to move interval arithmetic more into the center of scientific computing. With some minor hardware additions, interval operations can be made as fast as simple floating-point operations. In vector and matrix spaces for real, complex, and interval data, the dot product is a fundamental arithmetic operation. Computing the dot product of two vectors with floating-point components exactly substantially speeds up floating-point and interval arithmetic as well as the accuracy of the computed result. Hardware needed for the exact dot product is very modest. The exact dot product is essential for long real and long interval arithmetic. Section 9 illustrates that interval arithmetic as developed in this article already has a long tradition. Products based on these ideas have been available since 1980. Implementing what the article advocates would have a profound effect on mathematical software. Modern processor architecture from Intel, for example, comes quite close to what is requested in this article. © 2019 Association for Computing Machinery.",Computer hardware; Exact dot product; Super computer,Abstracting; Calculations; Digital arithmetic; Vector spaces; Arithmetic operations; Axiomatic definitions; Exact dot product; Floating point operations; Interval arithmetic; Mathematical software; Modern processors; Programming support; Computer hardware
Spin summations: A high-performance perspective,2019,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065727784&doi=10.1145%2f3301319&partnerID=40&md5=aa5d36936e2b418883ecc0859de69341,"In addition to tensor contractions, one of the most pronounced computational bottlenecks in the nonorthogonally spin-adapted forms of the quantum chemistry methods CCSDT and CCSDTQ, and their approximate forms-including CCSD(T) and CCSDT(Q)-are spin summations. At a first sight, spin summations are operations similar to tensor transpositions, but a closer look reveals additional challenges to high-performance calculations, including temporal locality and scattered memory accesses. This article explores a sequence of algorithmic solutions for spin summations, each exploiting individual properties of either the underlying hardware (e.g., caches, vectorization) or the problem itself (e.g., factorizability). The final algorithm combines the advantages of all the solutions while avoiding their drawbacks; this algorithm achieves high performance through parallelization and vectorization, and by exploiting the temporal locality inherent to spin summations. Combined, these optimizations result in speedups between 2.4× and 5.5× over the NCC quantum chemistry software package. In addition to such a performance boost, our algorithm can perform the spin summations in-place, thus reducing the memory footprint by 2× over an out-of-place variant. © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.",High-performance computing; In-place; Tensor transposition,Computational chemistry; Tensors; Algorithmic solutions; Computational bottlenecks; High performance computing; High-performance calculations; Memory footprint; Quantum chemistry methods; Temporal locality; Tensor contraction; Quantum chemistry
RanduTV: A blocked randomized algorithm for computing a rank-revealing UTV factorization,2019,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065720001&doi=10.1145%2f3242670&partnerID=40&md5=4ee8c3f691f3a39c74bd3de6845ad60b,"A randomized algorithm for computing a so-called UTV factorization efficiently is presented. Given a matrix A, the algorithm “randUTV” computes a factorization A = UTV∗, where U and V have orthonormal columns, and T is triangular (either upper or lower, whichever is preferred). The algorithm randUTV is developed primarily to be a fast and easily parallelized alternative to algorithms for computing the Singular Value Decomposition (SVD). randUTV provides accuracy very close to that of the SVD for problems such as low-rank approximation, solving ill-conditioned linear systems, and determining bases for various subspaces associated with the matrix. Moreover, randUTV produces highly accurate approximations to the singular values of A. Unlike the SVD, the randomized algorithm proposed builds a UTV factorization in an incremental, single-stage, and noniterative way, making it possible to halt the factorization process once a specified tolerance has been met. Numerical experiments comparing the accuracy and speed of randUTV to the SVD are presented. Other experiments also demonstrate that in comparison to column-pivoted QR, which is another factorization that is often used as a relatively economic alternative to the SVD, randUTV compares favorably in terms of speed while providing far higher accuracy. © 2019 Association for Computing Machinery.",High performance; Numerical linear algebra; Randomized methods; Rank-revealing matrix factorization; Singular value decomposition,Approximation algorithms; Approximation theory; Factorization; Linear systems; Numerical methods; High performance; Ill-conditioned linear systems; Low rank approximations; Matrix factorizations; Numerical experiments; Numerical Linear Algebra; Randomized Algorithms; Randomized method; Singular value decomposition
A unified 2D/3D large-scale software environment for nonlinear inverse problems,2019,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065726706&doi=10.1145%2f3291042&partnerID=40&md5=08b6d0efb411a104efc10a457b00fac7,"Large-scale parameter estimation problems are among some of the most computationally demanding problems in numerical analysis. An academic researcher's domain-specific knowledge often precludes that of software design, which results in inversion frameworks that are technically correct but not scalable to realistically sized problems. On the other hand, the computational demands for realistic problems result in industrial codebases that are geared solely for high performance, rather than comprehensibility or flexibility. We propose a new software design for inverse problems constrained by partial differential equations that bridges the gap between these two seemingly disparate worlds. A hierarchical and modular design reduces the cognitive burden on the user while exploiting high-performance primitives at the lower levels. Our code has the added benefit of actually reflecting the underlying mathematics of the problem, which lowers the cognitive load on the user using it and reduces the initial startup period before a researcher can be fully productive. We also introduce a new preconditioner for the 3D Helmholtz equation that is suitable for fault-tolerant distributed systems. Numerical experiments on a variety of 2D and 3D test problems demonstrate the effectiveness of this approach on scaling algorithms from small- to large-scale problems with minimal code changes. © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Acoustic wave equation; Full waveform inversion; Laplace's equation; Non-linear inverse problems; Sparsity-promoting optimization,Acoustic waves; Bridges; Differential equations; Fault tolerant computer systems; Laplace equation; Laplace transforms; Software design; Computational demands; Domain-specific knowledge; Fault tolerant distributed systems; Full-waveform inversion; Non-linear inverse problem; Numerical experiments; Software environments; Sparsity-promoting optimizations; Inverse problems
Extended bacoli: Solving one-dimensional multiscale parabolic PDE systems with error control,2019,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065716241&doi=10.1145%2f3301320&partnerID=40&md5=9a236fd2d1a13ac009b989e7a7beefc7,"BACOLI is a Fortran software package for solving one-dimensional parabolic partial differential equations (PDEs) with separated boundary conditions by B-spline adaptive collocation methods. A distinguishing feature of BACOLI is its ability to estimate and control error and correspondingly adapt meshes in both space and time. Many models of scientific interest, however, can be formulated as multiscale parabolic PDE systems, that is, models that couple a system of parabolic PDEs describing dynamics on a global scale with a system of ordinary differential equations describing dynamics on a local scale. This article describes the Fortran software eBACOLI, the extension of BACOLI to solve such multiscale models. The performance of the extended software is demonstrated to be statistically equivalent to the original for purely parabolic PDE systems. Results from eBACOLI are given for various multiscale models from the extended problem class considered. © 2019 Association for Computing Machinery.",1D parabolic PDEs; Adaptivity; B-spline collocation; Multiscale systems; Numerical software; Spatial error estimation,Boundary conditions; Errors; FORTRAN (programming language); Interpolation; 1-D parabolic PDEs; Adaptivity; B splines; Multi-scale system; Numerical software; Spatial error estimation; Ordinary differential equations
Algorithm 992: An OpenGL- And C++-based function library for curve and surface modeling in a large class of extended chebyshev spaces,2019,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065743666&doi=10.1145%2f3284979&partnerID=40&md5=98fdf1df45530bb893ea2c9a4b4be795,"We propose a platform-independent multi-threaded function library that provides data structures to generate, differentiate, and render both the ordinary basis and the normalized B-basis of a user-specified extended Chebyshev (EC) space that comprises the constants and can be identified with the solution space of a constant-coefficient homogeneous linear differential equation defined on a sufficiently small interval. Using the obtained normalized B-bases, our library can also generate, (partially) differentiate, modify, and visualize a large family of so-called B-curves and tensor product B-surfaces. Moreover, the library also implements methods that can be used to perform dimension elevation, to subdivide B-curves and B-surfaces by means of de Casteljau-like B-algorithms, and to generate basis transformations for the B-representation of arbitrary integral curves and surfaces that are described in traditional parametric form by means of the ordinary bases of the underlying EC spaces. Independently of the algebraic, exponential, trigonometric, or mixed type of the applied EC space, the proposed library is numerically stable and efficient up to a reasonable dimension number and may be useful for academics and engineers in the fields of Approximation Theory, Computer Aided Geometric Design, Computer Graphics, and Isogeometric and Numerical Analysis. © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.",B-curve/surface modeling; Basis transformation; Constant-coefficient homogeneous linear differential equations; Control-point-based exact description; Extended Chebyshev spaces; Normalized B-basis; OpenGL; OpenMP; Order (dimension) elevation; Subdivision (B-algorithm),Application programming interfaces (API); C++ (programming language); Computer aided analysis; Computer aided design; Differential equations; Linear transformations; Mathematical transformations; Three dimensional computer graphics; B-bases; Basis transformation; Chebyshev; Control point; Curve/surface modeling; Linear differential equation; OpenGL; OpenMP; Order (dimension) elevation; Subdivision (B-algorithm); Curve fitting
On quality of implementation of Fortran 2008 complex intrinsic functions on branch cuts,2019,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065772588&doi=10.1145%2f3301318&partnerID=40&md5=95b149b003ad7187ae974c30d441b5b9,"Branch cuts in complex functions have important uses in fracture mechanics, jet flow, and aerofoil analysis. This article introduces tests for validating Fortran 2008 complex functions—LOG, SQRT, ASIN, ACOS, ATAN, ASINH, ACOSH, and ATANH—on branch cuts with arguments of all 3 IEEE floating-point binary formats: binary32, binary64, and binary128, including signed zero and signed infinity. Multiple test failures were revealed, such as wrong signs of results or unexpected overflow, underflow, or NaN. We conclude that the quality of implementation of these Fortran 2008 intrinsics in many compilers is not yet sufficient to remove the need for special code for branch cuts. The electronic appendix contains the full test results with 8 Fortran 2008 compilers: GCC, Flang, Cray, Oracle, PGI, Intel, NAG, and IBM, detailed derivations of the values of these functions on branch cuts and conformal maps of the branch cuts, to be used as a reference. The tests and the results are freely available from https://cmplx.sourceforge.io. This work will be of interest to engineers who use complex functions, as well as to compiler and math library developers. © 2019 Association for Computing Machinery.",ACOS; ACOSH; ASIN; ASINH; ATAN; ATANH; Branch cuts; Fortran; LOG; Signed infinity; Signed zero; SQRT,Conformal mapping; Digital arithmetic; FORTRAN (programming language); Fracture mechanics; Program compilers; ACOS; ACOSH; ASIN; ASINH; ATAN; ATANH; Branch Cuts; Signed infinity; Signed zero; SQRT; Functions
Hierarchical matrix operations on GPUs: Matrix-vector multiplication and compression,2019,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062370047&doi=10.1145%2f3232850&partnerID=40&md5=2b1365274982931bfba24e85d9efa240,"Hierarchical matrices are space- and time-efficient representations of dense matrices that exploit the low-rank structure of matrix blocks at different levels of granularity. The hierarchically low-rank block partitioning produces representations that can be stored and operated on in near-linear complexity instead of the usual polynomial complexity of dense matrices. In this article, we present high-performance implementations of matrix vector multiplication and compression operations for the H 2 variant of hierarchical matrices on GPUs. The H2 variant exploits, in addition to the hierarchical block partitioning, hierarchical bases for the block representations and results in a scheme that requires only O (n) storage and O (n) complexity for the mat-vec and compression kernels. These two operations are at the core of algebraic operations for hierarchical matrices, the mat-vec being a ubiquitous operation in numerical algorithms while compression/recompression represents a key building block for other algebraic operations, which require periodic recompression during execution. The difficulties in developing efficient GPU algorithms come primarily from the irregular tree data structures that underlie the hierarchical representations, and the key to performance is to recast the computations on flattened trees in ways that allow batched linear algebra operations to be performed. This requires marshaling the irregularly laid out data in a way that allows them to be used by the batched routines. Marshaling operations only involve pointer arithmetic with no data movement and as a result have minimal overhead. Our numerical results on covariance matrices from 2D and 3D problems from spatial statistics show the high efficiency our routines achieve over 550GB/s for the bandwidth-limited matrix-vector operation and over 850GFLOPS/s in sustained performance for the compression operation on the P100 Pascal GPU. © 2019 Association for Computing Machinery.",CUDA; GPU; Hierarchical matrices; Manycore algorithms; Matrix compression; Matvec,Bandwidth compression; Digital storage; Forestry; Graphics processing unit; Program processors; Trees (mathematics); CUDA; Hierarchical matrices; Many-core; Matrix compression; Matvec; Covariance matrix
Polar affine arithmetic: Optimal affine approximation and operation development for computation in polar form under uncertainty,2019,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062335564&doi=10.1145%2f3274659&partnerID=40&md5=2bbd7078875f7e57c24b381ecc75e2a8,"Uncertainties practically arise from numerous factors, such as ambiguous information, inaccurate model, and environment disturbance. Interval arithmetic has emerged to solve problems with uncertain parameters, especially in the computational process where only the upper and lower bounds of parameters can be ascertained. In rectangular coordinate systems, the basic interval operations and improved interval algorithms have been developed in the numerical analysis. However, in polar coordinate systems, interval arithmetic still suffers from issues of complex computation and overestimation. This article defines a polar affine variable and develops a polar affine arithmetic (PAA) that extends affine arithmetic to the polar coordinate systems, which performs better in many aspects than the corresponding polar interval arithmetic (PIA). Basic arithmetic operations are developed based on the complex affine arithmetic. The Chebyshev approximation theory and the min-range approximation theory are used to identify the best affine approximation. PAA can accurately keep track of the interdependency among multiple variables throughout the calculation procedure, which prominently reduces the solution conservativeness. Numerical examples implemented in MATLAB programs show that, compared with benchmark results from the Monte Carlo method, the proposed PAA ensures completeness of the exact solution and presents a more compact solution region than PIA when dependency exists in the calculation process. Meanwhile, a comparison of affine arithmetic in polar and rectangular coordinates is presented. An application of PAA in circuit analysis is quantitatively presented and potential applications in other research fields involving complex variables in polar form will be gradually developed. © 2019 Association for Computing Machinery.",Affine approximation method; Monte Carlo sample method; Operation development; Polar affine arithmetic; Polar interval arithmetic; Uncertainty,Approximation theory; Chebyshev approximation; Computation theory; MATLAB; Numerical methods; Uncertainty analysis; Affine approximation; Affine arithmetic; Operation development; Polar intervals; Uncertainty; Monte Carlo methods
Verified Newton-Raphson iteration for multiplicative inverses modulo powers of any base,2019,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062350494&doi=10.1145%2f3301317&partnerID=40&md5=922eda681c43dc9e800733e936ad1bad,"We identify two faults in a published algorithm for fast computation of multiplicative inverses modulo prime powers. We patch the algorithm and present machine-assisted proofs of correctness of the repair. Our formal proofs also reveal that being prime is an unnecessary demand for the power base, thus attributing a wider scope of applications to the repaired algorithm. © 2019 Association for Computing Machinery.",Modular arithmetic; Multiplicative inverses; Newton-Raphson iteration; Theorem proving by induction,Computation theory; Fast computation; Formal proofs; Modular arithmetic; Multiplicative inverse; Newton Raphson iteration; Scope of application; Iterative methods
Computing the braid monodromy of completely reducible n-gonal curves,2019,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062353448&doi=10.1145%2f3291040&partnerID=40&md5=a8161bae0b955c06c93fb8fcb1b3263f,"Braid monodromy is an important tool for computing invariants of curves and surfaces. In this paper, the rectangular braid diagram (RBD) method is proposed to compute the braid monodromy of a completely reducible n-gonal curve, i.e., the curves in the form (y − y1(x))...(y − yn(x)) = 0, where n ∈ Z+ and yi ∈ C[x]. Also, an algorithm is presented to compute the Alexander polynomial of these curve complements using Burau representations of braid groups. Examples for each computation are provided. © 2019 Association for Computing Machinery.",Alexander polynomial; Braid monodromy; Buraure presentation; N-gonal curve,Computer software; Software engineering; Alexander polynomials; Braid groups; Buraure presentation; Curves and surfaces; Monodromy; Weaving
Performance and scalability of the block low-rank multifrontal factorization on multicore architectures,2019,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062349239&doi=10.1145%2f3242094&partnerID=40&md5=177b0018457f5a0983a6386c0337a6e2,"Matrices coming from elliptic Partial Differential Equations have been shown to have a low-rank property that can be efficiently exploited in multifrontal solvers to provide a substantial reduction of their complexity. Among the possible low-rank formats, the Block Low-Rank format (BLR) is easy to use in a general purpose multifrontal solver and its potential compared to standard (full-rank) solvers has been demonstrated. Recently, new variants have been introduced and it was proved that they can further reduce the complexity but their performance has never been analyzed. In this article, we present a multithreaded BLR factorization and analyze its efficiency and scalability in shared-memory multicore environments. We identify the challenges posed by the use of BLR approximations in multifrontal solvers and put forward several algorithmic variants of the BLR factorization that overcome these challenges by improving its efficiency and scalability. We illustrate the performance analysis of the BLR multifrontal factorization with numerical experiments on a large set of problems coming from a variety of real-life applications. © 2019 Association for Computing Machinery.",Block Low-Rank; Multicore architectures; Multifrontal factorization; Sparse linear algebra,Efficiency; Linear algebra; Memory architecture; Partial differential equations; Scalability; Software architecture; Block Low-Rank; Elliptic partial differential equation; Multicore architectures; Multicore environments; Multifrontal; Performance and scalabilities; Real-life applications; Substantial reduction; Factorization
Algorithm 991: The 2D tree sliding window discrete fourier transform,2019,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062367949&doi=10.1145%2f3264426&partnerID=40&md5=d261b5e39ea96ae94efc9c54eb8e7d23,"We present a new algorithm for the 2D sliding window discrete Fourier transform. Our algorithm avoids repeating calculations in overlapping windows by storing them in a tree data-structure based on the ideas of the Cooley-Tukey fast Fourier transform. For an N0 × N1 array and n0 × n1 windows, our algorithm takes O(N0N1n0n1) operations. We provide a C implementation of our algorithm for the Radix-2 case, compare ours to existing algorithms, and show how our algorithm easily extends to higher dimensions. © 2019 Copyright held by the owner/author(s).",Data Structure; Fast Fourier Transform,Data structures; Discrete Fourier transforms; Fast Fourier transforms; Forestry; Cooley-Tukey; Higher dimensions; Overlapping window; Radix 2; Sliding Window; Tree data structures; Trees (mathematics)
A note on using performance and data profiles for training algorithms,2019,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065609489&doi=10.1145%2f3310362&partnerID=40&md5=3bc4a2335d2e0dc68a228c9bfe6d699a,"This article shows how to use performance and data profile benchmarking tools to improve the performance of algorithms. We propose to achieve this goal by defining and approximately solving suitable optimization problems involving the parameters of the algorithm under consideration. Because these problems do not have derivatives and may involve integer variables, we suggest using a mixed-integer derivative-free optimizer for this task. A numerical illustration is presented (using the BFO package), which indicates that the obtained gains are potentially significant. © 2019 Association for Computing Machinery.",Algorithmic design; Derivative-free optimization; Hyper-parameters optimization; Trainable codes,Optimization; Algorithmic design; Benchmarking tools; Derivative-free optimization; Hyper-parameter; Optimization problems; Performance of algorithm; Trainable codes; Training algorithms; Benchmarking
"The peano software-parallel, automaton-based, dynamically adaptive grid traversals",2019,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065618833&doi=10.1145%2f3319797&partnerID=40&md5=7038e3b0aecc378cb107502ab3084d2a,"We discuss the design decisions, design alternatives, and rationale behind the third generation of Peano, a framework for dynamically adaptive Cartesian meshes derived from spacetrees. Peano ties the mesh traversal to the mesh storage and supports only one element-wise traversal order resulting from space-filling curves. The user is not free to choose a traversal order herself. The traversal can exploit regular grid subregions and shared memory as well as distributed memory systems with almost no modifications to a serial application code. We formalize the software design by means of two interacting automata-one automaton for the multiscale grid traversal and one for the application-specific algorithmic steps. This yields a callback-based programming paradigm. We further sketch the supported application types and the two data storage schemes realized before we detail high-performance computing aspects and lessons learned. Special emphasis is put on observations regarding the used programming idioms and algorithmic concepts. This transforms our report from a “one way to implement things” code description into a generic discussion and summary of some alternatives, rationale, and design decisions to be made for any tree-based adaptive mesh refinement software. © 2019 Association for Computing Machinery.",Adaptive mesh refinement; Parallel multiscale grid traversal; Software; Spacetree,Application programs; Computer software; Digital storage; Memory architecture; Mesh generation; Numerical analysis; Adaptive Cartesian mesh; Adaptive mesh refinement; Application specific; Distributed memory systems; High performance computing; Multi-scale grids; Programming paradigms; Spacetree; Software design
ROPTLIB: An object-oriented C++ library for optimization on riemannian manifolds,2018,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060535675&doi=10.1145%2f3218822&partnerID=40&md5=fd8d319f3b4b66523e98a12c58cb3cc1,"Riemannian optimization is the task of finding an optimum of a real-valued function defined on a Riemannian manifold. Riemannian optimization has been a topic of much interest over the past few years due to many applications including computer vision, signal processing, and numerical linear algebra. The substantial background required to successfully design and apply Riemannian optimization algorithms is a significant impediment for many potential users. Therefore, multiple packages, such as Manopt (in Matlab) and Pymanopt (in Python), have been developed. This article describes ROPTLIB, a C++ library for Riemannian optimization. Unlike prior packages, ROPTLIB simultaneously achieves the following goals: (i) it has user-friendly interfaces in Matlab, Julia, and C++; (ii) users do not need to implement manifold- and algorithm-related objects; (iii) it provides efficient computational time due to its C++ core; (iv) it implements state-of-the-art generic Riemannian optimization algorithms, including quasi-Newton algorithms; and (v) it is based on objectoriented programming, allowing users to rapidly add new algorithms and manifolds. © 2018 ACM. All rights reserved.",Julia interface; Low-rank matrices; Matlab interface; Non-convex optimization; Orthogonal constraints; Riemannian optimization; Symmetric positive definite matrices,Computational efficiency; Convex optimization; Geometry; Interface states; MATLAB; Matrix algebra; Object oriented programming; Signal processing; Low-rank matrices; Nonconvex optimization; Orthogonal constraints; Riemannian optimizations; Symmetric positive definite matrices; C++ (programming language)
Solution of dense linear systems via roundoff-error-free factorization algorithms: Theoretical connections and computational comparisons,2018,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060552334&doi=10.1145%2f3199571&partnerID=40&md5=325ebfcacddce40955c906b813a5d28c,"Exact solving of systems of linear equations (SLEs) is a fundamental subroutine within number theory, formal verification of mathematical proofs, and exact-precision mathematical programming. Moreover, efficient exact SLE solution methods could be valuable for a growing body of science and engineering applications where current fixed-precision standards have been deemed inadequate. This article contains key derivations relating, and computational tests comparing, two exact direct solution frameworks: roundoff-error-free (REF) LU factorization and rational arithmetic LU factorization. Specifically, both approaches solve the linear system Ax = b by factoring the matrix A into the product of a lower triangular (L) and upper triangular (U) matrix, A = LU . Most significantly, the featured findings reveal that the integer-preserving REF factorization framework solves dense SLEs one order of magnitude faster than the exact rational arithmetic approach while requiring half the memory. Since rational LU is utilized for basic solution validation in exact linear and mixed-integer programming, these results offer preliminary evidence of the potential of the REF factorization framework to be utilized within this specific context. Additionally, this article develops and analyzes an efficient streamlined version of Edmonds's Q-matrix approach that can be implemented as another basic solution validation approach. Further experiments demonstrate that the REF factorization framework also outperforms this alternative integer-preserving approach in terms of memory requirements and computational effort. General purpose codes to solve dense SLEs exactly via any of the aforementioned methods have been made available to the research and academic communities. © 2018 ACM.",Dense linear systems; Exact linear programming; Roundoff errors,Computation theory; Errors; Factorization; Integer programming; Linear programming; Linear systems; Number theory; Precision engineering; Subroutines; Computational comparisons; Computational effort; Factorization algorithms; Linear and mixed-integer programming; Rational arithmetics; Round-off errors; Science and engineering; Systems of linear equations; Matrix algebra
Algorithm 990: Efficient atlasing and search of configuration spaces of point-sets constrained by distance intervals,2018,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060556433&doi=10.1145%2f3204472&partnerID=40&md5=6e6bcd54be4261513a6853d6c8df84d5,"For configurations of point-sets that are pairwise constrained by distance intervals, the EASAL software implements a suite of algorithms that characterize the structure and geometric properties of the configuration space. The algorithms generate, describe, and explore these configuration spaces using generic rigidity properties, classical results for stratification of semi-algebraic sets, and new results for efficient sampling by convex parametrization. The article reviews the key theoretical underpinnings, major algorithms, and their implementation. The article outlines the main applications such as the computation of free energy and kinetics of assembly of supramolecular structures or of clusters in colloidal and soft materials. In addition, the article surveys select experimental results and comparisons. © 2018 ACM.",Cayley configuration spaces; Geometric constraint systems; Molecular modeling; Spherical particle assembly,Free energy; Geometry; Molecular modeling; Configuration space; Distance intervals; Efficient sampling; Geometric constraint systems; Geometric properties; Semi-algebraic sets; Spherical particle; Supramolecular structure; Wave functions
Validated and numerically efficient chebyshev spectral methods for linear ordinary differential equations,2018,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060522228&doi=10.1145%2f3208103&partnerID=40&md5=dcc561d2e20c987e42f2dcbb79a0f069,"In this work, we develop a validated numeric method for the solution of linear ordinary differential equations (LODEs). A wide range of algorithms (i.e., Runge-Kutta, collocation, spectral methods) exist for numerically computing approximations of the solutions. Most of these come with proofs of asymptotic convergence, but usually, provided error bounds are nonconstructive. However, in some domains like critical systems and computer-aided mathematical proofs, one needs validated effective error bounds. We focus on both the theoretical and practical complexity analysis of a so-called a posteriori quasi-Newton validation method, which mainly relies on a fixed-point argument of a contracting map. Specifically, given a polynomial approximation, obtained by some numerical algorithm and expressed on a Chebyshev basis, our algorithm efficiently computes an accurate and rigorous error bound. For this, we study theoretical properties like compactness, convergence, and invertibility of associated linear integral operators and their truncations in a suitable coefficient space of Chebyshev series. Then, we analyze the almost-banded matrix structure of these operators, which allows for very efficient numerical algorithms for both numerical solutions of LODEs and rigorous computation of the approximation error. Finally, several representative examples show the advantages of our algorithms as well as their theoretical and practical limits. © 2018 ACM.",Chebyshev series; D-finite functions; Fixed-point validation; Linear ordinary differential equations; Quasi-Newton method; Rigorous computing; Spectral methods,Approximation algorithms; Computational efficiency; Error analysis; Newton-Raphson method; Polynomial approximation; Runge Kutta methods; Spectroscopy; Structure (composition); Chebyshev series; D-finite functions; Fixed points; Linear ordinary differential equations; Quasi-Newton methods; Rigorous computing; Spectral methods; Ordinary differential equations
Interval enclosures of upper bounds of roundoff errors using semidefinite programming,2018,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060544599&doi=10.1145%2f3206430&partnerID=40&md5=be19b508ddaa9fef8547f27a39d01778,"A long-standing problem related to floating-point implementation of numerical programs is to provide efficient yet precise analysis of output errors. We present a framework to compute lower bounds on largest absolute roundoff errors, for a particular rounding model. This method applies to numerical programs implementing polynomial functions with box constrained input variables. Our study is based on three different hierarchies, relying respectively on generalized eigenvalue problems, elementary computations, and semidefinite programming (SDP) relaxations. This is complementary of over-approximation frameworks, consisting of obtaining upper bounds on the largest absolute roundoff error. Combining the results of both frameworks allows one to get enclosures for upper bounds on roundoff errors. The under-approximation framework provided by the third hierarchy is based on a new sequence of convergent robust SDP approximations for certain classes of polynomial optimization problems. Each problem in this hierarchy can be solved exactly via SDP. By using this hierarchy, one can provide a monotone nondecreasing sequence of lower bounds converging to the absolute roundoff error of a program implementing a polynomial function, applying for a particular rounding model. We investigate the efficiency and precision of our method on nontrivial polynomial programs coming from space control, optimization, and computational biology. 7copy; 2018 ACM. © 2018 ACM. All rights reserved.",Floating-point arithmetic; Generalized eigenvalues; Polynomial optimization; Robust optimization; Roundoff error; Semidefinite programming,Digital arithmetic; Eigenvalues and eigenfunctions; Enclosures; Functions; Numerical methods; Optimization; Polynomial approximation; Generalized eigenvalues; Polynomial optimization; Robust optimization; Round-off errors; Semi-definite programming; Errors
Algorithm 989: Perm_mateda: A matlab toolbox of estimation of distribution algorithms for permutation-based combinatorial optimization problems,2018,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060519438&doi=10.1145%2f3206429&partnerID=40&md5=dd086c03d9c2c3bab5f26bd773d49f05,"Permutation problems are combinatorial optimization problemswhose solutions are naturally codified as permutations. Due to their complexity, motivated principally by the factorial cardinality of the search space of solutions, they have been a recurrent topic for the artificial intelligence and operations research community. Recently, among the vast number of metaheuristic algorithms, new advances on estimation of distribution algorithms (EDAs) have shown outstanding performance when solving some permutation problems. These novel EDAs implement distance-based exponential probability models such as the Mallows and Generalized Mallows models. In this article, we present a Matlab package, perm_mateda, of estimation of distribution algorithms on permutation problems, which has been implemented as an extension to the Mateda-2.0 toolbox of EDAs. Particularly, we provide implementations of the Mallows and Generalized Mallows EDAs under the Kendall's-τ , Cayley, and Ulam distances. In addition, four classical permutation problems have also been implemented: Traveling Salesman Problem, Permutation Flowshop Scheduling Problem, Linear Ordering Problem, and Quadratic Assignment Problem. © 2018 ACM.",Estimation of distribution algorithms; Mallows and generalized mallows models; Matlab; Optimization; Permutation-based problems,Combinatorial optimization; MATLAB; Operations research; Optimization; Traveling salesman problem; Combinatorial optimization problems; Estimation of distribution algorithms; Linear ordering problems; Mallows models; Meta heuristic algorithm; Permutation flowshop scheduling problems; Permutation-based problems; Quadratic assignment problems; Problem solving
A computational architecture for coupling heterogeneous numerical models and computing coupled derivatives,2018,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050850495&doi=10.1145%2f3182393&partnerID=40&md5=b09bb7bbcab5def5633479807103eac3,"One of the challenges in computational modeling is coupling models to solve multidisciplinary problems. Flow-based computational frameworks alleviate part of the challenge through a modular approach, where data flows from component to component. However, existing flow-based frameworks are inefficient when coupled derivatives are needed for optimization. To address this, we develop the modular analysis and unified derivatives (MAUD) architecture. MAUD formulates the multidisciplinary model as a nonlinear system of equations, which leads to a linear equation that unifies all methods for computing derivatives. This enables flow-based frameworks that use the MAUD architecture to provide a common interface for the chain rule, adjoint method, coupled adjoint method, and hybrid methods; MAUD automatically uses the appropriate method for the problem. A hierarchical, matrix-free approach enables modern solution techniques such as Newton-Krylov solvers to be used within this monolithic formulation without computational overhead. Two demonstration problems are solved using a Python implementation of MAUD: a nanosatellite optimization with more than 2 million unknowns and 25,000 design variables, and an aircraft optimization involving over 6,000 design variables and 23,000 constraints. MAUD is now implemented in the open source framework OpenMDAO, which has been used to solve aircraft, satellite, wind turbine, and turbofan engine design problems. 2018 Copyright is held by the owner/author(s). © 2018 ACM. All rights reserved.",Adjoint methods; Complex systems; Derivative computation; Engineering design; High-performance computing; Multidisciplinary design optimization; Multiphysics simulation; Optimization; Parallel computing; PDE-constrained optimization; Python,Computer software; Constrained optimization; Design aids; High level languages; Large scale systems; Machine design; Nanosatellites; Nonlinear equations; Optimization; Parallel processing systems; Turbofan engines; Wind turbines; Adjoint methods; Engineering design; High performance computing; Multidisciplinary design optimization; Multiphysics simulations; PDE-constrained optimization; Python; Problem solving
BLASFEO: Basic linear algebra subroutines for embedded optimization,2018,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057125390&doi=10.1145%2f3210754&partnerID=40&md5=4123fc173c8d37409d25a23df5ec5ec0,"Basic Linear Algebra Subroutines for Embedded Optimization (BLASFEO) is a dense linear algebra library providing high-performance implementations of BLAS- and LAPACK-like routines for use in embedded optimization and small-scale high-performance computing, in general. A key difference with respect to existing high-performance implementations of BLAS is that the computational performance is optimized for small-to medium-scale matrices, i.e., for sizes up to a few hundred. BLASFEO comes with three different implementations: a high-performance implementation aimed at providing the highest performance for matrices fitting in cache, a reference implementation providing portability and embeddability and optimized for very small matrices, and a wrapper to standard BLAS and LAPACK providing high performance on large matrices. The three implementations of BLASFEO together provide high-performance dense linear algebra routines for matrices ranging from very small to large. Compared to both open-source and proprietary highly tuned BLAS libraries, for matrices of size up to about 100, the high-performance implementation of BLASFEO is about 20–30% faster than the corresponding level 3 BLAS routines and two to three times faster than the corresponding LAPACK routines. © 2018 ACM.",BLAS; High-performance; Libraries; Linear algebra; Matrix,Libraries; Linear algebra; Basic linear algebra subroutines; BLAS; Computational performance; Embedded optimizations; High performance computing; High performance implementations; High-performance; Reference implementation; Matrix algebra
Practical polytope volume approximation,2018,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048947707&doi=10.1145%2f3194656&partnerID=40&md5=02f4ad446c7ec56e7c4ec115fe7cf456,"                             We experimentally study the fundamental problem of computing the volume of a convex polytope given as an intersection of linear halfspaces. We implement and evaluate randomized polynomial-time algorithms for accurately approximating the polytope’s volume in high dimensions (e.g., few hundreds) based onhit-and-run random walks. To carry out this efficiently, we experimentally correlate the effect of parameters, such as random walk length and number of sample points, with accuracy and runtime. Our method is based on Monte Carlo algorithms with guaranteed speed and provably high probability of success for arbitrarily high precision. We exploit the problem’s features in implementing a practical rounding procedure of polytopes, in computing only partial “generations” of random points, and in designing fast polytope boundary oracles. Our publicly available software is significantly faster than exact computation and more accurate than existing approximation methods. For illustration, volume approximations of Birkhoff polytopes B                             11                             , . . ., B                             15                              are computed, in dimensions up to 196, whereas exact methods have only computed volumes of up to B                             10                          © 2018 ACM.",Algorithm engineering; Birkhoff polytopes; General dimension; Open source software; Polytope oracle; Random walk; Volume approximation,Monte Carlo methods; Open source software; Open systems; Polynomial approximation; Random processes; Topology; Algorithm engineering; General dimension; Polytopes; Random Walk; Volume approximation; Approximation algorithms
Secure and fast encryption (SAFE) with classical random number generators,2018,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060518843&doi=10.1145%2f3212673&partnerID=40&md5=9cbf311c972fdb2a13aa852666212f43,"Pseudo-random number generators (PRNGs) play an important role in both areas of computer simulation and computer security. Currently, there appears to be a huge divide between the types of PRNGs used in these two areas. For PRNGs in computer security applications, the security concern is extremely important. For PRNGs in computer simulation applications, the properties of high-dimensional equi-distribution, efficiency, long period-length, and portability are important. In recent years, there have been many PRNGs proposed in the area of computer simulation satisfying these nice properties. However, most of them are linear generators, thus sharing the same weakness in predictability. The major aim of this article is to propose a general class of secure generators, called SAFE (secure and fast encryption) generators, by properly ""mixing"" two baseline generators with the aforementioned properties to obtain a secure generator that would inherit these nice properties. Specifically, we propose applying a general mutual-shuffling method to certain linear generators, such as the currently most popular MT19937 generator and large-order multiple recursive generators, as well as outputting certain nonlinear transformations of the generated variates to construct secure PRNGS. © 2018 ACM.",Algorithms; Design; Measurement; Performance; Theory,Algorithms; Cryptography; Design; Linear transformations; Mathematical transformations; Measurement; Number theory; Security of data; Security systems; Computer security applications; Multiple recursive generators; Non-linear transformations; Performance; Pseudo random number generators; Random number generators; Simulation applications; Theory; Random number generation
Quasi-matrix-free hybrid multigrid on dynamically adaptive cartesian grids,2018,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042470446&doi=10.1145%2f3165280&partnerID=40&md5=61774bd97a9ad593a28615e5994aabf2,"We present a family of spacetree-based multigrid realizations using the tree’s multiscale nature to derive coarse grids. They align with matrix-free geometric multigrid solvers as they never assemble the system matrices, which is cumbersome for dynamically adaptive grids and full multigrid. The most sophisticated realizations use BoxMG to construct operator-dependent prolongation and restriction in combination with Galerkin/Petrov-Galerkin coarse-grid operators. This yields robust solvers for nontrivial elliptic problems. We embed the algebraic, problem-dependent, and grid-dependent multigrid operators as stencils into the grid and evaluate all matrix-vector products in situ throughout the grid traversals. Such an approach is not literally matrix-free as the grid carries the matrix. We propose to switch to a hierarchical representation of all operators. Only differences of algebraic operators to their geometric counterparts are held. These hierarchical differences can be stored and exchanged with small memory footprint. Our realizations support arbitrary dynamically adaptive grids while they vertically integrate the multilevel operations through spacetree linearization. This yields good memory access characteristics, while standard colouring of mesh entities with domain decomposition allows us to use parallel many-core clusters. All realization ingredients are detailed such that they can be used by other codes. © 2018 ACM.",Adaptive mesh refinement; Algebraic multigrid; BoxMG; Full approximation storage; Geometric multigrid; Matrix-free; Operator compression; Parallel linear algebra,Algebra; Domain decomposition methods; Geometry; Linear algebra; Mesh generation; Adaptive mesh refinement; Algebraic multigrids; BoxMG; Full approximation storages; Geometric multigrids; Matrix free; Operator compressions; Parallel linear algebras; Matrix algebra
Implementing 64-bit maximally equidistributed f2-Linear generators with mersenne prime period,2018,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042483899&doi=10.1145%2f3159444&partnerID=40&md5=a0da9f052c64f1a2edbdfd893c7f4329,"CPUs and operating systems are moving from 32 to 64 bits, and hence it is important to have good pseudorandom number generators designed to fully exploit these word lengths. However, existing 64-bit very long period generators based on linear recurrences modulo 2 are not completely optimized in terms of the equidistribution properties. Here, we develop 64-bit maximally equidistributed pseudorandom number generators that are optimal in this respect and have speeds equivalent to 64-bit Mersenne Twisters. We provide a table of specific parameters with period lengths from 2607 − 1 to 244497 − 1. © 2018 ACM.",Empirical statistical testing; Equidistribution; Mersenne twister; Random number generator,Number theory; Program processors; Equidistribution; Linear generators; Linear recurrences; Mersenne twisters; Period length; Pseudo random number generators; Random number generators; Statistical testing; Random number generation
Numerical bifurcation analysis of homoclinic orbits embedded in one-Dimensional manifolds of maps,2018,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042465245&doi=10.1145%2f3134443&partnerID=40&md5=664bbaa79e390d75977260fb93615447,"We describe new methods for initializing the computation of homoclinic orbits for maps in a state space with arbitrary dimension and for detecting their bifurcations. The initialization methods build on known and improved methods for computing one-dimensional stable and unstable manifolds. The methods are implemented in MatContM, a freely available toolbox in Matlab for numerical analysis of bifurcations of fixed points, periodic orbits, and connecting orbits of smooth nonlinear maps. The bifurcation analysis of homoclinic connections under variation of one parameter is based on continuation methods and allows us to detect all known codimension 1 and 2 bifurcations in three-dimensional (3D) maps, including tangencies and generalized tangencies. MatContM provides a graphical user interface, enabling interactive control for all computations. As the prime new feature, we discuss an algorithm for initializing connecting orbits in the important special case where either the stable or unstable manifold is one-dimensional, allowing us to compute all homoclinic orbits to saddle points in 3D maps. We illustrate this algorithm in the study of the adaptive control map, a 3D map introduced in 1991 by Frouzakis, Adomaitis, and Kevrekidis, to obtain a rather complete bifurcation diagram of the resonance horn in a 1:5 Neimark-Sacker bifurcation point, revealing new features. © 2018 ACM.",Bifurcation; Homoclinic orbit; Iterated map; MatContM,Dynamical systems; Graphical user interfaces; MATLAB; Numerical methods; User interfaces; Homoclinic connection; Homoclinic orbits; Initialization methods; Iterated maps; MatContM; Neimark-Sacker bifurcation; Numerical bifurcation analysis; Threedimensional (3-d); Bifurcation (mathematics)
SparseX: A library for high-Performance sparse matrix-Vector multiplication on multicore platforms,2018,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042509308&doi=10.1145%2f3134442&partnerID=40&md5=68c5c7dd47367d94be8217dd85dc70da,"The Sparse Matrix-Vector Multiplication (SpMV) kernel ranks among the most important and thoroughly studied linear algebra operations, as it lies at the heart of many iterative methods for the solution of sparse 6 linear systems, and often constitutes a severe performance bottleneck. Its optimization, which is intimately associated with the data structures used to store the sparse matrix, has always been of particular interest to the applied mathematics and computer science communities and has attracted further attention since the advent of multicore architectures. In this article, we present SparseX, an open source software package for SpMV targeting multicore platforms, that employs the state-of-the-art Compressed Sparse eXtended (CSX) sparse matrix storage format to deliver high efficiency through a highly usable “BLAS-like” interface that requires limited or no tuning. Performance results indicate that our library achieves superior performance over competitive libraries on large-scale problems. © 2018 ACM.",CSX; Data compression; High-performance computing; HPC; Multicore; Scientific applications; Sparse matrix-vector multiplication; SpMV; SpMV library,Computer architecture; Data compression; Digital storage; Interface states; Iterative methods; Linear algebra; Linear systems; Open source software; Open systems; Software architecture; Software engineering; High performance computing; Multi core; Scientific applications; Sparse matrix-vector multiplication; SpMV; Matrix algebra
Design of a high-Performance GEMM-like tensor–Tensor multiplication,2018,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042493220&doi=10.1145%2f3157733&partnerID=40&md5=922b03d25f687c882fe723c86e280e5b,"We present “GEMM-like Tensor–Tensor multiplication” (GETT), a novel approach for dense tensor contractions that mirrors the design of a high-performance general matrix–matrix multiplication (GEMM). The critical insight behind GETT is the identification of three index sets, involved in the tensor contraction, which enable us to systematically reduce an arbitrary tensor contraction to loops around a highly tuned “macro-kernel.” This macro-kernel operates on suitably prepared (“packed”) sub-tensors that reside in a specified level of the cache hierarchy. In contrast to previous approaches to tensor contractions, GETT exhibits desirable features such as unit-stride memory accesses, cache-awareness, as well as full vectorization, without requiring auxiliary memory. We integrate GETT alongside the so-called Transpose–Transpose- GEMM-Transpose and Loops-over- GEMM approaches into an open source “Tensor Contraction Code Generator.” The performance results for a wide range of tensor contractions suggest that GETT has the potential of becoming the method of choice: While GETT exhibits excellent performance across the board, its effectiveness for bandwidth-bound tensor contractions is especially impressive, outperforming existing approaches by up to 12.4×. More precisely, GETT achieves speedups of up to 1.41× over an equivalent-sized GEMM for bandwidth-bound tensor contractions while attaining up to 91.3% of peak floating-point performance for compute-bound tensor contractions. © 2018 ACM.",Domain-specific code generator; High-performance computing; Matrix–matrix multiplication; Tensor contractions,Bandwidth; Cache memory; Digital arithmetic; Matrix algebra; Open systems; Auxiliary memory; Cache hierarchies; Desirable features; Domain-specific codes; High performance computing; MAtrix multiplication; Stride memory access; Tensor contraction; Tensors
Simultaneous conversions with the residue number system using linear algebra,2018,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042513194&doi=10.1145%2f3145573&partnerID=40&md5=0d11ad0d0eb47a448611d32444daa7c9,We present an algorithm for simultaneous conversions between a given set of integers and their Residue Number System representations based on linear algebra. We provide a highly optimized implementation of the algorithm that exploits the computational features of modern processors. The main application of our algorithm is matrix multiplication over integers. Our speed-up of the conversions to and from the Residue Number System significantly improves the overall running time of matrix multiplication. © 2018 ACM.,Chinese remainder theorem; Integer matrix multiplication; Integer polynomial multiplication; Linear algebra library; Multiprecision arithmetic; Residue number system,Algebra; Computation theory; Linear algebra; Numbering systems; Chinese remainder theorem; Integer matrices; Linear algebra libraries; Multiprecision arithmetics; Polynomial multiplication; Residue number system; Matrix algebra
"Efficient parallel random sampling—Vectorized, cache-Efficient, and online",2018,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042489692&doi=10.1145%2f3157734&partnerID=40&md5=5eea69be2ba640df21798e2ef29d1f79,"We consider the problem of sampling n numbers from the range {1, . . ., N } without replacement on modern architectures. The main result is a simple divide-and-conquer scheme that makes sequential algorithms more cache efficient and leads to a parallel algorithm running in expected time O (n/p + log p) on p processors, i.e., scales to massively parallel machines even for moderate values of n. The amount of communication between the processors is very small (at most O (log p)) and independent of the sample size. We also discuss modifications needed for load balancing, online sampling, sampling with replacement, Bernoulli sampling, and vectorization on SIMD units or GPUs. 2018 Copyright is held by the owner,author's.",Communication efficient algorithms; Hypergeometric random deviates; Parallel algorithms,Parallel algorithms; Program processors; Divide and conquer; Hypergeometric; Massively parallel machine; Modern architectures; On-line sampling; Random sampling; Sampling with replacement; Sequential algorithm; Parallel processing systems
Automatic reformulation of ODEs to systems of first-Order equations,2018,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042472624&doi=10.1145%2f3159443&partnerID=40&md5=75e4b5ca79128d00ba8631321c15ba04,"Most numerical ODE solvers require problems to be written as systems of first-order differential equations. This normally requires the user to rewrite higher-order differential equations as coupled first-order systems. Here, we introduce the treeVar class, written in object-oriented Matlab, which is capable of algorithmically reformulating higher-order ODEs to equivalent systems of first-order equations. This allows users to specify problems using a more natural syntax and saves them from having to manually derive the first-order reformulation. The technique works by using operator overloading to build up syntax trees of expressions as mathematical programs are evaluated. It then applies a set of rules to the resulting trees to obtain the first-order reformulation, which is returned as another program. This technique has connections with algorithmic/automatic differentiation. We present how treeVar has been incorporated in Chebfun, greatly improving the ODE capabilities of the system. © 2018 ACM.",Analysis of mathematical functions; Automatic differentiation; Automatic reformulation; Initial value problems; Object-oriented programming; Operator overloading; Ordinary differential equations; Syntax trees,Differential equations; Equivalence classes; Functions; Initial value problems; Mathematical operators; MATLAB; Object oriented programming; Syntactics; Trees (mathematics); Automatic differentiations; Automatic reformulations; Mathematical functions; Operator overloading; Syntax tree; Ordinary differential equations
BootCMatch: A software package for bootstrap AMG based on graph weighted matching,2018,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049674986&doi=10.1145%2f3190647&partnerID=40&md5=840d922d2f14dc556efb886fe14c95c6,"This article has two main objectives: one is to describe some extensions of an adaptive Algebraic Multigrid (AMG) method of the form previously proposed by the first and third authors, and a second one is to present a new software framework, named BootCMatch, which implements all the components needed to build and apply the described adaptive AMG both as a stand-alone solver and as a preconditioner in a Krylov method. The adaptive AMG presented is meant to handle general symmetric and positive definite (SPD) sparse linear systems, without assuming any a priori information of the problem and its origin; the goal of adaptivity is to achieve a method with a prescribed convergence rate. The presented method exploits a general coarsening process based on aggregation of unknowns, obtained by a maximum weight matching in the adjacency graph of the system matrix. More specifically, a maximum product matching is employed to define an effective smoother subspace (complementary to the coarse space), a process referred to as compatible relaxation, at every level of the recursive two-level hierarchical AMG process. Results on a large variety of test cases and comparisons with related work demonstrate the reliability and efficiency of the method and of the software. © 2018 Association for Computing Machinery. All rights reserved.",Algebraic multigrid; Graph matching; Iterative solver; Preconditioner,Computer programming; Iterative methods; Linear systems; Software reliability; Software testing; Algebraic multigrid methods; Algebraic multigrids; Compatible relaxation; Graph matchings; Iterative solvers; Maximum weight matching; Preconditioners; Sparse linear systems; Algebra
Algorithm 988: AMGKQ: An efficient implementation of adaptive multivariate gauss-kronrod quadrature for simultaneous integrands in Octave/MATLAB,2018,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047017314&doi=10.1145%2f3157735&partnerID=40&md5=16b622d057b700e18df9a65ffb65ab1f,"The algorithm AMGKQ for adaptive multivariate Gauss-Kronrod quadrature over hyperrectangular regions of arbitrary dimensionality is proposed and implemented in Octave/MATLAB. It can approximate numerically any number of integrals over a common domain simultaneously. Improper integrals are addressed through singularity weakening coordinate transformations. Internal singularities are addressed through the use of breakpoints. Its accuracy performance is investigated thoroughly, and its running time is compared to other commonly available routines in two and three dimensions. Its running time can be several orders of magnitude faster than recursively called quadrature routines. Its performance is limited only by the memory structure of its operating environment. Included with the software are numerous examples of its invocation. © 2018 ACM.",Computation of integrals over hyperrectangular regions; Multidimensional numeric integration; Multiple integrals,Software engineering; Co-ordinate transformation; Efficient implementation; Improper integrals; Memory structure; Multiple integral; Numeric integration; Operating environment; Orders of magnitude; Computer software
Design and implementation of adaptive SpMV library for multicore and many-core architecture,2018,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044305052&doi=10.1145%2f3218823&partnerID=40&md5=59b8133d741b0aa6f7cf3131f4ea87b0,"Sparse matrix vector multiplication (SpMV) is an important computational kernel in traditional high-performance computing and emerging data-intensive applications. Previous SpMV libraries are optimized by either application-specific or architecture-specific approaches but present difficulties for use in real applications. In this work, we develop an auto-tuning system (SMATER) to bridge the gap between specific optimizations and general-purpose use. SMATER provides programmers a unified interface based on the compressed sparse row (CSR) sparse matrix format by implicitly choosing the best format and fastest implementation for any input sparse matrix during runtime. SMATER leverages a machine-learning model and retargetable back-end library to quickly predict the optimal combination. Performance parameters are extracted from 2,386 matrices in the SuiteSparse matrix collection. The experiments show that SMATER achieves good performance (up to 10 times that of the Intel Math Kernel Library (MKL) on Intel E5-2680 v3) while being portable on state-of-the-art x86 multicore processors, NVIDIA GPUs, and Intel Xeon Phi accelerators. Compared with the Intel MKL library, SMATER runs faster by more than 2.5 times on average. We further demonstrate its adaptivity in an algebraic multigrid solver from the Hypre library and report greater than 20% performance improvement. © 2018 Copyright is held by the owner/author(s). Publication rights licensed to ACM.",Auto-tuning; Machine learning; Multicore; Sparse matrix vector multiplication,Artificial intelligence; Learning systems; Matrix algebra; Program processors; Autotuning; Data-intensive application; Design and implementations; High performance computing; Multi core; Multi core and many cores; Performance improvements; Sparse matrix-vector multiplication; Computer architecture
A domain-specific language and editor for parallel particle methods,2018,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047003520&doi=10.1145%2f3175659&partnerID=40&md5=113ce0c09fb6bf3e4e2376da06ce4170,"Domain-specific languages (DSLs) are of increasing importance in scientific high-performance computing to reduce development costs, raise the level of abstraction, and, thus, ease scientific programming. However, designing DSLs is not easy, as it requires knowledge of the application domain and experience in language engineering and compilers. Consequently, many DSLs follow a weak approach using macros or text generators, which lack many of the features that make a DSL comfortable for programmers. Some of these features-e.g., syntax highlighting, type inference, error reporting-are easily provided by language workbenches, which combine language engineering techniques and tools in a common ecosystem. In this article, we present the Parallel Particle-Mesh Environment (PPME), a DSL and development environment for numerical simulations based on particle methods and hybrid particle-mesh methods. PPME uses the Meta Programming System, a projectional language workbench. PPME is the successor of the Parallel Particle-Mesh Language, a Fortran-based DSL that uses conventional implementation strategies. We analyze and compare both languages and demonstrate how the programmer's experience is improved using static analyses and projectional editing, i.e., code-structure editing, constrained by syntax, as opposed to free-text editing. We present an explicit domain model for particle abstractions and the first formal type system for particle methods. © 2018 Association for Computing Machinery. All rights reserved.",Language workbenches; Mathematical software; MPS; Particle methods; Scientific computing,Abstracting; Computer aided software engineering; Digital subscriber lines; Graphical user interfaces; Mesh generation; Natural sciences computing; Numerical methods; Problem oriented languages; Syntactics; Development environment; Domain specific languages; High performance computing; Implementation strategies; Language workbenches; Mathematical software; Particle methods; Scientific programming; Static analysis
On roundoff error growth in elliptic problems,2018,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047007397&doi=10.1145%2f3134444&partnerID=40&md5=90af8730033c3ad46814f60dfc2436bf,"Large-scale linear systems arise in finite-difference and finite-element discretizations of elliptic problems. With increasing computer performance, ever larger systems are solved using direct methods. How large can such systems be without roundoff compromising accuracy? Here we model roundoff dynamics in standard LU and LDLT decompositions with respect to problem size N. For the one-dimensional (1D) Poisson equation with Dirichlet boundary conditions on an equidistant grid, we show that the relative error in the factorized matrix grows like O(ϵN) if roundoffs are modeled as independent, expectation zero random variables. With bias, the growth rate changes to O(ϵN). Subsequent back substitution results in typical error growths of O(ϵNN) and O(ϵN2), respectively. Error growth is governed by the dynamics of the computational process and by the structure of the boundary conditions rather than by the condition number. Computational results are demonstrated in several examples, including a few fourth-order 1D problems and second-order 2D problems, showing that error accumulation depends strongly on the solution method. Thus, the same LU solver may exhibit different growth rates for the same 2D Poisson problem, depending on whether the five-point or nine-point FDM operator is used. © 2018 ACM.",Roundoff error analysis,Boundary conditions; Computational mechanics; Linear systems; Number theory; One dimensional; Poisson equation; Computational process; Computational results; Computer performance; Dirichlet boundary condition; Error accumulation; Finite element discretizations; Large-scale linear systems; Round-off error analysis; Errors
Algorithm 987: MANBIS-A C++ mathematical software package for locating and computing efficiently many roots of a function: Theoretical issues,2018,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047019381&doi=10.1145%2f3155744&partnerID=40&md5=71b249befcd55711df37672ed83d3068,"MANBIS is a C++ mathematical software package for tackling the problem of computing the roots of a function when the number of roots is very large (of the order of hundreds or thousands). This problem has attracted increasing attention in recent years because of the broad variety of applications in various fields of science and technology. MANBIS applies the bisection method to obtain an approximate root according to a predetermined accuracy. Thus, the only computable information required is the algebraic signs of the considered function, which is the smallest amount of information (one bit of information) necessary for the purpose needed, and not any additional information. MANBIS is able to compute very efficiently a user-given percentage of roots and draws its strength from the fact that the roots are expected to be many. Furthermore, MANBIS is capable of estimating without any additional function computational cost the total number of roots within the user-given interval. Our approach can also be efficiently applied in cases where the distribution of the roots is not known. This article is accompanied by another article where the user manual, some implementation details, and some examples are included. © 2018 ACM.",Bisection-based methods; Computing roots; Counting; Expected behavior; Imprecise function values; Many zeroes; Root finding; Very large problems; Zero finding,C++ (programming language); Software packages; Bisection-based methods; Computing roots; Counting; Expected behavior; Function values; Large problems; Many zeroes; Root finding; Zero finding; Functions
Remark on algorithm 539: A modern fortran reference implementation for carefully computing the Euclidean norm,2017,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040028071&doi=10.1145%2f3134441&partnerID=40&md5=c37879b9c811b4adf31500579eefdbd6,"We propose a set of newFortran reference implementations, based on an algorithm proposed by Kahan, for the Level 1 BLAS routines ∗NRM2 that compute the Euclidean norm of a real or complex input vector. The principal advantage of these routines over the current offerings is that, rather than losing accuracy as the length of the vector increases, they generate results that are accurate to almost machine precision for vectors of length N < Nmax where Nmax depends upon the precision of the floating point arithmetic being used. In addition, we make use of intrinsic modules, introduced in the latest Fortran standards, to detect occurrences of nonfinite numbers in the input data and return suitable values as well as setting IEEE floating point status flags as appropriate. A set of C interface routines is also provided to allow simple, portable access to the new routines. To improve execution speed, we advocate a hybrid algorithm; a simple loop is used first and, only if IEEE floating point exception flags signal, dowe fall back on Kahan's algorithm. Sincemost input vectors are ""easy,"" i.e., they do not require the sophistication of Kahan's algorithm, the simple loop improves performance while the use of compensated summation ensures high accuracy. We also report on a comprehensive suite of test problems that has been developed to test both our new implementation and existing codes for both accuracy and the appropriate settings of the IEEE arithmetic status flags. © 2017 ACM.",Accuracy; C language interoperability; Compensated summation; Euclidean norms; Exception handling; Fortran; Reliability,Digital arithmetic; FORTRAN (programming language); Reliability; Vectors; Accuracy; C language; Compensated summation; Euclidean norm; Exception handling; C (programming language)
Algorithm 986: A suite of compact finite difference schemes,2017,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031750662&doi=10.1145%2f3119905&partnerID=40&md5=97866c135e8fffb86d50d1ae39c457a2,"A collection of Matlab routines that compute derivative approximations of arbitrary functions using highorder compact finite difference schemes is presented. Tenth-order accurate compact finite difference schemes for first and second derivative approximations and sixth-order accurate compact finite difference schemes for third and fourth derivative approximations are discussed for the functions with periodic boundary conditions. Fourier analysis of compact finite difference schemes is explained, and it is observed that compact finite difference schemes have better resolution characteristics when compared to classical finite difference schemes. Compact finite difference schemes for the functions with Dirichlet and Neumann boundary conditions are also discussed. Moreover, compact finite difference schemes for partial derivative approximations of functions in two variables are also given. For each case a Matlab routine is provided to compute the differentiation matrix and results are validated using the test functions. © 2017 ACM.",Compact finite difference schemes; Fourier analysis; Numerical differentiation; Taylor series expansion,Boundary conditions; Differentiation (calculus); Fourier analysis; Fourier series; Classical finite difference scheme; Compact finite difference schemes; Differentiation matrices; Dirichlet and Neumann boundary conditions; High-order compact finite difference schemes; Numerical differentiation; Periodic boundary conditions; Taylor series expansions; Finite difference method
Tight and rigorous error bounds for basic building blocks of double-word arithmetic,2017,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85033216006&doi=10.1145%2f3121432&partnerID=40&md5=b10241ed14ae0388cb9aad780610e63c,"We analyze several classical basic building blocks of double-word arithmetic (frequently called ""double-double arithmetic"" in the literature): the addition of a double-word number and a floating-point number, the addition of two double-word numbers, the multiplication of a double-word number by a floating-point number, the multiplication of two double-word numbers, the division of a double-word number by a floating-point number, and the division of two double-word numbers. For multiplication and division we get better relative error bounds than the ones previously published. For addition of two double-word numbers, we show that the previously published bound was incorrect, and we provide a new relative error bound. We introduce new algorithms for division. We also give examples that illustrate the tightness of our bounds. © 2017 ACM.",Double-double arithmetic; Double-word arithmetic; Error-free transforms; Floating-point arithmetic,Error analysis; Errors; Basic building block; Error bound; Floating point numbers; Relative error bounds; Digital arithmetic
Algorithm 979: Recursive algorithms for dense linear algebra - The ReLAPACK collection,2017,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029820356&doi=10.1145%2f3061664&partnerID=40&md5=cdedee67c1816e2efadef167c4e1ee7b,"To exploit both memory locality and the full performance potential of highly tuned kernels, dense linear algebra libraries, such as linear algebra package (LAPACK), commonly implement operations as blocked algorithms. However, to achieve near-optimal performance with such algorithms, significant tuning is required. In contrast, recursive algorithms are virtually tuning free and attain similar performance. In this article, we first analyze and compare blocked and recursive algorithms in terms of performance and then introduce recursive LAPACK (ReLAPACK), an open-source library of recursive algorithms to seamlessly replace many of LAPACK's blocked algorithms. In most scenarios, ReLAPACK outperforms reference LAPACK and in many situations improves upon the performance of optimized libraries. © 2017 ACM.",Dense linear algebra; Recursion,Algebra; Libraries; Blocked algorithms; Dense linear algebra; Linear algebra package; Near-optimal performance; Open-source libraries; Performance potentials; Recursions; Recursive algorithms; Linear algebra
"Algorithm 985: Simple, efficient, and relatively accurate approximation for the evaluation of the Faddeyeva function",2017,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029801926&doi=10.1145%2f3119904&partnerID=40&md5=e972a86bc5de90201c17b6c20bb5a1e5,"We present a new simple algorithm for efficient, and relatively accurate computation of the Faddeyeva function w(z). The algorithm carefully exploits previous approximations by Hui et al. (1978) and Humlíček (1982) along with asymptotic expressions from Laplace continued fractions. Over a wide and fine grid of the complex argument, z = x + iy, numerical results from the present approximation show a maximum relative error less than 4.0 × 10-5 for both real and imaginary parts of w while running in a relatively shorter execution time than other competitive techniques. In addition to the calculation of the Faddeyeva function, w, partial derivatives of the real and imaginary parts of the function can easily be calculated and returned as optional output. © 2017 ACM.",Accuracy; Complex probability function; Fortran; Function evaluation; Matlab,Approximation algorithms; Computational efficiency; FORTRAN (programming language); MATLAB; Accuracy; Accurate computations; Asymptotic expressions; Continued fraction; Faddeyeva functions; Maximum relative errors; Partial derivatives; Probability functions; Function evaluation
"Algorithm 984: ADiGator, a toolbox for the algorithmic differentiation of mathematical functions in MATLAB using source transformation via operator overloading",2017,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029792282&doi=10.1145%2f3104990&partnerID=40&md5=29e43f92738cc19885e4c79325c8cc56,"A toolbox called ADiGator is described for algorithmically differentiating mathematical functions in MATLAB. ADiGator performs source transformation via operator overloading using forward mode algorithmic differentiation and produces a file that can be evaluated to obtain the derivative of the original function at a numeric value of the input. A convenient by-product of the file generation is the sparsity pattern of the derivative function. Moreover, because both the input and output to the algorithm are source codes, the algorithm may be applied recursively to generate derivatives of any order. A key component of the algorithm is its ability to statically exploit derivative sparsity at the MATLAB operation level to improve runtime performance. The algorithm is applied to four different classes of example problems and is shown to produce runtime efficient derivative code. Due to the static nature of the approach, the algorithm is well suited and intended for use with problems requiring many repeated derivative computations. © 2017 ACM.",Algorithmic differentiation; Applied mathematics; Chain rule; Forward mode; Overloading; Scientific computation; Source transformation,Mathematical operators; Mathematical transformations; Algorithmic differentiations; Applied mathematics; Chain rules; Forward mode; Overloading; Scientific computation; Source transformation; Functions
Numerically aware orderings for sparse symmetric indefinite linear systems,2017,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028511231&doi=10.1145%2f3104991&partnerID=40&md5=958aa11798ff527b49bb4ac2f552ebcc,"Sparse symmetric indefinite problems arise in a large number of important application areas; they are often solved through the use of an LDLT factorization via a sparse direct solver. While for many problems prescaling the system matrix A is sufficient to maintain stability of the factorization, for a small but important fraction of problems numerical pivoting is required. Pivoting often incurs a significant overhead, and consequently, a number of techniques have been proposed to try and limit the need for pivoting. In particular, numerically aware ordering algorithms may be used, that is, orderings that depend not only on the sparsity pattern of A but also on the values of its (scaled) entries. Current approaches identify large entries of A and symmetrically permute them onto the subdiagonal, where they can be used as part of a 2 × 2 pivot. This is numerically effective, but the fill in the factor L and hence the runtime of the factorization and subsequent triangular solves may be significantly increased over a standard ordering if no pivoting is required. We present a new algorithm that combines a matching-based approach with a numerically aware nested dissection ordering. Numerical comparisons with current approaches for some tough symmetric indefinite problems are given. © 2017 ACM.",Nested dissection; Numerically aware ordering; Sparse direct methods; Sparse matrix ordering; Sparse symmetric matrices,Dissection; Factorization; Linear systems; Direct method; Nested dissection; Numerically aware ordering; Sparse matrix ordering; Symmetric matrices; Matrix algebra
Geometric reconstruction of implicitly defined surfaces and domains with topological guarantees,2017,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028499829&doi=10.1145%2f3104989&partnerID=40&md5=50f19851d5a136d1c103b2db81856af1,"Implicitly described domains are a well-established tool in the simulation of time-dependent problems, for example, using level-set methods. To solve partial differential equations on such domains, a range of numerical methodswas developed, for example, the Immersed Boundary method, the Unfitted Finite Element or Unfitted Discontinuous Galerkin methods, and the eXtended or Generalised Finite Element methods, just to name a few. Many of these methods involve integration over cut-cells or their boundaries, as they are described by sub-domains of the original level-set mesh. We present a new algorithm to geometrically evaluate the integrals over domains described by a first-order, conforming level-set function. The integration is based on a polyhedral reconstruction of the implicit geometry, following the concepts of the marching cubes algorithm. The algorithm preserves various topological properties of the implicit geometry in its polyhedral reconstruction, making it suitable for Finite Element computations. Numerical experiments show second-order accuracy of the integration. An implementation of the algorithm is available as free software, which allows for an easy incorporation into other projects. The software is in productive use within the DUNE framework (Bastian et al. 2008a). © 2017 ACM.",Cut-cell methods; Geometry reconstruction; Implicit domains; Level-sets; Marching cubes; Numerical quadrature; Surface reconstruction,Computational fluid dynamics; Finite element method; Galerkin methods; Geometry; Integration; Surface reconstruction; Topology; Turbulent flow; Cut cell methods; Geometry reconstruction; Level Set; Marching cube; Numerical quadrature; Numerical methods
Algorithm 982: Explicit solutions of triangular systems of first-order linear initial-value ordinary differential equations with constant coefficients,2017,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028504001&doi=10.1145%2f3092892&partnerID=40&md5=f661d3ebb9509dcb6837eec0353b7d81,"A method to compute explicit solutions of homogeneous triangular systems of first-order linear initial-value ordinary differential equations with constant coefficients is described. It is suitable for the limited case of well separated eigenvalues, or for multiple zero eigenvalues provided the entire column corresponding to a zero eigenvalue is zero. The solution for the case of constant inhomogeneity is described. The method requires only the computation of a constant matrix using a simple recurrence. Computing the solutions of the system from that matrix, for values of the independent variable, requires one to exponentiate only the diagonal of a matrix. It is not necessary to compute the exponential of a general triangular matrix. Although this work was motivated by a study of nuclear decay without fission or neutron absorption, which is used throughout as an example, it has wider applicability. © 2017 ACM.",Constant coefficient; Explicit solution; First-order; Initial value problem; Ordinary differential equations; Triangular system,Differential equations; Eigenvalues and eigenfunctions; Initial value problems; Matrix algebra; Constant coefficients; Constant matrix; Explicit solutions; First order; Independent variables; Inhomogeneities; Triangular matrices; Triangular system; Ordinary differential equations
Algorithm 983: Fast computation of the non-asymptotic Cochran's Q statistic for heterogeneity detection,2017,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028514805&doi=10.1145%2f3095076&partnerID=40&md5=3c7e1d22d52bc95be7b3bd729aad3a96,"The detection of heterogeneity among objects (products, treatments, medical studies) assessed on a series of blocks (consumers, patients, methods, pathologists) is critical in numerous areas such as clinical research, cosmetic studies, or survey analysis. The Cochran's Q test is the most widely used test for identifying heterogeneity on binary data (success vs. failure, cure vs. not cure, 1 vs. 0, etc.). For a large number of blocks, the Q distribution can be approximated by a χ2 distribution. Unfortunately, this does not hold for limited sample sizes or sparse tables. In such situations, one has to either run Monte Carlo simulations or compute the exact Q distribution to obtain an accurate and reliable result. However, the latter method is often disregarded in favor of the former due to computational expense considerations. The purpose of this article is to propose an extremely fast implementation of the exact Cochran's Q test so one can benefit from its accuracy at virtually no cost regarding computation time. It is implemented as a part of the XLSTAT statistical software (Addinsoft 2015). After a short presentation of the Cochran's Q test and the motivation for its exact version, we detail our approach and present its actual implementation. We then demonstrate the gain of this algorithm with performance evaluations and measurements. Comparisons against a well-established implementation have shown an increase of the computational velocity by a factor ranging from 100 up to 1 × 106 in the most favorable cases. © 2017 ACM.",Binary response; Categorical; Distribution; Heterogeneity; K related samples; Non asymptotic; Test,Curing; Intelligent systems; Monte Carlo methods; Object detection; Binary response; Categorical; Distribution; Heterogeneity; Non-asymptotic; Testing
TTC: A high-performance Compiler for tensor transpositions,2017,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028516429&doi=10.1145%2f3104988&partnerID=40&md5=49624fe511f833218ff1bc48b90a6d3f,"We present Tensor Transpose Compiler (TTC), an open-source parallel compiler for multidimensional tensor transpositions. To generate high-performance C++ code, TTC explores a number of optimizations, including software prefetching, blocking, loop-reordering, and explicit vectorization. To evaluate the performance of multidimensional transpositions across a range of possible use-cases, we also release a benchmark covering arbitrary transpositions of up to six dimensions. Performance results showthat the routines generated by TTC achieve close to peak memory bandwidth on both the Intel Haswell and the AMD Steamroller architectures and yield significant performance gains over modern compilers. By implementing a set of pruning heuristics, TTC allows users to limit the number of potential solutions; this option is especially useful when dealing with high-dimensional tensors, as the search space might become prohibitively large. Experiments indicate that when only 100 potential solutions are considered, the resulting performance is about 99% of that achieved with exhaustive search. © 2017 ACM.",Domain-specific compiler; Highperformance computing; Multidimensional transpositions,Benchmarking; C++ (programming language); Computer software; Open source software; Program compilers; Domain specific; High-dimensional; High-performance computing; Memory bandwidths; Multidimensional transpositions; Parallel compilers; Performance Gain; Software prefetching; Tensors
Algorithm 980: Sparse QR factorization on the GPU,2017,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028513954&doi=10.1145%2f3065870&partnerID=40&md5=d09620df9e87967f5276c299ef1efc49,"Sparse matrix factorization involves a mix of regular and irregular computation, which is a particular challenge when trying to obtain high-performance on the highly parallel general-purpose computing cores available on graphics processing units (GPUs). We present a sparse multifrontal QR factorization method that meets this challenge and is significantly faster than a highly optimized method on a multicore CPU. Our method factorizes many frontal matrices in parallel and keeps all the data transmitted between frontal matrices on the GPU. A novel bucket scheduler algorithm extends the communication-avoiding QR factorization for dense matrices by exploiting more parallelism and by exploiting the staircase form present in the frontal matrices of a sparse multifrontal method. © 2017 ACM.",GPU; Least-square problems; QR factorization; Sparse matrices,Computer graphics; Factorization; Graphics processing unit; Program processors; General-purpose computing; Highly parallels; Irregular computations; Least square problems; Multifrontal methods; QR factorizations; Sparse matrices; Sparse matrix factorization; Matrix algebra
Algorithm 981: Talbot Suite DE: Application of modified Talbot's method to solve differential problems,2017,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028517211&doi=10.1145%2f3089248&partnerID=40&md5=99cfa0f200e02711098bcd9b390be464,"In order to solve a differential problem, the Laplace Transform method, when applicable, replaces the problem with a simpler one; the solution is obtained by solving the new problem and then by computing the inverse Laplace Transform of this function. In a numerical context, since the solution of the transformed problem consists of a sequence of Laplace Transform samples, most of the software for the numerical inversion cannot be used since the transform, among parameters, must be passed as a function. To fill this gap, we present Talbot Suite DE, a C software collection for Laplace Transform inversions, specifically designed for these problems and based on Talbot's method. It contains both sequential and parallel implementations; the latter is accomplished by means of OpenMP. We also report some performance results. Aimed at non-expert users, the software is equipped with several examples and a User Guide that includes the external documentation, explains how to use all the sample code, and reports its results about accuracy and efficiency. Some examples are entirely in C and others combine different programming languages (C/MATLAB, C/FORTRAN). The User Guide also contains useful hints to avoid possible errors issued during the compilation or execution of mixed-language code. © 2017 ACM.",Inverse Laplace transform; Parallel algorithms; Talbot's method,Application programming interfaces (API); C (programming language); Inverse transforms; Laplace transforms; Parallel algorithms; Problem oriented languages; Problem solving; Differential problems; Expert users; Inverse Laplace transform; Laplace transform inversions; Laplace transform method; Numerical inversion; Parallel implementations; Talbot's method; Inverse problems
Algorithm 975: TMATROM-A T-matrix reduced order model software,2017,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025137051&doi=10.1145%2f3054945&partnerID=40&md5=a2fab303c4be65185cda639a4ee2ba7f,"The T-matrix (TMAT) of a scatterer fully describes the way the scatterer interacts with incident fields and scatters waves, and is therefore used extensively in several science and engineering applications. The T-matrix is independent of several input parameters in a wave propagation model and hence the offline computation of the T-matrix provides an efficient reduced order model (ROM) framework for performing online scattering simulations for various choices of the input parameters. The authors developed and mathematically analyzed a numerically stable formulation for computing the T-matrix (J. Comput. Appl. Math. 234 (2010), 1702-1709). The TMATROM software package provides an object-oriented implementation of the numerically stable formulation and can be used in conjunction with the user's preferred forward solver for the two-dimensional Helmholtz model. We compare TMATROM with standard methods to compute the T-matrix for a range of two-dimensional test scatterers with large aspect ratios and acoustic sizes. Our numerical results demonstrate the robust numerical stability of the TMATROM implementation, even with scatterers for which the standard methods are numerically unstable. The efficiency and flexibility of the TMATROM software package to handle a wide range of two-dimensional scatterers with various shapes and material properties are also demonstrated. © 2017 ACM.",Acoustic scattering; Far field; Numerical stability; T-matrix,Acoustic wave scattering; Aspect ratio; Convergence of numerical methods; Numerical methods; Object oriented programming; Software packages; Wave propagation; Acoustic Scattering; Far field; Object-oriented implementation; Scattering simulations; Science and engineering; T matrix; Two-dimensional tests; Wave propagation modeling; Matrix algebra
Performance evaluation of a two-dimensional lattice Boltzmann solver using CUDA and PGAS UPC based parallelisation,2017,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025153257&doi=10.1145%2f3085590&partnerID=40&md5=ff0e75cbfc365677132a5605dcbbb778,"The Unified Parallel C (UPC) language from the Partitioned Global Address Space (PGAS) family unifies the advantages of shared and local memory spaces and offers a relatively straightforward code parallelisation with the Central Processing Unit (CPU). In contrast, the Computer Unified Device Architecture (CUDA) development kit gives a tool to make use of the Graphics Processing Unit (GPU). We provide a detailed comparison between these novel techniques through the parallelisation of a two-dimensional lattice Boltzmann method based fluid flow solver. Our comparison between the CUDA and UPC parallelisation takes into account the required conceptual effort, the performance gain, and the limitations of the approaches from the application oriented developers' point of view. We demonstrated that UPC led to competitive efficiency with the local memory implementation. However, the performance of the shared memory code fell behind our expectations, and we concluded that the investigated UPC compilers could not efficiently treat the shared memory space. The CUDA implementation proved to be more complex compared to the UPC approach mainly because of the complicated memory structure of the graphics card which also makes GPUs suitable for the parallelisation of the lattice Boltzmann method. © 2017 ACM.",CFD; Computational fluid dynamics; Compute unified device architecture; CUDA; Lattice Boltzmann method; LBM; NVIDIA; Partitioned global address space; PGAS; Unified parallel C; UPC,C (programming language); Computer graphics; Computer graphics equipment; Flow of fluids; Graphics processing unit; Memory architecture; Program processors; Compute unified device architectures; CUDA; Lattice Boltzmann method; NVIDIA; Partitioned Global Address Space; PGAS; Unified parallel C; Computational fluid dynamics
On the robustness of the 2Sum and Fast2Sum algorithms,2017,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025085309&doi=10.1145%2f3054947&partnerID=40&md5=e9af583a331eda26d0349788df1c6a92,"The 2Sum and Fast2Sum algorithms are important building blocks in numerical computing. They are used (implicitelyorexplicitely)inmanycompensated algorithms (suchascompensated summationorcompensated polynomial evaluation). They are also used for manipulating floating-point expansions. We show that these algorithms are much more robust than it is usually believed: The returned result makes sense even when the rounding function is not round-to-nearest, and they are almost immune to overflow. © 2017 ACM.",2Sum; Error-free transformation; Faithful rounding; Fast2Sum; Floating-point; Rounding errors,Computer software; Software engineering; 2Sum; Error-free transformations; Faithful rounding; Fast2Sum; Floating points; Rounding errors; Digital arithmetic
Algorithm 977: A QR-preconditioned QR SVD method for computing the SVD with high accuracy,2017,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025146893&doi=10.1145%2f3061709&partnerID=40&md5=18df20ca94236d22c61afc0925db7282,"A new software for computing the singular value decomposition (SVD) of real or complex matrices is proposed. The method implemented in the code xGESVDQ is essentially the QR SVD algorithm available as xGESVD in LAPACK. The novelty is an extra step, the QR factorization with column (or complete row and column) pivoting, also already available in LAPACK as xGEQP3. For experts in matrix computations, the combination of the QR factorization and an SVD computation routine is not new. However, what seems to be new and important for applications is that the resulting procedure is numerically superior to xGESVD and that it is capable of reaching the accuracy of the Jacobi SVD. Further, when combined with pivoted Cholesky factorization, xGESVDQ provides numerically accurate and fast solvers (designated as xPHEVC, xPSEVC) for the Hermitian positive definite eigenvalue problem. For instance, using accurately computed Cholesky factor, xPSEVC computes all eigenvalues of the 200×200 Hilbert matrix (whose spectral condition number is greater that 10300) to nearly full machine precision. Furthermore, xGESVDQ can be used for accurate spectral decomposition of general (indefinite) Hermitian matrices. © 2017 ACM.",Accuracy; Condition number; Jacobi method; Pivoting; SVD,Eigenvalues and eigenfunctions; Factorization; Jacobian matrices; Matrix algebra; Number theory; Accuracy; Cholesky factorizations; Condition numbers; Jacobi methods; Matrix computation; Pivoting; Spectral condition number; Spectral decomposition; Singular value decomposition
Algorithm 976: Bertini-real: Numerical decomposition of real algebraic curves and surfaces,2017,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025114977&doi=10.1145%2f3056528&partnerID=40&md5=febe54815baf84a5726b23f55f07ef7f,"Bertini-real is a compiled command line program for numerically decomposing the real portion of a positivedimensional complex component of an algebraic set. The software uses homotopy continuation to solve a series of systems via regeneration from a witness set to compute a cell decomposition. The implemented decomposition algorithms are similar to the well-known cylindrical algebraic decomposition (CAD) first established by Collins in that they produce a set of connected cells. In contrast to the CAD, Bertini-real produces cells with midpoints connected to boundary points by homotopies, which can easily be numerically tracked. Furthermore, the implemented decomposition for surfaces naturally yields a triangulation. This CAD-like decomposition captures the topological information and permits further computation on the real sets, such as sampling, visualization, and three-dimensional printing. © 2017 ACM.",Cell decompositions; Homotopy continuation; Numerical algebraic geometry; Polynomial system; Real solutions,Cells; Computer aided design; Cytology; Polynomials; Three dimensional computer graphics; Cell decomposition; Homotopy continuation; Numerical algebraic geometry; Polynomial systems; Real solutions; Algebra
"BFO, a trainable derivative-free brute force optimizer for nonlinear bound-constrained optimization and equilibrium computations with continuous and discrete variables",2017,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024481256&doi=10.1145%2f3085592&partnerID=40&md5=c0793aa514b5f0b3f43bd29a0924a70e,"A direct-search derivative-free Matlab optimizer for bound-constrained problems is described, whose remarkable features are its ability to handle a mix of continuous and discrete variables, a versatile interface as well as a novel self-training option. Its performance compares favorably with that of NOMAD (Nonsmooth Optimization by Mesh Adaptive Direct Search), a well-known derivative-free optimization package. It is also applicable to multilevel equilibrium- or constrained-type problems. Its easy-to-use interface provides a number of user-oriented features, such as checkpointing and restart, variable scaling, and early termination tools. © 2017 ACM.",Bound constraints; Derivative-free optimization; Direct-search methods; Mixed-integer optimization; Trainable algorithms,Integer programming; Optimization; Bound constrained optimization; Bound constrained problem; Bound constraints; Derivative-free optimization; Direct search methods; Mesh adaptive direct search; Mixed integer optimization; Nonsmooth optimization; Constrained optimization
Algorithm 978: Safe scaling in the level 1 BLAS,2017,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026483047&doi=10.1145%2f3061665&partnerID=40&md5=7624ae392c3a6db687a0354f790d7ecc,"The square root of a sum of squares is well known to be prone to overflow and underflow. Ad hoc scaling of intermediate results, as has been done in numerical software such as the BLAS and LAPACK, mostly avoids the problem, but it can still occur at extreme values in the range of representable numbers. More careful scaling, as has been implemented in recent versions of the standard algorithms, may come at the expense of performance or clarity. This work reimplements the vector 2-norm and the generation of Givens rotations from the Level 1 BLAS to improve their performance and design. In addition, support for negative increments is extended to the Level 1 BLAS operations on a single vector, and a comprehensive test suite for all the Level 1 BLAS is included. ©2017 ACM.",Givens rotation; Sum of squares; Vector operation,Software engineering; Extreme value; Givens Rotation; Intermediate results; Numerical software; Single vectors; Standard algorithms; Sum of squares; Vector operations; Computer software
On orienting edges of unstructured two- & three-dimensional meshes,2017,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026448090&doi=10.1145%2f3061708&partnerID=40&md5=bd0eec833053b4ae8572c7913245fd85,"Finite element codes typically use data structures that represent unstructured meshes as collections of cells, faces, and edges, each of which require associated coordinate systems. One then needs to store how the coordinate system of each edge relates to that of neighboring cells. However, we can simplify data structures and algorithms if we can a priori orient coordinate systems in such a way that the coordinate systems on the edges follow uniquely from those on the cells by rule. Such rules require that every unstructured mesh allow the assignment of directions to edges that satisfy the convention in adjacent cells. We show that the convention chosen for unstructured quadrilateral meshes in the DEAL.II library always allows to orient meshes. It can therefore be used to make codes simpler, faster, and less bug prone. We present an algorithm that orients meshes in O(N) operations. We then show that consistent orientations are not always possible for 3D hexahedral meshes. Thus, cells generally need to store the direction of adjacent edges, but our approach also allows the characterization of cases where this is not necessary. The 3D extension of our algorithm either orients edges consistently, or aborts, both within O(N) steps. © 2017 ACM.",Finite element meshes; Mesh generation; Orientation of edges; Quadrilateral & hexahedral meshes,Cells; Cytology; Data structures; Co-ordinate system; Consistent orientations; Finite element codes; Finite element meshes; Hexahedral mesh; Quadrilateral meshes; Unstructured meshes; Mesh generation
Implementing high-performance complex matrix multiplication via the 3m & 4m methods,2017,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026475510&doi=10.1145%2f3086466&partnerID=40&md5=e9d5c6bbddc433e7909af3e10af682e5,"In this article, we explore the implementation of complex matrix multiplication.We begin by briefly identifying various challenges associated with the conventional approach, which calls for a carefully written kernel that implements complex arithmetic at the lowest possible level (i.e., assembly language).We then set out to develop a method of complex matrix multiplication that avoids the need for complex kernels altogether. This constraint promotes code reuse and portability within libraries such as Basic Linear Algebra Subprograms and BLAS-Like Library Instantiation Software (BLIS) and allows kernel developers to focus their efforts on fewer and simpler kernels. We develop two alternative approaches-one based on the 3M method and one that reflects the classic 4M formulation-each with multiple variants, all of which rely only on real matrix multiplication kernels. We discuss the performance characteristics of these ""induced"" methods and observe that the assembly-level method actually resides along the 4M spectrum of algorithmic variants. Implementations are developed within the BLIS framework, and testing on modern hardware confirms that while the less numerically stable 3M method yields the fastest runtimes, the more stable (and thus widely applicable) 4M method's performance is somewhat limited due to implementation challenges that appear inherent in nature. ©2017 ACM.",3m; 4m; BLAS; BLIS; Complex; DLA; High-performance; Induced; Kernel; Linear algebra; Matrix; Micro-kernel; Multiplication,Algebra; Computer software portability; Computer software reusability; Linear algebra; BLAS; BLIS; Complex; High-performance; Induced; Kernel; Micro kernel; Multiplication; Matrix algebra
Complex additive geometric multilevel solvers for Helmholtz equations on spacetrees,2017,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017620213&doi=10.1145%2f3054946&partnerID=40&md5=a0ac89e74fc349e577f09444424ed976,"We introduce a family of implementations of low-order, additive, geometric multilevel solvers for systems of Helmholtz equations arising from Schrodinger equations. Both grid spacing and arithmetics may comprise complex numbers, and we thus can apply complex scaling to the indefinite Helmholtz operator. Our implementations are based on the notion of a spacetree and work exclusively with a finite number of precomputed local element matrices. They are globally matrix-free. Combining various relaxation factors with two grid transfer operators allows us to switch from additive multigrid over a hierarchical basis method into a Bramble-Pasciak-Xu (BPX)-type solver, with several multiscale smoothing variants within one code base. Pipelining allows us to realize full approximation storage (FAS) within the additive environment where, amortized, each grid vertex carrying degrees of freedom is read/written only once per iteration. The codes realize a single-touch policy. Among the features facilitated by matrix-free FAS is arbitrary dynamic mesh refinement (AMR) for all solver variants. AMR as an enabler for full multigrid (FMG) cycling - the grid unfolds throughout the computation - allows us to reduce the cost per unknown. The present work primary contributes toward software realization and design questions. Our experiments show that the consolidation of single-touch FAS, dynamic AMR, and vectorization-friendly complex scaled, matrix-free FMG cycles delivers a mature implementation blueprint for solvers of Helmholtz equations in general. For this blueprint, we put particular emphasis on a strict implementation formalism as well as some implementation correctness proofs. © 2017 ACM.",Additive multigrid; AMR; BPX; Helmholtz; Vectorization,Degrees of freedom (mechanics); Helmholtz equation; Iterative methods; Schrodinger equation; Full approximation storages; Helmholtz; Helmholtz operators; Hierarchical basis methods; Multi-grid; Multi-scale smoothing; Relaxation factors; Vectorization; Matrix algebra
SYM-ILDL: Incomplete LDLT factorization of symmetric indefinite and skew-symmetric matrices,2017,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017639899&doi=10.1145%2f3054948&partnerID=40&md5=27bbd86febf5c5de6e055a03ae504a00,"SYM-ILDL is a numerical software package that computes incomplete LDL (ILDL) factorizations of symmetric indefinite and real skew-symmetric matrices. The core of the algorithm is a Crout variant of incomplete LU (ILU), originally introduced and implemented for symmetric matrices by Li and Saad [2005]. Our code is economical in terms of storage, and it deals with real skew-symmetric matrices as well as symmetric ones. The package is written in C++ and is templated, is open source, and includes a MATLAB interface. The code includes built-in RCM and AMD reordering, two equilibration strategies, threshold Bunch-Kaufman pivoting, and rook pivoting, as well as a wrapper to MC64, a popular matching-based equilibration and reordering algorithm. We also include two built-in iterative solvers: SQMR, preconditioned with ILDL, and MINRES, preconditioned with a symmetric positive definite preconditioner based on the ILDL factorization. © 2017 ACM.",Skew-symmetric matrices; Symmetric indefinite,C++ (programming language); Factorization; Iterative methods; MATLAB; Open source software; Iterative solvers; Numerical software; Popular matching; Preconditioners; Skew-symmetric matrices; Symmetric indefinite; Symmetric matrices; Symmetric positive definite; Matrix algebra
An algorithm for the optimization of finite element integration loops,2017,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017476552&doi=10.1145%2f3054944&partnerID=40&md5=7a296853aa2c1932b394badeb6bf3e07,"We present an algorithm for the optimization of a class of finite-element integration loop nests. This algorithm, which exploits fundamental mathematical properties of finite-element operators, is proven to achieve a locally optimal operation count. In specified circumstances the optimum achieved is global. Extensive numerical experiments demonstrate significant performance improvements over the state of the art in finiteelement code generation in almost all cases. This validates the effectiveness of the algorithm presented here and illustrates its limitations. © 2017 ACM.",Compilers; Finite element integration; Local assembly; Performance optimization,Integration; Mathematical operators; Program compilers; Finite-element codes; Local assembly; Loop nests; Mathematical properties; Numerical experiments; Optimal operation; Performance optimizations; State of the art; Optimization
Algorithm 974: The outlierLib-A MATLAB library for outliers' detection,2017,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017177557&doi=10.1145%2f3054078&partnerID=40&md5=4e907382eac8820634d72a0f4e93f046,The article presents a library of MATLAB functions that implement the widely used algorithms of outlier detection. The library includes the outlier tests for univariate and multivariate data sets with an approximately normal distribution. The software library is accompanied by a brief review of the methods for detecting and treating outliers. © 2017 ACM.,Outlier,MATLAB; Normal distribution; Matlab functions; Multivariate data sets; Outlier; Outlier Detection; Software libraries; Univariate; Statistics
Remark on Algorithm 936: A Fortran Message Processor,2017,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017100183&doi=10.1145%2f3004279&partnerID=40&md5=8b0528b7783b2d4f2b72f044e10e4682,[No abstract available],Accuracy; Debugging; Fortran,
Algorithm 973: Extended rational Fejér quadrature rules based on chebyshev orthogonal rational functions,2017,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017126522&doi=10.1145%2f3054077&partnerID=40&md5=d653118c9af771d8e5238663e140a197,"We present a numerical procedure to approximate integrals of the form ∫ab f (x)dx, where f is a function with singularities close to, but outside the interval [a, b], with -∞ ≤ a < b ≤ +∞. The algorithm is based on rational interpolatory Fejér quadrature rules, together with a sequence of real and/or complex conjugate poles that are given in advance. Since for n fixed in advance, the accuracy of the computed nodes and weights in the n-point rational quadrature formula strongly depends on the given sequence of poles, we propose a small number of iterations over the number of points in the rational quadrature rule, limited by the value n (instead of fixing the number of points in advance) in order to obtain the best approximation among the first n. The proposed algorithm is implemented as a MATLAB program. © 2017 ACM.",Fejér; Numerical integration; Orthogonal rational functions; Quadrature; Rational interpolation,Approximation algorithms; MATLAB; Orthogonal functions; Poles; Best approximations; Complex conjugate poles; Number of iterations; Numerical integrations; Numerical procedures; Orthogonal rational functions; Quadrature; Rational interpolation; Rational functions
Algorithm 971: An implementation of a randomized algorithm for principal component analysis,2017,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011340773&doi=10.1145%2f3004053&partnerID=40&md5=2f30831ded610e3d0c5d551cb092d712,"Recent years have witnessed intense development of randomizedmethods for low-rank approximation. These methods target principal component analysis and the calculation of truncated singular value decompositions. The present article presents an essentially black-box, foolproof implementation for Mathworks' MATLAB, a popular software platform for numerical computation. As illustrated via several tests, the randomized algorithms for low-rank approximation outperform or at least match the classical deterministic techniques (such as Lanczos iterations run to convergence) in basically all respects: accuracy, computational efficiency (both speed and memory usage), ease-of-use, parallelizability, and reliability. However, the classical procedures remain the methods of choice for estimating spectral norms and are far superior for calculating the least singular values and corresponding singular vectors (or singular subspaces). © 2017 ACM.",PCA; Principal component analysis; Singular value decomposition; SVD,Approximation algorithms; Approximation theory; Computational efficiency; Digital filters; MATLAB; Singular value decomposition; Deterministic technique; Lanczos iterations; Low rank approximations; Numerical computations; Randomized Algorithms; Singular subspaces; Software platforms; Truncated singular value decomposition; Principal component analysis
Certified roundoff error bounds using semidefinite programming,2017,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009349595&doi=10.1145%2f3015465&partnerID=40&md5=d5e534e02797d964c9857a7f19b0e507,"Roundoff errors cannot be avoided when implementing numerical programs with finite precision. The ability to reason about rounding is especially important if one wants to explore a range of potential representations, for instance, for FPGAs or custom hardware implementations. This problem becomes challenging when the program does not employ solely linear operations as non-linearities are inherent to many interesting computational problems in real-world applications. Existing solutions to reasoning possibly lead to either inaccurate bounds or high analysis time in the presence of nonlinear correlations between variables. Furthermore, while it is easy to implement a straight-forward method such as interval arithmetic, sophisticated techniques are less straight-forward to implement in a formal setting. Thus there is a need for methods that output certificates that can be formally validated inside a proof assistant. We present a framework to provide upper bounds on absolute roundoff errors of floating-point nonlinear programs. This framework is based on optimization techniques employing semidefinite programming and sums of squares certificates, which can be checked inside the Coq theorem prover to provide formal roundoff error bounds for polynomial programs. Our tool covers a wide range of nonlinear programs, including polynomials and transcendental operations as well as conditional statements. We illustrate the efficiency and precision of this tool on non-trivial programs coming from biology, optimization, and space control. Our tool produces more accurate error bounds for 23% of all programs and yields better performance in 66% of all programs. © 2017 ACM.",Correlation sparsity pattern; Floating-point arithmetic; Formal verification; Polynomial optimization; Proof assistant; Roundoff error; Semidefinite programming; Transcendental functions,Digital arithmetic; Error analysis; Formal verification; Hardware; Nonlinear programming; Polynomials; Theorem proving; Polynomial optimization; Proof assistant; Round-off errors; Semi-definite programming; Sparsity patterns; Transcendental functions; Application programs
Parallel minimum norm solution of sparse block diagonal column overlapped underdetermined systems,2017,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009355063&doi=10.1145%2f3004280&partnerID=40&md5=7e3ca473d62ce717cab8b0ea525161a7,"Underdetermined systems of equations in which the minimum norm solution needs to be computed arise in many applications, such as geophysics, signal processing, and biomedical engineering. In this article, we introduce a new parallel algorithm for obtaining the minimum 2-norm solution of an underdetermined system of equations. The proposed algorithm is based on the Balance scheme, which was originally developed for the parallel solution of banded linear systems. The proposed scheme assumes a generalized banded form where the coefficient matrix has column overlapped block structure in which the blocks could be dense or sparse. In this article, we implement the more general sparse case. The blocks can be handled independently by any existing sequential or parallel QR factorization library. A smaller reduced system is formed and solved before obtaining the minimum norm solution of the original system in parallel. We experimentally compare and confirm the error bound of the proposed method against the QR factorization based techniques by using true single-precision arithmetic. We implement the proposed algorithm by using the message passing paradigm. We demonstrate numerical effectiveness as well as parallel scalability of the proposed algorithm on both shared and distributed memory architectures for solving various types of problems. © 2017 ACM.",Balance method; Minimum norm solution; Parallel algorithms; Underdetermined least square problems,Biomedical engineering; Factorization; Linear systems; Memory architecture; Message passing; Parallel algorithms; Signal processing; Balance methods; Least square problems; Message passing paradigms; Minimum norm solutions; Numerical effectiveness; Parallel scalability; Shared and distributed memory architectures; Underdetermined systems; Least squares approximations
Biqcrunch: A semidefinite branch-and-bound method for solving binary quadratic problems,2017,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009431629&doi=10.1145%2f3005345&partnerID=40&md5=901d97131931be203336f59b50fe714e,"This article presents BiqCrunch, an exact solver for binary quadratic optimization problems. BiqCrunch is a branch-and-bound method that uses an original, efficient semidefinite-optimization-based bounding procedure. It has been successfully tested on a variety of well-known combinatorial optimization problems, such as Max-Cut, Max-k-Cluster, and Max-Independent-Set. The code is publicly available online; a web interface and many conversion tools are also provided. © 2017 ACM.",Binary quadratic programming; Exact resolution; NP-hard; Quasi-Newton; Semidefinite relaxations,Bins; Combinatorial optimization; Optimization; Problem solving; Quadratic programming; Binary quadratic programming; Combinatorial optimization problems; Max independent sets; NP-hard; Quadratic optimization problems; Quasi-Newton; Semidefinite optimization; Semidefinite relaxation; Branch and bound method
Algorithm 972: JMarkov: An integrated framework for Markov chain modeling,2017,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011339690&doi=10.1145%2f3009968&partnerID=40&md5=64fd83b593cb72483141e59f1e1ba7dc,"Markov chains (MC) are a powerful tool for modeling complex stochastic systems. Whereas a number of tools exist for solving different types ofMCmodels, the first step inMCmodeling is to define themodel parameters. This step is, however, error prone and far from trivial when modeling complex systems. In this article, we introduce jMarkov, a framework for MC modeling that provides the user with the ability to define MC models from the basic rules underlying the system dynamics. From these rules, jMarkov automatically obtains the MC parameters and solves the model to determine steady-state and transient performance measures. The jMarkov framework is composed of four modules: (i) the main module supports MC models with a finite state space; (ii) the jQBD module enables the modeling of Quasi-Birth-and-Death processes, a class of MCs with infinite state space; (iii) the jMDP module offers the capabilities to determine optimal decision rules based on Markov Decision Processes; and (iv) the jPhase module supports the manipulation and inclusion of phase-type variables to representmore general behaviors than that of the standard exponential distribution. In addition, jMarkov is highly extensible, allowing the users to introduce new modeling abstractions and solvers. © 2017 ACM.",Markov chains; Markov decision processes; Phase-type distributions; Quasi-birth-and-death processes; Stochastic modeling,Chains; Queueing theory; Stochastic models; Stochastic systems; Exponential distributions; Infinite state space; Integrated frameworks; Markov Decision Processes; Optimal decision-rule; Phase type distributions; Quasi-birth and death process; Steady state and transients; Markov processes
Chopping a chebyshev series,2017,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011371552&doi=10.1145%2f2998442&partnerID=40&md5=5163d33d2d798dddb15693f2b6872a9f,"Chebfun and related software projects for numerical computing with functions are based on the idea that at each step of a computation, a function f (x) defined on an interval [a, b] is ""rounded"" to a prescribed precision by constructing a Chebyshev series and chopping it at an appropriate point. Designing a chopping algorithm with the right properties proves to be a surprisingly complex and interesting problem.We describe the chopping algorithm introduced in Chebfun Version 5.3 in 2015 after many years of discussion and the considerations that led to this design. © 2017 ACM.",Chebfun; Chebyshev series; Floating point arithmetic,Computer software; Software engineering; Chebfun; Chebyshev series; Chopping algorithms; nocv1; Numerical computing; Software project; Digital arithmetic
The state-of-the-art of preconditioners for sparse linear least-squares problems,2017,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009349602&doi=10.1145%2f3014057&partnerID=40&md5=5d18fafae90b50ecc9bf8b85dd36cb89,"In recent years, a variety of preconditioners have been proposed for use in solving large sparse linear least-squares problems. These include simple diagonal preconditioning, preconditioners based on incomplete factorizations, and stationary inner iterations used with Krylov subspace methods. In this study, we briefly review preconditioners for which software has been made available, then present a numerical evaluation of them using performance profiles and a large set of problems arising from practical applications. Comparisons are made with state-of-the-art sparse direct methods. © 2017 ACM.",Augmented system; Direct solvers; Iterative solvers; Least-squares problems; Normal equations; Preconditioning; Sparse matrices,Application programs; Iterative methods; Surface reconstruction; Augmented systems; Direct solvers; Iterative solvers; Least squares problems; Normal equations; Preconditioning; Sparse matrices; Channel capacity
Sparse matrix-vector multiplication on GPGPUs,2017,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011343064&doi=10.1145%2f3017994&partnerID=40&md5=17e87545cfafb805ed1f1b634d670107,"The multiplication of a sparse matrix by a dense vector (SpMV) is a centerpiece of scientific computing applications: it is the essential kernel for the solution of sparse linear systems and sparse eigenvalue problems by iterative methods. The efficient implementation of the sparse matrix-vector multiplication is therefore crucial and has been the subject of an immense amount of research, with interest renewed with every major new trend in high-performance computing architectures. The introduction of General-Purpose Graphics Processing Units (GPGPUs) is no exception, and many articles have been devoted to this problem. With this article, we provide a review of the techniques for implementing the SpMV kernel on GPGPUs that have appeared in the literature of the last few years.We discuss the issues and tradeoffs that have been encountered by the various researchers, and a list of solutions, organized in categories according to common features. We also provide a performance comparison across different GPGPU models and on a set of test matrices coming from various application domains. © 2017 ACM.",GPU programming; Sparse matrices,Computer architecture; Computer graphics; Eigenvalues and eigenfunctions; Graphics processing unit; Iterative methods; Linear systems; Program processors; Efficient implementation; GPU programming; High performance computing; Performance comparison; Scientific computing applications; Sparse linear systems; Sparse matrices; Sparse matrix-vector multiplication; Matrix algebra
PSelInv-A distributed memory parallel algorithm for selected inversion: The symmetric case,2016,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85008943101&doi=10.1145%2f2786977&partnerID=40&md5=c0a1c08e22355d65642e1340554ebf1e,"We describe an efficient parallel implementation of the selected inversion algorithm for distributed memory computer systems, which we call PSelInv. The PSelInv method computes selected elements of a general sparse matrix Athat can be decomposed as A= LU, where L is lower triangular and U is upper triangular. The implementation described in this article focuses on the case of sparse symmetric matrices. It contains an interface that is compatible with the distributed memory parallel sparse direct factorization SuperLU-DIST. However, the underlying data structure and design of PSelInv allows it to be easily combined with other factorization routines, such as PARDISO. We discuss general parallelization strategies such as data and task distribution schemes. In particular, we describe how to exploit the concurrency exposed by the elimination tree associated with the LU factorization of A. We demonstrate the efficiency and accuracy of PSelInv by presenting several numerical experiments. In particular, we show that PSelInv can run efficiently on more than 4,000 cores for a modestly sized matrix. We also demonstrate how PSelInv can be used to accelerate large-scale electronic structure calculations. © 2016 ACM.",Distributed memory parallel algorithm; Electronic structure theory; High-performance computation; Selected inversion; Sparse direct method,Computation theory; Distributed computer systems; Electronic structure; Factorization; Memory architecture; Parallel algorithms; Uranium; Direct method; Distributed Memory; Electronic structure theory; High performance computation; Selected inversion; Matrix algebra
Algorithm 970: Optimizing the NIST statistical test suite and the berlekamp-massey algorithm,2016,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006974268&doi=10.1145%2f2988228&partnerID=40&md5=c636316855a25e9f2e7a047762f09426,"The NIST Statistical Test Suite (NIST STS) is one of the most popular tools for the analysis of randomness. This test battery is widely used, but its implementation is quite inefficient. A complete randomness analysis using the NIST STS can take hours on a standard computer when the tested data volume is on the order of GB. We improved the most time-consuming test (Linear Complexity) from the previous most efficient implementation of the NIST STS. We also optimized other tests and achieved an overall speedup of 50.6× compared with the reference implementation. This means that 20MB of data can be tested within a minute using our new optimized version of the NIST STS. To speed up the Linear Complexity test, we proposed a new version of the Berlekamp-Massey algorithm that computes only the linear complexity of a sequence. This new variant does not construct a linear feedback shift register and is approximately 187× faster than the original NIST implementation of the Berlekamp-Massey algorithm. © 2016 ACM.",Berlekamp-Massey algorithm; NIST STS; Statistical randomness testing,Random processes; Shift registers; Statistical tests; Berlekamp-Massey algorithm; Efficient implementation; Linear complexity; Linear feedback shift registers; NIST STS; Randomness testing; Reference implementation; Test batteries; Frequency standards
An efficient representation format for fuzzy intervals based on symmetric membership functions,2016,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85007019971&doi=10.1145%2f2939364&partnerID=40&md5=469a63ee4d468c40e8f20acd03a765d6,"This article addresses the execution cost of arithmetic operations with a focus on fuzzy arithmetic. Thanks to an appropriate representation format for fuzzy intervals, we show that it is possible to halve the number of operations and divide by 2 to 8 the memory requirements compared to conventional solutions. In addition, we demonstrate the benefit of some hardware features encountered in today's accelerators (GPU) such as static rounding, memory usage, instruction-level parallelism (ILP), and thread-level parallelism (TLP). We then describe a library of fuzzy arithmetic operations written in CUDA and C++. The library is evaluated against traditional approaches using compute-bound and memory-bound benchmarks on Nvidia GPUs, with an observed performance gain of 2 to 20. © 2016 ACM.",Fuzzy intervals; Graphic processing units; Lower-upper; Midpoint-radius,Benchmarking; Fuzzy sets; Program processors; Fuzzy arithmetic operations; Fuzzy interval; Graphic processing units; Instruction level parallelism; Lower-upper; Midpoint-radius; Thread-level parallelism; Traditional approaches; Membership functions
GPU-accelerated generation of correctly rounded elementary functions,2016,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85007007611&doi=10.1145%2f2935746&partnerID=40&md5=d962867106337b85aa881880a35bdfc2,"The IEEE 754-2008 standard recommends the correct rounding of some elementary functions. This requires solving the Table Maker's Dilemma (TMD), which implies a huge amount of CPU computation time. In this article, we consider accelerating such computations, namely the Lefèvre algorithm on graphics processing units (GPUs), which are massively parallel architectures with a partial single instruction, multiple data execution. We first propose an analysis of the Lefèvre hard-to-round argument search using the concept of continued fractions. We then propose a new parallel search algorithm that is much more efficient on GPUs thanks to its more regular control flow. We also present an efficient hybrid CPU-GPU deployment of the generation of the polynomial approximations required in the Lefèvre algorithm. In the end, we manage to obtain overall speedups up to 53.4× on one GPU over a sequential CPU execution and up to 7.1× over a hex-core CPU, which enable a much faster solution of the TMD for the double-precision format. © 2016 ACM 0098-3500/2016/12-ART22 $15.00.",Control flow divergence; Correct rounding; Elementary function; Floating-point arithmetic; GPU computing; Lefèvre algorithm; Simd; Table Maker's Dilemma,Computer graphics; Digital arithmetic; Parallel architectures; Polynomial approximation; Program processors; Control flows; Correct rounding; Elementary function; GPU computing; Simd; Table Maker's Dilemma; Approximation algorithms
IEEE754 precision-kbase-β arithmetic inherited by precision-m base-β arithmetic for k<m,2016,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006963171&doi=10.1145%2f2785965&partnerID=40&md5=b2a3181adbf916ef35b2c1182b123949,"Suppose an m-digit floating-point arithmetic in base β ≥ 2 following the IEEE754 arithmetic standard is available. We show how a k-digit arithmetic with k < mcan be inherited solely using m-digit operations. This includes the rounding into kdigits, the four basic operations and the square root, all for even or odd base β. In particular, we characterize the relation between k and mso that no double rounding occurs when computing in mdigits and rounding the result into k digits. We discuss rounding to nearest as well as directed rounding, and our approach covers exceptional values including signed zero. For binary arithmetic, a Matlab toolbox based on binary64 including k-bit scalar, vector and matrix operations as well as k-bit interval arithmetic is part of Version 8 of INTLAB, the Matlab toolbox for reliable computing. © 2016 ACM.",Base-β; Double rounding; Floating-point arithmetic; IEEE754; Interval arithmetic; INTLAB; Unit in the first place (UFP),Bins; MATLAB; Double rounding; IEEE-754; Interval arithmetic; INTLAB; Unit in the first place (UFP); Digital arithmetic
Algorithm 968: Disode45: A matlab Runge-Kutta solver for piecewise smooth IVPs of Filippov type,2016,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006980621&doi=10.1145%2f2907054&partnerID=40&md5=742d56cb82dab5eb106a6899aa5c9404,"In this article, an adaptive Runge-Kutta code, based on the DOPRI5(4) pair for solving initial value problems (IVPs) for differential systems with piecewise smooth solutions (PWS) is presented and the algorithms used in the code are described. The code automatically detects and locates accurately the switching points of the PWS, restarting the integration after each discontinuity. Further, in the case of Filippov systems, algorithms to handle properly sliding mode regimes in an automatic way are included. The code requires the user to provide a description of the IVP and the functions defining the hypersurfaces where the switching points are located, and it returns the discrete approximated solution together with the switching points. Several numerical experiments are presented to illustrate the reliability and efficiency of the code. © 2016 ACM.",Adaptive runge-kutta methods; Detection of discontinuities; Filippov systems; Nonsmooth initial value problems,Codes (symbols); Initial value problems; Approximated solutions; Detection-of-discontinuities; Differential systems; Filippov systems; Non-smooth initial value problems; Numerical experiments; Piecewise smooth; Switching points; Runge Kutta methods
Firedrake: Automating the finite element method by composing abstractions,2016,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994033910&doi=10.1145%2f2998441&partnerID=40&md5=29fe7aab85ec249e76f44737ced02cb2,"Firedrake is a new tool for automating the numerical solution of partial differential equations. Firedrake adopts the domain-specific language for the finite element method of the FEniCS project, but with a pure Python runtime-only implementation centered on the composition of several existing and new abstractions for particular aspects of scientific computing. The result is a more complete separation of concerns that eases the incorporation of separate contributions from computer scientists, numerical analysts, and application specialists. These contributions may add functionality or improve performance. Firedrake benefits from automatically applying new optimizations. This includes factorizing mixed function spaces, transforming and vectorizing inner loops, and intrinsically supporting block matrix operations. Importantly, Firedrake presents a simple public API for escaping the UFL abstraction. This allows users to implement common operations that fall outside of pure variational formulations, such as flux limiters.",Abstraction; Code generation; UFL,Abstracting; Computer programming languages; Problem oriented languages; Abstraction; Code Generation; Computer scientists; Domain specific languages; Improve performance; Numerical solution of partial differential equations; Separation of concerns; Variational formulation; Finite element method
Algorithm 969: Computation of the incomplete gamma function for negative values of the argument,2016,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006994737&doi=10.1145%2f2972951&partnerID=40&md5=df268cf762d8ca81868d4fe70ec48808,"An algorithm for computing the incomplete gamma function γ∗(a, z) for real values of the parameter a and negative real values of the argument z is presented. The algorithm combines the use of series expansions, Poincare-type expansions, uniform asymptotic expansions, and recurrence relations, depending on the parameter region. A relative accuracy ∼10-13 in the parameter region (a, z) [-500, 500] × [-500, 0) can be obtained when computing the function γ∗(a, z) with the Fortran 90 module IncgamNEG implementing the algorithm. © 2016 ACM.",Asymptotic expansions; Incomplete gamma function; Recurrence relations,Asymptotic analysis; Expansion; Asymptotic expansion; Incomplete gamma functions; Parameter regions; Recurrence relations; Relative accuracy; Series expansion; Type expansions; Uniform asymptotic expansion; Parameter estimation
Parallel memory-efficient adaptive mesh refinement on structured triangular meshes with billions of grid cells,2016,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009351705&doi=10.1145%2f2947668&partnerID=40&md5=e32982381425eea6a58c2c70d6852372,"We present sam(oa)2, a software package for a dynamically adaptive, parallel solution of 2D partial differential equations on triangular grids created via newest vertex bisection. An element order imposed by the Sierpinski space-filling curve provides an algorithm for grid generation, refinement, and traversal that is inherently memory efficient. Based purely on stack and stream data structures, it completely avoids random memory access. Using an element-oriented data view suitable for local operators, concrete simulation scenarios are implemented based on control loops and event hooks, which hide the complexity of the underlying traversal scheme. Two case studies are presented: two-phase flow in heterogeneous porous media and tsunami wave propagation, demonstrated on the Tohoku tsunami 2011 in Japan. sam(oa)2 features hybrid MPI+OpenMP parallelization based on the Sierpinski order induced on the elements. Sections defined by contiguous grid cells define atomic tasks for OpenMP work sharing and stealing, as well as for migration of grid cells between MPI processes. Using optimized communication and load balancing algorithms, sam(oa)2 achieves 88% strong scaling efficiency from 16 to 512 cores and 92% efficiency in a weak scaling test on 8,192 cores with 10 billion elements-all tests including adaptive mesh refinement and load balancing in each time step. © 2016 Copyright is held by the owner/author(s).",Cache efficient; Hybrid parallelization; Memory efficient; Parallel adaptive mesh refinement; Porous media flow; Space-filling curve; Tsunami wave propagation,Application programming interfaces (API); Assembly; Efficiency; Memory architecture; Mesh generation; Numerical analysis; Porous materials; Random access storage; Tsunamis; Two phase flow; Wave propagation; Adaptive mesh refinement; Cache-efficient; Hybrid parallelization; Memory efficient; Porous-media flow; Space-filling curve; Tsunami waves; Cache memory
ADiJaC-automatic differentiation of Java classfiles,2016,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988373696&doi=10.1145%2f2904901&partnerID=40&md5=c4be0d7a756b39aa7dd07c4e327fdd90,"This work presents the current design and implementation of ADiJaC, an automatic differentiation tool for Java classfiles. ADiJaC uses source transformation to generate derivative codes in both the forward and the reverse modes of automatic differentiation. We describe the overall architecture of the tool and present various details and examples for each of the two modes of differentiation. We emphasize the enhancements that have been made over previous versions of ADiJaC and illustrate their influence on the generality of the tool and on the performance of the generated derivative codes. The ADiJaC tool has been used to generate derivatives for a variety of problems, including real-world applications. We evaluate the performance of such codes and compare it to derivatives generated by Tapenade, a well-established automatic differentiation tool for Fortran and C/C++. Additionally, we present a more detailed performance analysis of a real-world application. Apart from being the only general-purpose automatic differentiation tool for Java bytecode, we argue that ADiJaC's features and performance are comparable to those of similar mature tools for other programming languages such as C/C++ or Fortran. © 2016 ACM.",Source transformation,Codes (symbols); FORTRAN (programming language); Java programming language; Automatic differentiation tool; Automatic differentiations; Design and implementations; Java byte codes; Performance analysis; Real-world; Reverse mode; Source transformation; C (programming language)
Algorithm 965: RIDC methods: A family of parallel time integrators,2016,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057826995&doi=10.1145%2f2964377&partnerID=40&md5=6a1760c8d46f4a04458bc9666472e4dd,"Revisionist integral deferred correction methods are a family of parallel-in-time methods to solve systems of initial values problems. The approach is able to bootstrap lower-order time integrators to provide high-order approximations in approximately the same wall-clock time, hence providing a multiplicative increase in the number of compute cores utilized. Here we provide a library that automatically produces a parallel-in-time solution of a system of initial value problems given user-supplied code for the right-hand side of the system and a sequential code for a first-order timestep. The user-supplied timestep routine may be explicit or implicit and may make use of any auxiliary libraries that take care of the solution of any nonlinear algebraic systems that may arise or the numerical linear algebra required. © 2016 ACM 0098-3500/2016/08-ART8 $15.00",Deferred correction; Parallel in time,Linear algebra; Deferred correction; High-order approximation; Integral deferred corrections; Non-linear algebraic system; Numerical Linear Algebra; Parallel in time; Right-hand sides; Time integrators; Initial value problems
General template units for the finite volume method in box-shaped domains,2016,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057866245&doi=10.1145%2f2835175&partnerID=40&md5=a4198353f1a52c6f480b681254adb0e5,"In this work, we develop an extension of the Curiously Recurring Template Pattern (CRTP), which allows us to organize three related concepts in a class hierarchy. Generalizations, specializations and special procedures are the concepts that we use to define and implement several tools. We call these tools general template units because they are well-defined building blocks (units) for numerically solving partial differential equations (PDEs), are based on the use of templates of the C++ language, and can be applied in the solution of different kinds of problems. We focus on the solution of PDEs using the Finite Volume Method (FVM) in box-shaped domains. The three concepts just mentioned are intensively used to generate optimized codes for each case study. The convenience of our approach is highlighted in the numerical solutions of the examples of application, including laminar thermal convection, turbulent thermal convection, as well as a two-phase flow model in porous media, all of them in one, two, and three dimensions. The mathematical models of these examples were obtained using the axiomatic formulation, which provides generality, simplicity, and clarity to tackle any continuum mechanics application. The ideas explained in this work are quite simple but powerful in solving fluid dynamics problems, in which the conservativeness of the FVM is an important feature. The techniques developed in this work allow us to swap easily between numerical schemes for computing the coefficients obtained by applying the FVM. © 2016 ACM 0098-3500/2016/08-ART1 $15.00",Generic programming; Natural convection; Two-phase flow,C++ (programming language); Continuum mechanics; Finite volume method; Heat convection; Natural convection; Porous materials; Class hierarchies; Generic programming; Important features; Numerical scheme; Numerical solution; Partial Differential Equations (PDEs); Turbulent thermal convection; Two phase flow model; Two phase flow
Systematic alias sampling: An efficient and low-variance way to sample from a discrete distribution,2016,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84989234925&doi=10.1145%2f2935745&partnerID=40&md5=a7b87262e61c49f3d920833e26c7b69c,"In this article, we combine the Alias method with the concept of systematic sampling, a method commonly used in particle filters for efficient low-variance resampling. The proposed method allows very fast sampling from a discrete distribution: drawing k samples is up to an order of magnitude faster than binary search from the cumulative distribution function (cdf) or inversion methods used in many libraries. The produced empirical distribution function is evaluated using a modified Cramér-Von Mises goodness-of-fit statistic, showing that the method compares very favorably to multinomial sampling. As continuous distributions can often be approximated with discrete ones, the proposed method can be used as a very general way to efficiently produce random samples for particle filter proposal distributions, for example, for motion models in robotics. © 2016 ACM.",Particle filtering; Sampling; Variance reduction,Distributed computer systems; Monte Carlo methods; Sampling; Signal filtering and prediction; Target tracking; Continuous distribution; Cumulative distribution function; Discrete distribution; Empirical distribution functions; Goodness-of-fit statistics; Particle Filtering; Proposal distribution; Variance reductions; Distribution functions
Stability and performance of various singular value QR implementations on multicore CPU with a GPU,2016,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988410061&doi=10.1145%2f2898347&partnerID=40&md5=575fa00b4a9bc2ad780179c50de8e38e,"Singular Value QR (SVQR) can orthonormalize a set of dense vectors with the minimum communication (one global reduction between the parallel processing units, and BLAS-3 to performmost of its local computation). As a result, compared to other orthogonalization schemes, SVQR obtains superior performance on many of the current computers, where the communication has become significantly more expensive compared to the arithmetic operations. In this article, we study the stability and performance of various SVQR implementations on multicore CPUs with a GPU. Our focus is on the dense triangular solve, which performs half of the total floating-point operations of SVQR. As a part of this study, we examine an adaptive mixed-precision variant of SVQR, which decides if a lower-precision arithmetic can be used for the triangular solution at runtime without increasing the order of its orthogonality error (though its backward error is significantly greater). If the greater backward error can be tolerated, then our performance results with an NVIDIA Kepler GPU show that the mixed-precision SVQR can obtain a speedup of up to 1.36 over the standard SVQR. © 2016 ACM.",GPU computation; Mixed precision; Orthogonalization,Digital arithmetic; Program processors; Arithmetic operations; Floating point operations; GPU computation; Local computation; Mixed precision; Multi-core cpus; Orthogonalization; Parallel processing; Errors
On BLAS level-3 implementations of common solvers for (quasi-) triangular generalized Lyapunov equations,2016,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057848950&doi=10.1145%2f2850415&partnerID=40&md5=d7df05ab8b9c7ce7e81b1541899096f7,"The solutions of Lyapunov and generalized Lyapunov equations are a key player in many applications in systems and control theory. Their stable numerical computation, when the full solution is sought, is considered solved since the seminal work of Bartels and Stewart [1972] and its generalization by Penzl [1997]. Those variants do not go completely beyond BLAS level-2 style implementation. On modern computers, however, the formulation of level-3 BLAS type implementations is crucial to enable optimal usage of cache hierarchies and modern block scheduling methods based on directed acyclic graphs describing the interdependence of single block computations. Although there exists a recursive blocked solution scheme for (quasi-) triangular generalized Lyapunov equations [Jonsson and Kågström 2002b], we focus on standard blocking techniques. Using the standard blocking approach our contribution lifts the aforementioned level-2 algorithm by Penzl to BLAS level-3 for (quasi-) triangular equations. Especially, we consider the solution of the appearing Sylvester equations and provide a hybrid algorithm merging our strategy with the recursive blocking method above. © 2016 ACM 0098-3500/2016/08-ART3 $15.00",Bartels-Stewart algorithm; BLAS level-3; Blocked algorithm; Lyapunov equation; Sylvester equation,Computation theory; Directed graphs; BLAS level-3; Blocked algorithms; Directed acyclic graph (DAG); Lyapunov equation; Numerical computations; Standard blocking; Sylvester equation; Systems and control theory; Lyapunov functions
Pipelined iterative solvers with kernel fusion for graphics processing units,2016,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994114095&doi=10.1145%2f2907944&partnerID=40&md5=0644127b947549d3801be9ce9c0d4e48,"We revisit the implementation of iterative solvers on discrete graphics processing units and demonstrate the benefit of implementations using extensive kernel fusion for pipelined formulations over conventional implementations of classical formulations. The proposed implementations with both CUDA and OpenCL are freely available in ViennaCL and are shown to be competitive with or even superior to other solver packages for graphics processing units. The highest-performance gains are obtained for small to mediumsized systems, while our implementations are on par with vendor-tuned implementations for very large systems. Our results are especially beneficial for transient problems, where many small to medium-sized systems instead of a single big system need to be solved. © 2016 ACM.",BiCGStab method; Conjugate gradientmethod; CUDA; GMRES method; GPU; Iterative solvers; OpenCL,Computer graphics; Gradient methods; Pipeline processing systems; Program processors; Bi-CGSTAB; CUDA; GMRES methods; Iterative solvers; OpenCL; Graphics processing unit
Implementing multifrontal sparse solvers for multicore architectures with sequential task flow runtime systems,2016,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020383616&doi=10.1145%2f2898348&partnerID=40&md5=27a0ac18ff5a92b0a9ea6f576e78906b,"To face the advent of multicore processors and the ever increasing complexity of hardware architectures, programming models based on DAG parallelism regained popularity in the high performance, scientific computing community. Modern runtime systems offer a programming interface that complies with this paradigm and powerful engines for scheduling the tasks into which the application is decomposed. These tools have already proved their effectiveness on a number of dense linear algebra applications. This article evaluates the usability and effectiveness of runtime systems based on the Sequential Task Flow model for complex applications, namely, sparse matrix multifrontal factorizations that feature extremely irregular workloads, with tasks of different granularities and characteristics and with a variablememory consumption. Most importantly, it shows how this parallel programming model eases the development of complex features that benefit the performance of sparse, direct solvers as well as their memory consumption. We illustrate our discussion with the multifrontal QR factorization running on top of the StarPU runtime system. © 2016 ACM.",Communicationavoiding; Memory-aware; Multicores; Runtime systems; Sparse direct solvers,Computer systems programming; Factorization; Linear algebra; MATLAB; Memory architecture; Parallel programming; Software architecture; Communicationavoiding; Memory aware; Multi-cores; Runtime systems; Sparse direct solver; Multicore programming
Algorithm 967: A distributed-memory fast multipole method for volume potentials,2016,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034772440&doi=10.1145%2f2898349&partnerID=40&md5=6b374fe028c9753df09dc8a691631a0b,"The solution of a constant-coefficient elliptic Partial Differential Equation (PDE) can be computed using an integral transform: A convolution with the fundamental solution of the PDE, also known as a volume potential. We present a Fast Multipole Method (FMM) for computing volume potentials and use them to construct spatially adaptive solvers for the Poisson, Stokes, and low-frequency Helmholtz problems. Conventional N-body methods apply to discrete particle interactions. With volume potentials, one replaces the sums with volume integrals. Particle N-body methods can be used to accelerate such integrals. but it is more efficient to develop a special FMM. In this article, we discuss the efficient implementation of such an FMM. We use high-order piecewise Chebyshev polynomials and an octree data structure to represent the input and output fields and enable spectrally accurate approximation of the near-field and the Kernel Independent FMM (KIFMM) for the far-field approximation. For distributed-memory parallelism, we use space-filling curves, locally essential trees, and a hypercube-like communication scheme developed previously in our group. We present new near and far interaction traversals that optimize cache usage. Also, unlike particle N-body codes, we need a 2:1 balanced tree to allow for precomputations. We present a fast scheme for 2:1 balancing. Finally, we use vectorization, including the AVX instruction set on the Intel Sandy Bridge architecture to get better than 50% of peak floating-point performance. We use task parallelism to employ the Xeon Phi on the Stampede platform at the Texas Advanced Computing Center (TACC).We achieve about 600GFLOP/s of double-precision performance on a single node. Our largest run on Stampede took 3.5s on 16K cores for a problem with 18E+9 unknowns for a highly nonuniform particle distribution (corresponding to an effective resolution exceeding 3E+23 unknowns since we used 23 levels in our octree). © 2016 ACM.",FMM; N-body problems; Potential theory,Computation theory; Digital arithmetic; Forestry; Integral equations; Partial differential equations; Polynomials; Distributed-memory parallelisms; Efficient implementation; Elliptic partial differential equation; Far field approximation; N body problem; Particle distributions; Potential theory; Sandy Bridge architectures; Memory architecture
Topology-oriented incremental algorithm for the robust construction of the voronoi diagrams of disks,2016,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046366679&doi=10.1145%2f2939366&partnerID=40&md5=467ccc099fd49deaf11aad1543806b73,"Voronoi diagrams are useful for spatial reasoning, and the robust and efficient construction of the ordinary Voronoi diagram of points is well known. However, its counterpart for circular disks in ℝ2 and spherical balls in ℝ3 remains a challenge. In this article, we propose a topology-oriented incremental algorithm which robustly and efficiently computes a Voronoi diagram by incrementing a new disk generator to an existing one. The key idea is to enforce the convexity of the Voronoi cell corresponding to the incrementing disk so that a simple variation of the algorithm for points proposed by Sugihara in 1992 can be applied. A benchmark using both random and degenerate disks shows that the proposed algorithm is superior to CGAL in both computational efficiency and algorithmic robustness. © 2016 ACM.",Additively-weighted Voronoi diagram; Betacomplex; Disks; Quasi-triangulation; Simplex; Simplicial complex; Topology,Computational geometry; Graphic methods; Topology; Beta complexes; Disks; Quasi triangulations; Simplex; Simplicial complex; Weighted voronoi diagram; Computational efficiency
Analytical modeling is enough for high-performance BLIS,2016,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016043112&doi=10.1145%2f2925987&partnerID=40&md5=f66156ebea00a9d825ef0357cd2a23db,"We show how the BLAS-like Library Instantiation Software (BLIS) framework, which provides a more detailed layering of the GotoBLAS (now maintained as OpenBLAS) implementation, allows one to analytically determine tuning parameters for high-end instantiations of the matrix-matrix multiplication. This is of both practical and scientific importance, as it greatly reduces the development effort required for the implementation of the level-3 BLAS while also advancing our understanding of how hierarchically layered memories interact with high-performance software. This allows the community to move on from valuable engineering solutions (empirically autotuning) to scientific understanding (analytical insight). © 2016 ACM.",Analytical modeling; high performance; Libraries; Linear algebra; Matrix multiplication,Analytical models; Libraries; Linear algebra; Autotuning; Engineering solutions; high performance; Level-3 BLAS; Matrix matrix multiplications; MAtrix multiplication; Tuning parameter; Matrix algebra
WorkStream - A design pattern for multicore-enabled finite element computations,2016,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84986563715&doi=10.1145%2f2851488&partnerID=40&md5=628dbb4d74ee41452503cb06016a655e,"Many operations that need to be performed in modern finite element codes can be described as an operation that needs to be done independently on every cell, followed by a reduction of these local results into a global data structure. For example, matrix assembly, estimating discretization errors, or converting nodal values into data structures that can be output in visualization file formats all fall into this class of operations. Using this realization, we identify a software design pattern that we callWorkStream and that can be used to model such operations and enables the use of multicore shared memory parallel processing. We also describe in detail how this design pattern can be efficiently implemented, and we provide numerical scalability results from its use in the DEAL.II software library. © 2016 ACM.",Assembly; Finite element algorithms; Pipeline software pattern,Assembly; Data structures; Data visualization; Software architecture; Software design; Discretization errors; Finite element algorithms; Finite element codes; Finite element computations; Numerical scalabilities; Shared-memory parallels; Software design patterns; Software patterns; Finite element method
Modular SIMD arithmetic in MATHEMAGIX,2016,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84986575801&doi=10.1145%2f2876503&partnerID=40&md5=7d8d0440ba66ff3877fb07ac697fa021,"Modular integer arithmetic occurs in many algorithms for computer algebra, cryptography, and error correcting codes. Although recent microprocessors typically offer a wide range of highly optimized arithmetic functions, modular integer operations still require dedicated implementations. In this article, we survey existing algorithms for modular integer arithmetic and present detailed vectorized counterparts. We also describe several applications, such as fast modular Fourier transforms and multiplication of integer polynomials and matrices. The vectorized algorithms have been implemented in C++ inside the free computer algebra and analysis system MATHEMAGIX. The performance of our implementation is illustrated by various benchmarks. © is held by the owner/author(s).",Fast Fourier transform; Integer product; Mathemagix; Matrix product; Modular integer arithmetic; Polynomial product,Algebra; Algorithms; Benchmarking; C++ (programming language); Fast Fourier transforms; Integer programming; Number theory; Polynomials; Arithmetic functions; Computer algebra; Error correcting code; Integer arithmetic; Integer operations; Integer product; Mathemagix; Matrix products; Matrix algebra
A note on performance profiles for benchmarking software,2016,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048237496&doi=10.1145%2f2950048&partnerID=40&md5=50e39d39bc5fa74d611e62e968b5891d,"In recent years, performance profiles have become a popular and widely used tool for benchmarking and evaluating the performance of several solvers when run on a large test set. Here we use data from a real application as well as a simple artificial example to illustrate that caution should be exercised when trying to interpret performance profiles to assess the relative performance of the solvers. © 2016 ACM.",Benchmarking; Performance profiles; Testing,Computer software; Software engineering; Testing; Performance profile; Real applications; Relative performance; Test sets; Benchmarking
Algorithm 966: A practical iterative algorithm for the art gallery problem using integer linear programming,2016,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84995664604&doi=10.1145%2f2890491&partnerID=40&md5=dd8621daf37c84f966e3e8cc323c23e8,"In the last few decades, the search for exact algorithms for known NP-hard geometric problems has intensified. Many of these solutions use Integer Linear Programming (ILP) modeling and rely on state-of-the- art solvers to be able to find optimal solutions for large instances in a matter of minutes. In this work, we discuss an ILP-based algorithm that solves to optimality the Art Gallery Problem (AGP), one of the most studied problems in computational geometry. The basic idea of our method is to iteratively generate upper and lower bounds for the problem through the resolution of discretized versions of the AGP, which are reduced to instances of the Set Cover Problem. Our algorithm was implemented and tested on almost 3,000 instances and attained optimal solutions for the vast majority of them, greatly increasing the set of instances forwhich exact solutions are known. To the best of our knowledge, in spite of the extensive study of the AGP in the last four decades, no other algorithm has shown the ability to solve the AGP as effectively and efficiently as the one described here. Evidence of its robustness is presented through tests done on a number of classes of polygons of various sizes with and without holes. A software package implementing the algorithm is made available. © 2016 ACM.",Art gallery problem; Combinatorial optimization; Computational geometry; Exact algorithm,Combinatorial optimization; Computational geometry; Integer programming; Optimal systems; Art gallery problem; Exact algorithms; Geometric problems; Integer Linear Programming; Integer linear programming models; Iterative algorithm; Set cover problem; Upper and lower bounds; Iterative methods
A robust and scalable implementation of the Parks-McClellan algorithm for designing FIR filters,2016,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046023461&doi=10.1145%2f2904902&partnerID=40&md5=cb1f71e64faa4fb028d12dd696940acf,"With a long history dating back to the beginning of the 1970s, the Parks-McClellan algorithm is probably the most well known approach for designing finite impulse response filters. Despite being a standard routine in many signal processing packages, it is possible to find practical design specifications where existing codes fail to work. Our goal is twofold. We first examine and present solutions for the practical difficulties related to weighted minimax polynomial approximation problems on multi-interval domains (i.e., the general setting under which the Parks-McClellan algorithm operates). Using these ideas, we then describe a robust implementation of this algorithm. It routinely outperforms existing minimax filter design routines. © 2016 ACM 0098-3500/2016/08-ART7 $15.00",Barycentric interpolation; Chebyshev approximation; Colleague matrix; FIR filters; Lebesgue constant; Minimax approximation,Approximation algorithms; Chebyshev approximation; Chebyshev filters; Impulse response; Polynomial approximation; Signal processing; Barycentric interpolation; Design specification; Interval domain; Lebesgue constants; Minimax approximation; Parks-McClellan algorithm; Scalable implementation; Weighted minimax; FIR filters
Manycore algorithms for batch scalar and block tridiagonal solvers,2016,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978998199&doi=10.1145%2f2830568&partnerID=40&md5=12ac7a737871af71aba20400aa91bdb5,"Engineering, scientific, and financial applications often require the simultaneous solution of a large number of independent tridiagonal systems of equations with varying coefficients. Since the number of systems is large enough to offer considerable parallelism on manycore systems, the choice between different tridiagonal solution algorithms, such as Thomas, Cyclic Reduction (CR) or Parallel Cyclic Reduction (PCR) needs to be reexamined. This work investigates the optimal choice of tridiagonal algorithm for CPU, Intel MIC, and NVIDIA GPU with a focus on minimizing the amount of data transfer to and from the main memory using novel algorithms and the register-blocking mechanism, and maximizing the achieved bandwidth. It also considers block tridiagonal solutions, which are sometimes required in Computational Fluid Dynamic (CFD) applications. A novel work-sharing and register blocking-based Thomas solver is also presented. © 2016 ACM.",Block tridiagonal solver; CPU; CUDA; GPU; MIC; Scalar tridiagonal solver; Vectorization; Xeon Phi,Assembly; Computational fluid dynamics; Data transfer; Fluid dynamics; Microwave integrated circuits; Program processors; Blocking mechanisms; CUDA; Financial applications; Simultaneous solution; Tri-diagonal solver; Varying coefficients; Vectorization; Xeon Phi; Algorithms
Discrete wavelet transforms in the large time-frequency analysis toolbox for MATLAB/GNU octave,2016,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84975288438&doi=10.1145%2f2839298&partnerID=40&md5=b88b1ac8c72d6aa0283b254f1a053811,"The discrete wavelet transform module is a recent addition to the Large Time-Frequency Analysis Toolbox (LTFAT). It provides implementations of various generalizations of Mallat's well-known algorithm (iterated filterbank) such that completely general filterbank trees, dual-tree complex wavelet transforms, and wavelet packets can be computed. The resulting transforms can be equivalently represented as filterbanks and analyzed as filterbank frames using fast algorithms. © 2016 ACM.",Discrete wavelet packets; Discrete wavelet transform; Dual-tree complex wavelet transform; Filterbanks; Finite frames,Filter banks; Image segmentation; Partial discharges; Signal reconstruction; Trees (mathematics); Wavelet analysis; Discrete wavelets; Dual-tree complex wavelet transform; Fast algorithms; Finite frames; Time frequency analysis; Wavelet Packet; Discrete wavelet transforms
Algorithm 964: An efficient algorithm to compute the genus of discrete surfaces and applications to turbulent flows,2016,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84975217877&doi=10.1145%2f2845076&partnerID=40&md5=6dd01b4a65fb3201c3bcf67d20309fb8,"A simple and efficient algorithm to numerically compute the genus of surfaces of three-dimensional objects using the Euler characteristic formula is presented. The algorithm applies to objects obtained by thresholding a scalar field in a structured-collocated grid and does not require any triangulation of the data. This makes the algorithm fast, memory efficient, and suitable for large datasets. Applications to the characterization of complex surfaces in turbulent flows are presented to illustrate the method. © 2016 ACM.",Coherent structures; Euler characteristic; Genus; Turbulence; Turbulent/nonturbulent interface; Voxels,Computer software; Software engineering; Turbulence; Coherent structure; Collocated grids; Discrete surfaces; Euler characteristic; Genus; Simple and efficient algorithms; Three-dimensional object; Voxels; Turbulent flow
Matrix multiplication over word-size modular rings using approximate formulas,2016,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976371898&doi=10.1145%2f2829947&partnerID=40&md5=9d71b1e3b2f1b7dfe4984a1aa5c1251e,"Bini-Capovani-Lotti-Romani approximate formula (or border rank) for matrix multiplication achieves a better complexity than Strassen's matrix multiplication formula. In this article, we show a novel way to use the approximate formula in the special case where the ring is Z/pZ. In addition, we show an implementation à la FFLAS-FFPACK, where p is a word-size modulo, that improves on state-of-the-art Z/pZ matrix multiplication implementations. © 2016 ACM.",Bini-Capovani-Lotti-Romani approximate bilinear algorithm; Efficient implementations; Exact linear algebra; Matrix multiplication; Memory placement and scheduling; Strassen-Winograd's algorithm; Symbolic-numeric computing,Approximation algorithms; Bilinear algorithms; Efficient implementation; MAtrix multiplication; Symbolic numerics; Winograd's algorithms; Matrix algebra
"An experimental exploration of Marsaglia's xorshift Generators, Scrambled",2016,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84975316905&doi=10.1145%2f2845077&partnerID=40&md5=e9b8499e3f77e90568ea0aebd2227ea1,"Marsaglia proposed xorshift generators are a class of very fast, good-quality pseudorandom number generators. Subsequent analysis by Panneton and L'Ecuyer has lowered the expectations raised by Marsaglia's article, showing several weaknesses of such generators. Nonetheless, many of the weaknesses of xorshift generators fade away if their result is scrambled by a nonlinear operation (as originally suggested by Marsaglia). In this article we explore the space of possible generators obtained by multiplying the result of a xorshift generator by a suitable constant. We sample generators at 100 points of their state space and obtain detailed statistics that lead us to choices of parameters that improve on the current ones. We then explore for the first time the space of high-dimensional xorshift generators, following another suggestion in Marsaglia's article, finding choices of parameters providing periods of length 21024 - 1 and 24096 - 1. The resulting generators are of extremely high quality, faster than current similar alternatives, and generate long-period sequences passing strong statistical tests using only eight logical operations, one addition, and one multiplication by a constant. © 2016 ACM.",Pseudorandom number generators,Number theory; High quality; High-dimensional; Logical operations; Nonlinear operation; Pseudo random number generators; Random number generation
A distributed-memory package for dense Hierarchically Semi-Separable matrix computations using randomization,2016,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978151162&doi=10.1145%2f2930660&partnerID=40&md5=0db16a8a1c9e81ba9987229366282f9a,"We present a distributed-memory library for computations with dense structured matrices. A matrix is considered structured if its off-diagonal blocks can be approximated by a rank-deficient matrix with low numerical rank. Here, we use Hierarchically Semi-Separable (HSS) representations. Such matrices appear in many applications, for example, finite-element methods, boundary element methods, and so on. Exploiting this structure allows for fast solution of linear systems and/or fast computation of matrix-vector products, which are the two main building blocks of matrix computations. The compression algorithm that we use, that computes the HSS form of an input dense matrix, relies on randomized sampling with a novel adaptive sampling mechanism. We discuss the parallelization of this algorithm and also present the parallelization of structured matrix-vector product, structured factorization, and solution routines. The efficiency of the approach is demonstrated on large problems from different academic and industrial applications, on up to 8,000 cores. This work is part of a more global effort, the STRUctured Matrices PACKage (STRUMPACK) software package for computations with sparse and dense structured matrices. Hence, although useful on their own right, the routines also represent a step in the direction of a distributed-memory sparse solver. © 2016 ACM.",Algorithms; Design; Performance,Algorithms; Boundary element method; Design; Finite element method; Linear systems; Memory architecture; Compression algorithms; Distributed Memory; Matrix computation; Matrix-vector products; Performance; Randomized sampling; Separable matrices; Structured matrixes; Matrix algebra
Algorithm 963: Estimation of stochastic covariance models using a continuum of moment conditions,2016,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978085690&doi=10.1145%2f2834115&partnerID=40&md5=ae035a584078f4f02b08f2ba3e535268,"We describe the implementation of a parameter estimation method suitable for models commonly used in quantitative finance. The Continuum-Generalized Method of Moments (CGMM) is a Generalized Method of Moments (GMM) type of methodology that applies a continuum of moment conditions to achieve the efficiency of aMaximum Likelihood method. Instead of the transition density, the more commonly available conditional characteristic function is used for estimation. We test the CGMM and a simpler version, called the CMM, on simulated time series to check the recovery of the parameters. We also applied CMM to two stochastic covariance models, the Wishart Affine Stochastic Correlation (WASC) model and the Principal Components Stochastic Volatility (PCSV) model. This illustrates the power of CGMM, as stochastic covariance models are generally hard to estimate. The estimation method is fully implemented in MATLAB. © 2016 ACM.",Characteristic function; Continuous time; Continuum of moment conditions; Estimation; Principal component process; Stochastic covariance; Wishart process,Algorithms; Continuous time systems; Estimation; Method of moments; Principal component analysis; Stochastic systems; Characteristic functions; Continuous-time; Continuum of moment conditions; Principal Components; Stochastic covariance; Wishart; Stochastic models
The BLIS framework: Experiments in portability,2016,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84975482275&doi=10.1145%2f2755561&partnerID=40&md5=46c1ecbd6efc23a35424cd538b2a49d3,"BLIS is a new software framework for instantiating high-performance BLAS-like dense linear algebra libraries. We demonstrate how BLIS acts as a productivity multiplier by using it to implement the level-3 BLAS on a variety of current architectures. The systems for which we demonstrate the framework include state-of-the-art general-purpose, low-power, and many-core architectures. We show, with very little effort, how the BLIS framework yields sequential and parallel implementations that are competitive with the performance of ATLAS, OpenBLAS (an effort to maintain and extend the GotoBLAS), and commercial vendor implementations such as AMD's ACML, IBM's ESSL, and Intel's MKL libraries. Although most of this article focuses on single-core implementation, we also provide compelling results that suggest the framework's leverage extends to the multithreaded domain. © 2016 ACM.",BLAS; High performance; Libraries; Linear algebra; Matrix; Multiplication,Algebra; Computer programming; Libraries; Linear algebra; Matrix algebra; BLAS; Dense linear algebra; High performance; Many-core architecture; Multiplication; Parallel implementations; Software frameworks; State of the art; Computer architecture
MATSLISE 2.0: A matlab toolbox for sturm-liouville computations,2016,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84975299085&doi=10.1145%2f2839299&partnerID=40&md5=1419d4d358dc27b90c57632d9bc4e360,"The MATSLISE 2.0 software package is a thorough revision of the successful Matlab package MATSLISE of 2005. The package can be used to compute the eigenvalues and eigenfunctions of regular and some important classes of singular self-adjoint Sturm-Liouville boundary value problems. The code uses new or improved algorithms, offers some new features, and has an updated graphical user interface. © 2016 ACM.",Eigenvalues; Schrödinger equations; Shooting; Sturm-Liouville problems,Boundary value problems; Explosive well stimulation; Graphical user interfaces; MATLAB; User interfaces; Adjoints; Dinger equation; Eigenvalues; Matlab toolboxes; Sturm-Liouville; Sturm-liouville boundary value problems; Sturm-Liouville problem; Eigenvalues and eigenfunctions
A competitive divide-and-conquer algorithm for unconstrained large-scale black-box optimization,2016,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84975215482&doi=10.1145%2f2791291&partnerID=40&md5=ce3853f85fe052d4b2c8d4ed1e44a938,"This article proposes a competitive divide-and-conquer algorithm for solving large-scale black-box optimization problems for which there are thousands of decision variables and the algebraic models of the problems are unavailable. We focus on problems that are partially additively separable, since this type of problem can be further decomposed into a number of smaller independent subproblems. The proposed algorithm addresses two important issues in solving large-scale black-box optimization: (1) the identification of the independent subproblems without explicitly knowing the formula of the objective function and (2) the optimization of the identified black-box subproblems. First, a Global Differential Grouping (GDG) method is proposed to identify the independent subproblems. Then, a variant of the Covariance Matrix Adaptation Evolution Strategy (CMA-ES) is adopted to solve the subproblems resulting from its rotation invariance property. GDG and CMA-ES work together under the cooperative co-evolution framework. The resultant algorithm, named CC-GDG-CMAES, is then evaluated on the CEC'2010 large-scale global optimization (LSGO) benchmark functions, which have a thousand decision variables and black-box objective functions. The experimental results show that, on most test functions evaluated in this study, GDG manages to obtain an ideal partition of the index set of the decision variables, and CC-GDG-CMAES outperforms the state-of-the-art results. Moreover, the competitive performance of the well-known CMA-ES is extended from low-dimensional to high-dimensional black-box problems. © 2016 ACM.",Cooperative coevolution; Covariance matrix adaptation evolutionary strategy (CMA-ES); Decomposition; Differential grouping; Large-scale black-box optimization,Decision making; Decomposition; Evolutionary algorithms; Global optimization; Black-box optimization; Competitive performance; Cooperative co-evolution; Covariance matrix adaptation evolution strategies; Differential grouping; Divide-and-conquer algorithm; Evolutionary strategies; Large scale global optimizations; Covariance matrix
"Remark on ""Algorithm 916: Computing the Faddeyeva and Voigt functions"": Efficiency improvements and fortran translation",2016,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84975165538&doi=10.1145%2f2806884&partnerID=40&md5=74047455620ca552eeaa81fd3499f115,"This remark describes efficiency improvements to Algorithm 916 [Zaghloul and Ali 2011]. It is shown that the execution time required by the algorithm, when run at its highest accuracy, may be improved by more than a factor of 2. A better accuracy vs efficiency tradeoff scheme is also implemented; this requires the user to supply the number of significant figures desired in the computed values as an extra input argument to the function. Using this tradeoff, it is shown that the efficiency of the algorithm may be further improved significantly while maintaining reasonably accurate and safe results that are free of the pitfalls and complete loss of accuracy seen in other competitive techniques. The current version of the code is provided in Matlab and Scilab in addition to a Fortran translation prepared to meet the needs of real-world problems where very large numbers of function evaluations would require the use of a compiled language. To fulfill this last requirement, a recently proposed reformed version of Humlíček's w4 routine, shown to maintain the claimed accuracy of the algorithm over a wide and fine grid, is implemented in the present Fortran translation for the case of four significant figures. This latter modification assures the reliability of the code in the solution of practical problems requiring numerous evaluation of the function for applications requiring low-accuracy computations (<10-4). © 2016 ACM.",Accuracy; Faddeyeva function; Fortran; Function evaluation; Matlab; Scilab,Efficiency; FORTRAN (programming language); MATLAB; Accuracy; Compiled languages; Efficiency improvement; Faddeyeva functions; Loss of accuracy; Practical problems; Real-world problem; Scilab; Function evaluation
Algorithm 959: VBF: A library of C++ classes for vector boolean functions in cryptography,2016,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84973598844&doi=10.1145%2f2794077&partnerID=40&md5=68fab60d4751421c75abd9e42e33358d,"VBF is a collection of C++ classes designed for analyzing vector Boolean functions (functions that map a Boolean vector to another Boolean vector) from a cryptographic perspective. This implementation uses the NTL library from Victor Shoup, adding new modules that call NTL functions and complement the existing ones, making it better suited to cryptography. The class representing a vector Boolean function can be initialized by several alternative types of data structures such as Truth Table, Trace Representation, and Algebraic Normal Form (ANF), among others. The most relevant cryptographic criteria for both block and stream ciphers as well as for hash functions can be evaluated with VBF: it obtains the nonlinearity, linearity distance, algebraic degree, linear structures, and frequency distribution of the absolute values of the Walsh Spectrum or the Autocorrelation Spectrum, among others. In addition, operations such as equality testing, composition, inversion, sum, direct sum, bricklayering (parallel application of vector Boolean functions as employed in Rijndael cipher), and adding coordinate functions of two vector Boolean functions are presented. Finally, three real applications of the library are described: the first one analyzes the KASUMI block cipher, the second one analyzes the Mini-AES cipher, and the third one finds Boolean functions with very high nonlinearity, a key property for robustness against linear attacks. © 2016 ACM 0098-3500/2016/05-ART16 $15.00.",Boolean functions; cryptography; nontrigonometric Fourier analysis; software,Algebra; Algorithms; Boolean functions; C++ (programming language); Computer software; Fourier analysis; Hash functions; Vectors; Algebraic normal forms; Autocorrelation spectrum; Coordinate functions; Cryptographic criterion; Frequency distributions; KASUMI block cipher; Parallel application; Trace representations; Cryptography
A source transformation via operator overloading method for the automatic differentiation of mathematical functions in MATLAB,2016,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84973662745&doi=10.1145%2f2699456&partnerID=40&md5=61d256b6541f886575b3b4de43104d8d,"A source transformation via operator overloading method is presented for computing derivatives of mathematical functions defined by MATLAB computer programs. The transformed derivative code that results from the method of this article computes a sparse representation of the derivative of the function defined in the original code. As in all source transformation automatic differentiation techniques, an important feature of the method is that any flow control in the original function code is preserved in the derivative code. Furthermore, the resulting derivative code relies solely upon the native MATLAB library. The method is useful in applications where it is required to repeatedly evaluate the derivative of the original function. The approach is demonstrated on several examples and is found to be highly efficient when compared to well-known MATLAB automatic differentiation programs.",applied mathematics; Scientific computation,Codes (symbols); Mathematical operators; Mathematical transformations; MATLAB; Applied mathematics; Automatic differentiations; Important features; Mathematical functions; Operator overloading; Scientific computation; Source transformation; Sparse representation; Functions
What are the correct results for the special values of the operands of the power operation?,2016,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84973614825&doi=10.1145%2f2809783&partnerID=40&md5=de8a59edbb1576f1482b2f66f6f87fe5,"Language standards such as C99 and C11, as well as the IEEE Standard for Floating-Point Arithmetic 754 (IEEE Std 754-2008) specify the expected behavior of binary and decimal floating-point arithmetic in computer-programming environments and the handling of special values and exception conditions. Many researchers focus on verifying the compliance of implementations for binary and decimal floating-point operations with these standards. In this article, we are concerned with the special values of the operands of the power function Z = XY .We study how the standards define the correct results for this operation, propose a mathematically justified definition for the correct results of the power function on the occurrence of these special values as its operands, test how different software implementations for the power function deal with these special values, and classify the behavior of different programming languages from the viewpoint of how much they conform to the standards and our proposed mathematical definition. We present inconsistencies between the implementations and the standards, and discuss incompatibilities between different versions of the same software. © 2016 ACM 0098-3500/2016/05-ART14 $15.00.",Floating-point arithmetic; incompatibility; inconsistency; indeterminate; l'Ĥopital's rule; limits; NaN,Behavioral research; Bins; Computational linguistics; Computer programming; Functions; Regulatory compliance; Software testing; Standards; Decimal floating points; incompatibility; inconsistency; indeterminate; limits; Mathematical definitions; Programming environment; Software implementation; Digital arithmetic
Algorithm 958: Lattice builder: A general software tool for constructing rank-1 lattice rules,2016,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84973643702&doi=10.1145%2f2754929&partnerID=40&md5=16cceded245e7864cc30085013651954,"We introduce a new software tool and library named Lattice Builder, written in C++, that implements a variety of construction algorithms for good rank-1 lattice rules. It supports exhaustive and random searches, as well as component-by-component (CBC) and random CBC constructions, for any number of points, and for various measures of (non)uniformity of the points. The measures currently implemented are all shiftinvariant and represent the worst-case integration error for certain classes of integrands. They include, for example, the weighted Pα square discrepancy, the Rα criterion, and figures of merit based on the spectral test, with projection-dependent weights. Each of these measures can be computed as a finite sum. For the Pα and Rα criteria, efficient specializations of the CBC algorithm are provided for projection-dependent, order-dependent, and product weights. For numbers of points that are integer powers of a prime base, the construction of embedded rank-1 lattice rules is supported through any of these algorithms, and through a fast CBC algorithm, with a variety of possibilities for the normalization of the merit values of individual embedded levels and for their combination into a single merit value. The library is extensible, thanks to the decomposition of the algorithms into decoupled components, which makes it easy to implement new types of weights, new search domains, new figures of merit, and so on. © 2016 ACM 0098-3500/2016/05-ART15 $15.00.",CBC construction; figures of merit; Lattice rules; multidimensional integration; quasi-Monte Carlo,C++ (programming language); Computer software; Construction algorithms; Figures of merits; Integration error; Lattice rules; Multidimensional integration; Quasi-Monte Carlo; Random searches; Shift invariant; Algorithms
Algorithm 962: BACOLI: B-spline adaptive collocation software for PDEs with interpolation-based spatial error control,2016,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84975138779&doi=10.1145%2f2818312&partnerID=40&md5=863679dd27ab32a856f8d0e3f85bc035,"BACOL and BACOLR are (Fortran 77) B-spline adaptive collocation packages for the numerical solution of 1D parabolic Partial Differential Equations (PDEs). The packages have been shown to be superior to other similar packages, especially for problems exhibiting sharp, moving spatial layer regions, where a stringent tolerance is imposed. In addition to providing temporal error control through the timestepping software, BACOL and BACOLR feature control of a high-order estimate of the spatial error of the approximate solution, obtained by computing a second approximate solution of one higher order of accuracy; the cost is substantial - execution time and memory usage are almost doubled. In this article, we discuss BACOLI, a new version of BACOL that computes only one approximate solution and uses efficient interpolation-based schemes to obtain a spatial error estimate. In previous studies these schemes have been shown to provide spatial error estimates of comparable quality to those of BACOL. We describe the substantial modification of BACOL needed to obtain BACOLI, and provide numerical results showing that BACOLI is significantly more efficient than BACOL, in some cases by as much as a factor of 2. We also introduce a Fortran 95 wrapper for BACOLI (called BACOLI95) and discuss its simplified user interface. © 2016 ACM.",1D parabolic PDEs; Algorithms; Collocation; Efficiency; G.1.8 [partial differential equations]: method of lines; Interpolation; Numerical software; Performance; Spatial error estimation,Algorithms; Efficiency; Errors; FORTRAN (programming language); Numerical methods; Partial differential equations; User interfaces; 1-D parabolic PDEs; Collocation; Method of lines; Numerical software; Performance; Spatial error estimation; Interpolation
Algorithm 961: Fortran 77 subroutines for the solution of skew-Hamiltonian/Hamiltonian eigenproblems,2016,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84975140575&doi=10.1145%2f2818313&partnerID=40&md5=07371dde144bc8c7e650628d7fd948b0,"Skew-Hamiltonian/Hamiltonian matrix pencils λS - H appear in many applications, including linear-quadratic optimal control problems, H∞-optimization, certain multibody systems, and many other areas in applied mathematics, physics, and chemistry. In these applications it is necessary to compute certain eigenvalues and/or corresponding deflating subspaces of these matrix pencils. Recently developed methods exploit and preserve the skew-Hamiltonian/Hamiltonian structure and hence increase the reliability, accuracy, and performance of the computations. In this article, we describe the corresponding algorithms which have been implemented in the style of subroutines of the Subroutine Library in Control Theory (SLICOT). Furthermore, we address some of their applications. We describe variants for real and complex problems, as well as implementation details and perform numerical tests using real-world examples to demonstrate the superiority of the new algorithms compared to standard methods. © 2016 ACM.",Algorithms; D.3.2 [programming languages]: language classifications - Fortran 77; Deflating subspaces; Documentation; Eigenvalue reordering; G.1.3 [numerical analysis]: numerical linear algebra - eigenvalues and eigenvectors (direct and iterative methods); Generalized eigenvalues; Generalized schur form; Reliability; Skew-Hamiltonian/Hamiltonian matrix pencil; Software; Structure-preservation,Algorithms; Computational linguistics; Computer software; Eigenvalues and eigenfunctions; FORTRAN (programming language); Iterative methods; Linear algebra; Matrix algebra; Numerical methods; Optimal control systems; Optimization; Quadratic programming; Reliability; Reliability analysis; Software reliability; Subroutines; System program documentation; Deflating subspace; Eigen-value; Fortran 77; Generalized eigenvalues; Matrix pencil; Numerical Linear Algebra; Schur form; Structure preservation; Hamiltonians
Algorithm 960: Polynomial: An object-oriented matlab library of fast and efficient algorithms for polynomials,2016,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84969983799&doi=10.1145%2f2814567&partnerID=40&md5=7936f51c7e6847683cf8027bd985f191,"The design and implementation of a Matlab object-oriented software library for working with polynomials is presented. The construction and evaluation of polynomials in Bernstein form are motivated and justified. Efficient constructions for the coefficients of a polynomial in Bernstein form when the polynomial is not given with this representation are provided. The presented adaptive evaluation algorithm uses the VS (Volk and Schumaker) algorithm, the de Casteljau algorithm, and a compensated VS algorithm. In addition, we have completed the library with other algorithms to perform other usual operations with polynomials in Bernstein form. © 2016 ACM.",Bernstein basis; Numerical stability; Polynomial algorithms,Convergence of numerical methods; MATLAB; Object oriented programming; Polynomials; Bernstein basis; De Casteljau algorithms; Design and implementations; Efficient construction; Fast and efficient algorithms; Object oriented software; Object-oriented matlabs; Polynomial algorithm; Algorithms
PUMI: Parallel Unstructured Mesh Infrastructure,2016,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84971351123&doi=10.1145%2f2814935&partnerID=40&md5=0251becc30434b5f3f278f19ecc8b6ce,"The Parallel Unstructured Mesh Infrastructure (PUMI) is designed to support the representation of, and operations on, unstructured meshes as needed for the execution of mesh-based simulations on massively parallel computers. In PUMI, the mesh representation is complete in the sense of being able to provide any adjacency of mesh entities of multiple topologies in O(1) time, and fully distributed to support relationships of mesh entities across multiple memory spaces in a manner consistent with supporting massively parallel simulation workflows. PUMI's mesh maintains links to the high-level model definition in terms of a model topology as produced by CAD systems, and is specifically designed to efficiently support evolving meshes as required for mesh generation and adaptation. To support the needs of parallel unstructured mesh simulations, PUMI also supports a specific set of services such as the migration of mesh entities between parts while maintaining the mesh adjacencies, maintaining read-only mesh entity copies from neighboring parts (ghosting), repartitioning parts as the mesh evolves, and dynamic mesh load balancing. Here we present the overall design, software structures, example programs, and performance results. The effectiveness of PUMI is demonstrated by its applications to massively parallel adaptive simulation workflows.c? 2016 ACM 0098-3500/2016/05-ART17 $15.00.",Additional and phrases: Unstructured mesh; Hybrid mpi/thread; Massively parallel; Partial differential equation simulation,Computer aided design; Network management; Systems analysis; Topology; Adaptive simulation; Hybrid mpi/thread; Massively parallel computers; Massively parallels; Mesh representation; Multiple topologies; Software structures; Unstructured meshes; Mesh generation
An efficient hybrid algorithm for the separable convex quadratic knapsack problem,2016,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84970016497&doi=10.1145%2f2828635&partnerID=40&md5=a2c187ba0105df563c1359531da02f7a,"This article considers the problem of minimizing a convex, separable quadratic function subject to a knapsack constraint and a box constraint. An algorithm called NAPHEAP has been developed to solve this problem. The algorithm solves the Karush-Kuhn-Tucker system using a starting guess to the optimal Lagrange multiplier and updating the guess monotonically in the direction of the solution. The starting guess is computed using the variable fixing method or is supplied by the user. A key innovation in our algorithm is the implementation of a heap data structure for storing the break points of the dual function and computing the solution of the dual problem. Also, a new version of the variable fixing algorithm is developed that is convergent even when the objective Hessian is not strictly positive definite. The hybrid algorithm NAPHEAP that uses a Newton-type method (variable fixing method, secant method, or Newton's method) to bracket a root, followed by a heap-based monotone break point search, can be faster than a Newton-type method by itself, as demonstrated in the numerical experiments. © 2016 ACM.",Continuous quadratic knapsack; Convex programming; Heap; Nonlinear programming; Quadratic programming; Separable programming,Algorithms; Combinatorial optimization; Convex optimization; Lagrange multipliers; Newton-Raphson method; Nonlinear programming; Numerical methods; Quadratic programming; Heap; Heap data structures; Karush-Kuhn-Tucker systems; Knapsack constraints; Newton-type methods; Numerical experiments; Quadratic knapsack problems; Quadratic knapsacks; Problem solving
A parallel geometric multifrontal solver using hierarchically semiseparable structure,2016,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84970046182&doi=10.1145%2f2830569&partnerID=40&md5=07e7f40f6c62e3bfa34c4c14332d2da1,"We present a structured parallel geometry-based multifrontal sparse solver using hierarchically semisepa-rable (HSS) representations and exploiting the inherent low-rank structures. Parallel strategies for nested dissection ordering (taking low rankness into account), symbolic factorization, and structured numerical factorization are shown. In particular, we demonstrate how to manage two layers of tree parallelism to integrate parallel HSS operations within the parallel multifrontal sparse factorization. Such a structured multifrontal factorization algorithm can be shown to have asymptotically lower complexities in both operation counts and memory than the conventional factorization algorithms for certain partial differential equations. We present numerical results from the solution of the anisotropic Helmholtz equations for seismic imaging, and demonstrate that our new solver was able to solve 3D problems up to 6003 mesh size, with 216M degrees of freedom in the linear system. For this specific model problem, our solver is both faster and more memory efficient than a geometry-based multifrontal solver (which is further faster than general-purpose algebraic solvers such as MUMPS and SuperLU-DIST). For the 6003 mesh size, the structured factors from our solver need about 5.9 times less memory. © 2016 ACM.",HSS matrices; Multifrontal method; Parallel algorithm; Sparse Gaussian elimination,Degrees of freedom (mechanics); Geometry; Linear systems; Mesh generation; Parallel algorithms; Algebraic solver; Factorization algorithms; Gaussian elimination; Multifrontal methods; Nested dissection; Numerical results; Parallel strategies; Symbolic factorization; Factorization
A radix-independent error analysis of the cornea-harrison-tang method,2016,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84969940657&doi=10.1145%2f2824252&partnerID=40&md5=10168df7a1fd132cf4411a5aa3597e96,"Assuming floating-point arithmetic with a fused multiply-add operation and rounding to nearest, the Cornea-Harrison-Tang method aims to evaluate expressions of the form ab + cd with high relative accuracy. In this article, we provide a rounding error analysis of this method, which unlike previous studies is not restricted to binary floating-point arithmetic but holds for any radix β. We show first that an asymptotically optimal bound on the relative error of this method is 2βu+2u2/β+2u2 = 2u + 2/β u2 + O(u3), where u = 1/2β1-p is the unit roundoff in radix β and precision p. Then we show that the possibility of removing the O(u2) term from this bound is governed by the radix parity and the tie-breaking strategy used for rounding: if β is odd or rounding is to nearest even, then the simpler bound 2u is obtained, while if β is even and rounding is to nearest away, then there exist floating-point inputs a, b,c,d that lead to a relative error larger than 2u + 2/β u2 - 4u3. All these results hold provided underflows and overflows do not occur and under some mild assumptions on p satisfied by IEEE 754-2008 formats. © 2016 ACM.",Floating-point arithmetic; Fused multiply-add operation; High relative accuracy; Rounding error; Rounding to nearest; Tie-breaking strategy; Unit in the first place; Unit roundoff,Error analysis; Errors; Fused multiply-add; Relative accuracy; Rounding errors; Rounding to nearest; Tie-breaking; Unit in the first place; Unit roundoff; Digital arithmetic
KBLAS: An optimized library for dense matrix-vector multiplication on GPU accelerators,2016,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84969916633&doi=10.1145%2f2818311&partnerID=40&md5=c416cbc65566a16ace60392a0bc4d017,"KBLAS is an open-source, high-performance library that provides optimized kernels for a subset of Level 2 BLAS functionalities on CUDA-enabled GPUs. Since performance of dense matrix-vector multiplication is hindered by the overhead of memory accesses, a double-buffering optimization technique is employed to overlap data motion with computation. After identifying a proper set of tuning parameters, KBLAS efficiently runs on various GPU architectures while avoiding code rewriting and retaining compliance with the standard BLAS API. Another optimization technique allows ensuring coalesced memory access when dealing with submatrices, especially for high-level dense linear algebra algorithms. All KBLAS kernels have been leveraged to a multi-GPU environment, which requires the introduction of new APIs. Considering general matrices, KBLAS is very competitive with existing state-of-the-art kernels and provides a smoother performance across a wide range of matrix dimensions. Considering symmetric and Hermitian matrices, the KBLAS performance outperforms existing state-of-the-art implementations on all matrix sizes and achieves asymptotically up to 50% and 60% speedup against the best competitor on single GPU and multi-GPUs systems, respectively. Performance results also validate our performance model. A subset of KBLAS highperformance kernels have been integrated into NVIDIA's standard BLAS implementation (cuBLAS) for larger dissemination, starting from version 6.0. © 2016 ACM.",Basic linear algebra subroutines; CUDA optimizations; GPU accelerators; Memory-bound kernels,Graphics processing unit; Memory architecture; Program processors; Regulatory compliance; Basic linear algebra subroutines; Coalesced memory access; Dense linear algebra; GPU accelerators; Hermitian matrices; High-performance libraries; Memory bounds; Optimization techniques; Matrix algebra
"Algorithm 956: PAMPAC, a parallel adaptive method for pseudo-arclength continuation",2016,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964328663&doi=10.1145%2f2714570&partnerID=40&md5=c374cbd8628bdfcd714bb1777fa1b8b9,"Pseudo-arclength continuation is a well-established method for generating a numerical curve approximating the solution of an underdetermined system of nonlinear equations. It is an inherently sequential predictorcorrector method in which new approximate solutions are extrapolated from previously converged results and then iteratively refined. Convergence of the iterative corrections is guaranteed only for sufficiently small prediction steps. In high-dimensional systems, corrector steps are extremely costly to compute and the prediction step length must be adapted carefully to avoid failed steps or unnecessarily slow progress. We describe a parallel method for adapting the step length employing several predictor-corrector sequences of different step lengths computed concurrently. In addition, the algorithm permits intermediate results of correction sequences that have not converged to seed new predictions. This strategy results in an aggressive optimization of the step length at the cost of redundancy in the concurrent computation. We present two examples of convoluted solution curves of high-dimensional systems showing that speed-up by a factor of two can be attained on a multicore CPU while a factor of three is attainable on a small cluster. 2016 Copyright is held by the owner/author(s).",Adaptivity; Parallel computing; Pseudo-arclength continuation,Algorithms; Nonlinear equations; Numerical methods; Optimization; Parallel processing systems; Adaptivity; Arc length; Concurrent computation; High-dimensional systems; Intermediate results; Iterative corrections; Predictor-corrector methods; Underdetermined system of nonlinear equations; Iterative methods
Parameterized complexity of discrete morse theory,2016,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84968665573&doi=10.1145%2f2738034&partnerID=40&md5=0e25cdcabf9eccfeba6ceb0c9f0797a9,"Optimal Morse matchings reveal essential structures of cell complexes that lead to powerful tools to study discrete geometrical objects, in particular, discrete 3-manifolds. However, such matchings are known to be NP-hard to compute on 3-manifolds through a reduction to the erasability problem. Here, we refine the study of the complexity of problems related to discrete Morse theory in terms of parameterized complexity. On the one hand, we prove that the erasability problem is W[P]-complete on the natural parameter. On the other hand, we propose an algorithm for computing optimal Morse matchings on triangulations of 3-manifolds, which is fixed-parameter tractable in the treewidth of the bipartite graph representing the adjacency of the 1- And 2-simplices. This algorithm also shows fixed-parameter tractability for problems such as erasability and maximum alternating cycle-free matching. We further show that these results are also true when the treewidth of the dual graph of the triangulated 3-manifold is bounded. Finally, we discuss the topological significance of the chosen parameters and investigate the respective treewidths of simplicial and generalized triangulations of 3-manifolds. © 2016 ACM 0098-3500/2016/02-ART6 $15.00.",Alternating cycle-free Matching; Collapsibility; Computational Topology; Discrete Morse Theory; Erasability; Fixed-Parameter Tractability; Parameterized Complexity; Treewidth; W[P]- Completeness,Formal logic; Optimization; Parameterization; Polynomials; Surveying; Topology; Triangulation; Alternating cycle; Collapsibility; Computational topology; Discrete Morse theory; Erasability; Fixed-parameter tractability; Parameterized complexity; Tree-width; Parameter estimation
Algorithm 957: Evaluation of the repeated integral of the coerror function by half-range gauss-hermite quadrature,2016,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964321037&doi=10.1145%2f2735626&partnerID=40&md5=6c088a845e9142f1be6aa8f0dfee5ee1,"Nonstandard Gaussian quadrature is applied to evaluate the repeated integral inerfc x of the coerror function for n ∈ ℕ0, x ∈ ℝ in an appropriate domain of the (n, x)-plane. Relevant software in MATLAB is provided: in particular, two routines evaluating the function to an accuracy of 12 respective 30-decimal digits. © 2016 ACM.",Half-range Gauss-Hermite quadrature; MATLAB software; Repeated integral of the coerror function,Algorithms; Function evaluation; Gaussian distribution; MATLAB; Gauss-Hermite quadrature; Gaussian quadratures; Matlab- software; Repeated integrals; Integral equations
Replicated Computational Results (RCR) Report for A Sparse Symmetric Indefinite Direct Solver for GPU Architectures,2016,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964579481&doi=10.1145%2f2851489&partnerID=40&md5=04e4be82e7ebae83dcc706e6f5c4b3f3,"A Sparse Symmetric Indefinite Direct Solver for GPU Architectures includes performance results and comparisons of the developed GPU direct solver against a CPU direct solver. New performance data were gathered using software provided by the manuscript authors on two new platforms and compared against the performance of the MUMPS direct solver. After completing this process, the published results have been deemed replicable by the reviewer. © 2016 ACM.",Bit compatibility; GPU; Indefinite symmetric systems; LDL factorization; Multifrontal direct solver; Replicated computational results; Sparse linear systems,Computer software; Software engineering; Computational results; Direct solvers; Performance data; Sparse linear systems; Symmetric indefinite; Symmetric systems; Linear systems
"Remark on ""algorithm 673: Dynamic huffman coding""",2016,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072039179&doi=10.1145%2f2740959&partnerID=40&md5=6ff66634ff1f79a002f0907f956daf83,[No abstract available],,
A Sparse Symmetric Indefinite Direct Solver for GPU Architectures,2016,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019896703&doi=10.1145%2f2756548&partnerID=40&md5=47b9be44e041f7b98c40ce54735545dc,"In recent years, there has been considerable interest in the potential for graphics processing units (GPUS) to speed up the performance of sparse direct linear solvers. Efforts have focused on symmetric positive-definite systems for which no pivoting is required, while little progress has been reported for the much harder indefinite case. We address this challenge by designing and developing a sparse symmetric indefinite solver SSIDS. This new library-quality LDLT factorization is designed for use on GPU architectures and incorporates threshold partial pivoting within a multifrontal approach. Both the factorize and the solve phases are performed using the GPU. Another important feature is that the solver produces bit-compatible results. Numerical results for indefinite problems arising from a range of practical applications demonstrate that, for large problems, SSIDS achieves performance improvements of up to a factor of4.6x compared with a state-of-The-Art multifrontal solver on a multicore CPU. © 2016 ACM.",,Computer graphics; Program processors; Direct solvers; Important features; Ldlt factorizations; Multi-core cpus; Numerical results; State of the art; Symmetric indefinite; Symmetric positive definite; Graphics processing unit
Algorithm 955: Approximation of the inverse poisson cumulative distribution function,2016,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042642414&doi=10.1145%2f2699466&partnerID=40&md5=5cbdb9e9d2e9c85b1688df681cb14788,"New approximations for the inverse of the incomplete gamma function are derived, which are used to develop efficient evaluations of the inverse Poisson cumulative distribution function. An asymptotic approximation based on the standard Normal approximation is particularly good for CPUs with MIMD cores, while for GPUs and other hardware with vector units, a second asymptotic approximation based on Temme's approximation of the incomplete gamma function is more efficient due to conditional branching within each vector. The accuracy and efficiency of the software implementations is assessed on both CPUs and GPUs. © 2016 ACM.",CUDA; GPU; Poisson distribution,Approximation algorithms; Graphics processing unit; Poisson distribution; Program processors; Asymptotic approximation; CUDA; Cumulative distribution function; Incomplete gamma functions; Normal approximation; Software implementation; Vector units; Distribution functions
A high performance QDWH-SVD solver using hardware accelerators,2016,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84984847711&doi=10.1145%2f2894747&partnerID=40&md5=03bba385882c2dde329d079f38ec069e,"This article describes a new high performance implementation of the QR-based Dynamically Weighted Halley Singular Value Decomposition (QDWH-SVD) solver on multicore architecture enhanced with multiple GPUs. The standard QDWH-SVD algorithm was introduced by Nakatsukasa and Higham (SIAM SISC, 2013) and combines three successive computational stages: (1) the polar decomposition calculation of the original matrix using the QDWH algorithm, (2) the symmetric eigendecomposition of the resulting polar factor to obtain the singular values and the right singular vectors, and (3) the matrix-matrix multiplication to get the associated left singular vectors. A comprehensive test suite highlights the numerical robustness of the QDWH-SVD solver. Although it performs up to two times more flops when computing all singular vectors compared to the standard SVD solver algorithm, our new high performance implementation on single GPU results in up to 4× improvements for asymptotic matrix sizes, compared to the equivalent routines from existing state-of-the-art open-source and commercial libraries. However, when only singular values are needed, QDWH-SVD is penalized by performing more flops by an order of magnitude. The singular value only implementation of QDWH-SVD on single GPU can still run up to 18% faster than the best existing equivalent routines. © 2016 ACM 0098-3500/2016/08-ART6 $15.00",GPU-based scientific computing; Mixed precision algorithms; Polar decomposition; Singular value decomposition; Symmetric eigensolver,Computer hardware; Open systems; Program processors; Software architecture; Eigensolvers; Gpu-based; Hardware accelerators; Matrix matrix multiplications; Mixed precision; Multicore architectures; Numerical robustness; Polar decompositions; Singular value decomposition
Sampling exactly from the normal distribution,2016,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013842563&doi=10.1145%2f2710016&partnerID=40&md5=b8f9a459ad8822cbd1d5d122d2f3c05c,"An algorithm for sampling exactly from the normal distribution is given. The algorithm reads some number of uniformly distributed random digits in a given base and generates an initial portion of the representation of a normal deviate in the same base. Thereafter, uniform random digits are copied directly into the representation of the normal deviate. Thus, in contrast to existing methods, it is possible to generate normal deviates exactly rounded to any precision with a mean cost that scales linearly in the precision. The method performs no extended precision arithmetic, calls no transcendental functions, and uses no floating point arithmetic whatsoever; it uses only simple integer operations. It can easily be adapted to sample exactly from the discrete normal distribution whose parameters are rational numbers. © 2016 ACM 0098-3500/2016/01-ART3 $15.00",Exact sampling; Normal distribution; Random deviates,Digital arithmetic; Exact samplings; Integer operations; Precision arithmetic; Random deviates; Rational numbers; Transcendental functions; Normal distribution
ShearLab 3D: Faithful digital shearlet transforms based on compactly supported shearlets,2016,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84975918311&doi=10.1145%2f2740960&partnerID=40&md5=d72373f1a3f75dd929528476e5be38d8,"Wavelets and their associated transforms are highly efficient when approximating and analyzing onedimensional signals. However, multivariate signals such as images or videos typically exhibit curvilinear singularities, which wavelets are provably deficient in sparsely approximating and also in analyzing in the sense of, for instance, detecting their direction. Shearlets are a directional representation system extending the wavelet framework, which overcomes those deficiencies. Similar to wavelets, shearlets allow a faithful implementation and fast associated transforms. In this article, we will introduce a comprehensive carefully documented software package coined ShearLab 3D (www.ShearLab.org) and discuss its algorithmic details. This package provides MATLAB code for a novel faithful algorithmic realization of the 2D and 3D shearlet transform (and their inverses) associated with compactly supported universal shearlet systems incorporating the option of using CUDA. We will present extensive numerical experiments in 2D and 3D concerning denoising, inpainting, and feature extraction, comparing the performance of ShearLab 3D with similar transform-based algorithms such as curvelets, contourlets, or surfacelets. In the spirit of reproducible research, all scripts are accessible on www.ShearLab.org. © 2016 ACM.",Imaging sciences; Shearlets; Software package; Wavelets,MATLAB; Software packages; Algorithmic realization; Curvilinear singularity; Digital shearlet transforms; Multivariate signals; Numerical experiments; Reproducible research; Shearlets; Wavelets; Wavelet transforms
Algorithm 1045: A Covariate-Dependent Approach to Gaussian Graphical Modeling in R,2024,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85197497622&doi=10.1145%2f3659206&partnerID=40&md5=ab7ab129f422d1457282eee789d506c9,"Graphical models are used to capture complex multivariate relationships and have applications in diverse disciplines such as biology, physics, and economics. Within this field, Gaussian graphical models aim to identify the pairs of variables whose dependence is maintained even after conditioning on the remaining variables in the data, known as the conditional dependence structure of the data. There are many existing software packages for Gaussian graphical modeling, however, they often make restrictive assumptions that reduce their flexibility for modeling data that are not identically distributed. Conversely, covdepGE is an R implementation of a variational weighted pseudo-likelihood algorithm for modeling the conditional dependence structure as a continuous function of an extraneous covariate. To build on the efficiency of this algorithm, covdepGE leverages parallelism and C++ integration with R. Additionally, covdepGE provides fully-automated and data-driven hyperparameter specification while maintaining flexibility for the user to decide key components of the estimation procedure. Through an extensive simulation study spanning diverse settings, covdepGE is demonstrated to be top of its class in recovering the ground truth conditional dependence structure while efficiently managing computational overhead. © 2024 Copyright held by the owner/author(s).",Gaussian graphical models; heterogeneous graphs; pseudo-likelihood; structure learning; variational inference,C++ (programming language); Gaussian distribution; Inference engines; Conditional dependence; Covariates; Dependence structures; Gaussian graphical models; GraphicaL model; Heterogeneous graph; Modeling data; Pseudo-likelihood; Structure-learning; Variational inference; Graphic methods
Optimal Re-Materialization Strategies for Heterogeneous Chains: How to Train Deep Neural Networks with Limited Memory,2024,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85197299840&doi=10.1145%2f3648633&partnerID=40&md5=b324be326ee1cdce8930757371062dc0,"Training in Feed Forward Deep Neural Networks is a memory-intensive operation which is usually performed on GPUs with limited memory capacities. This may force data scientists to limit the depth of the models or the resolution of the input data if data does not fit in the GPU memory. The re-materialization technique, whose idea comes from the checkpointing strategies developed in the Automatic Differentiation literature, allows data scientists to limit the memory requirements related to the storage of intermediate data (activations), at the cost of an increase in the computational cost.This paper introduces a new strategy of re-materialization of activations that significantly reduces memory usage. It consists in selecting which activations are saved and which activations are deleted during the forward phase and then recomputing the deleted activations when they are needed during the backward phase.We propose an original computation model that combines two types of activation savings: either only storing the layer inputs or recording the complete history of operations that produced the outputs. This paper focuses on the fully heterogeneous case, where the computation time and the memory requirement of each layer is different. We prove that finding the optimal solution is NP-hard and that classical techniques from Automatic Differentiation literature do not apply. Moreover, the classical assumption of memory persistence of materialized activations, used to simplify the search of optimal solutions, does not hold anymore. Thus, we propose a weak memory persistence property and provide a dynamic program to compute the optimal sequence of computations.This algorithm is made available through the Rotor software, a PyTorch plug-in dealing with any network consisting of a sequence of layers, each of them having an arbitrarily complex structure. Through extensive experiments, we show that our implementation consistently outperforms existing re-materialization approaches for a large class of networks, image sizes, and batch sizes.  © 2024 Copyright held by the owner/author(s).",Additional Key Words and PhrasesCheckpointing; convolutional neural networks; dynamic programming; memory; re-materialization,Convolutional neural networks; Deep neural networks; Digital storage; Dynamic programming; Feedforward neural networks; Optimal systems; Program processors; Additional key word and phrasescheckpointing; Automatic differentiations; Convolutional neural network; Feed forward; Key words; Limited memory; Memory capacity; Memory requirements; Optimal solutions; Re-materialization; Chemical activation
PyOED: An Extensible Suite for Data Assimilation and Model-Constrained Optimal Design of Experiments,2024,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85197043759&doi=10.1145%2f3653071&partnerID=40&md5=7e619c22a9fc150d7e383589eef0b93a,"This article describes PyOED, a highly extensible scientific package that enables developing and testing model-constrained optimal experimental design (OED) for inverse problems. Specifically, PyOED aims to be a comprehensive Python toolkit for model-constrained OED. The package targets scientists and researchers interested in understanding the details of OED formulations and approaches. It is also meant to enable researchers to experiment with standard and innovative OED technologies with a wide range of test problems (e.g., simulation models). OED, inverse problems (e.g., Bayesian inversion), and data assimilation (DA) are closely related research fields, and their formulations overlap significantly. Thus, PyOED is continuously being expanded with a plethora of Bayesian inversion, DA, and OED methods as well as new scientific simulation models, observation error models, and observation operators. These pieces are added such that they can be permuted to enable testing OED methods in various settings of varying complexities. The PyOED core is completely written in Python and utilizes the inherent object-oriented capabilities; however, the current version of PyOED is meant to be extensible rather than scalable. Specifically, PyOED is developed to ""enable rapid development and benchmarking of OED methods with minimal coding effort and to maximize code reutilization.""This article provides a brief description of the PyOED layout and philosophy and provides a set of exemplary test cases and tutorials to demonstrate the potential of the package. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",data assimilation; inverse problems; mathematical software; OED; Optimal experimental design,Codes (symbols); Design of experiments; Differential equations; High level languages; Object oriented programming; Python; Statistics; Bayesian inversion; Constrained optimal designs; Data assimilation; Design approaches; Experimental design method; Mathematical software; Optimal experimental designs; Simulation model; Testing models; Inverse problems
Algorithm 1042: Sparse Precision Matrix Estimation with SQUIC,2024,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85197210050&doi=10.1145%2f3650108&partnerID=40&md5=05a5ec296a6444316cfa263e1d1c979a,"We present SQUIC, a fast and scalable package for sparse precision matrix estimation. The algorithm employs a second-order method to solve the ℓ1-regularized maximum likelihood problem, utilizing highly optimized linear algebra subroutines. In comparative tests using synthetic datasets, we demonstrate that SQUIC not only scales to datasets of up to a million random variables but also consistently delivers runtimes that are significantly faster than other well-established sparse precision matrix estimation packages. Furthermore, we showcase the application of the introduced package in classifying microarray gene expressions. We demonstrate that by utilizing a matrix form of the tuning parameter (also known as the regularization parameter), SQUIC can effectively incorporate prior information into the estimation procedure, resulting in improved application results with minimal computational overhead. © 2024 Copyright held by the owner/author(s).",covariance matrix; matrix factorization; matrix inversion; Sparse precision matrix estimation,Gene expression; Matrix factorization; Maximum likelihood estimation; Comparative tests; Covariance matrices; Ma ximum likelihoods; Matrix estimation; Matrix factorizations; Matrix inversions; Maximum-likelihood; Precision matrix; Second-order methods; Sparse precision matrix estimation; Covariance matrix
Avoiding Breakdown in Incomplete Factorizations in Low Precision Arithmetic,2024,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85194009969&doi=10.1145%2f3651155&partnerID=40&md5=f95bbefc40f07f0b7d7df11d30d965a0,"The emergence of low precision floating-point arithmetic in computer hardware has led to a resurgence of interest in the use of mixed precision numerical linear algebra. For linear systems of equations, there has been renewed enthusiasm for mixed precision variants of iterative refinement. We consider the iterative solution of large sparse systems using incomplete factorization preconditioners. The focus is on the robust computation of such preconditioners in half precision arithmetic and employing them to solve symmetric positive definite systems to higher precision accuracy; however, the proposed ideas can be applied more generally. Even for well-conditioned problems, incomplete factorizations can break down when small entries occur on the diagonal during the factorization. When using half precision arithmetic, overflows are an additional possible source of breakdown. We examine how breakdowns can be avoided and implement our strategies within new half precision Fortran sparse incomplete Cholesky factorization software. Results are reported for a range of problems from practical applications. These demonstrate that, even for highly ill-conditioned problems, half precision preconditioners can potentially replace double precision preconditioners, although unsurprisingly this may be at the cost of additional iterations of a Krylov solver. © 2024 Copyright held by the owner/author(s).",half precision arithmetic; incomplete factorizations; iterative refinement; mixed precision arithmetic; preconditioning; sparse linear systems; Sparse matrices,Digital arithmetic; Factorization; Linear algebra; Linear systems; Half precision arithmetic; Incomplete factorization; Iterative refinement; Mixed precision; Mixed precision arithmetic; Precision arithmetic; Preconditioning; Sparse linear systems; Sparse matrices; Computer hardware
Algorithm 1043: Faster Randomized SVD with Dynamic Shifts,2024,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85197252975&doi=10.1145%2f3660629&partnerID=40&md5=dbf3882daf3f765f92054435616422aa,"Aiming to provide a faster and convenient truncated SVD algorithm for large sparse matrices from real applications (i.e., for computing a few of the largest singular values and the corresponding singular vectors), a dynamically shifted power iteration technique is applied to improve the accuracy of the randomized SVD method. This results in a dynamic shifts-based randomized SVD (dashSVD) algorithm, which also collaborates with the skills for handling sparse matrices. An accuracy-control mechanism is included in the dashSVD algorithm to approximately monitor the per vector error bound of computed singular vectors with negligible overhead. Experiments on real-world data validate that the dashSVD algorithm largely improves the accuracy of a randomized SVD algorithm or attains the same accuracy with fewer passes over the matrix, and provides an efficient accuracy-control mechanism to the randomized SVD computation, while demonstrating the advantages on runtime and parallel efficiency. A bound of the approximation error of the randomized SVD with the shifted power iteration is also proved.  © 2024 Copyright held by the owner/author(s).",random embedding; shifted power iteration; sparse matrix; Truncated singular value decomposition,Computational efficiency; Dynamics; Iterative methods; Accuracy control; Control mechanism; Dynamic shift; Embeddings; Power; Random embedding; Shifted power iteration; Singular vectors; Sparse matrices; Truncated singular value decomposition; Singular value decomposition
"Algorithm 1044: PyGenStability, a Multiscale Community Detection with Generalized Markov Stability",2024,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85197263586&doi=10.1145%2f3651225&partnerID=40&md5=fd16d2a98da72d4411def4ca90c2f4fa,"We present PyGenStability, a general-use Python software package that provides a suite of analysis and visualization tools for unsupervised multiscale community detection in graphs. PyGenStability finds optimized partitions of a graph at different levels of resolution by maximizing the generalized Markov Stability quality function with the Louvain or Leiden algorithm. The package includes automatic detection of robust graph partitions and allows the flexibility to choose quality functions for weighted undirected, directed, and signed graphs and to include other user-defined quality functions.  © 2024 Copyright held by the owner/author(s).",Additional Key Words and PhrasesMultiscale community detection; generalized Markov Stability; graph clustering; graphs; Leiden algorithm; Louvain algorithm; modularity; network science; Python; unsupervised learning,Clustering algorithms; Computer software; Directed graphs; High level languages; Population dynamics; Unsupervised learning; Additional key word and phrasesmultiscale community detection; Community detection; Generalized markov stability; Graph; Graph clustering; Key words; Leiden algorithm; Louvain algorithm; Modularity; Network science; Python
Remark on Algorithm 1012: Computing Projections with Large Datasets,2024,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85197278452&doi=10.1145%2f3656581&partnerID=40&md5=c4059d09633a477589d61f9b39abae6c,"In ACM TOMS Algorithm 1012, the DELAUNAYSPARSE software is given for performing Delaunay interpolation in medium to high dimensions. When extrapolating outside the convex hull of the training set, DELAUNAYSPARSE calls the nonnegative least squares solver DWNNLS to compute projections onto the convex hull. However, DWNNLS and many other available sum-of-squares optimization solvers were not intended for usage with many variable problems, which result from the large training sets that are typical in machine learning applications. Thus, a new PROJECT subroutine is given, based on the highly customizable quadratic program solver BQPD. This solution is shown to be as robust as DELAUNAYSPARSE for projection onto both synthetic and real-world datasets, where other available solvers frequently fail. Although it is intended as an update for DELAUNAYSPARSE, due to the difficulty and prevalence of the problem, this solution is likely to be of external interest as well.  © 2024 Copyright held by the owner/author(s).",Additional Key Words and PhrasesDelaunay interpolation; data skew; projection; quadratic programming,Computational geometry; Interpolation; Large datasets; Subroutines; Additional key word and phrasesdelaunay interpolation; Convex hull; Data skew; Delaunay; Higher dimensions; Key words; Large datasets; Nonnegative least squares; Projection; Training sets; Quadratic programming
Algorithm 1039: Automatic Generators for a Family of Matrix Multiplication Routines with Apache TVM,2024,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189106771&doi=10.1145%2f3638532&partnerID=40&md5=24ae15969c499b63211f69ca09c3e714,"We explore the utilization of the Apache TVM open source framework to automatically generate a family of algorithms that follow the approach taken by popular linear algebra libraries, such as GotoBLAS2, BLIS, and OpenBLAS, to obtain high-performance blocked formulations of the general matrix multiplication (gemm). In addition, we fully automatize the generation process by also leveraging the Apache TVM framework to derive a complete variety of the processor-specific micro-kernels for gemm. This is in contrast with the convention in high-performance libraries, which hand-encode a single micro-kernel per architecture using Assembly code. In global, the combination of our TVM-generated blocked algorithms and micro-kernels for gemm (1) improves portability, maintainability, and, globally, streamlines the software life cycle; (2) provides high flexibility to easily tailor and optimize the solution to different data types, processor architectures, and matrix operand shapes, yielding performance on a par (or even superior for specific matrix shapes) with that of hand-Tuned libraries; and (3) features a small memory footprint.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesPortability and maintainability; Apache TVM; BLIS framework; blocking; high performance; matrix multiplication; SIMD vectorization; software lifecycle,Computer software portability; Computer software reusability; Libraries; Life cycle; Maintainability; Open source software; Additional key word and phrasesportability and maintainability; Apache TVM; BLIS framework; Blockings; High performance; Key words; MAtrix multiplication; Performance; SIMD vectorization; Software life cycles; Vectorization; Matrix algebra
Efficient and Validated Numerical Evaluation of Abelian Integrals,2024,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189101076&doi=10.1145%2f3637550&partnerID=40&md5=8ab2594b7111a66d930d27e988a4b5d2,"Abelian integrals play a key role in the infinitesimal version of Hilbert's 16th problem. Being able to evaluate such integrals-with guaranteed error bounds-is a fundamental step in computer-Aided proofs aimed at this problem. Using interpolation by trigonometric polynomials and quasi-Newton-Kantorovitch validation, we develop a validated numerics method for computing Abelian integrals in a quasi-linear number of arithmetic operations. Our approach is both effective, as exemplified on two practical perturbed integrable systems, and amenable to an implementation in a formal proof assistant, which is key to provide fully reliable computer-Aided proofs.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Abelian integral; Additional Key Words and PhrasesHilbert's 16th problem; limit cycles; Newton-like operator; rigorous numerics; trigonometric polynomial interpolation,Error analysis; Abelian integral; Additional key word and phraseshilbert 16th problem; Computer-aided proofs; Key words; Limit-cycle; Newton-like operator; Polynomial interpolation; Rigorous numerics; Trigonometric polynomial; Trigonometric polynomial interpolation; Interpolation
Algorithm 1041: HiPPIS-A High-order Positivity-preserving Mapping Software for Structured Meshes,2024,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189086429&doi=10.1145%2f3632291&partnerID=40&md5=f4128dc45ce55cdcdf5f9332ba4d6794,"Polynomial interpolation is an important component of many computational problems. In several of these computational problems, failure to preserve positivity when using polynomials to approximate or map data values between meshes can lead to negative unphysical quantities. Currently, most polynomial-based methods for enforcing positivity are based on splines and polynomial rescaling. The spline-based approaches build interpolants that are positive over the intervals in which they are defined and may require solving a minimization problem and/or system of equations. The linear polynomial rescaling methods allow for high-degree polynomials but enforce positivity only at limited locations (e.g., quadrature nodes). This work introduces open-source software (HiPPIS) for high-order data-bounded interpolation (DBI) and positivity-preserving interpolation (PPI) that addresses the limitations of both the spline and polynomial rescaling methods. HiPPIS is suitable for approximating and mapping physical quantities such as mass, density, and concentration between meshes while preserving positivity. This work provides Fortran and Matlab implementations of the DBI and PPI methods, presents an analysis of the mapping error in the context of PDEs, and uses several 1D and 2D numerical examples to demonstrate the benefits and limitations of HiPPIS.  © 2024 Copyright held by the owner/author(s).",Additional Key Words and PhrasesPositivity-preserving; data-bounded; polynomial interpolation; vectorization,Interpolation; Mapping; MATLAB; Numerical methods; Open source software; Open systems; Additional key word and phrasespositivity-preserving; Computational problem; Data-bounded; High-order; Higher-order; Key words; Polynomial interpolation; Positivity preserving; Rescaling method; Vectorization; Polynomials
Data-flow Reversal and Garbage Collection,2024,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189160132&doi=10.1145%2f3627537&partnerID=40&md5=d45f48099dd7741f7f6af2db4c5ebe6a,"Data-flow reversal is at the heart of source-Transformation reverse algorithmic differentiation (reverse ST-AD), arguably the most efficient way to obtain gradients of numerical models. However, when the model implementation language uses garbage collection (GC), for instance, in Java or Python, the notion of address that is needed for data-flow reversal disappears. Moreover, GC is asynchronous and does not appear explicitly in the source. This article presents an extension to the model of reverse ST-AD suitable for a language with GC. The approach is validated on a Java implementation of a simple Navier-Stokes solver. Performance is compared with existing AD tools ADOL-C and Tapenade on an equivalent implementation in C and Fortran.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Automatic differentiation; data-flow reversal; dynamic memory; garbage collection,Data flow analysis; Data transfer; Java programming language; Metadata; Navier Stokes equations; Algorithmic differentiations; Automatic differentiations; Data-flow reversal; Dataflow; Dynamic memory; Flow reversals; Garbage collection; Implementation languages; Model implementation; Source transformation; Python
An Interface-Preserving Moving Mesh in Multiple Space Dimensions,2024,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189145890&doi=10.1145%2f3630000&partnerID=40&md5=5f35f0b6955ac92d63e21a2e7b21a342,"An interface-preserving moving mesh algorithm in two or higher dimensions is presented. It resolves a moving (d-1)-dimensional manifold directly within the d-dimensional mesh, which means that the interface is represented by a subset of moving mesh cell-surfaces. The underlying mesh is a conforming simplicial partition that fulfills the Delaunay property. The local remeshing algorithms allow for strong interface deformations. We give a proof that the given algorithms preserve the interface after interface deformation and remeshing steps. Originating from various numerical methods, data is attached cell-wise to the mesh. After each remeshing operation, the interface-preserving moving mesh retains valid data by projecting the data to the new mesh cells.An open source implementation of the moving mesh algorithm is available at Reference [1].  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Delaunay triangulation; interface tracking; local dynamic remeshing; mesh adaptation; Moving mesh,Cell membranes; Mesh generation; Delaunay triangulation; Interface deformation; Interface tracking; Local dynamic remeshing; Local dynamics; Mesh adaptation; Mesh cells; Moving mesh; Moving mesh algorithms; Remeshing; Numerical methods
Hermitian Dynamic Mode Decomposition-Numerical Analysis and Software Solution,2024,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189103576&doi=10.1145%2f3641884&partnerID=40&md5=1d1106600a3564d61060b469edb5d08b,"The Dynamic Mode Decomposition (DMD) is a versatile and increasingly popular method for data driven analysis of dynamical systems that arise in a variety of applications in, e.g., computational fluid dynamics, robotics or machine learning. In the framework of numerical linear algebra, it is a data driven Rayleigh-Ritz procedure applied to a DMD matrix that is derived from the supplied data. In some applications, the physics of the underlying problem implies hermiticity of the DMD matrix, so the general DMD procedure is not computationally optimal. Furthermore, it does not guarantee important structural properties of the Hermitian eigenvalue problem and may return non-physical solutions. This paper proposes a software solution to the Hermitian (including the real symmetric) DMD matrices, accompanied with a numerical analysis that contains several fine and instructive numerical details. The eigenpairs are computed together with their residuals, and perturbation theory provides error bounds for the eigenvalues and eigenvectors. The software is developed and tested using the LAPACK package.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesDynamic mode decomposition; dynamical systems; eigenvalues; Koopman operator; LAPACK; QR factorization; residuals; singular value decomposition; streaming DMD,Computation theory; Computational fluid dynamics; Dynamical systems; Eigenvalues and eigenfunctions; Error analysis; Perturbation techniques; Singular value decomposition; Software testing; Additional key word and phrasesdynamic mode decomposition; Dynamic mode decompositions; Eigen-value; Key words; Koopman operator; LAPACK; Mode decomposition; QR factorizations; Residual; Streaming dynamic mode decomposition; Dynamic mode decomposition; Mode decomposition
Algorithm 1040: The Sparse Grids Matlab Kit-a Matlab implementation of sparse grids for high-dimensional function approximation and uncertainty quantification,2024,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189158602&doi=10.1145%2f3630023&partnerID=40&md5=a6560c1ad16c48eb9d71ca6b7906d5d2,"The Sparse Grids Matlab Kit provides a Matlab implementation of sparse grids, and can be used for approximating high-dimensional functions and, in particular, for surrogate-model-based uncertainty quantification. It is lightweight, high-level and easy to use, good for quick prototyping and teaching; however, it is equipped with some features that allow its use also in realistic applications. The goal of this paper is to provide an overview of the data structure and of the mathematical aspects forming the basis of the software, as well as comparing the current release of our package to similar available software.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",High-dimensional functions; reduced order modeling; sparse grids; surrogate modeling; uncertainty quantification,Approximation algorithms; Uncertainty analysis; High dimensional function approximations; Higher dimensional function; Mathematical aspects; Model-based OPC; Realistic applications; Reduced order modelling; Reduced-order model; Sparse grid; Surrogate modeling; Uncertainty quantifications; MATLAB
A LAPACK Implementation of the Dynamic Mode Decomposition,2024,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189080360&doi=10.1145%2f3640012&partnerID=40&md5=143517388fabd7ba63e16de95f2dda83,"The Dynamic Mode Decomposition (DMD) is a method for computational analysis of nonlinear dynamical systems in data driven scenarios. Based on high fidelity numerical simulations or experimental data, the DMD can be used to reveal the latent structures in the dynamics or as a forecasting or a model order reduction tool. The theoretical underpinning of the DMD is the Koopman operator on a Hilbert space of observables of the dynamics under study. This paper describes a numerically robust and versatile variant of the DMD and its implementation using the state-of-the-art dense numerical linear algebra software package LAPACK. The features of the proposed software solution include residual bounds for the computed eigenpairs of the DMD matrix, eigenvectors refinements and computation of the eigenvectors of the Exact DMD, compressed DMD for efficient analysis of high dimensional problems that can be easily adapted for fast updates in a streaming DMD. Numerical analysis is the bedrock of numerical robustness and reliability of the software, that is tested following the highest standards and practices of LAPACK. Important numerical topics are discussed in detail and illustrated using numerous numerical examples.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesDynamic mode decomposition; dynamical systems; eigenvalues; Koopman operator; LAPACK; QR factorization; residuals; singular value decomposition; streaming DMD,Dynamical systems; Eigenvalues and eigenfunctions; Nonlinear dynamical systems; Numerical methods; Reliability analysis; Singular value decomposition; Software reliability; Software testing; Additional key word and phrasesdynamic mode decomposition; Dynamic mode decompositions; Eigen-value; Key words; Koopman operator; LAPACK; Mode decomposition; QR factorizations; Residual; Streaming dynamic mode decomposition; Dynamic mode decomposition; Mode decomposition
Computation of Turing Bifurcation Normal Form for n-Component Reaction-Diffusion Systems,2023,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181488068&doi=10.1145%2f3625560&partnerID=40&md5=3ca087a971cf9bb3a4bf81421a497b6c,"General expressions are derived for the amplitude equation valid at a Turing bifurcation of a system of reaction-diffusion equations in one spatial dimension, with an arbitrary number of components. The normal form is computed up to fifth order, which enables the detection and analysis of codimension-two points where the criticality of the bifurcation changes. The expressions are implemented within a Python package, in which the user needs to specify only expressions for the reaction kinetics and the values of diffusion constants. The code is augmented with a Mathematica routine to compute curves of Turing bifurcations in a parameter plane and automatically detect codimension-two points. The software is illustrated with examples that show the versatility of the method including a case with cross-diffusion, a higher-order scalar equation and a four-component system.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Normal form; pattern formation; reaction diffusion; Turing bifurcation,Bifurcation (mathematics); Computer software; Diffusion in liquids; Linear equations; Reaction kinetics; Amplitude equation; Codimension-two; General expression; Normal form; Pattern formation; Reaction diffusion equations; Reaction diffusion systems; Reaction-Diffusion; Turing bifurcation; Two-point; Python
Parametric Information Geometry with the Package Geomstats,2023,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181739015&doi=10.1145%2f3627538&partnerID=40&md5=5b907967a2cd51e66e0fc6ff9681d922,"We introduce the information geometry module of the Python package Geomstats. The module first implements Fisher-Rao Riemannian manifolds of widely used parametric families of probability distributions, such as normal, gamma, beta, Dirichlet distributions, and more. The module further gives the Fisher-Rao Riemannian geometry of any parametric family of distributions of interest, given a parameterized probability density function as input. The implemented Riemannian geometry tools allow users to compare, average, interpolate between distributions inside a given family. Importantly, such capabilities open the door to statistics and machine learning on probability distributions. We present the object-oriented implementation of the module along with illustrative examples and show how it can be used to perform learning on manifolds of parametric probability distributions.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Fisher-Rao metric; Information geometry; Learning on manifolds; Riemannian manifold,Geometry; Probability density function; Python; Dirichlet distributions; Fisher-Rao metric; Information geometry; Learning on manifold; Parameterized; Parametric family; Parametric information; Probability: distributions; Riemannian geometry; Riemannian manifold; Probability distributions
Manifolds.jl: An Extensible Julia Framework for Data Analysis on Manifolds,2023,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180813143&doi=10.1145%2f3618296&partnerID=40&md5=13672bdf8804c57c30cd1b8176d1f14a,"We present the Julia package Manifolds.jl, providing a fast and easy-to-use library of Riemannian manifolds and Lie groups. This package enables working with data defined on a Riemannian manifold, such as the circle, the sphere, symmetric positive definite matrices, or one of the models for hyperbolic spaces. We introduce a common interface, available in ManifoldsBase.jl, with which new manifolds, applications, and algorithms can be implemented. We demonstrate the utility of Manifolds.jl using Bézier splines, an optimization task on manifolds, and principal component analysis on nonlinear data. In a benchmark, Manifolds.jl outperforms all comparable packages for low-dimensional manifolds in speed; over Python and Matlab packages, the improvement is often several orders of magnitude, while over C/C++ packages, the improvement is two-fold. For high-dimensional manifolds, it outperforms all packages except for Tensorflow-Riemopt, which is specifically tailored for high-dimensional manifolds.  © 2023 Copyright held by the owner/author(s).",exponential map; Julia; Lie group; logarithmic map; nonlinear spaces; optimization on manifolds; Riemannian manifold; scientific computing,Geometry; MATLAB; Principal component analysis; Python; Exponential map; High-dimensional; Higher-dimensional; Julium; Lie-groups; Logarithmic map; Nonlinear space; Optimization on manifolds; Riemannian manifold; Symmetric positive definite matrices; Lie groups
KiT-RT: An Extendable Framework for Radiative Transfer and Therapy,2023,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181515185&doi=10.1145%2f3630001&partnerID=40&md5=865885642ba944f3851276ba00e661ad,"In this article, we present Kinetic Transport Solver for Radiation Therapy (KiT-RT), an open source C++-based framework for solving kinetic equations in therapy applications available at https://github.com/CSMMLab/KiT-RT. This software framework aims to provide a collection of classical deterministic solvers for unstructured meshes that allow for easy extendability. Therefore, KiT-RT is a convenient base to test new numerical methods in various applications and compare them against conventional solvers. The implementation includes spherical harmonics, minimal entropy, neural minimal entropy, and discrete ordinates methods. Solution characteristics and efficiency are presented through several test cases ranging from radiation transport to electron radiation therapy. Due to the variety of included numerical methods and easy extendability, the presented open source code is attractive for both developers, who want a basis to build their numerical solvers, and users or application engineers, who want to gain experimental insights without directly interfering with the codebase.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",finite volume methods; Kinetic theory; machine learning; radiation therapy; radiation transport,Entropy; Heat radiation; Kinetic energy; Kinetic theory; Kinetics; Machine learning; Numerical methods; Open source software; Open systems; Radiotherapy; Deterministics; Finite-volume method; Kinetic equations; Kinetic transport; Machine-learning; Open-source; Radiation transport; Software frameworks; Spherical harmonics; Unstructured meshes; Finite volume method
Algorithm 1038: KCC: A MATLAB Package for k-Means-based Consensus Clustering,2023,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174864116&doi=10.1145%2f3616011&partnerID=40&md5=8a45dd8fa3e81d99da860bbb1069b9dd,"Consensus clustering is gaining increasing attention for its high quality and robustness. In particular, k-means-based Consensus Clustering (KCC) converts the usual computationally expensive problem to a classic k-means clustering with generalized utility functions, bringing potentials for large-scale data clustering on different types of data. Despite KCC's applicability and generalizability, implementing this method such as representing the binary dataset in the k-means heuristic is challenging and has seldom been discussed in prior work. To fill this gap, we present a MATLAB package, KCC, that completely implements the KCC framework and utilizes a sparse representation technique to achieve a low space complexity. Compared to alternative consensus clustering packages, the KCC package is of high flexibility, efficiency, and effectiveness. Extensive numerical experiments are also included to show its usability on real-world datasets.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Consensus clustering; k-means; MATLAB; utility functions,Cluster analysis; K-means clustering; Consensus clustering; Data clustering; High quality; High robustness; K-means; K-means++ clustering; Large scale data; Representation techniques; Sparse representation; Utility functions; Heuristic methods
IEEE-754 Precision-p base-β Arithmetic Implemented in Binary,2023,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181742863&doi=10.1145%2f3596218&partnerID=40&md5=74a251499fe4dd556ca6bbc5a3d7c4ee,"We show how an IEEE-754 conformant precision-p base-β arithmetic can be implemented based on some binary floating-point and/or integer arithmetic. This includes the four basic operations and square root subject to the five IEEE-754 rounding modes, namely the nearest roundings with roundTiesToEven and roundTiesToAway, the directed roundings downwards and upwards, as well as rounding towards zero. Exceptional values like ∞ of NaN are covered according to the IEEE-754 arithmetic standard. The results of the precision-p base-β operations are computed using some underlying precision-q binary arithmetic. We distinguish two cases. When using a precision-q binary integer arithmetic, the base-β precision p is limited for all operations by β2p ≤ 2q, whereas using a precision-q binary floating-point arithmetic imposes stronger limits on the base-β precision, namely β2p ≤ 2q for addition and multiplication, β2p ≤ 2q-1 for division and β2p ≤ 2q-3 for the square root. Those limitations cannot be improved. The algorithms are implemented in a Matlab/Octave flbeta-toolbox with the choice of using uint64 or binary64 as underlying arithmetic. The former allows larger precisions, the latter is advantageous for the square root, whereas computing times are similar. The flbeta-toolbox offers precision-p base-β scalar, vector and matrix operations including sparse matrices as well as corresponding interval operations. The base β can be chosen in the range β [2,64]. The flbeta-toolbox will be part of Version 13 of INTLAB [18], the Matlab/Octave toolbox for reliable computing.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",base-β; double rounding; Floating-point arithmetic; IEEE-754; interval arithmetic; INTLAB; precision-p,IEEE Standards; MATLAB; Matrix algebra; Base-β; Double rounding; Floating points; Floating-point arithmetic; IEEE-754; Integer arithmetic; Interval arithmetic; INTLAB; Precision-p; Square-root; Digital arithmetic
New Subspace Method for Unconstrained Derivative-Free Optimization,2023,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181436293&doi=10.1145%2f3618297&partnerID=40&md5=b7693098f6f5be0c0973cc799378a492,"This article defines an efficient subspace method, called SSDFO, for unconstrained derivative-free optimization problems where the gradients of the objective function are Lipschitz continuous but only exact function values are available. SSDFO employs line searches along directions constructed on the basis of quadratic models. These approximate the objective function in a subspace spanned by some previous search directions. A worst-case complexity bound on the number of iterations and function evaluations is derived for a basic algorithm using this technique. Numerical results for a practical variant with additional heuristic features show that, on the unconstrained CUTEst test problems, SSDFO has superior performance compared to the best solvers from the literature.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",complexity; line search approach; subspace technique; Unconstrained derivative-free optimization,Complexity; Derivative-free optimization; Efficient subspace method; Line search approach; Line searches; Objective functions; Optimization problems; Subspace method; Subspace techniques; Unconstrained derivative-free optimization; Optimization
Efficient Implementation of Modern Entropy Stable and Kinetic Energy Preserving Discontinuous Galerkin Methods for Conservation Laws,2023,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85177757626&doi=10.1145%2f3625559&partnerID=40&md5=37faab535e54d828ee108d15e4b44014,"Many modern discontinuous Galerkin (DG) methods for conservation laws make use of summation by parts operators and flux differencing to achieve kinetic energy preservation or entropy stability. While these techniques increase the robustness of DG methods significantly, they are also computationally more demanding than standard weak form nodal DG methods. We present several implementation techniques to improve the efficiency of flux differencing DG methods that use tensor product quadrilateral or hexahedral elements, in 2D or 3D, respectively. Focus is mostly given to CPUs and DG methods for the compressible Euler equations, although these techniques are generally also useful for other physical systems, including the compressible Navier-Stokes and magnetohydrodynamics equations. We present results using two open source codes, Trixi.jl written in Julia and FLUXO written in Fortran, to demonstrate that our proposed implementation techniques are applicable to different code bases and programming languages.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",conservation laws; discontinuous Galerkin; entropy stability; Flux differencing; summation by parts,Entropy; Galerkin methods; Kinetics; Magnetohydrodynamics; Navier Stokes equations; Open source software; Open systems; Physical properties; Program processors; Conservation law; Discontinous Galerkin methods; Discontinuous galerkin; Efficient implementation; Entropy stabilities; Flux differencing; Implementation techniques; Summation by parts; Summation-by-parts operators; Weak form; Kinetic energy
HAZniCS - Software Components for Multiphysics Problems,2023,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181742443&doi=10.1145%2f3625561&partnerID=40&md5=9a80fa0041b1d02a84fd5598bb4fe574,"We introduce the software toolbox HAZniCS for solving interface-coupled multiphysics problems. HAZniCS is a suite of modules that combines the well-known FEniCS framework for finite element discretization with solver and graph library HAZmath. The focus of this article is on the design and implementation of robust and efficient solver algorithms which tackle issues related to the complex interfacial coupling of the physical problems often encountered in applications in brain biomechanics. The robustness and efficiency of the numerical algorithms and methods is shown in several numerical examples, namely the Darcy-Stokes equations that model the flow of cerebrospinal fluid in the human brain and the mixed-dimensional model of electrodiffusion in the brain tissue.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",FEniCS project; HAZmath; Mathematical software; multiphysics; partial differential equations; preconditioning,Brain; Cerebrospinal fluid; Flow of fluids; Numerical methods; Design and implementations; FEniCS project; Finite-element discretization; Hazmath; Mathematical software; Multi-physics; Multiphysics problems; Preconditioning; Software toolbox; Software-component; Multiphysics
Emgr - EMpirical GRamian Framework Version 5.99,2023,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173577668&doi=10.1145%2f3609860&partnerID=40&md5=f93b7673b03588fac8ed12491fffc097,"Version 5.99 of the empirical Gramian framework - emgr - completes a development cycle which focused on parametric model order reduction of gas network models while preserving compatibility to the previous development for the application of combined state and parameter reduction for neuroscience network models. Second, new features concerning empirical Gramian types, perturbation design, and trajectory post-processing, as well as a Python version in addition to the default MATLAB / Octave implementation, have been added. This work summarizes these changes, particularly since emgr version 5.4, see Himpe, 2018 [Algorithms 11(7): 91], and gives recent as well as future applications, such as parameter identification in systems biology, based on the current feature set. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesControl theory; empirical gramians; nonlinear systems; system gramians; system theory,Additional key word and phrasescontrol theory; Development cycle; Empirical gramian; Gramians; Key words; Model order reduction; Network models; Parametric models; System gramian; Python
Improvements to SLEPc in Releases 3.14-3.18,2023,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173609383&doi=10.1145%2f3603373&partnerID=40&md5=479c5aec2be38da1c82b5fee86ff4d73,"This short article describes the main new features added to SLEPc, the Scalable Library for Eigenvalue Problem Computations, in the past two and a half years, corresponding to five release versions. The main novelty is the extension of the SVD module with new problem types, such as the generalized SVD or the hyperbolic SVD. Additionally, many improvements have been incorporated in different parts of the library, including contour integral eigensolvers, preconditioning, and GPU support.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesEigenvalue computations; message-passing parallelization; SLEPc,Message passing; Additional key word and phraseseigenvalue computation; Contour integrals; Eigensolvers; Eigenvalue problem; Hyperbolic SVD; Key words; Message-passing parallelization; SLEPc; Eigenvalues and eigenfunctions
IFISS3D: A Computational Laboratory for Investigating Finite Element Approximation in Three Dimensions,2023,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173611110&doi=10.1145%2f3604934&partnerID=40&md5=47e2ffbdca7f18383c187ecee4752438,"IFISS is an established MATLAB finite element software package for studying strategies for solving partial differential equations (PDEs). IFISS3D is a new add-on toolbox that extends IFISS capabilities for elliptic PDEs from two to three space dimensions. The open-source MATLAB framework provides a computational laboratory for experimentation and exploration of finite element approximation and error estimation, as well as iterative solvers. The package is designed to be useful as a teaching tool for instructors and students who want to learn about state-of-the-art finite element methodology. It will also be useful for researchers as a source of reproducible test matrices of arbitrarily large dimension.  © 2023 Copyright held by the owner/author(s).",Additional Key Words and PhrasesFinite elements; algebraic multigrid; iterative solvers; mathematical software; MATLAB,Finite element method; Iterative methods; Open source software; Additional key word and phrasesfinite element; Algebraic multigrids; Elliptic partial differential equation; Finite element approximations; Finite element software; Iterative solvers; Key words; Mathematical software; Three dimensions; Three space dimensions; MATLAB
Array-Aware Matching: Taming the Complexity of Large-Scale Simulation Models,2023,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173604783&doi=10.1145%2f3611661&partnerID=40&md5=34d4e6769b2dd1dd5dba0491aee525b6,"Equation-based modelling is a powerful approach to tame the complexity of large-scale simulation problems. Equation-based tools automatically translate models into imperative languages. When confronted with nowadays' problems, however, well assessed model translation techniques exhibit scalability issues that are particularly severe when models contain very large arrays. In fact, such models can be made very compact by enclosing equations into looping constructs, but reflecting the same compactness into the translated imperative code is nontrivial. In this paper, we face this issue by concentrating on a key step of equations-to-code translation, the equation/variable matching. We first show that an efficient translation of models with (large) arrays needs awareness of their presence, by defining a figure of merit to measure how much the looping constructs are preserved along the translation. We then show that the said figure of merit allows to define an optimal array-aware matching, and as our main result, that the so stated optimal array-aware matching problem is NP-complete. As an additional result, we propose a heuristic algorithm capable of performing array-aware matching in polynomial time. The proposed algorithm can be proficiently used by model translator developers in the implementation of efficient tools for large-scale system simulation.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesEquation-based modeling languages; array-aware matching; NP-completeness proof,Codes (symbols); Heuristic algorithms; Large scale systems; Optimization; Polynomial approximation; Additional key word and phrasesequation-based modeling language; Array-aware matching; Based modelling; Equation-based models; Key words; Large scale simulations; Matchings; NP-completeness proof; Optimal arrays; Simulation model; Modeling languages
"Cache Optimization and Performance Modeling of Batched, Small, and Rectangular Matrix Multiplication on Intel, AMD, and Fujitsu Processors",2023,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173605478&doi=10.1145%2f3595178&partnerID=40&md5=879e87603c1ae6a0affa09e0ae68e805,"Factorization and multiplication of dense matrices and tensors are critical, yet extremely expensive pieces of the scientific toolbox. Careful use of low rank approximation can drastically reduce the computation and memory requirements of these operations. In addition to a lower arithmetic complexity, such methods can, by their structure, be designed to efficiently exploit modern hardware architectures. The majority of existing work relies on batched BLAS libraries to handle the computation of many small dense matrices. We show that through careful analysis of the cache utilization, register accumulation using SIMD registers and a redesign of the implementation, one can achieve significantly higher throughput for these types of batched low-rank matrices across a large range of block and batch sizes. We test our algorithm on three CPUs using diverse ISAs - the Fujitsu A64FX using ARM SVE, the Intel Xeon 6148 using AVX-512, and AMD EPYC 7502 using AVX-2, and show that our new batching methodology is able to obtain more than twice the throughput of vendor optimized libraries for all CPU architectures and problem sizes.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesLow-rank matrix multiplication; batched matrix multiplication; cache blocking; performance modeling,Approximation theory; Libraries; Matrix algebra; Additional key word and phraseslow-rank matrix multiplication; Batched matrix multiplication; Cache blocking; Cache optimization; Dense matrix; Fujitsu; Key words; MAtrix multiplication; Performance Modeling; Program processors
Algorithms for Parallel Generic hp-Adaptive Finite Element Software,2023,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173606841&doi=10.1145%2f3603372&partnerID=40&md5=9579546c4d6c0811c52c205d2b3c1ec2,"The hp-adaptive finite element method - where one independently chooses the mesh size (h) and polynomial degree (p) to be used on each cell - has long been known to have better theoretical convergence properties than either h- or p-adaptive methods alone. However, it is not widely used, owing at least in part to the difficulty of the underlying algorithms and the lack of widely usable implementations. This is particularly true when used with continuous finite elements.Herein, we discuss algorithms that are necessary for a comprehensive and generic implementation of hp-adaptive finite element methods on distributed-memory, parallel machines. In particular, we will present a multistage algorithm for the unique enumeration of degrees of freedom suitable for continuous finite element spaces, describe considerations for weighted load balancing, and discuss the transfer of variable size data between processes. We illustrate the performance of our algorithms with numerical examples and demonstrate that they scale reasonably up to at least 16,384 message passage interface processes.We provide a reference implementation of our algorithms as part of the open source library deal.II.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesParallel algorithms; finite element methods; high performance computing; hp-adaptivity,Balancing; Degrees of freedom (mechanics); Open source software; Adaptive finite element; Adaptive finite element methods; Additional key word and phrasesparallel algorithm; Finite element software; High performance computing; Hp adaptivity; Key words; Mesh size; Performance computing; Polynomial degree; Finite element method
Approximating Inverse Cumulative Distribution Functions to Produce Approximate Random Variables,2023,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173618882&doi=10.1145%2f3604935&partnerID=40&md5=65c1d2f4edb4f9effd7557a93b422c7e,"For random variables produced through the inverse transform method, approximate random variables are introduced, which are produced using approximations to a distribution's inverse cumulative distribution function. These approximations are designed to be computationally inexpensive and much cheaper than library functions, which are exact to within machine precision and, thus, highly suitable for use in Monte Carlo simulations. The approximation errors they introduce can then be eliminated through use of the multilevel Monte Carlo method. Two approximations are presented for the Gaussian distribution: a piecewise constant on equally spaced intervals and a piecewise linear using geometrically decaying intervals. The errors of the approximations are bounded and the convergence demonstrated, and the computational savings are measured for C and C++ implementations. Implementations tailored for Intel and Arm hardware are inspected alongside hardware agnostic implementations built using OpenMP. The savings are incorporated into a nested multilevel Monte Carlo framework with the Euler-Maruyama scheme to exploit the speedups without losing accuracy, offering speed ups by a factor of 5-7. These ideas are empirically extended to the Milstein scheme and the non-central χ2 distribution for the Cox-Ingersoll-Ross process, offering speedups of a factor of 250 or more.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesApproximations; and high-performance computing; geometric Brownian motion; inverse cumulative distribution functions; multilevel Monte Carlo; random number generation; random variables; the Cox-Ingersoll-Ross process; the Euler-Maruyama scheme; the Gaussian distribution; the Milstein scheme; the non-central χ<sup>2</sup>distribution,Application programming interfaces (API); Brownian movement; Distribution functions; Gaussian distribution; Intelligent systems; Inverse problems; Piecewise linear techniques; Random number generation; Random variables; Additional key word and phrasesapproximation; And high-performance computing; Euler-Maruyama scheme; Geometric Brownian motion; Inverse cumulative distribution functions; Key words; Multilevel monte carlo; Multilevels; Performance computing; Random-number generation; The cox-ingersoll-ross process; The euler-maruyamum scheme; The gaussian distribution; The milstein scheme; The non-central χ2distribution; Monte Carlo methods
Algorithm 1037: SuiteSparse:GraphBLAS: Parallel Graph Algorithms in the Language of Sparse Linear Algebra,2023,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173630190&doi=10.1145%2f3577195&partnerID=40&md5=ee1fd3b2699f80efe1643816dba04199,"SuiteSparse:GraphBLAS is a full parallel implementation of the GraphBLAS standard, which defines a set of sparse matrix operations on an extended algebra of semirings using an almost unlimited variety of operators and types. When applied to sparse adjacency matrices, these algebraic operations are equivalent to computations on graphs. A description of the parallel implementation of SuiteSparse:GraphBLAS is given, including its novel parallel algorithms for sparse matrix multiply, addition, element-wise multiply, submatrix extraction and assignment, and the GraphBLAS mask/accumulator operation. Its performance is illustrated by solving the graph problems in the GAP Benchmark and by comparing it with other sparse matrix libraries.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesGraph algorithms; GraphBLAS; sparse matrices,Graph theory; Matrix algebra; Additional key word and phrasesgraph algorithm; Adjacency matrix; Algebraic operations; GraphBLAS; Key words; Matrix operations; Parallel graph algorithms; Parallel implementations; Semi-ring; Sparse matrices; Benchmarking
Sparse Approximate Multifrontal Factorization with Composite Compression Methods,2023,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173580808&doi=10.1145%2f3611662&partnerID=40&md5=5e75a9b38ab9f319523f72ae998378a4,"This article presents a fast and approximate multifrontal solver for large sparse linear systems. In a recent work by Liu et al., we showed the efficiency of a multifrontal solver leveraging the butterfly algorithm and its hierarchical matrix extension, HODBF (hierarchical off-diagonal butterfly) compression to compress large frontal matrices. The resulting multifrontal solver can attain quasi-linear computation and memory complexity when applied to sparse linear systems arising from spatial discretization of high-frequency wave equations. To further reduce the overall number of operations and especially the factorization memory usage to scale to larger problem sizes, in this article we develop a composite multifrontal solver that employs the HODBF format for large-sized fronts, a reduced-memory version of the nonhierarchical block low-rank format for medium-sized fronts, and a lossy compression format for small-sized fronts. This allows us to solve sparse linear systems of dimension up to 2.7 × larger than before and leads to a memory consumption that is reduced by 70% while ensuring the same execution time. The code is made publicly available in GitHub. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesSparse direct solver; block low-rank compression;; butterfly algorithm; multifrontal method,Linear systems; Matrix algebra; Wave equations; Additional key word and phrasessparse direct solv; Block low-rank compression;; Butterfly algorithms; Compression methods; Direct solvers; Key words; Low rank compression; Multifrontal; Multifrontal methods; Sparse linear systems; Factorization
"Algorithm 1036: Atc, an advanced tucker compression library for multidimensional data",2023,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164260667&doi=10.1145%2f3585514&partnerID=40&md5=e1fa8ec1e6d3ad2a39fa75f9e384e5b2,"We present ATC, a C++ library for advanced Tucker-based lossy compression of dense multidimensional numerical data in a shared-memory parallel setting, based on the sequentially truncated higher-order singular value decomposition (ST-HOSVD) and bit plane truncation. Several techniques are proposed to improve speed, memory usage, error control and compression rate. First, a hybrid truncation scheme is described which combines Tucker rank truncation and TTHRESH quantization. We derive a novel expression to approximate the error of truncated Tucker decompositions in the case of core and factor perturbations. We parallelize the quantization and encoding scheme and adjust this phase to improve error control. Implementation aspects are described, such as an ST-HOSVD procedure using only a single transposition. We also discuss several usability features of ATC, including the presence of multiple interfaces, extensive data type support, and integrated downsampling of the decompressed data. Numerical results show that ATC maintains state-of-the-art Tucker compression rates while providing average speed-up factors of 2.2 to 3.5 and halving memory usage. Our compressor provides precise error control, deviating only 1.4% from the requested error on average. Finally, ATC often achieves higher compression than non-Tucker-based compressors in the high-error domain.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesData compression; bit plane truncation; ST-HOSVD; tensors; Tucker decomposition,Singular value decomposition; Additional key word and phrasesdata compression; Bit plane truncation; Bit planes; Compression rates; Error control; Key words; Memory usage; Multidimensional data; Sequentially truncated higher-order singular value decompositions; Tucker decompositions; Errors
Arkode: A flexible ivp solver infrastructure for one-step methods,2023,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160778870&doi=10.1145%2f3594632&partnerID=40&md5=f0e8e70248f9543115409047810252eb,"We describe the ARKODE library of one-step time integration methods for ordinary differential equation (ODE) initial-value problems (IVPs). In addition to providing standard explicit and diagonally implicit Runge-Kutta methods, ARKODE supports one-step methods designed to treat additive splittings of the IVP, including implicit-explicit (ImEx) additive Runge-Kutta methods and multirate infinitesimal (MRI) methods. We present the role of ARKODE within the SUNDIALS suite of time integration and nonlinear solver libraries, the core ARKODE infrastructure for utilities common to large classes of one-step methods, as well as its use of ""time stepper""modules enabling easy incorporation of novel algorithms into the library. Numerical results show example problems of increasing complexity, highlighting the algorithmic flexibility afforded through this infrastructure, and include a larger multiphysics application leveraging multiple algorithmic features from ARKODE and SUNDIALS. © 2023 Association for Computing Machinery.",adaptive integration; additive Runge-Kutta methods; ImEx methods; multirate methods; ODEs,Additives; Computational complexity; Integration; Multiphysics; Ordinary differential equations; Parallel processing systems; Runge Kutta methods; Adaptive integration; Additive Runge-Kutta method; Algorithmics; Implicit-explicit methods; Initial-value problem; Multirate method; ODE; One-step methods; One-step time integration; Time integration methods; Initial value problems
CPFloat: A C Library for Simulating Low-precision Arithmetic,2023,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153596846&doi=10.1145%2f3585515&partnerID=40&md5=bfa5b69644d099e8792f6631be279e2f,"One can simulate low-precision floating-point arithmetic via software by executing each arithmetic operation in hardware and then rounding the result to the desired number of significant bits. For IEEE-compliant formats, rounding requires only standard mathematical library functions, but handling subnormals, underflow, and overflow demands special attention, and numerical errors can cause mathematically correct formulae to behave incorrectly in finite arithmetic. Moreover, the ensuing implementations are not necessarily efficient, as the library functions these techniques build upon are typically designed to handle a broad range of cases and may not be optimized for the specific needs of rounding algorithms. CPFloat is a C library for simulating low-precision arithmetics. It offers efficient routines for rounding, performing mathematical computations, and querying properties of the simulated low-precision format. The software exploits the bit-level floating-point representation of the format in which the numbers are stored and replaces costly library calls with low-level bit manipulations and integer arithmetic. In numerical experiments, the new techniques bring a considerable speedup (typically one order of magnitude or more) over existing alternatives in C, C++, and MATLAB. To our knowledge, CPFloat is currently the most efficient and complete library for experimenting with custom low-precision floating-point arithmetic.  © 2023 Copyright held by the owner/author(s).",Additional Key Words and PhrasesLow-precision arithmetic; bfloat16; binary16; directed rounding; floating-point arithmetic; IEEE 754 standard; mixed precision; round-to-nearest; round-to-odd; stochastic rounding,C++ (programming language); Digital arithmetic; IEEE Standards; MATLAB; Additional key word and phraseslow-precision arithmetic; Bfloat16; Binary16; Directed rounding; Floating-point arithmetic; IEEE-754 standard; Key words; Mixed precision; Precision arithmetic; Round-to-near; Round-to-odd; Stochastic rounding; Stochastics; Stochastic systems
Truncated Log-concave Sampling for Convex Bodies with Reflective Hamiltonian Monte Carlo,2023,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164265730&doi=10.1145%2f3589505&partnerID=40&md5=64792d78499b553d2e5b39fea00f5148,"We introduce Reflective Hamiltonian Monte Carlo (ReHMC), an HMC-based algorithm to sample from a log-concave distribution restricted to a convex body. The random walk is based on incorporating reflections to the Hamiltonian dynamics such that the support of the target density is the convex body. We develop an efficient open source implementation of ReHMC and perform an experimental study on various high-dimensional datasets. The experiments suggest that ReHMC outperforms Hit-and-Run and Coordinate-Hit-and-Run regarding the time it needs to produce an independent sample, introducing practical truncated sampling in thousands of dimensions. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",experiments; geometric random walks; mixing time; Statistical software; truncated sampling,Hamiltonians; Monte Carlo methods; Open source software; Random processes; Convex body; Geometric random walk; Hamiltonian dynamics; Log concaves; Mixing time; Open source implementation; Random Walk; Statistical software; Target density; Truncated sampling; Sampling
Computing with B-series,2023,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164206303&doi=10.1145%2f3573384&partnerID=40&md5=e5fdf5bcb536a0404e18a3a2a0538d20,"We present BSeries.jl, a Julia package for the computation and manipulation of B-series, which are a versatile theoretical tool for understanding and designing discretizations of differential equations. We give a short introduction to the theory of B-series and associated concepts and provide examples of their use, including method composition and backward error analysis. The associated software is highly performant and makes it possible to work with B-series of high order. © 2023 Association for Computing Machinery.",B-series; backward error analysis; composition methods; discretization; Ordinary differential equations; rooted trees; Runge-Kutta methods,Computation theory; Error analysis; Runge Kutta methods; Associated softwares; B-series; Backward error analysis; Composition method; Discretizations; High-order; Higher-order; Rooted trees; Ordinary differential equations
Enabling research through the scip optimization suite 8.0,2023,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162150157&doi=10.1145%2f3585516&partnerID=40&md5=3ae9e9ca206bb275b230bd43e4c952d7,"The SCIP Optimization Suite provides a collection of software packages for mathematical optimization centered around the constraint integer programming framework SCIP. The focus of this article is on the role of the SCIP Optimization Suite in supporting research. SCIP's main design principles are discussed, followed by a presentation of the latest performance improvements and developments in version 8.0, which serve both as examples of SCIP's application as a research tool and as a platform for further developments. Furthermore, this article gives an overview of interfaces to other programming and modeling languages, new features that expand the possibilities for user interaction with the framework, and the latest developments in several extensions built upon SCIP. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",branch-and-cut; branch-and-price; column generation; Constraint integer programming; linear programming; mixed-integer linear programming; mixed-integer nonlinear programming; mixed-integer semidefinite programming; optimization solver; parallelization,Constraint programming; Linear programming; Modeling languages; Nonlinear programming; User interfaces; Branch and price; Branch-and-cut; Column generation; Constraint integer programming; Integer Linear Programming; Integer Program- ming; Linear-programming; Mixed integer; Mixed integer linear; Mixed-integer linear programming; Mixed-integer nonlinear programming; Mixed-integer semidefinite programming; Optimization solvers; Parallelizations; Semi-definite programming; Integer programming
Distributed H2-Matrices for Boundary Element Methods,2023,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164146680&doi=10.1145%2f3582494&partnerID=40&md5=be4361a90fda436255b1f88b7c54f150,"Standard discretization techniques for boundary integral equations, e.g., the Galerkin boundary element method, lead to large densely populated matrices that require fast and efficient compression techniques like the fast multipole method or hierarchical matrices. If the underlying mesh is very large, running the corresponding algorithms on a distributed computer is attractive, e.g., since distributed computers frequently are cost-effective and offer a high accumulated memory bandwidth.Compared to the closely related particle methods, for which distributed algorithms are well-established, the Galerkin discretization poses a challenge, since the supports of the basis functions influence the block structure of the matrix and therefore the flow of data in the corresponding algorithms. This article introduces distributed ĝ.2-matrices, a class of hierarchical matrices that is closely related to fast multipole methods and particularly well-suited for distributed computing. While earlier efforts required the global tree structure of the ĝ.2-matrix to be stored in every node of the distributed system, the new approach needs only local multilevel information that can be obtained via a simple distributed algorithm, allowing us to scale to significantly larger systems. Experiments show that this approach can handle very large meshes with more than 130 million triangles efficiently. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",boundary element method; Distributed computing; hierarchical matrix,Boundary integral equations; Cost effectiveness; Distributed computer systems; Galerkin methods; Matrix algebra; Sailing vessels; Trees (mathematics); Boundary-element methods; Compression techniques; Cost effective; Discretizations; Fast-multipole methods; Galerkin boundary element method; Hierarchical matrix; IS costs; matrix; Memory bandwidths; Boundary element method
Task-based parallel programming for scalable matrix product algorithms,2023,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164270445&doi=10.1145%2f3583560&partnerID=40&md5=515dba96447d60def44d14f59f668be1,"Task-based programming models have succeeded in gaining the interest of the high-performance mathematical software community because they relieve part of the burden of developing and implementing distributed-memory parallel algorithms in an efficient and portable way.In increasingly larger, more heterogeneous clusters of computers, these models appear as a way to maintain and enhance more complex algorithms. However, task-based programming models lack the flexibility and the features that are necessary to express in an elegant and compact way scalable algorithms that rely on advanced communication patterns. We show that the Sequential Task Flow paradigm can be extended to write compact yet efficient and scalable routines for linear algebra computations. Although, this work focuses on dense General Matrix Multiplication, the proposed features enable the implementation of more complex algorithms. We describe the implementation of these features and of the resulting GEMM operation. Finally, we present an experimental analysis on two homogeneous supercomputers showing that our approach is competitive up to 32,768 CPU cores with state-of-the-art libraries and may outperform them for some problem dimensions. Although our code can use GPUs straightforwardly, we do not deal with this case because it implies other issues which are out of the scope of this work.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesParallel programming models; distributed memory parallelism; runtime systems; scalable linear algebra algorithms; sequential task flow,Clustering algorithms; Matrix algebra; Memory architecture; Program processors; Supercomputers; Additional key word and phrasesparallel programming model; Distributed-memory parallelisms; Key words; Linear algebra algorithms; Programming models; Run- time systems; Scalable linear algebra algorithm; Sequential task; Sequential task flow; Task flows; Parallel programming
Algorithm 1035: A Gradient-based Implementation of the Polyhedral Active Set Algorithm,2023,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160074254&doi=10.1145%2f3583559&partnerID=40&md5=37e3943d1f58c6719652919fa1ae7ea5,"The Polyhedral Active Set Algorithm (PASA) is designed to optimize a general nonlinear function over a polyhedron. Phase one of the algorithm is a nonmonotone gradient projection algorithm, while phase two is an active set algorithm that explores faces of the constraint polyhedron. A gradient-based implementation is presented, where a projected version of the conjugate gradient algorithm is employed in phase two. Asymptotically, only phase two is performed. Comparisons are given with IPOPT using polyhedral-constrained problems from CUTEst and the Maros/Meszaros quadratic programming test set. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",active set method; CG-DESCENT; conjugate gradient method; gradient projection method; NAPHEAP; Nonlinear optimization; PASA; polyhedral-constrained optimization; PPROJ; projection on polyhedron,Conjugate gradient method; Geometry; Quadratic programming; Active-Set algorithms; Active-set methods; CG-DESCENT; Conjugate-gradient method; Gradient projection methods; NAPHEAP; Non-linear optimization; Polyhedral active set algorithm; Polyhedral-constrained optimization; PPROJ; Projection on polyhedron; Constrained optimization
HIPPYlib-MUQ: A Bayesian Inference Software Framework for Integration of Data with Complex Predictive Models under Uncertainty,2023,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150220716&doi=10.1145%2f3580278&partnerID=40&md5=b86641a2e604cc1c6061b8bf6d6533de,"Bayesian inference provides a systematic framework for integration of data with mathematical models to quantify the uncertainty in the solution of the inverse problem. However, the solution of Bayesian inverse problems governed by complex forward models described by partial differential equations (PDEs) remains prohibitive with black-box Markov chain Monte Carlo (MCMC) methods. We present hIPPYlib-MUQ, an extensible and scalable software framework that contains implementations of state-of-The art algorithms aimed to overcome the challenges of high-dimensional, PDE-constrained Bayesian inverse problems. These algorithms accelerate MCMC sampling by exploiting the geometry and intrinsic low-dimensionality of parameter space via derivative information and low rank approximation. The software integrates two complementary open-source software packages, hIPPYlib and MUQ. hIPPYlib solves PDE-constrained inverse problems using automatically-generated adjoint-based derivatives, but it lacks full Bayesian capabilities. MUQ provides a spectrum of powerful Bayesian inversion models and algorithms, but expects forward models to come equipped with gradients and Hessians to permit large-scale solution. By combining these two complementary libraries, we created a robust, scalable, and efficient software framework that realizes the benefits of each and allows us to tackle complex large-scale Bayesian inverse problems across a broad spectrum of scientific and engineering disciplines. To illustrate the capabilities of hIPPYlib-MUQ, we present a comparison of a number of MCMC methods available in the integrated software on several high-dimensional Bayesian inverse problems. These include problems characterized by both linear and nonlinear PDEs, various noise models, and different parameter dimensions. The results demonstrate that large (∼50×) speedups over conventional black box and gradient-based MCMC algorithms can be obtained by exploiting Hessian information (from the log-posterior), underscoring the power of the integrated hIPPYlib-MUQ framework. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",adjoint-based methods; Bayesian inference; generic PDE toolkit; inexact Newton-CG method; Infinite-dimensional inverse problems; low-rank approximation; sampling; uncertainty quantification,Approximation algorithms; Approximation theory; Bayesian networks; Computer programming; Data integration; Differential equations; Inference engines; Markov processes; Monte Carlo methods; Open source software; Open systems; Adjoint based method; Bayesian inference; CG methods; Generic partial differential equation toolkit; Inexact newton-CG method; Inexact newtons; Infinite dimensional; Infinite-dimensional inverse problem; Low rank approximations; Uncertainty quantifications; Inverse problems
FastSpline: Automatic Generation of Interpolants for Lattice Samplings,2023,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153712364&doi=10.1145%2f3577194&partnerID=40&md5=dde1898c276832d42426bf53a00d358c,"Interpolation is a foundational concept in scientific computing and is at the heart of many scientific visualization techniques. There is usually a tradeoff between the approximation capabilities of an interpolation scheme and its evaluation efficiency. For many applications, it is important for a user to navigate their data in real time. In practice, evaluation efficiency outweighs any incremental improvements in reconstruction fidelity. We first analyze, from a general standpoint, the use of compact piece-wise polynomial basis functions to efficiently interpolate data that is sampled on a lattice. We then detail our automatic code-generation framework on both CPU and GPU architectures. Specifically, we propose a general framework that can produce a fast evaluation scheme by analyzing the algebro-geometric structure of the convolution sum for a given lattice and basis function combination. We demonstrate the utility and generality of our framework by providing fast implementations of various box splines on the Body Centered and Face Centered Cubic lattices, as well as some non-separable box splines on the Cartesian lattice. We also provide fast implementations for certain Voronoi-splines that have not yet appeared in the literature. Finally, we demonstrate that this framework may also be used for non-Cartesian lattices in 4D. © 2023 Association for Computing Machinery.",Interpolation; signal processing; volumetric rendering,Automatic programming; Efficiency; Function evaluation; Rendering (computer graphics); Signal processing; Approximation capabilities; Automatic Generation; Box splines; Cartesian lattices; Fast implementation; Interpolants; Interpolation schemes; Signal-processing; Visualization technique; Volumetric rendering; Interpolation
Algorithm 1033: Parallel Implementations for Computing the Minimum Distance of a Random Linear Code on Distributed-memory Architectures,2023,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151557584&doi=10.1145%2f3573383&partnerID=40&md5=5b16d0ff09464c2589068a2c2a79f1ba,"The minimum distance of a linear code is a key concept in information theory. Therefore, the time required by its computation is very important to many problems in this area. In this article, we introduce a family of implementations of the Brouwer-Zimmermann algorithm for distributed-memory architectures for computing the minimum distance of a random linear code over 2. Both current commercial and public-domain software only work on either unicore architectures or shared-memory architectures, which are limited in the number of cores/processors employed in the computation. Our implementations focus on distributed-memory architectures, thus being able to employ hundreds or even thousands of cores in the computation of the minimum distance. Our experimental results show that our implementations are much faster, even up to several orders of magnitude, than current implementations widely used nowadays. © 2023 Association for Computing Machinery.",Additional Key Words and PhrasesInformation theory; distributed-memory; linear codes; minimum distance,Computation theory; Information theory; Parallel architectures; 'current; Additional key word and phrasesinformation theory; Distributed Memory; Distributed memory architecture; Key words; Linear codes; Minimums distance; Parallel implementations; Random linear codes; Shared memory architecture; Memory architecture
Certifying Zeros of Polynomial Systems Using Interval Arithmetic,2023,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151564023&doi=10.1145%2f3580277&partnerID=40&md5=9c8e98b6ef1dc09ee8db0fe4130e2ad2,"We establish interval arithmetic as a practical tool for certification in numerical algebraic geometry. Our software HomotopyContinuation.jl now has a built-in function certify, which proves the correctness of an isolated nonsingular solution to a square system of polynomial equations. The implementation rests on Krawczyk's method. We demonstrate that it dramatically outperforms earlier approaches to certification. We see this contribution as a powerful new tool in numerical algebraic geometry, which can make certification the default and not just an option. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesDatasets; gaze detection; neural networks; text tagging,Geometry; Additional key word and phrasesdataset; Built-in functions; Gaze detection; Interval arithmetic; Key words; Neural-networks; Numerical algebraic geometry; Polynomial systems; Text tagging; Zeros of polynomial; Polynomials
Accurate Calculation of Euclidean Norms Using Double-word Arithmetic,2023,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151557797&doi=10.1145%2f3568672&partnerID=40&md5=fd5f3f678aaf5db472e28203c791954b,"We consider the computation of the Euclidean (or L2) norm of an n-dimensional vector in floating-point arithmetic. We review the classical solutions used to avoid spurious overflow or underflow and/or to obtain very accurate results. We modify a recently published algorithm (that uses double-word arithmetic) to allow for a very accurate solution, free of spurious overflows and underflows. To that purpose, we use a double-word square-root algorithm of which we provide a tight error analysis. The returned L2 norm will be within very slightly more than 0.5 ulp from the exact result, which means that we will almost always provide correct rounding. © 2023 Association for Computing Machinery.",Additional Key Words and PhrasesFloating-point arithmetic; Coq; double-double arithmetic; double-word arithmetic; Euclidean norms; formalization; overflow; proof assistant; square-root; underflow,Theorem proving; Additional key word and phrasesfloating-point arithmetic; Coq; Double-double arithmetic; Double-word arithmetic; Euclidean norm; Formalisation; Key words; Overflow; Point arithmetic; Proof assistant; Square-root; Underflows; Digital arithmetic
"Algorithm 1034: An Accelerated Algorithm to Compute the QnRobust Statistic, with Corrections to Constants",2023,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151554797&doi=10.1145%2f3576920&partnerID=40&md5=cd43cb10665b030eadfd69b902bca3d2,"The robust scale estimator Qn developed by Croux and Rousseeuw [3], for the computation of which they provided a deterministic algorithm, has proven to be very useful in several domains including in quality management and time series analysis. It has interesting mathematical (50% breakdown, 82% Asymptotic Relative Efficiency) and computing (O(nlogn) time, O(n) space) properties. While working on a faster algorithm to compute Qn, we have discovered an error in the computation of the d constant, and as a consequence in the dn constants that are used to scale the statistic for consistency with the variance of a normal sample. These errors have been reproduced in several articles including in the International Standard Organisation 13,528 [12] document. In this article, we fix the errors and present a new approach, which includes a new algorithm, allowing computations to run 1.3 to 4.5 times faster when n grows from 10 to 100,000. © 2023 Association for Computing Machinery.",Additional Key Words and PhrasesRobust statistics; Q<sub>n</sub>; robustness; scale estimator,Quality management; Time series analysis; Additional key word and phrasesrobust statistic; Asymptotic relative efficiency; Deterministic algorithms; Key words; Property; Qn; Robust statistics; Robustness; Scale estimator; Time-series analysis; Errors
Algorithm 1032: Bi-cubic Splines for Polyhedral Control Nets,2023,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151535836&doi=10.1145%2f3570158&partnerID=40&md5=c85c3d5a9b5b7ba8e337fb545e5389da,"For control nets outlining a large class of topological polyhedra, not just tensor-product grids, bi-cubic polyhedral splines form a piecewise polynomial, first-order differentiable space that associates one function with each vertex. Akin to tensor-product splines, the resulting smooth surface approximates the polyhedron. Admissible polyhedral control nets consist of quadrilateral faces in a grid-like layout, star-configuration where n ≠ 4 quadrilateral faces join around an interior vertex, n-gon configurations, where 2n quadrilaterals surround an n-gon, polar configurations where a cone of n triangles meeting at a vertex is surrounded by a ribbon of n quadrilaterals, and three types of T-junctions where two quad-strips merge into one. The bi-cubic pieces of a polyhedral spline have matching derivatives along their break lines, possibly after a known change of variables. The pieces are represented in Bernstein-Bézier form with coefficients depending linearly on the polyhedral control net, so that evaluation, differentiation, integration, moments, and so on, are no more costly than for standard tensor-product splines. Bi-cubic polyhedral splines can be used both to model geometry and for computing functions on the geometry. Although polyhedral splines do not offer nested refinement by refinement of the control net, polyhedral splines support engineering analysis of curved smooth objects. Coarse nets typically suffice since the splines efficiently model curved features. Algorithm 1032 is a C++ library with input-output example pairs and an IGES output choice. © 2023 Association for Computing Machinery.",C1 spline; extraordinary point; free-form surface; functions on manifolds; isogeometric analysis; n-sided face; polar layout; Polyhedral spline; T-junction,C++ (programming language); Tensors; C1 spline; Extraordinary point; Free-form surface; Function on manifold; Isogeometric analysis; N-sided face; Polar layout; Polyhedral spline; T junctions; Tensor-product splines; Geometry
Computational Graphs for Matrix Functions,2023,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151501445&doi=10.1145%2f3568991&partnerID=40&md5=a051bec6c8b90848844150973529e06e,"Many numerical methods for evaluating matrix functions can be naturally viewed as computational graphs. Rephrasing these methods as directed acyclic graphs (DAGs) is a particularly effective approach to study existing techniques, improve them, and eventually derive new ones. The accuracy of these matrix techniques can be characterized by the accuracy of their scalar counterparts, thus designing algorithms for matrix functions can be regarded as a scalar-valued optimization problem. The derivatives needed during the optimization can be calculated automatically by exploiting the structure of the DAG in a fashion analogous to backpropagation. This article describes GraphMatFun.jl, a Julia package that offers the means to generate and manipulate computational graphs, optimize their coefficients, and generate Julia, MATLAB, and C code to evaluate them efficiently at a matrix argument. The software also provides tools to estimate the accuracy of a graph-based algorithm and thus obtain numerically reliable methods. For the exponential, for example, using a particular form (degree-optimal) of polynomials produces implementations that in many cases are cheaper, in terms of computational cost, than the Padé-based techniques typically used in mathematical software. The optimized graphs and the corresponding generated code are available online. © 2023 Association for Computing Machinery.",Additional Key Words and PhrasesPolynomials of matrices; computational graphs; functions of matrices,C (programming language); Directed graphs; Function evaluation; Graphic methods; Matrix algebra; Numerical methods; Acyclic graphs; Additional key word and phrasespolynomial of matrix; Computational graph; Effective approaches; Function of matrix; Key words; matrix; Matrix functions; Matrix technique; MATLAB
Algorithm 1031: MQSI - Monotone Quintic Spline Interpolation,2023,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151555535&doi=10.1145%2f3570157&partnerID=40&md5=ad9a46e1fcebb32a2b5a947db035c7b6,"MQSI is a Fortran 2003 subroutine for constructing monotone quintic spline interpolants to univariate monotone data. Using sharp theoretical monotonicity constraints, first and second derivative estimates at data provided by a quadratic facet model are refined to produce a univariate C2 monotone interpolant. Algorithm and implementation details, complexity and sensitivity analyses, usage information, a brief performance study, and comparisons with other spline approaches are included. © 2023 Association for Computing Machinery.",Additional Key Words and PhrasesSoftware; B-spline; interpolation; quintic spline; shape-preserving; univariate,Sensitivity analysis; Additional key word and phrasessoftware; B splines; First derivative; Interpolants; Key words; Monotonicity constraint; Quintic spline; Shape-preserving; Spline interpolation; Univariate; Interpolation
Robust Topological Construction of All-hexahedral Boundary Layer Meshes,2023,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151547242&doi=10.1145%2f3577196&partnerID=40&md5=1bef877ac267bccb67ede69eed9020ff,"We present a robust technique to build a topologically optimal all-hexahedral layer on the boundary of a model with arbitrarily complex ridges and corners. The generated boundary layer mesh strictly respects the geometry of the input surface mesh, and it is optimal in the sense that the hexahedral valences of the boundary edges are as close as possible to their ideal values (local dihedral angle divided by 90°). Starting from a valid watertight surface mesh (all-quad in practice), we build a global optimization integer programming problem to minimize the mismatch between the hexahedral valences of the boundary edges and their ideal values. The formulation of the integer programming problem relies on the duality between boundary hexahedral configurations and triangulations of the disk, which we reframe in terms of integer constraints. The global problem is solved efficiently by performing combinatorial branch-and-bound searches on a series of sub-problems defined in the vicinity of complicated ridges/corners, where the local mesh topology is necessarily irregular because of the inherent constraints in hexahedral meshes. From the integer solution, we build the topology of the all-hexahedral layer, and the mesh geometry is computed by untangling/smoothing. Our approach is fully automated, topologically robust, and fast. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesHexahedral meshing; boundary layer; hex-dominant meshing; mesh generation,Boundary layers; Dihedral angle; Global optimization; Integer programming; Topology; Additional key word and phraseshexahedral meshing; Boundary edges; Hex-dominant meshing; Ideal values; Input surfaces; Integer programming problems; Key words; Robust technique; Surface mesh; Topological constructions; Mesh generation
Combining Sparse Approximate Factorizations with Mixed-precision Iterative Refinement,2023,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151803258&doi=10.1145%2f3582493&partnerID=40&md5=e6a9f90d22e7be2d85ceb000f7103e32,"The standard LU factorization-based solution process for linear systems can be enhanced in speed or accuracy by employing mixed-precision iterative refinement. Most recent work has focused on dense systems. We investigate the potential of mixed-precision iterative refinement to enhance methods for sparse systems based on approximate sparse factorizations. In doing so, we first develop a new error analysis for LU- and GMRES-based iterative refinement under a general model of LU factorization that accounts for the approximation methods typically used by modern sparse solvers, such as low-rank approximations or relaxed pivoting strategies. We then provide a detailed performance analysis of both the execution time and memory consumption of different algorithms, based on a selected set of iterative refinement variants and approximate sparse factorizations. Our performance study uses the multifrontal solver MUMPS, which can exploit block low-rank factorization and static pivoting. We evaluate the performance of the algorithms on large, sparse problems coming from a variety of real-life and industrial applications showing that mixed-precision iterative refinement combined with approximate sparse factorization can lead to considerable reductions of both the time and memory consumption.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesIterative refinement; floating-point arithmetic; GMRES; linear system; mixed precision; multifrontal method; multiple precision; parallelism; preconditioning; rounding error analysis; sparse direct solver,Approximation theory; Digital arithmetic; Error analysis; Factorization; Iterative methods; Lower-upper decomposition; Additional key word and phrasesiterative refinement; Floating-point arithmetic; GMRES; Key words; Mixed precision; Multifrontal methods; Multiple precision; Parallelism; Preconditioning; Rounding error analysis; Sparse direct solver; Linear systems
A Geometric Multigrid Method for Space-Time Finite Element Discretizations of the Navier-Stokes Equations and its Application to 3D Flow Simulation,2023,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151816589&doi=10.1145%2f3582492&partnerID=40&md5=10ab4f4c5f534a33ad5ca5b82e093467,"We present a parallelized geometric multigrid (GMG) method, based on the cell-based Vanka smoother, for higher order space-time finite element methods (STFEM) to the incompressible Navier-Stokes equations. The STFEM is implemented as a time marching scheme. The GMG solver is applied as a preconditioner for generalized minimal residual iterations. Its performance properties are demonstrated for 2D and 3D benchmarks of flow around a cylinder. The key ingredients of the GMG approach are the construction of the local Vanka smoother over all degrees of freedom in time of the respective subinterval and its efficient application. For this, data structures that store pre-computed cell inverses of the Jacobian for all hierarchical levels and require only a reasonable amount of memory overhead are generated. The GMG method is built for the deal.II finite element library. The concepts are flexible and can be transferred to similar software platforms.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesNavier-Stokes equations; geometric multigrid method; higher order space-time FEM; local Vanka smoother; Newton's method; parallel algorithms,Benchmarking; Degrees of freedom (mechanics); Finite element method; Geometry; Incompressible flow; Navier Stokes equations; Viscous flow; Additional key word and phrasesnavi-stoke equation; Geometric multigrid methods; High order space-time FEM; High-order; Higher-order; Key words; Local vanka smooth; Newton's methods; Spacetime; Stokes equations; Newton-Raphson method
"Algorithm 1029: Encapsulated Error, a Direct Approach to Evaluate Floating-Point Accuracy",2023,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151559415&doi=10.1145%2f3549205&partnerID=40&md5=9411ce5682dac237c893432f38904ad7,"Floating-point numbers represent only a subset of real numbers. As such, floating-point arithmetic introduces approximations that can compound and have a significant impact on numerical simulations. We introduce encapsulated error, a new way to estimate the numerical error of an application and provide a reference implementation, the Shaman library. Our method uses dedicated arithmetic over a type that encapsulates both the result the user would have had with the original computation and an approximation of its numerical error. We thus can measure the number of significant digits of any result or intermediate result in a simulation. We show that this approach, although simple, gives results competitive with state-of-the-art methods. It has a smaller overhead, and it is compatible with parallelism, making it suitable for the study of large-scale applications. © 2023 Copyright held by the owner/author(s).",Floating-point arithmetic; numerical verification; round-off errors,Errors; Numerical methods; Direct approach; Floating point numbers; Floating-point accuracies; Floating-point arithmetic; Numerical errors; Numerical verification; Real number; Reference implementation; Round-off errors; Significant digits; Digital arithmetic
Newly Released Capabilities in the Distributed-Memory SuperLU Sparse Direct Solver,2023,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151540354&doi=10.1145%2f3577197&partnerID=40&md5=92bbe16f1f7d4ac4ee2d0b5143af15c9,"We present the new features available in the recent release of SuperLU_DIST, Version 8.1.1. SuperLU_DIST is a distributed-memory parallel sparse direct solver. The new features include (1) a 3D communication-avoiding algorithm framework that trades off inter-process communication for selective memory duplication, (2) multi-GPU support for both NVIDIA GPUs and AMD GPUs, and (3) mixed-precision routines that perform single-precision LU factorization and double-precision iterative refinement. Apart from the algorithm improvements, we also modernized the software build system to use CMake and Spack package installation tools to simplify the installation procedure. Throughout the article, we describe in detail the pertinent performance-sensitive parameters associated with each new algorithmic feature, show how they are exposed to the users, and give general guidance of how to set these parameters. We illustrate that the solver's performance both in time and memory can be greatly improved after systematic tuning of the parameters, depending on the input sparse matrix and underlying hardware. © 2023 Association for Computing Machinery.",Additional Key Words and PhrasesSparse direct solver; communication-avoiding; GPU; mixed precision,Iterative methods; Lower-upper decomposition; Memory architecture; Program processors; Additional key word and phrasessparse direct solv; Algorithm framework; Communication avoiding algorithms; Communication-avoiding; Direct solvers; Distributed Memory; Key words; Mixed precision; Sparse direct solver; Trade off; Graphics processing unit
Event-Based Automatic Differentiation of OpenMP with OpDiLib,2023,ACM Transactions on Mathematical Software,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151543238&doi=10.1145%2f3570159&partnerID=40&md5=6e11eb47bcd5db8f79855c135cb8e64d,"We present the new software OpDiLib, a universal add-on for classical operator overloading AD tools that enables the automatic differentiation (AD) of OpenMP parallelized code. With it, we establish support for OpenMP features in a reverse mode operator overloading AD tool to an extent that was previously only reported on in source transformation tools. We achieve this with an event-based implementation ansatz that is unprecedented in AD. Combined with modern OpenMP features around OMPT, we demonstrate how it can be used to achieve differentiation without any additional modifications of the source code; neither do we impose a priori restrictions on the data access patterns, which makes OpDiLib highly applicable. For further performance optimizations, restrictions like atomic updates on adjoint variables can be lifted in a fine-grained manner. OpDiLib can also be applied in a semi-automatic fashion via a macro interface, which supports compilers that do not implement OMPT. We demonstrate the applicability of OpDiLib for a pure operator overloading approach in a hybrid parallel environment. We quantify the cost of atomic updates on adjoint variables and showcase the speedup and scaling that can be achieved with the different configurations of OpDiLib in both the forward and the reverse pass. © 2023 Association for Computing Machinery.",AD tool design; Additional Key Words and PhrasesAlgorithmic differentiation; high performance computing; OMPT; OpenMP; reusable software,Application programming interfaces (API); Additional key word and phrasesalgorithmic differentiation; Automatic differentiation tool; Automatic differentiation tool design; High performance computing; Key words; OMPT; Openmp; Performance computing; Reusable softwares; Tool designs; Computer software reusability
