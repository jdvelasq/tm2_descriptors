Title,Year,Source title,Link,Abstract,Author Keywords,Index Keywords
Efficient personalized probabilistic retrieval of Chinese calligraphic manuscript images in mobile cloud environment,2014,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84919740519&doi=10.1145%2f2629575&partnerID=40&md5=3512af9b15cc31e34788b5568c462003,"Ancient language manuscripts constitute a key part of the cultural heritage of mankind. As one of the most important languages, Chinese historical calligraphy work has contributed to not only the Chinese cultural heritage but also the world civilization at large, especially for Asia. To support deeper and more convenient appreciation of Chinese calligraphy works, based on our previous work on the probabilistic retrieval of historical Chinese calligraphic character manuscripts repositories, we propose a system framework of the multi-feature-based Chinese calligraphic character images probabilistic retrieval in the mobile cloud network environment, which is called the DPRC. To ensure retrieval efficiency, we further propose four enabling techniques: (1) DRL-based probability propagation, (2) optimal data placement scheme, (3) adaptive data robust transmission algorithm, and (4) index support filtering scheme. Comprehensive experiments are conducted to testify the effectiveness and efficiency of our proposed DPRC method. © 2014 ACM.",Chinese calligraphic character; Probabilistic retrieval; Sentiment,Efficiency; Chinese calligraphic character; Chinese calligraphy; Effectiveness and efficiencies; Probabilistic retrieval; Probability propagation; Retrieval efficiency; Robust transmission; Sentiment; Search engines
Discriminative training for log-linear based SMT: GLobal or local methods,2014,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84919724830&doi=10.1145%2f2637478&partnerID=40&md5=6204a08b62125ac4fec15e2e4d5fe434,"In statistical machine translation, the standard methods such as MERT tune a single weight with regard to a given development data. However, these methods suffer from two problems due to the diversity and uneven distribution of source sentences. First, their performance is highly dependent on the choice of a development set, which may lead to an unstable performance for testing. Second, the sentence level translation quality is not assured since tuning is performed on the document level rather than on sentence level. In contrast with the standard global training in which a single weight is learned, we propose novel local training methods to address these two problems. We perform training and testing in one step by locally learning the sentence-wise weight for each input sentence. Since the time of each tuning step is unnegligible and learning sentence-wise weights for the entire test set means many passes of tuning, it is a great challenge for the efficiency of local training. We propose an efficient two-phase method to put the local training into practice by employing the ultraconservative update. On NIST Chinese-to-English translation tasks with both medium and large scales of training data, our local training methods significantly outperform standard methods with the maximal improvements up to 2.0 BLEU points, meanwhile their efficiency is comparable to that of the standard methods. © 2014 ACM.",Global training; Local training; Log-linear model; Ultraconservative update,Computer aided language translation; Regression analysis; Discriminative training; Local training; Loglinear model; Statistical machine translation; Training and testing; Translation quality; Two phase method; Ultraconservative update; Efficiency
Pronunciation variants prediction method to detect mispronunciations by Korean learners of English,2014,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84919724828&doi=10.1145%2f2629545&partnerID=40&md5=1cd67826c324d223d2408632a02d255d,"This article presents an approach to nonnative pronunciation variants modeling and prediction. The pronunciation variants prediction method was developed by generalized transformation-based error-driven learning (GTBL). The modified goodness of pronunciation (GOP) score was applied to effective mispronunciation detection using logistic regression machine learning under the pronunciation variants prediction. English-read speech data uttered by Korean-speaking learners of English were collected, then pronunciation variation knowledge was extracted from the differences between the canonical phonemes and the actual phonemes of the speech data. With this knowledge, an error-driven learning approach was designed that automatically learns phoneme variation rules from phoneme-level transcriptions. The learned rules generate an extended recognition network to detect mispronunciations. Three different mispronunciation detection methods were tested including our logistic regression machine learning method with modified GOP scores and mispronunciation preference features; all three methods yielded significant improvement in predictions of pronunciation variants, and our logistic regression method showed the best performance. © 2014 ACM.",Mispronunciation detection; Pronunciation variants prediction,Forecasting; Machine learning; Error-driven learning; Generalized transformation; Logistic regression method; Machine learning methods; Mispronunciation detections; Modeling and predictions; Pronunciation variants; Pronunciation variation; Logistic regression
TALIP Perspectives: Editorial Commentary: The State of the Journal,2014,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84907925135&doi=10.1145%2f2656620&partnerID=40&md5=0db6a7e6cd5a61e8190cdc0e196db30e,[No abstract available],,
Distortion model based on word sequence labeling for statistical machine translation,2014,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84897674721&doi=10.1145%2f2537128&partnerID=40&md5=b210a00a6798b38de5010673f8166c88,"This article proposes a new distortion model for phrase-based statistical machine translation. In decoding, a distortion model estimates the source word position to be translated next (subsequent position; SP) given the last translated source word position (current position; CP). We propose a distortion model that can simultaneously consider the word at the CP, the word at an SP candidate, the context of the CP and an SP candidate, relative word order among the SP candidates, and the words between the CP and an SP candidate. These considered elements are called rich context. Our model considers rich context by discriminating label sequences that specify spans from the CP to each SP candidate. It enables our model to learn the effect of relative word order among SP candidates as well as to learn the effect of distances from the training data. In contrast to the learning strategy of existing methods, our learning strategy is that the model learns preference relations among SP candidates in each sentence of the training data. This leaning strategy enables consideration of all of the rich context simultaneously. In our experiments, our model had higher BLUE and RIBES scores for Japanese-English, Chinese-English, and German-English translation compared to the lexical reordering models. © 2014 Copyright is held by the author/owner(s).",Distortion model; Machine translation; Reordering,Learning systems; Distortion model; Learning strategy; Machine translations; Phrase-based statistical machine translation; Preference relation; Reordering; Reordering models; Statistical machine translation; Linguistics
Cross-lingual annotation projection for weakly-supervised relation extraction,2014,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84897691349&doi=10.1145%2f2529994&partnerID=40&md5=a1f848ca99edbbb46f2535d00df51f1e,"Although researchers have conducted extensive studies on relation extraction in the last decade, statistical systems based on supervised learning are still limited, because they require large amounts of training data to achieve high performance level. In this article, we propose cross-lingual annotation projection methods that leverage parallel corpora to build a relation extraction system for a resource-poor language without significant annotation efforts. To make our method more reliable, we introduce two types of projection approaches with noise reduction strategies. We demonstrate the merit of our method using a Korean relation extraction system trained on projected examples from an English-Korean parallel corpus. Experiments show the feasibility of our approaches through comparison to other systems based on monolingual resources. © 2014 ACM.",Cross-lingual annotation projection; Relation extraction; Weakly-supervised learning,Noise abatement; Supervised learning; Cross-lingual; Parallel corpora; Performance level; Projection method; Reduction strategy; Relation extraction; Statistical systems; Weakly-supervised learning; Extraction
"TALIP perspectives, guest editorial commentary what counts (and what ought to count)?",2014,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84897733237&doi=10.1145%2f2559789&partnerID=40&md5=c43ad2b09cd9bce9b9753ba39f884b21,"The Association for Computational Linguistics (ACL) has started a second journal. There is a credible concern that for-profit indexing services might favor for-profit journals. In fact, when for-profit publishers are competing for a new journal, one of their selling points is their track record in advocating new journals to the for-profit indexing services with the implication that non-profit publishers (ACL, ACM, and IEEE) are relatively less successful in adding new journals to the indexes as quickly. There is a need for someone to do a careful comparison of Google Scholar with Web of Science and see which indexing service agrees more with surveys of experts. It would be a mistake to compare scores from one source with scores from another source without calibrating for the differences in the two sources. Similarly, as already mentioned, because of inflation, it would be a mistake to compare stale estimates for one researcher with fresh estimates for another.",,Computational linguistics; Indexing (of information); Google scholar; Indexing Service; Non-profit; Track record; Two sources; Web of Science; Profitability
"Time for more languages: Temporal tagging of Arabic, Italian, Spanish, and Vietnamese",2014,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84897732854&doi=10.1145%2f2540989&partnerID=40&md5=c2ae4f73b79b396c4ecbfd84ae402a56,"Most of the research on temporal tagging so far is done for processing English text documents. There are hardly any multilingual temporal taggers supporting more than two languages. Recently, the temporal tagger HeidelTime has been made publicly available, supporting the integration of new languages by developing language-dependent resources without modifying the source code. In this article, we describe our work on developing such resources for two Asian and two Romance languages: Arabic, Vietnamese, Spanish, and Italian. While temporal tagging of the two Romance languages has been addressed before, there has been almost no research on Arabic and Vietnamese temporal tagging so far. Furthermore, we analyze language-dependent challenges for temporal tagging and explain the strategies we followed to address them. Our evaluation results on publicly available and newly annotated corpora demonstrate the high quality of our new resources for the four languages, which we make publicly available to the research community. © 2014 ACM.",Arabic NLP; HeidelTime; Temporal tagging; TIMEX3; Vietnamese NLP,Industrial plants; Natural language processing systems; Arabic nlp; HeidelTime; Temporal tagging; TIMEX3; Vietnamese; Research
Arabic text categorization based on arabic wikipedia,2014,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84897708603&doi=10.1145%2f2537129&partnerID=40&md5=f4ad265915161d85140b4d4a06aafa4c,"This article describes an algorithm for categorizing Arabic text, relying on highly categorized corpus-based datasets obtained from the Arabic Wikipedia by using manual and automated processes to build and customize categories. The categorization algorithm was built by adopting a simple categorization idea then moving forward to more complex ones. We applied tests and filtration criteria to reach the best and most efficient results that our algorithm can achieve. The categorization depends on the statistical relations between the input (test) text and the reference (training) data supported by well-defined Wikipedia-based categories. Our algorithm supports two levels for categorizing Arabic text; categories are grouped into a hierarchy of main categories and subcategories. This introduces a challenge due to the correlation between certain subcategories and overlap between main categories. We argue that our algorithm achieved good performance compared to other methods reported in the literature. © 2014 ACM.",Arabic natural language processing; Arabic Wikipedia; Categorized corpora; Light stemming; Text analysis; Text categorization,Automation; Linguistics; Natural language processing systems; Text processing; Arabic natural language processing; Categorized corpora; Text analysis; Text categorization; Wikipedia; Algorithms
Linguistic analysis of non-ITG word reordering between language pairs with different word order typologies,2014,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84907854295&doi=10.1145%2f2644810&partnerID=40&md5=4138e0ab61b2e1bbe4f1f04df1ba8c4e,"The Inversion Transduction Grammar (ITG) constraints have been widely used for word reordering in machine translation studies. They are, however, so restricted that some types of word reordering cannot be handled properly. We analyze three corpora between SVO and SOV languages: Chinese-Korean, English- Japanese, and English-Korean. In our analysis, sentences that require non-ITG word reordering are manually categorized. We also report the results for two quantitative measures that reveal the significance of non-ITG word reordering. In conclusion, we suggest that ITG constraints are insufficient to deal with word reordering in real situations.",Corpus analysis; Inversion transduction grammar; Machine translation,Bacteriophages; Computational linguistics; Computer aided language translation; Linguistics; Corpus analysis; Inversion transduction grammars; Language pairs; Linguistic analysis; Machine translations; Quantitative measures; Real situation; Word reordering; Translation (languages)
Allograph modeling for online handwritten characters in devanagari using constrained stroke clustering,2014,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84907809477&doi=10.1145%2f2629622&partnerID=40&md5=87d51752ea10c884eef7ebcfc84871eb,"Writer-specific character writing variations such as those of stroke order and stroke number are an important source of variability in the input when handwriting is captured ""online"" via a stylus and a challenge for robust online recognition of handwritten characters and words. It has been shown by several studies that explicit modeling of character allographs is important for achieving high recognition accuracies in a writerindependent recognition system. While previous approaches have relied on unsupervised clustering at the character or stroke level to find the allographs of a character, in this article we propose the use of constrained clustering using automatically derived domain constraints to find a minimal set of stroke clusters. The allographs identified have been applied to Devanagari character recognition using Hidden Markov Models and Nearest Neighbor classifiers, and the results indicate substantial improvement in recognition accuracy and/or reduction in memory and computation time when compared to alternate modeling techniques. © 2014 ACM.",Allograph modeling; Constrained stroke clustering; Devanagari character recognition; Online handwriting recognition,Constrained stroke clustering; Hand-written characters; Online handwriting recognition
Stemming resource-poor Indian languages,2014,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84907808797&doi=10.1145%2f2629670&partnerID=40&md5=aab97a29eb74394d8d750f0d5b28a7d2,"Stemming is a basic method for morphological normalization of natural language texts. In this study, we focus on the problem of stemming several resource-poor languages from Eastern India, viz., Assamese, Bengali, Bishnupriya Manipuri and Bodo. While Assamese, Bengali and Bishnupriya Manipuri are Indo- Aryan, Bodo is a Tibeto-Burman language. We design a rule-based approach to remove suffixes from words. To reduce over-stemming and under-stemming errors, we introduce a dictionary of frequent words. We observe that, for these languages a dominant amount of suffixes are single letters creating problems during suffix stripping. As a result, we introduce an HMM-based hybrid approach to classify the mis-matched last character. For each word, the stem is extracted by calculating the most probable path in four HMMstates. At each step we measure the stemming accuracy for each language. We obtain 94% accuracy for Assamese and Bengali and 87%, and 82% for Bishnupriya Manipuri and Bodo, respectively, using the hybrid approach. We compare our work with Morfessor [Creutz and Lagus 2005]. As of now, there is no reported work on stemming for Bishnupriya Manipuri and Bodo. Our results on Assamese and Bengali show significant improvement over prior published work [Sarkar and Bandyopadhyay 2008; Sharma et al. 2002, 2003]. © 2014 ACM.",Assamese; Bengali; Bishnupriya Manipuri; Bodo; Markov model; Resource-poor languages; Stemming; Suffix stripping,Assamese; Bengalis; Bishnupriya Manipuri; Bodo; Markov model; Stemming
Splitting Arabic texts into elementary discourse units,2014,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84903266739&doi=10.1145%2f2601401&partnerID=40&md5=782c4d691c5a28a55808a2fc69f3952f,"In this article, we propose the first work that investigates the feasibility of Arabic discourse segmentation into elementary discourse units within the segmented discourse representation theory framework. We first describe our annotation scheme that defines a set of principles to guide the segmentation process. Two corpora have been annotated according to this scheme: elementary school textbooks and newspaper documents extracted from the syntactically annotated Arabic Treebank. Then, we propose a multiclass supervised learning approach that predicts nested units. Our approach uses a combination of punctuation, morphological, lexical, and shallow syntactic features. We investigate how each feature contributes to the learning process.We show that an extensive morphological analysis is crucial to achieve good results in both corpora. In addition, we show that adding chunks does not boost the performance of our system. © 2014 ACM.",Arabic language; Discourse segmentation; Elementary discourse units,Linguistics; Arabic languages; Discourse representation theory; Discourse segmentation; Elementary discourse units; Morphological analysis; Segmentation process; Supervised learning approaches; Syntactic features; Computer science
Towards Kurdish information retrieval,2014,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84903313207&doi=10.1145%2f2556948&partnerID=40&md5=a4af104545ac00285e20f6ba1e7b6f35,"The Kurdish language is an Indo-European language spoken in Kurdistan, a large geographical region in the Middle East. Despite having a large number of speakers, Kurdish is among the less-resourced languages and has not seen much attention from the IR and NLP research communities. This article reports on the outcomes of a project aimed at providing essential resources for processing Kurdish texts. A principal output of this project is Pewan, the first standard Test Collection to evaluate Kurdish Information Retrieval systems. The other language resources that we have built include a lightweight stemmer and a list of stopwords. Our second principal contribution is using these newly-built resources to conduct a thorough experimental study on Kurdish documents. Our experimental results show that normalization, and to a lesser extent, stemming, can greatly improve the performance of Kurdish IR systems. © 2014 ACM.",Cross-lingual information retrieval; Kurdish language; Kurmanji Kurdish; Sorani Kurdish; Stemming; Test collection,Information retrieval systems; Cross-lingual information retrieval; Kurdish language; Kurmanji Kurdish; Sorani Kurdish; Stemming; Test Collection; Natural language processing systems
Pragmatic and cultural considerations for deception detection in Asian Languages,2014,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84903317238&doi=10.1145%2f2605292&partnerID=40&md5=a48a944d9da15e85fed94a5907fc2a27,"In hopes of sparking a discussion, I argue for much needed research on automated deception detection in Asian languages. The task of discerning truthful texts from deceptive ones is challenging, but a logical sequel to opinion mining. I suggest that applied computational linguists pursue broader interdisciplinary research on cultural differences and pragmatic use of language in Asian cultures, before turning to detection methods based on a primarily Western (English-centric) worldview. Deception is fundamentally human, but how do various cultures interpret and judge deceptive behavior?. © 2014 ACM.",,Linguistics; Asian languages; Automated deception detections; Cultural difference; Deception detection; Deceptive behaviors; Detection methods; Interdisciplinary research; Opinion mining; Computer science
The effectiveness of a Jawi stemmer for retrieving relevant Malay documents in Jawi characters,2014,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84903275492&doi=10.1145%2f2540988&partnerID=40&md5=f27333bd6f6d8d5c503bf8e0a767bb15,"The Malay language has two types of writing script, known as Rumi and Jawi. Most previous stemmer results have reported on Malay Rumi characters and only a few have tested Jawi characters. In this article, a new Jawi stemmer has been proposed and tested for document retrieval. A total of 36 queries and datasets from the transliterated Jawi Quran were used. The experiment shows that the mean average precision for a ""stemmed Jawi"" document is 8.43%. At the same time, the mean average precision for a ""nonstemmed Jawi"" document is 5.14%. The result from a paired sample t-test showed that the use of a ""stemmed Jawi"" document increased the precision in document retrieval. Further experiments were performed to examine the precision of the relevant documents that were retrieved at various cutoff points for all 36 queries. The results for the ""stemmed Jawi"" document showed a significantly different start, at a cutoff of 40, compared with the ""nonstemmed Jawi"" documents. This result shows the usefulness of a Jawi stemmer for retrieving relevant documents in the Jawi script. © 2014 ACM.",Jawi document retrieval; Jawi stemmer; Malay stemmer; Stemming,Experiments; Natural language processing systems; Document Retrieval; Jawi stemmer; Malay languages; Malay stemmer; Paired sample; Relevant documents; Stemming; Information retrieval
Word prediction system for text entry in Hindi,2014,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84903311197&doi=10.1145%2f2617590&partnerID=40&md5=7b224427a9a8ecc4038ecf09c25cbf5d,"Word prediction is treated as an efficient technique to enhance text entry rate. Existing word prediction systems predict a word when a user correctly enters the initial few characters of the word. In fact, a word prediction system fails if the user makes errors in the initial input. Therefore, there is a need to develop a word prediction system that predicts desired words while coping with errors in initial entries. This requirement is more relevant in the case of text entry in Indian languages, which are involved with a large set of alphabets, words with complex characters and inflections, phonetically similar sets of characters, etc. In fact, text composition in Indian languages involves frequent spelling errors, which presents a challenge to develop an efficient word prediction system. In this article, we address this problem and propose a novel word prediction system. Our proposed approach has been tried with Hindi, the national language of India. Experiments with users substantiate 43.77% keystroke savings, 92.49% hit rate, and 95.82% of prediction utilization with the proposed word prediction system. Our system also reduces the spelling error by 89.75%. © 2014 ACM.",Hindi text entry; Text entry rate enhancement; Text entry system; Virtual keyboard; Word prediction,Errors; Rate enhancement; Text entry; Text entry systems; Virtual Keyboards; Word prediction; Forecasting
Greetings from the new Editor-in-Chief,2013,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885636347&doi=10.1145%2f2499955.2499956&partnerID=40&md5=bdc14e7535e6da45b36a348429b27a8e,[No abstract available],,
Normalizing complex functional expressions in Japanese predicates: Linguistically-directed rule-based paraphrasing and its application,2013,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885648275&doi=10.1145%2f2499955.2499959&partnerID=40&md5=8235ccc3a47276a001e956d7c086d4d9,"The growing need for text mining systems, such as opinion mining, requires a deep semantic understanding of the target language. In order to accomplish this, extracting the semantic information of functional expressions plays a crucial role, because functional expressions such as would like to and can't are key expressions to detecting customers' needs and wants. However, in Japanese, functional expressions appear in the form of suffixes, and two different types of functional expressions are merged into one predicate: one influences the factual meaning of the predicate while the other is merely used for discourse purposes. This triggers an increase in surface forms, which hinders information extraction systems. In this article, we present a novel normalization technique that paraphrases complex functional expressions into simplified forms that retain only the crucial meaning of the predicate. We construct paraphrasing rules based on linguistic theories in syntax and semantics. The results of experiments indicate that our system achieves a high accuracy of 79.7%, while it reduces the differences in functional expressions by up to 66.7%. The results also show an improvement in the performance of predicate extraction, providing encouraging evidence of the usability of paraphrasing as a means of normalizing different language expressions. © 2013 ACM.",Factuality analysis; Functional expressions; Linguistic theories; Opinion mining; Paraphrasing; Sentiment analysis; Text mining,Information retrieval systems; Semantics; Factuality analysis; Functional expression; Linguistic theory; Opinion mining; Paraphrasing; Sentiment analysis; Text mining; Data mining
Word sense disambiguation by combining labeled data expansion and semi-supervised learning method,2013,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885576957&doi=10.1145%2f2461316.2461319&partnerID=40&md5=41d9c18a55b3ca381fa105a5c39a7084,"Lack of labeled data is one of the severest problems facing word sense disambiguation (WSD). We overcome the problem by proposing a method that combines automatic labeled data expansion (Step 1) and semisupervised learning (Step 2). The Step 1 and 2 methods are both effective, but their combination yields a synergistic effect. In this article, in Step 1, we automatically extract reliable labeled data from raw corpora using dictionary example sentences, even the infrequent and unseen senses (which are not likely to appear in labeled data). Next, in Step 2, we apply a semi-supervised classifier and achieve an improvement using easy-to-get unlabeled data. In this step, we also show that we can guess even unseen senses. We target a SemEval-2010 Japanese WSD task, which is a lexical sample task. Both Step 1 and Step 2 methods performed better than the best published result (76.4 %). Furthermore, the combined method achieved much higher accuracy (84.2 %). In this experiment, up to 50 % of unseen senses are classified correctly. However, the number of unseen senses are small, therefore, we delete one senses per word and apply our proposed method; the results show that the method is effective and robust even for unseen senses. © 2013 ACM.",Example sentences; Hybrid generative/discriminative approaches; Lexicon; Unseen senses,Computer science; Linguistics; Example sentences; Hybrid generative/discriminative approaches; Lexicon; Semi- supervised learning; Semi-supervised learning methods; Synergistic effect; Unseen senses; Word-sense disambiguation; Supervised learning
Post-ordering by parsing with ITG for Japanese-English statistical machine translation,2013,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887067268&doi=10.1145%2f2518100&partnerID=40&md5=d0df7ca8e4ae0dd01ba1c4cdcecd2dab,"Word reordering is a difficult task for translation between languages with widely different word orders, such as Japanese and English. A previously proposed post-ordering method for Japanese-to-English translation first translates a Japanese sentence into a sequence of English words in a word order similar to that of Japanese, then reorders the sequence into an English word order. We employed this post-ordering framework and improved upon its reordering method. The existing post-ordering method reorders the sequence of English words via SMT, whereas our method reorders the sequence by (1) parsing the sequence using ITG to obtain syntactic structures which are similar to Japanese syntactic structures, and (2) transferring the obtained syntactic structures into English syntactic structures according to the ITG. The experiments using Japanese-to-English patent translation demonstrated the effectiveness of our method and showed that both the RIBES and BLEU scores were improved over compared methods. © 2013 ACM.",Inversion transduction grammar; Machine translation; Parsing; Post-ordering,Syntactics; English word; Inversion transduction grammars; Machine translations; Parsing; Post-ordering; Statistical machine translation; Syntactic structure; Word reordering; Formal languages
A Bayesian alignment approach to transliteration mining,2013,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885618855&doi=10.1145%2f2499955.2499957&partnerID=40&md5=073db9ccfbd22400dc8c4c60536127a3,"In this article we present a technique for mining transliteration pairs using a set of simple features derived from a many-to-many bilingual forced-alignment at the grapheme level to classify candidate transliteration word pairs as correct transliterations or not. We use a nonparametric Bayesian method for the alignment process, as this process rewards the reuse of parameters, resulting in compact models that align in a consistent manner and tend not to over-fit. Our approach uses the generative model resulting from aligning the training data to force-align the test data. We rely on the simple assumption that correct transliteration pairs would be well modeled and generated easily, whereas incorrect pairs-being more random in character-would be more costly to model and generate. Our generative model generates by concatenating bilingual grapheme sequence pairs. The many-to-many generation process is essential for handling many languages with non-Roman scripts, and it is hard to train well using a maximum likelihood techniques, as these tend to over-fit the data. Our approach works on the principle that generation using only grapheme sequence pairs that are in the model results in a high probability derivation, whereas if the model is forced to introduce a new parameter in order to explain part of the candidate pair, the derivation probability is substantially reduced and severely reduced if the new parameter corresponds to a sequence pair composed of a large number of graphemes. The features we extract from the alignment of the test data are not only based on the scores from the generative model, but also on the relative proportions of each sequence that are hard to generate. The features are used in conjunction with a support vector machine classifier trained on known positive examples together with synthetic negative examples to determine whether a candidate word pair is a correct transliteration pair. In our experiments, we used all data tracks from the 2010 Named-Entity Workshop (NEWS'10) and use the performance of the best system for each language pair as a reference point. Our results show that the new features we propose are powerfully predictive, enabling our approach to achieve levels of performance on this task that are comparable to the state of the art. © 2013 ACM.",Bayesian alignment; Dirichlet process model; Gibbs sampling; Katakana; Transliteration; Transliteration mining,Computer science; Linguistics; Bayesian; Dirichlet process; Gibbs sampling; Katakana; Transliteration; Bayesian networks
Syntax-based post-ordering for efficient Japanese-to-English translation,2013,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885594403&doi=10.1145%2f2499955.2499960&partnerID=40&md5=f2a603b5199d96b41af2c74f92b21d13,"This article proposes a novel reordering method for efficient two-step Japanese-to-English statistical machine translation (SMT) that isolates reordering from SMT and solves it after lexical translation. This reordering problem, called post-ordering, is solved as an SMT problem from Head-Final English (HFE) to English. HFE is syntax-based reordered English that is very successfully used for reordering with English-to-Japanese SMT. The proposed method incorporates its advantage into the reverse direction, Japanese-to-English, and solves the post-ordering problem by accurate syntax-based SMT with target language syntax. Two-step SMT with the proposed post-ordering empirically reduces the decoding time of the accurate but slow syntax-based SMT by its good approximation using intermediate HFE. The proposed method improves the decoding speed of syntax-based SMT decoding by about six times with comparable translation accuracy in Japanese-to-English patent translation experiments. © 2013 ACM.",Japanese-to-English translation; Long-distance reordering; Post-ordering; Statistical machine translation,Decoding; Linguistics; Syntactics; Decoding speed; Long-distance reordering; Post-ordering; Statistical machine translation; Target language; Speech transmission
A named entity recognition method based on decomposition and concatenation of word chunks,2013,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885602314&doi=10.1145%2f2499955.2499958&partnerID=40&md5=916dd2d7dc19dce6895d72312248dc4a,"We propose a named entity (NE) recognition method in which word chunks are repeatedly decomposed and concatenated. Our method identifies word chunks with a base chunker, such as a noun phrase chunker, and then recognizes NEs from the recognized word chunk sequences. By using word chunks, we can obtain features that cannot be obtained in word-sequence-based recognition methods, such as the first word of a word chunk, the last word of a word chunk, and so on. However, each word chunk may include a part of an NE or multiple NEs. To solve this problem, we use the following operators: SHIFT for separating the first word from a word chunk, POP for separating the last word from a word chunk, JOIN for concatenating two word chunks, and REDUCE for assigning an NE label to a word chunk. We evaluate our method on a Japanese NE recognition dataset that includes about 200,000 annotations of 191 types of NEs from over 8,500 news articles. The experimental results show that the training and processing speeds of our method are faster than those of a linear-chain structured perceptron and a semi-Markov perceptron, while maintaining high accuracy. © 2013 ACM.",Extended named entity recognition,Computer science; Linguistics; Named entities; Named entity recognition; News articles; Noun phrase; Perceptron; Processing speed; Recognition methods; Semi-Markov; Data processing
Design and evaluation of soft keyboards for brahmic scripts,2013,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885631582&doi=10.1145%2f2461316.2461318&partnerID=40&md5=ddedaedf721cbaa2d6b21ed5494fc6fc,"Despite being spoken by a large percentage of the world, Indic languages in general lack user-friendly and efficient methods for text input. These languages have poor or no support for typing. Soft keyboards, because of their ease of installation and lack of reliance on specific hardware, are a promising solution as an input device for many languages. Developing an acceptable soft keyboard requires the frequency analysis of characters in order to design a layout that minimizes text-input time. This article proposes the use of various development techniques, layout variations, and evaluation methods for the creation of soft keyboards for Brahmic scripts. We propose that using optimization techniques such as genetic algorithms and multi-objective Pareto optimization to develop multi-layer keyboards will increase the speed at which text can be entered. © 2013 ACM.",Assamese; Genetic algorithms; Indic languages; Mobile devices; Pareto optimization; Soft keyboards,Genetic algorithms; Mobile devices; Multiobjective optimization; Assamese; Design and evaluations; Development technique; Evaluation methods; Frequency Analysis; Optimization techniques; Pareto optimization; Soft keyboard; Typewriter keyboards
Chinese-Japanese machine translation exploiting Chinese characters,2013,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887056965&doi=10.1145%2f2523057.2523059&partnerID=40&md5=4bab555ae06b7249bd68d45d5b6061b0,"The Chinese and Japanese languages share Chinese characters. Since the Chinese characters in Japanese originated from ancient China, many common Chinese characters exist between these two languages. Since Chinese characters contain significant semantic information and common Chinese characters share the same meaning in the two languages, they can be quite useful in Chinese-Japanese machine translation (MT). We therefore propose a method for creating a Chinese character mapping table for Japanese, traditional Chinese, and simplified Chinese, with the aim of constructing a complete resource of common Chinese characters. Furthermore, we point out two main problems in Chinese word segmentation for Chinese-Japanese MT, namely, unknown words and word segmentation granularity, and propose an approach exploiting common Chinese characters to solve these problems. We also propose a statistical method for detecting other semantically equivalent Chinese characters other than the common ones and a method for exploiting shared Chinese characters in phrase alignment. Results of the experiments carried out on a state-of-the-art phrase-based statistical MT system and an example-based MT system show that our proposed approaches can improve MT performance significantly, thereby verifying the effectiveness of shared Chinese characters for Chinese-Japanese MT. © 2013 ACM.",Chinese characters; Chinese-Japanese; Machine translation; Phrase alignment; Segmentation,Alignment; Computational linguistics; Image segmentation; Ancient China; Chinese characters; Chinese word segmentation; Chinese-Japanese; Example-based; Machine translations; Semantic information; Word segmentation; Computer aided language translation
A computer-assisted translation and writing system,2013,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887057813&doi=10.1145%2f2505984&partnerID=40&md5=29dba68ee324474032ca1720161333f4,"We introduce a method for learning to predict text and grammatical construction in a computer-assisted translation and writing framework. In our approach, predictions are offered on the fly to help the user make appropriate lexical and grammar choices during the translation of a source text, thus improving translation quality and productivity. The method involves automatically generating general-to-specific word usage summaries (i.e., writing suggestion module), and automatically learning high-confidence word- or phrase-level translation equivalents (i.e., translation suggestion module). At runtime, the source text and its translation prefix entered by the user are broken down into n-grams to generate grammar and translation predictions, which are further combined and ranked via translation and language models. These ranked prediction candidates are iteratively and interactively displayed to the user in a pop-up menu as translation or writing hints. We present a prototype writing assistant, TransAhead, that applies the method to a human-computer collaborative environment. Automatic and human evaluations show that novice translators or language learners substantially benefit from our system in terms of translation performance (i.e., translation accuracy and productivity) and language learning (i.e., collocation usage and grammar). In general, our methodology of inline grammar and text predictions or suggestions has great potential in the field of computer-assisted translation, writing, or even language learning. © 2013 ACM.",Computer-assisted language learning; Computer-assisted translation; Grammar and text prediction; Grammar pattern; Translation model and language model; Word usage,Computational linguistics; Computer aided instruction; Forecasting; Iterative methods; Productivity; Computer assisted; Computer assisted language learning; Grammar pattern; Language model; Text prediction; Word usage; Translation (languages)
TALIP perspectives,2013,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887849621&doi=10.1145%2f2523057.2523058&partnerID=40&md5=115e14e2c9e872ab04f11e1503d1b07e,"Richard Sproat, Editor-in-Chief of the journal ACM Transactions on Asian Language Information Processing, expresses his views on an ongoing series of editorials entitled 'TALIP Perspectives.' By a happy coincidence one issue that has come up recently relates to the ACM, TALIP and the portrayal of scientific results in the popular science press a topic that is of a great deal of personal interest tome, and one that should properly be the interest of anyone working in science and engineering. A journal like TALIP cannot hope to compete in the popular press with a prestige publication like Science: diets rich in fiber and protein and low in sugars and fats often have a tough time competing with the processed food industry whose products have greater mass appeal. But by injecting some solid if perhaps less exciting results into the mix, one may hope to balance things out just a little better.",,Data processing; Asian languages; In-fiber; Science and engineering; Scientific results; Presses (machine tools)
How to choose the best pivot language for automatic translation of low-resource languages,2013,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887076517&doi=10.1145%2f2505126&partnerID=40&md5=438e565e0ce156185a524261a2cf906c,"Recent research on multilingual statistical machine translation focuses on the usage of pivot languages in order to overcome language resource limitations for certain language pairs. Due to the richness of available language resources, English is, in general, the pivot language of choice. However, factors like language relatedness can also effect the choice of the pivot language for a given language pair, especially for Asian languages, where language resources are currently quite limited. In this article, we provide new insights into what factors make a pivot language effective and investigate the impact of these factors on the overall pivot translation performance for translation between 22 Indo-European and Asian languages. Experimental results using state-of-the-art statistical machine translation techniques revealed that the translation quality of 54.8% of the language pairs improved when a non-English pivot language was chosen. Moreover, 81.0% of system performance variations can be explained by a combination of factors such as language family, vocabulary, sentence length, language perplexity, translation model entropy, reordering, monotonicity, and engine performance. © 2013 ACM.",Asian languages; Machine translation; Pivot language selection; Translation quality indicators,Linguistics; Asian languages; Automatic translation; Engine performance; Machine translations; Performance variations; Pivot language; Statistical machine translation; Translation quality; Translation (languages)
Learning abbreviations from Chinese and English terms by modeling non-local information,2013,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885577946&doi=10.1145%2f2461316.2461317&partnerID=40&md5=0a8fb7de2663df643ef13d24a66cbf80,"The present article describes a robust approach for abbreviating terms. First, in order to incorporate nonlocal information into abbreviation generation tasks, we present both implicit and explicit solutions: the latent variable model and the label encoding with global information. Although the two approaches compete with one another, we find they are also highly complementary. We propose a combination of the two approaches, and we will show the proposed method outperforms all of the existing methods on abbreviation generation datasets. In order to reduce computational complexity of learning non-local information, we further present an online training method, which can arrive the objective optimum with accelerated training speed. We used a Chinese newswire dataset and a English biomedical dataset for experiments. Experiments revealed that the proposed abbreviation generator with non-local information achieved the best results for both the Chinese and English languages. © 2013 ACM.",Abbreviation processing; Machine learning; Non-local information; Stochastic learning,Computer science; Learning systems; Linguistics; English languages; Explicit solutions; Global informations; Latent variable modeling; Nonlocal; Nonlocal information; Robust approaches; Stochastic learning; Experiments
The left and right context of a word: Overlapping chinese syllable word segmentation with minimal context,2013,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874839955&doi=10.1145%2f2425327.2425329&partnerID=40&md5=f7f1c5051f67249b0344f894b510d567,"Since a Chinese syllable can correspond to many characters (homophones), the syllable-to-character conversion task is quite challenging for Chinese phonetic input methods (CPIM). There are usually two stages in a CPIM: 1. segment the syllable sequence into syllable words, and 2. select the most likely character words for each syllable word. A CPIM usually assumes that the input is a complete sentence, and evaluates the performance based on a well-formed corpus. However, in practice, most Pinyin users prefer progressive text entry in several short chunks, mainly in one or two words each (most Chinese words consist of two or more characters). Short chunks do not provide enough contexts to perform the best possible syllable-to-character conversion, especially when a chunk consists of overlapping syllable words. In such cases, a conversion system often selects the boundary of a word with the highest frequency. Short chunk input is even more popular on platforms with limited computing power, such as mobile phones. Based on the observation that the relative strength of a word can be quite different when calculated leftwards or rightwards, we propose a simple division of the word context into the left context and the right context. Furthermore, we design a double ranking strategy for each word to reduce the number of errors in Step 1. Our strategy is modeled as the minimum feedback arc set problem on bipartite tournament with approximate solutions derived from genetic algorithm. Experiments show that, compared to the frequency-based method (FBM) (low memory and fast) and the conditional random fields (CRF) model (larger memory and slower), our double ranking strategy has the benefits of less memory and low power requirement with competitive performance. We believe a similar strategy could also be adopted to disambiguate conflicting linguistic patterns effectively. Copyright © 2013 ACM.",Chinese phonetic input methods; Syllable-to-word conversion,Computer science; Linguistics; Approximate solution; Bipartite tournament; Competitive performance; Conditional random field; Conversion systems; Input methods; Linguistic patterns; Minimum feedback arc-set; Computational linguistics
Toward a professional platform for chinese character conversion,2013,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874857649&doi=10.1145%2f2425327.2425328&partnerID=40&md5=c81e6fbaaf14e367889193cb4cad6c86,"Increasing communication among Chinese-speaking regions using respectively traditional and simplified Chinese character systems has highlighted the subtle-yet-extensive differences between the two systems, which can lead to unexpected hindrance in converting characters from one to the other. This article proposes a new priority-based multi-data resources management model, with a new algorithm called Fused Conversion algorithm from Multi-Data resources (FCMD), to ensure more context-sensitive, human controllable, and thus more reliable conversions, by drawing on reverse maximum matching, n-gram-based statistical model and pattern-based learning and matching. After parameter training on the Tagged Chinese Gigaword corpus, its conversion precision reaches 91.5% in context-sensitive cases, the most difficult part in the conversion, with an overall precision rate at 99.8%, a significant improvement over the state-of-the-art models. The conversion platform based on the model has extra features such as data resource selection and n-grams self-learning ability, providing a more sophisticated tool good especially for high-end professional uses. Copyright © 2013 ACM.",Chinese character conversion; FCMD algorithm; Multi-data resources; N-gram; Pattern learning; Reverse maximum matching,Computer science; Linguistics; Chinese character conversions; Multi-data resources; N-gram; Pattern Learning; Reverse maximum matching; Learning algorithms
Attention-feedback based robust segmentation of online handwritten isolated tamil words,2013,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874014034&doi=10.1145%2f2425327.2425331&partnerID=40&md5=e2990b7d8324f68d48422e3a9b773c11,"In this article, we propose a lexicon-free, script-dependent approach to segment online handwritten isolated Tamil words into its constituent symbols. Our proposed segmentation strategy comprises two modules, namely the (1) Dominant Overlap Criterion Segmentation (DOCS) module and (2) Attention Feedback Segmentation (AFS) module. Based on a bounding box overlap criterion in the DOCS module, the input word is first segmented into stroke groups. A stroke group may at times correspond to a part of a valid symbol (over-segmentation) or a merger of valid symbols (under-segmentation). Attention on specific features in the AFS module serve in detecting possibly over-segmented or under-segmented stroke groups. Thereafter, feedbacks from the SVM classifier likelihoods and stroke-group based features are considered in modifying the suspected stroke groups to form valid symbols. The proposed scheme is tested on a set of 10000 isolated handwritten words (containing 53,246 Tamil symbols). The results show that the DOCS module achieves a symbol-level segmentation accuracy of 98.1%, which improves to as high as 99.7% after the AFS strategy. This in turn entails a symbol recognition rate of 83.9% (at the DOCS module) and 88.4% (after the AFS module). The resulting word recognition rates at the DOCS and AFS modules are found to be, 50.9% and 64.9% respectively, without any postprocessing. Copyright © 2013 ACM.",Attention Feedback Segmentation (AFS) module; Dominant Overlap Criterion Segmentation (DOCS) module; Handwriting recognition; Online Tamil words; Stroke group; Support Vector Machines (SVM); Tamil,Feedback; Image segmentation; Support vector machines; Attention Feedback Segmentation (AFS) module; Dominant Overlap Criterion Segmentation (DOCS) module; Handwriting recognition; Online Tamil words; Stroke group; Tamil; Character recognition
A two-phase framework for learning logical structures of paragraphs in legal articles,2013,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874822251&doi=10.1145%2f2425327.2425330&partnerID=40&md5=b908add0cc750243c6b24be89ac64571,"Analyzing logical structures of texts is important to understanding natural language, especially in the legal domain, where legal texts have their own specific characteristics. Recognizing logical structures in legal texts does not only help people in understanding legal documents, but also in supporting other tasks in legal text processing. In this article, we present a new task, learning logical structures of paragraphs in legal articles, which is studied in research on Legal Engineering. The goals of this task are recognizing logical parts of law sentences in a paragraph, and then grouping related logical parts into some logical structures of formulas, which describe logical relations between logical parts. We present a two-phase framework to learn logical structures of paragraphs in legal articles. In the first phase, we model the problem of recognizing logical parts in law sentences as a multi-layer sequence learning problem, and present a CRF-based model to recognize them. In the second phase, we propose a graph-based method to group logical parts into logical structures. We consider the problem of finding a subset of complete subgraphs in a weighted-edge complete graph, where each node corresponds to a logical part, and a complete subgraph corresponds to a logical structure. We also present an integer linear programming formulation for this optimization problem. Our models achieve 74.37% in recognizing logical parts, 80.08% in recognizing logical structures, and 58.36% in the whole task on the Japanese National Pension Law corpus. Our work provides promising results for further research on this interesting task. Copyright © 2013 ACM.",Conditional random fields; Graph-based methods; Integer linear programming; Japanese National Pension Law; Legal text processing; Logical parts; Logical structures; Maximum entropy model; Sequence learning; Support vector machines,Graph theory; Integer programming; Maximum entropy methods; Support vector machines; Text processing; Conditional random field; Graph-based methods; Integer Linear Programming; Japanese National Pension Law; Logical parts; Logical structure; Maximum entropy models; Sequence learning; Engineering research
Leveraging diverse lexical resources for textual entailment recognition,2012,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871182323&doi=10.1145%2f2382593.2382600&partnerID=40&md5=4513f9053072de0cc4aebdcb892e0637,"Since the problem of textual entailment recognition requires capturing semantic relations between diverse expressions of language, linguistic and world knowledge play an important role. In this article, we explore the effectiveness of different types of currently available resources including synonyms, antonyms, hypernym-hyponym relations, and lexical entailment relations for the task of textual entailment recognition. In order to do so, we develop an entailment relation recognition system which utilizes diverse linguistic analyses and resources to align the linguistic units in a pair of texts and identifies entailment relations based on these alignments. We use the Japanese subset of the NTCIR-9 RITE-1 dataset for evaluation and error analysis, conducting ablation testing and evaluation on hand-crafted alignment gold standard data to evaluate the contribution of individual resources. Error analysis shows that existing knowledge sources are effective for RTE, but that their coverage is limited, especially for domain-specific and other low-frequency expressions. To increase alignment coverage on such expressions, we propose a method of alignment inference that uses syntactic and semantic dependency information to identify likely alignments without relying on external resources. Evaluation adding alignment inference to a system using all available knowledge sources shows improvements in both precision and recall of entailment relation recognition. © 2012 ACM 1530-0226/2012/12-ART14 $15.00.",Alignment; Lexical resources; Textual entailment,Alignment; Error analysis; Linguistics; Natural language processing systems; Statistical tests; Text processing; Ablation testing; Data sets; Domain specific; External resources; Gold standards; Knowledge sources; Lexical resources; Linguistic analysis; Linguistic units; Precision and recall; Recognition systems; Semantic dependency; Semantic relations; Textual entailment; World knowledge; Semantics
Predicate-argument structure-based textual entailment recognition system exploiting wide-coverage lexical knowledge,2012,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871209690&doi=10.1145%2f2382593.2382598&partnerID=40&md5=5b6607dab9cf15f8fe5b5f6eb6b56ddc,This article proposes a predicate-argument structure based Textual Entailment Recognition system exploiting wide-coverage lexical knowledge. Different from conventional machine learning approaches where several features obtained from linguistic analysis resources are utilized our proposed method regards a Predicate-argument Structure As A Basic Unit Performs The Matchingalignment Between A Text Hypothesis. In Matching Between Predicate-arguments Wide-coverage Relations Between Wordsphrases Such As Synonym Is-a Are Utilized Which Are Automatically Acquired From A Dictionary Web Corpus Wikipedia. © 2012 ACM 1530-0226/2012/12-ART14 $15.00.,Lexical knowledge; Phrases RTE; Predicate-argument structure,Computer science; Linguistics; Conventional machines; Learning approach; Lexical knowledge; Linguistic analysis; Phrases RTE; Recognition systems; Structure-based; Textual entailment; Web Corpora; Wikipedia; Text processing
Evaluating Textual Entailment Recognition for University Entrance Examinations,2012,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871221550&doi=10.1145%2f2382593.2382595&partnerID=40&md5=0655fac70afe27f8ac09d3a3fe8038ee,"The present article addresses an attempt to apply questions in university entrance examinations to the evaluation of textual entailment recognition. Questions in several fields, such as history and politics, primarily test the examinee's knowledge in the form of choosing true statements from multiple choices. Answering such questions can be regarded as equivalent to finding evidential texts from a textbase such as textbooks and Wikipedia. Therefore, this task can be recast as recognizing textual entailment between a description in a textbase and a statement given in a question. We focused on the National Center Test for University Admission in Japan and converted questions into the evaluation data for textual entailment recognition by using Wikipedia as a textbase. Consequently, it is revealed that nearly half of the questions can be mapped into textual entailment recognition; 941 text pairs were created from 404 questions from six subjects. This data set is provided for a subtask of NTCIR RITE (Recognizing Inference in Text), and 16 systems from six teams used the data set for evaluation. The evaluation results revealed that the best system achieved a correct answer ratio of 56%, which is significantly better than a random. © 2012 ACM 1530-0226/2012/12-ART14 $15.00.",Textual entailment recognition; University entrance examination,Text processing; Websites; Data sets; Entrance examination; Evaluation results; Multiple choice; Recognizing textual entailments; Textual entailment; Wikipedia; Character recognition
Recognizing inference in texts with markov logic networks,2012,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871185450&doi=10.1145%2f2382593.2382597&partnerID=40&md5=f9cf99bca7f4cc3529e5e21a7fc6bf16,inference in texts (RITE) attracts growing attention of natural language processing (NLP) researchers in recent years. In this article we propose a novel approach to recognize inference with probabilistic logical reasoning. Our approach is built on Markov logic networks (MLNs) framework which is a Probabilistic Extension Of First-order Logic. We Design Specific Semantic Rules Based On The Surface Syntactic Semantic Representations Of Texts Map These Rules To Logical Representations. We Also Extract Information From Some Knowledge Bases As Common Sense Logic Rules. Then We Utilize MLNs Framework To Make Predictions With Combining Statistical Logical Reasoning. Experiment Results Shows That Our System Can Achieve Better Performance Than State-of-the-art RITE Systems. © 2012 ACM 1530-0226/2012/12-ART14 $15.00.,Logical reasoning Recognizing; Markov logic networks; Phrases Recognizing inference in text,Markov processes; Natural language processing systems; Semantics; Common sense; First order logic; Knowledge basis; Logic rules; Logical reasoning; Logical representations; Markov logic networks; NAtural language processing; Phrases Recognizing inference in text; Probabilistic extension; Semantic representation; Specific semantics; Character recognition
Learning to Recognize Textual Entailment in Japanese Texts with the Utilization of Machine Translation,2012,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871197034&doi=10.1145%2f2382593.2382596&partnerID=40&md5=bd695ec2c1e3e4fd2e54d2f56a2c3663,"Recognizing Textual Entailment (RTE) is a fundamental task in Natural Language Understanding. The task is to decide whether the meaning of a text can be inferred from the meaning of another one. In this article, we conduct an empirical study of recognizing textual entailment in Japanese texts, in which we adopt a machine learning-based approach to the task. We quantitatively analyze the effects of various entailment features, machine learning algorithms, and the impact of RTE resources on the performance of an RTE system. This article also investigates the use of machine translation for the RTE task and determines whether machine translation can be used to improve the performance of our RTE system. Experimental results achieved on benchmark data sets show that our machine learning-based RTE system outperforms the baseline methods based on lexical matching and syntactic matching. The results also suggest that the machine translation component can be utilized to improve the performance of the RTE system. © 2012 ACM 1530-0226/2012/12-ART14 $15.00.",Japanese texts; Machine learning; Machine translation; Textual entailment,Computer aided language translation; Learning systems; Text processing; Baseline methods; Benchmark data; Empirical studies; Japanese text; Learning-based approach; Machine translations; Natural language understanding; Recognizing textual entailments; Syntactic matching; Textual entailment; Learning algorithms
Introduction to the special issue on Rite,2012,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871244556&doi=10.1145%2f2382593.2382594&partnerID=40&md5=06003ccb16e792be5aad8b7538749827,[No abstract available],,
Validating contradiction in texts using online co-mention pattern checking,2012,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871252211&doi=10.1145%2f2382593.2382599&partnerID=40&md5=364f64719070e7ccd3be0e21dc8efde0,"Detecting contradictive statements is a foundational and challenging task for text understanding applications such as textual entailment. In this article, we aim to address the problem of the shortage of specific background knowledge in contradiction detection. A novel contradiction detecting approach based on the distribution of the query composed of critical mismatch combinations on the Internet is proposed to tackle the problem. By measuring the availability of mismatch conjunction phrases (MCPs), the background knowledge about two target statements can be implicitly obtained for identifying contradictions. Experiments on three different configurations show that the MCP-based approach achieves remarkable improvement on contradiction detection and can significantly improve the performance of textual entailment recognition. © 2012 ACM 1530-0226/2012/12-ART14 $15.00.",Chinese; Contradiction detection; Textual entailment; WEB mining,Computer science; Linguistics; Background knowledge; Chinese; Textual entailment; Web Mining; Text processing
HPSG-based preprocessing for english-to-Japanese translation,2012,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866479469&doi=10.1145%2f2334801.2334802&partnerID=40&md5=48441cba5375051dbd932ec4cdfa86d6,"Japanese sentences have completely different word orders from corresponding English sentences. Typical phrase-based statistical machine translation (SMT) systems such as Moses search for the best word permutation within a given distance limit (distortion limit). For English-to-Japanese translation, we need a large distance limit to obtain acceptable translations, and the number of translation candidates is extremely large. Therefore, SMT systems often fail to find acceptable translations within a limited time. To solve this problem, some researchers use rule-based preprocessing approaches, which reorder English words just like Japanese by using dozens of rules. Our idea is based on the following two observations: (1) Japanese is a typical head-final language, and (2) we can detect heads of English sentences by a head-driven phrase structure grammar (HPSG) parser. The main contributions of this article are twofold: First, we demonstrate how off-the-shelf, state-of-the-art HPSG parser enables us to write the reordering rules in an abstract level and can easily improve the quality of English-to-Japanese translation. Second, we also show that syntactic heads achieve better results than semantic heads. The proposed method outperforms the best system of NTCIR-7 PATMT EJ task. © 2012 ACM.",English; HPSG; Japanese; Machine translation; SOV; SVO,Abstracting; Natural language processing systems; Semantics; English; HPSG; Japanese; Machine translations; SOV; SVO; Speech transmission
Cross-language latent relational search between Japanese and english languages using a web corpus,2012,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866488504&doi=10.1145%2f2334801.2334805&partnerID=40&md5=197e16a0b286fe822aabb64205c526e6,"Latent relational search is a novel entity retrieval paradigm based on the proportional analogy between two entity pairs. Given a latent relational search query {(Japan, Tokyo), (France, ?)}, a latent relational search engine is expected to retrieve and rank the entity ""Paris"" as the first answer in the result list. A latent relational search engine extracts entities and relations between those entities from a corpus, such as the Web. Moreover, from some supporting sentences in the corpus, (e.g., ""Tokyo is the capital of Japan"" and ""Paris is the capital and biggest city of France""), the search engine must recognize the relational similarity between the two entity pairs. In cross-language latent relational search, the entity pairs as well as the supporting sentences of the first entity pair and of the second entity pair are in different languages. Therefore, the search engine must recognize similar semantic relations across languages. In this article, we study the problem of cross-language latent relational search between Japanese and English using Web data. To perform cross-language latent relational search in high speed, we propose a multi-lingual indexing method for storing entities and lexical patterns that represent the semantic relations extracted from Web corpora. We then propose a hybrid lexical pattern clustering algorithm to capture the semantic similarity between lexical patterns across languages. Using this algorithm, we can precisely measure the relational similarity between entity pairs across languages, thereby achieving high precision in the task of cross-language latent relational search. Experiments show that the proposed method achieves an MRR of 0.605 on Japanese- English cross-language latent relational search query sets and it also achieves a reasonable performance on the INEX Entity Ranking task. © 2012 ACM.",Analogical search; Cross-language relational search; Latent relational analysis; Latent relational search,Clustering algorithms; Natural language processing systems; Search engines; Semantic Web; Semantics; Software agents; Analogical search; Cross-language relational search; English languages; Entity ranking; Entity retrieval; High precision; Indexing methods; Latent relational analysis; Latent relational search; Lexical patterns; Relational similarities; Search queries; Semantic relations; Semantic similarity; Web Corpora; Web data; Linguistics
Stacking model-based korean prosodic phrasing using speaker variability reduction and linguistic feature engineering,2012,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866480288&doi=10.1145%2f2334801.2334804&partnerID=40&md5=42693df77043ea8c975958caa550f237,"This article presents a prosodic phrasing model for a general purpose Korean speech synthesis system. To reflect the factors affecting prosodic phrasing in the model, linguistically motivated machine-learning features were investigated. These features were effectively incorporated using a stacking model. The phrasing performance was also improved through feature engineering. The corpus used in the experiment is a 4,392-sentence corpus (55,015 words with an average of 13 words per sentence). Because the corpus contains speaker-dependent variability and such variability is not appropriately reflected in a general purpose speech synthesis system, a method to reduce such variability is proposed. In addition, the entire set of data used in the experiment is provided to the public for future use in comparative research. © 2012 ACM.",Korean; Linguistic feature; Phrase break prediction; Prosodic phrasing; Prosody; Speech synthesis; Stacking model,Experiments; Speech synthesis; Comparative research; General purpose; Korean; Linguistic features; Machine-learning; Prosodic phrasing; Prosodic phrasing model; Prosody; Speaker variability; Speech synthesis system; Linguistics
Adaptive bayesian HMM for fully unsupervised chinese part-of-speech induction,2012,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866491555&doi=10.1145%2f2334801.2334803&partnerID=40&md5=22c5f5937a9f16b0031de3fdcf466eba,We propose an adaptive Bayesian hidden Markov model for fully unsupervised part-of-speech (POS) induction. The proposed model with its inference algorithm has two extensions to the first-order Bayesian HMM with Dirichlet priors. First our algorithm infers the optimal number of hidden states from the training corpus rather than fixes the dimensionality of state space beforehand. The second extension studies the Chinese unknown word processing module which measures similarities from both morphological properties and context distribution. Experimental results showed that both of these two extensions can help to find the optimal categories for Chinese in terms of both unsupervised clustering metrics and grammar induction accuracies on the Chinese Treebank. © 2012 ACM.,Bayesian HMM; Chinese language model; Dirichlet distribution; Part-of-speech induction; Variational inference,Algorithms; Computational linguistics; Hidden Markov models; Optimization; Word processing; Bayesian HMM; Chinese language; Dirichlet distributions; Part Of Speech; Variational inference; Inference engines
Statistical extraction and comparison of pivot words for bilingual lexicon extension,2012,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863640864&doi=10.1145%2f2184436.2184439&partnerID=40&md5=3044331bdcc7ee4bd4926a7e74419784,"Bilingual dictionaries can be automatically extended by new translations using comparable corpora. The general idea is based on the assumption that similar words have similar contexts across languages. However, previous studies have mainly focused on Indo-European languages, or use only a bag-of-words model to describe the context. Furthermore, we argue that it is helpful to extract only the statistically significant context, instead of using all context. The present approach addresses these issues in the following manner. First, based on the context of a word with an unknown translation (query word), we extract salient pivot words. Pivot words are words for which a translation is already available in a bilingual dictionary. For the extraction of salient pivot words, we use a Bayesian estimation of the point-wise mutual information to measure statistical significance. In the second step, we match these pivot words across languages to identify translation candidates for the query word. We therefore calculate a similarity score between the query word and a translation candidate using the probability that the same pivots will be extracted for both the query word and the translation candidate. The proposed method uses several context positions, namely, a bag-of-words of one sentence, and the successors, predecessors, and siblings with respect to the dependency parse tree of the sentence. In order to make these context positions comparable across Japanese and English, which are unrelated languages, we use several heuristics to adjust the dependency trees appropriately. We demonstrate that the proposed method significantly increases the accuracy of word translations, as compared to previous methods. © 2012 ACM.",Bayesian statistical methods; Bilingual dictionary creation; Comparable corpora; Dependency parse tree information,Forestry; Languages; Translation; Trees; Bayesian networks; Forestry; Bag of words; Bag-of-words models; Bayesian estimations; Bayesian statistical method; Bilingual dictionary; Bilingual lexicons; Comparable corpora; Dependency trees; Mutual informations; Parse trees; Query words; Similarity scores; Statistical significance; Word translation; Translation (languages)
Toward a unified framework for standard and update multi-document summarization,2012,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863682515&doi=10.1145%2f2184436.2184438&partnerID=40&md5=6e5c8cdb5f0f7ead7624b19955dce013,"This article presents a unified framework for extracting standard and update summaries from a set of documents. In particular, a topic modeling approach is employed for salience determination and a dynamic modeling approach is proposed for redundancy control. In the topic modeling approach for salience determination, we represent various kinds of text units, such as word, sentence, document, documents, and summary, using a single vector space model via their corresponding probability distributions over the inherent topics of given documents or a related corpus. Therefore, we are able to calculate the similarity between any two text units via their topic probability distributions. In the dynamic modeling approach for redundancy control, we consider the similarity between the summary and the given documents, and the similarity between the sentence and the summary, besides the similarity between the sentence and the given documents, for standard summarization while for update summarization, we also consider the similarity between the sentence and the history documents or summary. Evaluation on TAC 2008 and 2009 in English language shows encouraging results, especially the dynamic modeling approach in removing the redundancy in the given documents. Finally, we extend the framework to Chinese multi-document summarization and experiments show the effectiveness of our framework. © 2012 ACM.",Dynamic modeling; Latent Dirichlet allocation; Multi-document summarization; Topic modeling,Dynamic models; Probability distributions; Statistics; Dynamic modeling approach; English languages; Latent Dirichlet allocation; Modeling approach; Multi-document summarization; Redundancy control; Single vectors; Unified framework; Redundancy
Integrating generative and discriminative character-based models for chinese word segmentation,2012,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863688898&doi=10.1145%2f2184436.2184440&partnerID=40&md5=44c8bb58e47be7b41b374b1ea335158c,"Among statistical approaches to Chinese word segmentation, the word-based n-gram (generative) model and the character-based tagging (discriminative) model are two dominant approaches in the literature. The former gives excellent performance for the in-vocabulary (IV) words; however, it handles out-of-vocabulary (OOV) words poorly. On the other hand, though the latter is more robust for OOV words, it fails to deliver satisfactory performance for IV words. These two approaches behave differently due to the unit they use (word vs. character) and the model form they adopt (generative vs. discriminative). In general, characterbased approaches are more robust than word-based ones, as the vocabulary of characters is a closed set; and discriminative models are more robust than generative ones, since they can flexibly include all kinds of available information, such as future context. This article first proposes a character-based n-gram model to enhance the robustness of the generative approach. Then the proposed generative model is further integrated with the character-based discriminative model to take advantage of both approaches. Our experiments show that this integrated approach outperforms all the existing approaches reported in the literature. Afterwards, a complete and detailed error analysis is conducted. Since a significant portion of the critical errors is related to numerical/foreign strings, character-type information is then incorporated into the model to further improve its performance. Last, the proposed integrated approach is tested on cross-domain corpora, and a semi-supervised domain adaptation algorithm is proposed and shown to be effective in our experiments. © 2012 ACM.",Character-based approach; Chinese word segmentation; Discriminative model; Domain adaptation; Generative model; Model integration,Error analysis; Experiments; Integrated control; Character-based approach; Chinese word segmentation; Discriminative models; Domain adaptation; Generative model; Model integration; Stress intensity factors
Incorporating sentiment prior knowledge for weakly supervised sentiment analysis,2012,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863688889&doi=10.1145%2f2184436.2184437&partnerID=40&md5=073b34996b75ec50b5f893a550f12713,"This article presents two novel approaches for incorporating sentiment prior knowledge into the topic model for weakly supervised sentiment analysis where sentiment labels are considered as topics. One is by modifying the Dirichlet prior for topic-word distribution (LDA-DP), the other is by augmenting the model objective function through adding terms that express preferences on expectations of sentiment labels of the lexicon words using generalized expectation criteria (LDA-GE). We conducted extensive experiments on English movie review data and multi-domain sentiment dataset as well as Chinese product reviews about mobile phones, digital cameras, MP3 players, and monitors. The results show that while both LDA-DP and LDAGE perform comparably to existing weakly supervised sentiment classification algorithms, they are much simpler and computationally efficient, rendering themmore suitable for online and real-time sentiment classification on the Web. We observed that LDA-GE is more effective than LDA-DP, suggesting that it should be preferred when considering employing the topic model for sentiment analysis. Moreover, both models are able to extract highly domain-salient polarity words from text. © 2012 ACM.",Generalized expectation; Latent Dirichlet allocation; Sentiment analysis; Weakly supervised sentiment classification,Statistics; Computationally efficient; Data sets; Dirichlet prior; Generalized expectation; Latent Dirichlet allocation; Lexicon words; MP-3 players; Multi domains; Objective functions; Prior knowledge; Product reviews; Sentiment analysis; Sentiment classification; Topic model; Data mining
Error diagnosis of chinese sentences using inductive learning algorithm and decomposition-based testing mechanism,2012,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859097482&doi=10.1145%2f2090176.2090179&partnerID=40&md5=d3ac97dcd47fdd7fcf05ae79ea64b4a3,"This study presents a novel approach to error diagnosis of Chinese sentences for Chinese as second language (CSL) learners. A penalized probabilistic First-Order Inductive Learning (pFOIL) algorithm is presented for error diagnosis of Chinese sentences. The pFOIL algorithm integrates inductive logic programming (ILP), First-Order Inductive Learning (FOIL), and a penalized log-likelihood function for error diagnosis. This algorithm considers the uncertain, imperfect, and conflicting characteristics of Chinese sentences to infer error types and produce human-interpretable rules for further error correction. In a pFOIL algorithm, relation pattern background knowledge and quantized t-score background knowledge are proposed to characterize a sentence and then used for likelihood estimation. The relation pattern background knowledge captures the morphological, syntactic and semantic relations among the words in a sentence. One or two kinds of the extracted relations are then integrated into a pattern to characterize a sentence. The quantized t-score values are used to characterize various relations of a sentence for quantized t-score background knowledge representation. Afterwards, a decomposition-based testing mechanism which decomposes a sentence into background knowledge set needed for each error type is proposed to infer all potential error types and causes of the sentence. With the pFOIL method, not only the error types but also the error causes and positions can be provided for CSL learners. Experimental results reveal that the pFOIL method outperforms the C4.5, maximum entropy, and Naive Bayes classifiers in error classification. © 2012 ACM.",Chinese as second language (CSL) learner error sentence diagnosis; Inductive learning algorithm and decomposition-based testing mechanism; Inductive logic programming,Block codes; Evolutionary algorithms; Inductive logic programming (ILP); Knowledge representation; Learning algorithms; Maximum entropy methods; Semantics; Background knowledge; Chinese sentence; Error classification; Error diagnosis; Error types; First-order; Inductive learning; Likelihood estimation; Log-likelihood functions; Maximum entropy; Naive Bayes classifiers; Potential errors; Second language; Semantic relations; Testing mechanism; Learning systems
RENAR: A rule-based arabic named entity recognition system,2012,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859059733&doi=10.1145%2f2090176.2090178&partnerID=40&md5=696c6247ec5ebfded4ec25eb34be2d34,"Named entity recognition has served many natural language processing tasks such as information retrieval, machine translation, and question answering systems. Many researchers have addressed the name identification issue in a variety of languages and recently some research efforts have started to focus on named entity recognition for the Arabic language. We present a working Arabic information extraction (IE) system that is used to analyze large volumes of news texts every day to extract the named entity (NE) types person, organization, location, date, and number, as well as quotations (direct reported speech) by and about people. The named entity recognition (NER) system was not developed for Arabic, but instead a multilingual NER system was adapted to also cover Arabic. The Semitic language Arabic substantially differs from the Indo-European and Finno-Ugric languages currently covered. This article thus describes what Arabic language-specific resources had to be developed and what changes needed to be made to the rule set in order to be applicable to the Arabic language. The achieved evaluation results are generally satisfactory, but could be improved for certain entity types. © 2012 ACM.",Arabic natural language processing; Information extraction; Named entity recognition; Rule-based systems,Computational linguistics; Information retrieval; Knowledge based systems; Arabic languages; Arabic natural language processing; Evaluation results; Information Extraction; Information extraction systems; Machine translations; Named entities; Named entity recognition; NAtural language processing; NER system; Question answering systems; Research efforts; Rule based; Rule set; Semitic languages; Natural language processing systems
Handwriting recognition in indian regional scripts: A survey of offline techniques,2012,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859074677&doi=10.1145%2f2090176.2090177&partnerID=40&md5=f344bd45a6b36d64dd2f2bea3ac8a13d,"Offline handwriting recognition in Indian regional scripts is an interesting area of research as almost 460 million people in India use regional scripts. The nine major Indian regional scripts are Bangla (for Bengali and Assamese languages), Gujarati, Kannada, Malayalam, Oriya, Gurumukhi (for Punjabi language), Tamil, Telugu, and Nastaliq (for Urdu language). A state-of-the-art survey about the techniques available in the area of offline handwriting recognition (OHR) in Indian regional scripts will be of a great aid to the researchers in the subcontinent and hence a sincere attempt is made in this article to discuss the advancements reported in this regard during the last few decades. The survey is organized into different sections. A brief introduction is given initially about automatic recognition of handwriting and official regional scripts in India. The nine regional scripts are then categorized into four subgroups based on their similarity and evolution information. The first group contains Bangla, Oriya, Gujarati and Gurumukhi scripts. The second group contains Kannada and Telugu scripts and the third group contains Tamil and Malayalam scripts. The fourth group contains only Nastaliq script (Perso-Arabic script for Urdu), which is not an Indo-Aryan script. Various feature extraction and classification techniques associated with the offline handwriting recognition of the regional scripts are discussed in this survey. As it is important to identify the script before the recognition step, a section is dedicated to handwritten script identification techniques. A benchmarking database is very important for any pattern recognition related research. The details of the datasets available in different Indian regional scripts are also mentioned in the article. A separate section is dedicated to the observations made, future scope, and existing difficulties related to handwriting recognition in Indian regional scripts. We hope that this survey will serve as a compendium not only for researchers in India, but also for policymakers and practitioners in India. It will also help to accomplish a target of bringing the researchers working on different Indian scripts together. Looking at the recent developments in OHR of Indian regional scripts, this article will provide a better platform for future research activities. © 2012 ACM.",Handwritten documents; Indian regional languages; Indic script recognition; OCR; Offline handwriting recognition; Survey of offline techniques,Feature extraction; Optical character recognition; Research; Surveys; Automatic recognition; Data sets; Feature extraction and classification; Handwriting recognition; Handwritten document; Indian scripts; Indic script recognition; Offline; Offline handwriting recognition; Policy makers; Research activities; Script identification; Second group; Character recognition
Language modeling for syntax-based machine translation using tree substitution grammars: A case study on Chinese-english translation,2011,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855220723&doi=10.1145%2f2025384.2025386&partnerID=40&md5=a9c22a1a2a10c1d15eb2b484b7133d01,"The poor grammatical output of Machine Translation (MT) systems appeals syntax-based approaches within language modeling. However, previous studies showed that syntax-based language modeling using (Context- Free) Treebank Grammars was not very helpful in improving BLEU scores for Chinese-English machine translation. In this article we further study this issue in the context of Chinese-English syntax-based Statistical Machine Translation (SMT) where Synchronous Tree Substitution Grammars (STSGs) are utilized to model the translation process. In particular, we develop a Tree Substitution Grammar-based language model for syntax-based MT, and present three methods to efficiently integrate the proposed language model into MT decoding. In addition, we design a simple and effective method to adapt syntax-based language models for MT tasks. We demonstrate that the proposed methods are able to benefit a state-of-the-art syntax-based MT system. On the NIST Chinese-English MT evaluation corpora, we finally achieve an improvement of 0.6 BLEU points over the baseline. © 2011 ACM.",Machine translation; Syntax-based language model; Tree substitution grammar,Forestry; Information Retrieval; Languages; Translation; Computational linguistics; Forestry; Information theory; Syntactics; Translation (languages); Language model; Language modeling; Machine translation systems; Machine translations; MT evaluations; Statistical machine translation; Syntax-based approach; Translation process; Tree substitution grammar; Treebanks; Context free languages
Deep learning approac hes to semantic relevance modeling for chinese question-answer pairs,2011,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855238260&doi=10.1145%2f2025384.2025389&partnerID=40&md5=d464cdc22f8795e743c6cfea102966d4,"The human-generated question-answer pairs in the Web social communities are of great value for the research of automatic question-answering technique. Due to the large amount of noise information involved in such corpora, it is still a problem to detect the answers even though the questions are exactly located. Quantifying the semantic relevance between questions and their candidate answers is essential to answer detection in social media corpora. Since both the questions and their answers usually contain a small number of sentences, the relevance modeling methods have to overcome the problem of word feature sparsity. In this article, the deep learning principle is introduced to address the semantic relevance modeling task. Two deep belief networks with different architectures are proposed by us to model the semantic relevance for the question-answer pairs. According to the investigation of the textual similarity between the communitydriven question-answering (cQA) dataset and the forum dataset, a learning strategy is adopted to promote our models' performance on the social community corpora without hand-annotating work. The experimental results show that our method outperforms the traditional approaches on both the cQA and the forum corpora. © 2011 ACM.",Deep belief network; Question-answer pairs; Semantic relevance,Bayesian networks; Semantics; Data sets; Deep learning; Learning strategy; Modeling method; Modeling task; Noise information; Question Answering; Question-answer pairs; Semantic relevance; Social communities; Social media; Textual similarities; Semantic Web
"Improved Chinese-english SMT with Chinese ""DE construction classification and reordering",2011,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855222214&doi=10.1145%2f2025384.2025385&partnerID=40&md5=4c640a68e473a4baac008c5075cce7f2,"Syntactic reordering on the source side has been demonstrated to be helpful and effective for handling different word orders between source and target languages in SMT. In this article, we focus on the Chinese (DE) construction which is flexible and ubiquitous in Chinese and has many different ways to be translated into English so that it is a major source of word order differences in terms of translation quality. This article carries out the Chinese ""DE construction study for Chinese-English SMT in which we propose a new classifier model-discriminative latent variable model (DPLVM)-with new features to improve the classification accuracy and indirectly improve the translation quality compared to a log-linear classifier. The DE classifier is used to recognize DE structures in both training and test sentences of Chinese, and then perform word reordering to make the Chinese sentences better match the word order of English. In order to investigate the impact of the DE classification and reordering in the source side on different types of SMT systems (namely PB-SMT, hierarchical PB-SMT (HPB-SMT) as well as the syntax-based SMT (SAMT)), we conduct a series of experiments on NIST 2005 and 2008 test sets to verify the effectiveness of our proposed model. The experimental results show that the MT systems using the data reordered by our proposed model outperform the baseline systems by 3.01% and 4.03% relative points on the NIST 2005 test set, 4.64% and 4.62% relative points on the NIST 2008 test set in terms of BLEU score for PB-SMT and HPBSMT respectively. However, the DE classification method does not perform significantly well for SAMT. Additionally, we also conducted some experiments to evaluate our DE classification and reordering approach on the word alignment and phrase table in terms of these three types of SMT systems. © 2011 ACM.",Chinese DE construction; Dynamic probabilistic latent variable model; Log-linear model; Source-side reordering,Experiments; Regression analysis; Syntactics; Baseline systems; Chinese sentence; Classification accuracy; Classification methods; Latent variable models; Log-linear model; SMT systems; Source-side reordering; Target language; Test sets; Translation quality; Word alignment; Word orders; Word reordering; Hierarchical systems
User behaviors in related word retrieval and new word detection: A collaborative perspective,2011,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855223743&doi=10.1145%2f2025384.2025388&partnerID=40&md5=d7d1946f2b8153d1e3bcfaaa3ae0c6ee,"Nowadays, user behavior analysis and collaborative filtering have drawn a large body of research in the machine learning community. The goal is either to enhance the user experience or discover useful information hidden in the data. In this article, we conduct extensive experiments on a Chinese input method data set, which keeps the word lists that users have used. Then, from the collaborative perspective, we aim to solve two tasks in natural language processing, that is, related word retrieval and new word detection. Motivated by the observation that two words are usually highly related to each other if they co-occur frequently in users' records, we propose a novel semantic relatedness measure between words that takes both user behaviors and collaborative filtering into consideration. We utilize this measure to perform related word retrieval and new word detection tasks. Experimental results on both tasks indicate the applicability and effectiveness of our method. © 2011 ACM.",Collaborative filtering; Natural language processing; New word detection; Related words retrieval; User behaviors,Computational linguistics; Learning algorithms; Natural language processing systems; Semantics; Chinese input; Collaborative filtering; Data sets; Detection tasks; Machine learning communities; NAtural language processing; Related word; Semantic relatedness; User behavior analysis; User behaviors; User experience; Word lists; Behavioral research
Mining english-Chinese named entity pairs from comparable corpora,2011,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855251826&doi=10.1145%2f2025384.2025387&partnerID=40&md5=d64364c996728b1acb0968dda784cd9c,"Bilingual Named Entity (NE) pairs are valuable resources for many NLP applications. Since comparable corpora are more accessible, abundant and up-to-date, recent researches have concentrated on mining bilingual lexicons using comparable corpora. Leveraging comparable corpora, this research presents a novel approach to mining English-Chinese NE translations by combining multi-dimension features from various information sources for every possible NE pair, which include the transliteration model, English-Chinese matching, Chinese-English matching, translation model, length, and context vector. These features are integrated into one model with linear combination and minimum sample risk (MSR) algorithm. As for the high type-dependence of NE translation, we integrate different features according to different NE types. We experiment with the above individual feature or integrated features to mine person NE (PN) pairs, location NE (LN) pairs and organization NE (ON) pairs. When using transliteration and length to mine PN pairs, we achieve the best performance of 84.9% (F-score). The LN pairs can be mined with the features of transliteration model, length, translation model, English-Chinese matching and Chinese-English matching. And the best performance is 83.4% (F-score). The ON pairs can be mined with the features of English-Chinese matching and Chinese-English matching. It reaches the best performance with 84.1% (F-score). © 2011 ACM.",Chinese-English matching; Comparable corpora; English-Chinese matching; Mining; MSR; Named entity; Pairs; Translation model; Transliteration model,Computer science; Mining; Chinese-English matching; Comparable corpora; English-Chinese matching; MSR; Named entities; Pairs; Translation model; Transliteration models; Linguistics
Unified semantic role labeling for verbal and nominal predicates in the Chinese language,2011,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053182004&doi=10.1145%2f2002980.2002983&partnerID=40&md5=ff9e9be7ff66e3614118e1c42696355f,"This article explores unified semantic role labeling (SRL) for both verbal and nominal predicates in the Chinese language. This is done by considering SRL for both verbal and nominal predicates in a unified framework. First, we systematically examine various kinds of features for verbal SRL and nominal SRL, respectively, besides those widely used ones. Then we further improve the performance of nominal SRL with various kinds of verbal evidence, that is, merging the training instances from verbal predicates and integrating various kinds of features derived from SRL for verbal predicates. Finally, we address the issue of automatic predicate recognition, which is essential for nominal SRL. Evaluation on Chinese PropBank and Chinese NomBank shows that our unified approach significantly improves the performance, in particular that of nominal SRL. To the best of our knowledge, this is the first reported work of unified verbal and nominal SRL on Chinese PropBank and NomBank. © 2011 ACM.",Automatic predicate recognition; Nominal predicates; Semantic role labeling; Unified approach; Verbal predicates,Automatic predicate recognition; Nominal predicates; Semantic role labeling; Unified approach; Verbal predicates; Semantics
Introduction to the special issue on Chinese language processing,2011,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053190005&doi=10.1145%2f2002980.2002981&partnerID=40&md5=3f0354515272a1ccf82b348d405be8e1,[No abstract available],,
Using sublexical translations to handle the OOV problem in machine translation,2011,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053180610&doi=10.1145%2f2002980.2002986&partnerID=40&md5=7253da1bf2f5c64a0edb46e832287771,"We introduce a method for learning to translate out-of-vocabulary (OOV) words. The method focuses on combining sublexical/constituent translations of an OOV to generate its translation candidates. In our approach, wildcard searches are formulated based on our OOV analysis, aimed at maximizing the probability of retrieving OOVs' sublexical translations from existing resources of Machine Translation (MT) systems. At run-time, translation candidates of the unknown words are generated from their suitable sublexical translations and ranked based on monolingual and bilingual information. We have incorporated the OOV model into a state-of-the-art machine translation system and experimental results show that our model indeed helps to ease the impact of OOVs on translation quality, especially for sentences containing more OOVs (significant improvement). © 2011 ACM.",And sublexical translation; Language model; Machine translation; Out-of-vocabulary words; Phrase table; Translation model; Wildcard search query,Computational linguistics; Language model; Machine translations; Out-of-vocabulary words; Phrase table; Search queries; Translation model; Information theory
Employing constituent dependency information for tree kernel-based semantic relation extraction between named entities,2011,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053192107&doi=10.1145%2f2002980.2002985&partnerID=40&md5=4c137fc9b4fc6b1f4fafb6557e0ce4a8,"This article proposes a new approach to dynamically determine the tree span for tree kernel-based semantic relation extraction between named entities. The basic idea is to employ constituent dependency information in keeping the necessary nodes and their head children along the path connecting the two entities in the syntactic parse tree, while removing the noisy information from the tree, eventually leading to a dynamic syntactic parse tree. This article also explores various entity features and their possible combinations via a unified syntactic and semantic tree framework, which integrates both structural syntactic parse information and entity-related semantic information. Evaluation on the ACE RDC 2004 English and 2005 Chinese benchmark corpora shows that our dynamic syntactic parse tree much outperforms all previous tree spans, indicating its effectiveness in well representing the structural nature of relation instances while removing redundant information. Moreover, the unified parse and semantic tree significantly outperforms the single syntactic parse tree, largely due to the remarkable contributions from entity-related semantic features such as its type, subtype, mention-level as well as their bi-gram combinations. Finally, the best performance so far in semantic relation extraction is achieved via a composite kernel, which combines this tree kernel with a linear, state-of-the-art, feature-based kernel. © 2011 ACM.",Constituent dependency; Convolution tree kernel; Semantic relation extraction; Unified syntactic and semantic tree,Content based retrieval; Semantics; Syntactics; Composite kernels; Constituent dependency; Feature-based; Named entities; Redundant informations; Semantic features; Semantic information; Semantic relation extraction; Semantic relations; Semantic tree; Syntactic parse tree; Unified syntactic and semantic tree; Plant extracts
Developing position structure-based framework for chinese entity relation extraction,2011,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053183514&doi=10.1145%2f2002980.2002984&partnerID=40&md5=b5b09e73eca8d961e5c77ecf60637476,"Relation extraction is the task of finding semantic relations between two entities in text, and is often cast as a classification problem. In contrast to the significant achievements on English language, research progress in Chinese relation extraction is relatively limited. In this article, we present a novel Chinese relation extraction framework, which is mainly based on a 9-position structure. The design of this proposed structure is motivated by the fact that there are some obvious connections between relation types/subtypes and position structures of two entities. The 9-position structure can be captured with less effort than applying deep natural language processing, and is effective to relieve the class imbalance problem which often hurts the classification performance. In our framework, all involved features do not require Chinese word segmentation, which has long been limiting the performance of Chinese language processing. We also utilize some correction and inference mechanisms to further improve the classified results. Experiments on the ACE 2005 Chinese data set show that the 9-position structure feature can provide strong support for Chinese relation extraction. As well as this, other strategies are also effective to further improve the performance. © 2011 ACM.",Chinese language; Entity relation extraction; Imbalance class classification; Position structure,Computational linguistics; Natural language processing systems; Semantics; Chinese language; Chinese word segmentation; Class imbalance problems; Classification performance; Data sets; English languages; Inference mechanism; NAtural language processing; Relation extraction; Research progress; Semantic relations; Structure features; Structure-based; Text processing
Automatic treebank conversion via informed decoding-A case study on Chinese treebanks,2011,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053187288&doi=10.1145%2f2002980.2002982&partnerID=40&md5=678d59a12a6b0809adb4ddfeddd4a60a,"Treebanks are valuable resources for syntactic parsing. For some languages such as Chinese, we can obtain multiple constituency treebanks which are developed by different organizations. However, due to discrepancies of underlying annotation standards, such treebanks in general cannot be used together through direct data combination. To enlarge training data for syntactic parsing, we focus in this article on the challenge of unifying standards of disparate treebanks by automatically converting one treebank (source treebank) to fit a different standard which is exhibited by another treebank (target treebank). We propose to convert a treebank in two sequential steps which correspond to the part-of-speech level and syntactic structure level (including tree structures and grammar labels), respectively. Approaches used in both levels can be unified as an informed decoding procedure, where information derived from original annotation in a source treebank is used to guide the conversion conducted by a POS tagger (or a parser in the syntactic structure level) trained on a target treebank. We take two Chinese treebanks as a case study, and experiments on these two treebanks show significant improvements in conversion accuracy over baseline systems, especially in situations where a target treebank is small in size. © 2011 ACM.",Chinese POS tagging; Chinese syntactic parsing; Informed decoding; Treebank conversion,Decoding; Standards; Syntactics; Trees (mathematics); Baseline systems; Chinese POS tagging; Chinese syntactic parsing; Conversion accuracies; Data combination; Part Of Speech; PoS taggers; Syntactic parsing; Syntactic structure; Training data; Tree structures; Treebanks; Natural language processing systems
Weighted vote-based classifier ensemble for named entity recognition: A genetic algorithm-based approach,2011,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79960832442&doi=10.1145%2f1967293.1967296&partnerID=40&md5=f76e1ca6f7c2f351a4fcf42fbd8750a6,"In this article, we report the search capability of Genetic Algorithm (GA) to construct a weighted vote-based classifier ensemble for Named Entity Recognition (NER). Our underlying assumption is that the reliability of predictions of each classifier differs among the various named entity (NE) classes. Thus, it is necessary to quantify the amount of voting of a particular classifier for a particular output class. Here, an attempt is made to determine the appropriate weights of voting for each class in each classifier using GA. The proposed technique is evaluated for four leading Indian languages, namely Bengali, Hindi, Telugu, and Oriya, which are all resource-poor in nature. Evaluation results yield the recall, precision and F-measure values of 92.08%, 92.22%, and 92.15%, respectively for Bengali; 96.07%, 88.63%, and 92.20%, respectively for Hindi; 78.82%, 91.26%, and 84.59%, respectively for Telugu; and 88.56%, 89.98%, and 89.26%, respectively for Oriya. Finally, we evaluate our proposed approach with the benchmark dataset of CoNLL-2003 shared task that yields the overall recall, precision, and F-measure values of 88.72%, 88.64%, and 88.68%, respectively. Results also show that the vote based classifier ensemble identified by the GA-based approach outperforms all the individual classifiers, three conventional baseline ensembles, and some other existing ensemble techniques. In a part of the article, we formulate the problem of feature selection in any classifier under the single objective optimization framework and show that our proposed classifier ensemble attains superior performance to it. © 2011 ACM 1530-0226/2011/06-ART9 $10.00.",Classifier ensemble; Conditional random field; Feature selection; Genetic algorithm; Language independent named entity recognition; Maximum entropy; Support vector machine,Data processing; Feature extraction; Classifier ensembles; Conditional random field; Maximum entropy; Named entity recognition; Support vector; Genetic algorithms
A fast corpus-based stemmer,2011,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79960797180&doi=10.1145%2f1967293.1967295&partnerID=40&md5=5bd864d5525b07a99c23f15007fe881c,"Stemming is a mechanism of word form normalization that transforms the variant word forms to their common root. In an Information Retrieval system, it is used to increase the system's performance, specifically the recall and desirably the precision. Although its usefulness is shown to be mixed in languages such as English, because morphologically complex languages stemming produces a significant performance improvement. A number of linguistic rule-based stemmers are available for most European languages which employ a set of rules to get back the root word from its variants. But for Indian languages which are highly inflectional in nature, devising a linguistic rule-based stemmer needs some additional resources which are not available. We present an approach which is purely corpus based and finds the equivalence classes of variant words in an unsupervised manner. A set of experiments on four languages using FIRE, CLEF, and TREC test collections shows that our approach provides comparable results with linguistic rule-based stemmers for some languages and gives significant performance improvement for resource constrained languages such as Bengali and Marathi. © 2011 ACM 1530-0226/2011/06-ART8 $10.00.",Bengali; Corpus; Hungarian; Indian languages; Marathi; Stemming; Suffix,Equivalence classes; Information retrieval; Linguistics; Bengali; Corpus; Hungarian; Indian languages; Marathi; Stemming; Suffix; Search engines
Articulation-disordered speech recognition using speaker-adaptive acoustic models and personalized articulation patterns,2011,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79960828495&doi=10.1145%2f1967293.1967294&partnerID=40&md5=652382d09a96afc0b013a1fd453471f3,"This article presents a novel approach to speaker-adaptive recognition of speech from articulationdisordered speakers without a large amount of adaptation data. An unsupervised, incremental adaptation method is adopted for personalized model adaptation based on the recognized syllables with high recognition confidence from an automatic speech recognition (ASR) system. For articulation pattern discovery, the manually transcribed syllables and the corresponding recognized syllables are associated with each other using articulatory features. The Apriori algorithm is applied to discover the articulation patterns in the corpus, which are then used to construct a personalized pronunciation dictionary to improve the recognition accuracy of the ASR. The experimental results indicate that the proposed adaptation method achieves a syllable error rate reduction of 6.1%, outperforming the conventional adaptation methods that have a syllable error rate reduction of 3.8%. In addition, an average syllable error rate reduction of 5.04% is obtained for the ASR using the expanded pronunciation dictionary. © 2011 ACM 1530-0226/2011/06-ART7 $10.00.",Acoustical adaptation; Apriori algorithm; Articulation disorder; ASR; Pronunciation variation,Algorithms; Acoustic model; Acoustical adaptation; Adaptation methods; Apriori algorithms; Articulatory features; ASR; Automatic speech recognition system; Error rate reduction; Pattern discovery; Personalized model; Pronunciation dictionaries; Pronunciation variation; Recognition accuracy; Speech recognition
"Visually and phonologically similar characters in incorrect chinese words: Analyses, identification, and applications",2011,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79960802271&doi=10.1145%2f1967293.1967297&partnerID=40&md5=92e2d926ee0795bc6c09125379253a09,"Information about students' mistakes opens a window to an understanding of their learning processes, and helps us design effective course work to help students avoid replication of the same errors. Learning from mistakes is important not just in human learning activities; it is also a crucial ingredient in techniques for the developments of student models. In this article, we report findings of our study on 4,100 erroneous Chinese words. Seventy-six percent of these errors were related to the phonological similarity between the correct and the incorrect characters, 46% were due to visual similarity, and 29% involved both factors. We propose a computing algorithm that aims at replication of incorrect Chinese words. The algorithm extends the principles of decomposing Chinese characters with the Cangjie codes to judge the visual similarity between Chinese characters. The algorithm also employs empirical rules to determine the degree of similarity between Chinese phonemes. To show its effectiveness, we ran the algorithm to select and rank a list of about 100 candidate characters, from more than 5,100 characters, for the incorrectly written character in each of the 4,100 errors. We inspected whether the incorrect character was indeed included in the candidate list and analyzed whether the incorrect character was ranked at the top of the candidate list. Experimental results show that our algorithm captured 97% of incorrect characters for the 4,100 errors, when the average length of the candidate lists was 104. Further analyses showed that the incorrect characters ranked among the top 10 candidates in 89% of the phonologically similar errors and in 80% of the visually similar errors. © 2011 ACM 1530-0226/2011/06-ART10 $ 10.00.",Computer-assisted language learning; Error analysis of written Chinese text; Psycholinguistics; Simplified chinese; Student modeling; Traditional chinese,Algorithms; Computer aided analysis; Curricula; Error analysis; Linguistics; Speech; Students; Computer assisted language learning; Psycholinguistics; Simplified chinese; Student modeling; Traditional chinese; Computer aided instruction
Interruption point detection of spontaneous speech using inter-syllable boundary-based prosodic features,2011,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79953253065&doi=10.1145%2f1929908.1929914&partnerID=40&md5=a05ade3d90f555686a64c4b9c5fb3bc8,"This article presents a probabilistic scheme for detecting the interruption point (IP) in spontaneous speech based on inter-syllable boundary-based prosodic features. Because of the high error rate in spontaneous speech recognition, a combined acoustic model considering both syllable and subsyllable recognition units, is firstly used to determine the inter-syllable boundaries and output the recognition confidence of the input speech. Based on the finding that IPs always occur at inter-syllable boundaries, a probability distribution of the prosodic features at the current potential IP is estimated. The Conditional Random Field (CRF) model, which employs the clustered prosodic features of the current potential IP and its preceding and succeeding inter-syllable boundaries, is employed to output the IP likelihood measure. Finally, the confidence of the recognized speech, the probability distribution of the prosodic features and the CRF-based IP likelihood measure are integrated to determine the optimal IP sequence of the input spontaneous speech. In addition, pitch reset and lengthening are also applied to improve the IP detection performance. The Mandarin Conversional Dialogue Corpus is adopted for evaluation. Experimental results show that the proposed IP detection approach obtains 10.56% and 6.5% more effective results than the hidden Markov model and the Maximum Entropy model respectively under the same experimental conditions. Besides, the IP detection error rate can be further reduced by 9.15% using pitch reset and lengthening information. The experimental results confirm that the proposed model based on inter-syllable boundary-based prosodic features can effectively detect the interruption point in spontaneous Mandarin speech. © 2011.",Conditional random field; Disfluency; Feature clustering; Interruption point detection; Prosodic feature,Error detection; Hidden Markov models; Image segmentation; Probability distributions; Conditional random field; Disfluency; Feature clustering; Interruption point detection; Prosodic features; Speech recognition
Spelling correction for dialectal Arabic dictionary lookup,2011,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79953244044&doi=10.1145%2f1929908.1929911&partnerID=40&md5=e8bd601080ac96500dd6c5d9e8afb099,"The ""Did You Mean⋯ ?"" system, described in this article, is a spelling corrector for Arabic that is designed specifically for L2 learners of dialectal Arabic in the context of dictionary lookup. The authors use an orthographic density metric to motivate the need for a finer-grained ranking method for candidate words than unweighted Levenshtein edit distance. The Did You Mean⋯ ? architecture is described, and the authors show that mean reciprocal rank can be improved by tuning operation weights according to sound confusions, and by anticipating likely spelling variants. © 2011.",Arabic dialects; Dictionary lookup; Error correction for non-native language learners; Iraqi Arabic; Spelling correction; Weighted finite-state transducers,Transducers; Arabic dialects; Dictionary lookup; Iraqi Arabic; Spelling correction; Weighted finite-state transducers; Learning systems
Machine translation errors: English and Iraqi Arabic,2011,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79953239399&doi=10.1145%2f1929908.1929910&partnerID=40&md5=11fc73f303bc0d98b273b18abea61253,Errors in machine translations of English-Iraqi Arabic dialogues were analyzed using the methods developed for the Human Translation Error Rate measure (HTER). Human annotations were used to refine the Translation Error Rate (TER) annotations. The analyses were performed on approximately 100 translations into each language from four translation systems. Results include high frequencies of pronoun errors and errors involving the copula in translations to English. High frequencies of errors in subject/person inflection and closed-word classes characterized translations to Iraqi Arabic. There were similar frequencies of word order errors in both translation directions and low frequencies of polarity errors. The problems associated with many errors can be predicted from structural differences between the two languages. Also problematic is the need to insert lexemes not present in the source or vice versa. Some problems associated with deictic elements like pronouns will require knowledge of the discourse context to resolve. © 2011.,Arabic; English; Error analysis; Evaluation; Statistical machine translation,Error analysis; Information theory; Linguistics; Arabic; Discourse context; English; Error rate; Evaluation; High frequency; Human annotations; Low frequency; Machine translations; Statistical machine translation; Structural differences; Translation systems; Word order errors; Translation (languages)
Exploiting separation of closed-class categories for Arabic tokenization and part-of-speech tagging,2011,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79953251586&doi=10.1145%2f1929908.1929912&partnerID=40&md5=f33246d3d0191eecf24d53dbe4834b7a,"Research on the problem of morphological disambiguation of Arabic has noted that techniques developed for lexical disambiguation in English do not easily transfer over, since the affixation present in Arabic creates a very different tag set than for English, encoding both inflectional morphology and more complex tokenization sequences. This work takes a new approach to this problem based on a distinction between the open-class and closed-class categories of tokens, which differ both in their frequencies and in their possible morphological affixations. This separation simplifies the morphological analysis problem considerably, making it possible to use a Conditional Random Field model for joint tokenization and ""core"" part-of-speech tagging of the open-class items, while the closed-class items are handled by regular expressions. This work is therefore situated between data-driven approaches and those that use a morphological analyzer. For the tasks of tokenization and core part-of-speech tagging, the resulting system outperforms, on the given test set, a system that incorporates a morphological analyzer. We also evaluate the effects of the differences on parser performance when the tagger output is used for parser input. © 2011.",Arabic; Morphological analysis,Arabic; Conditional random field; Core part; Data-driven approach; Lexical disambiguation; Morphological analysis; Morphological analyzer; Morphological disambiguation; New approaches; Part of speech tagging; Problem-based; Regular expressions; Test sets; Tokenization; Morphology
Automatic detection of arabic non-anaphoric pronouns for improving anaphora resolution,2011,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79953241664&doi=10.1145%2f1929908.1929913&partnerID=40&md5=a8bdf42f120c4b18f0ac50990ca62882,"Anaphora resolution is one of themost difficult tasks in NLP. The ability to identify non-referential pronouns before attempting an anaphora resolution task would be significant, since the system would not have to attempt resolving such pronouns and hence end up with fewer errors. In addition, the number of nonreferential pronouns has been found to be non-trivial in many domains. The task of detecting non-referential pronouns could also be incorporated into a part-of-speech tagger or a parser, or treated as an initial step in semantic interpretation. In this article, I describe a machine learning method for identifying non-referential pronouns in an annotated subsegment of the Penn Arabic Treebank using three different feature settings. I achieve an accuracy of 97.22% with 52 different features extracted from a small window size of -5/+5 tokens surrounding each potentially non-referential pronoun. © 2011.",Anaphora resolution; Arabic expletive pronouns; Arabic non-referential pronouns; Arabic pleonastic pronouns; Memory-based learning,Semantics; Anaphora resolution; Arabic expletive pronouns; Arabic non-referential pronouns; Arabic pleonastic pronouns; Memory-based learning; Learning systems
Introduction to the special issue on Arabic computational linguistics,2011,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79953245228&doi=10.1145%2f1929908.1929909&partnerID=40&md5=b99209b17bcdae753de7c0729de3c233,[No abstract available],,
An information-extraction system for Urdu - A resource-poor language,2010,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78650642315&doi=10.1145%2f1838751.1838754&partnerID=40&md5=6c156c6f7724d3cb0539cd7040fb9cb0,"There has been an increase in the amount of multilingual text on the Internet due to the proliferation of news sources and blogs. The Urdu language, in particular, has experienced explosive growth on theWeb. Text mining for information discovery, which includes tasks such as identifying topics, relationships and events, and sentiment analysis, requires sophisticated natural language processing (NLP). NLP systems begin with modules such as word segmentation, part-of-speech tagging, and morphological analysis and progress to modules such as shallow parsing and named entity tagging. While there have been considerable advances in developing such comprehensive NLP systems for English, the work for Urdu is still in its infancy. The tasks of interest in Urdu NLP includes analyzing data sources such as blogs and comments to news articles to provide insight into social and human behavior. All of this requires a robust NLP system. The objective of this work is to develop an NLP infrastructure for Urdu that is customizable and capable of providing basic analysis on which more advanced information extraction tools can be built. This system assimilates resources from various online sources to facilitate improved named entity tagging and Urdu-to-English transliteration. The annotated data required to train the learning models used here is acquired by standardizing the currently limited resources available for Urdu. Techniques such as bootstrap learning and resource sharing from a syntactically similar language, Hindi, areexplored to augment the available annotated Urdu data. Each of the new Urdu text processing modules has been integrated into a general text-mining platform. The evaluations performed demonstrate that the accuracies have either met or exceeded the state of the art. © 2010 ACM.",Bootstrap learning; Named entity tagging; Part of speech tagging; Shallow parsing; Text mining; Transliterations; Urdu natural language processing,Behavioral research; Computational linguistics; Data mining; Internet; Robot learning; Text processing; Word processing; Bootstrap learning; Named entity tagging; NAtural language processing; Part of speech tagging; Shallow parsing; Text mining; Transliterations; Natural language processing systems
Compositional machine transliteration,2010,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78650635618&doi=10.1145%2f1838751.1838752&partnerID=40&md5=cfcf9e0058fc75daf1252a17fead4399,"Machine transliteration is an important problem in an increasingly multilingual world, as it plays a critical role in many downstream applications, such as machine translation or crosslingual information retrieval systems. In this article, we propose compositional machine transliteration systems, where multiple transliteration components may be composed either to improve existing transliteration quality, or to enable transliteration functionality between languages even when no direct parallel names corpora exist between them. Specifically, we propose two distinct forms of composition: serial and parallel. Serial compositional system chains individual transliteration components, say, X → Y and Y → Z systems, to provide transliteration functionality, X → Z. In parallel composition evidence from multiple transliteration paths between X → Z are aggregated for improving the quality of a direct system. We demonstrate the functionality and performance benefits of the compositional methodology using a state-of-the-art machine transliteration framework in English and a set of Indian languages, namely, Hindi, Marathi, and Kannada. Finally, we underscore the utility and practicality of our compositional approach by showing that a CLIR system integrated with compositional transliteration systems performs consistently on par with, and sometimes better than, that integrated with a direct transliteration system. © 2010 ACM.",Compositional machine transliteration; Crosslingual information retrieval; Machine transliteration; Multiple evidence; Resource reusage; Transliterability,Information retrieval; Information retrieval systems; Information theory; Crosslingual information retrieval; Machine transliteration; Multiple evidence; Resource reusage; Transliterability; Machine components
The state of the journal,2010,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77957015912&doi=10.1145%2f1838745.1838750&partnerID=40&md5=4b93a97d5cf63a6a928c9e9e9f73c2b6,[No abstract available],,
"Comparative study of indexing and search strategies for the Hindi, Marathi, and Bengali languages",2010,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77956992798&doi=10.1145%2f1838745.1838748&partnerID=40&md5=3d4ce23d14245c75692cb8161e72a927,"The main goal of this article is to describe and evaluate various indexing and search strategies for the Hindi, Bengali, and Marathi languages. These three languages are ranked among the world's 20 most spoken languages and they share similar syntax, morphology, and writing systems. In this article we examine these languages from an Information Retrieval (IR) perspective through describing the key elements of their inflectional and derivational morphologies, and suggest a light and more aggressive stemming approach based on them. In our evaluation of these stemming strategies we make use of the FIRE 2008 test collections, and then to broaden our comparisons we implement and evaluate two language independent indexing methods: the n-gram and trunc-n (truncation of the first n letters). We evaluate these solutions by applying our various IR models, including the Okapi, Divergence from Randomness (DFR) and statistical language models (LM) together with two classical vector-space approaches: tf idf and Lnu-ltc Experiments performed with all three languages demonstrate that the I(n2 C2 model derived from the Divergence from Randomness paradigm tends to provide the best mean average precision (MAP). Our own tests suggest that improved retrieval effectiveness would be obtained by applying more aggressive stemmers, especially those accounting for certain derivational suffixes, compared to those involving a light stemmer or ignoring this type of word normalization procedure. Comparisons between no stemming and stemming indexing schemes shows that performance differences are almost always statistically significant. When, for example, an aggressive stemmer is applied, the relative improvements obtained are ∼28% for the Hindi language, ∼42% for Marathi, and ∼18% for Bengali, as compared to a no-stemming approach. Based on a comparison of word-based and language-independent approaches we find that the trunc-4 indexing scheme tends to result in performance levels statistically similar to those of an aggressive stemmer, yet better than the 4-gram indexing scheme. A query-by-query analysis reveals the reasons for this, and also demonstrates the advantage of applying a stemming or a trunc-4 indexing scheme. © 2010 ACM.",Bengali language; Hindi language; Indic languages; Marathi language; Natural language processing with Indo-European languages; Search engines for Asian languages; Stemmer,Computational linguistics; Information retrieval; Morphology; Natural language processing systems; Random processes; Search engines; Vector spaces; Bengali language; Hindi language; Indic languages; Marathi languages; NAtural language processing; Stemmer; Indexing (of information)
Introduction to the special issue on Indian language information retrieval Part I,2010,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77956996269&doi=10.1145%2f1838745.1838746&partnerID=40&md5=b5adfd5f1d1fb6d40469e03a659c7bd2,"The special issue of Transactions on Asian Language Information Processing (TALIP) discusses six research papers on Indian language Information Retrieval (IR). The first article, 'The FIRE 2008 Evaluation Exercise' by Prasenjit Majumder and co-workers, provides the motivation and background for the FIRE initiative. It describes how the FIRE 2008 test collection was constructed, and summarizes the approaches adopted by various participants. The authors also discuss the limitations of the datasets, and outline the tasks planned for the next iteration of FIRE. Leveling and Jones in their article,'Sub-word Indexing and Blind Relevance Feedback for English, Bengali, Hindi, and Marathi IR,' try a corpus-based stemming approach based on morpheme induction, as well as sub-word indexing units. The final article, An Information Extraction System for Urdu - A Resource Poor Language' by Smruthi, addresses Natural Language Processing (NLP) tasks for Urdu, a language that is not addressed by any of the other articles.",,Computational linguistics; Feedback; Indexing (of information); Industrial research; Information retrieval; Information retrieval systems; Natural language processing systems; Asian languages; Data sets; Indexing units; Indian languages; Information extraction systems; Information processing; Natural language processing; Relevance feedback; Research papers; Test Collection; Fires
The fire 2008 evaluation exercise,2010,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77956988031&doi=10.1145%2f1838745.1838747&partnerID=40&md5=d1cb78583db0e01ee94fa42cb7bd4e45,"The aim of the Forum for Information Retrieval Evaluation (FIRE) is to create an evaluation framework in the spirit of TREC (Text REtrieval Conference), CLEF (Cross-Language Evaluation Forum), and NTCIR (NII Test Collection for IR Systems), for Indian language Information Retrieval. The first evaluation exercise conducted by FIRE was completed in 2008. This article describes the test collections used at FIRE 2008, summarizes the approaches adopted by various participants, discusses the limitations of the datasets, and outlines the tasks planned for the next iteration of FIRE. © 2010 ACM.",Evaluation; Indian languages; Information retrieval,Fires; Linguistics; Nickel compounds; Cross-language evaluation forums; Data sets; Evaluation; Evaluation framework; Indian languages; Test Collection; Text retrieval conferences; Information retrieval
"Sub-word indexing and blind relevance feedback for English, Bengali, Hindi, and Marathi IR",2010,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77956997366&doi=10.1145%2f1838745.1838749&partnerID=40&md5=74699db1781b136ec26f5bd8099c1608,"The Forum for Information Retrieval Evaluation (FIRE) provides document collections, topics, and relevance assessments for information retrieval (IR) experiments on Indian languages. Several research questions are explored in this article: 1) How to create create a simple, languageindependent corpus-based stemmer, 2) How to identify sub-words and which types of sub-words are suitable as indexing units, and 3) How to apply blind relevance feedback on sub-words and how feedback term selection is affected by the type of the indexing unit. More than 140 IR experiments are conducted using the BM25 retrieval model on the topic titles and descriptions (TD) for the FIRE 2008 English, Bengali, Hindi, and Marathi document collections. The major findings are: The corpus-based stemming approach is effective as a knowledge-light term conflation step and useful in the case of few language-specific resources. For English, the corpus-based stemmer performs nearly as well as the Porter stemmer and significantly better than the baseline of indexing words when combined with query expansion. In combination with blind relevance feedback, it also performs significantly better than the baseline for Bengali and Marathi IR. Sub-words such as consonant-vowel sequences and word prefixes can yield similar or better performance in comparison to word indexing. There is no best performing method for all languages. For English, indexing using the Porter stemmer performs best, for Bengali and Marathi, overlapping 3-grams obtain the best result, and for Hindi, 4-prefixes yield the highest MAP. However, in combination with blind relevance feedback using 10 documents and 20 terms, 6-prefixes for English and 4-prefixes for Bengali, Hindi, and Marathi IR yield the highest MAP. Sub-word identification is a general case of decompounding. It results in one or more index terms for a single word form and increases the number of index terms but decreases their average length. The corresponding retrieval experiments show that relevance feedback on sub-words benefits from selecting a larger number of index terms in comparison with retrieval on word forms. Similarly, selecting the number of relevance feedback terms depending on the ratio of word vocabulary size to sub-word vocabulary size almost always slightly increases information retrieval effectiveness compared to using a fixed number of terms for different languages. © 2010 ACM.",Blind relevance feedback; Evaluation; FIRE; Information retrieval; Stemming; Sub-word indexing,Experiments; Fires; Indexing (of information); Information retrieval; Linguistics; Natural language processing systems; Average length; Document collection; Evaluation; Fixed numbers; Index terms; Indexing units; Indian languages; Query expansion; Relevance assessments; Relevance feedback; Research questions; Retrieval effectiveness; Retrieval models; Stemming; Sub-word indexing; Term selection; Vocabulary size; Word identification; Feedback
A linguistically inspired statistical model for chinese punctuation generation,2010,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77953784659&doi=10.1145%2f1781134.1781136&partnerID=40&md5=4371f8b0e8c7887c79fa8c4f68be3222,"This article investigates a relatively underdeveloped subject in natural language processing-the generation of punctuation marks. From a theoretical perspective, we study 16 Chinese punctuation marks as defined in the Chinese national standard of punctuation usage, and categorize these punctuation marks into three different types according to their syntactic properties. We implement a three-tier maximum entropy model incorporating linguistically-motivated features for generating the commonly used Chinese punctuation marks in unpunctuated sentences output by a surface realizer. Furthermore, we present a method to automatically extract cue words indicating sentence-final punctuation marks as a specialized feature to construct a more precise model. Evaluating on the Penn Chinese Treebank data, the MaxEnt model achieves an f -score of 79.83% for punctuation insertion and 74.61% for punctuation restoration using gold data input, 79.50% for insertion and 73.32% for restoration using parser-based imperfect input. The experiments show that the MaxEnt model significantly outperforms a baseline 5-gram language model that scores 54.99% for punctuation insertion and 52.01% for restoration. We show that our results are not far from human performance on the same task with human insertion f -scores in the range of 81-87% and human restoration in the range of 71-82%. Finally, a manual error analysis of the generation output shows that close to 40% of the mismatched punctuation marks do in fact result in acceptable choices, a fact obscured in the automatic string-matching based evaluation scores. © 2010 ACM 1530-0226/2010/06-ART5 $10.00.",Chinese punctuation marks; Maximum entropy model; Sentence realization,Computational linguistics; Entropy; Error analysis; Feature extraction; Natural language processing systems; Chinese national standard; Data input; F-score; Human performance; Language model; MaxEnt models; Maximum entropy models; NAtural language processing; Penn Chinese Treebank; Punctuation marks; Statistical models; String matching; Syntactic properties; Restoration
Topic-dependent language model with voting on noun history,2010,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77953792478&doi=10.1145%2f1781134.1781137&partnerID=40&md5=51b0d59e4ff3081b59edfc99327cfef9,"Language models (LMs) are an important field of study in automatic speech recognition (ASR) systems. LM helps acoustic models find the corresponding word sequence of a given speech signal. Without it, ASR systems would not understand the language and it would be hard to find the correct word sequence. During the past few years, researchers have tried to incorporate long-range dependencies into statistical word-based n-gram LMs. One of these long-range dependencies is topic. Unlike words, topic is unobservable. Thus, it is required to find the meanings behind the words to get into the topic. This research is based on the belief that nouns contain topic information. We propose a new approach for a topic-dependent LM, where the topic is decided in an unsupervised manner. Latent Semantic Analysis (LSA) is employed to reveal hidden (latent) relations among nouns in the context words. To decide the topic of an event, a fixed size word history sequence (window) is observed, and voting is then carried out based on noun class occurrences weighted by a confidence measure. Experiments were conducted on an English corpus and a Japanese corpus: The Wall Street Journal corpus and Mainichi Shimbun (Japanese newspaper) corpus. The results show that our proposed method gives better perplexity than the comparative baselines, including a word-based/class-based n-gram LM, their interpolated LM, a cache-based LM, a topic-dependent LM based on n-gram, and a topic-dependent LM based on Latent Dirichlet Allocation (LDA). The n-best list rescoring was conducted to validate its application in ASR systems. © 2010 ACM 1530-0226/2010/06-ART5 $10.00.",Language model; Latent semantic analysis; Perplexity; Speech recognition; Topic dependent,Computational linguistics; Lagrange multipliers; Semantics; Acoustic model; Automatic speech recognition system; Confidence Measure; Context-word; Fixed size; Language model; Latent dirichlet allocations; Latent Semantic Analysis; Long-range dependencies; N-best list; New approaches; Speech signals; Unobservable; Wall Street Journal; Speech recognition
A unified character-based tagging framework for chinese word segmentation,2010,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77953745128&doi=10.1145%2f1781134.1781135&partnerID=40&md5=c4fd9fcaad65a8bc2f22a20003ad336f,"Chinese word segmentation is an active area in Chinese language processing though it is suffering from the argument about what precisely is a word in Chinese. Based on corpus-based segmentation standard, we launched this study. In detail, we regard Chinese word segmentation as a character-based tagging problem. We show that there has been a potent trend of using a character-based tagging approach in this field. In particular, learning from segmented corpus with or without additional linguistic resources is treated in a unified way in which the only difference depends on how the feature template set is selected. It differs from existing work in that both feature template selection and tag set selection are considered in our approach, instead of the previous feature template focus only technique. We show that there is a significant performance difference as different tag sets are selected. This is especially applied to a six-tag set, which is good enough for most current segmented corpora. The linguistic meaning of a tag set is also discussed. Our results show that a simple learning system with six n-gram feature templates and a six-tag set can obtain competitive performance in the cases of learning only from a training corpus. In cases when additional linguistic resources are available, an ensemble learning technique, assistant segmenter, is proposed and its effectiveness is verified. Assistant segmenter is also proven to be an effective method as segmentation standard adaptation that outperforms existing ones. Based on the proposed approach, our system provides state-of-the-art performance in all 12 corpora of three international Chinese word segmentation bakeoffs. © 2010 ACM 1530-0226/2010/06-ART5 $10.00.",Assistant segmenter; Character-based tagging method; Chinese word segmentation; Conditional random field; Tag set selection,Image segmentation; Learning algorithms; Linguistics; Software agents; Active area; Chinese language; Chinese word segmentation; Conditional random field; Ensemble learning; Feature template; Linguistic resources; Segmenter; State-of-the-art performance; Tagging problem; Training corpus; Feature extraction
Inducing morphemes using light knowledge,2010,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77950364700&doi=10.1145%2f1731035.1731038&partnerID=40&md5=81ee2bbc7f83d2831e765b9083dd0cd1,"Allomorphic variation, or form variation among morphs with the same meaning, is a stumbling block to morphological induction (MI). To address this problem, we present a hybrid approach that uses a small amount of linguistic knowledge in the form of orthographic rewrite rules to help refine an existing MI-produced segmentation. Using rules, we derive underlying analyses of morphs-generalized with respect to contextual spelling differences-from an existing surface morph segmentation, and from these we learn a morpheme-level segmentation. To learn morphemes, we have extended the Morfessor segmentation algorithm [Creutz and Lagus 2004; 2005; 2006] by using rules to infer possible underlying analyses from surface segmentations. A segmentation produced by Morfessor Categories-MAP Software v. 0.9.2 is used as input to our procedure and as a baseline that we evaluate against. To suggest analyses for our procedure, a set of languagespecific orthographic rules is needed. Our procedure has yielded promising improvements for English and Turkish over the baseline approach when tested on the Morpho Challenge 2005 and 2007 style evaluations. On the Morpho Challenge 2007 test evaluation, we report gains over the current best unsupervised contestant for Turkish, where our technique shows a 2.5% absolute F-score improvement. © 2010 ACM.",Allomorphy; Computational linguistics; Machine learning; Morphological induction,Computational linguistics; Learning systems; F-score; Hybrid approach; Linguistic knowledge; Machine-learning; Rewrite rules; Segmentation algorithms; Stumbling blocks; Surface morph; Surface segmentation; Test evaluation; Turkishs; Learning algorithms
Identification of soundbite and its speaker name using transcripts of broadcast news speech,2010,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77950351072&doi=10.1145%2f1731035.1731037&partnerID=40&md5=ce905b0fa142c653c40cc3cdc869c4c4,"This article presents a pipeline framework for identifying soundbite and its speaker name from Mandarin broadcast news transcripts. Both of the two modules, soundbite segment detection and soundbite speaker name recognition, are based on a supervised classification approach using multiple linguistic features. We systematically evaluated performance for each module as well as the entire system, and investigated the effect of using speech recognition (ASR) output and automatic sentence segmentation. We found that both of the two components impact the pipeline system, with more degradation in the entire system performance due to automatic speaker name recognition errors than soundbite segment detection. In addition, our experimental results show that using ASR output degrades the system performance significantly, and that using automatic sentence segmentation greatly impacts soundbite detection, but has much less effect on speaker name recognition. © 2010 ACM.",Automatic speech recognition; Sentence segmentation; Soundbite detection; Speaker name recognition,Error detection; Remelting; Automatic speech recognition; Broadcast news; Entire system; Linguistic features; Name recognition; Pipe-line systems; Sentence segmentation; Supervised classification; Two-component; Speech recognition
Mining synonymous transliterations from the world wide web,2010,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77950347532&doi=10.1145%2f1731035.1731036&partnerID=40&md5=e1f857ffe747cd8db208b631058bc5c1,"The World Wide Web has been considered one of the important sources for information. Using search engines to retrieve Web pages can gather lots of information, including foreign information. However, to be better understood by local readers, proper names in a foreign language, such as English, are often transliterated to a local language such as Chinese. Due to different translators and the lack of translation standard, translating foreign proper nouns may result in different transliterations and pose a notorious headache. In particular, it may cause incomplete search results. Using one transliteration as a query keyword will fail to retrieve the Web pages which use a different word as the transliteration. Consequently, important information may be missed. We present a framework for mining synonymous transliterations as many as possible from the Web for a given transliteration. The results can be used to construct a database of synonymous transliterations which can be utilized for query expansion so as to alleviate the incomplete search problem. Experimental results show that the proposed framework can effectively retrieve the set of snippets which may contain synonymous transliterations and then extract the target terms. Most of the extracted synonymous transliterations have higher rank of similarity to the input transliteration compared to other noise terms. © 2010 ACM.",Chinese transliteration; Cross-lingual information retrieval; Synonymous transliteration; Text mining; Web mining,Content based retrieval; Information retrieval; Linguistics; Natural language processing systems; Search engines; Translation (languages); Cross-lingual information retrieval; Foreign language; Local language; Noise terms; Proper nouns; Query expansion; Search problem; Search results; Text mining; Web Mining; Web page; World Wide Web
A reexamination of MRD-based word sense disambiguation,2010,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77950358372&doi=10.1145%2f1731035.1731039&partnerID=40&md5=e3be0024609f8ad77062f7adaa044348,"This article reconsiders the task of MRD-based word sense disambiguation, in extending the basic Lesk algorithm to investigate the impact on WSD performance of different tokenization schemes and methods of definition extension. In experimentation over the Hinoki Sensebank and the Japanese Senseval-2 dictionary task, we demonstrate that sense-sensitive definition extension over hyponyms, hypernyms, and synonyms, combined with definition extension and word tokenization leads to WSD accuracy above both unsupervised and supervised baselines. In doing so, we demonstrate the utility of ontology induction and establish new opportunities for the development of baseline unsupervised WSD methods. © 2010 ACM.",Japanese; Machine-readable dictionary; Word sense disambiguation,Ontology; Hypernyms; Hyponyms; Machine-readable dictionaries; New opportunities; Tokenization; Word Sense Disambiguation; Natural language processing systems
Transliteration for resource-scarce languages,2010,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78650632527&doi=10.1145%2f1838751.1838753&partnerID=40&md5=51f8bf69b2023a7681849a6e93e03233,"Today, parallel corpus-based systems dominate the transliteration landscape. But the resourcescarce languages do not enjoy the luxury of large parallel transliteration corpus. For these languages, rule-based transliteration is the only viable option. In this article, we show that by properly harnessing the monolingual resources in conjunction with manually created rule base, one can achieve reasonable transliteration performance. We achieve this performance by exploiting the power of Character Sequence Modeling (CSM), which requires only monolingual resources. We present the results of our rule-based system for Hindi to English, English to Hindi, and Persian to English transliteration tasks. We also perform extrinsic evaluation of transliteration systems in the context of Cross Lingual Information Retrieval. Another important contribution of our work is to explain the widely varying accuracy numbers reported in transliteration literature, in terms of the entropy of the language pairs and the datasets involved. © 2010 ACM.",Character sequence modeling; Cross entropy; Prefix-based partial match (PPM); Resource-scarce languages; Transliteration,Modeling languages; Search engines; Character sequence modeling; Cross entropy; Parallel corpora; Partial matches; Performance; Prefix based; Prefix-based partial match; Resource-scarce language; Sequence models; Transliteration; Entropy
Cross-language information propagation for arabic mention detection,2009,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-75649129026&doi=10.1145%2f1644879.1644884&partnerID=40&md5=1076ecc540e3c69c7da65ca4a08ece32,"In the last two decades, significant effort has been put into annotating linguistic resources in several languages. Despite this valiant effort, there are still many languages left that have only small amounts of such resources. The goal of this article is to present and investigate a method of propagating information (specifically mention detection) from a resource-rich language into a relatively resource-poor language such as Arabic. Part of the investigation is to quantify the contribution of propagating information in different conditions based on the availability of resources in the target language. Experiments on the language pair Arabic-English show that one can achieve relatively decent performance by propagating information from a language with richer resources such as English into Arabic alone (no resources or models in the source language Arabic). Furthermore, results show that propagated features from English do help improve the Arabic system performance even when used in conjunction with all feature types built from the source language. Experiments also show that using propagated features in conjunction with lexically derived features only (as can be obtained directly from a mention annotated corpus) brings the system performance at the one obtained in the target language by using feature derived from many linguistic resources, therefore improving the system when such resources are not available. In addition to Arabic-English language pair, we investigate the effectiveness of our approach on other language pairs such as Chinese-English and Spanish-English. © 2009 ACM.",Arabic information extraction; Arabic mention detection,Experiments; Information analysis; Query languages; Cross-language information; English languages; Feature types; Information Extraction; Language pairs; Linguistic resources; Resource-Rich; Source language; Target language; Linguistics
Arabic natural language processing: Challenges and solutions,2009,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-75649114161&doi=10.1145%2f1644879.1644881&partnerID=40&md5=5ca3cc6d33418179618ee2d48ee89030,"The Arabic language presents researchers and developers of natural language processing (NLP) applications for Arabic text and speech with serious challenges. The purpose of this article is to describe some of these challenges and to present some solutions that would guide current and future practitioners in the field of Arabic natural language processing (ANLP). We begin with general features of the Arabic language in Sections 1, 2, and 3 and then we move to more specific properties of the language in the rest of the article. In Section 1 of this article we highlight the significance of the Arabic language today and describe its general properties. Section 2 presents the feature of Arabic Diglossia showing how the sociolinguistic aspects of the Arabic language differ from other languages. The stability of Arabic Diglossia and its implications for ANLP applications are discussed and ways to deal with this problematic property are proposed. Section 3 deals with the properties of the Arabic script and the explosion of ambiguity that results from the absence of short vowel representations and overt case markers in contemporary Arabic texts. We present in Section 4 specific features of the Arabic language such as the nonconcatenative property of Arabic morphology, Arabic as an agglutinative language, Arabic as a pro-drop language, and the challenge these properties pose to ANLP. We also present solutions that have already been adopted by some pioneering researchers in the field. In Section 5 we point out to the lack of formal and explicit grammars of Modern Standard Arabic which impedes the progress of more advanced ANLP systems. In Section 6 we draw our conclusion. © 2009 ACM.",Arabic dialects; Arabic script; Modern Standard Arabic,Computational linguistics; Image retrieval; Natural language processing systems; Standards; Agglutinative language; Arabic languages; Arabic natural language processing; Arabic scripts; Arabic texts; Modern standards; Natural language processing; Non-concatenative; Query languages
Introduction to the special issue on arabic natural language processing,2009,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-75649110023&doi=10.1145%2f1644879.1644880&partnerID=40&md5=433859400c3c5a29b9d636b4c43ed291,[No abstract available],,
Sura length and lexical probability estimation in cluster analysis of the Qur'an,2009,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-75649127569&doi=10.1145%2f1644879.1644886&partnerID=40&md5=d92f1181ea463e10c4c44afd80887663,"Thabet [2005] applied cluster analysis to the Qur'an in the hope of generating a classification of the (suras) that is useful for understanding of its thematic structure. The result was positive, but variation in (sura) length was a problem because clustering of the shorter was found to be unreliable. The present discussion addresses this problem in four parts. The first part summarizes Thabet's work. The second part argues that unreliable clustering of the shorter is a consequence of poor estimation of lexical population probabilities in those. The third part proposes a solution to the problem based on calculation of a minimum length threshold using concepts from statistical sampling theory followed by selection of and lexical variables based on that threshold. The fourth part applies the proposed solution to a reanalysis of the Qur'an. © 2009 ACM.",Arabic natural language processing; Cluster analysis; Document length normalization; Lexical probability estimation; Qur'an; Sampling,Cluster analysis; Computational linguistics; Estimation; Natural language processing systems; Probability density function; Sampling; Speech recognition; Arabic natural language processing; Document length normalization; Lexical probability; Problem-based; Reanalysis; Statistical sampling; Thematic structures; Probability
Automatic speech-to-text transcription in arabic,2009,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-75649084128&doi=10.1145%2f1644879.1644885&partnerID=40&md5=3d8f552e7e0e7261e0f1616bc6d0bbef,"The Arabic language presents a number of challenges for speech recognition, arising in part from the significant differences in the spoken and written forms, in particular the conventional form of texts being non-vowelized. Being a highly inflected language, the Arabic language has a very large lexical variety and typically with several possible (generally semantically linked) vowelizations for each written form. This article summarizes research carried out over the last few years on speech-to-text transcription of broadcast data in Arabic. The initial research was oriented toward processing of broadcast news data in Modern Standard Arabic, and has since been extended to address a larger variety of broadcast data, which as a consequence results in the need to also be able to handle dialectal speech. While standard techniques in speech recognition have been shown to apply well to the Arabic language, taking into account language specificities help to significantly improve system performance. © 2009 ACM.",Arabic language processing; Automatic speech recognition; Mophological decomposition; Speech processing; Speech-to-text transcription,Character recognition; Industrial research; Linguistics; Speech processing; Transcription; Arabic language processing; Arabic languages; Automatic speech recognition; Broadcast data; Broadcast news; Modern standards; Mophological decomposition; Speech recognition
Morphology-based segmentation combination for arabic mention detection,2009,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-75649120307&doi=10.1145%2f1644879.1644883&partnerID=40&md5=553fbc6cc7b40e4b7777a49fca5825eb,"The Arabic language has a very rich/complex morphology. Each Arabic word is composed of zero or more prefixes, one stem and zero or more suffixes. Consequently, the Arabic data is sparse compared to other languages such as English, and it is necessary to conduct word segmentation before any natural language processing task. Therefore, the word-segmentation step is worth a deeper study since it is a preprocessing step which shall have a significant impact on all the steps coming afterward. In this article, we present an Arabic mention detection system that has very competitive results in the recent Automatic Content Extraction (ACE) evaluation campaign. We investigate the impact of different segmentation schemes on Arabic mention detection systems and we show how these systems may benefit from more than one segmentation scheme. We report the performance of several mention detection models using different kinds of possible and known segmentation schemes for Arabic text: punctuation separation, Arabic Treebank, and morphological and character-level segmentations. We show that the combination of competitive segmentation styles leads to a better performance. Results indicate a statistically significant improvement when Arabic Treebank and morphological segmentations are combined. © 2009 ACM.",Arabic information extraction; Arabic mention detection; Arabic segmentation,Computational linguistics; Information analysis; Morphology; Query languages; Arabic languages; Arabic texts; Automatic content; Detection models; Detection system; Information Extraction; Morphological segmentation; NAtural language processing; Pre-processing step; Segmentation scheme; Significant impacts; Treebanks; Word segmentation; Natural language processing systems
Discriminative phrase-based models for arabic machine translation,2009,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-75649145317&doi=10.1145%2f1644879.1644882&partnerID=40&md5=f080b7808f2ca1081bc59de333264747,"A design for an Arabic-to-English translation system is presented. The core of the system implements a standard phrase-based statistical machine translation architecture, but it is extended by incorporating a local discriminative phrase selection model to address the semantic ambiguity of Arabic. Local classifiers are trained using linguistic information and context to translate a phrase, and this significantly increases the accuracy in phrase selection with respect to the most frequent translation traditionally considered. These classifiers are integrated into the translation system so that the global task gets benefits from the discriminative learning. As a result, we obtain significant improvements in the full translation task at the lexical, syntactic, and semantic levels as measured by an heterogeneous set of automatic evaluation metrics. © 2009 ACM.",Arabic; Discriminative learning; English; Statistical machine translation,Classifiers; Information theory; Linguistics; Semantics; Speech transmission; Automatic evaluation; Discriminative learning; Linguistic information; Local classifier; Machine translations; Phrase-based statistical machine translation; Selection model; Semantic levels; Statistical machine translation; Translation systems; Translation (languages)
Using short dependency relations from auto-parsed data for Chinese dependency parsing,2009,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-70349084103&doi=10.1145%2f1568292.1568293&partnerID=40&md5=f45de29897f2805b17a448ea304635cf,"Dependency parsing has become increasingly popular for a surge of interest lately for applications such as machine translation and question answering. Currently, several supervised learning methods can be used for training high-performance dependency parsers if sufficient labeled data are available. However, currently used statistical dependency parsers provide poor results for words separated by long distances. In order to solve this problem, this article presents an effective dependency parsing approach of incorporating short dependency information from unlabeled data. The unlabeled data is automatically parsed by using a deterministic dependency parser, which exhibits a relatively high performance for short dependencies between words. We then train another parser that uses the information on short dependency relations extracted from the output of the first parser. The proposed approach achieves an unlabeled attachment score of 86.52%, an absolute 1.24% improvement over the baseline system on the Chinese Treebank data set. The results indicate that the proposed approach improves the parsing performance for longer distance words. © 2009 ACM.",Chinese dependency parsing; Semi-supervised learning; Unlabeled data,Computer aided language translation; Information theory; Speech transmission; Baseline systems; Chinese dependency parsing; Data sets; Dependency parser; Dependency parsing; Dependency relation; Labeled data; Long distances; Machine translations; Question Answering; Semi-supervised learning; Statistical dependencies; Supervised learning methods; Treebanks; Unlabeled data; Supervised learning
Web search clustering and labeling with hidden topics,2009,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-70349094440&doi=10.1145%2f1568292.1568295&partnerID=40&md5=a1cb194ec3eb30ba5e4a4a43a4bc8f89,"Web search clustering is a solution to reorganize search results (also called snippets) in a more convenient way for browsing. There are three key requirements for such post-retrieval clustering systems: (1) the clustering algorithm should group similar documents together; (2) clusters should be labeled with descriptive phrases; and (3) the clustering system should provide high-quality clustering without downloading the whole Web page. This article introduces a novel framework for clustering Web search results in Vietnamese which targets the three above issues. The main motivation is that by enriching short snippets with hidden topics from huge resources of documents on the Internet, it is able to cluster and label such snippets effectively in a topic-oriented manner without concerning whole Web pages. Our approach is based on recent successful topic analysis models, such as Probabilistic-Latent Semantic Analysis, or Latent Dirichlet Allocation. The underlying idea of the framework is that we collect a very large external data collection called universal dataset, and then build a clustering system on both the original snippets and a rich set of hidden topics discovered from the universal data collection. This can be seen as a richer representation of snippets to be clustered. We carry out careful evaluation of our method and show that our method can yield impressive clustering quality. © 2009 ACM.",Cluster labeling; Collocation; Hidden topics analysis; Hierarchical Agglomerative Clustering; Latent Dirichlet allocation; Vietnamese; Web search clustering,Clustering algorithms; Concentration (process); Data acquisition; Information retrieval; Labeling; Websites; Cluster labeling; Collocation; Hidden topics analysis; Hierarchical Agglomerative Clustering; Latent Dirichlet allocation; Vietnamese; Web search clustering; Cluster analysis
Word-wise Thai and Roman script identification,2009,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-70349153425&doi=10.1145%2f1568292.1568294&partnerID=40&md5=68e354411817da88047155a6b83282eb,"In some Thai documents, a single text line of a printed document page may contain words of both Thai and Roman scripts. For the Optical Character Recognition (OCR) of such a document page it is better to identify, at first, Thai and Roman script portions and then to use individual OCR systems of the respective scripts on these identified portions. In this article, an SVM-based method is proposed for identification of word-wise printed Roman and Thai scripts from a single line of a document page. Here, at first, the document is segmented into lines and then lines are segmented into character groups (words). In the proposed scheme, we identify the script of a character group combining different character features obtained from structural shape, profile behavior, component overlapping information, topological properties, and water reservoir concept, etc. Based on the experiment on 10,000 data (words) we obtained 99.62% script identification accuracy from the proposed scheme. © 2009 ACM.",Multi-script OCR; Script identification; SVM; Thai Script,Linguistics; Optical character recognition; Reservoirs (water); Topology; Character feature; Multi-script OCR; Overlapping information; Printed documents; Script identification; Structural shape; SVM; SVM-based methods; Text lines; Thai Script; Topological properties; Water reservoir; Support vector machines
Bilingually motivated word segmentation for statistical machine translation,2009,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-67650327608&doi=10.1145%2f1526252.1526255&partnerID=40&md5=c152c9d1e3662fdda6224b3ce46788a6,"We introduce a bilingually motivated word segmentation approach to languages where word boundaries are not orthographically marked, with application to Phrase-Based Statistical Machine Translation (PB-SMT). Our approach is motivated from the insight that PB-SMT systems can be improved by optimizing the input representation to reduce the predictive power of translation models. We firstly present an approach to optimize the existing segmentation of both source and target languages for PB-SMT and demonstrate the effectiveness of this approach using a Chinese - English MT task, that is, to measure the influence of the segmentation on the performance of PB-SMT systems. We report a 5.44% relative increase in Bleu score and a consistent increase according to other metrics. We then generalize this method for Chinese word segmentation without relying on any segmenters and show that using our segmentation PB-SMT can achieve more consistent state-of-the-art performance across two domains. There are two main advantages of our approach. First of all, it is adapted to the specific translation task at hand by taking the corresponding source (target) language into account. Second, this approach does not rely on manually segmented training data so that it can be automatically adapted for different domains. © 2009 ACM.",Alignment; Bilingually motivated; Phrase-based statistical machine translation; Word segmentation,Alignment; Computer aided language translation; Information theory; Lead alloys; Linguistics; Query languages; Translation (languages); Bilingually motivated; Chinese word segmentation; Consistent state; Different domains; Phrase-based statistical machine translation; Predictive power; SMT systems; Statistical machine translation; Target language; Training data; Translation models; Two domains; Word segmentation; Speech transmission
Improved monolingual hypothesis alignment for machine translation system combination,2009,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-67650327609&doi=10.1145%2f1526252.1526254&partnerID=40&md5=520030eba913f36276427c6fd6ba16e9,"This article presents a new hypothesis alignment method for combining outputs of multiple machine translation (MT) systems. An indirect hidden Markov model (IHMM) is proposed to address the synonym matching and word ordering issues in hypothesis alignment. Unlike traditional HMMs whose parameters are trained via maximum likelihood estimation (MLE), the parameters of the IHMM are estimated indirectly from a variety of sources including word semantic similarity, word surface similarity, and a distance-based distortion penalty. The IHMM-based method significantly outperforms the state-of-the-art, TER-based alignment model in our experiments on NIST benchmark datasets. Our combined SMT system using the proposed method achieved the best Chinese-to-English translation result in the constrained training track of the 2008 NIST Open MT Evaluation. © 2009 ACM.",Hidden Markov model; Statistical machine translation; System combination; Word alignment,Computer aided language translation; Hidden Markov models; Information theory; Linguistics; Maximum likelihood estimation; Object recognition; Semantics; Speech transmission; Surface mount technology; Telluric prospecting; Alignment methods; Benchmark datasets; Distance-based; Machine translation systems; Multiple machine; Semantic similarity; SMT systems; Statistical machine translation; System combination; Word alignment; Alignment
Introduction to the special issue on machine translation of Asian languages,2009,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-67650284501&doi=10.1145%2f1526252.1526253&partnerID=40&md5=a55b96bcacd15d42e85e0db1be2e2d5a,[No abstract available],,
Discriminative machine translation using global lexical selection,2009,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-67650321549&doi=10.1145%2f1526252.1526256&partnerID=40&md5=adea5001e3701eb8b867debffbd1bb0e,"Statistical phrase-based machine translation models crucially rely on word alignments. The search for word-alignments assumes a model of word locality between source and target languages that is violated in starkly different word-order languages such as English-Hindi. In this article, we present models that decouple the steps of lexical selection and lexical reordering with the aim of minimizing the role of word-alignment in machine translation. Indian languages are morphologically rich and have relatively free-word order where the grammatical role of content words is largely determined by their case markers and not just by their positions in the sentence. Hence, lexical selection plays a far greater role than lexical reordering. For lexical selection, we investigate models that take the entire source sentence into account and evaluate their performance for English-Hindi translation in a tourism domain. © 2009 ACM.",Global lexical selection; Machine translation,Alignment; Computer aided language translation; Information theory; Linguistics; Speech transmission; Global lexical selection; Indian languages; Machine translation; Machine translation models; Machine translations; Target language; Word alignment; Word-order languages; Translation (languages)
A Chinese-Japanese lexical machine translation through a Pivot language,2009,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-67650287651&doi=10.1145%2f1526252.1526257&partnerID=40&md5=40e722674c838794043c26df0f2a476a,"The bilingual lexicon is an expensive but critical resource for multilingual applications in natural language processing. This article proposes an integrated framework for building a bilingual lexicon between the Chinese and Japanese languages. Since the language pair Chinese-Japanese does not include English, which is a central language of the world, few large-scale bilingual resources between Chinese and Japanese have been constructed. One solution to alleviate this problem is to build a Chinese-Japanese bilingual lexicon through English as the pivot language. In addition to the pivotal approach, we can make use of the characteristics of Chinese and Japanese languages that use Han characters. We incorporate a translation model obtained from a small Chinese-Japanese lexicon and use the similarity of the hanzi and kanji characters by using the log-linear model. Our experimental results show that the use of the pivotal approach can improve the translation performance over the translation model built from a small Chinese-Japanese lexicon. The results also demonstrate that the similarity between the hanzi and kanji characters provides a positive effect for translating technical terms. © 2009 ACM.",Bilingual lexicon; Han characters; Hanzi; Kanji; Pivot language; Statistical machine translation,Computational linguistics; Computer aided language translation; Information theory; Natural language processing systems; Query languages; Regression analysis; Speech transmission; Bilingual lexicon; Han characters; Hanzi; Kanji; Pivot language; Statistical machine translation; Translation (languages)
Introduction to the special issue on recent advances in asian language spoken document retrieval,2009,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-67149105511&doi=10.1145%2f1482343.1482344&partnerID=40&md5=0711ef16a4c714f459a82d9c926b8f10,[No abstract available],,
A comparative study of probabilistic ranking models for Chinese spoken document summarization,2009,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-67149093166&doi=10.1145%2f1482343.1482346&partnerID=40&md5=948e1c122e11524087fa0aa0b3c8a4ed,"Extractive document summarization automatically selects a number of indicative sentences, passages, or paragraphs from an original document according to a target summarization ratio, and sequences them to form a concise summary. In this article, we present a comparative study of various probabilistic ranking models for spoken document summarization, including supervised classification-based summarizers and unsupervised probabilistic generative summarizers. We also investigate the use of unsupervised summarizers to improve the performance of supervised summarizers when manual labels are not available for training the latter. A novel training data selection approach that leverages the relevance information of spoken sentences to select reliable document-summary pairs derived by the probabilistic generative summarizers is explored for training the classification-based summarizers. Encouraging initial results on Mandarin Chinese broadcast news data are demonstrated. © 2009 ACM.",Extractive summarization; Probabilistic ranking models; Relevance information; Spoken document summarization,Data reduction; Broadcast news; Comparative studies; Document summarization; Extractive summarization; Mandarin Chinese; Probabilistic ranking; Probabilistic ranking models; Relevance information; Reliable documents; Spoken document; Spoken document summarization; Supervised classification; Training data; Information retrieval systems
Word topic models for spoken document retrieval and transcription,2009,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-67149144176&doi=10.1145%2f1482343.1482345&partnerID=40&md5=fba8137dd25fcf65c2c261d9e5eed327,"Statistical language modeling (LM), which aims to capture the regularities in human natural language and quantify the acceptability of a given word sequence, has long been an interesting yet challenging research topic in the speech and language processing community. It also has been introduced to information retrieval (IR) problems, and provided an effective and theoretically attractive probabilistic framework for building IR systems. In this article, we propose a word topic model (WTM) to explore the co-occurrence relationship between words, as well as the long-span latent topical information, for language modeling in spoken document retrieval and transcription. The document or the search history as a whole is modeled as a composite WTM model for generating a newly observed word. The underlying characteristics and different kinds of model structures are extensively investigated, while the performance of WTM is thoroughly analyzed and verified by comparison with the well-known probabilistic latent semantic analysis (PLSA) model as well as the other models. The IR experiments are performed on the TDT Chinese collections (TDT-2 and TDT-3), while the large vocabulary continuous speech recognition (LVCSR) experiments are conducted on the Mandarin broadcast news collected in Taiwan. Experimental results seem to indicate that WTM is a promising alternative to the existing models. © 2009 ACM.",Adaptation; Information retrieval; Language model; Speech recognition; Word topic model,Computational linguistics; Continuous speech recognition; Industrial research; Information retrieval; Information services; Natural language processing systems; Security of data; Speech analysis; Transcription; Adaptation; Broadcast news; Co-occurrence; Language model; Language modeling; Language processing; Large vocabulary continuous speech recognition; Long span; Natural languages; Probabilistic framework; Probabilistic latent semantic analysis; Research topics; Spoken document retrieval; Statistical language modeling; Topic model; Word topic model; Model structures
Two-stage hypotheses generation for spoken language translation,2009,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-67149097674&doi=10.1145%2f1482343.1482347&partnerID=40&md5=b41814f0aeb7c4823b44992dc7b6ecc0,"Spoken Language Translation (SLT) is the research area that focuses on the translation of speech or text between two spoken languages. Phrase-based and syntax-based methods represent the state-of-the-art for statistical machine translation (SMT). The phrase-based method specializes in modeling local reorderings and translations of multiword expressions. The syntax-based method is enhanced by using syntactic knowledge, which can better model long word reorderings, discontinuous phrases, and syntactic structure. In this article, we leverage on the strength of these two methods and propose a strategy based on multiple hypotheses generation in a two-stage framework for spoken language translation. The hypotheses are generated in two stages, namely, decoding and regeneration. In the decoding stage, we apply state-of-the-art, phrase-based, and syntax-based methods to generate basic translation hypotheses. Then in the regeneration stage, much more hypotheses that cannot be captured by the decoding algorithms are produced from the basic hypotheses. We study three regeneration methods: redecoding, n-gram expansion, and confusion network in the second stage. Finally, an additional reranking pass is introduced to select the translation outputs by a linear combination of rescoring models. Experimental results on the Chinese-to-English IWSLT-2006 challenge task of translating the transcription of spontaneous speech show that the proposed mechanism achieves significant improvements over the baseline of about 2.80 BLEU-score. © 2009 ACM.",Hypotheses generation; Spoken language translation; Statistical machine translation,Computer aided language translation; Decoding; Information theory; Linguistics; Natural language processing systems; Query languages; Speech transmission; Syntactics; Basic hypothesis; Decoding algorithm; Hypotheses generation; Linear combinations; Multiple hypothesis; Multiword expressions; Re-ranking; Research areas; Spoken language translation; Spoken languages; Spontaneous speech; Statistical machine translation; Syntactic structure; Two stage; Translation (languages)
Variant Chinese domain name resolution,2008,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-59449104629&doi=10.1145%2f1450295.1450296&partnerID=40&md5=28dfd10d5d6d2fa37ee29d892631831e,"Many efforts in past years have been made to lower the linguistic barriers for non-native English speakers to access the Internet. Internet standard RFC 3490, referred to as IDNA (Internationalizing Domain Names in Applications), focuses on access to IDNs (Internationalized Domain Names) in a range of scripts that is broader in scope than the original ASCII. However, the use of character variants that have similar appearances and/or interpretations could create confusion. A variant IDL (Internationalized Domain Label), derived from an IDL by replacing some characters with their variants, should match the original IDL; and thus a variant IDN does. In RFC 3743, referred to as JET (Joint Engineering Team) Guidelines, it is suggested that zone administrators model this concept of equivalence as an atomic IDL package. When an IDL is registered, an IDL package is created that contains its variant IDLs generated according to the zone-specific Language Variant Tables (LVTs). In addition to the registered IDL, the name holder can request the domain registry to activate some of the variant IDLs, free or by an extra fee. The activated variant IDLs are stored in the zone files, and thus become resolvable. However, an issue of scalability arises when there is a large number of variant IDLs to be activated. In this article, the authors present a resolution protocol that resolves the variant IDLs into the registered IDL, specifically for Han character variants. Two Han characters are said to be variants of each other if they have the same meaning and are pronounced the same. Furthermore, Han character variants usually have similar appearances. It is not uncommon that a Chinese IDL has a large number of variant IDLs. The proposed protocol introduces a new RR (resource record) type, denoted as VarIdx RR, to associate a variant expression of the variant IDLs with the registered IDL. The label of the VarIdx RR, denoted as the variant index, is assigned by an indexing function that is designed to give the same value to all of the variant IDLs enumerated by the variant expression. When one of the variant IDLs is accessed, Internet applications can compute the variant index, look up the VarIdx RRs, and resolve the variant IDL into the registered IDL. The authors examine two sets of Chinese IDLs registered in TWNIC and CNNIC, respectively. The results show that for a registered Chinese IDL, a very small number of VarIdx RRs, usually one or two, are sufficient to activate all of its variant IDLs. The authors also represent a Web redirection service that employs the proposed resolution protocol to redirect a URL addressed by a variant IDN to the URL addressed by the registered IDN. The experiment results show that the proposed protocol successfully resolves the variant IDNs into the registered IDNs. © 2008 ACM.",Conversion between traditional Chinese and simplified Chinese; Han character folding; Han character variant; IDN spoof; Internationalized domain name; Localization,Internet; Linguistics; Conversion between traditional Chinese and simplified Chinese; Han character folding; Han character variant; IDN spoof; Internationalized domain name; Localization; Internet protocols
Boosting Chinese question answering with two lightweight methods: ABSPs and SCO-QAT,2008,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-59449102310&doi=10.1145%2f1450295.1450297&partnerID=40&md5=152deedb866a1473e0e1c0729ee80682,"Question Answering (QA) research has been conducted in many languages. Nearly all the top performing systems use heavy methods that require sophisticated techniques, such as parsers or logic provers. However, such techniques are usually unavailable or unaffordable for under-resourced languages or in resource-limited situations. In this article, we describe how a top-performing Chinese QA system can be designed by using lightweight methods effectively. We propose two lightweight methods, namely the Sum of Co-occurrences of Question and Answer Terms (SCO-QAT) and Alignment-based Surface Patterns (ABSPs). SCO-QAT is a co-occurrence-based answer-ranking method that does not need extra knowledge, word-ignoring heuristic rules, or tools. It calculates co-occurrence scores based on the passage retrieval results. ABSPs are syntactic patterns trained from question-answer pairs with a multiple alignment algorithm. They are used to capture the relations between terms and then use the relations to filter answers. We attribute the success of the ABSPs and SCO-QAT methods to the effective use of local syntactic information and global co-occurrence information. By using SCO-QAT and ABSPs, we improved the RU-Accuracy of our testbed QA system, ASQA, from 0.445 to 0.535 on the NTCIR-5 dataset. It also achieved the top 0.5 RU-Accuracy on the NTCIR-6 dataset. The result shows that lightweight methods are not only cheaper to implement, but also have the potential to achieve state-of-the-art performances. © 2008 ACM.",Answer filtering; Answer ranking; Chinese question answering; Co-occurrence; Lightweight method; Surface pattern,Alignment; Linguistics; Natural language processing systems; Syntactics; Textile printing; Answer filtering; Answer ranking; Chinese question answering; Co-occurrence; Lightweight method; Surface pattern; Heuristic methods
Using a hybrid convolution tree kernel for semantic role labeling,2008,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-59449109429&doi=10.1145%2f1450295.1450298&partnerID=40&md5=494acef433a41d1c08e903bfd84c07da,"As a kind of Shallow Semantic Parsing, Semantic Role Labeling (SRL) is gaining more attention as it benefits a wide range of natural language processing applications. Given a sentence, the task of SRL is to recognize semantic arguments (roles) for each predicate (target verb or noun). Feature-based methods have achieved much success in SRL and are regarded as the state-of-the-art methods for SRL. However, these methods are less effective in modeling structured features. As an extension of feature-based methods, kernel-based methods are able to capture structured features more efficiently in a much higher dimension. Application of kernel methods to SRL has been achieved by selecting the tree portion of a predicate and one of its arguments as feature space, which is named as predicate-argument feature (PAF) kernel. The PAF kernel captures the syntactic tree structure features using convolution tree kernel, however, it does not distinguish between the path structure and the constituent structure. In this article, a hybrid convolution tree kernel is proposed to model different linguistic objects. The hybrid convolution tree kernel consists of two individual convolution tree kernels. They are a Path kernel, which captures predicate-argument link features, and a Constituent Structure kernel, which captures the syntactic structure features of arguments. Evaluations on the data sets of the CoNLL-2005 SRL shared task and the Chinese PropBank (CPB) show that our proposed hybrid convolution tree kernel statistically significantly outperforms the previous tree kernels. Moreover, in order to maximize the system performance, we present a composite kernel through combining our hybrid convolution tree kernel method with a feature-based method extended by the polynomial kernel. The experimental results show that the composite kernel achieves better performance than each of the individual methods and outperforms the best reported system on the CoNLL-2005 corpus when only one syntactic parser is used and on the CPB corpus when automated syntactic parse results and correct syntactic parse results are used respectively. © 2008 ACM.",Hybrid convolution tree kernel; Semantic role labeling,Arsenic; Computation; Information Systems; Labeling; Languages; Processing; Arsenic; Breakwaters; Computational linguistics; Content based retrieval; Convolution; Information theory; Labeling; Linguistics; Natural language processing systems; Semantics; Syntactics; Composite kernels; Data sets; Feature spaces; Feature-based methods; Higher dimensions; Hybrid convolution tree kernel; Kernel methods; Kernel-based methods; Link features; NAtural language processing; Polynomial kernels; Semantic role labeling; State-of-the-art methods; Syntactic parsers; Syntactic structures; Syntactic trees; Trees (mathematics)
TRUES: Tone recognition using extended segments,2008,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-50849105754&doi=10.1145%2f1386869.1386872&partnerID=40&md5=162715fa04fc3e18fd8aa9ae1dc03bad,"Tone recognition has been a basic but important task for speech recognition and assessment of tonal languages, such as Mandarin Chinese. Most previously proposed approaches adopt a two-step approach where syllables within an utterance are identified via forced alignment first, and tone recognition using a variety of classifiers - -such as neural networks, Gaussian mixture models (GMM), hidden Markov models (HMM), support vector machines (SVM) - -is then performed on each segmented syllable to predict its tone. However, forced alignment does not always generate accurate syllable boundaries, leading to unstable voiced-unvoiced detection and deteriorating performance in tone recognition. Aiming to alleviate this problem, we propose a robust approach called Tone Recognition Using Extended Segments (TRUES) for HMM-based continuous tone recognition. The proposed approach extracts an unbroken pitch contour from a given utterance based on dynamic programming over time-domain acoustic features of average magnitude difference function (AMDF). The pitch contour of each syllable is then extended for tri-tone HMM modeling, such that the influence from inaccurate syllable boundaries is lessened. Our experimental results demonstrate that the proposed TRUES achieves 49.13% relative error rate reduction over that of the recently proposed supratone modeling, which is deemed the state of the art of tone recognition that outperforms several previously proposed approaches. The encouraging improvement demonstrates the effectiveness and robustness of the proposed TRUES, as well as the corresponding pitch determination algorithm which produces unbroken pitch contours. © 2008 ACM.",Context-dependent tone modeling; Continuous tone recognition; Extended segment for tone recognition; HMM; Mandarin Chinese; Supratone modeling,Alignment; Artificial intelligence; BASIC (programming language); Blind source separation; Computer networks; Error analysis; Hidden Markov models; Learning systems; Markov processes; Mathematical programming; Neural networks; Object recognition; Speech analysis; Support vector machines; Systems engineering; Context-dependent tone modeling; Continuous tone recognition; Extended segment for tone recognition; HMM; Mandarin Chinese; Supratone modeling; Speech recognition
Integrating cross-language hierarchies and its application to retrieving relevant documents,2008,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-50849138493&doi=10.1145%2f1386869.1386870&partnerID=40&md5=2ce66765434e2142a2a4a28d002eed74,"Internet directories such as Yahoo! are an approach to improve the efficacy and efficiency of Information Retrieval (IR) on the Web, as pages (documents) are organized into hierarchical categories, and similar pages are grouped together. Most of the search engines on the Web service find documents that are assigned to a single classification hierarchy. Categories in the hierarchy are carefully defined by human experts and documents are well organized. However, a single hierarchy in one language is often insufficient to find all relevant material, as each hierarchy tends to have some bias in both defining hierarchical structure and classifying documents. Moreover, documents written in a language other than the users native language often include large amounts of information related to the users request. In this article, we propose a method of integrating cross-language (CL) category hierarchies, that is, Reuters 96 hierarchy and UDC code hierarchy of Japanese by estimating category similarities. The method does not simply merge two different hierarchies into one large hierarchy but instead extracts sets of similar categories, where each element of the sets is relevant with each other. It consists of three steps. First, we classify documents from one hierarchy into categories with another hierarchy using a cross-language text classification (CLTC) technique, and extract category pairs of two hierarchies. Next, we apply 2 statistics to these pairs to obtain similar category pairs, and finally we apply the generating function of the Apriori algorithm (Apriori-Gen) to the category pairs, and find sets of similar categories. Moreover, we examined whether integrating hierarchies helps to support retrieval of documents with similar contents. The retrieval results showed a 42.7% improvement over the baseline nonhierarchy model, and a 21.6% improvement over a single hierarchy. © 2008 ACM.",Cross-language hierarchies; Information integration; Retrieval of relevant documents; Text classification,Classification (of information); Computer networks; Information retrieval; Information services; Natural language processing systems; Search engines; Text processing; World Wide Web; Cross-language hierarchies; Information integration; Internet directories; Retrieval of relevant documents; Text classification; Information retrieval systems
Acquisition of morphology of an indic language from text corpus,2008,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-50849089623&doi=10.1145%2f1386869.1386871&partnerID=40&md5=d1eed0d117f23992e089174d28b85666,"This article describes an approach to unsupervised learning of morphology from an unannotated corpus for a highly inflectional Indo-European language called Assamese spoken by about 30 million people. Although Assamese is one of Indias national languages, it utterly lacks computational linguistic resources. There exists no prior computational work on this language spoken widely in northeast India. The work presented is pioneering in this respect. In this article, we discuss salient issues in Assamese morphology where the presence of a large number of suffixal determiners, sandhi, samas, and the propensity to use suffix sequences make approximately 50% of the words used in written and spoken text inflected. We implement methods proposed by Gaussier and Goldsmith on acquisition of morphological knowledge, and obtain F-measure performance below 60%. This motivates us to present a method more suitable for handling suffix sequences, enabling us to increase the F-measure performance of morphology acquisition to almost 70%. We describe how we build a morphological dictionary for Assamese from the text corpus. Using the morphological knowledge acquired and the morphological dictionary, we are able to process small chunks of data at a time as well as a large corpus. We achieve approximately 85% precision and recall during the analysis of small chunks of coherent text. © 2008 ACM.",Assamese; Indo-European languages; Machine learning; Morphology,Computational linguistics; Knowledge based systems; Mergers and acquisitions; Morphology; Query languages; Assamese; F-measure; Indo-European languages; Machine learning; Knowledge acquisition
A hybrid technique for English-Chinese cross language information retrieval,2008,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-44649150175&doi=10.1145%2f1362782.1362784&partnerID=40&md5=47c45458e4eee4028f7153770cfae524,In this article we describe a hybrid technique for dictionary-based query translation suitable for English-Chinese cross language information retrieval. This technique marries a graph-based model for the resolution of candidate term ambiguity with a pattern-based method for the translation of out-of-vocabulary (OOV) terms. We evaluate the performance of this hybrid technique in an experiment using several NTCIR test collections. Experimental results indicate a substantial increase in retrieval effectiveness over various baseline systems incorporating machine- and dictionary-based translation. © 2008 ACM.,Cross language information retrieval; Disambiguation; Graph-based analysis; Patterns; Unknown term translation,Graph theory; Linguistics; Mathematical models; Query languages; Cross language information retrieval; Graph-based analysis; Unknown term translation; Information retrieval
Introduction to the NTCIR-6 special issue,2008,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-44649190373&doi=10.1145%2f1362782.1362783&partnerID=40&md5=dccd34e02e19ccae614c490b32975fdd,[No abstract available],,
Adapting support vector machines for f-term-based classification of patents,2008,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-44649197692&doi=10.1145%2f1362782.1362786&partnerID=40&md5=e6302a2d857d02b1a89ffa734fc76635,"Support Vector Machines (SVM) have obtained state-of-the-art results on many applications including document classification. However, previous works on applying SVMs to the F-term patent classification task did not obtain as good results as other learning algorithms such as kNN. This is due to the fact that F-term patent classification is different from conventional document classification in several aspects, mainly because it is a multiclass, multilabel classification problem with semi-structured documents and multi-faceted hierarchical categories. This article describes our SVM-based system and several techniques we developed successfully to adapt SVM for the specific features of the F-term patent classification task. We evaluate the techniques using the NTCIR-6 F-term classification terms assigned to Japanese patents. Moreover, our system participated in the NTCIR-6 patent classification evaluation and obtained the best results according to two of the three metrics used for task performance evaluation. Following the NTCIR-6 participation, we developed two new techniques, which achieved even better scores using all three NTCIR-6 metrics, effectively outperforming all participating systems. This article presents this new work and the experimental results that demonstrate the benefits of the latest approach. © 2008 ACM.",F-term classification; Patent processing; Support vector machines,Algorithms; Classification (of information); Problem solving; Support vector machines; F-term classification; Patent processing; Patents and inventions
Automatically acquiring causal expression patterns from relation-annotated corpora to improve question answering for why-questions,2008,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-44649199830&doi=10.1145%2f1362782.1362785&partnerID=40&md5=d6fdc05decb08970ac7faf34e58afb95,"This article describes our approach for answering why-questions that we initially introduced at NTCIR-6 QAC-4. The approach automatically acquires causal expression patterns from relation-annotated corpora by abstracting text spans annotated with a causal relation and by mining syntactic patterns that are useful for distinguishing sentences annotated with a causal relation from those annotated with other relations. We use these automatically acquired causal expression patterns to create features to represent answer candidates, and use these features together with other possible features related to causality to train an answer candidate ranker that maximizes the QA performance with regards to the corpus of why-questions and answers. NAZEQA, a Japanese why-QA system based on our approach, clearly outperforms baselines with a Mean Reciprocal Rank (top-5) of 0.223 when sentences are used as answers and with a MRR (top-5) of 0.326 when paragraphs are used as answers, making it presumably the best-performing fully implemented why-QA system. Experimental results also verified the usefulness of the automatically acquired causal expression patterns. © 2008 ACM.",Causal expression; Pattern mining; Question answering; Relation-annotated corpus,Abstracting; Data mining; Optimization; Relational database systems; Syntactics; Causal expression; Pattern mining; Question answering; Relation-annotated corpus; Pattern recognition
Multidocument summary generation: Using informative and event words,2008,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-39149124104&doi=10.1145%2f1330291.1330294&partnerID=40&md5=043158a0297424dd58945a13c2e80756,"Summary generation for multiple documents poses a number of issues including sentence selection, sentence ordering, and sentence reduction over single-document summarization. In addition, the temporal resolution among extracted sentences is also important. This article considers informative words and event words to deal with multidocument summarization. These words indicate the important concepts and relationships in a document or among a set of documents, and can be used to select salient sentences. We present a temporal resolution algorithm, using focusing time and coreference chains, to convert Chinese temporal expressions in a document into calendrical forms. Moreover, we consider the last calendrical form of a sentence as a sentence time stamp to address sentence ordering. Informative words, event words, and temporal words are introduced to a sentence reduction algorithm, which deals with both length constraints and information coverage. Experiments on Chinese-news data sets show significant improvements of both information coverage and readability. © 2008 ACM.",Latent semantic analysis; Multidocument summary generation; Sentence ordering; Sentence reduction; Sentence selection; Temporal processing,Algorithms; Information analysis; Program documentation; Latent semantic analysis; Multidocument summary generation; Temporal processing; Word processing
A structure-based model for Chinese organization name translation,2008,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-39149114710&doi=10.1145%2f1330291.1330292&partnerID=40&md5=f74df2f6249ef22e26a9846bb73c1f81,"Named entity (NE) translation is a fundamental task in multilingual natural language processing. The performance of a machine translation system depends heavily on precise translation of the inclusive NEs. Furthermore, organization name (ON) is the most complex NE for translation among all the NEs. In this article, the structure formulation of ONs is investigated and a hierarchical structure-based ON translation model for Chinese-to-English translation system is presented. First, the model performs ON chunking; then both the translation of words within chunks and the process of chunk-reordering are achieved by synchronous context-free grammar (CFG). The CFG rules are extracted from bilingual ON pairs in a training program. The main contributions of this article are: (1) defining appropriate chunk-units for analyzing the internal structure of Chinese ONs; (2) making the chunk-based ON translation feasible and flexible via a hierarchical CFG derivation; and (3) proposing a training architecture to automatically learn the synchronous CFG for constructing ONs with chunk-units from aligned bilingual ON pairs. The experiments show that the proposed approach translates the Chinese ONs into English with an accuracy of 93.75% and significantly improves the performance of a baseline statistical machine translation (SMT) system. © 2008 ACM.",Alignment; Chunk; Hierarchical derivation; Machine translation; Named entity; Organization name; Rules extraction; Structural analysis; Synchronous context-free grammar,Learning systems; Mathematical models; Text processing; Translation (languages); Machine translation; Rules extraction; Synchronous context-free grammar; Natural language processing systems
Improving speech recognition and understanding using error-corrective reranking,2008,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-39149135811&doi=10.1145%2f1330291.1330293&partnerID=40&md5=f7b6bb9829ead5f0533756eb12ac956d,"The main issues of practical spoken-language applications forhuman-computer interface are how to overcome speech recognitionerrors and guarantee the reasonable end-performance ofspoken-language applications. Therefore, handling the erroneouslyrecognized outputs is a key in developing robust spoken-languagesystems. To address this problem, we present a method to improvethe accuracy of speech recognition and performance ofspoken-language applications. The proposed error correctivereranking approach exploits recognition environment characteristicsand domain-specific semantic information to provide robustness andadaptability for a spoken-language system. We demonstrate someexperiments of spoken dialogue tasks and empirical results thatshow an improvement in accuracy for both speech recognition andspoken-language understanding. In our experiment, we show an errorreduction of up to 9.7% and 16.8%; of word error rate, and 5.5% and7.9% of understanding error for the air travel and telebankingservice domains. © 2008 ACM.",Automatic speech recognition; Error-corrective reranking; Improving spoken dialogue system; Spoken-language understanding,Error correction; Information retrieval; Natural language processing systems; Semantic Web; Automatic speech recognition; Error-corrective reranking; Spoken-language understanding; Speech recognition
Named entity recognition in Vietnamese using classifier voting,2007,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-38149100370&doi=10.1145%2f1316457.1316460&partnerID=40&md5=d3dd80c2283a11c936db8347eea4f80a,"Named entity recognition (NER) is one of the fundamental tasks in natural-language processing (NLP). Though the combination of different classifiers has been widely applied in several well-studied languages, this is the first time this method has been applied to Vietnamese. In this article, we describe how voting techniques can improve the performance of Vietnamese NER. By combining several state-of-the-art machine-learning algorithms using voting strategies, our final result outperforms individual algorithms and gained an F-measure of 89.12. A detailed discussion about the challenges of NER in Vietnamese is also presented. © 2007 ACM.",C4.5; Conditional random fields; Naive bayes; Named entity recognition; Support vector machines; Transformation based learning; Vietnamese; Voting,Formal languages; Learning algorithms; Support vector machines; Text processing; Conditional random fields; Named entity recognition; Transformation based learning; Natural language processing systems
Stemming Indonesian: A confix-stripping approach,2007,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-38149091838&doi=10.1145%2f1316457.1316459&partnerID=40&md5=d85b453882f2a3bc30a39cfbd993bd2f,"Stemming words to (usually) remove suffixes has applications in text search, machine translation, document summarization, and text classification. For example, English stemming reduces the words ""computer,"" ""computing,"" ""computation,"" and ""computability"" to their common morphological root, ""comput-."" In text search, this permits a search for ""computers"" to find documents containing all words with the stem ""comput-."" In the Indonesian language, stemming is of crucial importance: words have prefixes, suffixes, infixes, and confixes that make matching related words difficult. This work surveys existing techniques for stemming Indonesian words to their morphological roots, presents our novel and highly accurate CS algorithm, and explores the effectiveness of stemming in the context of general-purpose text information retrieval through ad hoc queries. © 2007 ACM.",Indonesian; Information retrieval; Stemming,Algorithms; Information retrieval systems; Text processing; Translation (languages); Word processing; Confix-stripping approach; Morphological roots; Stemming Indonesian; Formal languages
Zero-anaphora resolution by learning rich syntactic pattern features,2007,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-38149129444&doi=10.1145%2f1316457.1316458&partnerID=40&md5=b1b22e9dbd278a7131e30fa7f5dbc60b,"We approach the zero-anaphora resolution problem by decomposing it into intrasentential and intersentential zero-anaphora resolution tasks. For the former task, syntactic patterns of zeropronouns and their antecedents are useful clues. Taking Japanese as a target language, we empirically demonstrate that incorporating rich syntactic pattern features in a state-of-the-art learning-based anaphora resolution model dramatically improves the accuracy of intrasentential zero-anaphora, which consequently improves the overall performance of zero-anaphora resolution. © 2007 ACM.",Anaphora resolution; Zero-pronouns,Demonstrations; Formal languages; Information systems; Problem solving; Intersentential; Zero-anaphora resolution; Syntactics
Comparison of performance of enhanced morpheme-based language model with different word-based language models for improving the performance of Tamil speech recognition system,2007,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-36448970944&doi=10.1145%2f1290002.1290003&partnerID=40&md5=c3d111b4e5a1f6eb4322e2587e320c5a,"This paper describes a new technique of language modeling for a highly inflectional Dravidian language, Tamil. It aims to alleviate the main problems encountered in processing of Tamil language, like enormous vocabulary growth caused by the large number of different forms derived from one word. The size of the vocabulary was reduced by, decomposing the words into stems and endings and storing these sub word units (morphemes) in the vocabulary separately. A enhanced morpheme-based language model was designed for the inflectional language Tamil. The enhanced morpheme-based language model was trained on the decomposed corpus. The perplexity and Word Error Rate (WER) were obtained to check the efficiency of the model for Tamil speech recognition system. The results were compared with word-based bigram and trigram language models, distance based language model, dependency based language model and class based language model. From the results it was analyzed that the enhanced morpheme-based trigram model with Katz back-off smoothing effect improved the performance of the Tamil speech recognition system when compared to the word-based language models. © 2001 ACM.",Language model; Morphemes; Perplexity; Word error rate and speech recognition,Error analysis; Mathematical models; Natural language processing systems; Language models; Morphemes; Perplexity; Word error rate; Speech recognition
Developing lexicographic sorting: An example for Urdu,2007,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-36448960592&doi=10.1145%2f1290002.1290004&partnerID=40&md5=d36dbe3ab1c318fb3a846cf27bca9958,"Collation or lexicographic sorting is essential to develop multilingual computing. This paper presents the challenges faced in developing collation sequence for a language. The paper discusses both theoretical linguistic and practical standardization and encoding related considerations that need to be addressed for languages for which relevant standards and/or solutions have not been defined. The paper also defines the process, by giving the details of the procedure followed for Urdu language, which is the national language of Pakistan and is spoken by more than 100 million people across the world. The paper is oriented towards organizations involved in developing and using collation standards and the localization industry, and not focused on theoretical issues. © 2001 ACM.",Text processing; Urdu,Encoding (symbols); Societies and institutions; Text processing; Lexicographic sorting; Urdu; Natural language processing systems
Topic tracking based on bilingual comparable corpora and semisupervised clustering,2007,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-36448984971&doi=10.1145%2f1290002.1290005&partnerID=40&md5=151284fe6d745383df9e185fccc6f490,"In this paper, we address the problem of skewed data in topic tracking: the small number of stories labeled positive as compared to negative stories and propose a method for estimating effective training stories for the topic-tracking task. For a small number of labeled positive stories, we use bilingual comparable, i.e., English, and Japanese corpora, together with the EDR bilingual dictionary, and extract story pairs consisting of positive and associated stories. To overcome the problem of a large number of labeled negative stories, we classified them into clusters. This is done using a semisupervised clustering algorithm, combining k means with EM. The method was tested on the TDT English corpus and the results showed that the system works well when the topic under tracking is talking about an event originating in the source language country, even for a small number of initial positive training stories. © 2001 ACM.",Bilingual comparable corpora; Clustering; EM algorithm; N-gram model; Topic detection and tracking,Glossaries; Natural language processing systems; Problem solving; Bilingual comparable corpora; EM algorithm; N-gram models; Topic detection and tracking; Clustering algorithms
The study of a nonstationary maximum entropy Markov model and its application on the pos-tagging task,2007,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34948911237&doi=10.1145%2f1282080.1282082&partnerID=40&md5=718ed7c6b6acdf19f18f5e58d90feb13,"Sequence labeling is a core task in natural language processing. The maximum entropy Markov model (MEMM) is a powerful tool in performing this task. This article enhances the traditional MEMM by exploiting the positional information of language elements. The stationary hypothesis is relaxed in MEMM, and the nonstationary MEMM (NS-MEMM) is proposed. Several related issues are discussed in detail, including the representation of positional information, NS-MEMM implementation, smoothing techniques, and the space complexity issue. Furthermore, the asymmetric NS-MEMM presents a more flexible way to exploit positional information. In the experiments, NS-MEMM is evaluated on both the Chinese and the English pos-tagging tasks. According to the experimental results, NS-MEMM yields effective improvements over MEMM by exploiting positional information. The smoothing techniques in this article effectively solve the NS-MEMM data-sparseness problem; the asymmetric NS-MEMM is also an improvement by exploiting positional information in a more flexible way.",Data sparseness problem; Markov property; MEMM; Pos-tagging; Stationary hypothesis,Computational complexity; Information analysis; Mathematical models; Natural language processing systems; Data sparseness problem; Markov property; Stationary hypothesis; Markov processes
Interactive high-dimensional index for large Chinese calligraphic character databases,2007,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34948865282&doi=10.1145%2f1282080.1282083&partnerID=40&md5=4a88f4f1c1bc013ae47fd55725f9d3bf,"The large numbers of Chinese calligraphic scripts in existence are valuable part of the Chinese cultural heritage. However, due to the shape complexity of these characters, it is hard to employ existing techniques to effectively retrieve and efficiently index them. In this article, using a novel shape-similarity- based retrieval method in which shapes of calligraphic characters are represented by their contour points extracted from the character images, we propose an interactive partial-distance-map(PDM)- based high-dimensional indexing scheme which is designed specifically to speed up the retrieval performance of the large Chinese calligraphic character databases effectively. Specifically, we use the approximate minimal bounding sphere of a query character and utilize users' relevance feedback to refine the query gradually. Comprehensive experiments are conducted to testify the efficiency and effectiveness of this method. In addition, a new k-NN search called Pseudo k-NN (Pk-NN) search is presented to better facilitate the PDM-based character retrieval.",Chinese calligraphic character; Hyper-centre relocation; Pseudo k-NN,Database systems; Indexing (of information); Interactive computer systems; Word processing; Chinese calligraphic character; Hyper-centre relocation; Natural language processing systems
A phonetic similarity model for automatic extraction of transliteration pairs,2007,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34948830230&doi=10.1145%2f1282080.1282081&partnerID=40&md5=1e0214171a355363ce516e1e83532498,"This article proposes an approach for the automatic extraction of transliteration pairs from Chinese Web corpora. In this approach, we formulate the machine transliteration process using a syllable-based phonetic similarity model which consists of phonetic confusion matrices and a Chinese character n-gram language model. With the phonetic similarity model, the extraction of transliteration pairs becomes a two-step process of recognition followed by validation: First, in the recognition process, we identify the most probable transliteration in the k-neighborhood of a recognized English word. Then, in the validation process, we qualify the transliteration pair candidates with a hypothesis test. We carry out an analytical study on the statistics of several key factors in English-Chinese transliteration to help formulate phonetic similarity modeling. We then conduct both supervised and unsupervised learning of a phonetic similarity model on a development database. The experimental results validate the effectiveness of the phonetic similarity model by achieving an F-measure of 0.739 in supervised learning. The unsupervised learning approach works almost as well as the supervised one, thus allowing us to deploy automatic extraction of transliteration pairs in the Web space.",Extraction of transliteration pairs; Machine translation; Machine transliteration; Phonetic confusion probability; Phonetic similarity modeling,Formal languages; Mathematical models; Word processing; Extraction of transliteration pairs; Machine translation; Machine transliteration; Speech analysis
Measuring similarity between transliterations against noise data,2007,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34247186442&doi=10.1145%2f1227850.1227855&partnerID=40&md5=b4cc7fbb81b76c750c290cf8ffbd6759,"When editors of newspapers and magazines translate proper nouns from foreign languages into Chinese, the Chinese translation (termed transliterations) they choose will typically be phonetically similar to the original word. With many different translators working without a common standard, there may be many different Chinese transliterations for the same proper noun, such as using the same sounds but different Chinese characters or even using different sounds and characters. This causes confusion for the reader and, more importantly, leads to incomplete Chinese Web search results. This article investigates the similarity comparison of transliterations as a first step toward solving the incomplete search problem. We devise a method based on comparing digitalized Chinese character (or Hanzi) sounds. Along with four other methods based on comparing grapheme or phoneme similarity, we compare their performance of identifying synonymous transliterations against noise words taken from Web pages. Experimental results indicate that our method surpasses the other methods due to its advantage of containing more discriminative information in sound vectors. The method performing the second best is based on a scheme which assigns similarity between phonemes by carefully considering articulatory features of phonemes, including using multivalued features and placing different weights on the features. Among six pinyin schemes used to romanize Chinese transliterations, the Tongyong scheme outperforms the others. © 2007 ACM.",Chinese transliteration; Cross-lingual information retrieval; Grapheme; Phoneme; Pinyin; Romanization; Similarity comparison; Speech signal processing,Acoustic noise; Character recognition; Data reduction; Natural language processing systems; Speech analysis; Websites; Chinese transliteration; Cross-lingual information retrieval; Noise words; Romanization; Similarity comparison; Speech signal processing; Translation (languages)
Conjugation-based compression for Hebrew texts,2007,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34247331915&doi=10.1145%2f1227850.1227854&partnerID=40&md5=44b9192895dad83b0cb8c93c8b324329,"Traditional compression techniques do not look deeply into the morphology of languages. This can be less critical in languages like English where most of the sequences are illegal according to the grammatical rules of the language, for example, zx, bv or qe; hence the morphology can add a little information that can be beneficial for the compression algorithm. However, this negligence can be a significant flaw in languages like Hebrew where the grammatical rules allow much more freedom in the sequences of letters and, except tet after gimel, any pair is legal; hence compressing without taking the morphological rules into account can yield a poorer compression ratio. This article suggests a tool that optimizes the Burrows-Wheeler algorithm which is an unaware morphological rules compression method. It first preprocesses a Hebrew text file according to the Hebrew conjugation rules, and, after that, it provides the Burrows-Wheeler algorithm with this preprocessed file so that can be compressed better. Experimental results show a significant improvement. © 2007 ACM.",Burrows-Wheeler algorithm; Hebrew text analysis; Root conjugations; Semitic languages; Text compression,Algorithms; Data compression; Numerical methods; Burrows-Wheeler algorithms; Hebrew text analysis; Root conjugations; Semitic languages; Text compression; Linguistics
Transfer-based statistical translation of Taiwanese sign language using PCFG,2007,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34247182112&doi=10.1145%2f1227850.1227851&partnerID=40&md5=4eea61dd2f54b2fe3dc3e94e74b5214e,"This article presents a transfer-based statistical model for Chinese to Taiwanese sign-language (TSL) translation. Two sets of probabilistic context-free grammars (PCFGs) are derived from a Chinese Treebank and a bilingual parallel corpus. In this approach, a three-stage translation model is proposed. First, the input Chinese sentence is parsed into possible phrase structure trees (PSTs) based on the Chinese PCFGs. Second, the Chinese PSTs are then transferred into TSL PSTs according to the transfer probabilities between the context-free grammar (CFG) rules of Chinese and TSL derived from the bilingual parallel corpus. Finally, the TSL PSTs are used to generate the possible translation results. The Viterbi algorithm is adopted to obtain the best translation result via the three-stage translation. For evaluation, three objective evaluation metrics including AER, Top-N, and BLUE and one subjective evaluation metric using MOS were used. Experimental results show that the proposed approach outperforms the IBM Model 3 in the task of Chinese to sign-language translation. © 2007 ACM.",Taiwanese sign language,Algorithms; Context free grammars; Probabilistic logics; Statistical methods; Phrase structure trees (PST); Taiwanese sign language; Computer programming languages
Using data mining techniques and rough set theory for language modeling,2007,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34247263912&doi=10.1145%2f1227850.1227852&partnerID=40&md5=87242508abffb5817493d3c7bdd82198,"In this article, we propose a new postprocessing strategy, word suggestion, based on a multiple word trigger-pair language model for Chinese character recognizers. With the word suggestion strategy, Chinese character recognizers may even achieve a recognition rate greater than the top-n candidate recognition rate. To construct the multiple word trigger-pair model, data mining techniques are used to alleviate the intensive computation problem. Furthermore, rough set theory is first used in the study to discover negatively correlated relationships between words in order to prevent introducing wrong words in the process of word suggestion. © 2007 ACM.",Chinese character recognizer; Postprocessing,Computer programming languages; Computer simulation; Correlation methods; Rough set theory; Word processing; Chinese character recognizer; Computation problems; Postprocessing; Data mining
On the reliability of factoid question answering evaluation,2007,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34247192084&doi=10.1145%2f1227850.1227853&partnerID=40&md5=51defdde1f374eac6b94e9ebdd96ff1f,"This paper compares some existing evaluation metrics for factoid question answering (QA) from the viewpoint of stability and sensitivity, using the NTCIR-4 QAC2 Japanese factoid QA tasks and the Buckley/Voorhees stability method and Voorhees/Buckley swap method. Our main findings are: (1) For QA evaluation with ranked lists containing up to five answers, the fraction of questions with a correct answer within top 5 (NQcorrect5) and that with a correct answer at rank 1 (NQcorrect1) are not as stable and sensitive as reciprocal rank. (2) Q-measure, which can handle multiple correct answers and answer correctness levels, is at least as stable and sensitive as reciprocal rank, provided that a mild gain value assignment is used. Emphasizing answer correctness levels tends to hurt stability and sensitivity, while handling multiple correct answers improves them. As our experimental methods are language-independent, we believe that these findings apply to QA in languages other than Japanese as well. © 2007 ACM.",Evaluation metrics; Question answering,Data processing; Query languages; Reliability theory; Evaluation metrics; Question answering (QA); Computer science
Introduction to special issue on reasoning in natural language information processing,2006,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34147194451&doi=10.1145%2f1236181.1236182&partnerID=40&md5=dd9fdf1a12e243166cba05fdd7f30d21,"For any applications related to Natural Language Processing (NLP), reasoning has been recognized as a necessary underlying aspect. Many of the existing work in NLP deals with specific NLP problems in a highly heuristic manner, yet not from an explicit reasoning perspective. Recently, there have been developments on models that allow reasoning in NLP such as language models, logical models, and so on. The goal of this special issue is to present high-quality contributions that integrate reasoning involved in different areas of natural language processing both at theoretical and/or practical levels. In this article, we give a brief overview on some major aspects of explicating reasoning in NLP and summarize the articles included in this special issue. © 2006 ACM.",,Formal logic; Heuristic methods; Logic programming; Mathematical models; Language models; Logical models; Natural language processing systems
Statistical query translation models for cross-language information retrieval,2006,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34147207726&doi=10.1145%2f1236181.1236184&partnerID=40&md5=a1688edd38a1b5e8cf6835bb1f4ba01b,"Query translation is an important task in cross-language information retrieval (CLIR), which aims to determine the best translation words and weights for a query. This article presents three statistical query translation models that focus on the resolution of query translation ambiguities. All the models assume that the selection of the translation of a query term depends on the translations of other terms in the query. They differ in the way linguistic structures are detected and exploited. The co-occurrence model treats a query as a bag of words and uses all the other terms in the query as the context for translation disambiguation. The other two models exploit linguistic dependencies among terms. The noun phrase (NP) translation model detects NPs in a query, and translates each NP as a unit by assuming that the translation of a term only depends on other terms within the same NP. Similarly, the dependency translation model detects and translates dependency triples, such as verb-object, as units. The evaluations show that linguistic structures always lead to more precise translations. The experiments of CLIR on TREC Chinese collections show that all three models have a positive impact on query translation and lead to significant improvements of CLIR performance over the simple dictionary-based translation method. The best results are obtained by combining the three models. © 2006 ACM.",CLIR; Linguistic structures; Query translation; Statistical models,Computational linguistics; Query languages; Query processing; Statistical methods; Cross language information retrieval (CLIR); Linguistic dependencies; Linguistic structures; Query translation; Statistical models; Information retrieval
Improving discriminative sequential learning by discovering important association of statistics,2006,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34147209081&doi=10.1145%2f1236181.1236187&partnerID=40&md5=d20e793a44e5cc62b38915c505d063a4,"Discriminative sequential learning models like Conditional Random Fields (CRFs) have achieved significant success in several areas such as natural language processing or information extraction. Their key advantage is the ability to capture various nonindependent and overlapping features of inputs. However, several unexpected pitfalls have a negative influence on the model's performance; these mainly come from a high imbalance among classes, irregular phenomena, and potential ambiguity in the training data. This article presents a data-driven approach that can deal with such difficult data instances by discovering and emphasizing important conjunctions or associations of statistics hidden in the training data. Discovered associations are then incorporated into these models to deal with difficult data instances. Experimental results of phrase-chunking and named entity recognition using CRFs show a significant improvement in accuracy. In addition to the technical perspective, our approach also highlights a potential connection between association mining and statistical learning by offering an alternative strategy to enhance learning performance with interesting and useful patterns discovered from large datasets. © 2006 ACM.",Association rule mining; Discriminative sequential learning; Feature selection; Information extraction; Text segmentation,Database systems; Feature extraction; Information analysis; Natural language processing systems; Text processing; Association rule mining; Discriminative sequential learning; Information extraction; Text segmentation; Learning systems
Topic tracking with time granularity reasoning,2006,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34147203258&doi=10.1145%2f1236181.1236186&partnerID=40&md5=4549958cc9aaa39c5e88116e7b1f0af9,"Temporal information is an important attribute of a topic, and a topic usually exists in a limited period. Therefore, many researchers have explored the utilization of temporal information in topic detection and tracking (TDT). They use either a story's publication time or temporal expressions in text to derive temporal relatedness between two stories or a story and a topic. However, past research neglects the fact that people tend to express a time with different granularities as time lapses. Based on a careful investigation of temporal information in news streams, we propose a new strategy with time granularity reasoning for utilizing temporal information in topic tracking. A set of topic times, which as a whole represent the temporal attribute of a topic, are distinguished from others in the given on-topic stories. The temporal relatedness between a story and a topic is then determined by the highest coreference level between each time in the story and each topic time where the coreference level between a test time and a topic time is inferred from the two times themselves, their granularities, and the time distance between the topic time and the publication time of the story where the test time appears. Furthermore, the similarity value between an incoming story and a topic, that is the likelihood that a story is on-topic, can be adjusted only when the new story is both temporally and semantically related to the target topic. Experiments on two different TDT corpora show that our proposed method could make good use of temporal information in news stories, and it consistently outperforms the baseline centroid algorithm and other algorithms which consider temporal relatedness. © 2006 ACM.",Event tracking; Time granularity; Time reasoning; Topic detection and tracking; Topic tracking,Algorithms; Feature extraction; Information systems; Information use; Logic programming; Event tracking; Time granularity; Time reasoning; Topic detection and tracking (TDT); Information retrieval
A statistical framework for query translation disambiguation,2006,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34147204764&doi=10.1145%2f1236181.1236185&partnerID=40&md5=9c3eb15aa874670f3f8f68f334fdecc4,"Resolving ambiguity in the process of query translation is crucial to cross-language information retrieval (CLIR), given the short length of queries. This problem is even more challenging when only a bilingual dictionary is available, which is the focus of our work described here. In this paper, we will present a statistical framework for dictionary-based CLIR that estimates the translation probabilities of query words based on the monolingual word co-occurrence statistics. In addition, we will present two realizations of the proposed framework, i.e., the maximum coherence model and the spectral query-translation model, that exploit different metrics for the coherence measurement between a translation of a query word and the theme of the entire query. Compared to previous work on dictionary-based CLIR, the proposed framework is advantageous in three aspects: (1) Translation probabilities are calculated explicitly to capture the uncertainty in translating queries; (2) translations of all query words are estimated simultaneously rather than independently; and (3) the formulated problem can be solved efficiently with a unique optimal solution. Empirical studies with Chinese - English cross-language information retrieval using TREC datasets have shown that the proposed models achieve a relative 10% - 50% improvement, compared to other approaches that also exploit word co-occurrence statistics for query translation disambiguation. © 2006 ACM.",Co-occurrence statistics; Cross-language information retrieval; Graph partitioning; Maximum coherence,Mathematical models; Problem solving; Query languages; Query processing; Statistical methods; Co-occurrence statistics; Cross language information retrieval (CLIR); Graph partitioning; Maximum coherence; Information retrieval systems
Inferential language models for information retrieval,2006,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34147196588&doi=10.1145%2f1236181.1236183&partnerID=40&md5=a9512c5438731a74f05b1315e934bcb8,"Language modeling (LM) has been widely used in IR in recent years. An important operation in LM is smoothing of the document language model. However, the current smoothing techniques merely redistribute a portion of term probability according to their frequency of occurrences only in the whole document collection. No relationships between terms are considered and no inference is involved. In this article, we propose several inferential language models capable of inference using term relationships. The inference operation is carried out through a semantic smoothing either on the document model or query model, resulting in document or query expansion. The proposed models implement some of the logical inference capabilities proposed in the previous studies on logical models, but with necessary simplifications in order to make them tractable. They are a good compromise between inference power and efficiency. The models have been tested on several TREC collections, both in English and Chinese. It is shown that the integration of term relationships into the language modeling framework can consistently improve the retrieval effectiveness compared with the traditional language models. This study shows that language modeling is a suitable framework to implement basic inference operations in IR effectively. © 2006 ACM.",Document expansion; Inference; Inferential model; Query expansion,Formal languages; Inference engines; Mathematical models; Query languages; Semantics; Document expansion; Inferential models; Language models; Query expansion; Information retrieval
Adapting pivoted document-length normalization for query size: Experiments in Chinese and English,2006,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33846560987&doi=10.1145%2f1194936.1194941&partnerID=40&md5=ad8106c283ea9849fc53fcbf337c7a54,"The vector space model (VSM) is one of the most widely used information retrieval (IR) models in both academia and industry. It was less effective at the Chinese ad hoc retrieval tasks than other retrieval models in the NTCIR-3 evaluation workshop, but comparable to those in the NTCIR-4 and NTCIR-5 workshops. We do not know whether the lower level performance was due to the VSM's inherent deficiencies or to a less effective normalization of document length. Hence we evaluated the VSM with various pivoted normalizations of document length using the NTCIR-3 collection for confirmation. We found that VSM's retrieval effectiveness with pivoted normalization was comparable to other competitive retrieval models (for example, 2-Poisson), and that VSM's retrieval speed with pivoted normalization was similar to competitive retrieval models (2-Poisson). We proposed a novel adaptive scheme that automatically estimates the (near) best parameters for pivoted document-length normalization based on query size; the new normalization is called adaptive pivoted document-length normalization. This scheme achieved good retrieval effectiveness, sometimes for short (title) queries and sometimes for long queries, without manually adjusting parameter values. We found that unique, adaptive pivoted normalization can enhance fixed pivoted normalizations for different test collections (TREC-5 and TREC-6). We also evaluated the VSM with the adaptive pivoted normalization using the pseudo-relevance feedback (PRF) and found that this type of VSM performs similarly to the competitive retrieval models (2-Poisson) with PRF. Hence, we conclude that the VSM with unique (adaptive) pivoted document-length normalization is effective for Chinese IR and that its retrieval effectiveness is comparable to that of other competitive retrieval models with or without PRF for the reference test collections used in this evaluation. © 2006 ACM.",Chinese information retrieval; Indexing strategies; Pivoted normalization,Computational linguistics; Indexing (of information); Information retrieval; Mathematical models; Text processing; Indexing strategies; Pivoted normalization; Pseudo-relevance feedback (PRF); Vector space model (VSM); Query languages
Two-phase learning for biological event extraction and verification,2006,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33748438269&doi=10.1145%2f1131348.1131353&partnerID=40&md5=667b672cf9c1a1474c99e8da11d1a00f,"Many previous biological event-extraction systems were based on hand-crafted rules which were specifically tuned to a specific biological application domain. But manually constructing and tuning the rules are time-consuming processes and make the systems less portable. So supervised machine-learning methods were developed to generate the extraction rules automatically, but accepting the trade-off between precision and recall (high recall with low precision, and vice versa) is a barrier to improving performance. To make matters worse, a text in the biological domain is more complex because it often contains more than two biological events in a sentence, and one event in a noun chunk can be an entity for the other event. As a result, there are as yet no systems that give a good performance in extracting events in biological domains by using supervised machine learning. To overcome the limitations of previous systems and the complexity of biological texts, we present the following new ideas. First, we adopted a supervised machine-learning method to reduce the human effort in making extraction rules in order to obtain a highly domain-portable system. Second, we overcame the classical trade-off between precision and recall by using an event component verification method. Thus, machine learning occurs in two phases in our architecture. In the first phase, the system focuses on improving recall in extracting events between biological entities during a supervised machine-learning period. After extracting the biological events with automatically learned rules, in the second phase the system removes incorrect biological events by verifying the extracted event components with a maximum entropy (ME) classification method. In other words, the system targets for high recall in the first phase and tries to achieve high precision with a classifier in the second phase. Finally, we improved a supervised machine-learning algorithm so that it could learn a rule in a noun chunk and a rule extending throughout a sentence at two different levels, separately, for nested biological events. © 2006 ACM.",Biological event extraction; Event component verification; Two-level supervised machine learning,Biomedical engineering; Classification (of information); Computational complexity; Feature extraction; Learning algorithms; Logic programming; Biological event extraction; Event component verification; Maximum entropy (ME); Learning systems
A machine transliteration model based on correspondence between graphemes and phonemes,2006,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33846623181&doi=10.1145%2f1194936.1194938&partnerID=40&md5=12de208b4b47b2a89ad9125c1a88832e,"Machine transliteration is an automatic method for converting words in one language into phonetically equivalent ones in another language. There has been growing interest in the use of machine transliteration to assist machine translation and information retrieval. Three types of machine transliteration models - -grapheme-based, phoneme-based, and hybrid - -have been proposed. Surprisingly, there have been few reports of efforts to utilize the correspondence between source graphemes and source phonemes, although this correspondence plays an important role in machine transliteration. Furthermore, little work has been reported on ways to dynamically handle source graphemes and phonemes. In this paper, we propose a transliteration model that dynamically uses both graphemes and phonemes, particularly the correspondence between them. With this model, we have achieved better performance - -improvements of about 15 to 41% in English-to-Korean transliteration and about 16 to 44% in English-to-Japanese transliteration - -than has been reported for other models. © 2006 ACM.",Grapheme and phoneme; Information retrieval; Machine transliteration; Natural language processing,Computational linguistics; Computer aided language translation; Information retrieval; Learning systems; Mathematical models; Semantics; Graphemes; Machine transliteration; Phonemes; Transliteration model; Natural language processing systems
ME-based biomedical named entity recognition using lexical knowledge,2006,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33748426379&doi=10.1145%2f1131348.1131350&partnerID=40&md5=6de2c8ff1d2386ebf666250f8180382f,"In this paper, we present a two-phase biomedical NE-recognition method based on a ME model: we first recognize biomedical terms and then assign appropriate semantic classes to the recognized terms. In the two-phase NE-recognition method, the performance of the term-recognition phase is very important, because the semantic classification is performed on the region identified at the recognition phase. In this study, in order to improve the performance of term recognition, we try to incorporate lexical knowledge into pre- and postprocessing of the term-recognition phase. In the preprocessing step, we use domain-salient words as lexical knowledge obtained by corpus comparison. In the postprocessing step, we utilize χ 2-based collocations gained from Medline corpus. In addition, we use morphological patterns extracted from the training data as features for learning the ME-based classifiers. Experimental results show that the performance of NE-recognition can be improved by utilizing such lexical knowledge. © 2006 ACM.",Biomedical term recognition; Collocations; Maximum-entropy model; Morphological patterns; Postprocessing; Preprocessing; Salient words; Semantic classification,Classification (of information); Feature extraction; Knowledge representation; Semantics; Biomedical term recognition; Maximum-entropy model; Morphological patterns; Semantic classification; Biomedical engineering
Guest Editorial Text Mining and Management in Biomedicine,2006,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33748422692&doi=10.1145%2f1131348.1131349&partnerID=40&md5=3f08302115f3cb02909051f169c2e7a7,[No abstract available],,
Introduction to the special section: Extended best papers from IJCNLP 2005,2006,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33846625045&doi=10.1145%2f1194936.1194937&partnerID=40&md5=23af52c85c4411058e4efd08a8fe5ff6,[No abstract available],Natural language processing,
Semantic role labeling of prepositional phrases,2006,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33846618128&doi=10.1145%2f1194936.1194940&partnerID=40&md5=ac0fd7131af8da8fc709e83c903d3ab8,"We propose a method for labelling prepositional phrases according to two different semantic role classifications, as contained in the Penn treebank and the CoNLL 2004 Semantic Role Labeling data set. Our results illustrate the difficulties in determining preposition semantics, but also demonstrate the potential for PP semantic role labelling to improve the performance of a holistic semantic role labelling system. © 2006 ACM.",Preposition; Semantic role,Classification (of information); Computational grammars; Data reduction; Text processing; Preposition semantics; Prepositional phrases; Semantic role classifications; Semantic role labeling; Semantics
Mining semantically related terms from biomedical literature,2006,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33748415214&doi=10.1145%2f1131348.1131351&partnerID=40&md5=65b35fe3ba2e2483a7358cf8f0f542c8,"Discovering links and relationships is one of the main challenges in biomedical research, as scientists are interested in uncovering entities that have similar functions, take part in the same processes, or are coregulated. This article discusses the extraction of such semantically related entities (represented by domain terms) from biomedical literature. The method combines various text-based aspects, such as lexical, syntactic, and contextual similarities between terms. Lexical similarities are based on the level of sharing of word constituents. Syntactic similarities rely on expressions (such as term enumerations and conjunctions) in which a sequence of terms appears as a single syntactic unit. Finally, contextual similarities are based on automatic discovery of relevant contexts shared among terms. The approach is evaluated using the Genia resources, and the results of experiments are presented. Lexical and syntactic links have shown high precision and low recall, while contextual similarities have resulted in significantly higher recall with moderate precision. By combining the three metrics, we achieved F measures of 68% for semanticalty related terms and 37% for highly related entities. © 2006 ACM.",Biomedical literature; Contextual patterns; Term similarities; Text mining,Biomedical engineering; Engineering research; Feature extraction; Semantics; Biomedical literature; Contextual patterns; Term similarities; Text mining; Data mining
Effect of relationships between words on Japanese information retrieval,2006,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33846623641&doi=10.1145%2f1194936.1194942&partnerID=40&md5=34a272b9e39a15122704e4b98de2aaff,"Two Japanese-language information retrieval (IR) methods that enhance retrieval effectiveness by utilizing the relationships between words are proposed. The first method uses dependency relationships between words in a sentence. The second method uses proximity relationships, particularly information about the ordered co-occurrence of words in a sentence, to approximate the dependency relationships between them. A Structured Index has been constructed for these two methods, which represents the dependency relationships between words in a sentence as a set of binary trees. The Structured Index is created by morphological analysis and dependency analysis based on simple template matching and compound noun analysis derived from word statistics. Through retrieval experiments using the Japanese test collection for information retrieval systems (NTCIR-1, the NACSIS Test Collection for IR systems), it is shown that these two methods offer superior retrieval effectiveness compared with the TF - IDF method, and are effective with different databases and diverse search topics sets. There is little difference in retrieval effectiveness between these two methods. © 2006 ACM.",Co-occurrence; Compound noun analysis; Dependency relationships; Information retrieval; Morphological analysis; Natural language processing; NTCIR; Phrases; Proximity operation; Structured index; Test collection,Approximation theory; Computational linguistics; Database systems; Indexing (of information); Natural language processing systems; Semantics; Text processing; Compound noun analysis; Dependency relationships; Morphological analysis; Proximity operation; Structured index; Test collection; Information retrieval
Extracting contrastive information from negation patterns in biomedical literature,2006,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33748413821&doi=10.1145%2f1131348.1131352&partnerID=40&md5=922d887b5eb8e4da3d43652c0c0254ab,"Expressions of negation in the biomedical literature often encode information of contrast as a means for explaining significant differences between the objects that are so contrasted. We show that such information gives additional insights into the nature of the structures and/or biological functions of these objects, leading to valuable knowledge for subcategorization of protein families by the properties that the involved proteins do not have in common. Based on the observation that the expressions of negation employ mostly predictable syntactic structures that can be characterized by subclausal coordination and by clause-level parallelism, we present a system that extracts such contrastive information by identifying those syntactic structures with natural language processing techniques and with additional linguistic resources for semantics. The implemented system shows the performance of 85.7% precision and 61.5% recall, including 7.7% partial recall, or an P score of 76.6. We apply the system to the biological interactions as extracted by our biomedical information-extraction system in order to enrich proteome databases with contrastive information. © 2006 ACM.",Biomedical literature; Contrastive information; Information extraction,Biomedical engineering; Data structures; Feature extraction; Knowledge representation; Linguistics; Proteins; Biological functions; Biomedical literature; Contrastive information; Information extraction; Information analysis
An empirical study on language model adaptation,2006,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33846648632&doi=10.1145%2f1194936.1194939&partnerID=40&md5=5d964d244c974ff75706c65518aaaa25,"This article presents an empirical study of four techniques for adapting language models, including a maximum a posteriori (MAP) method and three discriminative training models, in the application of Japanese Kana-Kanji conversion. We compare the performance of these methods from various angles by adapting the baseline model to four adaptation domains. In particular, we attempt to interpret the results in terms of the character error rate (CER) by correlating them with the characteristics of the adaptation domain, measured by using the information-theoretic notion of cross entropy. We show that such a metric correlates well with the CER performance of the adaptation methods, and also show that the discriminative methods are not only superior to a MAP-based method in achieving larger CER reduction, but also in having fewer side effects and being more robust against the similarity between background and adaptation domains. © 2006 ACM.",Asian language text input; Discriminative training; Domain adaption; Entropy; Statistical language modeling,Character recognition; Computational linguistics; Correlation theory; Data processing; Error analysis; Mathematical models; Robustness (control systems); Character error rate (CER); Discriminative training; Domain adaption; Statistical language modeling; Formal languages
Alignment of bilingual named entities in parallel corpora using statistical models and multiple knowledge sources,2006,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33749554399&doi=10.1145%2f1165255.1165257&partnerID=40&md5=01b948037d52958660e3e5eec9d3c51b,"Named entity (NE) extraction is one of the fundamental tasks in natural language processing (NLP). Although many studies have focused on identifying NEs within monolingual documents, aligning NEs in bilingual documents has not been investigated extensively due to the complexity of the task. In this article we introduce a new approach to aligning bilingual NEs in parallel corpora by incorporating statistical models with multiple knowledge sources. In our approach, we model the process of translating an English NE phrase into a Chinese equivalent using lexical translation/transliteration probabilities for word translation and alignment probabilities for word reordering. The method involves automatically learning phrase alignment and acquiring word translations from a bilingual phrase dictionary and parallel corpora, and automatically discovering transliteration transformations from a training set of name-transliteration pairs. The method also involves language-specific knowledge functions, including handling abbreviations, recognizing Chinese personal names, and expanding acronyms. At runtime, the proposed models are applied to each source NE in a pair of bilingual sentences to generate and evaluate the target NE candidates; the source and target NEs are then aligned based on the computed probabilities. Experimental results demonstrate that the proposed approach, which integrates statistical models with extra knowledge sources, is highly feasible and offers significant improvement in performance compared to our previous work, as well as the traditional approach of IBM Model 4. © 2006 ACM.",Named entity; Named entity alignment; Parallel corpora; Phrase translation; Transliteration,Computational complexity; Knowledge based systems; Natural language processing systems; Statistical methods; Named entity (NE) extraction; Natural language processing (NLP); Parallel corpora; Transliteration; Linguistics
Using Japanese honorific expressions: A psychological study,2006,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33749559194&doi=10.1145%2f1165255.1165258&partnerID=40&md5=63fd8910e23a71b82b9b9405c5253a76,"We investigated, via experiment, knowledge of normative honorific expressions as used in textbooks and in practice by people. Forty subjects divided into four groups according to age (younger/older) and gender (male/female) participated in the experiments. The results show that knowledge about the use of normative honorific expressions in textbooks is similar to that demonstrated by the younger subject groups, but differed from that of the older subject groups. The knowledge of the older subjects was more complex than that shown in textbooks or demonstrated by the younger subjects. A model that can identify misuse of honorific expressions in sentences is the framework for this investigation. The model is minimal, but could represent 76% to 92% of the subjects' knowledge regarding each honorific element. This model will be useful in the development of computer-aided systems to help teach how honorific expressions should be used. © 2006 ACM.",Honorific expressions; Japanese; Misuse,Computer aided analysis; Social aspects; Textbooks; Computer-aided systems; Honorific expressions; Subject groups; Knowledge based systems
Aligning word senses using bilingual corpora,2006,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33749551453&doi=10.1145%2f1165255.1165256&partnerID=40&md5=598f19a6464c2db7925352873f638ee8,"The growing importance of multilingual information retrieval and machine translation has made multilingual ontologies extremely valuable resources. Since the construction of an ontology from scratch is a very expensive and time-consuming undertaking, it is attractive to consider ways of automatically aligning monolingual ontologies, which already exist for many of the world's major languages. Previous research exploited similarity in the structure of the ontologies to align, or manually created bilingual resources. These approaches cannot be used to align ontologies with vastly different structures and can only be applied to much studied language pairs for which expensive resources are already available. In this paper, we propose a novel approach to align the ontologies at the node level: Given a concept represented by a particular word sense in one ontology, our task is to find the best corresponding word sense in the second language ontology. To this end, we present a language-independent, corpus-based method that borrows from techniques used in information retrieval and machine translation. We show its efficiency by applying it to two very different ontologies in very different languages: the Mandarin Chinese HowNet and the American English WordNet. Moreover, we propose a methodology to measure bilingual corpora comparability and show that our method is robust enough to use noisy nonparallel bilingual corpora efficiently, when clean parallel corpora are not available. © 2006 ACM.",Algorithms; Languages,Computer programming languages; Information retrieval; Resource allocation; Machine translation; Monolingual ontologies; Multilingual information; Ontology; Linguistics
Emotion recognition from text using semantic labels and separable mixture models,2006,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33749552624&doi=10.1145%2f1165255.1165259&partnerID=40&md5=6c021bc7bd7bd23fa7f3f086588f4905,"This study presents a novel approach to automatic emotion recognition from text. First, emotion generation rules (EGRs) are manually deduced from psychology to represent the conditions for generating emotion. Based on the EGRs, the emotional state of each sentence can be represented as a sequence of semantic labels (SLs) and attributes (ATTs); SLs are defined as the domain-independent features, while ATTs are domain-dependent. The emotion association rules (EARs) represented by SLs and ATTs for each emotion are automatically derived from the sentences in an emotional text corpus using the a priori algorithm. Finally, a separable mixture model (SMM) is adopted to estimate the similarity between an input sentence and the EARs of each emotional state. Since some features defined in this approach are domain-dependent, a dialog system focusing on the students' daily expressions is constructed, and only three emotional states, happy, unhappy, and neutral, are considered for performance evaluation. According to the results of the experiments, given the domain corpus, the proposed approach is promising, and easily ported into other domains. © 2006 ACM.",Emotion extraction,Algorithms; Mathematical models; Psychology computing; Semantics; Dialog system; Domain corpus; Emotion association rules (EAR); Semantic labels (SL); Pattern recognition
Terminology-based knowledge mining for new knowledge discovery,2006,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33748438867&doi=10.1145%2f1131348.1131354&partnerID=40&md5=52e1bc423239b83a5dcbe3ce7671fb5e,"In this article we present an integrated knowledge-mining system for the domain of biomedicine, in which automatic term recognition, term clustering, information retrieval, and visualization are combined. The primary objective of this system is to facilitate knowledge acquisition from documents and aid knowledge discovery through terminology-based similarity calculation and visualization of automatically structured knowledge. This system also supports the integration of different types of databases and simultaneous retrieval of different types of knowledge. In order to accelerate knowledge discovery, we also propose a visualization method for generating similarity-based knowledge maps. The method is based on real-time terminology-based knowledge clustering and categorization and allows users to observe real-time generated knowledge maps, graphically. Lastly, we discuss experiments using the GENIA corpus to assess the practicality and applicability of the system. © 2006 ACM.",Automatic term recognition; Biomedicine; Natural language processing; Structuring knowledge; Terminology; Visualization,Biomedical engineering; Database systems; Information retrieval; Natural language processing systems; Pattern recognition; Automatic term recognition; Biomedicine; Information visualization; Structuring knowledge; Knowledge acquisition
Introduction to the Special Issue: Recent Advances in Information Processing and Access for Japanese,2005,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024244685&doi=10.1145%2f1113308.1113309&partnerID=40&md5=d658e28c1b06ef032ddba5de989110ec,[No abstract available],,
Anaphora resolution by antecedent identification followed by anaphoricity determination,2005,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745177326&doi=10.1145%2f1113308.1113312&partnerID=40&md5=5f6dc0eb1e80800a653ed86c2855c8b5,"We propose a machine learning-based approach to noun-phrase anaphora resolution that combines the advantages of previous learning-based models while overcoming their drawbacks. Our anaphora resolution process reverses the order of the steps in the classification-then-search model proposed by Ng and Cardie [2002b], inheriting all the advantages of that model. We conducted experiments on resolving noun-phrase anaphora in Japanese. The results show that with the selection-then-classification-based modifications, our proposed model outperforms earlier learning-based approaches. © 2005 ACM.",Anaphora resolution; Anaphoricity determination; Antecedent identification,Classification (of information); Learning systems; Mathematical models; Anaphora resolution; Anaphoricity determination; Antecedent identification; Natural language processing systems
Acquiring causal knowledge from text using the connective marker tame,2005,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745125716&doi=10.1145%2f1113308.1113313&partnerID=40&md5=eb04fe3c464e3b9497b93b1b3f170921,"In this paper, we deal with automatic knowledge acquisition from text, specifically the acquisition of causal relations. A causal relation is the relation existing between two events such that one event causes (or enables) the other event, such as ""hard rain causes flooding"" or ""taking a train requires buying a ticket."" In previous work these relations have been classified into several types based on a variety of points of view. In this work, we consider four types of causal relations-cause, effect, precond(ition) and means-mainly based on agents' volitionality, as proposed in the research field of discourse understanding. The idea behind knowledge acquisition is to use resultative connective markers, such as ""because,"" ""but,"" and ""if"" as linguistic cues. However, there is no guarantee that a given connective marker always signals the same type of causal relation. Therefore, we need to create a computational model that is able to classify samples according to the causal relation. To examine how accurately we can automatically acquire causal knowledge, we attempted an experiment using Japanese newspaper articles, focusing on the resultative connective ""tame."" By using machine-learning techniques, we achieved 80% recall with over 95% precision for the cause, precond, and means relations, and 30% recall with 90% precision for the effect relation. Furthermore, the classification results suggest that one can expect to acquire over 27,000 instances of causal relations from 1 year of Japanese newspaper articles. © 2005 ACM.",Causal relation; Connective marker; Volitionality,Computational methods; Data processing; Learning systems; Mathematical models; Causal relation; Connective marker; Volitionality; Knowledge acquisition
Preface to the special issues on NTCIR-4,2005,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745175882&doi=10.1145%2f1111667.1111668&partnerID=40&md5=7931b2abe118ef5d0bcb069fdd201448,[No abstract available],,
Improving chronological ordering of sentences extracted from multiple newspaper articles,2005,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745163931&doi=10.1145%2f1111667.1111673&partnerID=40&md5=c3c5ab6535e7c6693c44f0f706caeda2,"It is necessary to determine a proper arrangement of extracted sentences to generate a well-organized summary from multiple documents. This paper describes our Multi-Document Summarization (MDS) system for TSC-3. It specifically addresses an approach to coherent sentence ordering for MDS. An impediment to the use of chronological ordering, which is widely used by conventional summarization system, is that it arranges sentences without considering the presupposed information of each sentence. We propose a method to improve chronological ordering by resolving precedent information of arranging sentences. Combining the refinement algorithm with topical segmentation and chronological ordering, we address our experiments and metrics to test the effectiveness of MDS tasks. Results demonstrate that the proposed method significantly improves chronological sentence ordering. At the end of the paper, we also report an outline/evaluation of important sentence extraction and redundant clause elimination integrated in our MDS system. © 2005 ACM.",Arrange; Coherence; Multi-document summarization; Order; Sentence ordering,Algorithms; Evaluation; Information Retrieval; Newsprint; Algorithms; Coherent light; Information retrieval; Newsprint; Arrange; Multi document summarization; Order; Sentence ordering; Information science
Example-based machine translation using efficient sentence retrieval based on edit-distance,2005,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745124363&doi=10.1145%2f1113308.1113310&partnerID=40&md5=995b0cc7c1865edd27628fb5c445d5ae,"An Example-Based Machine Translation (EBMT) system, whose translation example unit is a sentence, can produce an accurate and natural translation if translation examples similar enough to an input sentence are retrieved. Such a system, however, suffers from the problem of narrow coverage. To reduce the problem, a large-scale parallel corpus is required and, therefore, an efficient method is needed to retrieve translation examples from a large-scale corpus. The authors propose an efficient retrieval method for a sentence-wise EBMT using edit-distance. The proposed retrieval method efficiently retrieves the most similar sentences using the measure of edit-distance without omissions. The proposed method employs search-space division, word graphs, and an A* search algorithm. The performance of the EBMT was evaluated through Japanese-to-English translation experiments using a bilingual corpus comprising hundreds of thousands of sentences from a travel conversation domain. The EBMT system achieved a high-quality translation ability by using a large corpus and also achieved efficient processing by using the proposed retrieval method. © 2005 ACM.",A* search; Edit-distance; Example retrieval; Example-based machine translation; Word graph,Algorithms; Data processing; Information retrieval; Natural language processing systems; A; Edit-distance; Example retrieval; Example-based machine translation; search; Word graph; Computer aided language translation
Are open-domain question answering technologies useful for information access dialogues? - An empirical study and a proposal of a novel challenge,2005,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745122506&doi=10.1145%2f1111667.1111669&partnerID=40&md5=a5ff9a44aace98ebb8b2a2a2a28b88df,"There are strong expectations for the use of question answering technologies in information access dialogues, such as for information gathering and browsing. In this paper, we empirically examine what kinds of abilities are needed for question answering systems in such situations, and propose a challenge for evaluating those abilities objectively and quantitatively. We also show that existing technologies have the potential to address this challenge. From the empirical study, we found that questions that have values and names as answers account for a majority in realistic information-gathering situations and that those sequences of questions contain a wide range of reference expressions and are sometimes complicated by the inclusion of subdialogues and focus shifts. The challenge proposed is not only novel as an evaluation of the handling of information access dialogues, but also includes several valuable ideas such as categorization and characterization of information access dialogues, and introduces three measures to evaluate various aspects in addressing list-type questions and reference test sets for evaluating context-processing ability in isolation. © 2005 ACM.",Evaluation; Information access dialogue; Question answering,Computer science; Computer systems; Data handling; Evaluation; Information analysis; Web browsers; Information access; Information access dialogue; Information gathering; Question answering; Information technology
Estimating satisfactoriness of selectional restriction from corpus without a thesaurus,2005,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745176969&doi=10.1145%2f1113308.1113311&partnerID=40&md5=730fdc4c152670a8fd8d9432b52289dd,"A selectional restriction specifies what combinations of words are semantically valid in a particular syntactic construction. This is one of the basic and important pieces of knowledge in natural language processing and has been used for syntactic and word sense disambiguation. In the case of acquiring the selectional restriction for many combinations of words from a corpus, it is necessary to estimate whether or not a word combination that is not observed in the corpus satisfies the selectional restriction. This paper proposes a new method for estimating the degree of satisfaction of the selectional restriction for a word combination from a tagged corpus, based on the multiple regression model. The independent variables of this model correspond to modifiers. Unlike a conventional multiple regression analysis, the independent variables are also parameters to be learned. We experiment on estimating the degree of satisfaction of the selectional restriction for Japanese word combinations (noun, postpositional-particle, verb). The experimental results indicate that our method estimates the degree of satisfaction of a word combination not very well observed in the corpus, and that the accuracy of syntactic disambiguation using the co-occurrencies estimated by our method is higher than using co-occurrence probabilities smoothed by previous methods. © 2005 ACM.",Co-occurrence of word combination; Multiple regression model; Similarity between words with respect to co-occurrence; Syntactic disambiguation,Mathematical models; Parameter estimation; Probability; Regression analysis; Syntactics; Thesauri; Co-occurrence of word combination; Multiple regression model; Similarity between words with respect to co-occurrence; Syntactic disambiguation; Natural language processing systems
Topic-structure-based complementary information retrieval and its application,2005,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33744923229&doi=10.1145%2f1113308.1113314&partnerID=40&md5=89dc3548d97f68a352305c9a7cd431b5,"A great deal of technology has been developed to help people access the information they require. With advances in the availability of information, information-seeking activities are becoming more sophisticated. This means that information technology must move to the next stage, i.e., enable users to acquire information from multiple perspectives to satisfy diverse needs. For instance, with the spread of digital broadcasting and broadband Internet connection services, infrastructure for the integration of TV programs and the Internet has been developed that enables users to acquire information from different media at the same time to improve information quality and the level of detail. In this paper, we propose a novel content-based join model for data streams (closed captions of videos or TV programs) and Web pages based on the concept of topic structures. We then propose a mechanism based on this model for retrieving complementary Web pages to augment the content of video or television programs. One of the most notable features of this complementary retrieval mechanism is that the retrieved information is not just similar to the video or TV program, but also provides additional information. In addition, we introduce an application system called WebTelop, which augments the content of TV programs in real time by using complementary Web pages. We also describe some experimental results. © 2005 ACM.",Complementary information retrieval; Content fusion; Information complementation; Topic structure,Broadband networks; Data processing; Information technology; Internet; Mathematical models; Quality of service; Television broadcasting; Complementary information retrieval; Content fusion; Information complementation; Topic structure; WebTelop; Information retrieval
An analysis of a high-performance Japanese question answering system,2005,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745133381&doi=10.1145%2f1111667.1111670&partnerID=40&md5=576aa5bde67cd6d4cc92ff8119aa9827,"Twenty-five Japanese Question Answering systems participated in NTCIR QAC2 subtask 1. Of these, our system SAIQA-QAC2 performed the best: MRR = 0.607. SAIQA-QAC2 is an improvement on our previous system SAIQA-Ii that achieved MRR = 0.46 for QAC1. We mainly improved the answer-type determination module and the retrieval module. In general, a fine-grained answer taxonomy improves QA performance but it is difficult to build an accurate answer extraction module for the fine-grained taxonomy because Machine Learning methods require a huge training corpus and hand-crafted rules are hard to maintain. Therefore, we built a fine-grained system by using a coarse-grained named entity recognizer and a Japanese lexicon ""Nihongo Goi-taikei."" Our experiments show that named entity/numerical expression recognition and word sense-based answer extraction mainly contributed to the performance. In addition, we developed a new proximity-based document retrieval module that performs better than BM25. We also compared its performance with MultiText, a conventional proximity-based retrieval method developed for QA. © 2005 ACM.",Document retrieval; Question answering,Data processing; Information retrieval; Learning systems; Numerical analysis; Taxation; Document retrieval; Modules; Question answering; Question answering system; Natural language processing systems
Multi-answer-focused multi-document summarization using a question-answering engine,2005,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745175509&doi=10.1145%2f1111667.1111672&partnerID=40&md5=539bf01268a64e8489ec9bb9be827544,"In recent years, answer-focused summarization has gained attention as a technology complementary to information retrieval and question answering. In order to realize multi-document summarization focused by multiple questions, we propose a method to calculate sentence importance using scores, for responses to multiple questions, generated by a Question-Answering engine. Further, we describe the integration of this method with a generic multi-document summarization system. The evaluation results demonstrate that the performance of the proposed method is better than not only several baselines but also other participants' systems at the evaluation workshop NTCIR4 TSC3 Formal Run. However, it should be noted that some of the other systems do not use the information of questions. © 2005 ACM.",Information gain ratio; Maximal marginal relevance; Question-answering engine,Computer systems; Information retrieval; Information technology; Integration; Search engines; Information gain ratio; Maximal marginal relevance; Question-answering engine; Summarization; Information analysis
Japanese question-answering system using A* search and its improvement,2005,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745167322&doi=10.1145%2f1111667.1111671&partnerID=40&md5=6485c443f0e9b506ff87e69f1b0c329e,"We have proposed a method to introduce A* search control in a sentential matching mechanism for Japanese question-answering systems in order to reduce the turnaround time while maintaining the accuracy of the answers. Using this method, preprocessing need not be performed on a document database and we may use any information retrieval systems by writing a simple wrapper program. However, the disadvantage is that the accuracy is not sufficiently high and the mean reciprocal rank (MRR) is approximately 0.3 in NTCIR3 QAC1, an evaluation workshop for question-answering systems. In order to improve the accuracy, we propose several measures of the degree of sentence matching and a variant of a voting method. Both of them can be integrated with our system of controlled search. Using these techniques, the system achieves a higher MRR of 0.5 in the evaluation workshop NTCIR4 QAC2. © 2005 ACM.",A* search; Dependency vectors; Pseudo voting method; Question answering; Sentence chaining,Computer software; Computer systems; Database systems; Evaluation; Information analysis; Information retrieval; A; Dependency vectors; Pseudo voting method; Question answering; search; Sentence chaining; Information science
Chinese information retrieval based on terms and relevant terms,2005,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745127725&doi=10.1145%2f1111667.1111675&partnerID=40&md5=ed30aee4be2af087159abe55e44dc6b3,"In this article we describe our approach to Chinese information retrieval, where a query is a short natural language description. First, we use automatically extracted short terms from document sets to build indexes and use the short terms in both the query and documents to do initial retrieval. Next, we use long terms extracted from the document collection to reorder the top N retrieved documents to improve precision. Finally, we acquire the relevant terms of the short terms from the Internet and the top retrieved documents and use them to do query expansion. Experiments on the NTCIR-4 CLIR Chinese SLIR sub-collection show that document reranking can both improve the retrieval performance on its own and make a significant contribution to query expansion. The experiments also show that the extended query expansion proposed in this article is more effective than the standard Rocchio query expansion. © 2005 ACM.",Document re-ranking; Information retrieval; Query expansion; Relevant term; Term clustering; Term extraction,Data processing; Information science; Internet; Natural language processing systems; Query languages; Document re-ranking; Query expansion; Relevant term; Term clustering; Term extraction; Information retrieval
Introduction to the special issue: Recent advances in information processing and access for Japanese,2005,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745152333&doi=10.1111%2fj.1442-2042.2005.01070.x&partnerID=40&md5=cefd93639d5443089972d60cd775bc52,[No abstract available],,
On a combination of probabilistic and Boolean IR models for WWW document retrieval,2005,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745143239&doi=10.1145%2f1111667.1111674&partnerID=40&md5=55b3f4c3325e415fc016a831b36a9e75,"Even though a Boolean query can express the information need precisely enough to select relevant documents, it is not easy to construct an appropriate Boolean query that covers all relevant documents. To utilize a Boolean query effectively, a mechanism to retrieve as many as possible relevant documents is therefore required. In accordance with this requirement, we propose a method for modifying a given Boolean query by using information from a relevant document set. The retrieval results, however, may deteriorate if some important query terms are removed by this reformulation. A further mechanism is thus required in order to use other query terms that are useful for finding more relevant documents, but are not strictly required in relevant documents. To meet this requirement, we propose a new method that combines the probabilistic IR and the Boolean IR models. We also introduce a new IR system - called appropriate Boolean query reformulation for information retrieval (ABRIR) - based on these two methods and the Okapi system. ABRIR uses both a word index and a phrase index formed from combinations of two adjacent noun words. The effectiveness of these two methods was confirmed according to the NTCIR-4 Web test collection. © 2005 ACM.",Boolean IR model; Probabilistic IR model,Boolean algebra; Data processing; Information science; Mathematical models; Probabilistic logics; World Wide Web; Boolean IR model; Okapi system; Probabilistic IR model; Reformulation; Information retrieval
Proposal of Two-Stage Patent Retrieval Method Considering the Claim Structure,2005,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016706269&doi=10.1145%2f1105696.1105702&partnerID=40&md5=5b968fdd72014c968d75a13c6d9eb785,"The importance of patents is increasing in global society. In preparing a patent application, it is essential to search for related patents that may invalidate the invention. However, it is time-consuming to identify them among the millions of patents. This article proposes a patent-retrieval method that considers a claim structure for a more accurate search for invalidity. This method uses a claim text as input; it consists of two retrieval stages. In stage 1, general text analysis and retrieval methods are applied to improve recall. In stage 2, the top N documents retrieved in stage 1 are rearranged to improve precision by applying text analysis and retrieval methods using the claim structure. Our two-stage retrieval introduces five precision-oriented analysis and retrieval methods: query-term extraction from a portion of a claim text that describes the characteristics of a claim; query term-weighting without term frequency; query term-weighting with “measurement terms” text retrieval using only claims as a target; and calculating the relevant score by “partially” adding scores in stage 2 to those in stage 1. Evaluation results using test sets of the NTCIR4 Patent Retrieval Task show that our methods are effective, though the degree of the effectiveness varies depending on the test sets. © The importance of patents is increasing in global society. In preparing a patent application, it is essential to search for related patents that may invalidate the invention. However, it is time-consuming to identify them among the millions of patents. This article proposes a patent-retrieval method that considers a claim structure for a more accurate search for invalidity. This method uses a claim text as input; it consists of two retrieval stages. In stage 1, general text analysis and retrieval methods are applied to improve recall. In stage 2, the top N documents retrieved in stage 1 are rearranged to improve precision by applying text analysis and retrieval methods using the claim structure. Our two-stage retrieval introduces five precision-oriented analysis and retrieval methods: query-term extraction from a portion of a claim text that describes the characteristics of a claim; query term-weighting without term frequency; query term-weighting with “measurement terms” text retrieval using only claims as a target; and calculating the relevant score by “partially” adding scores in stage 2 to those in stage 1. Evaluation results using test sets of the NTCIR4 Patent Retrieval Task show that our methods are effective, though the degree of the effectiveness varies depending on the test sets. © 2005, ACM. All rights reserved.",Algorithms; claim structure; Experimentation; Languages; Patent retrieval; relevant score calculation; term extraction; term weighting,
Towards Effective Strategies for Monolingual and Bilingual Information RetrievalRetrieval: Lessons Learned from NTCIR-4,2005,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33645969110&doi=10.1145%2f1105696.1105698&partnerID=40&md5=8a6fdaa311b6945f4654a792fad49922,"At the NTCIR-4 workshop, Justsystem Corporation (JSC) and Clairvoyance Corporation (CC) collaborated in the cross-language retrieval task (CLIR). Our goal was to evaluate the performance and robustness of our recently developed commercial-grade CLIR systems for English and Asian languages. The main contribution of this article is the investigation of different strategies, their interactions in both monolingual and bilingual retrieval tasks, and their respective contributions to operational retrieval systems in the context of NTCIR-4. We report results of Japanese and English monolingual retrieval and results of Japanese-to-English bilingual retrieval. In monolingual retrieval analysis, we examine two special properties of the NTCIR experimental design (two levels of relevance and identical queries in multiple languages) and explore how they interact with strategies of our retrieval system, including pseudo-relevance feedback, multi-word term down-weighting, and term weight merging strategies. Our analysis shows that the choice of language (English or Japanese) does not have a significant impact on retrieval performance. Query expansion is slightly more effective with relaxed judgments than with rigid judgments. For better retrieval performance, weights of multi-word terms should be lowered. In the bilingual retrieval analysis, we aim to identify robust strategies that are effective when used alone and when used in combination with other strategies. We examine cross-lingual specific strategies such as translation disambiguation and translation structuring, as well as general strategies such as pseudo-relevance feedback and multi-word term down-weighting. For shorter title topics, pseudo-relevance feedback is a major performance enhancer, but translation structuring affects retrieval performance negatively when used alone or in combination with other strategies. All experimented strategies improve retrieval performance for the longer description topics, with pseudo-relevance feedback and translation structuring as the major contributors. © At the NTCIR-4 workshop, Justsystem Corporation (JSC) and Clairvoyance Corporation (CC) collaborated in the cross-language retrieval task (CLIR). Our goal was to evaluate the performance and robustness of our recently developed commercial-grade CLIR systems for English and Asian languages. The main contribution of this article is the investigation of different strategies, their interactions in both monolingual and bilingual retrieval tasks, and their respective contributions to operational retrieval systems in the context of NTCIR-4. We report results of Japanese and English monolingual retrieval and results of Japanese-to-English bilingual retrieval. In monolingual retrieval analysis, we examine two special properties of the NTCIR experimental design (two levels of relevance and identical queries in multiple languages) and explore how they interact with strategies of our retrieval system, including pseudo-relevance feedback, multi-word term down-weighting, and term weight merging strategies. Our analysis shows that the choice of language (English or Japanese) does not have a significant impact on retrieval performance. Query expansion is slightly more effective with relaxed judgments than with rigid judgments. For better retrieval performance, weights of multi-word terms should be lowered. In the bilingual retrieval analysis, we aim to identify robust strategies that are effective when used alone and when used in combination with other strategies. We examine cross-lingual specific strategies such as translation disambiguation and translation structuring, as well as general strategies such as pseudo-relevance feedback and multi-word term down-weighting. For shorter title topics, pseudo-relevance feedback is a major performance enhancer, but translation structuring affects retrieval performance negatively when used alone or in combination with other strategies. All experimented strategies improve retrieval performance for the longer description topics, with pseudo-relevance feedback and translation structuring as the major contributors. © 2005, ACM. All rights reserved.",comparison; cross-language information retrieval; Design; Experimentation; Languages; Measurement; Monolingual information retrieval; NTCIR; Performance,
Flexible Pseudo-Relevance Feedback via Selective Sampling,2005,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33750320351&doi=10.1145%2f1105696.1105699&partnerID=40&md5=c4182a4302528077854f395045323c77,"Although Pseudo-Relevance Feedback (PRF) is a widely used technique for enhancing average retrieval performance, it may actually hurt performance for around one-third of a given set of topics. To enhance the reliability of PRF, Flexible PRF has been proposed, which adjusts the number of pseudo-relevant documents and/or the number of expansion terms for each topic. This paper explores a new, inexpensive Flexible PRF method, called Selective Sampling, which is unique in that it can skip documents in the initial ranked output to look for more “novel” pseudo-relevant documents. While Selective Sampling is only comparable to Traditional PRF in terms of average performance and reliability, per-topic analyses show that Selective Sampling outperforms Traditional PRF almost as often as Traditional PRF outperforms Selective Sampling. Thus, treating the top P documents as relevant is often not the best strategy. However, predicting when Selective Sampling outperforms Traditional PRF appears to be as difficult as predicting when a PRF method fails. For example, our per-topic analyses show that even the proportion of truly relevant documents in the pseudo-relevant set is not necessarily a good performance predictor. © Although Pseudo-Relevance Feedback (PRF) is a widely used technique for enhancing average retrieval performance, it may actually hurt performance for around one-third of a given set of topics. To enhance the reliability of PRF, Flexible PRF has been proposed, which adjusts the number of pseudo-relevant documents and/or the number of expansion terms for each topic. This paper explores a new, inexpensive Flexible PRF method, called Selective Sampling, which is unique in that it can skip documents in the initial ranked output to look for more “novel” pseudo-relevant documents. While Selective Sampling is only comparable to Traditional PRF in terms of average performance and reliability, per-topic analyses show that Selective Sampling outperforms Traditional PRF almost as often as Traditional PRF outperforms Selective Sampling. Thus, treating the top P documents as relevant is often not the best strategy. However, predicting when Selective Sampling outperforms Traditional PRF appears to be as difficult as predicting when a PRF method fails. For example, our per-topic analyses show that even the proportion of truly relevant documents in the pseudo-relevant set is not necessarily a good performance predictor. © 2005, ACM. All rights reserved.",Experimentation; flexible pseudo-relevance feedback; Performance; Pseudo-relevance feedback; selective sampling,
Comparative Study of Monolingual and Multilingual Search Models for Use with Asian Languages,2005,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016697546&doi=10.1145%2f1105696.1105701&partnerID=40&md5=85e8ba57bf573bd7b0160a8cfe8a90fc,"Based on the NTCIR-4 test-collection, our first objective is to present an overview of the retrieval effectiveness of nine vector-space and two probabilistic models that perform monolingual searches in the Chinese, Japanese, Korean, and English languages. Our second goal is to analyze the relative merits of the various automated and freely available toolsto translate the English-language topics into Chinese, Japanese, or Korean, and then submit the resultant query in order to retrieve pertinent documents written in one of the three Asian languages. We also demonstrate how bilingual searches could be improved by applying both the combined query translation strategies and data-fusion approaches. Finally, we address basic problems related to multilingual searches, in which queries written in English are used to search documents written in the English, Chinese, Japanese, and Korean languages. © Based on the NTCIR-4 test-collection, our first objective is to present an overview of the retrieval effectiveness of nine vector-space and two probabilistic models that perform monolingual searches in the Chinese, Japanese, Korean, and English languages. Our second goal is to analyze the relative merits of the various automated and freely available toolsto translate the English-language topics into Chinese, Japanese, or Korean, and then submit the resultant query in order to retrieve pertinent documents written in one of the three Asian languages. We also demonstrate how bilingual searches could be improved by applying both the combined query translation strategies and data-fusion approaches. Finally, we address basic problems related to multilingual searches, in which queries written in English are used to search documents written in the English, Chinese, Japanese, and Korean languages. © 2005, ACM. All rights reserved.",Algorithms; Chinese language; cross-language information retrieval; Experimentation; Japanese language; Korean language; Measurement; Multilingual information retrieval; natural language processing with Asian languages; results-merging; search engines with Asian languages,
Rich Results From Poor Resources: NTCIR-4 Monolingual and Cross-Lingual Retrieval of Korean Texts Using Chinese and English,2005,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-39649086547&doi=10.1145%2f1105696.1105700&partnerID=40&md5=ee8d7aa9dfadb4b9e9037564b8743f1f,"We report on Korean monolingual, Chinese-Korean English-as-pivot bilingual, and Chinese-English bilingual CLIR experiments using MT software augmented with Web-based entity-oriented translation as resources in the NTCIR-4 environment. Simple stemming is helpful in improving bigram indexing for Korean retrieval. For word indexing, keeping nouns only is preferable. Web-based translation reduces untranslated terms left over after MT and substantially improves CLIR results. Translation concatenation is found to consistently improve CLIR effectiveness, while combining a retrieval list from bigram and word indexing is also helpful. A method to disambiguate multiple MT outputs using a log likelihood ratio threshold was tested. Depending on the nature of the title or description queries, bigram only or a retrieval combination, or relaxed or rigid evaluations, direct bilingual CLIR returned an average precision of 71-79% (English-Korean) and 76-84% (Chinese-English) of the corresponding Korean-Korean and English-English monolingual results. Using English as a pivot in Chinese-Korean CLIR provides about 55-65% the effectiveness that Korean alone does. Entity/terminology translation at the pivot language stage accounts for a large portion of this deficiency. A topic with comparatively worse Chinese-English bilingual result does not necessarily mean that it will continue to underperform (after further transitive Korean translation) at the Korean retrieval level. © We report on Korean monolingual, Chinese-Korean English-as-pivot bilingual, and Chinese-English bilingual CLIR experiments using MT software augmented with Web-based entity-oriented translation as resources in the NTCIR-4 environment. Simple stemming is helpful in improving bigram indexing for Korean retrieval. For word indexing, keeping nouns only is preferable. Web-based translation reduces untranslated terms left over after MT and substantially improves CLIR results. Translation concatenation is found to consistently improve CLIR effectiveness, while combining a retrieval list from bigram and word indexing is also helpful. A method to disambiguate multiple MT outputs using a log likelihood ratio threshold was tested. Depending on the nature of the title or description queries, bigram only or a retrieval combination, or relaxed or rigid evaluations, direct bilingual CLIR returned an average precision of 71-79% (English-Korean) and 76-84% (Chinese-English) of the corresponding Korean-Korean and English-English monolingual results. Using English as a pivot in Chinese-Korean CLIR provides about 55-65% the effectiveness that Korean alone does. Entity/terminology translation at the pivot language stage accounts for a large portion of this deficiency. A topic with comparatively worse Chinese-English bilingual result does not necessarily mean that it will continue to underperform (after further transitive Korean translation) at the Korean retrieval level. © 2005, ACM. All rights reserved.",bigram indexing; Chinese-English-Korean pivot CLIR; Chinese-Korean CLIR; Experimentation; translation disambiguation; Web-based entity-oriented translation,
Chinese OOV Translation and Post-translation Query Expansion in Chinese-English Cross-lingual Information Retrieval,2005,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33751365487&doi=10.1145%2f1105696.1105697&partnerID=40&md5=7e519668b083a6ff6cd47e0b74843421,"Cross-lingual information retrieval allows users to query mixed-language collections or to probe for documents written in an unfamiliar language. A major difficulty for cross-lingual information retrieval is the detection and translation of out-of-vocabulary (OOV) terms; for OOV terms in Chinese, another difficulty is segmentation. At NTCIR-4, we explored methods for translation and disambiguation for OOV terms when using a Chinese query on an English collection. We have developed a new segmentation-free technique for automatic translation of Chinese OOV terms using the web. We have also investigated the effects of distance factor and window size when using a hidden Markov model to provide disambiguation. Our experiments show these methods significantly improve effectiveness; in conjunction with our post-translation query expansion technique, effectiveness approaches that of monolingual retrieval. © Cross-lingual information retrieval allows users to query mixed-language collections or to probe for documents written in an unfamiliar language. A major difficulty for cross-lingual information retrieval is the detection and translation of out-of-vocabulary (OOV) terms; for OOV terms in Chinese, another difficulty is segmentation. At NTCIR-4, we explored methods for translation and disambiguation for OOV terms when using a Chinese query on an English collection. We have developed a new segmentation-free technique for automatic translation of Chinese OOV terms using the web. We have also investigated the effects of distance factor and window size when using a hidden Markov model to provide disambiguation. Our experiments show these methods significantly improve effectiveness; in conjunction with our post-translation query expansion technique, effectiveness approaches that of monolingual retrieval. © 2005, ACM. All rights reserved.",Algorithms; CLIR; HMM; Languages; mutual information; OOV terms; post-translation query expansion; query translations; translation disambiguation; web mining,
Revisiting Document Length Hypotheses: A Comparative Study of Japanese Newspaper and Patent Retrieval,2005,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-69249227950&doi=10.1145%2f1105696.1105853&partnerID=40&md5=60726a4d3d8abfd25f7696f8728ccc27,"NTCIR-4 experiments of the CLIR J-J (Japanese monolingual newspaper retrieval) and patent tasks are described, focusing on comparative studies of two test collections and two retrieval approaches in view of document length hypotheses. TF*IDF outperformed the language modeling approach in the CLIR J-J task whereas the language modeling approach performed better in the patent task. Two different document length hypotheses behind two tasks/collections are assumed by analyzing document length distributions of relevant/retrieved documents in the NTCIR-3 and -4 collections. Given these hypotheses, TF*IDF is easily adapted to patent retrieval tasks. Document length prior probabilities are applied to the language modeling approach. For the patent task, task-specific techniques, such as IPC priors and different indexing strategies, are evaluated and reported. To facilitate retrieval from large patent collections, a simple distributed search strategy is applied and found to be efficient, despite a slight deterioration of effectiveness. We found that TF*IDF performed similarly to the language modeling runs against the patent collection by controlling the document length normalization, whereas the language modeling approach does not perform as well as TF*IDF, despite calibration against the CLIR J-J collection. The different characteristics of the document lengths of the two test collections are illustrated through comparative studies. © NTCIR-4 experiments of the CLIR J-J (Japanese monolingual newspaper retrieval) and patent tasks are described, focusing on comparative studies of two test collections and two retrieval approaches in view of document length hypotheses. TF*IDF outperformed the language modeling approach in the CLIR J-J task whereas the language modeling approach performed better in the patent task. Two different document length hypotheses behind two tasks/collections are assumed by analyzing document length distributions of relevant/retrieved documents in the NTCIR-3 and -4 collections. Given these hypotheses, TF*IDF is easily adapted to patent retrieval tasks. Document length prior probabilities are applied to the language modeling approach. For the patent task, task-specific techniques, such as IPC priors and different indexing strategies, are evaluated and reported. To facilitate retrieval from large patent collections, a simple distributed search strategy is applied and found to be efficient, despite a slight deterioration of effectiveness. We found that TF*IDF performed similarly to the language modeling runs against the patent collection by controlling the document length normalization, whereas the language modeling approach does not perform as well as TF*IDF, despite calibration against the CLIR J-J collection. The different characteristics of the document lengths of the two test collections are illustrated through comparative studies. © 2005, ACM. All rights reserved.",Document length hypotheses; Experimentation; Language modeling approach to IR; Test collections,
A speech synthesizer for Persian text using a neural network with a smooth ergodic HMM,2005,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-23844486811&doi=10.1145%2f1066078.1066081&partnerID=40&md5=affc62fdd61b4d0058e0f8ff84bcc499,"The feasibility of converting text into speech using an inexpensive computer with minimal memory is of great interest. Speech synthesizers have been developed for many popular languages (e.g., English, Chinese, Spanish, French, etc.), but designing a speech synthesizer for a language is largely dependant on the language structure. In this article, we develop a Persian synthesizer that includes an innovative text analyzer module. In the synthesizer, the text is segmented into words and after preprocessing, a neural network is passed over each word. In addition to preprocessing, a new model (SEHMM) is used as a postprocessor to compensate for errors generated by the neural network. The performance of the proposed model is verified and the intelligibility of the synthetic speech is assessed via listening tests. © 2005 ACM.",Hidden Markov model; TD-PSOLA,Linguistics; Markov processes; Mathematical models; Natural language processing systems; Neural networks; Speech analysis; English language; Hidden Markov model; Persian synthesizer; TD-PSOLA; Speech synthesis
Domain-specific FAQ retrieval using independent aspects,2005,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-23844493498&doi=10.1145%2f1066078.1066079&partnerID=40&md5=d07ab53351f908acd350dd36402eda31,"This investigation presents an approach to domain-specific FAQ (frequently-asked question) retrieval using independent aspects. The data analysis classifies the questions in the collected QA (question-answer) pairs into ten question types in accordance with question stems. The answers in the QA pairs are then paragraphed and clustered using latent semantic analysis and the K-means algorithm. For semantic representation of the aspects, a domain-specific ontology is constructed based on WordNet and HowNet. A probabilistic mixture model is then used to interpret the query and QA pairs based on independent aspects; hence the retrieval process can be viewed as the maximum likelihood estimation problem. The expectation-maximization (EM) algorithm is employed to estimate the optimal mixing weights in the probabilistic mixture model. Experimental results indicate that the proposed approach outperformed the FAQ-Finder system in medical FAQ retrieval. © 2005 ACM.",FAQ retrieval; Information retrieval; Latent semantic analysis; Natural language processing; Ontology; Probabilistic mixture model; Question-answering,Algorithms; Classification (of information); Data mining; Data reduction; Knowledge based systems; Natural language processing systems; Semantics; Expectation-maximization (EM) algorithm; FAQ retrieval; Latent semantic analysis; Ontology; Probabilistic mixture model; Question-answering (QA) systems; Information retrieval
Correction of errors in a verb modality corpus for machine translation with a machine-learning method,2005,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-23844440317&doi=10.1145%2f1066078.1066080&partnerID=40&md5=f6103a2ab7ce5a9e2e3c69f9919d6a7a,"In recent years, various types of tagged corpora have been constructed and much research using tagged corpora has been done. However, tagged corpora contain errors, which impedes the progress of research. Therefore, the correction of errors in corpora is an important research issue. In this study we investigate the correction of such errors, which we call corpus correction. Using machine-learning methods, we applied corpus correction to a verb modality corpus for machine translation. We used the maximum-entropy and decision-list methods as machine-learning methods. We compared several kinds of methods for corpus correction in our experiments, and determined which is most effective by using a statistical test. We obtained several noteworthy findings: (1) Precision was almost the same for both detection and correction, so it is more convenient to do both correction and detection, rather than detection only. (2) In general, the maximum-entropy method worked better than the decision-list method; but the two methods had almost the same precision for the top 50 pieces of extracted data when closed data was used. (3) In terms of precision, the use of closed data was better than the use of open data; however, in terms of the total number of extracted errors, the use of open data was better than the use of closed data. Based on our analysis of these results, we developed a good method for corpus correction. We confirmed the effectiveness of our method by carrying out experiments on machine translation. As corpus-based machine translation continues to be developed, the corpus correction we discuss in this article should prove to be increasingly significant. © 2005 ACM.",Corpus correction; Machine learning; Machine translation; Modality corpus,Algorithms; Error detection; Information retrieval; Linguistics; Natural language processing systems; Statistical methods; Translation (languages); Corpus correction; Machine translation; Maximum-entropy methods; Modality corpus; Learning systems
Introduction to the Special Issue on Computer Processing of Oriental Languages,2004,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024270573&doi=10.1145%2f1039621.1039622&partnerID=40&md5=50f901cf88629affb239cb2a037052a8,[No abstract available],,
Usefulness of temporal information automatically extracted from news articles for topic tracking,2004,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-12744271507&doi=10.1145%2f1039621.1039624&partnerID=40&md5=efbf1e9d4c8f8d0a3c53553fad416b06,"Temporal information plays an important role in natural language processing (NLP) applications such as information extraction, discourse analysis, automatic summarization, and question-answering. In the topic detection and tracking (TDT) area, the temporal information often used is the publication date of a message, which is readily available but limited in its usefulness. We developed a relatively simple NLP method for extracting temporal information from Korean news articles, with the goal of improving performance of TDT tasks. To extract temporal information, we make use of finite state automata and a lexicon containing timerevealing vocabulary. Extracted information is converted into a canonicalized representation of a time point or a time duration. We first evaluated and investigated the extraction and canonicalization methods for their accuracy and the extent to which temporal information extracted as such can help TDT tasks. The experimental results show that time information extracted from the text does indeed help to significantly improve both precision and recall.",Event detection and tracking; Temporal information extraction,Automation; Broadcasting; Error analysis; Finite automata; Formal languages; Information analysis; Learning systems; Performance; Event detection and tracking; Natural language processing (NLP); Temporal information extraction; Topic detection and tracking (TDT); Neural networks
An adaptive k-nearest neighbor text categorization strategy,2004,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-12744267752&doi=10.1145%2f1039621.1039623&partnerID=40&md5=e4b384d0e19b37f8e4ee61023f5e0ab6,"k is the most important parameter in a text categorization system based on the k-nearest neighbor algorithm (kNN). To classify a new document, the k-nearest documents in the training set are determined first. The prediction of categories for this document can then be made according to the category distribution among the k nearest neighbors. Generally speaking, the class distribution in a training set is not even; some classes may have more samples than others. The system's performance is very sensitive to the choice of the parameter k. And it is very likely that a fixed k value will result in a bias for large categories, and will not make full use of the information in the training set. To deal with these problems, an improved kNN strategy, in which different numbers of nearest neighbors for different categories are used instead of a fixed number across all categories, is proposed in this article. More samples (nearest neighbors) will be used to decide whether a test document should be classified in a category that has more samples in the training set. The numbers of nearest neighbors selected for different categories are adaptive to their sample size in the training set. Experiments on two different datasets show that our methods are less sensitive to the parameter k than the traditional ones, and can properly classify documents belonging to smaller classes with a large k. The strategy is especially applicable and promising for cases where estimating the parameter k via cross-validation is not possible and the class distribution of a training set is skewed.",K-nearest neighbor algorithm; Machine learning; Text categorization; Text classification,Adaptive systems; Algorithms; Efficiency; Information analysis; Neural networks; Parameter estimation; Pattern recognition; Set theory; Datasets; K-nearest neighbour algorithms; Text categorization; Text classification; Learning systems
An evaluation of statistical spam filtering techniques,2004,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-12744250266&doi=10.1145%2f1039621.1039625&partnerID=40&md5=02ca7168fe3ae1a1221100f0c9dad791,"This paper evaluates five supervised learning methods in the context of statistical spam filtering. We study the impact of different feature pruning methods and feature set sizes on each learner's performance using cost-sensitive measures. It is observed that the significance of feature selection varies greatly from classifier to classifier. In particular, we found support vector machine, AdaBoost, and maximum entropy model are top performers in this evaluation, sharing similar characteristics: not sensitive to feature selection strategy, easily scalable to very high feature dimension, and good performances across different datasets. In contrast, naive Bayes, a commonly used classifier in spam filtering, is found to be sensitive to feature selection methods on small feature set, and fails to function well in scenarios where false positives are penalized heavily, The experiments also suggest that aggressive feature pruning should be avoided when building filters to be used in applications where legitimate mails are assigned a cost much higher than spams (such as λ = 999), so as to maintain a better-than-baseline performance. An interesting finding is the effect of mail headers on spam filtering, which is often ignored in previous studies. Experiments show that classifiers using features from message header alone can achieve comparable or better performance than filters utilizing body features only. This implies that message headers can be reliable and powerfully discriminative feature sources for spam filtering.",Spam filtering; Text categorization,Algorithms; Automation; Computational complexity; Feature extraction; Information analysis; Parameter estimation; Spamming; Vectors; Datasets; Feature dimensions; Spam filtering; Text categorization; Learning systems
Analysis and modeling of F0 contours for cantonese text-to-speech,2004,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-12344322621&doi=10.1145%2f1037811.1037813&partnerID=40&md5=eaf4649e75744c43123f26096eef6a95,"For the generation of highly natural synthetic speech, the control of prosody is of primary importance. The fundamental frequency (F0) is one of the most important components of speech prosody. This research investigates the variation of F0 in continuous Cantonese speech, with the goal of establishing an effective mechanism of prosody control in Cantonese text-to-speech (TTS) applications. Cantonese is a commonly used Chinese dialect that is well known for being rich in tones. This article describes a simple yet effective approach to the analysis and modeling of F0. The surface F0 contour of a continuous Cantonese utterance is considered to be the combination of a global component - phrase-level intonation curve, and local components - syllable-level tone contoursA novel method of F0 normalization is proposed to separate the local components from the global one. As a result, the variation in tone contours is greatly reduced. Statistical analysis is performed for the phrase curves and context-dependent tone contours that are extracted from a large corpus of 1,200 utterances. Specifically, the analysis is focused on co-articulated tone contours for disyllabic words, cross-word contours, and phrase-initial tone contours. Based on the results of the analysis, a template-based model for F0 generation is established and integrated with a Cantonese TTS system. Subjective listening tests show that the proposed model significantly improves the naturalness of the output speech.",Chinese dialects; Fundamental frequency; Prosody; Text-to-speech; Tones,Artificial intelligence; Curve fitting; Interfaces (computer); Mathematical models; Natural language processing systems; Research; Speech recognition; Speech synthesis; Chinese dialects; Design methodology; Experimentation; Fundamental frequency; Human factors; Pattern analysis; Prosody; Text-to-speech; Tones; Speech processing
Using a web-based categorization approach to generate thematic metadata from texts,2004,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-12344328532&doi=10.1145%2f1037811.1037812&partnerID=40&md5=08595ba6a42edebdd6bc03a70206330e,"Conventional tools for automatic metadata creation mostly extract named entities or text segments from texts and annotate them with information about persons, locations, dates, and so on. However, this kind of entity type information is often insufficient for machines to understand the facts contained in the texts, thus precluding the possibility of implementing more advanced, intelligent applications, such as concept-based search. In this work, we try to create more refined thematic metadata inherent in texts. Based on Web resource mining, our approach acquires training corpora necessary to describe both the thematic categories and the metadata extracted from the texts. The approach then finds the corresponding relationships among them by means of categorization and thus generates thematic metadata for the textual data. Experimental results confirm the potential and wide adaptability of our approach.",Categorization; Metadata; Web mining,Algorithms; Data mining; HTML; Information retrieval; World Wide Web; XML; Categorization; Experimentation; Text segments; Web mining; Metadata
A discriminative HMM/N-gram-based retrieval approach for Mandarin spoken documents,2004,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-10044234199&doi=10.1145%2f1034780.1034784&partnerID=40&md5=2a442308333512fb08e4eb6279a1ca89,"In recent years, statistical modeling approaches have steadily gained in popularity in the field of information retrieval. This article presents an HMM/N-gram-based retrieval approach for Mandarin spoken documents. The underlying characteristics and the various structures of this approach were extensively investigated and analyzed. The retrieval capabilities were verified by tests with word- and syllable-level indexing features and comparisons to the conventional vector-space model approach. To further improve the discrimination capabilities of the HMMs, both the expectation-maximization (EM) and minimum classification error (MCE) training algorithms were introduced in training. Fusion of information via indexing word- and syllable-level features was also investigated. The spoken document retrieval experiments were performed on the Topic Detection and Tracking Corpora (TDT-2 and TDT-3). Very encouraging retrieval performance was obtained.",Hidden Markov models; Mandarin spoken documents; Syllable-level Indexing features,Error analysis; Information retrieval; Mathematical models; Pattern recognition; Probability; Problem solving; Speech recognition; Statistical methods; Hidden Markov models; Mandarin spoken documents; Statistical modeling; Syllable-level indexing features; Indexing (of information)
Introduction to the special issue on statistical language modeling,2004,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-10044283239&doi=10.1145%2f1034780.1034781&partnerID=40&md5=cacd877a5a81bb75e28d74c4aa916a60,Introduction to the special issue on statistical language modeling (SLM) in estimating the likelihood of a word string was discussed. The research on SLM involved two main tasks which were modeling and estimation. Estimation was used to determine the free parameters of the model using training data. It was found that SLM used a parametric model with Maximum Likelihood Estimation (MLE) and various smoothing methods for tackling data sparseness problems.,Discriminative training; N-gram models; Source-channel models; Statistical language modeling,Computer simulation; Data processing; Information retrieval; Mathematical models; Parameter estimation; Probability; Problem solving; Statistical methods; Discriminative training; N-gram models; Source-channel models; Statistical language modeling; Formal languages
A hybrid language model based on a combination of N-grams and stochastic context-free grammars,2004,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-10044298606&doi=10.1145%2f1034780.1034783&partnerID=40&md5=94bdb8f2447c197322f28022b7e5d803,"In this paper, a hybrid language model is defined as a combination of a word-based n-gram, which is used to capture the local relations between words, and a category-based stochastic context-free grammar (SCFG) with a word distribution into categories, which is defined to represent the long-term relations between these categories. The problem of unsupervised learning of a SCFG in General Format and in Chomsky Normal Form by means of estimation algorithms is studied. Moreover, a bracketed version of the classical estimation algorithm based on the Earley algorithm is proposed. This paper also explores the use of SCFGs obtained from a treebank corpus as initial models for the estimation algorithms. Experiments on the UPenn Treebank corpus are reported. These experiments have been carried out in terms of the test set perplexity and the word error rate in a speech recognition experiment.",Language model; Stochastic context-free grammar,Algorithms; Computer science; Error analysis; Information analysis; Parameter estimation; Probability; Problem solving; Syntactics; Hybrid languages; Language model; N-gram; Stochastic context free grammer; Linguistics
A maximum-entropy Chinese parser augmented by transformation-based learning,2004,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-10044253419&doi=10.1145%2f1034780.1034786&partnerID=40&md5=175de36cc1679c5e6a267f88f5486a4c,"Parsing, the task of identifying syntactic components, e.g., noun and verb phrases, in a sentence, is one of the fundamental tasks in natural language processing. Many natural language applications such as spoken-language understanding, machine translation, and information extraction, would benefit from, or even require, high accuracy parsing as a preprocessing step. Even though most state-of-the-art statistical parsers were initially constructed for parsing in English, most of them are not language-specific, in that they do not rely on properties of the language that are specific to English. Therefore, construction of a parser in a given language becomes a matter of retraining the statistical parameters with a Treebank in the corresponding language. The development of the Chinese treebank [Xia et al. 2000] spurred the construction of parsers for Chinese. However, Chinese as a language poses some unique problems for the development of a statistical parser, the most apparent being word segmentation. Since words in written Chinese are not delimited in the same way as in Western languages, the first problem that needs to be solved before an existing statistical method can be applied to Chinese is to identify the word boundaries. This is a step that is neglected by most pre-existing Chinese parsers, which assume that the input data has already been pre-segmented. This article describes a character-based statistical parser, which gives the best performance to-date on the Chinese treebank data. We augment an existing maximum entropy parser with transformation-based learning, creating a parser that can operate at the character level. We present experiments that show that our parser achieves results that are close to those achievable under perfect word segmentation conditions.",Chunking and parsing for chinese; Maximum entropy; Parsing for Chinese; POS tagging; Transformation-based learning,Computational linguistics; Data processing; Entropy; Information analysis; Parameter estimation; Problem solving; Statistical methods; Syntactics; Chunking and parsing for chinese; Maximum entropy; Parsing for chinese; POS tagging; Transformation-based learning; Learning systems
Example-based sentence reduction using the hidden Markov model,2004,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-10044222178&doi=10.1145%2f1034780.1034785&partnerID=40&md5=49068003cd02687e6d713aa023ed1bbf,"Sentence reduction is the removal of redundant words or phrases from an input sentence by creating a new sentence in which the gist of the original meaning of the sentence remains unchanged. All previous methods required a syntax parser before sentences could be reduced; hence it was difficult to apply them to a language with no reliable parser. In this article we propose two new sentence-reduction algorithms that do not use syntactic parsing for the input sentence. The first algorithm, based on the template-translation learning algorithm, one of example-based machine-translation methods, works quite well in reducing sentences, but its computational complexity can be exponential in certain cases. The second algorithm, an extension of the template-translation algorithm via innovative employment of the Hidden Markov model, which uses the set of template rules learned from examples, can overcome this computation problem. Experiments show that the proposed algorithms achieve acceptable results in comparison to sentence reduction done by humans.",Example-based sentence reduction; HMM-based sentence reduction; Sentence reduction,Algorithms; Computation theory; Data processing; Markov processes; Mathematical models; Problem solving; Scanning; Example-based sentence reduction; HMM-based sentence reduction; Markov models; Sentence reduction; Linguistics
Lexical triggers and latent semantic analysis for cross-lingual language model adaptation,2004,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-10044246236&doi=10.1145%2f1034780.1034782&partnerID=40&md5=9409ae9263a0c4f37e1e5c28ec9a3939,"In-domain texts for estimating statistical language models are not easily found for most languages of the world. We present two techniques to take advantage of in-domain text resources in other languages. First, we extend the notion of lexical triggers, which have been used monolingually for language model adaptation, to the cross-lingual problem, permitting the construction of sharper language models for a target-language document by drawing statistics from related documents in a resource-rich language. Next, we show that cross-lingual latent semantic analysis is similarly capable of extracting useful statistics for language modeling. Neither technique requires explicit translation capabilities between the two languages! We demonstrate significant reductions in both perplexity and word error rate on a Mandarin speech recognition task by using these techniques.",Automatic speech recognition; Language model adaptation; Latent semantic analysis; Lexical trigger; Multilingual processing; Statistical language modeling,Character recognition; Data reduction; Formal languages; Information retrieval; Mathematical models; Speech recognition; Statistical methods; Automatic speech recognition; Language model adaptation; Latent semantic analysis; Lexical triggers; Multilingual processing; statistical language modeling; Semantics
Extracting meaning from temporal nouns and temporal prepositions,2004,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-5444237688&doi=10.1145%2f1017068.1017071&partnerID=40&md5=f893ae2a5bf13fe5072b5c8a66c3784f,"This article provides a compositional semantics for temporal nouns and temporal prepositions that are annotated as temporal prepositional phrases or noun phrases by an automatic tagging system (e.g., last Monday, on Dec. 1 st, for three weeks or before Christmas). Current temporal tagging systems rely on an ad-hoc-representation for temporal date and time expressions, but the more demanding tasks of temporal question-answering and automatic text summarization require a sound logical derivation and representation of temporal expressions. Our proposal draws from two formal accounts of temporal prepositional phrases by Pratt and Francez [2001] and von Stechow [2002b], and is realized within an automatic temporal tagging system for German newspaper articles.",Information extraction; Semantics for temporal expressions and temporal prepositions; Temporal information; Temporal reasoning; Temporal reference in discourse,Algorithms; Data storage equipment; Information retrieval; Linguistics; Mathematical models; Semantics; Theory; Content analysis and indexing; Document analysis; Information extraction; Linguistic processing; Natural language processing; Temporal information; Temporal reasoning; Data processing
A framework for resolution of time in natural language,2004,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-5444222121&doi=10.1145%2f1017068.1017070&partnerID=40&md5=18c01c11ffa6b5aaa1a3968481273992,"Automatic extraction and reasoning over temporal properties in natural language discourse has not had wide use in practical systems due to its demand for a rich and compositional, yet inference-friendly, representation of time. Motivated by our study of temporal expressions from the Penn Treebank corpora, we address the problem by proposing a two-level constraint-based framework for processing and reasoning over temporal information in natural language. Within this framework, temporal expressions are viewed as partial assignments to the variables of an underlying calendar constraint system, and multiple expressions together describe a temporal constraint-satisfaction problem (TCSP). To support this framework, we designed a typed formal language for encoding natural language expressions. The language can cope with phenomena such as under-specification and granularity change. The constraint problems can be solved using various constraint propagation and search methods, and the solutions can then be used to answer a wide range of time-related queries.",Computational Semantics; Constraint Solving; Knowledge Representation; Temporal Information Processing; Temporal Reasoning,Constraint theory; Data processing; Database systems; Knowledge representation; Semantics; Specifications; Computational semantics; Constraint solving; Knowledge representations formalisms and methods; Natural language processing; Representation languages; Temporal information processing; Temporal reasoning; Text analysis; Artificial intelligence
Automatic TIMEX2 tagging of korean news,2004,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-5444245124&doi=10.1145%2f1017068.1017072&partnerID=40&md5=a1da62210413f0340e1b72a13786504c,"This article reports on a temporal tagger for Korean based on a Korean extension of the TIDES TIMEX2 guidelines. The extension, which primarily addresses the idiosyncrasies of Korean morphology, shows high inter-annotator reliability (0.893 F-measure for tag extent) when applied to a corpus of Korean newspaper articles. A machine-learning approach based on rote learning from a human-edited, automatically-derived dictionary of temporal expressions is compared with a second approach that adds manual patterns, and a third onethat tries to learn the patterns. Results for the first two are promising (0.87 F-measure for tag extent). Overall, the article shows that rote learning approaches can be very useful when language-specific features such as morphology are taken into account.",Koreans; Temporal expressions; Temporal information; Time,Artificial intelligence; Information retrieval; Learning systems; Linguistics; Reliability; Content analysis; Korean; Linguistic processing; Natural language processing; Temporal expressions; Temporal information; Text analysis; Data processing
An ontology of time for the semantic web,2004,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-5444253328&doi=10.1145%2f1017068.1017073&partnerID=40&md5=5efda7379d6253bbe9d1c7a80c6d6cf7,"In connection with the DAML project for bringing about the Semantic Web, an ontology of time is being developed for describing the temporal content of Web pages and the temporal properties of Web services. This ontology covers topological properties of instants and intervals, measures of duration, and the meanings of clock and calendar terms.",Clock and calendar; Duration; Ontology; Semantic web; Temporal information; Temporal relation; Time; Time zone,Artificial intelligence; Information retrieval; Knowledge representation; Linguistics; Project management; World Wide Web; Content analysis; Duration; Knowledge representations formalisms and methods; Linguistics processing; Online information services; Ontology; Temporal logic; Temporal relation; Text analysis; Time zone; Semantics
Introduction to the special issue on temporal information processing,2004,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-5444234952&doi=10.1145%2f1017068.1017069&partnerID=40&md5=7d24705e0996b4e691be68f0ec4916c8,"Time is a key dimension of our information space, with many applications standing to benefit from exploiting it. This special issue is devoted to temporal information processing for natural language as well as temporal reasoning. We begin by describing some of the ways time is expressed in natural language, and the particular requirements this imposes on information processing systems. We then provide an overview of current annotation-based approaches to temporal information extraction, followed by an introduction to the relevant literature on temporal reasoning. We also provide brief synopses of the articles in this issue, situating them in a broader context. We end with administrative remarks about the creation of this special issue.",,Artificial intelligence; Data storage equipment; Formal languages; Information retrieval; Knowledge representation; Linguistics; Content analysis; Knowledge representations formalisms and methods; Linguistic processing; Natural language processing; Text analysis; Data processing
Resolution of referring expressions in a Korean multimodal dialogue system,2003,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-3343009066&doi=10.1145%2f1007551.1007553&partnerID=40&md5=cf608b9346c8b70714c64c4ac571d273,"Referring expressions in multimodal dialogues have different aspects compared to those in language-only dialogues. They often refer to the items signified by either a gesture or visual means. In this article we classify referring expressions into two types (i.e., a deictic reference and an anaphoric reference), and propose two general methods to resolve these referring expressions. One method is a simple mapping algorithm that can find items referred with/without pointing gestures on a screen. The other is the centering algorithm with a dual cache model, to which Walker's centering algorithm is extended for a multimodal dialogue system. The extended algorithm is appropriate for resolving various anaphoric references in a multimodal dialogue. In the experiments, the proposed system correctly resolved 376 out of 405 referring expressions in 40 dialogues (0.54 referring expressions per utterance) showing 92.84 percent correctness.",Anaphoric reference; Deictic reference; Reference resolution; multimodal dialogue system; extended centering algorithm; dual cache model,Algorithms; Artificial intelligence; Buffer storage; Communication channels (information theory); Conformal mapping; Natural language processing systems; Speech processing; User interfaces; Visual communication; Anaphoric reference; Deictic reference; Dual cache models; Extended centering algorithms; Multimodal dialogue systems; Reference resolution; Computer programming languages
Cross-lingual C*ST*RD: English access to hindi information,2003,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745117938&doi=10.1145%2f979872.979877&partnerID=40&md5=f543d88c79c4ee44472611082e426305,"We present C*ST*RD, a cross-language information delivery system that supports cross-language information retrieval, information space visualization and navigation, machine translation, and text summarization of single documents and clusters of documents. C*ST*RD was assembled and trained within 1 month, in the context of DARPA's Surprise Language Exercise, that selected as source a heretofore unstudied language, Hindi. Given the brief time, we could not create deep Hindi capabilities for all the modules, but instead experimented with combining shallow Hindi capabilities, or even English-only modules, into one integrated system. Various possible configurations, with different tradeoffs in processing speed and ease of use, enable the rapid deployment of C*ST*RD to new languages under various conditions. © 2003 ACM.",Cross-language information retrieval; Headline generation; Hindi-to-English machine translation; Information retrieval and information space navigation; Single- and multi-document text summarization,Information retrieval; Linguistics; Translation (languages); Visualization; Cross-language information retrieval; Headline generation; Hindi-to-English machine translation; Information retrieval and information space navigation; Single- and multi-document text summarization; Natural language processing systems
Rapid customization of an information extraction system for a surprise language,2003,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745147887&doi=10.1145%2f979872.979880&partnerID=40&md5=ac685f41e1c53cf6cecfa7a8ab2b6e7d,"This paper describes the rapid adaptation for surprise languages of a flexible and robust Information Extraction system based on GATE, a portable Natural Language Processing infrastructure. Our experiences show that even without a native speaker and in the absence of training data, we can quickly customize the system to a new language. We adapted the default English system for the Cebuano language in 10 days, achieving an F measure of 77.5%. © 2003 ACM.",Information Extraction; Language agility; Named entity recognition,Data reduction; Feature extraction; Robustness (control systems); Speech analysis; Information Extraction; Language agility; Named entity recognition; Surprise language; Natural language processing systems
Hindi-English cross-lingual question-answering system,2003,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-30144432966&doi=10.1145%2f979872.979874&partnerID=40&md5=2810a147e8240ccc545b880a18914f07,"We developed a cross-lingual, question-answering (CLQA) system for Hindi and English. It accepts questions in English, finds candidate answers in Hindi newspapers, and translates the answer candidates into English along with the context surrounding each answer. The system was developed as part of the surprise language exercise (SLE) within the TIDES program. © 2004 ACM.",Hindi,"Computer software; Linguistics; Systems analysis; Cross-lingual, question-answering (CLQA); Surprise language exercise (SLE); Translation (languages)"
Cross-language headline generation for hindi,2003,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745128054&doi=10.1145%2f979872.979878&partnerID=40&md5=c491ec5ecc8ab653ea13e65ecd4a9910,"This paper presents new approaches to headline generation for English newspaper texts, with an eye toward the production of document surrogates for document selection in cross-language information retrieval. This task is difficult because the user must make decisions about relevance based on (often poor) translations of retrieved documents. To facilitate the decision-making process we need translations that can be assessed rapidly and accurately; our approach is to provide an English headline for the non-English document. We describe two approaches to headline generation and their application to the recent DARPA TIDES-2003 Surprise Language Exercise for Hindi. For comparison, we also implemented an alternative method for surrogate generation: a system that produces topic lists for (Hindi) articles. We present the results of a series of experiments comparing each of these approaches. We demonstrate in both automatic and human evaluations that our linguistically motivated approach outperforms two other surrogate-generation methods: a statistical system and a topic discovery system. © 2003 ACM.",Cross-language information retrieval; Headline generation; Text summarization,Decision making; Information retrieval; Linguistics; Translation (languages); Cross-language information retrieval; Headline generation; Text summarization; Natural language processing systems
Adaptive Hindi OCR using generalized Hausdorff image comparison,2003,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745167311&doi=10.1145%2f979872.979875&partnerID=40&md5=f31c68f31839e3a0dc660c75cdf0a89f,"We present an adaptive Hindi OCR implemented as part of a rapidly retargetable language tool effort. The system includes: script identification, character segmentation, training sample creation, and character recognition. In script identification, Hindi words are identified from bilingual or multilingual documents based on features of the Devanagari script or using Support Vector Machines. Identified words are then segmented into individual characters in the next step, where the composite characters are identified and further segmented based on the structural properties of the script and statistical information. Segmented characters are recognized using generalized Hausdorff image comparison (GHIC) and postprocessing is applied to improve the performance. The OCR system, which was designed and implemented in one month, was applied to a complete Hindi-English bilingual dictionary and a set of ideal images extracted from Hindi documents in PDF format. Experimental results show the recognition accuracy can reach 88% for noisy images and 95% for ideal images. The presented method can also be extended to design OCR systems for different scripts. © 2003 ACM.",Document processing; Generalized Hausdorff image comparison; Optical character recognition (OCR); Script identification,Adaptive systems; Character recognition; Image processing; Personnel training; Product design; Statistical methods; Document processing; Generalized Hausdorff image comparison; Optical character recognition (OCR); Script identification; Linguistics
Rapid development of hindi named entity recognition using conditional random fields and feature induction,2003,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745158297&doi=10.1145%2f979872.979879&partnerID=40&md5=42476fdba7e28eb80e683e938466335d,"This paper describes our application of conditional random fields with feature induction to a Hindi named entity recognition task. With only five days development time and little knowledge of this language, we automatically discover relevant features by providing a large array of lexical tests and using feature induction to automatically construct the features that most increase conditional likelihood. In an effort to reduce overfitting, we use a combination of a Gaussian prior and early stopping based on the results of 10-fold cross validation. © 2003 ACM.",Conditional random fields; Extraction; Feature induction,Feature extraction; Knowledge engineering; Pattern recognition; Conditional random fields; Cross validation; Feature induction; Gaussian; Natural language processing systems
Making MIRACLEs: Interactive translingual search for Cebuano and Hindi,2003,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-24944538491&doi=10.1145%2f979872.979876&partnerID=40&md5=1ad233d8fd56e369715cacd5868ed94c,"Searching is inherently a user-centered process; people pose the questions for which machines seek answers, and ultimately people judge the degree to which retrieved documents meet their needs. Rapid development of interactive systems that use queries expressed in one language to search documents written in another poses five key challenges: (1) interaction design, (2) query formulation, (3) cross-language search, (4) construction of translated summaries, and (5) machine translation. This article describes the design of MIRACLE, an easily extensible system based on English queries that has previously been used to search French, German, and Spanish documents, and explains how the capabilities of MIRACLE were rapidly extended to accommodate Cebuano and Hindi. Evaluation results for the cross-language search component are presented for both languages, along with results from a brief full-system interactive experiment with Hindi. The article concludes with some observations on directions for further research on interactive cross-language information retrieval. © 2003 ACM.",Cross-language information retrieval; Interactive information retrieval; Machine translation,Information retrieval; Interactive computer systems; Query languages; Translation (languages); Cross-language information retrieval; Interactive information retrieval; Machine translation; Natural language processing systems
Improving partial parsing based on error-pattern analysis for a Korean grammar-checker,2003,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-3342971440&doi=10.1145%2f1007551.1007552&partnerID=40&md5=5e4af801730c412da0f8bac4d1c999e0,"The main aim of this work is to make improvements to Mirine 2.2, a Korean grammar-checker that takes Korean language properties into account and to develop a system that satisfies user expectations. Particular attention is given to processing Korean texts that may contain users' grammatical and practical errors. In order to treat these errors efficiently, the system includes methods based on asymmetric relations from which partial parsing and the potential governing relationship are derived, implying the starting point for checking, the direction for parsing, and the limits of the scope for parsing. To organize partial parsing efficiently, the system requires an appropriate knowledge base. As its essential prerequisites, this study (a) considers the factors from which the various error types encountered while parsing various Korean texts arise; (b) extracts general patterns from the linguistic or extra-linguistic factors obtained in this manner; and (c) demonstrates how the system, based on a linguistic analysis, procures an adequate knowledge base for partial parsing to satisfy end-users. Mirine 2.2 achieves an F-measure of about 0.80 in detecting unknown erroneous words; an F-measure of about 0.98 when not considering unknown words; and 98.94% precision in correcting erroneous words.","Error-pattern analysis; Korean grammar checker; Linguistic analysis; Parsing triggering condition; Parsing, partial parsing; Text preprocessing",Artificial intelligence; Decision theory; Encoding (symbols); Error analysis; Information analysis; Knowledge based systems; Linguistics; Natural language processing systems; Semantics; Servers; Text processing; User interfaces; Error-pattern analysis; Korean grammar checker; Linguistic analysis; Parsing; Parsing triggering condition; Partial parsing; Text preprocessing; Computer programming languages
Surprise! What's in a Cebuano or Hindi name?,2003,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745169686&doi=10.1145%2f979872.979873&partnerID=40&md5=8962f767433bcb22eb6b0eb75db9c99f,"Empirical results are presented for creating training data and training a statistical name learning algorithm on Cebuano and Hindi in roughly three weeks time. The empirical study compares performance in a compressed time frame against performance of the same statistical language model in English (where there was no compressed time frame). Rapid development of several co-reference heuristics in Hindi are also described, and co-reference performance in Hindi is compared to previously developed English techniques. © 2003 ACM.",Cebuano; Extraction; Hindi,Feature extraction; Learning algorithms; Mathematical models; Personnel training; Statistical methods; Cebuano; Hindi; Statistical language model; Training data; Linguistics
A Month to Topic Detection and Tracking in Hindi,2003,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-36448969694&doi=10.1145%2f974740.974742&partnerID=40&md5=ec5017ed288eb2af114ce0c0faef7d96,"We describe the one-month (June 2003) effort to create a topic detection and tracking (TDT) system to support news stories in Hindi. The University of Massachusetts submitted results for three different TDT tasks in the DARPA surprise language evaluation. The official task was topic tracking, but we also provided results for the new event detection and topic detection (clustering) tasks. Our approach to all three tasks was based on the vector-space model of information retrieval. We also describe the process we used to create the relevance judgments used to evaluate the system. Results suggest that topic tracking effectiveness is comparable to that of TDT tracking systems in other languages. Results for clustering and new event detection indicate that parameter settings for those tasks are sensitive to the language being used. © 2003, ACM. All rights reserved.",Algorithms; Hindi; Languages; Measurement; Performance; surprise language; TDT; topic detection and tracking,
Experiments with a Hindi-to-English Transfer-Based MT System Under a Miserly Data Scenario,2003,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-35048819551&doi=10.1145%2f974740.974747&partnerID=40&md5=b3b31b5c94e6a2a50929f1b43c034abd,"We describe an experiment designed to evaluate the capabilities of our trainable transfer-based (XFER) machine translation approach, as applied to the task of Hindi-to-English translation, and trained under an extremely limited data scenario. We compare the performance of the XFER approach with two corpus-based approaches—Statistical MT (SMT) and Example-based MT (EBMT)—under the limited data scenario. The results indicate that the XFER system significantly outperforms both EBMT and SMT in this scenario. Results also indicate that automatically learned transfer rules are effective in improving translation performance, compared with a baseline wordto-word translation version of the system. XFER system performance with a limited number of manually written transfer rules is, however, still better than the current automatically inferred rules. Furthermore, a “multiengine” version of our system that combined the output of the XFER and SMT systems and optimizes translation selection outperformed both individual systems. © 2003, ACM. All rights reserved.",Design; Evaluation; example-based machine translation; Experimentation; Hindi; Languages; limited data resources; machine learning; Measurement; multiengine machine translation; statistical translation; transfer rules,
The Surprise Language Exercises,2003,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880221234&doi=10.1145%2f974740.974741&partnerID=40&md5=fc1f48ad6c0e4f0fc956cb797902517d,"For ten days in March and twenty-nine days in June of 2003, sixteen teams in two nations sought to develop language technologies for two previously unanticipated languages; Cebuano and Hindi. This introduction to a pair of special issues explains the motivation for those exercises, the approaches that were tried, and some of the lessons that were learned. © 2003, ACM. All rights reserved.",Algorithms; Cross-language information retrieval; Design; Experimentation; information extraction; language parsing and understanding; Languages; machine translation; Measurement; summarization; text analysis,
Linguistic Resource Creation for Research and Technology Development: A Recent Experiment,2003,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-10644292104&doi=10.1145%2f974740.974743&partnerID=40&md5=095637d9acee9d3074c4570975ed6ba9,"Advances in statistical machine learning encourage language-independent approaches to linguistic technology development. Experiments in “porting” technologies to handle new natural languages have revealed a great potential for multilingual computing, but also a frustrating lack of linguistic resources for most languages. Recent efforts to address the lack of available resources have focused either on intensive resource development for a small number of languages or development of technologies for rapid porting. The Linguistic Data Consortium recently participated in an experiment falling primarily under the first approach, the surprise language exercise. This article describes linguistic resource creation within this context, including the overall methodology for surveying and collecting language resources, as well as details of the resources developed during the exercise. The article concludes with discussion of a new approach to solving the problem of limited linguistic resources, one that has recently proven effective in identifying core linguistic resources for less common studied languages. © 2003, ACM. All rights reserved.",Cebuano; crosslanguage information retrieval; Design; Experimentation; Hindi; information extraction; language parsing and understanding; Languages; linguistic resources; Machine translation; machine translation; summarization; text analysis; translingual information access technology,
Hindi CLIR in Thirty Days,2003,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-29244432248&doi=10.1145%2f974740.974746&partnerID=40&md5=606b394de1621240afe98b094b9d226b,"As participants in the TIDES Surprise language exercise, researchers at the University of Massachusetts helped collect Hindi-English resources and developed a cross-language information retrieval system. Components included normalization, stop-word removal, transliteration, structured query translation, and language modeling using a probabilistic dictionary derived from a parallel corpus. Existing technology was successfully applied to Hindi. The biggest stumbling blocks were collection of parallel English and Hindi text and dealing with numerous proprietary encodings. © 2003, ACM. All rights reserved.",cross-language; cross-lingual information retrieval; Design; evaluation; Experimentation; Hindi; Languages,
Cross-Lingual Retrieval for Hindi,2003,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33646042275&doi=10.1145%2f974740.974748&partnerID=40&md5=ec51fa0cfd964e56d7a60ec9d911daa2,"In this paper we describe the evaluation results of applying a cross-lingual retrieval model to retrieve Hindi documents relevant to an English query. Though the technique has been previously applied and evaluated for retrieving Chinese and Arabic documents given an English query, what is new about these experiments is porting the model to Hindi in two weeks' time. © 2003, ACM. All rights reserved.",Algorithms; cross-lingual retrieval; Experimentation; Hindi; Measurement,
Extracting Named Entity Translingual Equivalence with Limited Resources,2003,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053281856&doi=10.1145%2f974740.974745&partnerID=40&md5=99da34ccc5da9d8b3e21a927b6bf909e,"In this article we present an automatic approach to extracting Hindi-English (H-E) Named Entity (NE) translingual equivalences from bilingual parallel corpora. In the absence of a Hindi NE tagger or H-E translation dictionary, this approach adapts a Chinese-English (C-E) surface string transliteration model for H-E NE extraction. The model is initially trained using automatically extracted C-E NE pairs, then iteratively updated based on newly extracted H-E NE pairs. For each English person and location NE in each sentence pair, this approach searches for its Hindi correspondence with minimum transliteration cost and constructs an H-E NE list from the bilingual corpus. Experiments show that this approach extracted 1000 H-E NE pairs with a precision of 91.8%. © 2003, ACM. All rights reserved.",Algorithms; information extraction; Language; machine translation; Named entity translation; Performance; transliteration,
Rapid Porting of DUSTer to Hindi,2003,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025403314&doi=10.1145%2f974740.974744&partnerID=40&md5=aedcbe6fd69b243df04167134ea727b2,"The frequent occurrence of divergences-structural differences between languages-presents a great challenge for statistical word-level alignment and machine translation. This paper describes the adaptation of DUSTer, a divergence unraveling package, to Hindi during the DARPA TIDES-2003 Surprise Language Exercise. We show that it is possible to port DUSTer to Hindi in under 3 days. © 2003, ACM. All rights reserved.",Divergences; Experimentation; Languages; machine translation,
An efficient accessing technique for Taiwanese phonetic transcriptions,2003,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-3042777280&doi=10.1145%2f964161.964165&partnerID=40&md5=bef3550598c4ef699f5e1235ccbcca15,"Recently, the Taiwan government has been enthusiastically promoting the study of the languages of her native inhabitants, including Taiwanese. Hence the focus of our research is to design and develop an efficient retrieval technique for phonetic transcriptions. This new technique will make possible widespread utilization of Taiwanese, e.g., in PDA (mobile phone applications). In this paper we propose a minimal perfect hashing function for the 3028 existing Taiwanese phonetic transcriptions. Compared to the hashing designs based on the Chinese remainder theorem for various data sets, the proposed design is shown to be superior in space utilization.",Chinese remainder theorem; Hashing function design; Minimal perfect hashing function; Taiwanese phonetic transcriptions,Algorithms; Functions; Information retrieval; Mathematical operators; Personal digital assistants; Speech analysis; Theorem proving; Translation (languages); Vocabulary control; Chinese remainder theorem; Hashing function design; Minimal perfect hashing function; Taiwanese phonetic transcriptions; Natural language processing systems
Offline handwritten Chinese character recognition by radical decomposition,2003,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-3042858565&doi=10.1145%2f964161.964163&partnerID=40&md5=833880fc4901fa6dd35184817182fe4e,"An approach to radical recognition for offline hardwritten chinese characters, based on nonlinear active shape modeling was proposed. The apaproach is effectively holistic, encoding the complete character image. chamfer distance minimization is used to match radicals within a character using the dynamic tunneling algorithms (DTA) to search for the best shape parameters. It was found that method superior generalizability and enabled to develop techniques for automatic landmark labeling.",Active shape modeling; Chinese computing; Offline character recognition; Viterbi decoding,Algorithms; Data reduction; Hierarchical systems; Image analysis; Knowledge acquisition; Markov processes; Mathematical models; Natural language processing systems; Pattern matching; Probability; Robustness (control systems); Active shape modeling; Chinese computing; Offline character recognition; Viterbi decoding; Character recognition
Task adaptation in stochastic language model for Chinese homophone disambiguation,2003,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-3042832032&doi=10.1145%2f964161.964164&partnerID=40&md5=8bbeae9bd3fcae0496f0bce14c5851ea,"The runtime application domain has a great effect on the performance of practical corpus-based applications. Previous smoothing techniques and class-based and similarity-based models could not handle the dynamic status perfectly. In this paper, an adaptive learning algorithm is proposed for task adaptation that best fits the runtime application domain in applying Chinese homophone disambiguation. The proposed algorithm is first formulated by a neural network model and then generalized to avoid the problem of slow convergence. The resulting techniques are greatly simplified and robust. The experimental results demonstrate the effects of the learning algorithm from a generic domain to a specific one. Amethodology is also presented to show how these techniques can be extended to various language models and corpus-based applications.",Adaptive learning; Chinese homophone disambiguation; Language model; Neural network; Runtime application domain; Task adaptation,Adaptive algorithms; Convergence of numerical methods; Feedback; Global optimization; Information analysis; Learning algorithms; Neural networks; Probability; Random processes; Robustness (control systems); Adaptive learning; Chinese homophone disambiguation; Language models; Runtime application domain; Task adaptation; Natural language processing systems
Cross-language spoken document retrieval using HMM-based retrieval model with multi-scale fusion,2003,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-3042818033&doi=10.1145%2f964161.964162&partnerID=40&md5=76549f5471c6e385993757317db946b0,"Cross-language spoken document retrieval (CL-SDR) is the technology that facilitates automatic retrieval of relevant information from a collection of spoken documents in a language that is different from that used in the queries. Information sources that are in different languages can then be retrieved automatically with CL-SDR, and the number of searchable information sources will increase significantly. The HMM-based retrieval model is a probabilistic formulation for the retrieval problem. Extensions to this retrieval model can be made by taking advantage of its probabilistic nature. Specifically, we have incorporated the translation component to make it possible to perform cross-language information retrieval (CLIR). In addition, this HMM-based CLIR retrieval model is also extended for retrieval at subword scales. In this work the extended HMM-based retrieval model has been applied to an English-Mandarin CL-SDR task, which is to search the Mandarin spoken document collection with English queries at word and subword scales. Retrieval results obtained from these indexing scales are then fused for multi-scale CL-SDR. Experimental results demonstrate that improvement in CL-SDR retrieval performance can be achieved by fusion of word and subword scales.",Cross-language information retrieval; Multi-scale data fusion; Spoken document retrieval,Data storage equipment; Feedback; Natural language processing systems; Probability; Query languages; Robustness (control systems); Translation (languages); Vocabulary control; Word processing; Cross-Language Information Retrieval (CLIR); Indexing scales; Multi-Scale Data Fusion; Spoken document retrieval; Information retrieval systems
A Comparison of Chinese Document Indexing Strategies and Retrieval Models,2002,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34247334591&doi=10.1145%2f772755.772758&partnerID=40&md5=f24f0dc33c465dd8a7c636dfa653d9be,"With the advent of the Internet and intranets, substantial interest is being shown in Asian language information retrieval; especially in Chinese, which is a good example of an Asian ideographic language (other examples include Japanese and Korean). Since, in this type of language, spaces do not delimit words, an important issue is which index terms should be extracted from documents. This issue also has wider implications for indexing other languages such as agglutinating languages (e.g., Finnish and Turkish), archaic ideographic languages like Egyptian hieroglyphs, and other types of information such as data stored in genomic databases. Although comparisons of indexing strategies for Chinese documents have been made, almost all of them are based on a single retrieval model. This article compares the performance of various combinations of indexing strategies (i.e., character, word, short-word, bigram, and Pircs indexing) and retrieval models (i.e., vector space, 2- Poisson, logistic regression, and Pircs models). We determine which model (and its parameters) achieves the (near) best retrieval effectiveness without relevance feedback, and compare it with the open evaluations (i.e.,TREC and NTCIR) for both long and title queries. In addition, we describe a more extensive investigation of retrieval efficiency. In particular, the storage cost of word indexing is only slightly more than character indexing, and bigram indexing is about double the storage cost of other indexing strategies. The retrieval time typically varies linearly with the number of unique terms in the query, which is supported by correlation values above 90%. The Pircs retrieval system achieves robust and good retrieval performance, but it appears to be the slowest method, whereas vector space models were not very effective in retrieval, but were able to respond quickly. For robust, near-best retrieval effectiveness, without considering storage overhead, the 2-Poisson model using bigram indexing appears to be a good compromise between retrieval effectiveness and efficiency for both long and title queries. © 2002, ACM. All rights reserved.",Algorithms; Chinese information retrieval; comparison; Experimentation; indexing strategies; Languages; Performance,
A Chinese Dictionary Construction Algorithm for Information Retrieval,2002,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33645295002&doi=10.1145%2f795458.795460&partnerID=40&md5=53b8e26d7c2576ccdb1884de6c7de81f,"dictionary. The method makes use of local statistical information (i.e., data within a document) to identify and discard repeated string patterns, which, at an earlier stage, were substrings of legitimate words. Global statistical information (which exists throughout the entire corpus) and contextual constraints are then used for further filtering. The method can be used to alleviate the out-of-vocabulary (OOV) problem, which is commonly found in dictionary-based natural language information-processing applications, e.g., word segmentation. It can handle text corpora dynamically and, further, it does not impose any strict requirements on the size and quality of the training corpora. Based on our method, we constructed Chinese dictionaries from different Chinese corpora. We then applied the words in the constructed dictionaries to indexing in information retrieval (IR). Retrieval performance using such indexes was compared to the same, but based on indexes produced by static dictionaries. Three Chinese corpora using various character-encoding schemes and language styles were used in the experiments. The results show that retrieval using indexes based on the constructed dictionary is effective. This implies that fully automatic Chinese dictionary construction based on dynamic data sources, e.g., from the Internet, for the purposes of IR is feasible. Drawing on the experiment, we were able to make some interesting observations: (1) using only a portion of a dictionary is enough to produce good retrieval performance, e.g., adictionary consisting of only the 500 highest-frequency strings extracted from the NTCIR 2 Chinese corpus produced as good a retrieval result as using a more complete dictionary with over 100K entries; and (2) complete word segmentation is not a strict requirement for achieving practical information retrieval. © 2002, ACM. All rights reserved.",Algorithms; Automatic word extraction; Chinese information retrieval; dictionary construction; Languages; Performance,
Combining Character-Based Bigrams with Word-Based Bigrams in Contextual Postprocessing for Chinese Script Recognition,2002,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-3042574909&doi=10.1145%2f795458.795461&partnerID=40&md5=278c08c02b64fef28e1f8394b0086c5a,"It is crucial to use contextual information to improve the recognition accuracy of Chinese script in an offline, handwritten Chinese character-recognition system. However, with the increase in the number of candidates given by a character recognizer, contextual postprocessing using a word-based bigram is time-consuming. This article presents a novel contextual postprocessing method that integrates character-based bigram postprocessing with word-based bigram postprocessing in light of the complementary action between Chinese characters and Chinese words. On the basis of isolated character recognition, character-based bigram postprocessing using a forward-backward search is first executed on a big candidate set, which improves both the accuracy and efficiency of the candidate set (the cumulative accuracy of the top ten candidates is greatly boosted). Then, to further improve accuracy, word-based bigram postprocessing (WBP) is executed on a small candidate set. This method obtains high accuracy while paying attention to postprocessing speed at the same time. Experimental results for three Chinese scripts (about 66,000 characters in total) demonstrate the effectiveness of our method: character-based bigram postprocessing improves accuracy from 81.58% to 94.50%, and the cumulative accuracy of the top ten candidates rises from 94.33% to 98.25%. After WBP, 95.75% accuracy is achieved, which is equivalent to the accuracy of WBP executed on a big candidate set. However, our method is more than 100 times faster than that of WBP. © 2002, ACM. All rights reserved.",Algorithms; Chinese character recognition; contextual postprocessing; efficiency of candidate set; forward-backward search; Languages; Performance; statistical language model,
A Language and Character Set Determination Method Based on N-gram Statistics,2002,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80155181779&doi=10.1145%2f772755.772759&partnerID=40&md5=50ce95d998f95ebd99337376ce8fe3ff,"An N-gram-based language, script, and encoding scheme-detection method is introduced in this article. The method detects language, script, and encoding schemes using a target text document encoded by computer by checking how many byte sequences of the target match the byte sequences that can appear in the texts belonging to a language, script, and encoding scheme. This detection mechanism is different from conventional N-gram-based methods in that its threshold for any category is uniquely predetermined. The method was originally created for a survey of web pages conducted to find how many web pages are written in a particular language, script, and encoding scheme. The requirement is that the method must be able to respond to either “correct answer” or “unable to detect” where “unable to detect” includes “other than registered.” There are some minor problems with this method, but its effectiveness as a language, script, and encoding schemedetection method has been confirmed by experiments. © 2002, ACM. All rights reserved.",Algorithms; character set; corpus-based analysis; Languages; local language site; N-gram; natural languages; Text categorization; Unicode,
Automatic Corpus-Based Tone and Break-Index Prediction Using K-ToBI Representation,2002,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994781069&doi=10.1145%2f772755.772757&partnerID=40&md5=de65928011d82c118a46a0b4eeea490d,"In this article we present a prosody generation architecture based on K-ToBI (Korean Tone and Break Index) representation. ToBI is a multitier representation system based on linguistic knowledge that transcribes events in an utterance. The TTS (Text-To-Speech) system, which adopts ToBI as an intermediate representation, is known to exhibit higher flexibility, modularity, and domain/task portability compared to the direct prosody generation TTS systems. However, for practical-level performance, the cost of corpus preparation is very expensive because the ToBI labeled corpus is constructed manually by many prosody experts, and normally requires large amounts of data for statistical prosody modeling. Unlike previous ToBI-based systems, this article proposes a new method, which transcribes the K-ToBI labels in Korean speech completely automatically. We develop automatic corpus-based K-ToBI labeling tools and prediction methods based on several lexicosyntactic linguistic features for decision-tree induction. We demonstrate the performance of F0 generation from automatically predicted K-ToBI labels, and confirm that the performance is reasonably comparable to state-ofthe-art direct prosody generation methods and previous ToBI-based methods. © 2002, ACM. All rights reserved.",Experimentation; intonation; K-ToBI; Languages; Performance; phrase break; pitch; prosodic phrase; prosody; Text-to-speech system,
A Word-Based Approach for Modeling and Discovering Temporal Relations Embedded in Chinese Sentences,2002,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988220373&doi=10.1145%2f772755.772756&partnerID=40&md5=ac7067a496d58e80cd6189fd30d0e3bb,"Conventional information extraction systems cannot effectively mine temporal information. For example, users queries on how one event is related to another in time could not be handled effectively. For this reason, it is important to capture and deduce temporal knowledge associated with the relevant events. It is generally acknowledged that information extraction cannot be isolated from natural language processing. As Chinese has no tenses, conventional means for finding temporal references based on verb forms no longer apply. In this article we present an approach for formulating and discovering temporal relations in Chinese. A set of rules is devised to map the combinational effects of the temporal indicators (also known as temporal markers, gathered from various grammatical categories) in a sentence to its corresponding temporal relation. To evaluate the proposed algorithm, experiments were conducted using a set of news reports and the results look promising. Problem discussions are also provided. Through this work, we hope to open up new doors for future research in Chinese temporal information extraction and processing. © 2002, ACM. All rights reserved.",Algorithms; Chinese language processing; Experimentation; Languages; Temporal information processing; temporal relationship discovery,
Translation of Web Queries Using Anchor Text Mining,2002,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84956503438&doi=10.1145%2f568954.568958&partnerID=40&md5=53f07cf269e16b048565acea61dae37b,"This article presents an approach to automatically extracting translations of Web query terms through mining of Web anchor texts and link structures. One of the existing difficulties in cross-language information retrieval (CLIR) and Web search is the lack of appropriate translations of new terminology and proper names. The proposed approach successfully exploits the anchor-text resources and reduces the existing difficulties of query term translation. Many query terms that cannot be obtained in general-purpose translation dictionaries are, therefore, extracted. © 2002, ACM. All rights reserved.",Algorithms; anchor text mining; comparable corpora; cross-language information retrieval; Experimentation; machine translation; parallel corpora; Performance; Web mining,
Building a Chinese-English WordNet for Translingual Applications,2002,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017265378&doi=10.1145%2f568954.568955&partnerID=40&md5=8276a9039554ec2f64ffd03f389a30cb,"A WordNet-like linguistic resource is useful, but difficult to construct. This article proposes a method to integrate five linguistic resources, including English/Chinese sense-tagged corpora, English/Chinese thesauruses, and a bilingual dictionary. Chinese words are mapped into WordNet. A Chinese WordNet and a Chinese-English WordNet are derived by following the structures of WordNet. Experiments with Chinese-English information retrieval are developed to evaluate the applicability of the Chinese-English WordNet. The best model achieves 0.1010 average precision, 69.23% of monolingual information retrieval. It also gains a 10.02% increase relative to a model that resolves translation ambiguity and target polysemy problems together. © 2002, ACM. All rights reserved.",Bilingual WordNet; cross language information retrieval; Experimentation; Languages; sense tagging; thesaurus construction; translingual application; word sense disambiguation,
Prologue,2002,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025401329&doi=10.1145%2f595576.595577&partnerID=40&md5=b8864acb18861d0204934da49d03149c,[No abstract available],,
GLR Parsing with Multiple Grammars for Natural Language Queries,2002,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-20244382869&doi=10.1145%2f568954.568956&partnerID=40&md5=8c25a44be2f0f1c9682ac384e81d11c8,"This article presents an approach for parsing natural language queries that integrates multiple subparsers and subgrammars, in contrast to the traditional single grammar and parser approach. In using LR(k) parsers for natural language processing, we are faced with the problem of rapid growth in parsing table sizes as the number of grammar rules increases. We propose to partition the grammar into multiple subgrammars, each having its own parsing table and parser. Grammar partitioning helps reduce the overall parsing table size when compared to using a single grammar. We used the GLR parser with an LR(1) parsing table in our framework because GLR parsers can handle ambiguity in natural language. A parser composition technique then combines the parsers outputs to produce an overall parse that is the same as the output parse of single parser. Two different strategies were used for parser composition: (i) parser composition by cascading; and (ii) parser composition with predictive pruning. Our experiments were conducted with natural language queries from the ATIS (Air Travel Information Service) domain. We have manually translated the ATIS-3 corpora into Chinese, and consequently we could experiment with grammar partitioning on parallel linguistic corpora. For English, the unpartitioned ATIS grammar has 72,869 states in its parsing table, while the partitioned English grammar has 3,350 states in total. For Chinese, grammar partitioning reduced the overall parsing table size from 29,734 states to 3,894 states. Both results show that grammar partitioning greatly economizes on the overall parsing table size. Language understanding performances were also examined. Parser composition imparts a robust parsing capability in our framework, and hence obtains a higher understanding performance when compared to using a single GLR parser. © 2002, ACM. All rights reserved.",Algorithms; Experimentation; Generalized LR parsing; grammar partitioning; lattice with multiple granularities; parser composition; Performance,
Comparison of Three Machine-Learning Methods for Thai Part-of-Speech Tagging,2002,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84969841009&doi=10.1145%2f568954.568957&partnerID=40&md5=05fc37ff3201c9753a9b7e8775a1bdef,"The elastic-input neuro-tagger and hybrid tagger, combined with a neural network and Brill's error-driven learning, have already been proposed to construct a practical tagger using as little training data as possible. When a small Thai corpus is used for training, these taggers have tagging accuracies of, respectively, 94.4% and 95.5% (accounting only for the ambiguous words that relate to the parts of speech). In this study, in order to construct more accurate taggers, we developed new tagging methods using three different machine-learning approaches: the decision list, maximum entropy, and the support vector machine methods. We then performed tagging experiments using them. Our results show that the support vector machine method has the best precision (96.1%), and that it is capable of improving the accuracy of tagging in the Thai language. The improvement in accuracy was also confirmed by using a statistical test (a sign test). Finally, we examined theoretically all these methods in an effort to determine how the improvements were achieved. We found that the improvements were due to our use of word information, which is helpful for tagging, and a support vector machine that performed well. © 2002, ACM. All rights reserved.",Algorithms; Decision list method; Experimentation; Languages; lexical information; machine learning; maximum entropy method; Performance; POS tagging; support vector machine,
Meaningful Term Extraction and Discriminative Term Selection in Text Categorization via Unknown-Word Methodology,2002,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-1842696200&doi=10.1145%2f595576.595579&partnerID=40&md5=dee5940685965d48cea43d202fc991cd,"In this article, an approach based on unknown words is proposed for meaningful term extraction and discriminative term selection in text categorization. For meaningful term extraction, a phrase-like unit (PLU)-based likelihood ratio is proposed to estimate the likelihood that a word sequence is an unknown word. On the other hand, a discriminative measure is proposed for term selection and is combined with the PLU-based likelihood ratio to determine the text category. We conducted several experiments on a news corpus, called MSDN. The MSDN corpus is collected from an online news Website maintained by the Min-Sheng Daily News, Taiwan. The corpus contains 44,675 articles with over 35 million words. The experimental results show that the system using a simple classifier achieved 95.31% accuracy. When using a state-of-the-art classifier, kNN, the average accuracy is 96.40%, outperforming all the other systems evaluated on the same collection, including the traditional term-word by kNN (88.52%); sleeping-experts (82.22%); sparse phrase by four-word sleeping-experts (86.34%); and Boolean combinations of words by RIPPER (87.54%). A proposed purification process can effectively reduce the dimensionality of the feature space from 50,576 terms in the word-based approach to 19,865 terms in the unknown word-based approach. In addition, more than 80% of automatically extracted terms are meaningful. Experiments also show that the proportion of meaningful terms extracted from training data is relative to the classification accuracy in outside testing. © 2002, ACM. All rights reserved.",AC-machine; Algorithms; dimensionality reduction; discriminability; discriminative term selection; Experimentation; inconsistency problem; meaningful term extraction; Measurement; n-gram; Performance; phrase-like unit; sparse data problem; term adaptation; term purification; text categorization; text indexing; unknown word detection; vector space modeling,
Toward a Unified Approach to Statistical Language Modeling for Chinese,2002,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016472884&doi=10.1145%2f595576.595578&partnerID=40&md5=3262b467228c18ab34b7f78aee12d2b3,"This article presents a unified approach to Chinese statistical language modeling (SLM). Applying SLM techniques like trigram language models to Chinese is challenging because (1) there is no standard definition of words in Chinese; (2) word boundaries are not marked by spaces; and (3) there is a dearth of training data. Our unified approach automatically and consistently gathers a high-quality training data set from the Web, creates a high-quality lexicon, segments the training data using this lexicon, and compresses the language model, all by using the maximum likelihood principle, which is consistent with trigram model training. We show that each of the methods leads to improvements over standard SLM, and that the combined method yields the best pinyin conversion result reported. © 2002, ACM. All rights reserved.",backoff; character error rate; Chinese language; Chinese pinyin-to-character conversion; domain adaptation; Experimentation; Human Factors; Languages; lexicon; Measurement; n-gram model; perplexity; pruning; smoothing; Statistical language modeling; word segmentation,
Morpheme-Based Grapheme to Phoneme Conversion Using Phonetic Patterns and Morphophonemic Connectivity Information,2002,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79959840526&doi=10.1145%2f595576.595580&partnerID=40&md5=fb0d9f960f2e33ef2800ec0ec8331631,"Both dictionary-based and rule-based methods on grapheme-to-phoneme conversion have their own advantages and limitations. For example, a large sized phonetic dictionary and complex morphophonemic rules are required for the dictionary-based method and the LTS (letter to sound) rule-based method itself cannot model the complete morphophonemic constraints. This paper describes a grapheme-to-phoneme conversion method for Korean using a dictionary-based and rule-based hybrid method with phonetic pattern dictionary and CCV (consonant consonant vowel) LTS (letter to sound) rules. The phonetic pattern dictionary, standing for the dictionary-based method, contains entries in the form of a morpheme pattern and its phonetic pattern. The patterns represent candidate phonological changes in left and right boundaries of morphemes. Obviously, the CCV LTS rules stand for the rule-based method. The rules are in charge of grapheme-to-phoneme conversion within morphemes. The conversion method consists of mainly two steps including morpheme to phoneme conversion and morphophonemic connectivity check, and two preprocessing steps including phrase break prediction and morpheme normalization. Phrase break prediction presumes phrase breaks using the stochastic method on part-of-speech (POS) information. Morpheme normalization is to replace non-Korean symbols with their corresponding standard Korean graphemes. In the morpheme-phoneticizing module, each morpheme in the phrase is converted into phonetic patterns by looking it up in the phonetic pattern dictionary. Graphemes within a morpheme are grouped into CCV units and converted into phonemes by the CCV LTS rules. The morphophonemic connectivity table supports grammaticality checking of the two adjacent phonetic morphemes. In experiments with a non-Korean symbol free corpus of 4,973 sentences, we achieved a 99.98% grapheme-to-phoneme conversion performance rate and a 99.0% sentence conversion performance rate. With a broadcast news corpus of 621 sentences, 99.7% of the graphemes and 86.6% of the sentences are correctly converted. The full Korean TTS (Text-to-Speech) system is now being implemented using this conversion method. © 2002, ACM. All rights reserved.",CCV LTS rule; Experimentation; grapheme-to-phoneme conversion; Languages; morpho-phonemic modeling; Performance; phonetic pattern dictionary; Text-to-speech system,
Using Tone Information in Cantonese Continuous Speech Recognition,2002,ACM Transactions on Asian Language Information Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-1242349959&doi=10.1145%2f595576.595581&partnerID=40&md5=d1e8834933daa5dbf8a47070f748c500,"In Chinese languages, tones carry important information at various linguistic levels. This research is based on the belief that tone information, if acquired accurately and utilized effectively, contributes to the automatic speech recognition of Chinese. In particular, we focus on the Cantonese dialect, which is spoken by tens of millions of people in Southern China and Hong Kong. Cantonese is well known for its complicated tone system, which makes automatic tone recognition very difficult. This article describes an effective approach to explicit tone recognition of Cantonese in continuously spoken utterances. Tone feature vectors are derived, on a short-time basis, to characterize the syllable-wide patterns of F0 (fundamental frequency) and energy movements. A moving-window normalization technique is proposed to reduce the tone-irrelevant fluctuation of F0 and energy features. Hidden Markov models are employed for context-dependent acoustic modeling of different tones. A tone recognition accuracy of 66.4% has been achieved in the speaker-independent case. The recognized tone patterns are then utilized to assist Cantonese large-vocabulary continuous speech recognition (LVCSR) via a lattice expansion approach. Experimental results show that reliable tone information helps to improve the overall performance of LVCSR. © 2002, ACM. All rights reserved.",Chinese dialects; F0 normalization; knowledge integration; Language; Performance; speech recognition; tone recognition,
