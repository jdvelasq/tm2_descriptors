Title,Year,Source title,Link,Abstract,Author Keywords,Index Keywords
A process for systematic development of symbolic models for activity recognition,2015,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84967185429&doi=10.1145%2f2806893&partnerID=40&md5=1e7ce04ca3d05377c0eae02ececdd0a3,"Several emerging approaches to activity recognition (AR) combine symbolic representation of user actions with probabilistic elements for reasoning under uncertainty. These approaches provide promising results in terms of recognition performance, coping with the uncertainty of observations, and model size explosion when complex problems are modelled. But experience has shown that it is not always intuitive to model even seemingly simple problems. To date, there are no guidelines for developing such models. To address this problem, in this work we present a development process for building symbolic models that is based on experience acquired so far as well as on existing engineering and data analysis workflows. The proposed process is a first attempt at providing structured guidelines and practices for designing, modelling, and evaluating human behaviour in the form of symbolic models for AR. As an illustration of the process, a simple example from the office domainwas developed. The processwas evaluated in a comparative study of an intuitive process and the proposed process. The results showed a significant improvement over the intuitive process. Furthermore, the study participants reported greater ease of use and perceived effectiveness when following the proposed process. To evaluate the applicability of the process to more complex AR problems, it was applied to a problem from the kitchen domain. The results showed that following the proposed process yielded an average accuracy of 78%. The developed model outperformed state-of-the-art methods applied to the same dataset in previous work, and it performed comparably to a symbolic model developed by a model expert without following the proposed development process. © 2015 ACM.",Activity recognition; Context awareness; Human behaviour models; Model development; Probabilistic models; Symbolic models,Behavioral research; Pattern recognition; Social sciences; Activity recognition; Context- awareness; Human behaviours; Model development; Probabilistic models; Symbolic model; Human form models
Generating robot gaze on the basis of participation roles and dominance estimation in multiparty interaction,2015,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84967185394&doi=10.1145%2f2743028&partnerID=40&md5=da142fc4a308b6dc03a797e5909f2b60,"Gaze is an important nonverbal feedback signal in multiparty face-to-face conversations. It is well known that gaze behaviors differ depending on participation role: speaker, addressee, or side participant. In this study, we focus on dominance as another factor that affects gaze. First, we conducted an empirical study and analyzed its results that showed how gaze behaviors are affected by both dominance and participation roles. Then, using speech and gaze information that was statistically significant for distinguishing the more dominant and less dominant person in an empirical study, we established a regression-based model for estimating conversational dominance. On the basis of the model, we implemented a dominance estimation mechanism that processes online speech and head direction data. Then we applied our findings to humanrobot interaction. To design robot gaze behaviors, we analyzed gaze transitions with respect to participation roles and dominance and implemented gaze-transition models as robot gaze behavior generation rules. Finally, we evaluated a humanoid robot that has dominance estimation functionality and determines its gaze based on the gaze models, and we found that dominant participants had a better impression of less dominant robot gaze behaviors. This suggests that a robot using our gaze models was preferred to a robot that was simply looking at the speaker. We have demonstrated the importance of considering dominance in human-robot multiparty interaction. © 2015 ACM.",Conversational dominance; Gaze generation; Humanoid robot; Participation role,Anthropomorphic robots; Machine design; Robots; Conversational dominance; Face-to-face conversation; Gaze generation; Humanoid robot; Multi-party interactions; Nonverbal feedback; Participation role; Regression-based model; Human robot interaction
The movielens datasets: History and context,2015,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84966800432&doi=10.1145%2f2827872&partnerID=40&md5=6126ab958a2246b90f5056dcc34a150d,"TheMovieLens datasets are widely used in education, research, and industry. They are downloaded hundreds of thousands of times each year, reflecting their use in popular press programming books, traditional and online courses, and software. These datasets are a product of member activity in the MovieLens movie recommendation system, an active research platform that has hosted many experiments since its launch in 1997. This article documents the history of MovieLens and the MovieLens datasets. We include a discussion of lessons learned from running a long-standing, live research platform from the perspective of a research organization. We document best practices and limitations of using the MovieLens datasets in new research. © 2015 ACM.",Datasets; Movielens; Ratings; Recommendations,Rating; Best practices; Datasets; Member activities; Movielens; Online course; Recommendations; Research organization; Research platforms; Artificial intelligence
Adaptive real-time emotion recognition from body movements,2015,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84967155283&doi=10.1145%2f2738221&partnerID=40&md5=48f90348390fc5dcb2b80cec51486e37,"We propose a real-time system that continuously recognizes emotions from body movements. The combined low-level 3D postural features and high-level kinematic and geometrical features are fed to a Random Forests classifier through summarization (statistical values) or aggregation (bag of features). In order to improve the generalization capability and the robustness of the system, a novel semisupervised adaptive algorithm is built on top of the conventional Random Forests classifier. The MoCap UCLIC affective gesture database (labeled with four emotions) was used to train the Random Forests classifier, which led to an overall recognition rate of 78% using a 10-fold cross-validation. Subsequently, the trained classifier was used in a stream-based semisupervised Adaptive Random Forests method for continuous unlabeled Kinect data classification. The very low update cost of our adaptive classifier makes it highly suitable for data stream applications. Tests performed on the publicly available emotion datasets (body gestures and facial expressions) indicate that our new classifier outperforms existing algorithms for data streams in terms of accuracy and computational costs. © 2015 ACM.",Emotion recognition; Online learning; Random forests; Real-time system; Semisupervised learning,Adaptive algorithms; Data communication systems; Decision trees; Interactive computer systems; Real time systems; Speech recognition; 10-fold cross-validation; Emotion recognition; Generalization capability; Geometrical features; Online learning; Random forests; Real-time emotion recognition; Semi- supervised learning; Classification (of information)
The WOZ Recognizer: A wizard of Oz sketch recognition system,2015,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84966874278&doi=10.1145%2f2743029&partnerID=40&md5=ed61a40a1b8c15bf24024c35ad510af1,"Sketch recognition has the potential to be an important input method for computers in the coming years, particularly for STEM (science, technology, engineering, and math) education. However, designing and building an accurate and sophisticated sketch recognition system is a time-consuming and daunting task. Since sketch recognition mistakes are still common, it is important to understand how users perceive and tolerate recognition errors and other user interface elements with these imperfect systems. In order to solve this problem, we developed a Wizard of Oz sketch recognition tool, the WOZ Recognizer, that supports controlled recognition accuracy, multiple recognition modes, and multiple sketching domains for performing controlled experiments. We present the design of the WOZ Recognizer and our process for representing recognition domains using graphs and symbol alphabets. In addition, we discuss how sketches are altered, how to control the WOZ Recognizer, and how users interact with it. Finally, we present an expert user case study that examines the WOZ Recognizer's usability. © 2015 ACM.",Diagrams; Mathematics; Pen-computing; Sketch recognition; Wizard of Oz,"Light pens; Mathematical techniques; Controlled experiment; Diagrams; Pen computing; Recognition accuracy; Science , technology , engineering , and maths; Sketch recognition; Sketch recognition systems; Wizard of Oz; User interfaces"
Behavior understanding for arts and entertainment,2015,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84966936651&doi=10.1145%2f2817208&partnerID=40&md5=3bb8baa25db9242cf2a7d98f6c6c2ece,"This editorial introduction complements the shorter introduction to the first part of the two-part special issue on Behavior Understanding for Arts and Entertainment. It offers a more expansive discussion of the use of behavior analysis for interactive systems that involve creativity, either for the producer or the consumer of such a system. We first summarise the two articles that appear in this second part of the special issue. We then discuss general questions and challenges in this domain that were suggested by the entire set of seven articles of the special issue and by the comments of the reviewers of these articles.",Affective computing; Behavior analysis; Human-environment interaction; Interactive arts; Social and nonverbal behaviors; Visual arts,Arts computing; Affective Computing; Art and entertainment; Behavior analysis; Behavior understanding; Human-environment interaction; Interactive arts; Interactive system; Non-verbal behaviours; Social behaviour; Visual arts; Consumer behavior
A review and taxonomy of interactive optimization methods in operations research,2015,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84966783571&doi=10.1145%2f2808234&partnerID=40&md5=1769d771124b7b4b505f23c47d4dc4d9,"This article presents a review and a classification of interactive optimization methods. These interactive methods are used for solving optimization problems. The interaction with an end user or decision maker aims at improving the efficiency of the optimization procedure, enriching the optimization model, or informing the user regarding the solutions proposed by the optimization system. First, we present the challenges of using optimization methods as a tool for supporting decision making, and we justify the integration of the user in the optimization process. This integration is generally achieved via a dynamic interaction between the user and the system. Next, the different classes of interactive optimization approaches are presented. This detailed review includes trial and error, interactive reoptimization, interactive multiobjective optimization, interactive evolutionary algorithms, human-guided search, and other approaches that are less well covered in the research literature. On the basis of this review, we propose a classification that aims to better describe and compare interaction mechanisms. This classification offers two complementary views on interactive optimization methods. The first perspective focuses on the user's contribution to the optimization process, and the second concerns the components of interactive optimization systems. Finally, on the basis of this review and classification, we identify some open issues and potential perspectives for interactive optimization methods. © 2015 ACM.",Combinatorial optimization; Decision support; Interactive optimization,Combinatorial optimization; Decision making; Decision support systems; Evolutionary algorithms; Human computer interaction; Operations research; Optimization; Decision supports; Interaction mechanisms; Interactive evolutionary algorithms; Interactive multiobjective optimization; Interactive optimization; Optimization modeling; Optimization problems; Optimization procedures; Multiobjective optimization
Quantitative study of music listening behavior in a smartphone context,2015,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84966800353&doi=10.1145%2f2738220&partnerID=40&md5=c3af54a33fe9cfa5d57352b31f2852bd,"Context-based services have attracted increasing attention because of the prevalence of sensor-rich mobile devices such as smartphones. The idea is to recommend information that a user would be interested in according to the user's surrounding context. Although remarkable progress has been made to contextualize music playback, relatively little research has been made using a large collection of real-life listening records collected in situ. In light of this fact, we present in this article a quantitative study of the personal, situational, and musical factors of musical preference in a smartphone context, using a new dataset comprising the listening records and self-report context annotation of 48 participants collected over 3wk via an Android app. Although the number of participants is limited and the population is biased towards students, the dataset is unique in that it is collected in a daily context, with sensor data and music listening profiles recorded at the same time. We investigate 3 core research questions evaluating the strength of a rich set of low-level and high-level audio features for music usage auto-tagging (i.e., music preference in different user activities), the strength of time-domain and frequency-domain sensor features for user activity classification, and how user factors such as personality traits are correlated with the predictability of music usage and user activity, using a closed set of 8 activity classes. We provide an in-depth discussion of the main findings of this study and their implications for the development of context-based music services for smartphones. © 2015 ACM 2160-6455/2015/09-ART14 $15.00.",Context-aware music recommendation; Mobile application; Music emotion recognition; Music information retrieval; Smartphone,Frequency domain analysis; Mobile devices; Population statistics; Signal encoding; Smartphones; Context annotation; Context based services; Frequency-domain sensor; Mobile applications; Music emotions; Music information retrieval; Music recommendation; Personality traits; Audio acoustics
Interactive visuals as metaphors for dance movement qualities,2015,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84967333731&doi=10.1145%2f2738219&partnerID=40&md5=8e1270a26f543f61313edbdbdc68a3da,"The notion of ""movement qualities"" is central in contemporary dance; it describes the manner in which a movement is executed. Movement qualities convey information revealing movement expressiveness; their use has strong potential for movement-based interaction with applications in arts, entertainment, education, or rehabilitation. The purpose of our research is to design and evaluate interactive reflexive visuals for movement qualities. The theoretical basis for this research is drawn from a collaboration with the members of the international dance company Emio Greco|PC to study their formalization of movement qualities. We designed a pedagogical interactive installation called Double Skin/Double Mind (DS/DM) for the analysis and visualization of movement qualities through physical model-based interactive renderings. In this article, we first evaluate dancers' perception of the visuals as metaphors for movement qualities. This evaluation shows that, depending on the physical model parameterization, the visuals are capable of generating dynamic behaviors that the dancers associate with DS/DM movement qualities. Moreover, we evaluate dance students' and professionals' experience of the interactive visuals in the context of a dance pedagogical workshop and a professional dance training. The results of these evaluations show that the dancers consider the interactive visuals to be a reflexive system that encourages them to perform, improves their experience, and contributes to a better understanding of movement qualities. Our findings support research on interactive systems for real-time analysis and visualization of movement qualities, which open new perspectives in movement-based interaction design. © 2015 ACM 2160-6455/2015/09-ART13 $15.00.",Dance; Interactive visuals; Mass-spring systems; Movement qualities; Physical models; User evaluation,Education; Education computing; Real time systems; Visualization; Dance; Interactive visuals; Mass spring systems; Movement qualities; Physical model; User evaluations; Quality control
Exploratory visual analysis and interactive pattern extraction from semi-structured data,2015,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84966908063&doi=10.1145%2f2812115&partnerID=40&md5=108e0ad446022a52b65c1230871c35ea,"Semi-structured documents are a common type of data containing free text in natural language (unstructured data) as well as additional information about the document, or meta-data, typically following a schema or controlled vocabulary (structured data). Simultaneous analysis of unstructured and structured data enables the discovery of hidden relationships that cannot be identified from either of these sources when analyzed independently of each other. In this work, we present a visual text analytics tool for semi-structured documents (ViTA-SSD), that aims to support the user in the exploration and finding of insightful patterns in a visual and interactive manner in a semi-structured collection of documents. It achieves this goal by presenting to the user a set of coordinated visualizations that allows the linking of the metadata with interactively generated clusters of documents in such a way that relevant patterns can be easily spotted. The system contains two novel approaches in its back end: a feature-learning method to learn a compact representation of the corpus and a fast-clustering approach that has been redesigned to allow user supervision. These novel contributions make it possible for the user to interact with a large and dynamic document collection and to perform several text analytical tasks more efficiently. Finally, we present two use cases that illustrate the suitability of the system for in-depth interactive exploration of semi-structured document collections, two user studies, and results of several evaluations of our text-mining components. © 2015 ACM 2160-6455/2015/09-ART16 $15.00.",Dimensionality reduction; Interactive clustering; Visual text analytics,Metadata; Natural language processing systems; Collection of documents; Compact representation; Dimensionality reduction; Interactive clustering; Interactive exploration; Semi-structured documents; Simultaneous analysis; Text analytics; Data mining
Brief introduction to the special issue on behavior understanding for arts and entertainment,2015,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84967188699&doi=10.1145%2f2786762&partnerID=40&md5=7daa5cc21399ee8f8b109380457b1648,"This editorial introduction describes the aims and scope of the special issue of the ACM Transactions on Interactive Intelligent Systems on Behavior Understanding for Arts and Entertainment, which is being published in issues 2 and 3 of volume 5 of the journal. Here we offer a brief introduction to the use of behavior analysis for interactive systems that involve creativity in either the creator or the consumer of a work of art.We then characterize each of the five articles included in this first part of the special issue, which span a wide range of applications. © 2015 ACM.",Affective computing; Behavior analysis; Human-environment interaction; Interactive arts; social and nonverbal behaviors; Visual arts,Arts computing; Behavioral research; Intelligent systems; Affective Computing; Behavior analysis; Human-environment interaction; Interactive arts; Nonverbal behavior; Visual arts; Consumer behavior
The VideoMob interactive art installation connecting strangers through inclusive digital crowds,2015,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84967333758&doi=10.1145%2f2768208&partnerID=40&md5=73fa9c557122dbd2b9a28326a3cf50a1,"VideoMob is an interactive video platform and an artwork that enables strangers visiting different installation locations to interact across time and space through a computer interface that detects their presence, video-records their actions while automatically removing the video background through computer vision, and co-situates visitors as part of the same digital environment. Through the combination of individual user videos to form a digital crowd, strangers are connected through the graphic display. Our work is inspired by the way distant people can interact with each other through technology and influenced by artists working in the realm of interactive art. We deployed VideoMob in a variety of settings, locations, and contexts to observe hundreds of visitors' reactions. By analyzing behavioral data collected through depth cameras from our 1,068 recordings across eight venues, we studied how participants behave when given the opportunity to record their own video portrait into the artwork. We report the specific activity performed in front of the camera and the influences that existing crowds impose on new participants. Our analysis informs the integration of a series of possible novel interaction paradigms based on real-time analysis of the visitors' behavior through specific computer vision and machine learning techniques that have the potential to increase the engagement of the artwork's visitors and to impact user experience. © 2015 ACM 2160-6455/2015/07-ART7 $15.00.",Arts; Crowd; Interactivity; Kinect; Projection; Real-time; Video,Artificial intelligence; Cameras; Computer vision; Learning systems; Time series analysis; User interfaces; Arts; Crowd; Interactivity; Kinect; Projection; Real time; Video; Computer graphics
In the mood for vlog: Multimodal inference in conversational social video,2015,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84966891604&doi=10.1145%2f2641577&partnerID=40&md5=0f9d1231c6f79a5e211e6583df000bc8,"The prevalent ""share what's on your mind"" paradigm of social media can be examined from the perspective of mood: short-term affective states revealed by the shared data. This view takes on new relevance given the emergence of conversational social video as a popular genre among viewers looking for entertainment and among video contributors as a channel for debate, expertise sharing, and artistic expression. From the perspective of human behavior understanding, in conversational social video both verbal and nonverbal information is conveyed by speakers and decoded by viewers. We present a systematic study of classification and ranking of mood impressions in social video, using vlogs from YouTube. Our approach considers eleven natural mood categories labeled through crowdsourcing by external observers on a diverse set of conversational vlogs. We extract a comprehensive number of nonverbal and verbal behavioral cues from the audio and video channels to characterize the mood of vloggers. Then we implement and validate vlog classification and vlog ranking tasks using supervised learning methods. Following a reliability and correlation analysis of the mood impression data, our study demonstrates that, while the problem is challenging, several mood categories can be inferred with promising performance. Furthermore, multimodal features perform consistently better than single-channel features. Finally, we show that addressing mood as a ranking problem is a promising practical direction for several of the mood categories studied. © 2015 ACM.",H.3.1 [information storage and retrieval]: Content analysis and indexing; Human factors; Mood; Nonverbal behavior; Social video; Verbal content; Video blogs; Vlogs,Data mining; Digital storage; Human engineering; Reliability analysis; Social networking (online); Content analysis; Mood; Nonverbal behavior; Social videos; Verbal content; Vlogs; Behavioral research
A general-purpose sensing floor architecture for human-environment interaction,2015,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84967189067&doi=10.1145%2f2751566&partnerID=40&md5=f835375a63cad1a12fd72ff80e7fb42d,"Smart environments are now designed as natural interfaces to capture and understand human behavior without a need for explicit human-computer interaction. In this article, we present a general-purpose architecture that acquires and understands human behaviors through a sensing floor. The pressure field generated by moving people is captured and analyzed. Specific actions and events are then detected by a low-level processing engine and sent to high-level interfaces providing different functions. The proposed architecture and sensors are modular, general-purpose, cheap, and suitable for both small- and large-area coverage. Some sample entertainment and virtual reality applications that we developed to test the platform are presented. © 2015 ACM.",1.4.8 [Image processing and computer vision]: Scene analysis-sensor fusion; Design; Floor image; H.5.2 [Information interfaces and presentation]: User interfaces; Human-environment interaction; Sensing floors,Behavioral research; Computer vision; Design; Floors; Image processing; Social sciences; User interfaces; Virtual reality; General purpose architectures; H.5.2 [Information Interfaces and Presentation]: User Interfaces; High level interface; Human-environment interaction; Natural interfaces; Processing engine; Proposed architectures; Scene analysis; Human computer interaction
Context-aware automated analysis and annotation of social human-agent interactions,2015,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84966928795&doi=10.1145%2f2764921&partnerID=40&md5=537388e899ac32c8bfe6b621877eb13c,"The outcome of interpersonal interactions depends not only on the contents that we communicate verbally, but also on nonverbal social signals. Because a lack of social skills is a common problem for a significant number of people, serious games and other training environments have recently become the focus of research. In this work, we present NovA (Nonverbal behavior Analyzer), a system that analyzes and facilitates the interpretation of social signals automatically in a bidirectional interaction with a conversational agent. It records data of interactions, detects relevant social cues, and creates descriptive statistics for the recorded data with respect to the agent's behavior and the context of the situation. This enhances the possibilities for researchers to automatically label corpora of human-agent interactions and to give users feedback on strengths and weaknesses of their social behavior. © 2015 ACM.",Affective computing; Automated behavior analysis; H.1.2 [user/machine systems]; Human information processing; Interaction design; Serious games; Serious games virtual agents; Social cue recognition; Social signal processing; Virtual job interviews,Bionics; Computer games; Signal processing; Affective Computing; Behavior analysis; Human information processing; Interaction design; Serious games; Social cue recognition; Social signal processing; User/machine systems; Virtual agent; Virtual job interview; Virtual reality
Affective analysis of professional and amateur abstract paintings using statistical analysis and art theory,2015,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84967332892&doi=10.1145%2f2768209&partnerID=40&md5=4b69c9f87feb0965a9a36b83330dfb03,"When artists express their feelings through the artworks they create, it is believed that the resulting works transform into objects with ""emotions"" capable of conveying the artists' mood to the audience. There is little to no dispute about this belief: Regardless of the artwork, genre, time, and origin of creation, people from different backgrounds are able to read the emotional messages. This holds true even for the most abstract paintings. Could this idea be applied to machines as well? Can machines learn what makes a work of art ""emotional""? In this work, we employ a state-of-the-art recognition system to learn which statistical patterns are associated with positive and negative emotions on two different datasets that comprise professional and amateur abstract artworks. Moreover, we analyze and compare two different annotation methods in order to establish the ground truth of positive and negative emotions in abstract art. Additionally, we use computer vision techniques to quantify which parts of a painting evoke positive and negative emotions. We also demonstrate how the quantification of evidence for positive and negative emotions can be used to predict which parts of a painting people prefer to focus on. This method opens new opportunities of research on why a specific painting is perceived as emotional at global and local scales. © 2015 ACM.",Abstract paintings; Algorithms; Emotion recognition; Experimentation; Eye tracking; H.3.1 [information search and retrieval]: Content analysis and indexing; I.2 [artificial intelligence]: Vision and scene understanding; J.5 [computer applications]: Arts and humanities-fine arts; Theory; Visual art,Algorithms; Artificial intelligence; Computer vision; Painting; Pattern recognition systems; Abstract paintings; Content analysis; Emotion recognition; Experimentation; Eye-tracking; J.5 [computer applications]: arts and humanities fine arts; Theory; Vision and scene understanding; Visual arts; Behavioral research
Gaze and attention management for embodied conversational agents,2015,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84966769889&doi=10.1145%2f2724731&partnerID=40&md5=80006df8c47836e9b789f610d9ee7bfc,"To facilitate natural interactions between humans and embodied conversational agents (ECAs), we need to endow the latter with the same nonverbal cues that humans use to communicate. Gaze cues in particular are integral in mechanisms for communication and management of attention in social interactions, which can trigger important social and cognitive processes, such as establishment of affiliation between people or learning new information. The fundamental building blocks of gaze behaviors are gaze shifts: coordinated movements of the eyes, head, and body toward objects and information in the environment. In this article, we present a novel computational model for gaze shift synthesis for ECAs that supports parametric control over coordinated eye, head, and upper body movements. We employed the model in three studies with human participants. In the first study, we validated the model by showing that participants are able to interpret the agent's gaze direction accurately. In the second and third studies, we showed that by adjusting the participation of the head and upper body in gaze shifts, we can control the strength of the attention signals conveyed, thereby strengthening or weakening their social and cognitive effects. The second study shows that manipulation of eye-head coordination in gaze enables an agent to convey more information or establish stronger affiliation with participants in a teaching task, while the third study demonstrates how manipulation of upper body coordination enables the agent to communicate increased interest in objects in the environment. © 2015 ACM.",Affiliation; Attention; Body orientation; Embodied conversational agents; Gaze model; Learning,Coordination reactions; Human computer interaction; Affiliation; Attention; Body orientation; Embodied conversational agent; Learning; Eye movements
A wearable assistant for gait training for Parkinson's disease with freezing of gait in out-of-the-lab environments,2015,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84967175464&doi=10.1145%2f2701431&partnerID=40&md5=fcec36a245edb52e5df8933f52b0a930,"People with Parkinson's disease (PD) suffer from decliningmobility capabilities, which cause a prevalent risk of falling. Commonly, short periods of motor blocks occur during walking, known as freezing of gait (FoG). To slow the progressive decline of motor abilities, people with PD usually undertake stationary motor-training exercises in the clinics or supervised by physiotherapists. We present a wearable system for the support of people with PD and FoG. The system is designed for independent use. It enables motor training and gait assistance at home and other unsupervised environments. The system consists of three components. First, FoG episodes are detected in real time using wearable inertial sensors and a smartphone as the processing unit. Second, a feedback mechanism triggers a rhythmic auditory signal to the user to alleviate freeze episodes in an assistive mode. Third, the smartphone-based application features support for training exercises. Moreover, the system allows unobtrusive and long-term monitoring of the user's clinical condition by transmitting sensing data and statistics to a telemedicine service. We investigate the at-home acceptance of the wearable system in a study with nine PD subjects. Participants deployed and used the system on their own, without any clinical support, at their homes during three protocol sessions in 1 week. Users' feedback suggests an overall positive attitude toward adopting and using the system in their daily life, indicating that the system supports them in improving their gait. Further, in a data-driven analysis with sensing data from five participants, we study whether there is an observable effect on the gait during use of the system. In three out of five subjects, we observed a decrease in FoG duration distributions over the protocol days during gait-training exercises. Moreover, sensing data-driven analysis shows a decrease in FoG duration and FoG number in four out of five participants when they use the system as a gait-assistive tool during normal daily life activities at home. © 2015 ACM 2160-6455/2015/03-ART5 $15.00.",Freezing of gait; Gait impairment; On-body sensors; Out-of-the-lab studies; System's acceptance; User centered; Wearable assistant,Diseases; Freezing; Neurodegenerative diseases; Signal encoding; Smartphones; Wearable technology; Freezing of gaits; Gait impairment; On-body; Out-of-the-lab studies; System's acceptance; User-centered; Wearable assistant; Wearable sensors
Exploring the benefits of context in 3D gesture recognition for game-based virtual environments,2015,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84966989719&doi=10.1145%2f2656345&partnerID=40&md5=8e919fbcabd4d3f974b5471adde7a27c,"We present a systematic exploration of how to utilize video game context (e.g., player and environmental state) to modify and augment existing 3D gesture recognizers to improve accuracy for large gesture sets. Specifically, our work develops and evaluates three strategies for incorporating context into 3D gesture recognizers. These strategies include modifying the well-known Rubine linear classifier to handle unsegmented input streams and per-frame retraining using contextual information (CA-Linear); a GPU implementation of dynamic time warping (DTW) that reduces the overhead of traditional DTW by utilizing context to evaluate only relevant time sequences inside of a multithreaded kernel (CA-DTW); and a multiclass SVM with per-class probability estimation that is combined with a contextually based prior probability distribution (CA-SVM). We evaluate each strategy using a Kinect-based third-person perspective VE game prototype that combines parkour-style navigation with hand-to-hand combat. Using a simple gesture collection application to collect a set of 57 gestures and the game prototype that implements 37 of these gestures, we conduct three experiments. In the first experiment, we evaluate the effectiveness of several established classifiers on our gesture set and demonstrate state-of-the-art results using our proposed method. In our second experiment, we generate 500 random scenarios having between 5 and 19 of the 57 gestures in context. We show that the contextually aware classifiers CA-Linear, CA-DTW, and CA-SVM significantly outperform their noncontextually aware counterparts by 37.74%, 36.04%, and 20.81%, respectively. On the basis of the results of the second experiment, we derive upper-bound expectations for in-game performance for the three CA classifiers: 96.61%, 86.79%, and 96.86%, respectively. Finally, our third experiment is an in-game evaluation of the three CA classifiers with and without context. Our results show that through the use of context, we are able to achieve an average in-game recognition accuracy of 89.67% with CA-Linear compared to 65.10% without context, 79.04% for CA-DTW compared to 58.1% without context, and 90.85% with CA-SVM compared to 75.2% without context. © 2015 ACM 2160-6455/2015/03-ART1 $15.00.",3D gesture recognition; Context; Context-aware; Dynamic time warping; Linear classifier; SVM; Video games,Classification (of information); Gesture recognition; Human computer interaction; Interactive computer graphics; Probability distributions; Virtual reality; Context; Context-Aware; Dynamic time warping; Linear classifiers; SVM; Video game; Three dimensional computer graphics
Human tutorial instruction in the raw,2015,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84967157695&doi=10.1145%2f2531920&partnerID=40&md5=f06e2160a4577e25110448a34ef01324,"Humans learn procedures from one another through a variety of methods, such as observing someone do the task, practicing by themselves, reading manuals or textbooks, or getting instruction from a teacher. Some of these methods generate examples that require the learner to generalize appropriately. When procedures are complex, however, it becomes unmanageable to induce the procedures from examples alone. An alternative and very common method for teaching procedures is tutorial instruction, where a teacher describes in general terms what actions to perform and possibly includes explanations of the rationale for the actions. This article provides an overview of the challenges in using human tutorial instruction for teaching procedures to computers. First, procedures can be very complex and can involve many different types of interrelated information, including (1) situating the instruction in the context of relevant objects and their properties, (2) describing the steps involved, (3) specifying the organization of the procedure in terms of relationships among steps and substeps, and (4) conveying control structures. Second, human tutorial instruction is naturally plagued with omissions, oversights, unintentional inconsistencies, errors, and simply poor design. The article presents a survey of work from the literature that highlights the nature of these challenges and illustrates them with numerous examples of instruction in many domains. Major research challenges in this area are highlighted, including the difficulty of the learning task when procedures are complex, the need to overcome omissions and errors in the instruction, the design of a natural user interface to specify procedures, the management of the interaction of a human with a learning system, and the combination of tutorial instruction with other teaching modalities. © 2015 ACM.",End-user programming; Instruction; Intelligent user interfaces; Interactive learning; Natural language interpretation; Procedure learning,Computer programming; Errors; Learning systems; User interfaces; End user programming; Instruction; Intelligent User Interfaces; Interactive learning; Natural languages; Procedure learning; Teaching
"Emotional states associated with music: Classification, prediction of changes, and consideration in recommendation",2015,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84967332968&doi=10.1145%2f2723575&partnerID=40&md5=776b6e30deb692db07d6683305959a89,"We present several interrelated technical and empirical contributions to the problem of emotion-based music recommendation and show how they can be applied in a possible usage scenario. The contributions are (1) a new three-dimensional resonance-arousal-valence model for the representation of emotion expressed in music, together with methods for automatically classifying a piece of music in terms of this model, using robust regression methods applied to musical/acoustic features; (2) methods for predicting a listener's emotional state on the assumption that the emotional state has been determined entirely by a sequence of pieces of music recently listened to, using conditional random fields and taking into account the decay of emotion intensity over time; and (3) a method for selecting a ranked list of pieces of music that match a particular emotional state, using a minimization iteration method. A series of experiments yield information about the validity of our operationalizations of these contributions. Throughout the article, we refer to an illustrative usage scenario in which all of these contributions can be exploited, where it is assumed that (1) a listener's emotional state is being determined entirely by the music that he or she has been listening to and (2) the listener wants to hear additional music that matches his or her current emotional state. The contributions are intended to be useful in a variety of other scenarios as well. © 2015 ACM.",Affective computing; Conditional random fields; Emotional state; Music emotion recognition; Music recommendation; Musical emotion,Random processes; Regression analysis; Affective Computing; Conditional random field; Emotional state; Music emotions; Music recommendation; Musical emotion; Iterative methods
Affectionate interaction with a small humanoid robot capable of recognizing social touch behavior,2015,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84966762433&doi=10.1145%2f2685395&partnerID=40&md5=79fdf732183fdaa54a10c8a5aba367a4,"Activity recognition, involving a capability to recognize people's behavior and its underlying significance, will play a crucial role in facilitating the integration of interactive robotic artifacts into everyday human environments. In particular, social intelligence in recognizing affectionate behavior will offer value by allowing companion robots to bond meaningfully with interacting persons. The current article addresses the issue of designing an affectionate haptic interaction between a person and a companion robot by exploring how a small humanoid robot can behave to elicit affection while recognizing touches. We report on an experiment conducted to gain insight into how people perceive three fundamental interactive strategies in which a robot is either always highly affectionate, appropriately affectionate, or superficially unaffectionate (emphasizing positivity, contingency, and challenge, respectively). Results provide insight into the structure of affectionate interaction between humans and humanoid robots-underlining the importance of an interaction design expressing sincere liking, stability and variation- and suggest the usefulness of novel modalities such as warmth and cold. © 2014 ACM.",Activity recognition; Affectionate touch behavior; Human-robot interaction; Intelligent systems; Small humanoid companion robot,Anthropomorphic robots; Behavioral research; Intelligent robots; Intelligent systems; Machine design; Pattern recognition; Robots; Activity recognition; Affectionate touch behavior; Companion robot; Haptic interactions; Interactive robotics; Interactive strategy; Small humanoid robots; Social intelligence; Human robot interaction
Incremental learning of daily routines as workflows in a smart home environment,2015,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84966925945&doi=10.1145%2f2675063&partnerID=40&md5=89fb7f023c079ae6c830cbc53f0eea6e,"Smart home environments should proactively support users in their activities, anticipating their needs according to their preferences. Understanding what the user is doing in the environment is important for adapting the environment's behavior, as well as for identifying situations that could be problematic for the user. Enabling the environment to exploit models of the user's most common behaviors is an important step toward this objective. In particular, models of the daily routines of a user can be exploited not only for predicting his/her needs, but also for comparing the actual situation at a given moment with the expected one, in order to detect anomalies in his/her behavior. While manually setting up process models in business and factory environments may be cost-effective, building models of the processes involved in people's everyday life is infeasible. This fact fully justifies the interest of the Ambient Intelligence community in automatically learning such models from examples of actual behavior. Incremental adaptation of the models and the ability to express/learn complex conditions on the involved tasks are also desirable. This article describes how process mining can be used for learning users' daily routines from a dataset of annotated sensor data. The solution that we propose relies on a First-Order Logic learning approach. Indeed, First-Order Logic provides a single, comprehensive and powerful framework for supporting all the previously mentioned features. Our experiments, performed both on a proprietary toy dataset and on publicly available real-world ones, indicate that this approach is efficient and effective for learning and modeling daily routines in Smart Home Environments. © 2015 ACM.",Incremental learning; Model of user daily routines; Process mining; Smart home environment,Automation; Cost effectiveness; Data mining; Formal logic; Industrial plants; Intelligent buildings; Ambient intelligence; Daily routines; First order logic; First-order logic learning; Incremental learning; Models of the user; Process mining; Smart homes; Behavioral research
Adaptive gesture recognition with variation estimation for interactive systems,2015,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84967020023&doi=10.1145%2f2643204&partnerID=40&md5=2e3ac0c97d4bfe91fcd2a3abae76f984,"This article presents a gesture recognition/adaptation system for human-computer interaction applications that goes beyond activity classification and that, as a complement to gesture labeling, characterizes the movement execution. We describe a template-based recognition method that simultaneously aligns the input gesture to the templates using a Sequential Monte Carlo inference technique. Contrary to standard template-based methods based on dynamic programming, such as Dynamic Time Warping, the algorithm has an adaptation process that tracks gesture variation in real time. The method continuously updates, during execution of the gesture, the estimated parameters and recognition results, which offers key advantages for continuous human-machine interaction. The technique is evaluated in several different ways: Recognition and early recognition are evaluated on 2D onscreen pen gestures; adaptation is assessed on synthetic data; and both early recognition and adaptation are evaluated in a user study involving 3Dfree-space gestures. The method is robust to noise, and successfully adapts to parameter variation. Moreover, it performs recognition as well as or better than nonadapting offline template-based methods. © 2014 ACM.",Adaptive decoding; Continuous gesture modeling; Gesture analysis; Gesture recognition; Particle filtering; Real time,Dynamic programming; Human computer interaction; Monte Carlo methods; Activity classifications; Adaptive decoding; Gesture analysis; Human machine interaction; Particle Filtering; Real time; Sequential Monte Carlo; Template based methods; Gesture recognition
A stimulus-response framework for robot control,2015,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84967006943&doi=10.1145%2f2677198&partnerID=40&md5=b44e3b47410001163964160b9e94d8ad,"We propose in this article a new approach to robot cognitive control based on a stimulus-response framework that models both a robot's stimuli and the robot's decision to switch tasks in response to or inhibit the stimuli. In an autonomous system, we expect a robot to be able to deal with the whole system of stimuli and to use them to regulate its behavior in real-world applications. The proposed framework contributes to the state of the art of robot planning and high-level control in that it provides a novel perspective on the interaction between robot and environment. Our approach is inspired by Gibson's constructive view of the concept of a stimulus and by the cognitive control paradigm of task switching. We model the robot's response to a stimulus in three stages. We start by defining the stimuli as perceptual functions yielded by the active robot processes and learned via an informed logistic regression. Then we model the stimulus-response relationship by estimating a score matrix that leads to the selection of a single response task for each stimulus, basing the estimation on low-rank matrix factorization. The decision about switching takes into account both an interference cost and a reconfiguration cost. The interference cost weighs the effort of discontinuing the current robot mental state to switch to a new state, whereas the reconfiguration cost weighs the effort of activating the response task. A choice is finally made based on the payoff of switching. Because processes play such a crucial role both in the stimulus model and in the stimulus-response model, and because processes are activated by actions, we address also the process model, which is built on a theory of action. The framework is validated by several experiments that exploit a full implementation on an advanced robotic platform and is compared with two known approaches to replanning. Results demonstrate the practical value of the system in terms of robot autonomy, flexibility, and usability. © 2015 Copyright is held by the owner/author(s). Publication rights licensed to ACM.",Behavior adaptation; Intelligent interaction; Stimulus-response model; Task switching,Behavioral research; Costs; Robots; Switching; Autonomous systems; Behavior adaptations; High level control; Intelligent interactions; Logistic regressions; Reconfiguration costs; Stimulus response; Task switching; Robot programming
Introduction to the special issue on Activity Recognition for Interaction,2015,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84967221275&doi=10.1145%2f2694858&partnerID=40&md5=c0c48e0c7e5e8647c59f1faba800e02a,"This editorial introduction describes the aims and scope of the ACM Transactions on Interactive Intelligent Systems special issue on Activity Recognition for Interaction. It explains why activity recognition is becoming crucial as part of the cycle of interaction between users and computing systems, and it shows how the five articles selected for this special issue reflect this theme. © 2015 ACM.",Activity Recognition; Human-computer interaction,Intelligent systems; Pattern recognition; Activity recognition; Computing system; Interactive intelligent systems; Human computer interaction
Automatic detection of social behavior of museum visitor pairs,2014,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84967211145&doi=10.1145%2f2662869&partnerID=40&md5=518d4c1f154a52f44f02f42becb51032,"In many cases, visitors come to a museum in small groups. In such cases, the visitors'social context has an impact on their museum visit experience. Knowing the social context may allow a system to provide socially aware services to the visitors. Evidence of the social context can be gained from observing/monitoring the visitors'social behavior. However, automatic identification of a social context requires, on the one hand, identifying typical social behavior patterns and, on the other, using relevant sensors that measure various signals and reason about them to detect the visitors'social behavior. We present such typical social behavior patterns of visitor pairs, identified by observations, and then the instrumentation, detection process, reasoning, and analysis of measured signals that enable us to detect the visitors'social behavior. Simple sensors'data, such as proximity to other visitors, proximity to museum points of interest, and visitor orientation are used to detect social synchronization, attention to the social companion, and interest in museum exhibits. The presented approach may allow future research to offer adaptive services to museum visitors based on their social context to support their group visit experience better. © 2014 ACM.",Co-presence; EF-formation; F-formation; Museum visitors'behavior; Social context; Social presence; Social signal processing,
USMART: An unsupervised semantic mining activity recognition technique,2014,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84967048392&doi=10.1145%2f2662870&partnerID=40&md5=83cc87573359a533c9efd0a387e1f67e,"Recognising high-level human activities from low-level sensor data is a crucial driver for pervasive systems that wish to provide seamless and distraction-free support for users engaged in normal activities. Research in this area has grown alongside advances in sensing and communications, and experiments have yielded sensor traces coupled with ground truth annotations about the underlying environmental conditions and user actions. Traditional machine learning has had some success in recognising human activities; but the need for large volumes of annotated data and the danger of overfitting to specific conditions represent challenges in connection with the building of models applicable to a wide range of users, activities, and environments. We present USMART, a novel unsupervised technique that combines data- and knowledge-driven techniques. USMART uses a general ontology model to represent domain knowledge that can be reused across different environments and users, and we augment a range of learning techniques with ontological semantics to facilitate the unsupervised discovery of patterns in how each user performs daily activities. We evaluate our approach against four real-world third-party datasets featuring different user populations and sensor configurations, and we find that USMART achieves up to 97.5% accuracy in recognising daily activities. © 2014 ACM.",Activity recognition; Clustering; Ontologies; Pervasive computing; Segmentation; Sequential pattern; Smart home; String alignment; Unsupervised learning,
Nonstrict hierarchical reinforcement learning for interactive systems and robots,2014,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84966839257&doi=10.1145%2f2659003&partnerID=40&md5=29ef1dc859d53f9cccba9786d1d225d9,"Conversational systems and robots that use reinforcement learning for policy optimization in large domains often face the problem of limited scalability. This problem has been addressed either by using function approximation techniques that estimate the approximate true value function of a policy or by using a hierarchical decomposition of a learning task into subtasks. We present a novel approach for dialogue policy optimization that combines the benefits of both hierarchical control and function approximation and that allows flexible transitions between dialogue subtasks to give human users more control over the dialogue. To this end, each reinforcement learning agent in the hierarchy is extended with a subtask transition function and a dynamic state space to allow flexible switching between subdialogues. In addition, the subtask policies are represented with linear function approximation in order to generalize the decision making to situations unseen in training. Our proposed approach is evaluated in an interactive conversational robot that learns to play quiz games. Experimental results, using simulation and real users, provide evidence that our proposed approach can lead to more flexible (natural) interactions than strict hierarchical control and that it is preferred by human users. © 2014 ACM.",Flexible interaction; Function approximation; Hierarchical control; Human-robot interaction; Interactive robots; Machine learning; Reinforcement learning; Spoken dialogue systems; User simulation,Artificial intelligence; Decision making; Hierarchical systems; Learning systems; Reinforcement learning; Robots; Speech processing; Flexible interaction; Function approximation; Hierarchical control; Interactive robot; Spoken dialogue system; User simulation; Human robot interaction
Efficient interactive multiclass learning from binary feedback,2014,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84967146007&doi=10.1145%2f2629631&partnerID=40&md5=cbbc8ba0d039a327231854718d6779a2,"We introduce a novel algorithm called upper confidence-weighted learning (UCWL) for online multiclass learning from binary feedback (e.g., feedback that indicates whether the prediction was right or wrong). UCWL combines the upper confidence bound (UCB) framework with the soft confidence-weighted (SCW) online learning scheme. In UCB, each instance is classified using both score and uncertainty. For a given instance in the sequence, the algorithm might guess its class label primarily to reduce the class uncertainty. This is a form of informed exploration, which enables the performance to improve with lower sample complexity compared to the case without exploration. Combining UCB with SCW leads to the ability to deal well with noisy and nonseparable data, and state-of-the-art performance is achieved without increasing the computational cost. A potential application setting is human-robot interaction (HRI), where the robot is learning to classify some set of inputs while the human teaches it by providing only binary feedback-or sometimes even the wrong answer entirely. Experimental results in the HRI setting and with two benchmark datasets from other settings show that UCWL outperforms other state-of-the-art algorithms in the online binary feedback setting-and surprisingly even sometimes outperforms state-of-the-art algorithms that get full feedback (e.g., the true class label), whereas UCWL gets only binary feedback on the same data sequence. © 2014 ACM.",Active learning; Contextual multiarmed bandit; Convolutional neural networks; Deep neural networks; Exploration-exploitation; Gesture recognition; Human-robot interaction; Online learning; Partial feedback; Upper confidence bound,Algorithms; Complex networks; E-learning; Feedback; Gesture recognition; Man machine systems; Neural networks; Robots; Social networking (online); Active Learning; Convolutional neural network; Deep neural networks; Exploration exploitations; Multi armed bandit; Online learning; Partial feedback; Upper confidence bound; Human robot interaction
Introduction to the special issue on machine learning for multiple modalities in interactive systems and robots,2014,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84966930447&doi=10.1145%2f2670539&partnerID=40&md5=a4a850c67ad0a3207d5abf9729e48d04,"This special issue highlights research articles that apply machine learning to robots and other systems that interact with users through more than one modality, such as speech, gestures, and vision. For example, a robot may coordinate its speech with its actions, taking into account (audio-)visual feedback during their execution. Machine learning provides interactive systems with opportunities to improve performance not only of individual components but also of the system as a whole. However, machine learning methods that encompass multiple modalities of an interactive system are still relatively hard to find. The articles in this special issue represent examples that contribute to filling this gap. © 2014 ACM.",Extrinsic evaluation; Human-machine interaction; Interactive robots; Interactive systems; Intrinsic evaluation; Machine learning from interaction,Artificial intelligence; Human computer interaction; Human robot interaction; Machine components; Robots; Visual communication; Extrinsic evaluation; Human machine interaction; Interactive robot; Interactive system; Intrinsic evaluation; Learning from interactions; Learning systems
Machine learning for social multiparty human-robot interaction,2014,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84967145625&doi=10.1145%2f2600021&partnerID=40&md5=0e1800a5509c17bb07589e773e077de2,"We describe a variety of machine-learning techniques that are being applied to social multiuser human- robot interaction using a robot bartender in our scenario. We first present a data-driven approach to social state recognition based on supervised learning. We then describe an approach to social skills execution-that is, action selection for generating socially appropriate robot behavior-which is based on reinforcement learning, using a data-driven simulation of multiple users to train execution policies for social skills. Next, we describe how these components for social state recognition and skills execution have been integrated into an end-to-end robot bartender system, and we discuss the results of a user evaluation. Finally, we present an alternative unsupervised learning framework that combines social state recognition and social skills execution based on hierarchical Dirichlet processes and an infinite POMDP interaction manager. The models make use of data from both human-human interactions collected in a number of German bars and human-robot interactions recorded in the evaluation of an initial version of the system. © 2014 ACM.",Machine learning; Multiuser interaction; Social robotics,Artificial intelligence; Human computer interaction; Learning systems; Man machine systems; Reinforcement learning; Robots; State estimation; Data-driven approach; Data-driven simulation; Hierarchical Dirichlet process; Human-human interactions; Interaction managers; Machine learning techniques; Multi-user interaction; Social robotics; Human robot interaction
"Interpreting natural language instructions using language, vision, and behavior",2014,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84967152741&doi=10.1145%2f2629632&partnerID=40&md5=c0f07b4810a7f0b796934e59b857011a,"We define the problem of automatic instruction interpretation as follows. Given a natural language instruction, can we automatically predict what an instruction follower, such as a robot, should do in the environment to follow that instruction? Previous approaches to automatic instruction interpretation have required either extensive domain-dependent rule writing or extensive manually annotated corpora. This article presents a novel approach that leverages a large amount of unannotated, easy-to-collect data from humans interacting in a game-like environment. Our approach uses an automatic annotation phase based on artificial intelligence planning, for which two different annotation strategies are compared: one based on behavioral information and the other based on visibility information. The resulting annotations are used as training data for different automatic classifiers. This algorithm is based on the intuition that the problem of interpreting a situated instruction can be cast as a classification problem of choosing among the actions that are possible in the situation. Classification is done by combining language, vision, and behavior information. Our empirical analysis shows that machine learning classifiers achieve 77% accuracy on this task on available English corpora and 74% on similar German corpora. Finally, the inclusion of human feedback in the interpretation process is shown to boost performance to 92% for the English corpus and 90% for the German corpus. © 2014 ACM.",Action recognition; Multimodal understanding; Natural language interpretation; Situated virtual agent; Unsupervised learning; Visual feedback,Artificial intelligence; Classification (of information); Learning systems; Unsupervised learning; Visual communication; Action recognition; Multi-modal; Natural languages; Virtual agent; Visual feedback; Visual languages
Design and evaluation techniques for authoring interactive and stylistic behaviors,2014,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84983573835&doi=10.1145%2f2499671&partnerID=40&md5=d7d602554a738b1e28a7aec05387df19,"We present a series of projects for end-user authoring of interactive robotic behaviors, with a particular focus on the style of those behaviors: we call this approach Style-by-Demonstration (SBD).We provide an overview introduction of three different SBD platforms: SBD for animated character interactive locomotion paths, SBD for interactive robot locomotion paths, and SBD for interactive robot dance. The primary contribution of this article is a detailed cross-project SBD analysis of the interaction designs and evaluation approaches employed, with the goal of providing general guidelines stemming from our experiences, for both developing and evaluating SBD systems. In addition, we provide the first full account of our Puppet Master SBD algorithm, with an explanation of how it evolved through the projects. © 2014 ACM.",Human-computer interaction; Human-robot interaction; Programming-by-demonstration; StyLe-by-demonstration,Demonstrations; Human computer interaction; Robot learning; Robot programming; Animated characters; Design and evaluations; Evaluation approach; Interaction design; Interactive robotics; Primary contribution; Programming by demon-stration; Style by demonstrations; Human robot interaction
Evaluation of normal model visualization for anomaly detection in maritime traffic,2014,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84983573797&doi=10.1145%2f2591511&partnerID=40&md5=dc6d2a9407963d696ef65d871619e85c,"Monitoring dynamic objects in surveillance applications is normally a demanding activity for operators, not only because of the complexity and high dimensionality of the data but also because of other factors like time constraints and uncertainty. Timely detection of anomalous objects or situations that need further investigation may reduce operators' cognitive load. Surveillance applications may include anomaly detection capabilities, but their use is not widespread, as they usually generate a high number of false alarms, they do not provide appropriate cognitive support for operators, and their outcomes can be difficult to comprehend and trust. Visual analytics can bridge the gap between computational and human approaches to detecting anomalous behavior in traffic data, making this process more transparent. As a step toward this goal of transparency, this article presents an evaluation that assesses whether visualizations of normal behavioral models of vessel traffic support two of the main analytical tasks specified during our field work in maritime control centers. The evaluation combines quantitative and qualitative usability assessments. The quantitative evaluation, which was carried out with a proof-of-concept prototype, reveals that participants who used the visualization of normal behavioral models outperformed the group that did not do so. The qualitative assessment shows that domain experts have a positive attitude toward the provision of automatic support and the visualization of normal behavioral models, as these aids may reduce reaction time and increase trust in and comprehensibility of the system. © 2014 ACM 2160-6455/2014/03-ART5 $ 15.00.",Analytical reasoning; Anomaly detection; Evaluation; Maritime surveillance,Behavioral research; Monitoring; Waterway transportation; Analytical reasoning; Anomaly detection; Evaluation; Maritime surveillance; Number of false alarms; Qualitative assessments; Quantitative evaluation; Surveillance applications; Visualization
Fluid gesture interaction design: Applications of continuous recognition for the design of modern gestural interfaces,2014,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84983580814&doi=10.1145%2f2543921&partnerID=40&md5=d330005bbca0bd5ffee963f371d14539,"This article presentsGesture InteractionDEsigner (GIDE), an innovative application for gesture recognition. Instead of recognizing gestures only after they have been entirely completed, as happens in classic gesture recognition systems,GIDE exploits the full potential of gestural interaction by tracking gestures continuously and synchronously, allowing users to both control the target application moment to moment and also receive immediate and synchronous feedback about system recognition states. By this means, they quickly learn how to interact with the system in order to develop better performances. Furthermore, rather than learning the predefined gestures of others, GIDE allows users to design their own gestures, making interaction more natural and also allowing the applications to be tailored by users' specific needs. We describe our system that demonstrates these new qualities-that combine to provide fluid gesture interaction design-through evaluations with a range of performers and artists. © 2014 ACM.",Continuous and synchronous control; Design and application of gesture interaction systems; Gesture interaction; Meaningful feedback; Personalisation,Design; Gesture recognition; Target tracking; Better performance; Continuous recognition; Gestural interaction; Gestural interfaces; Gesture interaction; Gesture recognition system; Personalisation; Synchronous control; Quality control
Integrated online localization and navigation for people with visual impairments using smart phones,2014,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84983589629&doi=10.1145%2f2499669&partnerID=40&md5=ace3b312748c5d9af01f40c048898413,"Indoor localization and navigation systems for individuals with Visual Impairments (VIs) typically rely upon extensive augmentation of the physical space, significant computational resources, or heavy and expensive sensors; thus, few systems have been implemented on a large scale. This work describes a system able to guide people with VIs through indoor environments using inexpensive sensors, such as accelerometers and compasses, which are available in portable devices like smart phones. The method takes advantage of feedback from the human user, who confirms the presence of landmarks, something that users with VIs already do when navigating in a building. The system calculates the user's location in real time and uses it to provide audio instructions on how to reach the desired destination. Initial early experiments suggested that the accuracy of the localization depends on the type of directions and the availability of an appropriate transition model for the user. A critical parameter for the transition model is the user's step length. Consequently, this work also investigates different schemes for automatically computing the user's step length and reducing the dependence of the approach on the definition of an accurate transition model. In this way, the direction provisionmethod is able to use the localization estimate and adapt to failed executions of paths by the users. Experiments are presented that evaluate the accuracy of the overall integrated system, which is executed online on a smart phone. Both people with VIs and blindfolded sighted people participated in the experiments, which included paths along multiple floors that required the use of stairs and elevators. © 2014 ACM.",Smart phones,Experiments; Navigation systems; Sensors; Computational resources; Indoor environment; Indoor localization and navigations; Integrated systems; On-line localization; Portable device; Transition model; Visual impairment; Smartphones
Triggering effective social support for online groups,2014,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84983572612&doi=10.1145%2f2499672&partnerID=40&md5=94c816aacbaa531834e883f818320bbe,"Conversational agent technology is an emerging paradigm for creating a social environment in online groups that is conducive to effective teamwork. Prior work has demonstrated advantages in terms of learning gains and satisfaction scores when groups learning together online have been supported by conversational agents that employ Balesian social strategies. This prior work raises two important questions that are addressed in this article. The first question is one of generality. Specifically, are the positive effects of the designed support specific to learning contexts? Or are they in evidence in other collaborative task domains as well?We present a study conducted within a collaborative decision-making task where we see that the positive effects of the Balesian social strategies extend to this new context. The second question is whether it is possible to increase the effectiveness of the Balesian social strategies by increasing the context sensitivity with which the social strategies are triggered. To this end, we present technical work that increases the sensitivity of the triggering. Next, we present a user study that demonstrates an improvement in performance of the support agent with the new, more sensitive triggering policy over the baseline approach from prior work. The technical contribution of this article is that we extend priorworkwhere such support agents were modeled using a composition of conversational behaviors integrated within an event-driven framework. Within the present approach, conversation is orchestrated through context-sensitive triggering of the composed behaviors. The core effort involved in applying this approach involves building a set of triggering policies that achieve this orchestration in a time-sensitive and coherent manner. In line with recent developments in data-driven approaches for building dialog systems, we present a novel technique for learning behaviorspecific triggering policies, deploying it as part of our efforts to improve a socially capable conversational tutor agent that supports collaborative learning. © 2014 ACM.",Collaborative learning; Conversational agents; Large-margin learner; Sequence modeling; Social behaviors; Social ratio filtering; Triggering policy,Behavioral research; Collaborative filtering; Collaborative learning; Conversational agents; Large-margin learner; Sequence modeling; Social behavior; Economic and social effects
Introduction to the special issue on interactive computational visual analytics,2014,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84983580828&doi=10.1145%2f2594648&partnerID=40&md5=58d9aaa599f83e10f8bf91cb54f88f95,[No abstract available],Computational methods; Exploratory data analysis; Information visualization; Statistics,
"Inferring visualization task properties, user performance, and user cognitive abilities from eye gaze data",2014,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84983585922&doi=10.1145%2f2633043&partnerID=40&md5=8d0107fb9e40cdd45d19c23180049dee,"Information visualization systems have traditionally followed a one-size-fits-all model, typically ignoring an individual user's needs, abilities, and preferences. However, recent research has indicated that visualization performance could be improved by adapting aspects of the visualization to the individual user. To this end, this article presents research aimed at supporting the design of novel user-adaptive visualization systems. In particular, we discuss results on using information on user eye gaze patterns while interacting with a given visualization to predict properties of the user's visualization task; the user's performance (in terms of predicted task completion time); and the user's individual cognitive abilities, such as perceptual speed, visual working memory, and verbal working memory. We provide a detailed analysis of different eye gaze feature sets, as well as over-time accuracies. We show that these predictions are significantly better than a baseline classifier even during the early stages of visualization usage. These findings are then discussed with a view to designing visualization systems that can adapt to the individual user in real time. © 2014 ACM.",Adaptation; Adaptive information visualization; Eye tracking; Machine learning,Artificial intelligence; Information systems; Learning systems; Visualization; Adaptation; Adaptive information visualizations; Eye-tracking; Information visualization system; Recent researches; Task completion time; Verbal working memory; Visualization system; Data visualization
Experiments with mobile drama in an instrumented museum for inducing conversation in small groups,2014,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84983590557&doi=10.1145%2f2584250&partnerID=40&md5=c5904720a1439209f29fd97caa8dcf75,"Small groups can have a better museum visit when that visit is both a social and an educational occasion. The unmediated discussion that often ensues during a shared cultural experience, especially when it is with a small group whose members already know each other, has been shown by ethnographers to be important for a more enriching experience. We present DRAMATRIC, a mobile presentation system that delivers hourlong dramas to small groups of museum visitors. DRAMATRIC continuously receives sensor data from the museum environment during a museum visit and analyzes group behavior from that data. On the basis of that analysis, DRAMATRIC delivers a series of dynamically coordinated dramatic scenes about exhibits that the group walks near, each designed to stimulate group discussion. Each drama presentation contains small, complementary differences in the narrative content heard by the different members of the group, leveraging the tension/release cycle of narrative to naturally lead visitors to fill in missing pieces in their own drama by interacting with their fellow group members. Using four specific techniques to produce these coordinated narrative variations, we describe two experiments: one in a neutral, nonmobile environment, and the other a controlled experiment with a full-scale drama in an actual museum. The first experiment tests the hypothesis that narrative differences will lead to increased conversation compared to hearing identical narratives, whereas the second experiment tests whether switching from presenting a drama using one technique to using another technique for the subsequent drama will result in increased conversation. The first experiment shows that hearing coordinated narrative variations can in fact lead to significantly increased conversation. The second experiment also serves as a framework for future studies that evaluate strategies for similar adaptive systems. © 2014 ACM 2160-6455/2014/03-ART2 $ 15.00.",Cultural heritage; Evaluation; Wireless sensor networks,Audition; Museums; Wireless sensor networks; Controlled experiment; Cultural heritages; Evaluation; Experiment tests; Group behavior; Group discussions; Museum visitor; Presentation system; Experiments
Task Model-Driven Realization of Interactive Application Functionality through Services,2014,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84983587678&doi=10.1145%2f2559979&partnerID=40&md5=4c620a5425942d43f5aa0968a9b7a840,"The Service-Oriented Computing (SOC) paradigm is currently being adopted by many developers, as it promises the construction of applications through reuse of existing Web Services (WSs). However, current SOC tools produce applications that interact with users in a limited way. This limitation is overcome by model-based Human-Computer Interaction (HCI) approaches that support the development of applications whose functionality is realized with WSs and whose User Interface (UI) is adapted to the user's context. Typically, such approaches do not consider various functional issues, such as the applications' semantics and their syntactic robustness in terms of the WSs selected to implement their functionality and the automation of the service discovery and selection processes. To this end, we propose a model-driven design method for interactive service-based applications that is able to consider the functional issues and their implications for the UI. This method is realized by a semiautomatic environment that can be integrated into current model-based HCI tools to complete the development of interactive service front-ends. The proposed method takes as input an HCI task model, which includes the user's view of the interactive system, and produces a concrete service model that describes how existing services can be combined to realize the application's functionality. To achieve its goal, our method first transforms system tasks into semantic service queries by mapping the task objects onto domain ontology concepts; then it sends each resulting query to a semantic service engine so as to discover the corresponding services. In the end, only one service from those associated with a system task is selected, through the execution of a novel service concretization algorithm that ensures message compatibility between the selected services. © 2014 ACM.",Interactive application design; Service discovery; Service front-ends,Human computer interaction; Semantics; Tools; User interfaces; Web services; Human computer interaction (HCI); Interactive applications; Interactive services; Model driven design; Service discovery; Service front-ends; Service-based applications; Service-oriented computing; Interactive devices
Employing a parametric model for analytic provenance,2014,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84983575825&doi=10.1145%2f2591510&partnerID=40&md5=c259bea606015d7704d60b1181d0229a,"We introduce a propagation-based parametric symbolic model approach to supporting analytic provenance. This approach combines a script language to capture and encode the analytic process and a parametrically controlled symbolic model to represent and reuse the logic of the analysis process. Our approach first appeared in a visual analytics system called CZSaw. Using a script to capture the analyst's interactions at a meaningful system action level allows the creation of a parametrically controlled symbolic model in the form of a Directed Acyclic Graph (DAG). Using the DAG allows propagating changes. Graph nodes correspond to variables in CZSaw scripts, which are results (data and data visualizations) generated from user interactions. The user interacts with variables representing entities or relations to create the next step's results. Graph edges represent dependency relationships among nodes. Any change to a variable triggers the propagation mechanism to update downstream dependent variables and in turn updates data views to reflect the change. The analyst can reuse parts of the analysis process by assigning new values to a node in the graph. We evaluated this symbolic model approach by solving three IEEE VAST Challenge contest problems (from IEEE VAST 2008, 2009, and 2010). In each of these challenges, the analyst first created a symbolic model to explore, understand, analyze, and solve a particular subproblem and then reused the model via its dependency graph propagation mechanism to solve similar subproblems. With the script and model, CZSaw supports the analytic provenance by capturing, encoding, and reusing the analysis process. The analyst can recall the chronological states of the analysis process with the CZSaw script and may interpret the underlying rationale of the analysis with the symbolic model. © 2014 ACM 2160-6455/2014/03-ART6 $ 15.00.",Analytical reasoning; Dependency graph; History; User interaction; Visual scripting,Encoding (symbols); Graph theory; History; Analytical reasoning; Dependency graphs; Dependency relationship; Directed acyclic graph (DAG); Propagation mechanism; User interaction; Visual analytics systems; Visual scripting; Process control
PromotionRank: Ranking and recommending grocery product promotions using personal shopping lists,2014,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84983590410&doi=10.1145%2f2584249&partnerID=40&md5=748474700fee9b2b7256cee6706a0522,"We present PromotionRank, a technique for generating a personalized ranking of grocery product promotions based on the contents of the customer's personal shopping list. PromotionRank consists of four phases. First, information retrieval techniques are used to map shopping list items onto potentially relevant product categories. Second, since customers typically buy more items than what appear on their shopping lists, the set of potentially relevant categories is expanded using collaborative filtering. Third, we calculate a rank score for each category using a statistical interest criterion. Finally, the available promotions are ranked using the newly computed rank scores. To validate the different phases, we consider 12 months of anonymized shopping basket data from a large national supermarket. To demonstrate the effectiveness of PromotionRank, we also present results from two user studies. The first user study was conducted in a controlled setting using shopping lists of different lengths, whereas the second study was conducted within a large national supermarket using real customers and their personal shopping lists. The results of the two studies demonstrate that PromotionRank is able to identify promotions that are considered both relevant and interesting. As part of the second study, we used PromotionRank to identify relevant promotions to advertise and measure the influence of the advertisements on purchases. The results of this evaluation indicate that PromotionRank is also capable of targeting advertisements, improving sales compared to a baseline that selects random advertisements. © 2014 ACM 2160-6455/2014/03-ART1 $ 15.00.",Advertising; Personalization; Ranking; Recommender Systems; Retailing; User study,Marketing; Recommender systems; Retail stores; Personalizations; Product categories; Rank scores; Ranking; Retailing; Shopping lists; User study; Sales
Modeling user preferences in recommender systems: A classification framework for explicit and implicit user feedback,2014,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84983587892&doi=10.1145%2f2512208&partnerID=40&md5=9e387536fe66374adb098ed286b38dc2,"Recommender systems are firmly established as a standard technology for assisting users with their choices; however, little attention has been paid to the application of the user model in recommender systems, particularly the variability and noise that are an intrinsic part of human behavior and activity. To enable recommender systems to suggest items that are useful to a particular user, it can be essential to understand the user and his or her interactions with the system. These interactions typically manifest themselves as explicit and implicit user feedback that provides the key indicators for modeling users' preferences for items and essential information for personalizing recommendations. In this article, we propose a classification framework for the use of explicit and implicit user feedback in recommender systems based on a set of distinct properties that include Cognitive Effort, UserModel, Scale of Measurement, and Domain Relevance.We develop a set of comparison criteria for explicit and implicit user feedback to emphasize the key properties. Using our framework, we provide a classification of recommender systems that have addressed questions about user feedback, and we review state-of-the-art techniques to improve such user feedback and thereby improve the performance of the recommender system. Finally, we formulate challenges for future research on improvement of user feedback. © 2014 ACM.",Explicit feedback; Feedback; Implicit feedback; Improvement of feedback; Recommender systems,Recommender systems; Classification framework; Cognitive efforts; Comparison criterion; Explicit feedback; Human behaviors; Implicit feedback; Standard technology; State-of-the-art techniques; Feedback
Interactive statistics with Illmo,2014,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84983573904&doi=10.1145%2f2509108&partnerID=40&md5=09f0c2f62f270246af945ff42e11739a,"Progress in empirical research relies on adequate statistical analysis and reporting. This article proposes an alternative approach to statistical modeling that is based on an old but mostly forgotten idea, namely Thurstone modeling. Traditional statistical methods assume that either the measured data, in the case of parametric statistics, or the rank-order transformed data, in the case of nonparametric statistics, are samples from a specific (usually Gaussian) distribution with unknown parameters. Consequently, such methods should not be applied when this assumption is not valid. Thurstone modeling similarly assumes the existence of an underlying process that obeys an a priori assumed distribution with unknown parameters, but combines this underlying process with a flexible response mechanism that can be either continuous or discrete and either linear or nonlinear. One important advantage of Thurstone modeling is that traditional statistical methods can still be applied on the underlying process, irrespective of the nature of the measured data itself. Another advantage is that Thurstone models can be graphically represented, which helps to communicate them to a broad audience. A new interactive statistical package, Interactive Log Likelihood MOdeling (Illmo), was specifically designed for estimating and rendering Thurstone models and is intended to bring Thurstone modeling within the reach of persons who are not experts in statistics. Illmo is unique in the sense that it provides not only extensive graphical renderings of the data analysis results but also an interface for navigating between different model options. In this way, users can interactively explore different models and decide on an adequate balance between model complexity and agreement with the experimental data. Hypothesis testing on model parameters is also made intuitive and is supported by both textual and graphical feedback. The flexibility and ease of use of Illmo means that it is also potentially useful as a didactic tool for teaching statistics. © 2014 ACM 2160-6455/2014/03-ART4 $ 15.00.",Maximum likelihood; Multilevel models; Thurstone,Maximum likelihood; Regression analysis; Rendering (computer graphics); Graphical rendering; Multilevel model; Non-parametric statistics; Response mechanisms; Statistical modeling; Statistical packages; Teaching statistics; Thurstone; Statistics
Collaborative language models for localized query prediction,2014,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84983574276&doi=10.1145%2f2622617&partnerID=40&md5=b219abd3d25d4d057d9200ab50878839,"Localized query prediction (LQP) is the task of estimating web query trends for a specific location. This problem subsumes many interesting personalized web applications such as personalization for buzz query detection, for query expansion, and for query recommendation. These personalized applications can greatly enhance user interaction with web search engines by providing more customized information discovered from user input (i.e., queries), but the LQP task has rarely been investigated in the literature. Although exist abundant work on estimating global web search trends does exist, it often encounters the big challenge of data sparsity when personalization comes into play. In this article, we tackle the LQP task by proposing a series of collaborative language models (CLMs). CLMs alleviate the data sparsity issue by collaboratively collecting queries and trend information from the other locations. The traditional statistical language models assume a fixed background language model, which loses the taste of personalization. In contrast, CLMs are personalized language models with flexible background language models customized to various locations. The most sophisticated CLM enables the collaboration to adapt to specific query topics, which further advances the personalization level. An extensive set of experiments have been conducted on a large-scale web query log to demonstrate the effectiveness of the proposed models. © 2014 ACM.",Discriminative models; Generative models; Language models; Query log mining; Trending topic recommendation,Computational linguistics; Data mining; Natural language processing systems; Search engines; User interfaces; Web services; Websites; Discriminative models; Generative model; Language model; Query-log minings; Trending topics; Information retrieval
Regression cube: A technique for multidimensional visual exploration and interactive pattern finding,2014,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84983573776&doi=10.1145%2f2590349&partnerID=40&md5=6433ffbf727a1516ebf7c1334d9202eb,"Scatterplots are commonly used to visualize multidimensional data; however, 2D projections of data offer limited understanding of the high-dimensional interactions between data points.We introduce an interactive 3D extension of scatterplots called the Regression Cube (RC), which augments a 3D scatterplot with three facets on which the correlations between the two variables are revealed by sensitivity lines and sensitivity streamlines. The sensitivity visualization of local regression on the 2D projections provides insights about the shape of the data through its orientation and continuity cues. We also introduce a series of visual operations such as clustering, brushing, and selection supported in RC. By iteratively refining the selection of data points of interest, RC is able to reveal salient local correlation patterns that may otherwise remain hidden with a global analysis.We have demonstrated our system with two examples and a user-oriented evaluation, and we show how RCs enable interactive visual exploration of multidimensional datasets via a variety of classification and information retrieval tasks. A video demo of RC is available. © 2014 ACM 2160-6455/2014/03-ART7 $ 15.00.",,Iterative methods; Regression analysis; Visualization; 2D projections; High-dimensional; Local correlations; Local regression; Multi-dimensional datasets; Multidimensional data; User oriented; Visual exploration; Data visualization
Context-sensitive affect recognition for a robotic game companion,2014,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84983580321&doi=10.1145%2f2622615&partnerID=40&md5=54f92fb79665b7d2794692e706de17b5,"Social perception abilities are among the most important skills necessary for robots to engage humans in natural forms of interaction. Affect-sensitive robots are more likely to be able to establish and maintain believable interactions over extended periods of time. Nevertheless, the integration of affect recognition frameworks in real-time human-robot interaction scenarios is still underexplored. In this article, we propose and evaluate a context-sensitive affect recognition framework for a robotic game companion for children. The robot can automatically detect affective states experienced by children in an interactive chess game scenario. The affect recognition framework is based on the automatic extraction of task features and social interaction-based features. Vision-based indicators of the children's nonverbal behaviour are merged with contextual features related to the game and the interaction and given as input to support vector machines to create a context-sensitive multimodal system for affect recognition. The affect recognition framework is fully integrated in an architecture for adaptive human-robot interaction. Experimental evaluation showed that children's affect can be successfully predicted using a combination of behavioural and contextual data related to the game and the interaction with the robot. It was found that contextual data alone can be used to successfully predict a subset of affective dimensions, such as interest toward the robot. Experiments also showed that engagement with the robot can be predicted using information about the user's valence, interest and anticipatory behaviour. These results provide evidence that social engagement can be modelled as a state consisting of affect and attention components in the context of the interaction. © 2014 ACM.",Affect recognition; Contextual information; Human-robot interaction; Multimodal video corpus; Robot companions,Man machine systems; Robotics; Affect recognition; Automatic extraction; Contextual feature; Contextual information; Experimental evaluation; Multi-modal; Non-verbal behaviours; Robot companion; Human robot interaction
Content-based tag propagation and tensor factorization for personalized item recommendation based on social tagging,2014,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84983580323&doi=10.1145%2f2487164&partnerID=40&md5=84b7737b2de32637469894dad218066a,"In this article, a novel method for personalized item recommendation based on social tagging is presented. The proposed approach comprises a content-based tag propagation method to address the sparsity and ""cold start"" problems, which often occur in social tagging systems and decrease the quality of recommendations. The proposed method exploits (a) the content of items and (b) users' tag assignments through a relevance feedbackmechanism in order to automatically identify the optimal number of content-based and conceptually similar items. The relevance degrees between users, tags, and conceptually similar items are calculated in order to ensure accurate tag propagation and consequently to address the issue of ""learning tag relevance."" Moreover, the ternary relation among users, tags, and items is preserved by performing tag propagation in the form of triplets based on users' personal preferences and ""cold start"" degree. The latent associations among users, tags, and items are revealed based on a tensor factorization model in order to build personalized item recommendations. In our experiments with real-world social data, we show the superiority of the proposed approach over other state-of-the-art methods, since several problems in social tagging systems are successfully tackled. Finally, we present the recommendation methodology in the multimodal engine of I-SEARCH, where users' interaction capabilities are demonstrated. © 2014 ACM.",Content-based information retrieval; Recommender systems; Relevance feedback; Social tagging; Tag propagation; Tensor factorization,Factorization; Recommender systems; Social networking (online); Content-based information retrieval; Relevance feedback; Social tagging; Tag propagation; Tensor factorization; Tensors
Human decision making and recommender systems,2013,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84983588773&doi=10.1145%2f2533670.2533675&partnerID=40&md5=c779357764a6e1e4fbef3037f0653849,"Recommender systems have already proved to be valuable for coping with the information overload problem in several application domains. They provide people with suggestions for items which are likely to be of interest for them; hence, a primary function of recommender systems is to help people make good choices and decisions. However, most previous research has focused on recommendation techniques and algorithms, and less attention has been devoted to the decision making processes adopted by the users and possibly supported by the system. There is still a gap between the importance that the community gives to the assessment of recommendation algorithms and the current range of ongoing research activities concerning human decision making. Different decision-psychological phenomena can influence the decision making of users of recommender systems, and research along these lines is becoming increasingly important and popular. This special issue highlights how the coupling of recommendation algorithms with the understanding of human choice and decision making theory has the potential to benefit research and practice on recommender systems and to enable users to achieve a good balance between decision accuracy and decision effort. © 2013 ACM.",Algorithms; Design; Experimentation; Human Factors,Algorithms; Behavioral research; Decision theory; Design; Human engineering; Recommender systems; Decision accuracies; Decision making process; Decision-making theories; Experimentation; Human decision making; Information overloads; Recommendation algorithms; Recommendation techniques; Decision making
Embodying services into physical places: Toward the design of a mobile environment browser,2013,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84983587684&doi=10.1145%2f2499474.2499477&partnerID=40&md5=28de4900dd118eec987889b6f75cdfd4,"The tremendous developments in mobile computing and handheld devices have allowed for an increasing usage of the resources of the World Wide Web. People today consume information and services on the go, through smart phones applications capable of exploiting their location in order to adapt the content according to the context of use. As location-based services gain traction and reveal their limitations, we argue there is a need for intelligent systems to be created to better support people's activities in their experience of the city, especially regarding their decision-making processes. In this article, we explore the opportunity to move closer to the realization of the ubiquitous computing vision by turning physical places into smart environments capable of cooperatively and autonomously collecting, processing, and transporting information about their characteristics (e.g., practical information, presence of people, and ambience). Following a multidisciplinary approach which leverages psychology, design, and computer science, we propose to investigate the potential of building communication and interaction spaces, called information spheres, on top of physical places such as businesses, homes, and institutions. We argue that, if the latter are exposed on the Web, they can act as a platform delivering information and services and mediating interactions with smart objects without requiring too much effort for the deployment of the architecture. After presenting the inherent challenges of our vision, we go through the protocol of two preliminary experiments that aim to evaluate users' perception of different types of information (i.e., reviews, check-in information, video streams, and real-time representations) and their influence on the decision-making process. Results of this study lead us to elaborate the design considerations that must be taken into account to ensure the intelligibility and user acceptance of information spheres. We finally describe a research prototype application called Environment Browser (Env-B) and present the underlying smart space middleware, before evaluating the user experience with our system through quantitative and qualitative methods. © 2013 ACM.",Interaction design; Location-based services; Mobile browser; Situated services; Smart environments; Smart objects; Ubiquitous computing; User interface design; User study; Web of things,Concrete pavements; Intelligent systems; Location based services; Middleware; Ubiquitous computing; User interfaces; Video streaming; World Wide Web; Interaction design; Mobile Browsers; Situated services; Smart environment; Smart objects; User interface designs; User study; Web of things; Human computer interaction
Phrase detectives: Utilizing collective intelligence for internet-scale language resource creation,2013,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84983578977&doi=10.1145%2f2448116.2448119&partnerID=40&md5=4e3a3760596622597f30e97eee7f57d1,"We are witnessing a paradigm shift in Human Language Technology (HLT) that may well have an impact on the field comparable to the statistical revolution: acquiring large-scale resources by exploiting collective intelligence. An illustration of this new approach is Phrase Detectives, an interactive online game with a purpose for creating anaphorically annotated resources that makes use of a highly distributed population of contributors with different levels of expertise. The purpose of this article is to first of all give an overview of all aspects of Phrase Detectives, from the design of the game and the HLT methods we used to the results we have obtained so far. It furthermore summarizes the lessons that we have learned in developing this game which should help other researchers to design and implement similar games. © 2013 ACM.",Anaphora; Corpus annotation; Games with a purpose; Human language technology; Resource creation; Web cooperation,Anaphora; Corpus annotations; Games with a purpose; Human language technologies; Resource creation; Web cooperation; Internet
Cooperative augmentation of mobile smart objects with projected displays,2013,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84983585709&doi=10.1145%2f2499474.2499476&partnerID=40&md5=2bfe5a53254a6bcc7ed5b70057484d22,"Sensors, processors, and radios can be integrated invisibly into objects to make them smart and sensitive to user interaction, but feedback is often limited to beeps, blinks, or buzzes. We propose to redress this input-output imbalance by augmentation of smart objects with projected displays, that-unlike physical displays-allow seamless integration with the natural appearance of an object. In this article, we investigate how, in a ubiquitous computing world, smart objects can acquire and control a projection. We consider that projectors and cameras are ubiquitous in the environment, and we develop a novel conception and system that enables smart objects to spontaneously associate with projector-camera systems for cooperative augmentation. Projector-camera systems are conceived as generic, supporting standard computer vision methods for different appearance cues, and smart objects provide a model of their appearance for method selection at runtime, as well as sensor observations to constrain the visual detection process. Cooperative detection results in accurate location and pose of the object, which is then tracked for visual augmentation in response to display requests by the smart object. In this article, we define the conceptual framework underlying our approach; report on computer vision experiments that give original insight into natural appearance-based detection of everyday objects; show how object sensing can be used to increase speed and robustness of visual detection; describe and evaluate a fully implemented system; and describe two smart object applications to illustrate the system's cooperative augmentation process and the embodied interactions it enables with smart objects. © 2013 ACM.",Augmented reality; Projector-camera systems; Smart objects; Ubiquitous computing,Augmented reality; Computer vision; Sensors; Ubiquitous computing; Conceptual frameworks; Cooperative augmentation; Cooperative detection; Embodied interaction; Projected displays; Projector-camera system; Seamless integration; Smart objects; Cameras
Characterizing and predicting the multifaceted nature of quality in educational web resources,2013,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84983584983&doi=10.1145%2f2533670.2533673&partnerID=40&md5=00b55d62c9780c3155a531308c6fb7f4,"Efficient learning from Web resources can depend on accurately assessing the quality of each resource. We present a methodology for developing computational models of quality that can assist users in assessing Web resources. The methodology consists of four steps: 1) a meta-analysis of previous studies to decompose quality into high-level dimensions and low-level indicators, 2) an expert study to identify the key low-level indicators of quality in the target domain, 3) human annotation to provide a collection of example resources where the presence or absence of quality indicators has been tagged, and 4) training of a machine learning model to predict quality indicators based on content and link features of Web resources. We find that quality is a multifaceted construct, with different aspects that may be important to different users at different times. We show that machine learning models can predict this multifaceted nature of quality, both in the context of aiding curators as they evaluate resources submitted to digital libraries, and in the context of aiding teachers as they develop online educational resources. Finally, we demonstrate how computational models of quality can be provided as a service, and embedded into applications such as Web search. © 2013 ACM.",Algorithms,Algorithms; Computational methods; Digital libraries; Forecasting; Gages; Learning systems; World Wide Web; Computational model; Educational resource; Efficient learning; Human annotations; Machine learning models; Meta-analysis; Quality indicators; Web resources; Quality control
LiveAction: Automating web task model generation,2013,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84983579950&doi=10.1145%2f2533670.2533672&partnerID=40&md5=bcaf254da755f12734061b3692b92635,"Task automation systems promise to increase human productivity by assisting us with our mundane and difficult tasks. These systems often rely on people to (1) identify the tasks they want automated and (2) specify the procedural steps necessary to accomplish those tasks (i.e., to create task models). However, our interviews with users of a Web task automation system reveal that people find it difficult to identify tasks to automate and most do not even believe they perform repetitive tasks worthy of automation. Furthermore, even when automatable tasks are identified, the well-recognized difficulties of specifying task steps often prevent people from taking advantage of these automation systems. In this research, we analyze realWeb usage data and find that people do in fact repeat behaviors on the Web and that automating these behaviors, regardless of their complexity, would reduce the overall number of actions people need to perform when completing their tasks, potentially saving time. Motivated by these findings, we developed LiveAction, a fully-automated approach to generating task models from Web usage data. LiveAction models can be used to populate the task model repositories required by many automation systems, helping us take advantage of automation in our everyday lives. © 2013 ACM.",Algorithms; Design; Experimentation; Human Factors,Algorithms; Automation; Design; Enterprise resource planning; Human engineering; Automation systems; Experimentation; Human productivity; Live actions; Procedural steps; Repetitive task; Task automation; Task modeling; Behavioral research
Introduction to the special section on internet-scale human problem solving,2013,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84983573789&doi=10.1145%2f2448116.2448117&partnerID=40&md5=8e6882ab33c0f5a5e433f2a0e06c01be,[No abstract available],Internet-scale human problem solving,
Introduction to the special issue on interaction with smart objects,2013,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84983577466&doi=10.1145%2f2499474.2499475&partnerID=40&md5=18229401c13c91930678aff2bf22e871,"Smart objects can be smart because of the information and communication technology that is added to human-made artifacts. It is not, however, the technology itself that makes them smart but rather the way in which the technology is integrated, and their smartness surfaces through how people are able to interact with these objects. Hence, the key challenge for making smart objects successful is to design usable and useful interactions with them. We list five features that can contribute to the smartness of an object, and we discuss how smart objects can help resolve the simplicity-featurism paradox. We conclude by introducing the three articles in this special issue, which dive into various aspects of smart object interaction: Augmenting objects with projection, service-oriented interaction with smart objects via a mobile portal, and an analysis of input-output relations in interaction with tangible smart objects. © 2013 ACM.",Interaction; Smart objects,Artificial intelligence; Information and Communication Technologies; Input-output relations; Interaction; Service Oriented; Smart objects; Information technology
PicoTrans: An intelligent icon-driven interface for cross-lingual communication,2013,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84983590556&doi=10.1145%2f2448116.2448121&partnerID=40&md5=902c0c6a5cc5fb167b1dcfc9ec71949c,"picoTrans is a prototype system that introduces a novel icon-based paradigm for cross-lingual communication on mobile devices. Our approach marries a machine translation system with the popular picture book. Users interact with picoTrans by pointing at pictures as if it were a picture book; the system generates natural language from these icons and the user is able to interact with the icon sequence to refine the meaning of the words that are generated. When users are satisfied that the sentence generated represents what they wish to express, they tap a translate button and picoTrans displays the translation. Structuring the process of communication in this way has many advantages. First, tapping icons is a very natural method of user input on mobile devices; typing is cumbersome and speech input errorful. Second, the sequence of icons which is annotated both with pictures and bilingually with words is meaningful to both users, and it opens up a second channel of communication between them that conveys the gist of what is being expressed. We performed a number of evaluations of picoTrans to determine: its coverage of a corpus of in-domain sentences; the input efficiency in terms of the number of key presses required relative to text entry; and users' overall impressions of using the system compared to using a picture book. Our results show that we are able to cover 74% of the expressions in our test corpus using a 2000-icon set; we believe that this icon set size is realistic for a mobile device. We also found that picoTrans requires fewer key presses than typing the input and that the system is able to predict the correct, intended natural language sentence from the icon sequence most of the time, making user interaction with the icon sequence often unnecessary. In the user evaluation, we found that in general users prefer using picoTrans and are able to communicate more rapidly and expressively. Furthermore, users had more confidence that they were able to communicate effectively using picoTrans. © 2013 ACM.",Machine translation; Mobile devices; User interface,Communication; Computer aided language translation; Presses (machine tools); User interfaces; Cross-lingual communication; Input efficiencies; Machine translation systems; Machine translations; Natural languages; Prototype system; User evaluations; User interaction; Mobile devices
Interacting with social networks of intelligent things and people in the world of gastronomy,2013,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84983574307&doi=10.1145%2f2448116.2448120&partnerID=40&md5=42197655fce0662e52d4c54beb4ac75c,"This article introduces a framework for creating rich augmented environments based on a social web of intelligent things and people. We target outdoor environments, aiming to transform a region into a smart environment that can share its cultural heritage with people, promoting itself and its special qualities. Using the applications developed in the framework, people can interact with things, listen to the stories that these things tell them, and make their own contributions. The things are intelligent in the sense that they aggregate information provided by users and behave in a socially active way. They can autonomously establish social relationships on the basis of their properties and their interaction with users. Hence when a user gets in touch with a thing, she is also introduced to its social network consisting of other things and of users; she can navigate this network to discover and explore the world around the thing itself. Thus the system supports serendipitous navigation in a network of things and people that evolves according to the behavior of users. An innovative interaction model was defined that allows users to interact with objects in a natural, playful way using smartphones without the need for a specially created infrastructure. The framework was instantiated into a suite of applications called WantEat, in which objects from the domain of tourism and gastronomy (such as cheese wheels or bottles of wine) are taken as testimonials of the cultural roots of a region. WantEat includes an application that allows the definition and registration of things, a mobile application that allows users to interact with things, and an application that supports stakeholders in getting feedback about the things that they have registered in the system. WantEat was developed and tested in a real-world context which involved a region and gastronomy-related items from it (such as products, shops, restaurants, and recipes), through an early evaluation with stakeholders and a final evaluation with hundreds of users. © 2013 ACM.",Gastronomy; Mobile intelligent applications; Playful interaction; Preserving cultural heritage and keeping it alive; Real-world testing; Smart objects; Social intelligence; Social networking; Social web of things,Computer applications; Economic and social effects; Historic preservation; Cultural heritages; Gastronomy; Intelligent applications; Playful interactions; Real-world testing; Smart objects; Social intelligence; Social webs; Social networking (online)
Making decisions about privacy: Information disclosure in context-aware recommender systems,2013,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84983590151&doi=10.1145%2f2499670&partnerID=40&md5=1d4a67154361bd0abe87de3edf52b64b,"Recommender systems increasingly use contextual and demographical data as a basis for recommendations. Users, however, often feel uncomfortable providing such information. In a privacy-minded design of recommenders, users are free to decide for themselves what data they want to disclose about themselves. But this decision is often complex and burdensome, because the consequences of disclosing personal information are uncertain or even unknown. Although a number of researchers have tried to analyze and facilitate such information disclosure decisions, their research results are fragmented, and they often do not hold up well across studies. This article describes a unified approach to privacy decision research that describes the cognitive processes involved in users' ""privacy calculus"" in terms of system-related perceptions and experiences that act as mediating factors to information disclosure. The approach is applied in an online experiment with 493 participants using a mock-up of a context-aware recommender system. Analyzing the results with a structural linear model, we demonstrate that personal privacy concerns and disclosure justification messages affect the perception of and experience with a system, which in turn drive information disclosure decisions. Overall, disclosure justification messages do not increase disclosure. Although they are perceived to be valuable, they decrease users' trust and satisfaction. Another result is that manipulating the order of the requests increases the disclosure of items requested early but decreases the disclosure of items requested later. © 2013 ACM.",Design; Experimentation; Human Factors; Measurement; Theory,Calculations; Design; Digital storage; Human engineering; Measurement; Cognitive process; Context-aware recommender systems; Experimentation; Information disclosure; On-line experiments; Personal information; Theory; Unified approach; Recommender systems
Gaze awareness in conversational agents: Estimating a user's conversational engagement from eye gaze,2013,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84983578968&doi=10.1145%2f2499474.2499480&partnerID=40&md5=c721465ffab7c8c682694690440b7739,"In face-to-face conversations, speakers are continuously checking whether the listener is engaged in the conversation, and they change their conversational strategy if the listener is not fully engaged. With the goal of building a conversational agent that can adaptively control conversations, in this study we analyze listener gaze behaviors and develop a method for estimating whether a listener is engaged in the conversation on the basis of these behaviors. First, we conduct a Wizard-of-Oz study to collect information on a user's gaze behaviors. We then investigate how conversational disengagement, as annotated by human judges, correlates with gaze transition, mutual gaze (eye contact) occurrence, gaze duration, and eye movement distance. On the basis of the results of these analyses, we identify useful information for estimating a user's disengagement and establish an engagement estimation method using a decision tree technique. The results of these analyses show that a model using the features of gaze transition, mutual gaze occurrence, gaze duration, and eye movement distance provides the best performance and can estimate the user's conversational engagement accurately. The estimation model is then implemented as a real-time disengagement judgment mechanism and incorporated into a multimodal dialog manager in an animated conversational agent. This agent is designed to estimate the user's conversational engagement and generate probing questions when the user is distracted from the conversation. Finally, we evaluate the engagement-sensitive agent and find that asking probing questions at the proper times has the expected effects on the user's verbal/nonverbal behaviors during communication with the agent. We also find that our agent system improves the user's impression of the agent in terms of its engagement awareness, behavior appropriateness, conversation smoothness, favorability, and intelligence. © 2013 ACM.",Conversational agent; Conversational engagement; Dialog management; Eye-gaze behavior; Wizard-of-oz experiment,Decision trees; Estimation; Eye movements; Conversational agents; Conversational engagement; Dialog management; Eye-gaze; Wizard of Oz; Behavioral research
Gaze and turn-taking behavior in casual conversational interactions,2013,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84983574255&doi=10.1145%2f2499474.2499481&partnerID=40&md5=80adf197b8edb3b817f3833a624c9d57,"Eye gaze is an important means for controlling interaction and coordinating the participants' turns smoothly. We have studied how eye gaze correlates with spoken interaction and especially focused on the combined effect of the speech signal and gazing to predict turn taking possibilities. It is well known that mutual gaze is important in the coordination of turn taking in two-party dialogs, and in this article, we investigate whether this fact also holds for three-party conversations. In group interactions, it may be that different features are used for managing turn taking than in two-party dialogs. We collected casual conversational data and used an eye tracker to systematically observe a participant's gaze in the interactions. By studying the combined effect of speech and gaze on turn taking, we aimed to answer our main questions: How well can eye gaze help in predicting turn taking? What is the role of eye gaze when the speaker holds the turn? Is the role of eye gaze as important in three-party dialogs as in two-party dialogue? We used Support Vector Machines (SVMs) to classify turn taking events with respect to speech and gaze features, so as to estimate how well the features signal a change of the speaker or a continuation of the same speaker. The results confirm the earlier hypothesis that eye gaze significantly helps in predicting the partner's turn taking activity, and we also get supporting evidence for our hypothesis that the speaker is a prominent coordinator of the interaction space. Such a turn taking model could be used in interactive applications to improve the system's conversational performance. © 2013 ACM.",,Computer supported cooperative work; Support vector machines; Combined effect; Conversational interaction; Group interaction; Interactive applications; Mutual gazes; Speech signals; Spoken interaction; Support vector machine (SVMs); Forecasting
An english-language argumentation interface for explanation generation with Markov decision processes in the domain of academic advising,2013,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84983580803&doi=10.1145%2f2513564&partnerID=40&md5=182a640158fc77a270d6477556e94aed,"A Markov Decision Process (MDP) policy presents, for each state, an action, which preferably maximizes the expected utility accrual over time. In this article, we present a novel explanation system for MDP policies. The system interactively generates conversational English-language explanations of the actions suggested by an optimal policy, and does so in real time. We rely on natural language explanations in order to build trust between the user and the explanation system, leveraging existing research in psychology in order to generate salient explanations. Our explanation system is designed for portability between domains and uses a combination of domain-specific and domain-independent techniques. The system automatically extracts implicit knowledge from an MDP model and accompanying policy. This MDP-based explanation system can be ported between applications without additional effort by knowledge engineers or model builders. Our system separates domain-specific data from the explanation logic, allowing for a robust system capable of incremental upgrades. Domain-specific explanations are generated through case-based explanation techniques specific to the domain and a knowledge base of concept mappings used to generate English-language explanations. © 2013 ACM.",Algorithms; Design; Experimentation; Human Factors,Algorithms; Design; Human engineering; Knowledge based systems; Markov processes; Academic advising; Expected utility; Experimentation; Explanation systems; Implicit knowledge; Incremental upgrades; Markov Decision Processes; Natural language explanations; Learning algorithms
Introduction to the special section on eye gaze and conversation,2013,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84983580868&doi=10.1145%2f2499474.2499479&partnerID=40&md5=a2b46dedb4f89f362fc71eaf710a2c9f,"This editorial introduction first explains the origin of this special section. It then outlines how each of the two articles included sheds light on possibilities for conversational dialog systems to use eye gaze as a signal that reflects aspects of participation in the dialog: degree of engagement and turn taking behavior, respectively. © 2013 ACM.",Eye gaze; Intelligent human-machine interaction,Dialog systems; Eye-gaze; Human machine interaction; Special sections; Turn-taking; Artificial intelligence
Rating bias and preference acquisition,2013,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84983579968&doi=10.1145%2f2499673&partnerID=40&md5=b17515e96aa901cbe62fe4a7b0f41794,"Personalized systems and recommender systems exploit implicitly and explicitly provided user information to address the needs and requirements of those using their services. User preference information, often in the form of interaction logs and ratings data, is used to identify similar users, whose opinions are leveraged to inform recommendations or to filter information. In this work we explore a different dimension of information trends in user bias and reasoning learned from ratings provided by users to a recommender system. Our work examines the characteristics of a dataset of 100,000 user ratings on a corpus of recipes, which illustrates stable user bias towards certain features of the recipes (cuisine type, key ingredient, and complexity). We exploit this knowledge to design and evaluate a personalized rating acquisition tool based on active learning, which leverages user biases in order to obtain ratings bearing high-value information and to reduce prediction errors with new users. © 2013 ACM.",Algorithms; Design; Human Factors,Algorithms; Artificial intelligence; Design; Human engineering; Acquisition tools; Active Learning; Prediction errors; Preference information; Rating bias; User information; User rating; Recommender systems
Plan recognition and visualization in exploratory learning environments,2013,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84983573802&doi=10.1145%2f2533670.2533674&partnerID=40&md5=d8c6d34929ddc7106adb9a043ab17626,"Modern pedagogical software is open-ended and flexible, allowing students to solve problems through exploration and trial-and-error. Such exploratory settings provide for a rich educational environment for students, but they challenge teachers to keep track of students' progress and to assess their performance. This article presents techniques for recognizing students' activities in such pedagogical software and visualizing these activities to teachers. It describes a new plan recognition algorithm that uses a recursive grammar that takes into account repetition and interleaving of activities. This algorithm was evaluated empirically using an exploratory environment for teaching chemistry used by thousands of students in several countries. It was always able to correctly infer students' plans when the appropriate grammar was available. We designed two methods for visualizing students' activities for teachers: one that visualizes students' inferred plans, and one that visualizes students' interactions over a timeline. Both of these visualization methods were preferred to and found more helpful than a baseline method which showed a movie of students' interactions. These results demonstrate the benefit of combining novel AI techniques and visualization methods for the purpose of designing collaborative systems that support students in their problem solving and teachers in their understanding of students' performance. © 2013 ACM.",Algorithms; Design,Algorithms; Design; Teaching; Visualization; Baseline methods; Collaborative systems; Educational environment; Exploratory learning; Keep track of; Plan recognition; Plan-recognition algorithms; Visualization method; Students
An internet-scale idea generation system,2013,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84983574295&doi=10.1145%2f2448116.2448118&partnerID=40&md5=871335f452b52a35366bdbd01ec8ec8e,"A method of organizing the crowd to generate ideas is described. It integrates crowds using evolutionary algorithms. The method increases the creativity of ideas across generations, and it works better than greenfield idea generation. Specifically, a design space of internet-scale idea generation systems is defined, and one instance is tested: a crowd idea generation system that uses combination to improve previous designs. The key process of the system is the following: A crowd generates designs, then another crowd combines the designs of the previous crowd. In an experiment with 540 participants, the combined designs were compared to the initial designs and to the designs produced by a greenfield idea generation system. The results show that the sequential combination system produced more creative ideas in the last generation and outperformed the greenfield idea generation system. The design space of crowdsourced idea generation developed here may be used to instantiate systems that can be applied to a wide range of design problems. The work has both pragmatic and theoretical implications: New forms of coordination are now possible, and, using the crowd, it is possible to test existing and emerging theories of coordination and participatory design. Moreover, it may be possible for human designers, organized as a crowd, to codesign with each other and with automated algorithms. © 2013 ACM.",Codesign; Creativity; Crowdsourcing; Evolutionary algorithms,Evolutionary algorithms; Internet; Automated algorithms; Co-designs; Combined design; Creativity; Crowdsourcing; Design problems; Participatory design; Sequential combination; Design
An analysis of input-output relations in interaction with smart tangible objects,2013,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84983574293&doi=10.1145%2f2499474.2499478&partnerID=40&md5=03914edc8f14ecc29a9204fb2078b639,"This article focuses on the conceptual relation between the user's input and a system's output in interaction with smart tangible objects. Understanding this input-output relation (IO relation) is a prerequisite for the design of meaningful interaction. A meaningful IO relation allows the user to know what to do with a system to achieve a certain goal and to evaluate the outcome. The work discussed in this article followed a design research process in which four concepts were developed and prototyped. An evaluation was performed using these prototypes to investigate the effect of highly different IO relations on the user's understanding of the interaction. The evaluation revealed two types of IO relations differing in functionality and the number of mappings between the user and system actions. These two types of relations are described by two IO models that provide an overview of these mappings. Furthermore, they illustrate the role of the user and the influence of the system in the process of understanding the interaction. The analysis of the two types of IO models illustrates the value of understanding IO relations for the design of smart tangible objects. © 2013 ACM.",Design research; Human-computer interaction; Input-output relation; Meaning; Model; Tangible interaction,Artificial intelligence; Human computer interaction; Models; Conceptual relations; Design research; Input-output relations; Meaning; Tangible interaction; Tangible objects; Types of relations; Design
"Common sense reasoning for detection, prevention, and mitigation of cyberbullying",2012,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84983569734&doi=10.1145%2f2362394.2362400&partnerID=40&md5=5e67e02831f67af86bd83222ce502501,"Cyberbullying (harassment on social networks) is widely recognized as a serious social problem, especially for adolescents. It is as much a threat to the viability of online social networks for youth today as spam once was to email in the early days of the Internet. Current work to tackle this problem has involved social and psychological studies on its prevalence as well as its negative effects on adolescents. While true solutions rest on teaching youth to have healthy personal relationships, few have considered innovative design of social network software as a tool for mitigating this problem. Mitigating cyberbullying involves two key components: robust techniques for effective detection and reflective user interfaces that encourage users to reflect upon their behavior and their choices. Spam filters have been successful by applying statistical approaches like Bayesian networks and hidden Markov models. They can, like Googles GMail, aggregate human spam judgments because spam is sent nearly identically to many people. Bullying is more personalized, varied, and contextual. In this work, we present an approach for bullying detection based on state-of-the-art natural language processing and a common sense knowledge base, which permits recognition over a broad spectrum of topics in everyday life. We analyze a more narrow range of particular subject matter associated with bullying (e.g. appearance, intelligence, racial and ethnic slurs, social acceptance, and rejection), and construct BullySpace, a common sense knowledge base that encodes particular knowledge about bullying situations. We then perform joint reasoning with common sense knowledge about a wide range of everyday life topics. We analyze messages using our novel AnalogySpace common sense reasoning technique. We also take into account social network analysis and other factors. We evaluate the model on real-world instances that have been reported by users on Formspring, a social networking website that is popular with teenagers. On the intervention side, we explore a set of reflective user-interaction paradigms with the goal of promoting empathy among social network participants. We propose an ""air traffic control""-like dashboard, which alerts moderators to large-scale outbreaks that appear to be escalating or spreading and helps them prioritize the current deluge of user complaints. For potential victims, we provide educational material that informs them about how to cope with the situation, and connects them with emotional support from others. A user evaluation shows that in-context, targeted, and dynamic help during cyberbullying situations fosters end-user reflection that promotes better coping strategies. © 2012 ACM.",affective computing; artificial intelligence; Common sense reasoning,Air traffic control; Artificial intelligence; Bayesian networks; Computer crime; Hidden Markov models; Knowledge based systems; Natural language processing systems; Social sciences computing; User interfaces; Affective Computing; Commonsense knowledge; Commonsense reasoning; Educational materials; NAtural language processing; On-line social networks; Personal relationships; Statistical approach; Social networking (online)
Attentive documents: Eye tracking as implicit feedback for information retrieval and beyond,2012,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84983573951&doi=10.1145%2f2070719.2070722&partnerID=40&md5=3c3f41c899eb0b174b67356d7e333fb7,"Reading is one of the most frequent activities of knowledge workers. Eye tracking can provide information on what document parts users read, and how they were read. This article aims at generating implicit relevance feedback from eye movements that can be used for information retrieval personalization and further applications. We report the findings from two studies which examine the relation between several eye movement measures and user-perceived relevance of read text passages. The results show that the measures are generally noisy, but after personalizing them we find clear relations between the measures and relevance. In addition, the second study demonstrates the effect of using reading behavior as implicit relevance feedback for personalizing search. The results indicate that gaze-based feedback is very useful and can greatly improve the quality of Web search. The article concludes with an outlook introducing attentive documents keeping track of how users consume them. Based on eye movement feedback, we describe a number of possible applications to make working with documents more effective. © 2012 ACM.",Attentive documents; Eye movement measures; Personalization; Relevance feedback,Feedback; Information retrieval; Attentive documents; Eye-movement measures; Eye-tracking; Implicit feedback; Knowledge workers; Personalizations; Relevance feedback; Web searches; Eye movements
Adaptive persuasive systems: A study of tailored persuasive text messages to reduce snacking,2012,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84983585711&doi=10.1145%2f2209310.2209313&partnerID=40&md5=8a0349210cab6a9f04acb6ec4ddcea64,"This article describes the use of personalized short text messages (SMS) to reduce snacking. First, we describe the development and validation (N = 215) of a questionnaire to measure individual susceptibility to different social influence strategies. To evaluate the external validity of this Susceptibility to Persuasion Scale (STPS) we set up a two week text-messaging intervention that used text messages implementing social influence strategies as prompts to reduce snacking behavior. In this experiment (N = 73) we show that messages that are personalized (tailored) to the individual based on their scores on the STPS, lead to a higher decrease in snacking consumption than randomized messages or messages that are not tailored (contra-tailored) to the individual. We discuss the importance of this finding for the design of persuasive systems and detail how designers can use tailoring at the level of social influence strategies to increase the effects of their persuasive technologies. © 2012 ACM.",personalization; Persuasion; persuasion profiling; social influence; tailoring,Message passing; Technology; Telephone systems; Text messaging; Personalizations; Persuasion; persuasion profiling; Social influence; tailoring; Economic and social effects
"People, sensors, decisions: Customizable and adaptive technologies for assistance in healthcare",2012,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84983574265&doi=10.1145%2f2395123.2395125&partnerID=40&md5=ed5fbcc548500c84fe02bb940dd0abdc,"The ratio of healthcare professionals to care recipients is dropping at an alarming rate, particularly for the older population. It is estimated that the number of persons with Alzheimers disease, for example, will top 100 million worldwide by the year 2050 [Alzheimers Disease International 2009]. It will become harder and harder to provide needed health services to this population of older adults. Further, patients are becoming more aware and involved in their own healthcare decisions. This is creating a void in which technology has an increasingly important role to play as a tool to connect providers with recipients. Examples of interactive technologies range from telecare for remote regions to computer games promoting fitness in the home. Currently, such technologies are developed for specific applications and are difficult to modify to suit individual user needs. The future potential economic and social impact of technology in the healthcare field therefore lies in our ability to make intelligent devices that are customizable by healthcare professionals and their clients, that are adaptive to users over time, and that generalize across tasks and environments. A wide application area for technology in healthcare is for assistance and monitoring in the home. As the population ages, it becomes increasingly dependent on chronic healthcare, such as assistance for tasks of everyday life (washing, cooking, dressing), medication taking, nutrition, and fitness. This article will present a summary of work over the past decade on the development of intelligent systems that provide assistance to persons with cognitive disabilities. These systems are unique in that they are all built using a common framework, a decision-theoretic model for general-purpose assistance in the home. In this article, we will show how this type of general model can be applied to a range of assistance tasks, including prompting for activities of daily living, assistance for art therapists, and stroke rehabilitation. This model is a Partially Observable Markov Decision Process (POMDP) that can be customized by end-users, that can integrate complex sensor information, and that can adapt over time. These three characteristics of the POMDP model will allow for increasing uptake and long-term efficiency and robustness of technology for assistance. © 2012 ACM.",,Economic and social effects; Health care; Intelligent systems; Neurodegenerative diseases; Sensors; Technology; Thermal processing (foods); Activities of Daily Living; Cognitive disability; Health care professionals; Health-care decisions; Interactive technology; Partially observable Markov decision process; Social impact-of-technology; Stroke rehabilitation; Handicapped persons
Influencing individually: Fusing personalization and persuasion,2012,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84983585926&doi=10.1145%2f2209310.2209312&partnerID=40&md5=e0460d420cc98a1d7d41c1089f6a2307,"Personalized technologies aim to enhance user experience by taking into account users interests, preferences, and other relevant information. Persuasive technologies aim to modify user attitudes, intentions, or behavior through computer-human dialogue and social influence. While both personalized and persuasive technologies influence user interaction and behavior, we posit that this influence could be significantly increased if the two technologies were combined to create personalized and persuasive systems. For example, the persuasive power of a one-size-fits-all persuasive intervention could be enhanced by considering the users being influenced and their susceptibility to the persuasion being offered. Likewise, personalized technologies could cash in on increased success, in terms of user satisfaction, revenue, and user experience, if their services used persuasive techniques. Hence, the coupling of personalization and persuasion has the potential to enhance the impact of both technologies. This new, developing area clearly offers mutual benefits to both research areas, as we illustrate in this special issue. © 2012 ACM.",Personalization; persuasion,Personalizations; persuasion; Persuasive technology; Relevant informations; Social influence; User experience; User interaction; User satisfaction; Technology
Gliding and saccadic gaze gesture recognition in real time,2012,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84983585931&doi=10.1145%2f2070719.2070723&partnerID=40&md5=14f5f9db7f03ce9e03f2a193a190933d,"Eye movements can be consciously controlled by humans to the extent of performing sequences of predefined movement patterns, or 'gaze gestures'. Gaze gestures can be tracked noninvasively employing a video-based eye tracking system. Gaze gestures hold the potential to become an emerging input paradigm in the context of human-computer interaction (HCI) as low-cost eye trackers become more ubiquitous. The viability of gaze gestures as an innovative way to control a computer rests on how easily they can be assimilated by potential users and also on the ability of machine learning algorithms to discriminate in real time intentional gaze gestures from typical gaze activity performed during standard interaction with electronic devices. In this work, through a set of experiments and user studies, we evaluate the performance of two different gaze gestures modalities, gliding gaze gestures and saccadic gaze gestures, and their corresponding real-time recognition algorithms, Hierarchical Temporal Memory networks and the Needleman-Wunsch algorithm for sequence alignment. Our results show that a specific combination of gaze gesture modality, namely saccadic gaze gestures, and recognition algorithm, Needleman-Wunsch, allows for reliable usage of intentional gaze gestures to interact with a computer with accuracy rates higher than 95% and completion speeds of around 1.5 to 2.5 seconds per gesture. The optimal gaze gesture modality and recognition algorithm do not interfere with otherwise standard human-computer gaze interaction, generating very few false positives during real time recognition and positive feedback from the users. These encouraging results and the low cost eye tracking equipment used, open up a new HCI paradigm for the fields of accessibility and interaction with smartphones, tablets, projected displays and traditional desktop computers. © 2012 ACM.",Dynamic programming; Gaze gestures; Gaze tracking; Hierarchical temporal memory,Dynamic programming; Eye movements; Gesture recognition; Human computer interaction; Personal computers; Tracking (position); Gaze gestures; Gaze tracking; Human computer interaction (HCI); Low cost eye tracking; Needleman-Wunsch algorithm; Real time recognition; Recognition algorithm; Temporal memory; Learning algorithms
Taming Mona Lisa: Communicating gaze faithfully in 2D and 3D facial projections,2012,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84983580854&doi=10.1145%2f2070719.2070724&partnerID=40&md5=d4430a3b3f1ce90855f1514fb33a11e7,"The perception of gaze plays a crucial role in human-human interaction. Gaze has been shown to matter for a number of aspects of communication and dialogue, especially for managing the flow of the dialogue and participant attention, for deictic referencing, and for the communication of attitude. When developing embodied conversational agents (ECAs) and talking heads, modeling and delivering accurate gaze targets is crucial. Traditionally, systems communicating through talking heads have been displayed to the human conversant using 2D displays, such as flat monitors. This approach introduces severe limitations for an accurate communication of gaze since 2D displays are associated with several powerful effects and illusions, most importantly the Mona Lisa gaze effect, where the gaze of the projected head appears to follow the observer regardless of viewing angle. We describe the Mona Lisa gaze effect and its consequences in the interaction loop, and propose a new approach for displaying talking heads using a 3D projection surface (a physical model of a human head) as an alternative to the traditional flat surface projection. We investigate and compare the accuracy of the perception of gaze direction and the Mona Lisa gaze effect in 2D and 3D projection surfaces in a five subject gaze perception experiment. The experiment confirms that a 3Dprojection surface completely eliminates the Mona Lisa gaze effect and delivers very accurate gaze direction that is independent of the observer's viewing angle. Based on the data collected in this experiment, we rephrase the formulation of the Mona Lisa gaze effect. The data, when reinterpreted, confirms the predictions of the new model for both 2D and 3D projection surfaces. Finally, we discuss the requirements on different spatially interactive systems in terms of gaze direction, and propose new applications and experiments for interaction in a human-ECA and a human-robot settings made possible by this technology. © 2012 ACM.",3D projected avatars; Embodied conversational agents; Gaze perception; Mona Lisa gaze effect; Multiparty dialogue; Robot head; Situated interaction,Communication; Display devices; Experiments; Gesture recognition; Human computer interaction; Three dimensional; Two dimensional; User interfaces; 3D projected avatars; Embodied conversational agent; Mona Lisa gaze effect; Multi-party dialogues; Robot head; Situated interactions; Three dimensional computer graphics
Conversational gaze mechanisms for humanlike robots,2012,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84983577450&doi=10.1145%2f2070719.2070725&partnerID=40&md5=380e1ed1b521d6cfb56925ad85651e6b,"During conversations, speakers employ a number of verbal and nonverbal mechanisms to establish who participates in the conversation, when, and in what capacity. Gaze cues and mechanisms are particularly instrumental in establishing the participant roles of interlocutors, managing speaker turns, and signaling discourse structure. If humanlike robots are to have fluent conversations with people, they will need to use these gaze mechanisms effectively. The current work investigates people's use of key conversational gaze mechanisms, how they might be designed for and implemented in humanlike robots, and whether these signals effectively shape human-robot conversations. We focus particularly on whether humanlike gaze mechanisms might help robots signal different participant roles, manage turn-exchanges, and shape how interlocutors perceive the robot and the conversation. The evaluation of these mechanisms involved 36 trials of three-party human-robot conversations. In these trials, the robot used gaze mechanisms to signal to its conversational partners their roles either of two addressees, an addressee and a bystander, or an addressee and a nonparticipant. Results showed that participants conformed to these intended roles 97% of the time. Their conversational roles affected their rapport with the robot, feelings of groupness with their conversational partners, and attention to the task. © 2012 ACM.",Conversation; Conversational roles; Discourse structure; Gaze; Human-robot interaction; Humanlike robots; Turntaking,Conversation; Conversational roles; Discourse structure; Gaze; Humanlike robots; Turn-taking; Robots
System personality and persuasion in human-computer dialogue,2012,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84983577451&doi=10.1145%2f2209310.2209315&partnerID=40&md5=0378dff2240fa5e60b465c114fb05074,"The human-computer dialogue research field has been studying interaction with computers since the early stage of Artificial Intelligence, however, research has often focused on very practical tasks to be completed with the dialogues. A new trend in the field tries to implement persuasive techniques with automated interactive agents; unlike booking a train ticket, for example, such dialogues require the system to show more anthropomorphic qualities. The influences of such qualities in the effectiveness of persuasive dialogue is only starting to be studied. In this article we focus on one important perceived trait of the system: personality, and explore how it influences the persuasiveness of a dialogue system. We introduce a new persuasive dialogue system and combine it with a state of the art personality utterance generator. By doing so, we can control the systems extraversion personality trait and observe its influence on the users perception of the dialogue and its output. In particular, we observe that the users extraversion influences their perception of the dialogue and its persuasiveness, and that the perceived personality of the system can affect its trustworthiness and persuasiveness. We believe that theses observations will help to set up guidelines to tailor dialogue systems to the users interaction expectations and improve the persuasive interventions. © 2012 ACM.",big five; computers are social agents; conversational agent; dialogue management; evaluation; extraversion; Human-computer dialogue; personality; persuasion,Artificial intelligence; Speech processing; Big five; Conversational agents; Dialogue management; evaluation; extraversion; Human-computer dialogues; personality; persuasion; Social agents; Human computer interaction
Adaptive eye gaze patterns in interactions with human and artificial agents,2012,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84983577454&doi=10.1145%2f2070719.2070726&partnerID=40&md5=cf779123fdfc10da1ff163541b47e59e,"Efficient collaborations between interacting agents, be they humans, virtual or embodied agents, require mutual recognition of the goal, appropriate sequencing and coordination of each agent's behavior with others, and making predictions from and about the likely behavior of others. Moment-by-moment eye gaze plays an important role in such interaction and collaboration. In light of this, we used a novel experimental paradigm to systematically investigate gaze patterns in both human-human and human-agent interactions. Participants in the study were asked to interact with either another human or an embodied agent in a joint attention task. Fine-grained multimodal behavioral data were recorded including eye movement data, speech, first-person view video, which were then analyzed to discover various behavioral patterns. Those patterns show that human participants are highly sensitive to momentary multimodal behaviors generated by the social partner (either another human or an artificial agent) and they rapidly adapt their gaze behaviors accordingly. Our results from this data-driven approach provide new findings for understanding microbehaviors in human-human communication which will be critical for the design of artificial agents that can generate human-like gaze behaviors and engage in multimodal interactions with humans. © 2012 ACM.",Gaze-based interaction; Human-robot interaction; Multimodal interface,Digital storage; Eye movements; Interactive computer systems; Behavioral patterns; Data-driven approach; Gaze-based interaction; Human-agent interaction; Human-human communication; Interacting agents; Multi-Modal Interactions; Multi-modal interfaces; Virtual reality
Gaze guidance reduces the number of collisions with pedestrians in a driving simulator,2012,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84983575835&doi=10.1145%2f2070719.2070721&partnerID=40&md5=5ab13a00ec2b1bc9d3b55b3d91cccbd4,"Our study explores the potential of gaze guidance in driving and analyzes eye movements and driving behavior in safety-critical situations.We collected eye movements from subjects instructed to drive predetermined routes in a driving simulator. While driving, the subjects performed various cognitive tasks designed to divert their attention away from the road. The 30 subjects were equally divided in two groups, a control and a gaze guidance group. For the latter, potentially dangerous events, such as a pedestrian suddenly crossing the street, were highlighted with temporally transient gaze-contingent cues, which were triggered if the subject did not look at the pedestrian. For the group that drove with gaze guidance, eye movements have a reduced variability after the gaze-capturing event and shorter reaction times to it. More importantly, gaze guidance leads to a safer driving behavior and a significantly reduced number of collisions. © 2012 ACM.",Driver assistance systems; Driving; Driving simulator; Gaze guidance; Gaze-contingent display,Automobile simulators; Driver assistance system; Driving; Driving simulator; Gaze guidances; Gaze-contingent displays; Eye movements
AutoTutor and affective autotutor: Learning by talking with cognitively and emotionally intelligent computers that talk back,2012,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84983575319&doi=10.1145%2f2395123.2395128&partnerID=40&md5=ab83effcd041a36d8fdb8e0c87fdb684,"We present AutoTutor and Affective AutoTutor as examples of innovative 21st century interactive intelligent systems that promote learning and engagement. AutoTutor is an intelligent tutoring system that helps students compose explanations of difficult concepts in Newtonian physics and enhances computer literacy and critical thinking by interacting with them in natural language with adaptive dialog moves similar to those of human tutors. AutoTutor constructs a cognitive model of students knowledge levels by analyzing the text of their typed or spoken responses to its questions. The model is used to dynamically tailor the interaction toward individual students zones of proximal development. Affective AutoTutor takes the individualized instruction and human-like interactivity to a new level by automatically detecting and responding to students emotional states in addition to their cognitive states. Over 20 controlled experiments comparing AutoTutor with ecological and experimental controls such reading a textbook have consistently yielded learning improvements of approximately one letter grade after brief 3060-minute interactions. Furthermore, Affective AutoTutor shows even more dramatic improvements in learning than the original AutoTutor system, particularly for struggling students with low domain knowledge. In addition to providing a detailed description of the implementation and evaluation of AutoTutor and Affective AutoTutor, we also discuss new and exciting technologies motivated by AutoTutor such as AutoTutor-Lite, Operation ARIES, GuruTutor, DeepTutor, MetaTutor, and AutoMentor.We conclude this article with our vision for future work on interactive and engaging intelligent tutoring systems. © 2012 ACM.",Affect detection and synthesis; Affective computing; Constructivism; Deep learning; Intelligent tutoring systems; Natural language understanding; Student modeling; User modeling,Computer aided instruction; Computer science; Education computing; Intelligent systems; Affect detection; Affective Computing; Constructivism; Deep learning; Intelligent tutoring system; Natural language understanding; Student Modeling; User Modeling; Students
Investigating the persuasion potential of recommender systems from a quality perspective: An empirical study,2012,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84983573819&doi=10.1145%2f2209310.2209314&partnerID=40&md5=d2e2d3b41c21edcbde8228e05a0c8fe4,"Recommender Systems (RSs) help users search large amounts of digital contents and services by allowing them to identify the items that are likely to be more attractive or useful. RSs play an important persuasion role, as they can potentially augment the users trust towards in an application and orient their decisions or actions towards specific directions. This article explores the persuasiveness of RSs, presenting two vast empirical studies that address a number of research questions. First, we investigate if a design property of RSs, defined by the statistically measured quality of algorithms, is a reliable predictor of their potential for persuasion. This factor is measured in terms of perceived quality, defined by the overall satisfaction, as well as by how users judge the accuracy and novelty of recommendations. For our purposes, we designed an empirical study involving 210 subjects and implemented seven full-sized versions of a commercial RS, each one using the same interface and dataset (a subset of Netflix), but each with a different recommender algorithm. In each experimental configuration we computed the statistical quality (recall and F-measures) and collected data regarding the quality perceived by 30 users. The results show us that algorithmic attributes are less crucial than we might expect in determining the users perception of an RSs quality, and suggest that the users judgment and attitude towards a recommender are likely to be more affected by factors related to the user experience. Second, we explore the persuasiveness of RSs in the context of large interactive TV services. We report a study aimed at assessing whether measurable persuasion effects (e.g., changes of shopping behavior) can be achieved through the introduction of a recommender. Our data, collected for more than one year, allow us to conclude that, (1) the adoption of an RS can affect both the lift factor and the conversion rate, determining an increased volume of sales and influencing the users decision to actually buy one of the recommended products, (2) the introduction of an RS tends to diversify purchases and orient users towards less obvious choices (the long tail), and (3) the perceived novelty of recommendations is likely to be more influential than their perceived accuracy. Overall, the results of these studies improve our understanding of the persuasion phenomena induced by RSs, and have implications that can be of interest to academic scholars, designers, and adopters of this class of systems. © 2012 ACM.",accuracy; conversion rate; design; empirical study; F-measure; fallout; iTV; lift factor; long tail; novelty; perceived quality; persuasion; quality evaluation; recall; recommender algorithm; Recommender Systems; satisfaction,Algorithms; Design; Fallout; accuracy; Conversion rates; Empirical studies; F-measure; iTV; Long tail; novelty; Perceived quality; persuasion; Quality evaluation; recall; Recommender algorithms; satisfaction; Recommender systems
Introduction to the special issue on eye gaze in intelligent human-machine interaction,2012,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84983573775&doi=10.1145%2f2070719.2070720&partnerID=40&md5=9b1588f136a882fcb4720b553ede33b1,"Given the recent advances in eye tracking technology and the availability of nonintrusive and highperformance eye tracking devices, there has never been a better time to explore new opportunities to incorporate eye gaze in intelligent and natural human-machine communication. In this special issue, we present six articles that cover various aspects of eye gaze in human-machine interaction, including applications of gaze tracking in human-machine interaction, techniques that recognize gaze gestures and render gaze behaviors, and the analysis of gaze behaviors in social interactions. © 2012 ACM.",Eye gaze; Intelligent human-machine interaction,Eye tracking devices; Eye tracking technologies; Eye-gaze; Gaze tracking; Highperformance; Human machine interaction; Human-machine communication; Social interactions; Man machine systems
"Creating personalized systems that people can scrutinize and control: Drivers, principles and experience",2012,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84983587906&doi=10.1145%2f2395123.2395129&partnerID=40&md5=efa66933f365f1428a84bacee953af1a,"Widespread personalized computing systems play an already important and fast-growing role in diverse contexts, such as location-based services, recommenders, commercial Web-based services, and teaching systems. The personalization in these systems is driven by information about the user, a user model. Moreover, as computers become both ubiquitous and pervasive, personalization operates across the many devices and information stores that constitute the users personal digital ecosystem. This enables personalization, and the user models driving it, to play an increasing role in peoples everyday lives. This makes it critical to establish ways to address key problems of personalization related to privacy, invisibility of personalization, errors in user models, wasted user models, and the broad issue of enabling people to control their user models and associated personalization. We offer scrutable user models as a foundation for tackling these problems. This article argues the importance of scrutable user modeling and personalization, illustrating key elements in case studies from our work. We then identify the broad roles for scrutable user models. The article describes how to tackle the technical and interface challenges of designing and building scrutable user modeling systems, presenting design principles and showing how they were established over our twenty years of work on the Personis software framework. Our contributions are the set of principles for scrutable personalization linked to our experience from creating and evaluating frameworks and associated applications built upon them. These constitute a general approach to tackling problems of personalization by enabling users to scrutinize their user models as a basis for understanding and controlling personalization. © 2012 ACM.",Augmented cognition; Inconsistency; Open learner modeling; Personalization; Reasoning under uncertainty; Scrutability; Understandability; User control; User model; User modeling,Computer programming; Digital devices; Location based services; Personal computers; Augmented cognition; Inconsistency; Open learner modeling; Personalizations; Reasoning under uncertainty; Scrutability; Understandability; User control; User Modeling; User models; Mathematical models
Capturing common knowledge about tasks: Intelligent assistance for to-do lists,2012,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84983568904&doi=10.1145%2f2362394.2362397&partnerID=40&md5=6157a2c5f02ee14be80252096a105c3a,"Although to-do lists are a ubiquitous form of personal task management, there has been no work on intelligent assistance to automate, elaborate, or coordinate a users to-dos. Our research focuses on three aspects of intelligent assistance for to-dos. We investigated the use of intelligent agents to automate to-dos in an office setting. We collected a large corpus from users and developed a paraphrase-based approach to matching agent capabilities with to-dos. We also investigated to-dos for personal tasks and the kinds of assistance that can be offered to users by elaborating on them on the basis of substep knowledge extracted from the Web. Finally, we explored coordination of user tasks with other users through a to-do management application deployed in a popular social networking site. We discuss the emergence of Social Task Networks, which link users tasks to their social network as well as to relevant resources on the Web. We show the benefits of using common sense knowledge to interpret and elaborate to-dos. Conversely, we also show that to-do lists are a valuable way to create repositories of common sense knowledge about tasks. © 2012 ACM.",intelligent assistance; Intelligent user interfaces; knowledge collection from web volunteers; natural language interpretation; office assistants; personal information management; to-do lists,Information management; Intelligent agents; User interfaces; Intelligent assistances; Intelligent User Interfaces; knowledge collection from web volunteers; Natural languages; office assistants; Personal information management; to-do lists; Social networking (online)
Multimodal behavior and interaction as indicators of cognitive load,2012,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84983587904&doi=10.1145%2f2395123.2395127&partnerID=40&md5=db9557fa5cd93b88a4b811e9b858a36c,"High cognitive load arises from complex time and safety-critical tasks, for example, mapping out flight paths, monitoring traffic, or even managing nuclear reactors, causing stress, errors, and lowered performance. Over the last five years, our research has focused on using the multimodal interaction paradigm to detect fluctuations in cognitive load in user behavior during system interaction. Cognitive load variations have been found to impact interactive behavior: by monitoring variations in specific modal input features executed in tasks of varying complexity, we gain an understanding of the communicative changes that occur when cognitive load is high. So far, we have identified specific changes in: speech, namely acoustic, prosodic, and linguistic changes; interactive gesture; and digital pen input, both interactive and freeform. As ground-truth measurements, galvanic skin response, subjective, and performance ratings have been used to verify task complexity. The data suggest that it is feasible to use features extracted from behavioral changes in multiple modal inputs as indices of cognitive load. The speech-based indicators of load, based on data collected from user studies in a variety of domains, have shown considerable promise. Scenarios include single-user and team-based tasks; think-aloud and interactive speech; and single-word, reading, and conversational speech, among others. Pen-based cognitive load indices have also been tested with some success, specifically with pen-gesture, handwriting, and freeform pen input, including diagraming. After examining some of the properties of these measurements, we present a multimodal fusion model, which is illustrated with quantitative examples from a case study. The feasibility of employing user input and behavior patterns as indices of cognitive load is supported by experimental evidence. Moreover, symptomatic cues of cognitive load derived from user behavior such as acoustic speech signals, transcribed text, digital pen trajectories of handwriting, and shapes pen, can be supported by well-established theoretical frameworks, including ODonnell and Eggemeiers workload measurement [1986] Swellers Cognitive Load Theory [Chandler and Sweller 1991], and Baddeleys model of modal working memory [1992] as well as McKinstry et al.s [2008] and Rosenbaums [2005] action dynamics work. The benefit of using this approach to determine the users cognitive load in real time is that the data can be collected implicitly that is, during day-to-day use of intelligent interactive systems, thus overcomes problems of intrusiveness and increases applicability in real-world environments, while adapting information selection and presentation in a dynamic computer interface with reference to load. © 2012 ACM.",Assessment; Cognitive load; Multimodal; Pen input,Behavioral research; Electrophysiology; Flight paths; Nuclear reactors; Assessment; Cognitive loads; Galvanic skin response; Intelligent interactive systems; Multi-modal; Multi-Modal Interactions; Pen input; Real world environments; Cognitive systems
Introduction to the special issue on highlights of the decade in interactive intelligent systems,2012,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84983577435&doi=10.1145%2f2395123.2395124&partnerID=40&md5=bf4224bd038245b1cc476bcb6bc26444,[No abstract available],,
Affect recognition based on physiological changes during the watching of music videos,2012,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84983569154&doi=10.1145%2f2133366.2133373&partnerID=40&md5=7421872bc00dcf4b308be9836588a09b,"Assessing emotional states of users evoked during their multimedia consumption has received a great deal of attention with recent advances in multimedia content distribution technologies and increasing interest in personalized content delivery. Physiological signals such as the electroencephalogram (EEG) and peripheral physiological signals have been less considered for emotion recognition in comparison to other modalities such as facial expression and speech, although they have a potential interest as alternative or supplementary channels. This article presents our work on: (1) constructing a dataset containing EEG and peripheral physiological signals acquired during presentation of music video clips, which ismade publicly available, and (2) conducting binary classification of induced positive/negative valence, high/low arousal, and like/dislike by using the aforementioned signals. The procedure for the dataset acquisition, including stimuli selection, signal acquisition, self-assessment, and signal processing is described in detail. Especially, we propose a novel asymmetry index based on relative wavelet entropy for measuring the asymmetry in the energy distribution of EEG signals, which is used for EEG feature extraction. Then, the classification systems based on EEG and peripheral physiological signals are presented. Single-trial and single-run classification results indicate that, on average, the performance of the EEG-based classification outperforms that of the peripheral physiological signals. However, the peripheral physiological signals can be considered as a good alternative to EEG signals in the case of assessing a user's preference for a given music video clip (like/dislike) since they have a comparable performance to EEG signals while being more easily measured. © 2012 ACM.",Affective computing; EEG; Emotion classification; Pattern classification; Physiological signals; Signal processing,Classification (of information); Electroencephalography; Pattern recognition; Physiology; Signal processing; Speech recognition; Video cameras; Affective Computing; Binary classification; Classification results; Classification system; Electro-encephalogram (EEG); Emotion classification; Multimedia content distribution; Physiological signals; Biomedical signal processing
A computational framework for media bias mitigation,2012,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84983573768&doi=10.1145%2f2209310.2209311&partnerID=40&md5=bab43e26239437b6dab42d3137f97045,"Bias in the news media is an inherent flaw of the news production process. The bias often causes a sharp increase in political polarization and in the cost of conflict on social issues such as the Iraq war. This article presents NewsCube, a novel Internet news service which aims to mitigate the effect of media bias. NewsCube automatically creates and promptly provides readers with multiple classified views on a news event. As such, it helps readers understand the event from a plurality of views and to formulate their own, more balanced, viewpoints. The media bias problem has been studied extensively in mass communications and social science. This article reviews related mass communication and journalism studies and provides a structured view of the media bias problem and its solution. We propose media bias mitigation as a practical solution and demonstrate it through NewsCube. We evaluate and discuss the effectiveness of NewsCube through various performance studies. © 2012 ACM.",aspect-level browsing; Media bias; news; news distribution service,aspect-level browsing; Computational framework; Mass communication; Media bias; news; News distributions; Performance study; Practical solutions
Access to multimodal articles for individuals with sight impairments,2012,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84983574296&doi=10.1145%2f2395123.2395126&partnerID=40&md5=5cce3b74fc108a5931fe4d8113cfcd7c,"Although intelligent interactive systems have been the focus of many research efforts, very few have addressed systems for individuals with disabilities. This article presents our methodology for an intelligent interactive system that provides individuals with sight impairments with access to the content of information graphics (such as bar charts and line graphs) in popular media. The article describes the methodology underlying the systems intelligent behavior, its interface for interacting with users, examples processed by the implemented system, and evaluation studies both of the methodology and the effectiveness of the overall system. This research advances universal access to electronic documents. © 2012 ACM.",Accessibility; Blind individuals; Human-computer interaction; Intelligent systems; Multimodal,Computer graphics; Human computer interaction; Intelligent systems; Accessibility; Blind individuals; Electronic document; Information graphics; Intelligent behavior; Intelligent interactive systems; Multi-modal; Research advances; Graph theory
The tag genome: Encoding community knowledge to support novel interaction,2012,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84983499543&doi=10.1145%2f2362394.2362395&partnerID=40&md5=7900612537b9676a5309caac56a8b717,"This article introduces the tag genome, a data structure that extends the traditional tagging model to provide enhanced forms of user interaction. Just as a biological genome encodes an organism based on a sequence of genes, the tag genome encodes an item in an information space based on its relationship to a common set of tags. We present a machine learning approach for computing the tag genome, and we evaluate several learning models on a ground truth dataset provided by users. We describe an application of the tag genome called Movie Tuner which enables users to navigate from one item to nearby items along dimensions represented by tags. We present the results of a 7-week field trial of 2,531 users of Movie Tuner and a survey evaluating users subjective experience. Finally, we outline the broader space of applications of the tag genome. © 2012 ACM.",conversational recommenders; data mining; information retrieval; machine learning; recommender systems; Tagging,Data mining; Data processing; Data structures; Encoding (symbols); Information retrieval; Learning systems; Recommender systems; Tuners; User interfaces; conversational recommenders; Ground-truth dataset; Information spaces; Learning models; Machine learning approaches; Subjective experiences; Tagging; User interaction; Genes
Emotional body language displayed by artificial agents,2012,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84983513884&doi=10.1145%2f2133366.2133368&partnerID=40&md5=81b4e97a2648aa1e21e58e61ae07a3ee,"Complex and natural social interaction between artificial agents (computer-generated or robotic) and humans necessitates the display of rich emotions in order to be believable, socially relevant, and accepted, and to generate the natural emotional responses that humans show in the context of social interaction, such as engagement or empathy. Whereas some robots use faces to display (simplified) emotional expressions, for other robots such as Nao, body language is the best medium available given their inability to convey facial expressions. Displaying emotional body language that can be interpreted whilst interacting with the robot should significantly improve naturalness. This research investigates the creation of an affect space for the generation of emotional body language to be displayed by humanoid robots. To do so, three experiments investigating how emotional body language displayed by agents is interpreted were conducted. The first experiment compared the interpretation of emotional body language displayed by humans and agents. The results showed that emotional body language displayed by an agent or a human is interpreted in a similar way in terms of recognition. Following these results, emotional key poses were extracted from an actor's performances and implemented in a Nao robot. The interpretation of these key poses was validated in a second study where it was found that participants were better than chance at interpreting the key poses displayed. Finally, an affect space was generated by blending key poses and validated in a third study. Overall, these experiments confirmed that body language is an appropriate medium for robots to display emotions and suggest that an affect space for body expressions can be used to improve the expressiveness of humanoid robots. © 2012 ACM.",Emotional body language; Human computer interactions; Human robot interactions,Anthropomorphic robots; Blending; Human computer interaction; Human robot interaction; Artificial agents; Body language; Computer generated; Emotional expressions; Emotional response; Facial Expressions; Humanoid robot; Social interactions; Social robots
A multitask approach to continuous five-dimensional affect sensing in natural speech,2012,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84983561287&doi=10.1145%2f2133366.2133372&partnerID=40&md5=54073f79a4c98f8fbd5efb4cd6e01575,"Automatic affect recognition is important for the ability of future technical systems to interact with us socially in an intelligent way by understanding our current affective state. In recent years there has been a shift in the field of affect recognition from ""in the lab"" experiments with acted data to ""in the wild"" experiments with spontaneous and naturalistic data. Two major issues thereby are the proper segmentation of the input and adequate description and modeling of affective states. The first issue is crucial for responsive, real-time systems such as virtual agents and robots, where the latency of the analysis must be as small as possible. To address this issue we introduce a novel method of incremental segmentation to be used in combination with supra-segmental modeling. For modeling of continuous affective states we use Long Short- Term Memory Recurrent Neural Networks, with which we can show an improvement in performance over standard recurrent neural networks and feed-forward neural networks as well as Support Vector Regression. For experiments we use the SEMAINE database, which contains recordings of spontaneous and natural human to Wizard-of-Oz conversations. The recordings are annotated continuously in time and magnitude with FeelTrace for five affective dimensions, namely activation, expectation, intensity, power/dominance, and valence. To exploit dependencies between the five affective dimensions we investigate multitask learning of all five dimensions augmented with inter-rater standard deviation.We can show improvements formultitask over single-task modeling. Correlation coefficients of up to 0.81 are obtained for the activation dimension and up to 0.58 for the valence dimension. The performance for the remaining dimensions were found to be in between that for activation and valence. © 2012 ACM.",Audio features; Dimensional affect; Emotion recognition; Long short-term memory; Neural networks; SEMAINE,Brain; Chemical activation; Interactive computer systems; Learning systems; Long short-term memory; Neural networks; Real time systems; Support vector regression; Affect recognition; Affective state; Audio features; Correlation coefficient; Dimensional affect; Emotion recognition; SEMAINE; Technical systems; Feedforward neural networks
Planning for reasoning with multiple common sense knowledge bases,2012,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84983562849&doi=10.1145%2f2362394.2362399&partnerID=40&md5=2e1094cd5ae8618893c732466b0a0a75,"Intelligent user interfaces require common sense knowledge to bridge the gap between the functionality of applications and the users goals. While current reasoning methods have been used to provide contextual information for interface agents, the quality of their reasoning results is limited by the coverage of their underlying knowledge bases. This article presents reasoning composition, a planning-based approach to integrating reasoning methods from multiple common sense knowledge bases to answer queries. The reasoning results of one reasoning method are passed to other reasoning methods to form a reasoning chain to the target context of a query. By leveraging different weak reasoning methods, we are able to find answers to queries that cannot be directly answered by querying a single common sense knowledge base. By conducting experiments on ConceptNet and WordNet, we compare the reasoning results of reasoning composition, directly querying merged knowledge bases, and spreading activation. The results show an 11.03% improvement in coverage over directly querying merged knowledge bases and a 49.7% improvement in accuracy over spreading activation. Two case studies are presented, showing how reasoning composition can improve performance of retrieval in a video editing system and a dialogue assistant. © 2012 ACM.",Common sense; commonsense reasoning; contextual reasoning; intelligent user interface; interface agent,Chemical activation; Knowledge based systems; Knowledge management; Query processing; Common sense; Commonsense reasoning; Contextual reasoning; Intelligent User Interfaces; Interface agent; User interfaces
Say anything: Using textual case-based reasoning to enable open-domain interactive storytelling,2012,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84983548396&doi=10.1145%2f2362394.2362398&partnerID=40&md5=e41f2e3ee65933628ae9deeb508b7aa0,"We describe Say Anything, a new interactive storytelling system that collaboratively writes textual narratives with human users. Unlike previous attempts, this interactive storytelling system places no restrictions on the content or direction of the users contribution to the emerging storyline. In response to these contributions, the computer continues the storyline with narration that is both coherent and entertaining. This capacity for open-domain interactive storytelling is enabled by an extremely large repository of nonfiction personal stories, which is used as a knowledge base in a case-based reasoning architecture. In this article, we describe the three main components of our case-based reasoning approach: a million-item corpus of personal stories mined from internet weblogs, a case retrieval strategy that is optimized for narrative coherence, and an adaptation strategy that ensures that repurposed sentences from the case base are appropriate for the users emerging fiction. We describe a series of evaluations of the systems ability to produce coherent and entertaining stories, and we compare these narratives with single-author stories posted to internet weblogs. © 2012 ACM.",case-based reasoning; Interactive Storytelling; weblogs and social media,Case based reasoning; Internet; Knowledge based systems; Adaptation strategies; Case retrieval; Case-based reasoning approaches; Human users; Interactive storytelling; Is-enabled; Knowledge base; Social media; Social networking (online)
Spotting laughter in natural multiparty conversations: A comparison of automatic online and offline approaches using audiovisual data,2012,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84983561587&doi=10.1145%2f2133366.2133370&partnerID=40&md5=95c6d0c5f5ee96a809d0e901ca705166,"It is essential for the advancement of human-centered multimodal interfaces to be able to infer the current user's state or communication state. In order to enable a system to do that, the recognition and interpretation of multimodal social signals (i.e., paralinguistic and nonverbal behavior) in real-time applications is required. Since we believe that laughs are one of the most important and widely understood social nonverbal signals indicating affect and discourse quality, we focus in thiswork on the detection of laughter in natural multiparty discourses. The conversations are recorded in a natural environment without any specific constraint on the discourses using unobtrusive recording devices. This setup ensures natural and unbiased behavior, which is one of the main foci of this work. To compare results of methods, namely Gaussian Mixture Model (GMM) supervectors as input to a Support Vector Machine (SVM), so-called Echo State Networks (ESN), and a Hidden Markov Model (HMM) approach, are utilized in online and offline detection experiments. The SVM approach proves very accurate in the offline classification task, but is outperformed by the ESN and HMM approach in the online detection (F1 scores: GMM SVM 0.45, ESN 0.63, HMM 0.72). Further, we were able to utilize the proposed HMM approach in a cross-corpus experiment without any retraining with respectable generalization capability (F1 score: 0.49). The results and possible reasons for these outcomes are shown and discussed in the article. The proposed methods may be directly utilized in practical tasks such as the labeling or the online detection of laughter in conversational data and affect-aware applications. © 2012 ACM.",Echo state networks; Gaussian mixture model supervectors; Hidden Markov models; Laughter detection; Multimodal; Natural discourse; Support vector machines,Gaussian distribution; Interface states; Support vector machines; Trellis codes; Affect aware applications; Echo state networks; Gaussian Mixture Model; Generalization capability; Multi-modal; Multi-modal interfaces; Multi-party conversations; Natural discourse; Hidden Markov models
Eliciting caregiving behavior in dyadic human-robot attachment-like interactions,2012,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84983487470&doi=10.1145%2f2133366.2133369&partnerID=40&md5=f0c2958fce6d95facb30479649422281,"We present here the design and applications of an arousal-based model controlling the behavior of a Sony AIBO robot during the exploration of a novel environment: a children's play mat.When the robot experiences too many new perceptions, the increase of arousal triggers calls for attention towards its human caregiver. The caregiver can choose to either calm the robot down by providing it with comfort, or to leave the robot coping with the situation on its own. When the arousal of the robot has decreased, the robot moves on to further explore the play mat. We gathered results from two experiments using this arousal-driven control architecture. In the first setting, we show that such a robotic architecture allows the human caregiver to influence greatly the learning outcomes of the exploration episode, with some similarities to a primary caregiver during early childhood. In a second experiment, we tested how human adults behaved in a similar setup with two different robots: one ""needy"", often demanding attention, and one more independent, requesting far less care or assistance. Our results show that human adults recognise each profile of the robot for what they have been designed, and behave accordingly to what would be expected, caring more for the needy robot than for the other. Additionally, the subjects exhibited a preference and more positive affect whilst interacting and rating the robot we designed as needy. This experiment leads us to the conclusion that our architecture and setup succeeded in eliciting positive and caregiving behavior from adults of different age groups and technological background. Finally, the consistency and reactivity of the robot during this dyadic interaction appeared crucial for the enjoyment and engagement of the human partner. © 2012 ACM.",Affective bonds; Developmental robotics; Emotions; Human-robot interactions,Behavioral research; Machine design; Robotics; Control architecture; Design and application; Developmental robotics; Dyadic interaction; Emotions; Learning outcome; Positive affects; Robotic architectures; Human robot interaction
Introduction to the special issue on common sense for interactive systems,2012,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84983554550&doi=10.1145%2f2362394.2362396&partnerID=40&md5=e06499bfd543618bb09b4a49f31e11c9,"This editorial introduction describes the aims and scope of the special issue on Common Sense for Interactive Systems of the ACM Transactions on Interactive Intelligent Systems. It explains why the common sense knowledge problem is crucial for both artificial intelligence and human-computer interaction, and it shows how the four articles selected for this issue fit into the theme. © 2012 ACM.",Common sense knowledge,Common sense; Commonsense knowledge; Interactive intelligent systems; Interactive system; Intelligent systems
Continuous body and hand gesture recognition for natural human-computer interaction,2012,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84983483288&doi=10.1145%2f2133366.2133371&partnerID=40&md5=1b04c6cff0fd2557ed12ac7811973897,"Intelligent gesture recognition systems open a new era of natural human-computer interaction: Gesturing is instinctive and a skill we all have, so it requires little or no thought, leaving the focus on the task itself, as it should be, not on the interaction modality. We present a new approach to gesture recognition that attends to both body and hands, and interprets gestures continuously from an unsegmented and unbounded input stream. This article describes the whole procedure of continuous body and hand gesture recognition, from the signal acquisition to processing, to the interpretation of the processed signals. Our system takes a vision-based approach, tracking body and hands using a single stereo camera. Body postures are reconstructed in 3D space using a generative model-based approach with a particle filter, combining both static and dynamic attributes of motion as the input feature to make tracking robust to self-occlusion. The reconstructed body postures guide searching for hands. Hand shapes are classified into one of several canonical hand shapes using an appearance-based approach with a multiclass support vector machine. Finally, the extracted body and hand features are combined and used as the input feature for gesture recognition.We consider our task as an online sequence labeling and segmentation problem. A latentdynamic conditional random field is used with a temporal sliding window to perform the task continuously. We augment this with a novel technique called multilayered filtering, which performs filtering both on the input layer and the prediction layer. Filtering on the input layer allows capturing long-range temporal dependencies and reducing input signal noise; filtering on the prediction layer allows taking weighted votes of multiple overlapping prediction results as well as reducing estimation noise. We tested our system in a scenario of real-world gestural interaction using the NATOPS dataset, an official vocabulary of aircraft handling gestures. Our experimental results show that: (1) the use of both static and dynamic attributes of motion in body tracking allows statistically significant improvement of the recognition performance over using static attributes of motion alone; and (2) the multilayered filtering statistically significantly improves recognition performance over the nonfiltering method.We also show that, on a set of twenty-four NATOPS gestures, our system achieves a recognition accuracy of 75.37%. © 2012 ACM.",Conditional random fields; Gesture recognition; Human-computer interaction; Multilayered filtering; Online sequence labeling and segmentation; Pose tracking,Forecasting; Human computer interaction; Image segmentation; Motion tracking; Palmprint recognition; Random processes; Stereo image processing; Stereo vision; Support vector machines; Three dimensional computer graphics; Appearance based approach; Conditional random field; Gesture recognition system; Multi-class support vector machines; Multi-layered; Natural human computer interactions; Pose tracking; Sequence Labeling; Gesture recognition
Introduction to the special issue on affective interaction in natural environments,2012,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84983522026&doi=10.1145%2f2133366.2133367&partnerID=40&md5=4b4b785d7dd5f72ae91caa70d384a692,[No abstract available],Affective interaction; Affective robotics; Automatic affect recognition; Expressive agents,
Why-oriented end-user debugging of naive Bayes text classification,2011,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84983579946&doi=10.1145%2f2030365.2030367&partnerID=40&md5=59ec1e5da6bdd6682a42298a59eb55e3,"Machine learning techniques are increasingly used in intelligent assistants, that is, software targeted at and continuously adapting to assist end users with email, shopping, and other tasks. Examples include desktop SPAM filters, recommender systems, and handwriting recognition. Fixing such intelligent assistants when they learn incorrect behavior, however, has received only limited attention. To directly support end-user ""debugging"" of assistant behaviors learned via statistical machine learning, we present a Why-oriented approach which allows users to ask questions about how the assistantmade its predictions, provides answers to these ""why"" questions, and allows users to interactively change these answers to debug the assistant's current and future predictions. To understand the strengths and weaknesses of this approach, we then conducted an exploratory study to investigate barriers that participants could encounter when debugging an intelligent assistant using our approach, and the information those participants requested to overcome these barriers. To help ensure the inclusiveness of our approach, we also explored how gender differences played a role in understanding barriers and information needs. We then used these results to consider opportunities for Why-oriented approaches to address user barriers and information needs. © 2011 ACM.",Debugging; End-user programming; Machine learning,Classification (of information); Computer debugging; Computer programming; Forecasting; Information science; End user programming; Exploratory studies; Handwriting recognition; Intelligent assistants; Limited attentions; Machine learning techniques; Statistical machine learning; Text classification; Learning systems
Multimodal approach to affective human-robot interaction design with children,2011,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84983573831&doi=10.1145%2f2030365.2030370&partnerID=40&md5=1f00b5f67dc2d6bc3bb3ddf2a2d8c0a9,"Two studies examined the different features of humanoid robots and the influence on children's affective behavior. The first study looked at interaction styles and general features of robots. The second study looked at how the robot's attention influences children's behavior and engagement. Through activities familiar to young children (e.g., table setting, story telling), the first study found that cooperative interaction style elicited more oculesic behavior and social engagement. The second study found that quality of attention, type of attention, and length of interaction influences affective behavior and engagement. In the quality of attention, Wizard-of-Oz (woz) elicited the most affective behavior, but automatic attention worked as well as woz when the interaction was short. The type of attention going from nonverbal to verbal attention increased children's oculesic behavior, utterance, and physiological response. Affective interactions did not seem to depend on a single mechanism, but a well-chosen confluence of technical features. © 2011 ACM.",Affective interaction; Collaboration; Human-robot communication; Human-robot interaction; Social robots; Young children,Anthropomorphic robots; Human robot interaction; Man machine systems; Physiological models; Affective interaction; Collaboration; Human-robot communication; Social robots; Young children; Human computer interaction
Introduction to the transactions on interactive intelligent systems,2011,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84983577493&doi=10.1145%2f2030365.2030366&partnerID=40&md5=14708d502a910f8ccde94b61071e9e9f,[No abstract available],,
The SignCom system for data-driven animation of interactive virtual signers: Methodology and evaluation,2011,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84983573827&doi=10.1145%2f2030365.2030371&partnerID=40&md5=b92ed15660123cc2bc2dd624f3829c26,"In this article we present a multichannel animation system for producing utterances signed in French Sign Language (LSF) by a virtual character. The main challenges of such a system are simultaneously capturing data for the entire body, including the movements of the torso, hands, and face, and developing a data-driven animation engine that takes into account the expressive characteristics of signed languages. Our approach consists of decomposing motion along different channels, representing the body parts that correspond to the linguistic components of signed languages. We show the ability of this animation system to create novel utterances in LSF, and present an evaluation by target users which highlights the importance of the respective body parts in the production of signs. We validate our framework by testing the believability and intelligibility of our virtual signer. © 2011 ACM.",Communicative gestures; Data-driven animation; Multichannel animation; Multimedia generation; Multimodal corpora; Signed language gestures,Digital storage; Communicative gestures; Data-driven animation; Multi-modal; Multichannel; Multimedia generation; Signed language gestures; Animation
Active multiple kernel learning for interactive 3D object retrieval systems,2011,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84983579925&doi=10.1145%2f2030365.2030368&partnerID=40&md5=71fbc0f1a1a0e0845cfc875d8eb05e91,"An effective relevance feedback solution plays a key role in interactive intelligent 3D object retrieval systems. In this work, we investigate the relevance feedback problem for interactive intelligent 3D object retrieval, with the focus on studying effective machine learning algorithms for improving the user's interaction in the retrieval task. One of the key challenges is to learn appropriate kernel similarity measure between 3D objects through the relevance feedback interaction with users. We address this challenge by presenting a novel framework of Active multiple kernel learning (AMKL), which exploits multiple kernel learning techniques for relevance feedback in interactive 3D object retrieval. The proposed framework aims to efficiently identify an optimal combination of multiple kernels by asking the users to label the most informative 3D images. We evaluate the proposed techniques on a dataset of over 10, 000 3D models collected from the World Wide Web. Our experimental results show that the proposed AMKL technique is significantly more effective for 3D object retrieval than the regular relevance feedback techniques widely used in interactive contentbased image retrieval, and thus is promising for enhancing user's interaction in such interactive intelligent retrieval systems. © 2011 ACM.",Debugging; End-user programming; Machine learning,Computer debugging; Computer programming; Content based retrieval; Data processing; Learning algorithms; Learning systems; Three dimensional computer graphics; World Wide Web; 3D object retrieval; Content based image retrieval; End user programming; Intelligent retrieval; Multiple Kernel Learning; Optimal combination; Relevance feedback; Relevance feedback techniques; Information retrieval
Recognizing sketched multistroke primitives,2011,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84983585909&doi=10.1145%2f2030365.2030369&partnerID=40&md5=a665c43d7324c95fe45e38473f016233,"Sketch recognition attempts to interpret the hand-sketched markings made by users on an electronic medium. Through recognition, sketches and diagrams can be interpreted and sent to simulators or other meaningful analyzers. Primitives are the basic building block shapes used by high-level visual grammars to describe the symbols of a given sketch domain. However, one limitation of these primitive recognizers is that they often only support basic shapes drawn with a single stroke. Furthermore, recognizers that do support multistroke primitives place additional constraints on users, such as temporal timeouts or modal button presses to signal shape completion. The goal of this research is twofold. First, we wanted to determine the drawing habits of most users. Our studies found multistroke primitives to be more prevalent than multiple primitives drawn in a single stroke. Additionally, our studies confirmed that threading is less frequent when there are more sides to a figure. Next, we developed an algorithm that is capable of recognizing multistroke primitives without requiring special drawing constraints. The algorithm uses a graph-building and search technique that takes advantage of Tarjan's linear search algorithm, along with principles to determine the goodness of a fit. Our novel, constraint-free recognizer achieves accuracy rates of 96% on freely-drawn primitives. © 2011 ACM.",Intelligent user interfaces; Neural networks; Primitive recognition; Sketch recognition,Neural networks; Basic building block; Electronic medium; Intelligent User Interfaces; Linear search algorithms; Primitive recognition; Search technique; Sketch recognition; Visual grammar; Algorithms
Detection and Recognition of Driver Distraction Using Multimodal Signals,2022,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144603911&doi=10.1145%2f3519267&partnerID=40&md5=1c247b4eba45c8d711c7737b615dd440,"Distracted driving is a leading cause of accidents worldwide. The tasks of distraction detection and recognition have been traditionally addressed as computer vision problems. However, distracted behaviors are not always expressed in a visually observable way. In this work, we introduce a novel multimodal dataset of distracted driver behaviors, consisting of data collected using twelve information channels coming from visual, acoustic, near-infrared, thermal, physiological and linguistic modalities. The data were collected from 45 subjects while being exposed to four different distractions (three cognitive and one physical). For the purposes of this paper, we performed experiments with visual, physiological, and thermal information to explore potential of multimodal modeling for distraction recognition. In addition, we analyze the value of different modalities by identifying specific visual, physiological, and thermal groups of features that contribute the most to distraction characterization. Our results highlight the advantage of multimodal representations and reveal valuable insights for the role played by the three modalities on identifying different types of driving distractions.  © 2022 held by the owner/author(s). Publication rights licensed to ACM.",action unit analysis; Distracted driving; machine learning; multimodal datasets; multimodal interaction; physiological signal processing; thermal (keyword),Infrared devices; Physiology; User interfaces; Action Unit; Action unit analyse; Distracted driving; Machine-learning; Multi-modal; Multi-modal dataset; Multimodal Interaction; Physiological signal processing; Thermal; Thermal (keyword); Machine learning
On the Importance of User Backgrounds and Impressions: Lessons Learned from Interactive AI Applications,2022,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147982975&doi=10.1145%2f3531066&partnerID=40&md5=c62da329521f8b5b2bb3ec4f30f2b5cc,"While EXplainable Artificial Intelligence (XAI) approaches aim to improve human-AI collaborative decision-making by improving model transparency and mental model formations, experiential factors associated with human users can cause challenges in ways system designers do not anticipate. In this article, we first showcase a user study on how anchoring bias can potentially affect mental model formations when users initially interact with an intelligent system and the role of explanations in addressing this bias. Using a video activity recognition tool in cooking domain, we asked participants to verify whether a set of kitchen policies are being followed, with each policy focusing on a weakness or a strength. We controlled the order of the policies and the presence of explanations to test our hypotheses. Our main finding shows that those who observed system strengths early on were more prone to automation bias and made significantly more errors due to positive first impressions of the system, while they built a more accurate mental model of the system competencies. However, those who encountered weaknesses earlier made significantly fewer errors, since they tended to rely more on themselves, while they also underestimated model competencies due to having a more negative first impression of the model. Motivated by these findings and similar existing work, we formalize and present a conceptual model of user's past experiences that examine the relations between user's backgrounds, experiences, and human factors in XAI systems based on usage time. Our work presents strong findings and implications, aiming to raise the awareness of AI designers toward biases associated with user impressions and backgrounds.  © 2022 Association for Computing Machinery.",cognitive biases; conceptual models; Explainable AI; HCI; user studies,Cognitive systems; Human computer interaction; Intelligent systems; AI applications; Cognitive bias; Collaborative decision making; Conceptual model; Explainable AI; First impressions; Mental model; Model formations; Model transparency; User study; Decision making
Auto-Icon+: An Automated End-to-End Code Generation Tool for Icon Designs in UI Development,2022,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143079482&doi=10.1145%2f3531065&partnerID=40&md5=472b273b2904ab465cbd6173104199b6,"Approximately 50% of development resources are devoted to user interface (UI) development tasks [9]. Occupying a large proportion of development resources, developing icons can be a time-consuming task, because developers need to consider not only effective implementation methods but also easy-to-understand descriptions. In this article, we present Auto-Icon+, an approach for automatically generating readable and efficient code for icons from design artifacts. According to our interviews to understand the gap between designers (icons are assembled from multiple components) and developers (icons as single images), we apply a heuristic clustering algorithm to compose the components into an icon image. We then propose an approach based on a deep learning model and computer vision methods to convert the composed icon image to fonts with descriptive labels, thereby reducing the laborious manual effort for developers and facilitating UI development. We quantitatively evaluate the quality of our method in the real-world UI development environment and demonstrate that our method offers developers accurate, efficient, readable, and usable code for icon designs, in terms of saving 65.2% implementing time.  © 2022 Association for Computing Machinery.",Code accessibility; icon implementation; neural networks,Deep learning; Network coding; User interfaces; Code accessibility; Code generation tools; Development resources; Development tasks; End to end; Icon designs; Icon images; Icon implementation; Neural-networks; User interface development; Clustering algorithms
Learning Semantically Rich Network-based Multi-modal Mobile User Interface Embeddings,2022,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147988344&doi=10.1145%2f3533856&partnerID=40&md5=436bcf66c9791d2b753da725e8eddbc4,"Semantically rich information from multiple modalities - text, code, images, categorical and numerical data - co-exist in the user interface (UI) design of mobile applications. Moreover, each UI design is composed of inter-linked UI entities that support different functions of an application, e.g., a UI screen comprising a UI taskbar, a menu, and multiple button elements. Existing UI representation learning methods unfortunately are not designed to capture multi-modal and linkage structure between UI entities. To support effective search and recommendation applications over mobile UIs, we need UI representations that integrate latent semantics present in both multi-modal information and linkages between UI entities. In this article, we present a novel self-supervised model - Multi-modal Attention-based Attributed Network Embedding (MAAN) model. MAAN is designed to capture structural network information present within the linkages between UI entities, as well as multi-modal attributes of the UI entity nodes. Based on the variational autoencoder framework, MAAN learns semantically rich UI embeddings in a self-supervised manner by reconstructing the attributes of UI entities and the linkages between them. The generated embeddings can be applied to a variety of downstream tasks: predicting UI elements associated with UI screens, inferring missing UI screen and element attributes, predicting UI user ratings, and retrieving UIs. Extensive experiments, including user evaluations, conducted on datasets from RICO, a rich real-world mobile UI repository, demonstrate that MAAN out-performs other state-of-the-art models. The number of linkages between UI entities can provide further information on the role of different UI entities in UI designs. However, MAAN does not capture edge attributes. To extend and generalize MAAN to learn even richer UI embeddings, we further propose EMAAN to capture edge attributes. We conduct additional extensive experiments on EMAAN, which show that it improves the performance of MAAN and similarly out-performs state-of-the-art models.  © 2022 held by the owner/author(s). Publication rights licensed to ACM.",mobile application user interface; multi-modal; Network embedding; self-supervised learning; unsupervised retrieval; user interface design,Mobile computing; Network embeddings; Semantics; Embeddings; Learn+; Mobile application user interface; Mobile applications; Mobile user interface; Multi-modal; Network embedding; Self-supervised learning; Unsupervised retrieval; User interface designs; User interfaces
PEACE: A Model of Key Social and Emotional Qualities of Conversational Chatbots,2022,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147851321&doi=10.1145%2f3531064&partnerID=40&md5=0f6b7328787cace6bc72d1995e231a21,"Open-domain chatbots engage with users in natural conversations to socialize and establish bonds. However, designing and developing an effective open-domain chatbot is challenging. It is unclear what qualities of a chatbot most correspond to users' expectations and preferences. Even though existing work has considered a wide range of aspects, some key components are still missing. For example, the role of chatbots' ability to communicate with humans at the emotional level remains an open subject of study. Furthermore, these trait qualities are likely to cover several dimensions. It is crucial to understand how the different qualities relate and interact with each other and what the core aspects would be. For this purpose, we first designed an exploratory user study aimed at gaining a basic understanding of the desired qualities of chatbots with a special focus on their emotional intelligence. Using the findings from the first study, we constructed a model of the desired traits by carefully selecting a set of features. With the help of a large-scale survey and structural equation modeling, we further validated the model using data collected from the survey. The final outcome is called the PEACE model (Politeness, Entertainment, Attentive Curiosity, and Empathy). By analyzing the dependencies between the different PEACE constructs, we shed light on the importance of and interplay between the chatbots' qualities and the effect of users' attitudes and concerns on their expectations of the technology. Not only PEACE defines the key ingredients of the social qualities of a chatbot, it also helped us derive a set of design implications useful for the development of socially adequate and emotionally aware open-domain chatbots.  © 2022 Copyright held by the owner/author(s).",adoption; Chatbots; conversational agents; emotional intelligence; interviews; SEM; social intelligence; user study,Adoption; Chatbots; Conversational agents; Emotional intelligence; Emotional quality; Interview; Social intelligence; User expectations; User study; User's preferences
ForSense: Accelerating Online Research Through Sensemaking Integration and Machine Research Support,2022,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85148041980&doi=10.1145%2f3532853&partnerID=40&md5=477b8d57fe5d7ab90807260a806781c7,"Online research is a frequent and important activity people perform on the Internet, yet current support for this task is basic, fragmented and not well integrated into web browser experiences. Guided by sensemaking theory, we present ForSense, a browser extension for accelerating people's online research experience. The two primary sources of novelty of ForSense are the integration of multiple stages of online research and providing machine assistance to the user by leveraging recent advances in neural-driven machine reading. We use ForSense as a design probe to explore (1) the benefits of integrating multiple stages of online research, (2) the opportunities to accelerate online research using current advances in machine reading, (3) the opportunities to support online research tasks in the presence of imprecise machine suggestions, and (4) insights about the behaviors people exhibit when performing online research, the pages they visit, and the artifacts they create. Through our design probe, we observe people performing online research tasks, and see that they benefit from ForSense's integration and machine support for online research. From the information and insights we collected, we derive and share key recommendations for designing and supporting imprecise machine assistance for research tasks.  © 2022 Association for Computing Machinery.",Human-AI collaboration; sensemaking,Behavioral research; Integration; 'current; Human-AI collaboration; Multiple stages; Online research; Primary sources; Research experience; Research support; Sense making; Sense-making theory; Probes
Textflow: Toward Supporting Screen-free Manipulation of Situation-Relevant Smart Messages,2022,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85148029358&doi=10.1145%2f3519263&partnerID=40&md5=8b0b266a8388ce737d2b51106d922a71,"Texting relies on screen-centric prompts designed for sighted users, still posing significant barriers to people who are blind and visually impaired (BVI). Can we re-imagine texting untethered from a visual display? In an interview study, 20 BVI adults shared situations surrounding their texting practices, recurrent topics of conversations, and challenges. Informed by these insights, we introduce TextFlow, a mixed-initiative context-aware system that generates entirely auditory message options relevant to the users' location, activity, and time of the day. Users can browse and select suggested aural messages using finger-taps supported by an off-the-shelf finger-worn device without having to hold or attend to a mobile screen. In an evaluative study, 10 BVI participants successfully interacted with TextFlow to browse and send messages in screen-free mode. The experiential response of the users shed light on the importance of bypassing the phone and accessing rapidly controllable messages at their fingertips while preserving privacy and accuracy with respect to speech or screen-based input. We discuss how non-visual access to proactive, contextual messaging can support the blind in a variety of daily scenarios. © 2022 Association for Computing Machinery.",assistive technologies; aural navigation; intelligent wearable and mobile interfaces; Text entry; ubiquitous smart environments,Assistive technology; User interfaces; Wearable computers; Assistive technology; Aural navigation; Blind and visually impaired; Intelligent wearable and mobile interface; Mobile interface; Smart environment; Text entry; Texting; Ubiquitous smart environment; Wearable interfaces; Taps
GO-Finder: A Registration-free Wearable System for Assisting Users in Finding Lost Hand-held Objects,2022,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147848256&doi=10.1145%2f3519268&partnerID=40&md5=8e86a4a670d6a16a488248445203b42d,"People spend an enormous amount of time and effort looking for lost objects. To help remind people of the location of lost objects, various computational systems that provide information on their locations have been developed. However, prior systems for assisting people in finding objects require users to register the target objects in advance. This requirement imposes a cumbersome burden on the users, and the system cannot help remind them of unexpectedly lost objects. We propose GO-Finder (""Generic Object Finder""), a registration-free wearable camera-based system for assisting people in finding an arbitrary number of objects based on two key features: automatic discovery of hand-held objects and image-based candidate selection. Given a video taken from a wearable camera, GO-Finder automatically detects and groups hand-held objects to form a visual timeline of the objects. Users can retrieve the last appearance of the object by browsing the timeline through a smartphone app. We conducted user studies to investigate how users benefit from using GO-Finder. In the first study, we asked participants to perform an object retrieval task and confirmed improved accuracy and reduced mental load in the object search task by providing clear visual cues on object locations. In the second study, the system's usability on a longer and more realistic scenario was verified, accompanied by an additional feature of context-based candidate filtering. Participant feedback suggested the usefulness of GO-Finder also in realistic scenarios where more than one hundred objects appear.  © 2022 Copyright held by the owner/author(s).",hand-object interaction; lost objects; Memory aid; object discovery; wearable camera,Cameras; Wearable technology; Hand-object interaction; Lost object; Memory aids; Object based; Object discovery; Object interactions; Objects-based; Realistic scenario; Wearable cameras; Wearable systems; Location
Effects of Explanations in AI-Assisted Decision Making: Principles and Comparisons,2022,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140195462&doi=10.1145%2f3519266&partnerID=40&md5=a9b65b36d6072b1c94185d559d649a4b,"Recent years have witnessed the growing literature in empirical evaluation of explainable AI (XAI) methods. This study contributes to this ongoing conversation by presenting a comparison on the effects of a set of established XAI methods in AI-assisted decision making. Based on our review of previous literature, we highlight three desirable properties that ideal AI explanations should satisfy - improve people's understanding of the AI model, help people recognize the model uncertainty, and support people's calibrated trust in the model. Through three randomized controlled experiments, we evaluate whether four types of common model-agnostic explainable AI methods satisfy these properties on two types of AI models of varying levels of complexity, and in two kinds of decision making contexts where people perceive themselves as having different levels of domain expertise. Our results demonstrate that many AI explanations do not satisfy any of the desirable properties when used on decision making tasks that people have little domain expertise in. On decision making tasks that people are more knowledgeable, the feature contribution explanation is shown to satisfy more desiderata of AI explanations, even when the AI model is inherently complex. We conclude by discussing the implications of our study for improving the design of XAI methods to better support human decision making, and for advancing more rigorous empirical evaluation of XAI methods.  © 2022 held by the owner/author(s).",explainable AI; human-subject experiments; Interpretable machine learning; trust; trust calibration,Behavioral research; Decision making; Uncertainty analysis; Decisions makings; Domain expertise; Empirical evaluations; Explainable AI; Human subject experiments; Interpretable machine learning; Machine-learning; Property; Trust; Trust calibration; Machine learning
Generating User-Centred Explanations via Illocutionary Question Answering: From Philosophy to Interfaces,2022,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122367649&doi=10.1145%2f3519265&partnerID=40&md5=7ed1d7c5b2d606f5fe455f3db41df6c3,"We propose a new method for generating explanations with Artificial Intelligence (AI) and a tool to test its expressive power within a user interface. In order to bridge the gap between philosophy and human-computer interfaces, we show a new approach for the generation of interactive explanations based on a sophisticated pipeline of AI algorithms for structuring natural language documents into knowledge graphs, answering questions effectively and satisfactorily. With this work, we aim to prove that the philosophical theory of explanations presented by Achinstein can be actually adapted for being implemented into a concrete software application, as an interactive and illocutionary process of answering questions. Specifically, our contribution is an approach to frame illocution in a computer-friendly way, to achieve user-centrality with statistical question answering. Indeed, we frame the illocution of an explanatory process as that mechanism responsible for anticipating the needs of the explainee in the form of unposed, implicit, archetypal questions, hence improving the user-centrality of the underlying explanatory process. Therefore, we hypothesise that if an explanatory process is an illocutionary act of providing content-giving answers to questions, and illocution is as we defined it, the more explicit and implicit questions can be answered by an explanatory tool, the more usable (as per ISO 9241-210) its explanations. We tested our hypothesis with a user-study involving more than 60 participants, on two XAI-based systems, one for credit approval (finance) and one for heart disease prediction (healthcare). The results showed that increasing the illocutionary power of an explanatory tool can produce statistically significant improvements (hence with a P value lower than .05) on effectiveness. This, combined with a visible alignment between the increments in effectiveness and satisfaction, suggests that our understanding of illocution can be correct, giving evidence in favour of our theory.  © 2022 held by the owner/author(s). Publication rights licensed to ACM.",education and learning-related technologies; explanatory artificial intelligence (YAI); Methods for explanations,Application programs; Knowledge graph; Natural language processing systems; Philosophical aspects; Artificial intelligence algorithms; Education and learning-related technology; Explanatory artificial intelligence (YAI); Expressive power; Human computer interfaces; Method for explanation; Natural languages; New approaches; Question Answering; User-centred; User interfaces
"How to Support Users in Understanding Intelligent Systems? An Analysis and Conceptual Framework of User Questions Considering User Mindsets, Involvement, and Knowledge Outcomes",2022,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85148034725&doi=10.1145%2f3519264&partnerID=40&md5=909eaf27dcc657e1fe3a1550ff56476a,"The opaque nature of many intelligent systems violates established usability principles and thus presents a challenge for human-computer interaction. Research in the field therefore highlights the need for transparency, scrutability, intelligibility, interpretability and explainability, among others. While all of these terms carry a vision of supporting users in understanding intelligent systems, the underlying notions and assumptions about users and their interaction with the system often remain unclear. We review the literature in HCI through the lens of implied user questions to synthesise a conceptual framework integrating user mindsets, user involvement, and knowledge outcomes to reveal, differentiate and classify current notions in prior work. This framework aims to resolve conceptual ambiguity in the field and enables researchers to clarify their assumptions and become aware of those made in prior work. We further discuss related aspects such as stakeholders and trust, and also provide material to apply our framework in practice (e.g., ideation/design sessions). We thus hope to advance and structure the dialogue on supporting users in understanding intelligent systems.  © 2022 held by the owner/author(s). Publication rights licensed to ACM.",accountability; end-user debugging; explainability; intelligent systems; intelligibility; interactive machine learning; interpretability; Review; scrutability; transparency,Human computer interaction; Intelligent systems; Program debugging; Accountability; Analysis frameworks; Conceptual frameworks; End-user debugging; End-users; Explainability; Intelligibility; Interactive machine learning; Interpretability; Scrutability; Transparency
Improving Office Workers' Workspace Using a Self-adjusting Computer Screen,2022,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137667121&doi=10.1145%2f3545993&partnerID=40&md5=20c605593951b68a82e7a12372502760,"With the rapid evolution of technology, computers and their users' workspaces have become an essential part of our life in general. Today, many people use computers both for work and for personal needs, spending long hours sitting at a desk in front of a computer screen, changing their pose slightly from time to time. This phenomenon impacts people's health negatively, adversely affecting their musculoskeletal and ocular systems. To mitigate these risks, several different ergonomic solutions have been suggested. This study proposes, demonstrates, and evaluates a technological solution that automatically adjusts the computer screen position and orientation to its user's current pose, using a simple RGB camera and robotic arm. The automatic adjustment will reduce the physical load on users and better fit their changing poses. The user's pose is extracted from images continuously acquired by the system's camera. The most suitable screen position is calculated according to the user's pose and ergonomic guidelines. Thereafter, the robotic arm adjusts the screen accordingly. The evaluation was done through a user study with 35 users who rated both the idea and the prototype system itself highly.  © 2022 Association for Computing Machinery.",Automatic screen position adjustment; dynamic workspace adjustment; user's pose identification,Cameras; Office buildings; Personal computers; Robotic arms; Automatic screen position adjustment; Computer screens; Dynamic workspace; Dynamic workspace adjustment; Evolution of technology; Ocular system; Office workers; Self-adjusting; Technological solution; User pose identification; Ergonomics
Adaptive Driving Assistant Model (ADAM) for Advising Drivers of Autonomous Vehicles,2022,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137846176&doi=10.1145%2f3545994&partnerID=40&md5=2fdafe36ee624c42169b31b17af5a6fc,"Fully autonomous driving is on the horizon; vehicles with advanced driver assistance systems (ADAS) such as Tesla's Autopilot are already available to consumers. However, all currently available ADAS applications require a human driver to be alert and ready to take control if needed. Partially automated driving introduces new complexities to human interactions with cars and can even increase collision risk. A better understanding of drivers' trust in automation may help reduce these complexities. Much of the existing research on trust in ADAS has relied on use of surveys and physiological measures to assess trust and has been conducted using driving simulators. There have been relatively few studies that use telemetry data from real automated vehicles to assess trust in ADAS. In addition, although some ADAS technologies provide alerts when, for example, drivers' hands are not on the steering wheel, these systems are not personalized to individual drivers. Needed are adaptive technologies that can help drivers of autonomous vehicles avoid crashes based on multiple real-time data streams. In this paper, we propose an architecture for adaptive autonomous driving assistance. Two layers of multiple sensory fusion models are developed to provide appropriate voice reminders to increase driving safety based on predicted driving status. Results suggest that human trust in automation can be quantified and predicted with 80% accuracy based on vehicle data, and that adaptive speech-based advice can be provided to drivers with 90 to 95% accuracy. With more data, these models can be used to evaluate trust in driving assistance tools, which can ultimately lead to safer and appropriate use of these features.  © 2022 Association for Computing Machinery.",adaptive advice; advanced driver assistance systems (ADAS); autonomous driving assistance; Trust in automation; vehicle telemetry,Accidents; Automobile drivers; Autonomous vehicles; Telemetering equipment; Adaptive advice; Advanced driver assistance system; Autonomous driving; Autonomous driving assistance; Autonomous Vehicles; Driving assistance; Human drivers; System applications; Trust in automation; Vehicle telemetry; Advanced driver assistance systems
"An Agile New Research Framework for Hybrid Human-AI Teaming: Trust, Transparency, and Transferability",2022,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137223269&doi=10.1145%2f3514257&partnerID=40&md5=d26da57b4793cecb8ac7961fdc03a156,"We propose a new research framework by which the nascent discipline of human-AI teaming can be explored within experimental environments in preparation for transferal to real-world contexts. We examine the existing literature and unanswered research questions through the lens of an Agile approach to construct our proposed framework. Our framework aims to provide a structure for understanding the macro features of this research landscape, supporting holistic research into the acceptability of human-AI teaming to human team members and the affordances of AI team members. The framework has the potential to enhance decision-making and performance of hybrid human-AI teams. Further, our framework proposes the application of Agile methodology for research management and knowledge discovery. We propose a transferability pathway for hybrid teaming to be initially tested in a safe environment, such as a real-time strategy video game, with elements of lessons learned that can be transferred to real-world situations.  © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",collaboration; human-AI; human-machine; Hybrid teams; teaming; transparency; trust; video games,Agile manufacturing systems; Decision making; Human computer interaction; Interactive computer graphics; Collaboration; Experimental environment; Human-AI; Human-machine; Hybrid team; Research frameworks; Team members; Teaming; Trust; Video-games; Transparency
SketchMaker: Sketch Extraction and Reuse for Interactive Scene Sketch Composition,2022,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137781507&doi=10.1145%2f3543956&partnerID=40&md5=99f58c5ae258ebe970799143f4ac14fb,"Sketching is an intuitive and simple way to depict sciences with various object form and appearance characteristics. In the past few years, widely available touchscreen devices have increasingly made sketch-based human-AI co-creation applications popular. One key issue of sketch-oriented interaction is to prepare input sketches efficiently by non-professionals because it is usually difficult and time-consuming to draw an ideal sketch with appropriate outlines and rich details, especially for novice users with no sketching skills. Thus, sketching brings great obstacles for sketch applications in daily life. On the other hand, hand-drawn sketches are scarce and hard to collect. Given the fact that there are several large-scale sketch datasets providing sketch data resources, but they usually have a limited number of objects and categories in sketch, and do not support users to collect new sketch materials according to their personal preferences. In addition, few sketch-related applications support the reuse of existing sketch elements. Thus, knowing how to extract sketches from existing drawings and effectively re-use them in interactive scene sketch composition will provide an elegant way for sketch-based image retrieval (SBIR) applications, which are widely used in various touch screen devices. In this study, we first conduct a study on current SBIR to better understand the main requirements and challenges in sketch-oriented applications. Then we develop the SketchMaker as an interactive sketch extraction and composition system to help users generate scene sketches via reusing object sketches in existing scene sketches with minimal manual intervention. Moreover, we demonstrate how SBIR improves from composited scene sketches to verify the performance of our interactive sketch processing system. We also include a sketch-based video localization task as an alternative application of our sketch composition scheme. Our pilot study shows that our system is effective and efficient, and provides a way to promote practical applications of sketches.  © 2022 Association for Computing Machinery.",scene sketch extraction; Sketch composition; sketch reuse,Drawing (graphics); Large dataset; Touch screens; Co-creation; Interactive sketch; Key Issues; Reuse; Scene sketch extraction; Simple++; Sketch composition; Sketch reuse; Sketch-based image retrievals; Sketchings; Extraction
Toward Involving End-users in Interactive Human-in-the-loop AI Fairness,2022,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137831302&doi=10.1145%2f3514258&partnerID=40&md5=238453e3daa70459bc1777010bf09a2d,"Ensuring fairness in artificial intelligence (AI) is important to counteract bias and discrimination in far-reaching applications. Recent work has started to investigate how humans judge fairness and how to support machine learning experts in making their AI models fairer. Drawing inspiration from an Explainable AI approach called explanatory debugging used in interactive machine learning, our work explores designing interpretable and interactive human-in-the-loop interfaces that allow ordinary end-users without any technical or domain background to identify potential fairness issues and possibly fix them in the context of loan decisions. Through workshops with end-users, we co-designed and implemented a prototype system that allowed end-users to see why predictions were made, and then to change weights on features to ""debug""fairness issues. We evaluated the use of this prototype system through an online study. To investigate the implications of diverse human values about fairness around the globe, we also explored how cultural dimensions might play a role in using this prototype. Our results contribute to the design of interfaces to allow end-users to be involved in judging and addressing AI fairness through a human-in-the-loop approach.  © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",AI fairness; cultural dimensions; end-users; explanatory debugging; human-in-the-loop; loan application decisions,Program debugging; Artificial intelligence fairness; Cultural dimensions; End-users; Explanatory debugging; Human-in-the-loop; Intelligence models; Interactive machine learning; Loan application decision; Machine-learning; Prototype system; Machine learning
Evaluation of a Multi-agent “Human-in-the-loop” Game Design System,2022,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137816827&doi=10.1145%2f3531009&partnerID=40&md5=be7af0d0f5a377bc3c83d7467098f5a4,"Designing games is a complicated and time-consuming process, where developing new levels for existing games can take weeks. Procedural content generation offers the potential to shorten this timeframe, however, automated design tools are not adopted widely in the game industry. This article presents an expert evaluation of a human-in-the-loop generative design approach for commercial game maps that incorporates multiple computational agents. The evaluation aims to gauge the extent to which such an approach could support and be accepted by human game designers and to determine whether the computational agents improve the overall design. To evaluate the approach, 11 game designers utilized the approach to design game levels with the computational agents both active and inactive. Eye-tracking, observational, and think-aloud data was collected to determine whether designers favored levels suggested by the computational agents. This data was triangulated with qualitative data from semi-structured interviews that were used to gather overall opinions of the approach. The eye-tracking data indicates that the participating game level designers showed a clear preference for levels suggested by the computational agents, however, expert designers in particular appeared to reject the idea that the computational agents are helpful. The perception of computational tools not being useful needs to be addressed if procedural content generation approaches are to fulfill their potential for the game industry. © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",autonomous agents; evolutionary computation; Human-based computation; multi-agent systems; procedural content generation; user evaluation,Autonomous agents; Evolutionary algorithms; Eye tracking; Game design; Computational agents; Eye-tracking; Game design; Game industry; Games designers; Human-based computation; Human-in-the-loop; Multi agent; Procedural content generations; User evaluations; Multi agent systems
Expressive Latent Feature Modelling for Explainable Matrix Factorisation-based Recommender Systems,2022,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137803731&doi=10.1145%2f3530299&partnerID=40&md5=2f360a08cc1c43d666e492208f13e3b3,"The traditional matrix factorisation (MF)-based recommender system methods, despite their success in making the recommendation, lack explainable recommendations as the produced latent features are meaningless and cannot explain the recommendation. This article introduces an MF-based explainable recommender system framework that utilises the user-item rating data and the available item information to model meaningful user and item latent features. These features are exploited to enhance the rating prediction accuracy and the recommendation explainability. Our proposed feature-based explainable recommender system framework utilises these meaningful user and item latent features to explain the recommendation without relying on private or outer data. The recommendations are explained to the user using text message and bar chart. Our proposed model has been evaluated in terms of the rating prediction accuracy and the reasonableness of the explanation using six real-world benchmark datasets for movies, books, video games, and fashion recommendation systems. The results show that the proposed model can produce accurate explainable recommendations.  © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",explainable recommendation; feature extraction; Matrix factorisation,Matrix algebra; Matrix factorization; Bar chart; Explainable recommendation; Feature models; Feature-based; Features extraction; Items ratings; Matrix factorizations; Prediction accuracy; System framework; System methods; Recommender systems
ClioQuery: Interactive Query-oriented Text Analytics for Comprehensive Investigation of Historical News Archives,2022,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137797043&doi=10.1145%2f3524025&partnerID=40&md5=afa7079f1a6a2a375a785d9b8d750510,"Historians and archivists often find and analyze the occurrences of query words in newspaper archives to help answer fundamental questions about society. But much work in text analytics focuses on helping people investigate other textual units, such as events, clusters, ranked documents, entity relationships, or thematic hierarchies. Informed by a study into the needs of historians and archivists, we thus propose ClioQuery, a text analytics system uniquely organized around the analysis of query words in context. ClioQuery applies text simplification techniques from natural language processing to help historians quickly and comprehensively gather and analyze all occurrences of a query word across an archive. It also pairs these new NLP methods with more traditional features like linked views and in-text highlighting to help engender trust in summarization techniques. We evaluate ClioQuery with two separate user studies, in which historians explain how ClioQuery's novel text simplification features can help facilitate historical research. We also evaluate with a separate quantitative comparison study, which shows that ClioQuery helps crowdworkers find and remember historical information. Such results suggest possible new directions for text analytics in other query-oriented settings.  © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Digital humanities; history; interactive text analytics; user interfaces,History; Natural language processing systems; Query processing; Analytics systems; Digital humanities; Entity-relationship; In contexts; Interactive queries; Interactive text analytic; Interactive texts; Natural languages; Query-words; Text analytics; User interfaces
"""I don't know what you mean by 'I am anxious'"": A New Method for Evaluating Conversational Agent Responses to Standardized Mental Health Inputs for Anxiety and Depression",2022,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135051925&doi=10.1145%2f3488057&partnerID=40&md5=a89e075c385eef43c5992897c6ba1eb0,"Conversational agents (CAs) are increasingly ubiquitous and are now commonly used to access medical information. However, we lack systematic data about the quality of advice such agents provide. This paper evaluates CA advice for mental health (MH) questions, a pressing issue given that we are undergoing a mental health crisis. Building on prior work, we define a new method to systematically evaluate mental health responses from CAs. We develop multi-utterance conversational probes derived from two widely used mental health diagnostic surveys, the PHQ-9 (Depression) and the GAD-7 (Anxiety). We evaluate the responses of two text-based chatbots and four voice assistants to determine whether CAs provide relevant responses and treatments. Evaluations were conducted both by clinicians and immersively by trained raters, yielding consistent results across all raters. Although advice and recommendations were generally low quality, they were better for Crisis probes and for probes concerning symptoms of Anxiety rather than Depression. Responses were slightly improved for text versus speech-based agents, and when CAs had access to extended dialogue context. Design implications include suggestions for improved responses through clarification sub-dialogues. Responses may also be improved by the incorporation of empathy although this needs to be combined with effective treatments or advice.  © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",chatbots; conversation evaluation; Conversational agents; healthcare; mental health; voice agents,Diagnosis; Chatbots; Conversation evaluation; Conversational agents; Health crisis; Health response; Healthcare; Medical information; Mental health; Pressung; Voice agent; Probes
An Empirical Study of Older Adult's Voice Assistant Use for Health Information Seeking,2022,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134150441&doi=10.1145%2f3484507&partnerID=40&md5=99143f009fa44f8a84fef4e1bc4b992c,"Although voice assistants are increasingly being adopted by older adults, we lack empirical research on how they interact with these devices for health information seeking. Also, prior work shows how voice assistant responses can provide misleading or inaccurate information and be harmful particularly in health contexts. Because of increased health needs while aging, this paper studies older adult's (ages 65+) health-related voice assistant interactions. Motivated by a lack of empirical evidence for how older adults approach information seeking with emerging technologies, we first conducted a survey of n = 201 older adults to understand how they engage voice assistants compared to a range of offline and digital sources for health information seeking. Findings show how voice assistants were used for confirmatory health queries, with users showing signs of distrust. As much prior work focuses on perceptions of voice assistant use, we conducted scenario-based interviews with n = 35 older adults to study health-related voice assistant behavior. In interviews, participants engaged with different health topics (flu, migraine, high blood pressure) and scenario types (symptom-driven, behavior-driven) using a voice assistant. Findings show how conversational and human-like expectations with voice assistants lead to information breakdowns between the older adult and voice assistant. This paper contributes a nuanced query-level analysis of older adults' voice-based health information seeking behaviors. Further, data provide evidence for how query reformulation happens with complex topics in voice-based information seeking. We use our findings to discuss how voice interfaces can better support older adults' health information seeking behaviors and expectations.  © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",health; interactive systems; older adults; search; Voice assistants,Blood pressure; Information retrieval; Information use; Empirical research; Empirical studies; Health informations; Information seeking; Information seeking behaviors; Interactive system; Misleading informations; Older adults; Search; Voice assistant; Health
A Multilingual Neural Coaching Model with Enhanced Long-term Dialogue Structure,2022,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135059657&doi=10.1145%2f3487066&partnerID=40&md5=12d79efc0c9ecb21b85253705dc3a7a0,"In this work we develop a fully data driven conversational agent capable of carrying out motivational coaching sessions in Spanish, French, Norwegian, and English. Unlike the majority of coaching, and in general well-being related conversational agents that can be found in the literature, ours is not designed by hand-crafted rules. Instead, we directly model the coaching strategy of professionals with end users. To this end, we gather a set of virtual coaching sessions through a Wizard of Oz platform, and apply state of the art Natural Language Processing techniques. We employ a transfer learning approach, pretraining GPT2 neural language models and fine-tuning them on our corpus. However, since these only take as input a local dialogue history, a simple fine-tuning procedure is not capable of modeling the long-term dialogue strategies that appear in coaching sessions. To alleviate this issue, we first propose to learn dialogue phase and scenario embeddings in the fine-tuning stage. These indicate to the model at which part of the dialogue it is and which kind of coaching session it is carrying out. Second, we develop a global deep learning system which controls the long-term structure of the dialogue. We also show that this global module can be used to visualize and interpret the decisions taken by the the conversational agent, and that the learnt representations are comparable to dialogue acts. Automatic and human evaluation show that our proposals serve to improve the baseline models. Finally, interaction experiments with coaching experts indicate that the system is usable and gives rise to positive emotions in Spanish, French and English, while the results in Norwegian point out that there is still work to be done in fully data driven approaches with very low resource languages.  © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",coaching; Dialogue system; explainable artificial intelligence; multilingual; transfer learning,Deep learning; Natural language processing systems; Speech processing; Transfer learning; Coaching; Conversational agents; Data driven; Dialogue systems; Explainable artificial intelligence; Fine tuning; Learn+; Multilingual; Transfer learning; Well being; Learning systems
Special Issue on Conversational Agents for Healthcare and Wellbeing,2022,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135048140&doi=10.1145%2f3532860&partnerID=40&md5=d8370ffec66148e4722bf06976252166,[No abstract available],chatbots; Conversational agents; digital health; healthcare; wellbeing,
Chatbots to Support Young Adults' Mental Health: An Exploratory Study of Acceptability,2022,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135040132&doi=10.1145%2f3485874&partnerID=40&md5=960fd67588f3cc05207e22c892dfb237,"Despite the prevalence of mental health conditions, stigma, lack of awareness, and limited resources impede access to care, creating a need to improve mental health support. The recent surge in scientific and commercial interest in conversational agents and their potential to improve diagnosis and treatment seems a potentially fruitful area in this respect, particularly for young adults who widely use such systems in other contexts. Yet, there is little research that considers the acceptability of conversational agents in mental health. This study, therefore, presents three research activities that explore whether conversational agents and, in particular, chatbots can be an acceptable solution in mental healthcare for young adults. First, a survey of young adults (in a university setting) provides an understanding of the landscape of mental health in this age group and of their views around mental health technology, including chatbots. Second, a literature review synthesises current evidence relating to the acceptability of mental health conversational agents and points to future research priorities. Third, interviews with counsellors who work with young adults, supported by a chatbot prototype and user-centred design techniques, reveal the perceived benefits and potential roles of mental health chatbots from the perspective of mental health professionals, while suggesting preconditions for the acceptability of the technology. Taken together, these research activities: provide evidence that chatbots are an acceptable solution to offering mental health support for young adults; identify specific challenges relating to both the technology and environment; and argue for the application of user-centred approaches during development of mental health chatbots and more systematic and rigorous evaluations of the resulting solutions.  © 2022 Association for Computing Machinery.",artificial intelligence; Chatbots and conversational agents; innovations in mental health systems; user-centred design,Data visualization; Health; Chatbot and conversational agent; Chatbots; Conversational agents; Exploratory studies; Health condition; Health systems; Innovation in mental health system; Mental health; Research activities; Young adults; User centered design
Experiences of a Speech-enabled Conversational Agent for the Self-report of Well-being among People Living with Affective Disorders: An In-the-Wild Study,2022,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135017535&doi=10.1145%2f3484508&partnerID=40&md5=a136fca75b44262fcfb87b9e8e1c518b,"The growing commercial success of smart speaker devices following recent advancements in speech recognition technology has surfaced new opportunities for collecting self-reported health and well-being data. Speech-enabled conversational agents (CAs) in particular, deployed in home environments using just such systems, may offer increasingly intuitive and engaging means of self-report. To date, however, few real-world studies have examined users' experiences of engaging in the self-report of mental health using such devices or the challenges of deploying these systems in the home context. With these aims in mind, this article recounts findings from a 4-week ""in-the-wild""study during which 20 individuals with depression or bipolar disorder used a speech-enabled CA named ""Sofia""to maintain a daily diary log, responding also to the World Health Organization-Five Well-Being Index WHO-5 scale every 2 weeks. Thematic analysis of post-study interviews highlights actions taken by participants to overcome CAs' limitations, diverse personifications of a speech-enabled agent, and unique forms of valuing of this system among users' personal and social circles. These findings serve as initial evidence for the potential of CAs to support the self-report of mental health and well-being, while highlighting the need to address outstanding technical limitations in addition to design challenges of conversational pattern matching, filling unmet interpersonal gaps, and the use of self-report CAs in the at-home social context. Based on these insights, we discuss implications for the future design of CAs to support the self-report of mental health and well-being.  © 2022 Association for Computing Machinery.",conversational agent; Conversational user interface; mental health; self-reports; virtual assistant; virtual health assistant; voice user interface; who-5,Health; Pattern matching; Speech recognition; Conversational agents; Conversational user interface; Mental health; Self-report; Speech recognition technology; Virtual assistants; Virtual health assistant; Voice user interface; Well being; Who-5; User interfaces
Discourse Behavior of Older Adults Interacting with a Dialogue Agent Competent in Multiple Topics,2022,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135090803&doi=10.1145%2f3484510&partnerID=40&md5=f4529d0946a9f12f5b806576b1574329,"We present a conversational agent designed to provide realistic conversational practice to older adults at risk of isolation or social anxiety, and show the results of a content analysis on a corpus of data collected from experiments with elderly patients interacting with our system. The conversational agent, represented by a virtual avatar, is designed to hold multiple sessions of casual conversation with older adults. Throughout each interaction, the system analyzes the prosodic and nonverbal behavior of users and provides feedback to the user in the form of periodic comments and suggestions on how to improve. Our avatar is unique in its ability to hold natural dialogues on a wide range of everyday topics - 27 topics in three groups, developed in collaboration with a team of gerontologists. The three groups vary in ""degrees of intimacy,""and as such in degrees of cognitive difficulty for the user. After collecting data from nine participants who interacted with the avatar for seven to nine sessions over a period of 3 to 4 weeks, we present results concerning dialogue behavior and inferred sentiment of the users. Analysis of the dialogues reveals correlations such as greater elaborateness for more difficult topics, increasing elaborateness with successive sessions, stronger sentiments in topics concerned with life goals rather than routine activities, and stronger self-disclosure for more intimate topics. In addition to their intrinsic interest, these results also reflect positively on the sophistication and practical applicability of our dialogue system.  © 2022 Association for Computing Machinery.",Animated conversational agents; nonverbal behavior; older adults social isolation; schema-based dialogue management; social skills training,Speech processing; Animated conversational agents; Conversational agents; Dialogue management; In-Degree; Non-verbal behaviours; Old adult social isolation; Older adults; Schema-based dialog management; Social isolation; Social skills training; Risk assessment
Relational Agents for the Homeless with Tuberculosis Experience: Providing Social Support Through Human-agent Relationships,2022,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135064084&doi=10.1145%2f3488056&partnerID=40&md5=a6767b160de17cde605c9d895a021187,"In human-computer interaction (HCI) research, relational agents (RAs) are increasingly used to improve social support for vulnerable groups including people exposed to stigmas, alienation, and isolation. However, technical support for tuberculosis (TB) patients, one such vulnerable group, remains insufficient due to the nature of the infectious disease and difficulties in accessing the homeless community. To derive design considerations for developing RAs targeting homeless TB patients, we conducted an empirical study on the patients. Data were collected through participatory observations and interviews and were processed using deductive thematic analysis. The patients' environmental and behavioral characteristics were classified, which showed that understanding these factors in the design of an RA is important because the patients' perception, attitudes, and expectations towards the agent are shaped by (and also shape) their environmental and behavioral characteristics, which consequently affect the nature of relationships formed between them. Therefore, we drew the following design considerations: (1) protection of privacy is a prerequisite to the use of an RA for homeless TB patients and can be addressed from both short-term (technical) and long-term (sociotechnical) perspectives; (2) the homeless group emphasized affective support from the agent, suggesting that relationships per se are already valuable to people who have been socially isolated and stigmatized; (3) consideration of the past memories in selecting social cues can facilitate the exchange of affective expressions in user-agent interaction; and (4) an RA should clarify to its interlocuters its identity as a machine to avoid confusing people with low technological literacy.  © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",empirical study; homeless TB patients; human-computer relationships; Relational agents,Human computer interaction; Medical computing; Behavioral characteristics; Design considerations; Empirical studies; Environmental characteristic; Homeless tuberculosis patient; Human-computer relationship; Relational agents; Social support; Tuberculosis patients; Vulnerable groups; Design
Finding AI's Faults with AAR/AI: An Empirical Study,2022,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123751789&doi=10.1145%2f3487065&partnerID=40&md5=6dba16d8112ca7d98a20d30dee455358,"Would you allow an AI agent to make decisions on your behalf? If the answer is ""not always,""the next question becomes ""in what circumstances""? Answering this question requires human users to be able to assess an AI agent- A nd not just with overall pass/fail assessments or statistics. Here users need to be able to localize an agent's bugs so that they can determine when they are willing to rely on the agent and when they are not. After-Action Review for AI (AAR/AI), a new AI assessment process for integration with Explainable AI systems, aims to support human users in this endeavor, and in this article we empirically investigate AAR/AI's effectiveness with domain-knowledgeable users. Our results show that AAR/AI participants not only located significantly more bugs than non-AAR/AI participants did (i.e., showed greater recall) but also located them more precisely (i.e., with greater precision). In fact, AAR/AI participants outperformed non-AAR/AI participants on every bug and were, on average, almost six times as likely as non-AAR/AI participants to find any particular bug. Finally, evidence suggests that incorporating labeling into the AAR/AI process may encourage domain-knowledgeable users to abstract above individual instances of bugs; we hypothesize that doing so may have contributed further to AAR/AI participants' effectiveness. © 2022 Association for Computing Machinery.",AAR/AI; after-action review for AI; explainable AI (XAI),After action review; After-action review for AI; Assessment process; Empirical studies; Explainable AI (XAI); Human users; Knowledgeable users; User need; Abstracting
Learning GUI Completions with User-defined Constraints,2022,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127770304&doi=10.1145%2f3490034&partnerID=40&md5=def3734db35b6abeb2de417b2af45917,"A key objective in the design of graphical user interfaces (GUIs) is to ensure consistency across screens of the same product. However, designing a compliant layout is time-consuming and can distract designers from creative thinking. This paper studies layout recommendation methods that fulfill such consistency requirements using machine learning. Given a desired element type and size, the methods suggest element placements following real-world GUI design processes. Consistency requirements are given implicitly through previous layouts from which patterns are to be learned, comparable to existing screens of a software product. We adopt two recently proposed methods for this task, a Graph Neural Network (GNN) and a Transformer model, and compare them with a custom approach based on sequence alignment and nearest neighbor search (kNN). The methods were tested on handcrafted datasets with explicit layout patterns, as well as large-scale public datasets of diverse mobile design layouts. Our results show that our instance-based learning algorithm outperforms both neural network approaches. Ultimately, this work contributes to establishing smarter design tools for professional designers with explainable algorithms that increase their efficacy. © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",design; layout completion; layouts; machine learning; User interfaces,Large dataset; Learning algorithms; Machine learning; Nearest neighbor search; Product design; Consistency requirements; Creative thinking; Element sizes; Element type; Key objective; Layout; Layout completion; Real-world; Recommendation methods; User Interface design process; Graphical user interfaces
Editorial Introduction to TiiS Special Category Article: Practitioners' Toolbox,2022,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127748577&doi=10.1145%2f3519381&partnerID=40&md5=0eb46a8606a5df9ecf549f378af64951,[No abstract available],,
SmartShots: An Optimization Approach for Generating Videos with Data Visualizations Embedded,2022,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127796834&doi=10.1145%2f3484506&partnerID=40&md5=743e3367afa031ee386b9920b453ad82,"Videos are well-received methods for storytellers to communicate various narratives. To further engage viewers, we introduce a novel visual medium where data visualizations are embedded into videos to present data insights. However, creating such data-driven videos requires professional video editing skills, data visualization knowledge, and even design talents. To ease the difficulty, we propose an optimization method and develop SmartShots, which facilitates the automatic integration of in-video visualizations. For its development, we first collaborated with experts from different backgrounds, including information visualization, design, and video production. Our discussions led to a design space that summarizes crucial design considerations along three dimensions: Visualization, embedded layout, and rhythm. Based on that, we formulated an optimization problem that aims to address two challenges: (1) embedding visualizations while considering both contextual relevance and aesthetic principles and (2) generating videos by assembling multi-media materials. We show how SmartShots solves this optimization problem and demonstrate its usage in three cases. Finally, we report the results of semi-structured interviews with experts and amateur users on the usability of SmartShots. © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",data-driven videos; optimization; Visualization,Data visualization; Information systems; Optimization; Data driven; Data-driven video; Design talents; Even designs; Optimisations; Optimization approach; Optimization method; Optimization problems; Video editing; Visual media; Visualization
Tribe or Not? Critical Inspection of Group Differences Using TribalGram,2022,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127755199&doi=10.1145%2f3484509&partnerID=40&md5=c9f861b569e06d810a6ec27b980ce7a6,"With the rise of AI and data mining techniques, group profiling and group-level analysis have been increasingly used in many domains, including policy making and direct marketing. In some cases, the statistics extracted from data may provide insights to a group's shared characteristics; in others, the group-level analysis can lead to problems, including stereotyping and systematic oppression. How can analytic tools facilitate a more conscientious process in group analysis? In this work, we identify a set of accountable group analytics design guidelines to explicate the needs for group differentiation and preventing overgeneralization of a group. Following the design guidelines, we develop TribalGram, a visual analytic suite that leverages interpretable machine learning algorithms and visualization to offer inference assessment, model explanation, data corroboration, and sense-making. Through the interviews with domain experts, we showcase how our design and tools can bring a richer understanding of ""groups""mined from the data. © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",contrastive explanation; Group analysis; group difference; group profiling; interpretable machine learning; visual analytics,Data mining; Data visualization; Design; Inference engines; Learning algorithms; Machine learning; Contrastive explanation; Data-mining techniques; Direct marketing; Group analysis; Group differences; Group level; Group profiling; Interpretable machine learning; Policy making; Visual analytics; Visualization
FAtiMA Toolkit: Toward an Accessible Tool for the Development of Socio-emotional Agents,2022,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127788953&doi=10.1145%2f3510822&partnerID=40&md5=7a9efaf0eb56d62c4ec749e454d7bb20,"More than a decade has passed since the development of FearNot!, an application designed to help children deal with bullying through role-playing with virtual characters. It was also the application that led to the creation of FAtiMA, an affective agent architecture for creating autonomous characters that can evoke empathic responses. In this article, we describe the FAtiMA Toolkit, a collection of open-source tools that is designed to help researchers, game developers, and roboticists incorporate a computational model of emotion and decision-making in their work. The toolkit was developed with the goal of making FAtiMA more accessible, easier to incorporate into different projects, and more flexible in its capabilities for human-agent interaction, based upon the experience gathered over the years across different virtual environments and human-robot interaction scenarios. As a result, this work makes several different contributions to the field of Agent-Based Architectures. More precisely, the FAtiMA Toolkit's library-based design allows developers to easily integrate it with other frameworks, its meta-cognitive model affords different internal reasoners and affective components, and its explicit dialogue structure gives control to the author even within highly complex scenarios. To demonstrate the use of the FAtiMA Toolkit, several different use cases where the toolkit was successfully applied are described and discussed. © 2022 Association for Computing Machinery.",affective computing; cognitive architecture; Embodied agents; emotions; social robots,Autonomous agents; Decision making; Human robot interaction; Affective agent architectures; Affective Computing; Autonomous characters; Cognitive architectures; Embodied agent; Emotion; Emotional agents; Role playing; Social robots; Virtual character; Virtual reality
Adaptive Cognitive Training with Reinforcement Learning,2022,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127762906&doi=10.1145%2f3476777&partnerID=40&md5=0ec3a61e9b640f0e205043cb3e4a5984,"Computer-assisted cognitive training can help patients affected by several illnesses alleviate their cognitive deficits or healthy people improve their mental performance. In most computer-based systems, training sessions consist of graded exercises, which should ideally be able to gradually improve the trainee's cognitive functions. Indeed, adapting the difficulty of the exercises to how individuals perform in their execution is crucial to improve the effectiveness of cognitive training activities. In this article, we propose the use of reinforcement learning (RL) to learn how to automatically adapt the difficulty of computerized exercises for cognitive training. In our approach, trainees' performance in performed exercises is used as a reward to learn a policy that changes over time the values of the parameters that determine exercise difficulty. We illustrate a method to be initially used to learn difficulty-variation policies tailored for specific categories of trainees, and then to refine these policies for single individuals. We present the results of two user studies that provide evidence for the effectiveness of our method: A first study, in which a student category policy obtained via RL was found to have better effects on the cognitive function than a standard baseline training that adopts a mechanism to vary the difficulty proposed by neuropsychologists, and a second study, demonstrating that adding an RL-based individual customization further improves the training process. © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",adaptivity; Computerized cognitive training; personalization,Brain; Cognitive systems; E-learning; Medical computing; Adaptivity; Cognitive deficits; Cognitive functions; Cognitive training; Computer assisted; Computerized cognitive training; Learn+; Performance; Personalizations; Reinforcement learnings; Reinforcement learning
Initial Responses to False Positives in AI-Supported Continuous Interactions: A Colonoscopy Case Study,2022,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127754178&doi=10.1145%2f3480247&partnerID=40&md5=2be3efa09a0bbfedd946ce192c3dd4b8,"The use of artificial intelligence (AI) in clinical support systems is increasing. In this article, we focus on AI support for continuous interaction scenarios. A thorough understanding of end-user behaviour during these continuous human-AI interactions, in which user input is sustained over time and during which AI suggestions can appear at any time, is still missing. We present a controlled lab study involving 21 endoscopists and an AI colonoscopy support system. Using a custom-developed application and an off-the-shelf videogame controller, we record participants' navigation behaviour and clinical assessment across 14 endoscopic videos. Each video is manually annotated to mimic an AI recommendation, being either true positive or false positive in nature. We find that time between AI recommendation and clinical assessment is significantly longer for incorrect assessments. Further, the type of medical content displayed significantly affects decision time. Finally, we discover that the participant's clinical role plays a large part in the perception of clinical AI support systems. Our study presents a realistic assessment of the effects of imperfect and continuous AI support in a clinical scenario. © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",artificial intelligence; clinical decision support; colonoscopy; continuous interaction; false positives; Human-AI interaction; support system,Artificial intelligence; Behavioral research; Decision support systems; Case-studies; Clinical assessments; Clinical decision support; Clinical support systems; Colonoscopy; Continuous interactions; False positive; Human-artificial intelligence interaction; Intelligence support; Support systems; Endoscopy
Introduction to the Special Column for Human-Centered Artificial Intelligence,2021,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139184485&doi=10.1145%2f3490553&partnerID=40&md5=57f2dadb9cee9c0b94cf75a2583d6270,[No abstract available],,
Toward Responsible AI: An Overview of Federated Learning for User-centered Privacy-preserving Computing,2021,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128261504&doi=10.1145%2f3485875&partnerID=40&md5=587535dbb1415beab44481299adc5dc5,"With the rapid advances of Artificial Intelligence (AI) technologies and applications, an increasing concern is on the development and application of responsible AI technologies. Building AI technologies or machine-learning models often requires massive amounts of data, which may include sensitive, user private information to be collected from different sites or countries. Privacy, security, and data governance constraints rule out a brute force process in the acquisition and integration of these data. It is thus a serious challenge to protect user privacy while achieving high-performance models. This article reviews recent progress of federated learning in addressing this challenge in the context of privacy-preserving computing. Federated learning allows global AI models to be trained and used among multiple decentralized data sources with high security and privacy guarantees, as well as sound incentive mechanisms. This article presents the background, motivations, definitions, architectures, and applications of federated learning as a new paradigm for building privacy-preserving, responsible AI ecosystems. © 2021 Association for Computing Machinery.",blockchain; data security; decentralized AI; Federated learning; machine learning; privacy-preserving computing; responsible AI; user privacy,Engineering education; Historic preservation; Learning systems; Machine learning; Privacy-preserving techniques; Sensitive data; Artificial intelligence technologies; Block-chain; Decentralised; Decentralized artificial intelligence; Federated learning; Machine-learning; Privacy preserving; Privacy-preserving computing; Responsible artificial intelligence; User privacy; Blockchain
"Learn, Generate, Rank, Explain: A Case Study of Visual Explanation by Generative Machine Learning",2021,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139211204&doi=10.1145%2f3465407&partnerID=40&md5=d3d4c590a437d3e1c08b087a680ce9c9,"While the computer vision problem of searching for activities in videos is usually addressed by using discriminative models, their decisions tend to be opaque and difficult for people to understand. We propose a case study of a novel machine learning approach for generative searching and ranking of motion capture activities with visual explanation. Instead of directly ranking videos in the database given a text query, our approach uses a variant of Generative Adversarial Networks (GANs) to generate exemplars based on the query and uses them to search for the activity of interest in a large database. Our model is able to achieve comparable results to its discriminative counterpart, while being able to dynamically generate visual explanations. In addition to our searching and ranking method, we present an explanation interface that enables the user to successfully explore the model's explanations and its confidence by revealing query-based, model-generated motion capture clips that contributed to the model's decision. Finally, we conducted a user study with 44 participants to show that by using our model and interface, participants benefit from a deeper understanding of the model's conceptualization of the search query. We discovered that the XAI system yielded a comparable level of efficiency, accuracy, and user-machine synchronization as its black-box counterpart, if the user exhibited a high level of trust for AI explanation. © 2021 held by the owner/author(s). Publication rights licensed to ACM.",Explainable artificial intelligence; model-generated explanation; trust and reliance; user study,Query processing; Case-studies; Computer vision problems; Explainable artificial intelligence; Learn+; Machine-learning; Model-generated explanation; Motion capture; Searching and ranking; Trust and reliance; User study; Generative adversarial networks
A Taxonomy of Property Measures to Unify Active Learning and Human-centered Approaches to Data Labeling,2021,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130533960&doi=10.1145%2f3439333&partnerID=40&md5=561fa905a8282c18364a0c49f3354190,"Strategies for selecting the next data instance to label, in service of generating labeled data for machine learning, have been considered separately in the machine learning literature on active learning and in the visual analytics literature on human-centered approaches. We propose a unified design space for instance selection strategies to support detailed and fine-grained analysis covering both of these perspectives. We identify a concise set of 15 properties, namely measureable characteristics of datasets or of machine learning models applied to them, that cover most of the strategies in these literatures. To quantify these properties, we introduce Property Measures (PM) as fine-grained building blocks that can be used to formalize instance selection strategies. In addition, we present a taxonomy of PMs to support the description, evaluation, and generation of PMs across four dimensions: machine learning (ML) Model Output, Instance Relations, Measure Functionality, and Measure Valence. We also create computational infrastructure to support qualitative visual data analysis: a visual analytics explainer for PMs built around an implementation of PMs using cascades of eight atomic functions. It supports eight analysis tasks, covering the analysis of datasets and ML models using visual comparison within and between PMs and groups of PMs, and over time during the interactive labeling process. We iteratively refined the PM taxonomy, the explainer, and the task abstraction in parallel with each other during a two-year formative process, and show evidence of their utility through a summative evaluation with the same infrastructure. This research builds a formal baseline for the better understanding of the commonalities and differences of instance selection strategies, which can serve as the stepping stone for the synthesis of novel strategies in future work.  © 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM.",active learning; candidate selection strategies; classification; clustering; instance selection strategies; Interactive machine learning; machine learning explainers; property measures; query strategies; taxonomy; visual analytics; visual-interactive labeling,Machine learning; Visualization; Active Learning; Candidate selection; Candidate selection strategy; Clusterings; Instance selection; Instance selection strategy; Interactive labeling; Interactive machine learning; Machine learning explainer; Machine-learning; Property; Property measure; Query strategies; Visual analytics; Visual-interactive labeling; Taxonomies
Promoting Energy-Efficient Behavior by Depicting Social Norms in a Recommender Interface,2021,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135148528&doi=10.1145%2f3460005&partnerID=40&md5=5fed839075c4085a8dd2748b2d57e491,"How can recommender interfaces help users to adopt new behaviors? In the behavioral change literature, social norms and other nudges are studied to understand how people can be convinced to take action (e.g., towel re-use is boosted when stating that ""75% of hotel guests""do so), but most of these nudges are not personalized. In contrast, recommender systems know what to recommend in a personalized way, but not much human-computer interaction (HCI) research has considered how personalized advice should be presented to help users to change their current habits. We examine the value of depicting normative messages (e.g., ""75% of users do X""), based on actual user data, in a personalized energy recommender interface called ""Saving Aid.""In a study among 207 smart thermostat owners, we compared three different normative explanations (""Global.""""Similar,""and ""Experienced""norm rates) to a non-social baseline (""kWh savings""). Although none of the norms increased the total number of chosen measures directly, we show that depicting high peer adoption rates alongside energy-saving measures increased the likelihood that they would be chosen from a list of recommendations. In addition, we show that depicting social norms positively affects a user's evaluation of a recommender interface. © 2021 Association for Computing Machinery.",Behavioral change; energy conservation; Rasch model; recommender systems; user experience,Energy efficiency; Human computer interaction; User interfaces; 'current; Behavioral changes; Energy; Energy efficient; Energy savings measures; Human-computer interaction researches; Rasch modeling; Social norm; User data; Users' experiences; Recommender systems
Projection Path Explorer: Exploring Visual Patterns in Projected Decision-making Paths,2021,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126260766&doi=10.1145%2f3387165&partnerID=40&md5=fd4fa9ecf9fd07ec007ec24bd2eaec5c,"In problem-solving, a path towards a solutions can be viewed as a sequence of decisions. The decisions, made by humans or computers, describe a trajectory through a high-dimensional representation space of the problem. By means of dimensionality reduction, these trajectories can be visualized in lower-dimensional space. Such embedded trajectories have previously been applied to a wide variety of data, but analysis has focused almost exclusively on the self-similarity of single trajectories. In contrast, we describe patterns emerging from drawing many trajectories - for different initial conditions, end states, and solution strategies - in the same embedding space. We argue that general statements about the problem-solving tasks and solving strategies can be made by interpreting these patterns. We explore and characterize such patterns in trajectories resulting from human and machine-made decisions in a variety of application domains: logic puzzles (Rubik's cube), strategy games (chess), and optimization problems (neural network training). We also discuss the importance of suitably chosen representation spaces and similarity metrics for the embedding.  © 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Algorithm visualization; dimensionality reduction; game visualization; multivariate time series; trajectories,Decision making; Embeddings; Neural networks; Visualization; Algorithm visualization; Decisions makings; Dimensionality reduction; Embeddings; Game visualization; High-dimensional; Multivariate time series; Problem-solving; Representation space; Visual pattern; Trajectories
A Multidisciplinary Survey and Framework for Design and Evaluation of Explainable AI Systems,2021,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139251634&doi=10.1145%2f3387166&partnerID=40&md5=f6fa7ed698ff06828e3b45803350e231,"The need for interpretable and accountable intelligent systems grows along with the prevalence of artificial intelligence (AI) applications used in everyday life. Explainable AI (XAI) systems are intended to self-explain the reasoning behind system decisions and predictions. Researchers from different disciplines work together to define, design, and evaluate explainable systems. However, scholars from different disciplines focus on different objectives and fairly independent topics of XAI research, which poses challenges for identifying appropriate design and evaluation methodology and consolidating knowledge across efforts. To this end, this article presents a survey and framework intended to share knowledge and experiences of XAI design and evaluation methods across multiple disciplines. Aiming to support diverse design goals and evaluation methods in XAI research, after a thorough review of XAI related papers in the fields of machine learning, visualization, and human-computer interaction, we present a categorization of XAI design goals and evaluation methods. Our categorization presents the mapping between design goals for different XAI user groups and their evaluation methods. From our findings, we develop a framework with step-by-step design guidelines paired with evaluation methods to close the iterative design and evaluation cycles in multidisciplinary XAI teams. Further, we provide summarized ready-to-use tables of evaluation methods and recommendations for different goals in XAI research.  © 2021 Association for Computing Machinery.",Explainable artificial intelligence (XAI); explanation; human-computer interaction (HCI); machine learning; transparency,Design; Human computer interaction; Intelligent systems; Iterative methods; Surveys; Appropriate designs; Artificial intelligence systems; Design and evaluations; Design evaluation; Design goal; Evaluation methods; Explainable artificial intelligence (XAI); Explanation; Human-computer interaction; Machine-learning; Machine learning
Developing Conversational Agents for Use in Criminal Investigations,2021,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139259421&doi=10.1145%2f3444369&partnerID=40&md5=efe1d68ea0be232ca515516a5ce53872,"The adoption of artificial intelligence (AI) systems in environments that involve high risk and high consequence decision-making is severely hampered by critical design issues. These issues include system transparency and brittleness, where transparency relates to (i) the explainability of results and (ii) the ability of a user to inspect and verify system goals and constraints; and brittleness, (iii) the ability of a system to adapt to new user demands. Transparency is a particular concern for criminal intelligence analysis, where there are significant ethical and trust issues that arise when algorithmic and system processes are not adequately understood by a user. This prevents adoption of potentially useful technologies in policing environments. In this article, we present a novel approach to designing a conversational agent (CA) AI system for intelligence analysis that tackles these issues. We discuss the results and implications of three different studies; a Cognitive Task Analysis to understand analyst thinking when retrieving information in an investigation, Emergent Themes Analysis to understand the explanation needs of different system components, and an interactive experiment with a prototype conversational agent. Our prototype conversational agent, named Pan, demonstrates transparency provision and mitigates brittleness by evolving new CA intentions. We encode interactions with the CA with human factors principles for situation recognition and use interactive visual analytics to support analyst reasoning. Our approach enables complex AI systems, such as Pan, to be used in sensitive environments, and our research has broader application than the use case discussed.  © 2021 Association for Computing Machinery.",conversational agents; criminal intelligence analysis; Explainability; transparency,Brittleness; Crime; Decision making; Fracture mechanics; Information analysis; Job analysis; Plasticity; Artificial intelligence systems; Conversational agents; Criminal intelligence; Criminal intelligence analyse; Criminal investigation; Critical design; Decisions makings; Design issues; Explainability; Intelligence analysis; Transparency
After-Action Review for AI (AAR/AI),2021,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123791997&doi=10.1145%2f3453173&partnerID=40&md5=4a6da16288815eb28a69e9d418593417,"Explainable AI is growing in importance as AI pervades modern society, but few have studied how explainable AI can directly support people trying to assess an AI agent. Without a rigorous process, people may approach assessment in ad hoc ways - leading to the possibility of wide variations in assessment of the same agent due only to variations in their processes. AAR, or After-Action Review, is a method some military organizations use to assess human agents, and it has been validated in many domains. Drawing upon this strategy, we derived an After-Action Review for AI (AAR/AI), to organize ways people assess reinforcement learning agents in a sequential decision-making environment. We then investigated what AAR/AI brought to human assessors in two qualitative studies. The first investigated AAR/AI to gather formative information, and the second built upon the results, and also varied the type of explanation (model-free vs. model-based) used in the AAR/AI process. Among the results were the following: (1) participants reporting that AAR/AI helped to organize their thoughts and think logically about the agent, (2) AAR/AI encouraged participants to reason about the agent from a wide range of perspectives, and (3) participants were able to leverage AAR/AI with the model-based explanations to falsify the agent's predictions. © 2021 Association for Computing Machinery.",after-action review; Explainable AI,Behavioral research; Decision making; Intelligent agents; After action review; Explainable AI; Human agent; Human assessors; Military organizations; Model free; Model-based OPC; Qualitative study; Reinforcement learning agent; Sequential decision making; Reinforcement learning
BONNIE: Building Online Narratives from Noteworthy Interaction Events,2021,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139243522&doi=10.1145%2f3423048&partnerID=40&md5=6386e5f790ac7fd000762408338a1615,"Nowadays, we have access to data of unprecedented volume, high dimensionality, and complexity. To extract novel insights from such complex and dynamic data, we need effective and efficient strategies. One such strategy is to combine data analysis and visualization techniques, which are the essence of visual analytics applications. After the knowledge discovery process, a major challenge is to filter the essential information that has led to a discovery and to communicate the findings to other people, explaining the decisions they may have made based on the data. We propose to record and use the trace left by the exploratory data analysis, in the form of user interaction history, to aid this process. With the trace, users can choose the desired interaction steps and create a narrative, sharing the acquired knowledge with readers. To achieve our goal, we have developed the BONNIE (Building Online Narratives from Noteworthy Interaction Events) framework. BONNIE comprises a log model to register the interaction events, auxiliary code to help developers instrument their own code, and an environment to view users' own interaction history and build narratives. This article presents our proposal for communicating discoveries in visual analytics applications, the BONNIE framework, and the studies we conducted to evaluate our solution. After two user studies (the first one focused on history visualization and the second one focused on narrative creation), our solution has showed to be promising, with mostly positive feedback and results from a Technology Acceptance Model (TAM) questionnaire.  © 2021 Association for Computing Machinery.",history visualization; Interaction history; narrative; system log; visual analytics,Codes (symbols); Data handling; Data visualization; Information analysis; Trace analysis; Complex data; Dynamic data; Event-building; High complexity; High dimensionality; History visualization; Interaction history; Narrative; System log; Visual analytics; Visualization
Effect of Adaptive Guidance and Visualization Literacy on Gaze Attentive Behaviors and Sequential Patterns on Magazine-Style Narrative Visualizations,2021,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126017718&doi=10.1145%2f3447992&partnerID=40&md5=113878e7879a395e1b5699b30f16dd2e,"We study the effectiveness of adaptive interventions at helping users process textual documents with embedded visualizations, a form of multimodal documents known as Magazine-Style Narrative Visualizations (MSNVs). The interventions are meant to dynamically highlight in the visualization the datapoints that are described in the textual sentence currently being read by the user, as captured by eye-tracking. These interventions were previously evaluated in two user studies that involved 98 participants reading excerpts of real-world MSNVs during a 1-hour session. Participants' outcomes included their subjective feedback about the guidance, and well as their reading time and score on a set of comprehension questions. Results showed that the interventions can increase comprehension of the MSNV excerpts for users with lower levels of a cognitive skill known as visualization literacy. In this article, we aim to further investigate this result by leveraging eye-tracking to analyze in depth how the participants processed the interventions depending on their levels of visualization literacy. We first analyzed summative gaze metrics that capture how users process and integrate the key components of the narrative visualizations. Second, we mined the salient patterns in the users' scanpaths to contextualize how users sequentially process these components. Results indicate that the interventions succeed in guiding attention to salient components of the narrative visualizations, especially by generating more transitions between key components of the visualization (i.e., datapoints, labels, and legend), as well as between the two modalities (text and visualization). We also show that the interventions help users with lower levels of visualization literacy to better map datapoints to the legend, which likely contributed to their improved comprehension of the documents. These findings shed light on how adaptive interventions help users with different levels of visualization literacy, informing the design of personalized narrative visualizations.  © 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM.",adaptive visualization; eye-tracking; gaze metrics; guidance; Narrative visualization; pattern mining; scanpath; visualization literacy,Visualization; Adaptive visualization; Datapoints; Eye-tracking; Gaze metric; Guidance; Magazine style; Narrative visualization; Pattern mining; Scan path; Visualization literacy; Eye tracking
Special Issue on Interactive Visual Analytics for Making Explainable and Accountable Decisions,2021,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139233615&doi=10.1145%2f3471903&partnerID=40&md5=1df64cf3bbd7c322d397f6365a603699,[No abstract available],human-data interaction; intelligent systems; visual analytics; Visualization,
QuestionComb: A Gamification Approach for the Visual Explanation of Linguistic Phenomena through Interactive Labeling,2021,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134012452&doi=10.1145%2f3429448&partnerID=40&md5=ac7919b09c57b1318d0433a707730b75,"Linguistic insight in the form of high-level relationships and rules in text builds the basis of our understanding of language. However, the data-driven generation of such structures often lacks labeled resources that can be used as training data for supervised machine learning. The creation of such ground-truth data is a time-consuming process that often requires domain expertise to resolve text ambiguities and characterize linguistic phenomena. Furthermore, the creation and refinement of machine learning models is often challenging for linguists as the models are often complex, in-transparent, and difficult to understand. To tackle these challenges, we present a visual analytics technique for interactive data labeling that applies concepts from gamification and explainable Artificial Intelligence (XAI) to support complex classification tasks. The visual-interactive labeling interface promotes the creation of effective training data. Visual explanations of learned rules unveil the decisions of the machine learning model and support iterative and interactive optimization. The gamification-inspired design guides the user through the labeling process and provides feedback on the model performance. As an instance of the proposed technique, we present QuestionComb, a workspace tailored to the task of question classification (i.e., in information-seeking vs. non-information-seeking questions). Our evaluation studies confirm that gamification concepts are beneficial to engage users through continuous feedback, offering an effective visual analytics technique when combined with active learning and XAI.  © 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM.",active learning; explainable artificial intelligence; gamification; Visual interactive labeling,Classification (of information); High level languages; Information use; Supervised learning; Visualization; Active Learning; Analytic technique; Explainable artificial intelligence; Gamification; Interactive labeling; Linguistic phenomena; Machine learning models; Training data; Visual analytics; Visual interactive labeling; Iterative methods
VADAF: Visualization for Abnormal Client Detection and Analysis in Federated Learning,2021,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122326462&doi=10.1145%2f3426866&partnerID=40&md5=f67c0165296d19253b0236cec203d576,"Federated Learning (FL) provides a powerful solution to distributed machine learning on a large corpus of decentralized data. It ensures privacy and security by performing computation on devices (which we refer to as clients) based on local data to improve the shared global model. However, the inaccessibility of the data and the invisibility of the computation make it challenging to interpret and analyze the training process, especially to distinguish potential client anomalies. Identifying these anomalies can help experts diagnose and improve FL models. For this reason, we propose a visual analytics system, VADAF, to depict the training dynamics and facilitate analyzing potential client anomalies. Specifically, we design a visualization scheme that supports massive training dynamics in the FL environment. Moreover, we introduce an anomaly detection method to detect potential client anomalies, which are further analyzed based on both the client model's visual and objective estimation. Three case studies have demonstrated the effectiveness of our system in understanding the FL training process and supporting abnormal client detection and analysis.  © 2021 Association for Computing Machinery.",anomaly detection; Federated learning; visual analytics,Anomaly detection; Anomaly detection; Decentralised; Distributed machine learning; Federated learning; Global models; Large corpora; Local data; Privacy and security; Training process; Visual analytics; Visualization
AOI-shapes: An Efficient Footprint Algorithm to Support Visualization of User-defined Urban Areas of Interest,2021,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139253854&doi=10.1145%2f3431817&partnerID=40&md5=e5e01048e1cd61e6ffd42eb3c6fa9aaf,"Understanding urban areas of interest (AOIs) is essential in many real-life scenarios, and such AOIs can be computed based on the geographic points that satisfy user queries. In this article, we study the problem of efficient and effective visualization of user-defined urban AOIs in an interactive manner. In particular, we first define the problem of user-defined AOI visualization based on a real estate data visualization scenario, and we illustrate why a novel footprint method is needed to support the visualization. After extensively reviewing existing ""footprint""methods, we propose a parameter-free footprint method, named AOI-shapes, to capture the boundary information of a user-defined urban AOI. Next, to allow interactive query refinements by the user, we propose two efficient and scalable algorithms to incrementally generate urban AOIs by reusing existing visualization results. Finally, we conduct extensive experiments with both synthetic and real-world datasets to demonstrate the quality and efficiency of the proposed methods.  © 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM.",AOI-shapes; footprint; geographic visualization; incremental algorithm,Query processing; Visualization; Area of interest; Area of interest-shape; Boundary information; Footprint; Geographic visualization; Geographics; Incremental algorithm; Real estate data; Urban areas; User query; Data visualization
MI3: Machine-initiated Intelligent Interaction for Interactive Classification and Data Reconstruction,2021,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128050390&doi=10.1145%2f3412848&partnerID=40&md5=967869aca45f0d40f45f9cc7d1f50804,"In many applications, while machine learning (ML) can be used to derive algorithmic models to aid decision processes, it is often difficult to learn a precise model when the number of similar data points is limited. One example of such applications is data reconstruction from historical visualizations, many of which encode precious data, but their numerical records are lost. On the one hand, there is not enough similar data for training an ML model. On the other hand, manual reconstruction of the data is both tedious and arduous. Hence, a desirable approach is to train an ML model dynamically using interactive classification, and hopefully, after some training, the model can complete the data reconstruction tasks with less human interference. For this approach to be effective, the number of annotated data objects used for training the ML model should be as small as possible, while the number of data objects to be reconstructed automatically should be as large as possible. In this article, we present a novel technique for the machine to initiate intelligent interactions to reduce the user's interaction cost in interactive classification tasks. The technique of machine-initiated intelligent interaction (MI3) builds on a generic framework featuring active sampling and default labeling. To demonstrate the MI3 approach, we use the well-known cholera map visualization by John Snow as an example, as it features three instances of MI3 pipelines. The experiment has confirmed the merits of the MI3 approach.  © 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM.",active learning; data labeling; data reconstruction; historical visualization; Intelligent user interface; interaction reduction; interactive classification,Classification (of information); Learning systems; Machine learning; User interfaces; Visualization; % reductions; Active Learning; Data labelling; Data reconstruction; Historical visualization; Intelligent interactions; Intelligent User Interfaces; Interaction reduction; Interactive classification; Machine learning models; Data visualization
Expressive Cognitive Architecture for a Curious Social Robot,2021,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111442584&doi=10.1145%2f3451531&partnerID=40&md5=f6a1ef74aedd15e8b2eee002d678cb9c,"Artificial curiosity, based on developmental psychology concepts wherein an agent attempts to maximize its learning progress, has gained much attention in recent years. Similarly, social robots are slowly integrating into our daily lives, in schools, factories, and in our homes. In this contribution, we integrate recent advances in artificial curiosity and social robots into a single expressive cognitive architecture. It is composed of artificial curiosity and social expressivity modules and their unique link, i.e., the robot verbally and non-verbally communicates its internally estimated learning progress, or learnability, to its human companion. We implemented this architecture in an interaction where a fully autonomous robot took turns with a child trying to select and solve tangram puzzles on a tablet. During the curious robot's turn, it selected its estimated most learnable tangram to play, communicated its selection to the child, and then attempted at solving it. We validated the implemented architecture and showed that the robot learned, estimated its learnability, and improved when its selection was based on its learnability estimation. Moreover, we ran a comparison study between curious and non-curious robots, and showed that the robot's curiosity-based behavior influenced the child's selections. Based on the artificial curiosity module of the robot, we have formulated an equation that estimates each child's moment-by-moment curiosity based on their selections. This analysis revealed an overall significant decrease in estimated curiosity during the interaction. However, this drop in estimated curiosity was significantly larger with the non-curious robot, compared to the curious one. These results suggest that the new architecture is a promising new approach to integrate state-of-the-art curiosity-based algorithms to the growing field of social robots. © 2021 Association for Computing Machinery.",Artificial curiosity; child-robot interaction; social robots,Artificial intelligence; Cognitive architectures; Comparison study; Developmental psychology; Implemented architectures; Learnability; Learning progress; New approaches; State of the art; Social robots
Artificial Intelligence for Modeling Complex Systems: Taming the Complexity of Expert Models to Improve Decision Making,2021,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111414734&doi=10.1145%2f3453172&partnerID=40&md5=8928b341abf5c012e9cdf694177a5254,"Major societal and environmental challenges involve complex systems that have diverse multi-scale interacting processes. Consider, for example, how droughts and water reserves affect crop production and how agriculture and industrial needs affect water quality and availability. Preventive measures, such as delaying planting dates and adopting new agricultural practices in response to changing weather patterns, can reduce the damage caused by natural processes. Understanding how these natural and human processes affect one another allows forecasting the effects of undesirable situations and study interventions to take preventive measures. For many of these processes, there are expert models that incorporate state-of-the-art theories and knowledge to quantify a system's response to a diversity of conditions. A major challenge for efficient modeling is the diversity of modeling approaches across disciplines and the wide variety of data sources available only in formats that require complex conversions. Using expert models for particular problems requires integration of models with third-party data as well as integration of models across disciplines. Modelers face significant heterogeneity that requires resolving semantic, spatiotemporal, and execution mismatches, which are largely done by hand today and may take more than 2 years of effort. We are developing a modeling framework that uses artificial intelligence (AI) techniques to reduce modeling effort while ensuring utility for decision making. Our work to date makes several innovative contributions: (1) an intelligent user interface that guides analysts to frame their modeling problem and assists them by suggesting relevant choices and automating steps along the way; (2) semantic metadata for models, including their modeling variables and constraints, that ensures model relevance and proper use for a given decision-making problem; and (3) semantic representations of datasets in terms of modeling variables that enable automated data selection and data transformations. This framework is implemented in the MINT (Model INTegration) framework, and currently includes data and models to analyze the interactions between natural and human systems involving climate, water availability, agricultural production, and markets. Our work to date demonstrates the utility of AI techniques to accelerate modeling to support decision-making and uncovers several challenging directions for future work. © 2021 Association for Computing Machinery.",integrated modeling; Intelligent user interfaces; model metadata; regional-level decision-making; remote sensing data,Agricultural robots; Climate models; Cultivation; Data integration; Decision making; Integration; Semantics; User interfaces; Water quality; Agricultural practices; Agricultural productions; Decision-making problem; Environmental challenges; Intelligent User Interfaces; Interacting process; Preventive measures; Semantic representation; Artificial intelligence
"Understanding, Discovering, and Mitigating Habitual Smartphone Use in Young Adults",2021,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111410776&doi=10.1145%2f3447991&partnerID=40&md5=340fb0642c5c62f303b07e40caacc7d9,"People, especially young adults, often use their smartphones out of habit: They compulsively browse social networks, check emails, and play video-games with little or no awareness at all. While previous studies analyzed this phenomena qualitatively, e.g., by showing that users perceive it as meaningless and addictive, yet our understanding of how to discover smartphone habits and mitigate their disruptive effects is limited. Being able to automatically assess habitual smartphone use, in particular, might have different applications, e.g., to design better ""digital wellbeing""solutions for mitigating meaningless habitual use. To close this gap, we first define a data analytic methodology based on clustering and association rules mining to automatically discover complex smartphone habits from mobile usage data. We assess the methodology over more than 130,000 phone usage sessions collected from users aged between 16 and 33, and we show evidence that smartphone habits of young adults can be characterized by various types of links between contextual situations and usage sessions, which are highly diversified and differently perceived across users. We then apply the proposed methodology in Socialize, a digital wellbeing app that (i) monitors habitual smartphone behaviors in real time and (ii) uses proactive notifications and just-in-time reminders to encourage users to avoid any identified smartphone habits they consider as meaningless. An in-the-wild study with 20 users (ages 19-31) demonstrates that Socialize can assist young adults in better controlling their smartphone usage with a significant reduction of their unwanted smartphone habits. © 2021 Association for Computing Machinery.",digital wellbeing; smartphone habits; Smartphone usage,Artificial intelligence; Association rules mining; Disruptive effects; Just in time; Mobile usage; Real time; Video game; Wellbeing; Young adults; Smartphones
Humanized Recommender Systems: State-of-the-art and Research Issues,2021,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111465841&doi=10.1145%2f3446906&partnerID=40&md5=b1a430285590f12ee21776b515e45267,"Psychological factors such as personality, emotions, social connections, and decision biases can significantly affect the outcome of a decision process. These factors are also prevalent in the existing literature related to the inclusion of psychological aspects in recommender system development. Personality and emotions of users have strong connections with their interests and decision-making behavior. Hence, integrating these factors into recommender systems can help to better predict users' item preferences and increase the satisfaction with recommended items. In scenarios where decisions are made by groups (e.g., selecting a tourism destination to visit with friends), group composition and social connections among group members can affect the outcome of a group decision. Decision biases often occur in a recommendation process, since users usually apply heuristics when making a decision. These biases can result in low-quality decisions. In this article, we provide a rigorous review of existing research on the influence of the mentioned psychological factors on recommender systems. These factors are not only considered in single-user recommendation scenarios but, importantly, also in group recommendation ones, where groups of users are involved in a decision-making process. We include working examples to provide a deeper understanding of how to take into account these factors in recommendation processes. The provided examples go beyond single-user recommendation scenarios by also considering specific aspects of group recommendation settings. © 2021 Association for Computing Machinery.",decision biases; group dynamics; group recommender systems; human decision making; psychological factors; Recommender systems,Decision making; Decision making process; Decision process; Decision-making behaviors; Group composition; Group recommendations; Psychological Aspects; Psychological factors; Social connection; Recommender systems
Emerging ExG-based NUI Inputs in Extended Realities: A Bottom-up Survey,2021,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111464108&doi=10.1145%2f3457950&partnerID=40&md5=7b4a9b60db35f06a31d201b0d50887bd,"Incremental and quantitative improvements of two-way interactions with extended realities (XR) are contributing toward a qualitative leap into a state of XR ecosystems being efficient, user-friendly, and widely adopted. However, there are multiple barriers on the way toward the omnipresence of XR; among them are the following: computational and power limitations of portable hardware, social acceptance of novel interaction protocols, and usability and efficiency of interfaces. In this article, we overview and analyse novel natural user interfaces based on sensing electrical bio-signals that can be leveraged to tackle the challenges of XR input interactions. Electroencephalography-based brain-machine interfaces that enable thought-only hands-free interaction, myoelectric input methods that track body gestures employing electromyography, and gaze-tracking electrooculography input interfaces are the examples of electrical bio-signal sensing technologies united under a collective concept of ExG. ExG signal acquisition modalities provide a way to interact with computing systems using natural intuitive actions enriching interactions with XR. This survey will provide a bottom-up overview starting from (i) underlying biological aspects and signal acquisition techniques, (ii) ExG hardware solutions, (iii) ExG-enabled applications, (iv) discussion on social acceptance of such applications and technologies, as well as (v) research challenges, application directions, and open problems; evidencing the benefits that ExG-based Natural User Interfaces inputs can introduce to the area of XR. © 2021 Association for Computing Machinery.",Electroencephalography; Electromyography; Extended Reality; Mobile augmented reality; Silent Speech Interfaces; user interactions,Brain computer interface; Electroencephalography; Electrophysiology; Eye tracking; Signal processing; Social aspects; Surveys; Brain machine interface; Hands-free interactions; Hardware solutions; Inter-action protocols; Natural user interfaces; Research challenges; Signal acquisitions; Two-way interaction; User interfaces
Socially Aware Navigation: A Non-linear Multi-objective Optimization Approach,2021,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111423681&doi=10.1145%2f3453445&partnerID=40&md5=1ed3a3c1720e5e445c75a8e6f00a6b4f,"Mobile robots are increasingly populating homes, hospitals, shopping malls, factory floors, and other human environments. Human society has social norms that people mutually accept; obeying these norms is an essential signal that someone is participating socially with respect to the rest of the population. For robots to be socially compatible with humans, it is crucial for robots to obey these social norms. In prior work, we demonstrated a Socially-Aware Navigation (SAN) planner, based on Pareto Concavity Elimination Transformation (PaCcET), in a hallway scenario, optimizing two objectives so the robot does not invade the personal space of people. This article extends our PaCcET-based SAN planner to multiple scenarios with more than two objectives. We modified the Robot Operating System's (ROS) navigation stack to include PaCcET in the local planning task. We show that our approach can accommodate multiple Human-Robot Interaction (HRI) scenarios. Using the proposed approach, we achieved successful HRI in multiple scenarios such as hallway interactions, an art gallery, waiting in a queue, and interacting with a group. We implemented our method on a simulated PR2 robot in a 2D simulator (Stage) and a pioneer-3DX mobile robot in the real-world to validate all the scenarios. A comprehensive set of experiments shows that our approach can handle multiple interaction scenarios on both holonomic and non-holonomic robots; hence, it can be a viable option for a Unified Socially-Aware Navigation (USAN). © 2021 Association for Computing Machinery.",Human-robot interaction; socially-assistive robotics; socially-aware navigation,Air navigation; Automata theory; Mobile robots; Multiobjective optimization; Navigation; Robot programming; Elimination transformations; Factory floors; Human environment; Human robot Interaction (HRI); Multiple interactions; Non-holonomic robot; Personal spaces; Robot operating system; Social robots
Photo Sequences of Varying Emotion: Optimization with a Valence-Arousal Annotated Dataset,2021,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111446816&doi=10.1145%2f3458844&partnerID=40&md5=de4cf9626665d99bd1292b9acc63394c,"Synthesizing photo products such as photo strips and slideshows using a database of images is a time-consuming and tedious process that requires significant manual work. To overcome this limitation, we developed a method that automatically synthesizes photo sequences based on several design parameters. Our method considers the valence and arousal ratings of images in conjunction with parameters related to both the visual consistency of the synthesized photo sequence and the progression of valence and arousal throughout the photo sequence. Our method encodes valence, arousal, and visual consistency parameters as cost terms into a total cost function while applying a Markov chain Monte Carlo optimization techniques called simulated annealing to synthesize the photo sequence based on user-defined target objectives in a few seconds. As our method was developed for the synthesis of photo sequences using the valence-arousal emotional model, a user study was conducted to evaluate the efficacy of the synthesized photo sequences in triggering valence-arousal ratings as expected. Our results indicate that the proposed method synthesizes photo sequences in which valence and arousal dimensions are perceived as expected by participants; however, valence may be more appropriately perceived than arousal. © 2021 Association for Computing Machinery.",arousal; optimization; photo sequence; simulated annealing; Valence; visual consistency,Cost functions; Markov chains; Monte Carlo methods; Simulated annealing; Design parameters; Emotional models; Markov Chain Monte-Carlo; Photoproducts; Total cost function; User study; Visual consistency; Image processing
Predicting Visual Search Task Success from Eye Gaze Data as a Basis for User-Adaptive Information Visualization Systems,2021,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110737317&doi=10.1145%2f3446638&partnerID=40&md5=b89aa198b939f06d2316f727665dd46f,"Information visualizations are an efficient means to support the users in understanding large amounts of complex, interconnected data; user comprehension, however, depends on individual factors such as their cognitive abilities. The research literature provides evidence that user-adaptive information visualizations positively impact the users' performance in visualization tasks. This study attempts to contribute toward the development of a computational model to predict the users' success in visual search tasks from eye gaze data and thereby drive such user-adaptive systems. State-of-the-art deep learning models for time series classification have been trained on sequential eye gaze data obtained from 40 study participants' interaction with a circular and an organizational graph. The results suggest that such models yield higher accuracy than a baseline classifier and previously used models for this purpose. In particular, a Multivariate Long Short Term Memory Fully Convolutional Network shows encouraging performance for its use in online user-adaptive systems. Given this finding, such a computational model can infer the users' need for support during interaction with a graph and trigger appropriate interventions in user-adaptive information visualization systems. This facilitates the design of such systems since further interaction data like mouse clicks is not required. © 2021 Association for Computing Machinery.",Eye tracking; individual differences; time series classification; user-adaptation,Adaptive systems; Computation theory; Computational methods; Convolutional neural networks; Data visualization; Deep learning; Digital storage; Information analysis; Information systems; Mammals; Online systems; Visualization; Cognitive ability; Computational model; Convolutional networks; Individual factors; Information visualization; State of the art; Time series classifications; User-adaptive systems; Search engines
"The shoutcasters, the game enthusiasts, and the AI: Foraging for explanations of real-time strategy players",2021,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104940628&doi=10.1145%2f3396047&partnerID=40&md5=7dcb9d63da80234205f046c555aedb04,"Assessing and understanding intelligent agents is a difficult task for users who lack an AI background. ""Explainable AI""(XAI) aims to address this problem, but what should be in an explanation? One route toward answering this question is to turn to theories of how humans try to obtain information they seek. Information Foraging Theory (IFT) is one such theory. In this article, we present a series of studies1 using IFT: the first investigates how expert explainers supply explanations in the RTS domain, the second investigates what explanations domain experts demand from agents in the RTS domain, and the last focuses on how both populations try to explain a state-of-the-art AI. Our results show that RTS environments like StarCraft offer so many options that change so rapidly, foraging tends to be very costly. Ways foragers attempted to manage such costs included ""satisficing""approaches to reduce their cognitive load, such as focusing more on What information than on Why information, strategic use of language to communicate a lot of nuanced information in a few words, and optimizing their environment when possible to make their most valuable information patches readily available. Further, when a real AI entered the picture, even very experienced domain experts had difficulty understanding and judging some of the AI's unconventional behaviors. Finally, our results reveal ways Information Foraging Theory can inform future XAI interactive explanation environments, and also how XAI can inform IFT. © 2021 Association for Computing Machinery.",empirical studies with humans; Explainable AI; humans evaluating AI; information foraging; qualitative analysis; StarCraft,Intelligent agents; Cognitive loads; Domain experts; Information foraging; Real time strategies; Satisficing; State of the art; Artificial intelligence
"Theoretical, measured, and subjective responsibility in aided decision making",2021,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104953372&doi=10.1145%2f3425732&partnerID=40&md5=410a3f88661a1bd542ea5010e36b0ca7,"When humans interact with intelligent systems, their causal responsibility for outcomes becomes equivocal. We analyze the descriptive abilities of a newly developed responsibility quantification model (ResQu) to predict actual human responsibility and perceptions of responsibility in the interaction with intelligent systems. In two laboratory experiments, participants performed a classification task. They were aided by classification systems with different capabilities. We compared the predicted theoretical responsibility values to the actual measured responsibility participants took on and to their subjective rankings of responsibility. The model predictions were strongly correlated with both measured and subjective responsibility. Participants' behavior with each system was influenced by the system and human capabilities, but also by the subjective perceptions of these capabilities and the perception of the participant's own contribution. A bias existed only when participants with poor classification capabilities relied less than optimally on a system that had superior classification capabilities and assumed higher-than-optimal responsibility. The study implies that when humans interact with advanced intelligent systems, with capabilities that greatly exceed their own, their comparative causal responsibility will be small, even if formally the human is assigned major roles. Simply putting a human into the loop does not ensure that the human will meaningfully contribute to the outcomes. The results demonstrate the descriptive value of the ResQu model to predict behavior and perceptions of responsibility by considering the characteristics of the human, the intelligent system, the environment, and some systematic behavioral biases. The ResQu model is a new quantitative method that can be used in system design and can guide policy and legal decisions regarding human responsibility in events involving intelligent systems. © 2021 Association for Computing Machinery.",alert systems; Artificial intelligence (AI); decision making; Information Theory; Responsibility,Decision making; Forecasting; Classification system; Classification tasks; Laboratory experiments; Model prediction; Quantification model; Quantitative method; Subjective perceptions; Subjective ranking; Intelligent systems
I know what you know: What hand movements reveal about domain expertise,2021,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104929817&doi=10.1145%2f3423049&partnerID=40&md5=f164f1125298a8c9e1add6c6c459c415,"This research investigates whether students' level of domain expertise can be detected during authentic learning activities by analyzing their physical activity patterns. More expert students reduced their manual activity by a substantial 50%, which was evident in fine-grained signal analyses and total rate of gesturing. The quality of experts' discrete hand movements also averaged shorter in distance, briefer in duration, and slower in velocity than those of non-experts. Interestingly, experts adapted by nearly eliminating gestures on easier problems, while selectively increasing them on harder ones. They also strategically produced 62% more iconic gestures, which serve to retain spatial information in working memory while extracting inferences required to solve problems correctly. These findings highlight the close relation between hand movements and mental state and, more specifically, that hand movements provide an unusually clear window on students' level of domain expertise. Embodied Cognition and Limited Resource theories only partially account for the present findings, which specify future directions for theoretical work. © 2020 Copyright held by the owner/author(s). Publication rights licensed to ACM.",domain expertise; gestures; manual movements; mathematics; multimodal learning analytics; OpenPose; Physical activity patterns; prediction of cognitive state,Artificial intelligence; Authentic learning; Domain expertise; Embodied cognition; Hand movement; Iconic gestures; Physical activity patterns; Spatial informations; Working memory; Students
Holistic transfer to rank for Top-N recommendation,2021,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105004102&doi=10.1145%2f3434360&partnerID=40&md5=225cdfcf5d7504608a23ccef5fb4a7d5,"Recommender systems have been a valuable component in various online services such as e-commerce and entertainment. To provide an accurate top-N recommendation list of items for each target user, we have to answer a very basic question of how to model users' feedback effectively. In this article, we focus on studying users' explicit feedback, which is usually assumed to contain more preference information than the counterpart, i.e., implicit feedback. In particular, we follow two very recent transfer to rank algorithms by converting the original feedback to three different but related views of examinations, scores, and purchases, and then propose a novel solution called holistic transfer to rank (HoToR), which is able to address the uncertainty challenge and the inconvenience challenge in the existing works. More specifically, we take the rating scores as a weighting strategy to alleviate the uncertainty of the examinations, and we design a holistic one-stage solution to address the inconvenience of the two/three-stage training and prediction procedures in previous works. We then conduct extensive empirical studies in a direct comparison with the two closely related transfer learning algorithms and some very competitive factorization- and neighborhood-based methods on three public datasets and find that our HoToR performs significantly better than the other methods in terms of several ranking-oriented evaluation metrics. © 2021 Association for Computing Machinery.",top-N recommendation; transfer learning; Transfer to rank,Electronic commerce; Online systems; Transfer learning; Empirical studies; Evaluation metrics; Explicit feedback; Implicit feedback; Neighborhood-based method; On-line service; Preference information; Valuable component; Learning algorithms
A real-time interactive visualizer for large classroom,2021,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104963465&doi=10.1145%2f3418529&partnerID=40&md5=cf799289cc78b3ae8bdd79e587a7d16f,"In improving the teaching and learning experience in a classroom environment, it is crucial for a teacher to have a fair idea about the students who need help during a lecture. However, teachers of large classes usually face difficulties in identifying the students who are in a critical state. The current methods for classroom visualization are limited in showing both the status and location of a large number of students in a limited display area. Additionally, comprehension of the states adds cognitive load on the teacher working in a time-constrained classroom environment. In this article, we propose a two-level visualizer for large classrooms to address the challenges. In the first level, the visualizer generates a colored matrix representation of the classroom. The colored matrix is a quantitative illustration of the status of the class in terms of student clusters. We use three colors: red, yellow, and green, indicating the most critical, less critical, and the normal cluster on the screen, respectively. With tap/click on the first level, detailed information for a cluster is visualized as the second level. We conducted extensive studies for our visualizer in a simulated classroom with 12 tasks and 27 teacher participants. The results show that the visualizer is efficient and usable. © 2021 Association for Computing Machinery.",Blended learning; classroom status; overview+details; sensitive classroom,Critical current density (superconductivity); Classroom environment; Cognitive loads; Critical state; Matrix representation; Real time; Second level; Teaching and learning; Visualizers; Students
On the detection of structural aesthetic defects of android mobile user interfaces with a metrics-based tool,2021,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104958845&doi=10.1145%2f3410468&partnerID=40&md5=84512d9a73d9883c9c8ca6f0c00366aa,"Smartphone users are striving for easy-to-learn and use mobile apps user interfaces. Accomplishing these qualities demands an iterative evaluation of the Mobile User Interface (MUI). Several studies stress the value of providing a MUI with a pleasing look and feel to engaging end-users. The MUI, therefore, needs to be free from all kinds of structural aesthetic defects. Such defects are indicators of poor design decisions interfering with the consistency of a MUI and making it more difficult to use. To this end, we are proposing a tool (Aesthetic Defects DEtection Tool (ADDET)) to determine the structural aesthetic dimension of MUIs. Automating this process is useful to designers in evaluating the quality of their designs. Our approach is composed of two modules. (1) Metrics assessment is based on the static analysis of a tree-structured layout of the MUI. We used 15 geometric metrics (also known as structural or aesthetic metrics) to check various structural properties before a defect is triggered. (2) Defects detection: The manual combination of metrics and defects are time-consuming and user-dependent when determining a detection rule. Thus, we perceive the process of identification of defects as an optimization problem. We aim to automatically combine the metrics related to a particular defect and optimize the accuracy of the rules created by assigning a weight, representing the metric importance in detecting a defect. We conducted a quantitative and qualitative analysis to evaluate the accuracy of the proposed tool in computing metrics and detecting defects. The findings affirm the tool's reliability when assessing a MUI's structural design problems with 71% accuracy. © 2021 Association for Computing Machinery.",Android MUI; automated evaluation; NSGA-II; optimization algorithm; Structural aesthetic defects,Android (operating system); Defects; Quality control; Structural design; Aesthetic metrics; Defects detection; Detecting defects; Iterative evaluation; Mobile user interface; Optimization problems; Quantitative and qualitative analysis; Structural design problems; User interfaces
PRIME: A personalized recommender system for information visualization methods via extendedmatrix completion,2021,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103394374&doi=10.1145%2f3366484&partnerID=40&md5=631022728678bddc239caff55fa8fe60,"Adapting user interface designs for specific tasks performed by different users is a challenging yet important problem. Automatically adapting visualization designs to users and contexts (e.g., tasks, display devices, environments, etc.) can theoretically improve human-computer interaction to acquire insights from complex datasets. However, effectiveness of any specific visualization is moderated by individual differences in knowledge, skills, and abilities for different contexts. A modeling framework called Personalized Recommender System for Information visualization Methods via Extended matrix completion (PRIME) is proposed for recommending the optimal visualization designs for individual users in different contexts. PRIME quantitatively models covariates (e.g., psychological and behavioral measurements) to predict recommendation scores (e.g., perceived complexity, mental workload, etc.) for users to adapt the visualization specific to the context. An evaluation study was conducted and showed that PRIME can achieve satisfactory recommendation accuracy for adapting visualization, even when there are limited historical data. PRIME can make accurate recommendations even for new users or new tasks based on historical wearable sensor signals and recommendation scores. This capability contributes to designing a new generation of visualization systems that will adapt to users' states. PRIME can support researchers in reducing the sample size requirements to quantify individual differences, and practitioners in adapting visualizations according to user states and contexts. © 2021 Association for Computing Machinery.",Electroencephalography (EEG); eye tracking; information visualization; matrix completion; recommender systems,Behavioral research; Data visualization; Display devices; Human computer interaction; Information systems; User interfaces; Visualization; Individual Differences; Information visualization; Matrix completion; Personalized recommender systems; Recommendation accuracy; User interface designs; Visualization designs; Visualization system; Recommender systems
Exploring the Role of Common Model of Cognition in Designing Adaptive Coaching Interactions for Health Behavior Change,2021,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104291368&doi=10.1145%2f3375790&partnerID=40&md5=9a6d94d411a05badea876d0ddba9f0fe,"Our research aims to develop intelligent collaborative agents that are human-aware: They can model, learn, and reason about their human partner's physiological, cognitive, and affective states. In this article, we study how adaptive coaching interactions can be designed to help people develop sustainable healthy behaviors. We leverage the common model of cognition (CMC) [31] as a framework for unifying several behavior change theories that are known to be useful in human-human coaching. We motivate a set of interactive system desiderata based on the CMC-based view of behavior change. Then, we propose PARCoach, an interactive system that addresses the desiderata. PARCoach helps a trainee pick a relevant health goal, set an implementation intention, and track their behavior. During this process, the trainee identifies a specific goal-directed behavior as well as the situational context in which they will perform it. PARCCoach uses this information to send notifications to the trainee, reminding them of their chosen behavior and the context. We report the results from a 4-week deployment with 60 participants. Our results support the CMC-based view of behavior change and demonstrate that the desiderata for proposed interactive system design is useful in producing behavior change. © 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Designing adaptive coaching interactions,Physiological models; Psychophysiology; Affective state; Behavior change; Collaborative agents; Goal-directed behavior; Health behaviors; Implementation intentions; Interactive system; Situational context; Behavioral research
Introduction to the Special Issue on Highlights of ACM Intelligent User Interface (IUI) 2019,2020,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097305070&doi=10.1145%2f3429946&partnerID=40&md5=b04d10bd9d03dceec466f92e3894d428,[No abstract available],,
Learning Context-dependent Personal Preferences for Adaptive Recommendation,2020,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097254195&doi=10.1145%2f3359755&partnerID=40&md5=eeadfa6f060c80cd997f39055349079d,"We propose two online-learning algorithms for modeling the personal preferences of users of interactive systems. The proposed algorithms leverage user feedback to estimate user behavior and provide personalized adaptive recommendation for supporting context-dependent decision-making. We formulate preference modeling as online prediction algorithms over a set of learned policies, i.e., policies generated via supervised learning with interaction and context data collected from previous users. The algorithms then adapt to a target user by learning the policy that best predicts that user's behavior and preferences. We also generalize the proposed algorithms for a more challenging learning case in which they are restricted to a limited number of trained policies at each timestep, i.e., for mobile settings with limited resources. While the proposed algorithms are kept general for use in a variety of domains, we developed an image-filter-selection application. We used this application to demonstrate how the proposed algorithms can quickly learn to match the current user's selections. Based on these evaluations, we show that (1) the proposed algorithms exhibit better prediction accuracy compared to traditional supervised learning and bandit algorithms, (2) our algorithms are robust under challenging limited prediction settings in which a smaller number of expert policies is assumed. Finally, we conducted a user study to demonstrate how presenting users with the prediction results of our algorithms significantly improves the efficiency of the overall interaction experience.  © 2020 ACM.",Online preference modeling; Context-dependent decision-making,Behavioral research; Decision making; Forecasting; Learning systems; Online systems; Supervised learning; Context dependent; Interaction experiences; Interactive system; Learning context; Online learning algorithms; Online prediction; Prediction accuracy; Preference modeling; Learning algorithms
Special Issue on Data-Driven Personality Modeling for Intelligent Human-Computer Interaction,2020,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097244487&doi=10.1145%2f3402522&partnerID=40&md5=8a5db4f168d12b9da27d969ac2306149,[No abstract available],machine learning; Personality modeling; personalization; user modeling,
How Impactful Is Presentation in Email? The Effect of Avatars and Signatures,2020,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097253215&doi=10.1145%2f3345641&partnerID=40&md5=f9130a1533403347bfc6618dfe15a3a6,"A primary well-controlled study of 900 participants found that personal presentation choices in professional emails (non-content changes like Profile Avatar 8 Signature) impact the recipient's perception of the sender's personality and the quality of the email itself. By understanding the role these choices play, employees can gain better control over how they influence the recipient of their messages. Results further indicate that although some variations can positively impact the recipient's view of the sender, these same variations often also have negative side effects. This implies that many seemingly innocuous presentation decisions should be made in the context of who is receiving the email, and if these effects negatively impact the content of the message. For example, although statements in a Signature about the email having been written on a phone are included to preemptively apologize for typing mistakes, this causes the sender to appear less agreeable, less conscientious, and less open, and the email itself appears less well written and more poorly formatted. This is surprising given that the email itself was not changed in the study.  © 2020 ACM.",avatar; Big Five Index; Email; formatting; halo effect; layout; personality; signature,Quality control; Negative side effects; Electronic mail
An autonomous cognitive empathy model responsive to users' facial emotion expressions,2020,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097225306&doi=10.1145%2f3341198&partnerID=40&md5=6b3056773f580546273a9ec06bea4681,"Successful social robot services depend on how robots can interact with users. The effective service can be obtained through smooth, engaged, and humanoid interactions in which robots react properly to a user's affective state. This article proposes a novel Automatic Cognitive Empathy Model, ACEM, for humanoid robots to achieve longer and more engaged human-robot interactions (HRI) by considering humans' emotions and replying to them appropriately. The proposed model continuously detects the affective states of a user based on facial expressions and generates desired, either parallel or reactive, empathic behaviors that are already adapted to the user's personality. Users' affective states are detected using a stacked autoencoder network that is trained and tested on the RAVDESS dataset. The overall proposed empathic model is verified throughout an experiment, where different emotions are triggered in participants and then empathic behaviors are applied based on proposed hypothesis. The results confirm the effectiveness of the proposed model in terms of related social and friendship concepts that participants perceived during interaction with the robot.  © 2020 ACM.",adaptive interaction; Empathy; facial emotion detection; human robot interaction; non-verbal behavior; social robots,Anthropomorphic robots; Affective state; Auto encoders; Facial emotions; Facial Expressions; Human robot Interaction (HRI); Humanoid interaction; Humanoid robot; Users' affective state; Human robot interaction
Modeling dyslexic students' motivation for enhanced learning in E-learning systems,2020,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097212768&doi=10.1145%2f3341197&partnerID=40&md5=7cc82829f3892b7ae28b77724471e065,"E-Learning systems can support real-time monitoring of learners' learning desires and effects, thus offering opportunities for enhanced personalized learning. Recognition of the determinants of dyslexic users' motivation to use e-learning systems is important to help developers improve the design of e-learning systems and educators direct their efforts to relevant factors to enhance dyslexic students' motivation. Existing research has rarely attempted to model dyslexic users' motivation in e-learning context from a comprehensive perspective. The present work has conceived a hybrid approach, namely, combining the strengths of qualitative and quantitative analysis methods, to motivation modeling. It examines a variety of factors that affect dyslexic students' motivation to engage in e-learning systems from psychological, behavioral, and technical perspectives, and establishes their interrelationships. Specifically, the study collects data from a multi-item Likert-style questionnaire to measure relevant factors for conceptual motivation modeling. It then applies both covariance-based (CB-SEM) and variance-based structural equation modeling (PLS-SEM) approaches to determine the quantitative mapping between dyslexic students' continued use intention and motivational factors, followed by discussions about theoretical findings and design instructions according to our motivation model. Our research has led to a novel motivation model with new constructs of Learning Experience, Reading Experience, Perceived Control, and Perceived Privacy. From both the CB-SEM and PLS-SEM analyses, results on the total effects have indicated consistently that Visual Attractiveness, Reading Experience, and Feedback have the strongest effects on continued use intention.  © 2020 ACM.",Dyslexia; E-learning systems; Motivation modeling,Behavioral research; E-learning; Motivation; Privacy by design; Real time systems; Students; Enhanced learning; Learning experiences; Motivation models; Personalized learning; Qualitative and quantitative analysis; Quantitative mapping; Real time monitoring; Structural equation modeling; Learning systems
Being the Center of Attention,2020,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097217418&doi=10.1145%2f3338245&partnerID=40&md5=4fd3dad6df21f51dcadb650c726a2b98,"This article proposes a novel study on personality recognition using video data from different scenarios. Our goal is to jointly model nonverbal behavioral cues with contextual information for a robust, multi-scenario, personality recognition system. Therefore, we build a novel multi-stream Convolutional Neural Network (CNN) framework, which considers multiple sources of information. From a given scenario, we extract spatio-temporal motion descriptors from every individual in the scene, spatio-temporal motion descriptors encoding social group dynamics, and proxemics descriptors to encode the interaction with the surrounding context. All the proposed descriptors are mapped to the same feature space facilitating the overall learning effort. Experiments on two public datasets demonstrate the effectiveness of jointly modeling the mutual Person-Context information, outperforming the state-of-the art-results for personality recognition in two different scenarios. Last, we present CNN class activation maps for each personality trait, shedding light on behavioral patterns linked with personality attributes.  © 2020 ACM.",CNN networks; nonsocial behavior analysis; Personality recognition; social behaviors analysis,Encoding (symbols); Behavioral patterns; Context information; Contextual information; Learning efforts; Motion descriptors; Personality recognition; Personality traits; State of the art; Convolutional neural networks
Personality Sensing: Detection of Personality Traits Using Physiological Responses to Image and Video Stimuli,2020,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097241147&doi=10.1145%2f3357459&partnerID=40&md5=7a22313602cb173228d2e89ad5cbcced,"Personality detection is an important task in psychology, as different personality traits are linked to different behaviours and real-life outcomes. Traditionally it involves filling out lengthy questionnaires, which is time-consuming, and may also be unreliable if respondents do not fully understand the questions or are not willing to honestly answer them. In this article, we propose a framework for objective personality detection that leverages humans' physiological responses to external stimuli. We exemplify and evaluate the framework in a case study, where we expose subjects to affective image and video stimuli, and capture their physiological responses using non-invasive commercial-grade eye-tracking and skin conductivity sensors. These responses are then processed and used to build a machine learning classifier capable of accurately predicting a wide range of personality traits. We investigate and discuss the performance of various machine learning methods, the most and least accurately predicted traits, and also assess the importance of the different stimuli, features, and physiological signals. Our work demonstrates that personality traits can be accurately detected, suggesting the applicability of the proposed framework for robust personality detection and use by psychology practitioners and researchers, as well as designers of personalised interactive systems.  © 2020 ACM.",eye tracking; field study; framework; GSR; Personality detection,Eye tracking; Machine learning; Physiology; Surveys; Turing machines; Conductivity sensors; External stimulus; Interactive system; Machine learning methods; Personality detections; Personality traits; Physiological response; Physiological signals; Physiological models
Predicting users' movie preference and rating behavior from personality and values,2020,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097236485&doi=10.1145%2f3338244&partnerID=40&md5=acd8fa1045c8af9153fedbe228960606,"In this article, we propose novel techniques to predict a user's movie genre preference and rating behavior from her psycholinguistic attributes obtained from the social media interactions. The motivation of this work comes from various psychological studies that demonstrate that psychological attributes such as personality and values can influence one's decision or choice in real life. In this work, we integrate user interactions in Twitter and IMDb to derive interesting relations between human psychological attributes and their movie preferences. In particular, we first predict a user's movie genre preferences from the personality and value scores of the user derived from her tweets. Second, we also develop models to predict user movie rating behavior from her tweets in Twitter and movie genre and storyline preferences from IMDb. We further strengthen the movie rating model by incorporating the user reviews. In the above models, we investigate the role of personality and values independently and combinedly while predicting movie genre preferences and movie rating behaviors. We find that our combined models significantly improve the accuracy than that of a single model that is built by using personality or values independently. We also compare our technique with the traditional movie genre and rating prediction techniques. The experimental results show that our models are effective in recommending movies to users.  © 2020 ACM.",movie recommendation; Psychological attributes: Personality and values; social medias: Twitter and IMDb,Forecasting; Motion pictures; Social networking (online); Combined model; Movie ratings; Novel techniques; Prediction techniques; Single models; Social media; User interaction; User reviews; Behavioral research
"Bridging the gap between ethics and practice: Guidelines for reliable, safe, and trustworthy human-centered AI systems",2020,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096998670&doi=10.1145%2f3419764&partnerID=40&md5=64b8af777ccd9469712e24c94f60ec19,"This article attempts to bridge the gap between widely discussed ethical principles of Human-centered AI (HCAI) and practical steps for effective governance. Since HCAI systems are developed and implemented in multiple organizational structures, I propose 15 recommendations at three levels of governance: team, organization, and industry. The recommendations are intended to increase the reliability, safety, and trustworthiness of HCAI systems: (1) reliable systems based on sound software engineering practices, (2) safety culture through business management strategies, and (3) trustworthy certification by independent oversight. Software engineering practices within teams include audit trails to enable analysis of failures, software engineering workflows, verification and validation testing, bias testing to enhance fairness, and explainable user interfaces. The safety culture within organizations comes from management strategies that include leadership commitment to safety, hiring and training oriented to safety, extensive reporting of failures and near misses, internal review boards for problems and future plans, and alignment with industry standard practices. The trustworthiness certification comes from industry-wide efforts that include government interventions and regulation, accounting firms conducting external audits, insurance companies compensating for failures, nongovernmental and civil society organizations advancing design principles, and professional organizations and research institutes developing standards, policies, and novel ideas. The larger goal of effective governance is to limit the dangers and increase the benefits of HCAI to individuals, organizations, and society. © 2020 Association for Computing Machinery.",Artificial Intelligence; Design; Human-centered AI; Human-Computer Interaction; Independent oversight; Management strategies; Reliable; Safe; Software engineering practices; Trustworthy,Insurance; Philosophical aspects; Software reliability; Software testing; User interfaces; Verification; Engineering workflows; Government intervention; Leadership commitments; Management strategies; Organizational structures; Professional organization; Software engineering practices; Verification-and-validation; Accident prevention
PolicyFlow: Interpreting Policy Diffusion in Context,2020,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088294357&doi=10.1145%2f3385729&partnerID=40&md5=742e66d82021bb850b20d2618a33ecef,"Stability in social, technical, and financial systems, as well as the capacity of organizations to work across borders, requires consistency in public policy across jurisdictions. The diffusion of laws and regulations across political boundaries can reduce the tension that arises between innovation and consistency. Policy diffusion has been a topic of focus across the social sciences for several decades, but due to limitations of data and computational capacity, researchers have not taken a comprehensive and data-intensive look at the aggregate, cross-policy patterns of diffusion. This work combines visual analytics and text and network analyses to help understand how policies, as represented in digitized text, spread across states. As a result, our approach can quickly guide analysts to progressively gain insights into policy adoption data. We evaluate the effectiveness of our system via case studies with a real-world policy dataset and qualitative interviews with domain experts.  © 2020 ACM.",Cascades; diffusion networks; network inference; policy diffusion; visual analytics,Laws and legislation; Computational capacity; Financial system; Laws and regulations; Policy adoption; Policy diffusions; Political boundaries; Qualitative interviews; Visual analytics; Diffusion
Automatic Detection of Usability Problem Encounters in Think-aloud Sessions,2020,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088295877&doi=10.1145%2f3385732&partnerID=40&md5=f2d308c98a89eb65a209ab238b0c348a,"Think-aloud protocols are a highly valued usability testing method for identifying usability problems. Despite the value of conducting think-aloud usability test sessions, analyzing think-aloud sessions is often time-consuming and labor-intensive. Consequently, previous research has urged the community to develop techniques to support fast-paced analysis. In this work, we took the first step to design and evaluate machine learning (ML) models to automatically detect usability problem encounters based on users' verbalization and speech features in think-aloud sessions. Inspired by recent research that shows subtle patterns in users' verbalizations and speech features tend to occur when they encounter problems, we examined whether these patterns can be utilized to improve the automatic detection of usability problems. We first conducted and recorded think-aloud sessions and then examined the effect of different input features, ML models, test products, and users on usability problem encounters detection. Our work uncovers several technical and user interface design challenges and sets a baseline for automating usability problem detection and integrating such automation into UX practitioners' workflow.  © 2020 ACM.",AI-assisted UX analysis method; machine learning; speech features; Think aloud; usability problem; user experience (UX); verbalization,Speech recognition; Testing; Usability engineering; User interfaces; Automatic Detection; Labor intensive; Recent researches; Speech features; Think-aloud protocol; Usability problems; Usability testing methods; User interface designs; Feature extraction
Designing an AI Health Coach and Studying Its Utility in Promoting Regular Aerobic Exercise,2020,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088316938&doi=10.1145%2f3366501&partnerID=40&md5=4c6d17866edbb063d257e0a5ddd60f05,"Our research aims to develop interactive, social agents that can coach people to learn new tasks, skills, and habits. In this article, we focus on coaching sedentary, overweight individuals (i.e., ""trainees"") to exercise regularly. We employ adaptive goal setting in which the intelligent health coach generates, tracks, and revises personalized exercise goals for a trainee. The goals become incrementally more difficult as the trainee progresses through the training program. Our approach is model-based - the coach maintains a parameterized model of the trainee's aerobic capability that drives its expectation of the trainee's performance. The model is continually revised based on trainee-coach interactions. The coach is embodied in a smartphone application, NutriWalking, which serves as a medium for coach-trainee interaction. We adopt a task-centric evaluation approach for studying the utility of the proposed algorithm in promoting regular aerobic exercise. We show that our approach can adapt the trainee program not only to several trainees with different capabilities but also to how a trainee's capability improves as they begin to exercise more. Experts rate the goals selected by the coach better than other plausible goals, demonstrating that our approach is consistent with clinical recommendations. Further, in a 6-week observational study with sedentary participants, we show that the proposed approach helps increase exercise volume performed each week.  © 2020 ACM.",AI and society; AI for social good; coaching AI; evaluation of interactive AI; Health behavior change; health systems; human-aware AI systems; supporting human learning,Aerobic exercise; Evaluation approach; Model-based OPC; Observational study; Parameterized model; Smart-phone applications; Social agents; Training program; Artificial intelligence
Comparing and Combining Interaction Data and Eye-tracking Data for the Real-time Prediction of User Cognitive Abilities in Visualization Tasks,2020,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088284812&doi=10.1145%2f3301400&partnerID=40&md5=b87b9554155e78de465cd827ce4ed5ed,"Previous work has shown that some user cognitive abilities relevant for processing information visualizations can be predicted from eye-tracking data. Performing this type of user modeling is important for devising visualizations that can detect a user's abilities and adapt accordingly during the interaction. In this article, we extend previous user modeling work by investigating for the first time interaction data as an alternative source to predict cognitive abilities during visualization processing when it is not feasible to collect eye-tracking data. We present an extensive comparison of user models based solely on eye-tracking data, on interaction data, as well as on a combination of the two. Although we found that eye-tracking data generate the most accurate predictions, results show that interaction data can still outperform a majority-class baseline, meaning that adaptation for interactive visualizations could be enabled even when it is not feasible to perform eye tracking, using solely interaction data. Furthermore, we found that interaction data can predict several cognitive abilities with better accuracy at the very beginning of the task than eye-tracking data, which are valuable for delivering adaptation early in the task. We also extend previous work by examining the value of multimodal classifiers combining interaction data and eye-tracking data, with promising results for some of our target user cognitive abilities. Next, we contribute to previous work by extending the type of visualizations considered and the set of cognitive abilities that can be predicted from either eye-tracking data and interaction data. Finally, we evaluate how noise in gaze data impacts prediction accuracy and find that retaining rather noisy gaze datapoints can yield equal or even better predictions than discarding them, a novel and important contribution for devising adaptive visualizations in real settings where eye-tracking data are typically noisier than in laboratory settings.  © 2020 ACM.",classification; cognitive abilities; data quality; eye tracking; information visualization; interaction data; User modeling; user-adaptive interaction,Data visualization; Forecasting; Visualization; Accurate prediction; Adaptive visualization; Alternative source; Information visualization; Interactive visualizations; Multimodal classifiers; Prediction accuracy; Real-time prediction; Eye tracking
Mental Models of Mere Mortals with Explanations of Reinforcement Learning,2020,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088311098&doi=10.1145%2f3366485&partnerID=40&md5=db9ef21b92abacb03ddb16a518f2cd4d,"How should reinforcement learning (RL) agents explain themselves to humans not trained in AI? To gain insights into this question, we conducted a 124-participant, four-treatment experiment to compare participants' mental models of an RL agent in the context of a simple Real-Time Strategy (RTS) game. The four treatments isolated two types of explanations vs. neither vs. both together. The two types of explanations were as follows: (1) saliency maps (an ""Input Intelligibility Type""that explains the AI's focus of attention) and (2) reward-decomposition bars (an ""Output Intelligibility Type""that explains the AI's predictions of future types of rewards). Our results show that a combined explanation that included saliency and reward bars was needed to achieve a statistically significant difference in participants' mental model scores over the no-explanation treatment. However, this combined explanation was far from a panacea: It exacted disproportionately high cognitive loads from the participants who received the combined explanation. Further, in some situations, participants who saw both explanations predicted the agent's next action worse than all other treatments' participants.  © 2020 ACM.",human-computer interaction; Intelligent user interfaces,Cognitive systems; Cognitive loads; Focus of Attention; Gain insight; Mental model; Real-time strategy games; Reinforcement learning agent; Saliency map; Statistically significant difference; Reinforcement learning
A Data-Driven Approach to Designing for Privacy in Household IoT,2020,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127846955&doi=10.1145%2f3241378&partnerID=40&md5=42d468a49b69438ab417085453249f5b,"In this article, we extend and improve upon a previously developed data-driven approach to design privacy-setting interfaces for users of household IoT devices. The essence of this approach is to gather users’ feedback on household IoT scenarios before developing the interface, which allows us to create a navigational structure that preemptively maximizes users’ efficiency in expressing their privacy preferences, and develop a series of ‘privacy profiles’ that allow users to express a complex set of privacy preferences with the single click of a button. We expand upon the existing approach by proposing a more sophisticated translation of statistical results into interface design, and by extensively discussing and analyzing the tradeoff between user-model parsimony and accuracy in developing privacy profiles and default settings. © 2019 Association for Computing Machinery.",Designing for IoT; privacy,Data privacy; Internet of things; User profile; Data-driven approach; Designing for IoT; Interface designs; Navigational structures; Privacy; Privacy preferences; Privacy Settings; User efficiencies; User feedback; User Modelling; Air navigation
Introduction to the TiiS Special Column,2020,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097237130&doi=10.1145%2f3427592&partnerID=40&md5=f3f59478c5fcdec6f49ce4cda132ec06,[No abstract available],,
Generating and Understanding Personalized Explanations in Hybrid Recommender Systems,2020,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097245754&doi=10.1145%2f3365843&partnerID=40&md5=0b52ff4b26358c479da7f855abefc452,"Recommender systems are ubiquitous and shape the way users access information and make decisions. As these systems become more complex, there is a growing need for transparency and interpretability. In this article, we study the problem of generating and visualizing personalized explanations for recommender systems that incorporate signals from many different data sources. We use a flexible, extendable probabilistic programming approach and show how we can generate real-time personalized recommendations. We then turn these personalized recommendations into explanations. We perform an extensive user study to evaluate the benefits of explanations for hybrid recommender systems. We conduct a crowd-sourced user study where our system generates personalized recommendations and explanations for real users of the last.fm music platform. First, we evaluate the performance of the recommendations in terms of perceived accuracy and novelty. Next, we experiment with (1) different explanation styles (e.g., user-based, item-based), (2) manipulating the number of explanation styles presented, and (3) manipulating the presentation format (e.g., textual vs. visual). We also apply a mixed-model statistical analysis to consider user personality traits as a control variable and demonstrate the usefulness of our approach in creating personalized hybrid explanations with different style, number, and format. Finally, we perform a post analysis that shows different preferences for explanation styles between experienced and novice last.fm users. © 2020 ACM.",Explainable artificial intelligence; explainable intelligent user interfaces; explainable recommender systems; hybrid recommender systems,Artificial intelligence; Control variable; Hybrid recommender systems; Interpretability; Mixed modeling; Personalized recommendation; Presentation formats; Probabilistic programming; User personalities; Recommender systems
"Progressive Disclosure: When, Why, and How Do Users Want Algorithmic Transparency Information?",2020,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097219552&doi=10.1145%2f3374218&partnerID=40&md5=59ab6270bfd2afce482a973b909ae501,"It is essential that users understand how algorithmic decisions are made, as we increasingly delegate important decisions to intelligent systems. Prior work has often taken a techno-centric approach, focusing on new computational techniques to support transparency. In contrast, this article employs empirical methods to better understand user reactions to transparent systems to motivate user-centric designs for transparent systems. We assess user reactions to transparency feedback in four studies of an emotional analytics system. In Study 1, users anticipated that a transparent system would perform better but unexpectedly retracted this evaluation after experience with the system. Study 2 offers an explanation for this paradox by showing that the benefits of transparency are context dependent. On the one hand, transparency can help users form a model of the underlying algorithm's operation. On the other hand, positive accuracy perceptions may be undermined when transparency reveals algorithmic errors. Study 3 explored real-time reactions to transparency. Results confirmed Study 2, in showing that users are both more likely to consult transparency information and to experience greater system insights when formulating a model of system operation. Study 4 used qualitative methods to explore real-time user reactions to motivate transparency design principles. Results again suggest that users may benefit from initially simplified feedback that hides potential system errors and assists users in building working heuristics about system operation. We use these findings to motivate new progressive disclosure principles for transparency in intelligent systems and discuss theoretical implications. © 2020 ACM.",emotional analytics; error; expectation violation; explanation; intelligent systems; intelligibility; machine learning; progressive disclosure; Transparency,Intelligent systems; User centered design; Algorithmic errors; Analytics systems; Computational technique; Context dependent; Design Principles; Potential systems; Qualitative method; User-centric designs; Transparency
A Method and Analysis to Elicit User-Reported Problems in Intelligent Everyday Applications,2020,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097214092&doi=10.1145%2f3370927&partnerID=40&md5=00af06eef8bce21ca5ca12059d2ef946,"The complex nature of intelligent systems motivates work on supporting users during interaction, for example, through explanations. However, as of yet, there is little empirical evidence in regard to specific problems users face when applying such systems in everyday situations. This article contributes a novel method and analysis to investigate such problems as reported by users: We analysed 45,448 reviews of four apps on the Google Play Store (Facebook, Netflix, Google Maps, and Google Assistant) with sentiment analysis and topic modelling to reveal problems during interaction that can be attributed to the apps' algorithmic decision-making. We enriched this data with users' coping and support strategies through a follow-up online survey (N = 286). In particular, we found problems and strategies related to content, algorithm, user choice, and feedback. We discuss corresponding implications for designing user support, highlighting the importance of user control and explanations of output rather than processes. © 2020 ACM.",Algorithm; explanations; transparency; user control,Intelligent systems; Sentiment analysis; Complex nature; Follow up; Google maps; Google plays; Online surveys; Specific problems; User control; User support; Decision making
Affect-Aware Word Clouds,2020,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097221269&doi=10.1145%2f3370928&partnerID=40&md5=033171b5302efc8c8f789bb79ec6920a,"Word clouds are widely used for non-analytic purposes, such as introducing a topic to students, or creating a gift with personally meaningful text. Surveys show that users prefer tools that yield word clouds with a stronger emotional impact. Fonts and color palettes are powerful typographical signals that may determine this impact. Typically, these signals are assigned randomly, or expected to be chosen by the users. We present an affect-aware font and color palette selection methodology that aims to facilitate more informed choices. We infer associations of fonts with a set of eight affects, and evaluate the resulting data in a series of user studies both on individual words as well as in word clouds. Relying on a recent study to procure affective color palettes, we carry out a similar user study to understand the impact of color choices on word clouds. Our findings suggest that both fonts and color palettes are powerful tools contributing to the affects evoked by a word cloud. The experiments further confirm that the novel datasets we propose are successful in enabling this. We also find that, for the majority of the affects, both signals need to be congruent to create a stronger impact. Based on this data, we implement a prototype that allows users to specify a desired affect and recommends congruent fonts and color palettes for the word. © 2020 ACM.",Affective interfaces; color palettes; typography; word clouds,Artificial intelligence; Color palette; User study; Word clouds; Color
Photo Sleuth: Identifying Historical Portraits with Face Recognition and Crowdsourced Human Expertise,2020,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097244982&doi=10.1145%2f3365842&partnerID=40&md5=4f1a283eced3130539da536f77c9020b,"Identifying people in historical photographs is important for preserving material culture, correcting the historical record, and creating economic value, but it is also a complex and challenging task. In this article, we focus on identifying portraits of soldiers who participated in the American Civil War (1861-65), the first widely photographed conflict. Many thousands of these portraits survive, but only 10%-20% are identified. We created Photo Sleuth, a web-based platform that combines crowdsourced human expertise and automated face recognition to support Civil War portrait identification. Our mixed-methods evaluations of Photo Sleuth one month and 11 months after its public launch showed that it helped users successfully identify unknown portraits and provided a sustainable model for volunteer contribution. We also discuss implications for crowd-AI interaction and person identification pipelines. © 2020 ACM.",crowd-AI interaction; Crowdsourcing; face recognition; history; online communities; person identification,Crowdsourcing; Military photography; Automated face recognition; Economic values; Historical records; Human expertise; Material cultures; Person identification; Volunteer contributions; Web based platform; Face recognition
Smell Pittsburgh: Engaging Community Citizen Science for AirQuality,2020,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097231160&doi=10.1145%2f3369397&partnerID=40&md5=70223ff77e4f00dac37a72afa8a157f7,"Urban air pollution has been linked to various human health concerns, including cardiopulmonary diseases. Communities who suffer from poor air quality often rely on experts to identify pollution sources due to the lack of accessible tools. Taking this into account, we developed Smell Pittsburgh, a system that enables community members to report odors and track where these odors are frequently concentrated. All smell report data are publicly accessible online. These reports are also sent to the local health department and visualized on a map along with air quality data from monitoring stations. This visualization provides a comprehensive overview of the local pollution landscape. Additionally, with these reports and air quality data, we developed a model to predict upcoming smell events and send push notifications to inform communities. We also applied regression analysis to identify statistically significant effects of push notifications on user engagement. Our evaluation of this system demonstrates that engaging residents in documenting their experiences with pollution odors can help identify local air pollution patterns and can empower communities to advocate for better air quality. All citizen-contributed smell data are publicly accessible and can be downloaded from http://smellpgh.org. © 2020 ACM.",air quality; Community citizen science; community empowerment; machine learning; push notifications; regression analysis; smell; survey; sustainable HCI; system; visualization,Air quality; Odors; Cardiopulmonary disease; Human health concerns; Local air pollutions; Local pollutions; Monitoring stations; Pollution sources; Publicly accessible; Urban air pollution; Quality control
Algorithmic and HCI Aspects for Explaining Recommendations of Artistic Images,2020,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097239854&doi=10.1145%2f3369396&partnerID=40&md5=3eaaa7e84a50ab9f1bf5950b90110cfb,"Explaining suggestions made by recommendation systems is key to make users trust and accept these systems. This is specially critical in areas such as art image recommendation. Traditionally, artworks are sold in galleries where people can see them physically, and artists have the chance to persuade the people into buying them. On the other side, online art stores only offer the user the action of navigating through the catalog, but nobody plays the persuading role of the artist. Moreover, few works in recommendation systems provide a perspective of the many variables involved in the user perception of several aspects of the system such as domain knowledge, relevance, explainability, and trust. In this article, we aim to fill this gap by studying several aspects of the user experience with a recommender system of artistic images, from algorithmic and HCI perspectives. We conducted two user studies in Amazon Mechanical Turk to evaluate different levels of explainability, combined with different algorithms. While in study 1 we focus only on a desktop interface, in study 2 we attempt to understand the effect of explanations in mobile devices. In general, our experiments confirm that explanations of recommendations in the image domain are useful and increase user satisfaction, perception of explainability and relevance. In the first study, our results show that the observed effects are dependent on the underlying recommendation algorithm used. In the second study, our results show that these effects are also dependent of the device used in the study but with a smaller effect. Finally, using the framework by Knijnenburg et al., we provide a comprehensive model, for each study, which synthesizes the effects between different variables involved in the user experience with explainable visual recommender systems of artistic images. © 2020 ACM.",art; explainable AI; Visual recommender systems,Arts computing; Recommender systems; Amazon mechanical turks; Comprehensive model; Desktop interfaces; Domain knowledge; Image domain; Recommendation algorithms; User perceptions; User satisfaction; User experience
Individualising graphical layouts with predictive visual search models,2019,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075674398&doi=10.1145%2f3241381&partnerID=40&md5=ecbf8fa696dea75e9520a67bd99ecccd,"In domains where users are exposed to large variations in visuo-spatial features among designs, they often spend excess time searching for common elements (features) on an interface. This article contributes individualised predictive models of visual search, and a computational approach to restructure graphical layouts for an individual user such that features on a new, unvisited interface can be found quicker. It explores four technical principles inspired by the human visual system (HVS) to predict expected positions of features and create individualised layout templates: (I) the interface with highest frequency is chosen as the template; (II) the interface with highest predicted recall probability (serial position curve) is chosen as the template; (III) the most probable locations for features across interfaces are chosen (visual statistical learning) to generate the template; (IV) based on a generative cognitive model, the most likely visual search locations for features are chosen (visual sampling modelling) to generate the template. Given a history of previously seen interfaces, we restructure the spatial layout of a new (unseen) interface with the goal of making its features more easily findable. The four HVS principles are implemented in Familiariser, a web browser that automatically restructures webpage layouts based on the visual history of the user. Evaluation of Familiariser (using visual statistical learning) with users provides first evidence that our approach reduces visual search time by over 10%, and number of eye-gaze fixations by over 20%, during web browsing tasks. © 2019 Association for Computing Machinery.",Adaptive user interfaces; Computational design; Graphical layouts; Visual search,Sampling; Web browsers; Adaptive user interface; Computational approach; Computational design; Graphical layouts; Human visual systems; Statistical learning; Technical principle; Visual search; Graphical user interfaces
Human-in-the-loop learning for personalized diet monitoring from unstructured mobile data,2019,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075602761&doi=10.1145%2f3319370&partnerID=40&md5=68855e1f65ca4c219efae4a13bcd916d,"Lifestyle interventions with the focus on diet are crucial in self-management and prevention of many chronic conditions, such as obesity, cardiovascular disease, diabetes, and cancer. Such interventions require a diet monitoring approach to estimate overall dietary composition and energy intake. Although wearable sensors have been used to estimate eating context (e.g., food type and eating time), accurate monitoring of dietary intake has remained a challenging problem. In particular, because monitoring dietary intake is a self-administered task that involves the end-user to record or report their nutrition intake, current diet monitoring technologies are prone to measurement errors related to challenges of human memory, estimation, and bias. New approaches based on mobile devices have been proposed to facilitate the process of dietary intake recording. These technologies require individuals to use mobile devices such as smartphones to record nutrition intake by either entering text or taking images of the food. Such approaches, however, suffer from errors due to low adherence to technology adoption and time sensitivity to the dietary intake context. In this article, we introduce EZNutriPal,1 an interactive diet monitoring system that operates on unstructured mobile data such as speech and free-text to facilitate dietary recording, real-time prompting, and personalized nutrition monitoring. EZNutriPal features a natural language processing unit that learns incrementally to add user-specific nutrition data and rules to the system. To prevent missing data that are required for dietary monitoring (e.g., calorie intake estimation), EZNutriPal devises an interactive operating mode that prompts the end-user to complete missing data in real-time. Additionally, we propose a combinatorial optimization approach to identify the most appropriate pairs of food names and food quantities in complex input sentences. We evaluate the performance of EZNutriPal using real data collected from 23 human subjects who participated in two user studies conducted in 13 days each. The results demonstrate that EZNutriPal achieves an accuracy of 89.7% in calorie intake estimation. We also assess the impacts of the incremental training and interactive prompting technologies on the accuracy of nutrient intake estimation and show that incremental training and interactive prompting improve the performance of diet monitoring by 49.6% and 29.1%, respectively, compared to a system without such computing units. © 2019 Association for Computing Machinery.",Assignment problem; Combinatorial optimization; Diet monitoring; Human-in-the-loop learning; Mobile computing; Real-time prompting; Unstructured data; Wireless health,Combinatorial optimization; Diseases; Mobile computing; Natural language processing systems; Nutrition; Assignment problems; Human-in-the-loop; Real time; Unstructured data; Wireless healths; Monitoring
EventAction: A visual analytics approach to explainable recommendation for event sequences,2019,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075687290&doi=10.1145%2f3301402&partnerID=40&md5=e16e47dd37f331ef8eb5071aadbbad27,"People use recommender systems to improve their decisions; for example, item recommender systems help them find films to watch or books to buy. Despite the ubiquity of item recommender systems, they can be improved by giving users greater transparency and control. This article develops and assesses interactive strategies for transparency and control, as applied to event sequence recommender systems, which provide guidance in critical life choices such as medical treatments, careers decisions, and educational course selections. This article's main contribution is the use of both record attributes and temporal event information as features to identify similar records and provide appropriate recommendations.While traditional item recommendations are based on choices by people with similar attributes, such as those who looked at this product or watched this movie, our event sequence recommendation approach allows users to select records that share similar attribute values and start with a similar event sequence. Then users see how different choices of actions and the orders and times between them might lead to users' desired outcomes. This paper applies a visual analytics approach to present and explain recommendations of event sequences. It presents a workflow for event sequence recommendation that is implemented in EventAction and reports on three case studies in two domains to illustrate the use of generating event sequence recommendations based on personal histories. It also offers design guidelines for the construction of user interfaces for event sequence recommendation and discusses ethical issues in dealing with personal histories. A demo video of EventAction is available at http://hcil.umd.edu/eventaction. © 2019 Association for Computing Machinery.",Decision making; Multidimensional data visualization; Personal record; Similarity; Temporal visualization; Visual analytics,Behavioral research; Data visualization; Decision making; Real time systems; Recommender systems; Transparency; User interfaces; Attribute values; Interactive strategy; Medical treatment; Multi-dimensional data visualization; Personal record; Provide guidances; Similarity; Visual analytics; Visualization
Special issue on highlights of ACM intelligent user interface (IUI) 2018,2019,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075689610&doi=10.1145%2f3357206&partnerID=40&md5=05c3df782c3da69ed106021b244eef4f,[No abstract available],,
A visual analytics approach for interactive document clustering,2019,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075685524&doi=10.1145%2f3241380&partnerID=40&md5=e6e28f1b7ab46f7f212199b4542e0daa,"Document clustering is a necessary step in various analytical and automated activities. When guided by the user, algorithms are tailored to imprint a perspective on the clustering process that reflects the user's understanding of the dataset.More than just allowfor customized adjustment of the clusters, a visual analytics approach will provide tools for the user to draw new insights on the collection. While contributing his or her perspective, the user will also acquire a deeper understanding of the data set. To that effect, we propose a novel visual analytics system for interactive document clustering. We built our system on top of clustering algorithms that can adapt to user's feedback. In the proposed system, initial clustering is created based on the user-defined number of clusters and the selected clustering algorithm. A set of coordinated visualizations allow the examination of the dataset and the results of the clustering. The visualization provides the user with the highlights of individual documents and understanding of the evolution of documents over the time period to which they relate. The users then interact with the process by means of changing key-terms that drive the process according to their knowledge of the documents domain. In key-term-based interaction, the user assigns a set of key-terms to each target cluster to guide the clustering algorithm.We have improved that processwith a novel algorithm for choosing proper seeds for the clustering. Results demonstrate that not only the system has improved considerably its precision, but also its effectiveness in the document-based decision making. A set of quantitative experiments and a user study have been conducted to show the advantages of the approach for document analytics based on clustering. We performed and reported on the use of the framework in a real decision-making scenario that relates users discussion by email to decision making in improving patient care. Results show that the framework is useful even for more complex data sets such as email conversations. © 2019 Association for Computing Machinery.",Deterministic; Document projection; Email list; Interactive document clustering; Key-term; Seeding; Text; User study; Visualization,Automatic guided vehicles; Cloud seeding; Cluster analysis; Decision making; Digital storage; Electronic mail; Flow visualization; Information retrieval; Visualization; Deterministic; Document projection; Interactive documents; Key-term; Text; User study; Clustering algorithms
A bandit-based ensemble framework for exploration/exploitation of diverse recommendation components: An experimental study within e-commerce,2019,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075692959&doi=10.1145%2f3237187&partnerID=40&md5=f652c6e9c47b706b4815c4d07407d0f5,"This work presents an extension of Thompson Sampling bandit policy for orchestrating the collection of base recommendation algorithms for e-commerce.We focus on the problem of item-to-item recommendations, for which multiple behavioral and attribute-based predictors are provided to an ensemble learner. In addition, we detail the construction of a personalized predictor based on k-Nearest Neighbors (kNN), with temporal decay capabilities and event weighting. We show how to adapt Thompson Sampling to realistic situations when neither action availability nor reward stationarity is guaranteed. Furthermore, we investigate the effects of priming the sampler with pre-set parameters of reward probability distributions by utilizing the product catalog and/or event history, when such information is available. We report our experimental results based on the analysis of three real-world e-commerce datasets. © 2019 Association for Computing Machinery.",E-commerce recommender systems; Multiarm bandit ensembles; Reinforcement learning; Session-based recommendations; Streaming recommendations; Thompson Sampling,Nearest neighbor search; Probability distributions; Reinforcement learning; E-commerce recommender system; Exploration/exploitation; K nearest neighbor (KNN); Multiarm bandit ensembles; Product catalogs; Recommendation algorithms; Session-based recommendations; Thompson samplings; Electronic commerce
User evaluations on sentiment-based recommendation explanations,2019,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075668509&doi=10.1145%2f3282878&partnerID=40&md5=823d35a653b9e4ef8599241e7d146cce,"The explanation interface has been recognized as important in recommender systems because it can allow users to better judge the relevance of recommendations to their preferences and, hence, make more informed decisions. In different product domains, the specific purpose of explanation can be different. For high-investment products (e.g., digital cameras, laptops), how to educate the typical type of new buyers about product knowledge and, consequently, improve their preference certainty and decision quality is essentially crucial. With this objective, we have developed a novel tradeoff-oriented explanation interface that particularly takes into account sentiment features as extracted from product reviews to generate recommendations and explanations in a category structure. In this manuscript, we first reported the results of an earlier user study (in both before-after and counter-balancing setups) that compared our prototype system with the traditional one that purely considers static specifications for explanations. This experiment revealed that adding sentiment-based explanations can significantly increase users' product knowledge, preference certainty, perceived information usefulness, perceived recommendation transparency and quality, and purchase intention. In order to further identify the reason behind users' perception improvements on the sentiment-based explanation interface, we performed a follow-up lab controlled eye-tracking experiment that investigated how users viewed information and compared products on the interface. This study shows that incorporating sentiment features into the tradeoff-oriented explanations can significantly affect users' eye-gaze pattern. They were stimulated to not only notice bottom categories of products, but also, more frequently, to compare products across categories. The results also disclose users' inherent information needs for sentiment-based explanations, as they allow users to better understand the recommended products and gain more knowledge about static specifications. © 2019 Association for Computing Machinery.",Explanation interfaces; Eye-tracking experiment; Product reviews; Recommender systems; Sentiment analysis; User perceptions; User study,Investments; Recommender systems; Sentiment analysis; Specifications; Explanation interfaces; Product knowledge; Product reviews; Purchase intention; Sentiment features; User perceptions; User study; Users' perception; Eye tracking
Unobtrusive activity recognition and position estimation forwork surfaces using rf-radar sensing,2019,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075631343&doi=10.1145%2f3241383&partnerID=40&md5=cde818b1a8008c1a418cd7181c9cf39f,"Activity recognition is a core component of many intelligent and context-aware systems. We present a solution for discreetly and unobtrusively recognizing common work activities above a work surface without using cameras. We demonstrate our approach, which utilizes an RF-radar sensor mounted under the work surface, in three domains: recognizing work activities at a convenience-store counter, recognizing common office deskwork activities, and estimating the position of customers in a showroom environment. Our examples illustrate potential benefits for both post-hoc business analytics and for real-time applications. Our solution was able to classify seven clerk activities with 94.9% accuracy using data collected in a lab environment and able to recognize six common deskwork activities collected in real offices with 95.3% accuracy. Using two sensors simultaneously, we demonstrate coarse position estimation around a large surface with 95.4% accuracy.We show that using multiple projections of RF signal leads to improved recognition accuracy. Finally, we show how smartwatches worn by users can be used to attribute an activity, recognized with the RF sensor, to a particular user in multi-user scenarios. We believe our solution can mitigate some of users privacy concerns associated with cameras and is useful for a wide range of intelligent systems ©2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Activity recognition; Deskwork; IMU; Radio frequency radar sensor; Retail; Sensing,Cameras; Intelligent systems; Pattern recognition; Radar equipment; User interfaces; Activity recognition; Deskwork; Radar sensors; Retail; Sensing; Radar
A user-adaptive modeling for eating action identification from wristband time series,2019,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075631112&doi=10.1145%2f3300149&partnerID=40&md5=897023fa83c01a9bcd136c9d82b0b960,"Eating activity monitoring using wearable sensors can potentially enable interventions based on eating speed to mitigate the risks of critical healthcare problems such as obesity or diabetes. Eating actions are polycomponential gestures composed of sequential arrangements of three distinct components interspersed with gestures that may be unrelated to eating. This makes it extremely challenging to accurately identify eating actions. The primary reasons for the lack of acceptance of state-of-the-art eating action monitoring techniques include the following: (i) the need to install wearable sensors that are cumbersome to wear or limit the mobility of the user, (ii) the need for manual input from the user, and (iii) poor accuracy in the absence of manual inputs. In this work, we propose a novel methodology, IDEA, that performs accurate eating action identification within eating episodes with an average F1 score of 0.92. This is an improvement of 0.11 for precision and 0.15 for recall for the worst-case users as compared to the state of the art. IDEA uses only a single wristband and provides feedback on eating speed every 2 min without obtaining any manual input from the user. ©2019 Association for Computing Machinery.",Diet monitoring; Gesture recognition; Time-series data modeling; Wearable,Gesture recognition; Time series; Activity monitoring; Healthcare problems; Monitoring techniques; Novel methodology; State of the art; Time-series data; User-adaptive; Wearable; Wearable sensors
A Roadmap to User-Controllable Social Exploratory Search,2019,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075558469&doi=10.1145%2f3241382&partnerID=40&md5=03adee7b6de0582f772ab11a1fde2359,"Information-seeking tasks with learning or investigative purposes are usually referred to as exploratory search. Exploratory search unfolds as a dynamic process where the user, amidst navigation, trial and error, and on-the-fly selections, gathers and organizes information (resources). A range of innovative interfaces with increased user control has been developed to support the exploratory search process. In this work, we present our attempt to increase the power of exploratory search interfaces by using ideas of social search for instance, leveraging information left by past users of information systems. Social search technologies are highly popular today, especially for improving ranking. However, current approaches to social ranking do not allow users to decide to what extent social information should be taken into account for result ranking. This article presents an interface that integrates social search functionality into an exploratory search system in a user-controlled way that is consistent with the nature of exploratory search. The interface incorporates control features that allow the user to (i) express information needs by selecting keywords and (ii) to express preferences for incorporating social wisdom based on tag matching and user similarity. The interface promotes search transparency through color-coded stacked bars and rich tooltips. This work presents the full series of evaluations conducted to, first, assess the value of the social models in contexts independent to the user interface, in terms of objective and perceived accuracy. Then, in a study with the full-fledged system, we investigated system accuracy and subjective aspects with a structural model revealing that when users actively interacted with all of its control features, the hybrid system outperformed a baseline content-based-only tool and users were more satisfied. © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Exploratory search; Social search; User-controllable interface,Hybrid systems; Information use; User interfaces; Control features; Dynamic process; Exploratory search; Information seeking; Social information; Social search; Structural modeling; Subjective aspects; Search engines
FourEyes: Leveraging tool diversity as a means to improve aggregate accuracy in crowd sourcing,2019,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075044439&doi=10.1145%2f3237188&partnerID=40&md5=e65770f0a5a70586ec84cf7d762101b9,"Crowdsourcing is a common means of collecting image segmentation training data for use in a variety of computer vision applications. However, designing accurate crowd-powered image segmentation systems is challenging, because defining object boundaries in an image requires significant fine motor skills and handeye coordination, which makes these tasks error-prone. Typically, special segmentation tools are created and then answers from multiple workers are aggregated to generate more accurate results. However, individual tool designs can bias how and where people make mistakes, resulting in shared errors that remain even after aggregation. In this article, we introduce a novel crowdsourcing approach that leverages tool diversity as a means of improving aggregate crowd performance. Our idea is that given a diverse set of tools, answer aggregation done across tools can help improve the collective performance by offsetting systematic biases induced by the individual tools themselves. To demonstrate the effectiveness of the proposed approach, we design four different tools and present FourEyes, a crowd-powered image segmentation system that uses aggregation across different tools. We then conduct a series of studies that evaluate different aggregation conditions and show that using multiple tools can significantly improve aggregate accuracy. Furthermore, we investigate the idea of applying post-processing for multi-tool aggregation in terms of correction mechanism. We introduce a novel region-based method for synthesizing more accurate bounds for image segmentation tasks through averaging surrounding annotations. In addition,we explore the effect of adjusting the threshold parameter of an EM-based aggregation method. Our results suggest that not only the individual tool's design, but also the correction mechanism, can affect the performance of multi-tool aggregation. This article extends a work presented at ACM IUI 2018 [46] by providing a novel region-based error-correction method and additional in-depth evaluation of the proposed approach. © 2019 Association for Computing Machinery.",Computer vision; Crowdsourcing; Human computation; Multi-tool aggregation; Semantic image segmentation; Tool diversity,Aggregates; Computer vision; Crowdsourcing; Error correction; Semantics; Computer vision applications; Correction mechanism; Hand eye coordination; Human computation; Image segmentation system; Region-based methods; Semantic image segmentations; Threshold parameters; Image segmentation
Toward a unified theory of learned trust in interpersonal and human-machine interactions,2019,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075640119&doi=10.1145%2f3230735&partnerID=40&md5=505e55bc98fcb24cef180610e8669334,"A proposal for a unified theory of learned trust implemented in a cognitive architecture is presented. The theory is instantiated as a computational cognitive model of learned trust that integrates several seemingly unrelated categories of findings from the literature on interpersonal and human-machine interactions and makes unintuitive predictions for future studies. The model relies on a combination of learning mechanisms to explain a variety of phenomena such as trust asymmetry, the higher impact of early trust breaches, the black-hat/white-hat effect, the correlation between trust and cognitive ability, and the higher resilience of interpersonal as compared to human-machine trust. In addition, the model predicts that trust decays in the absence of evidence of trustworthiness or untrustworthiness. The implications of the model for the advancement of the theory on trust are discussed. Specifically, this work suggests two more trust antecedents on the trustors side: perceived trust necessity and cognitive ability to detect cues of trustworthiness. ©2019 Association for Computing Machinery.",Computational cognitive model; Learned trust; Trust; Trust propensity; Trustworthiness; Unified theories,Man machine systems; Computational cognitive modeling; Learned trust; Trust; Trust propensity; Trustworthiness; Unified theory; Computation theory
Exploring social recommendations with visual diversity-promoting interfaces,2019,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063197003&doi=10.1145%2f3231465&partnerID=40&md5=19a321518b4b4091ece1c82d7e432cfa,"The beyond-relevance objectives of recommender systems have been drawing more and more attention. For example, a diversity-enhanced interface has been shown to associate positively with overall levels of user satisfaction. However, little is known about how users adopt diversity-enhanced interfaces to accomplish various real-world tasks. In this article, we present two attempts at creating a visual diversity-enhanced interface that presents recommendations beyond a simple ranked list. Our goal was to design a recommender system interface to help users explore the different relevance prospects of recommended items in parallel and to stress their diversity. Two within-subject user studies in the context of social recommendation at academic conferences were conducted to compare our visual interfaces. Results from our user study show that the visual interfaces significantly reduced the exploration efforts required for given tasks and helped users to perceive the recommendation diversity. We show that the users examined a diverse set of recommended items while experiencing an improvement in overall user satisfaction. Also, the users' subjective evaluations show significant improvement in many user-centric metrics. Experiences are discussed that shed light on avenues for future interface designs. © 2019 Association for Computing Machinery.",Diversification; Diversity-promoting interface; Social recommendation; User interface; User-driven exploration,Recommender systems; Academic conferences; Diversification; Exploration effort; Recommendation diversities; Social recommendation; Subjective evaluations; User driven; User satisfaction; User interfaces
Anchorviz: Facilitating semantic data exploration and concept discovery for interactive machine learning,2019,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075634518&doi=10.1145%2f3241379&partnerID=40&md5=570a42200986ff078f3e173fcf5e23da,"When building a classifier in interactive machine learning (iML), human knowledge about the target class can be a powerful reference to make the classifier robust to unseen items. The main challenge lies in finding unlabeled items that can either help discover or refine concepts for which the current classifier has no corresponding features (i.e., it has feature blindness). Yet it is unrealistic to ask humans to come up with an exhaustive list of items, especially for rare concepts that are hard to recall. This article presents AnchorViz, an interactive visualization that facilitates the discovery of prediction errors and previously unseen concepts through human-driven semantic data exploration. By creating example-based or dictionary-based anchors representing concepts, users create a topology that (a) spreads data based on their similarity to the concepts and (b) surfaces the prediction and label inconsistencies between data points that are semantically related. Once such inconsistencies and errors are discovered, users can encode the new information as labels or features and interact with the retrained classifier to validate their actions in an iterative loop. We evaluated AnchorViz through two user studies. Our results show that AnchorViz helps users discover more prediction errors than stratified random and uncertainty sampling methods. Furthermore, during the beginning stages of a training task, an iML tool with AnchorViz can help users build classifiers comparable to the ones built with the same tool with uncertainty sampling and keyword search, but with fewer labels and more generalizable features.We discuss exploration strategies observed during the two studies and how AnchorViz supports discovering, labeling, and refining of concepts through a sensemaking loop. © 2019 Royal Society of Chemistry. All rights reserved.",Concept discovery; Error discovery; Interactive machine learning; Machine teaching; Semantic data exploration; Unlabeled data; Visualization,Data visualization; Flow visualization; Forecasting; Iterative methods; Machine learning; Random errors; Search engines; Semantics; Visualization; Concept discoveries; Exploration strategies; Interactive machine learning; Interactive visualizations; Prediction errors; Semantic data; Uncertainty samplings; Unlabeled data; Classification (of information)
Learning from sets of items in recommender systems,2019,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070057562&doi=10.1145%2f3326128&partnerID=40&md5=1354ae203e8508b429227ba7529cf47f,"Most of the existing recommender systems use the ratings provided by users on individual items. An additional source of preference information is to use the ratings that users provide on sets of items. The advantages of using preferences on sets are twofold. First, a rating provided on a set conveys some preference information about each of the set's items, which allows us to acquire a user's preferences for more items than the number of ratings that the user provided. Second, due to privacy concerns, users may not be willing to reveal their preferences on individual items explicitly but may be willing to provide a single rating to a set of items, since it provides some level of information hiding. This article investigates two questions related to using set-level ratings in recommender systems. First, how users' item-level ratings relate to their set-level ratings. Second, how collaborative filtering-based models for item-level rating prediction can take advantage of such set-level ratings. We have collected set-level ratings from active users of Movielens on sets of movies that they have rated in the past. Our analysis of these ratings shows that though the majority of the users provide the average of the ratings on a set's constituent items as the rating on the set, there exists a significant number of users that tend to consistently either under- or over-rate the sets. We have developed collaborative filtering-based methods to explicitly model these user behaviors that can be used to recommend items to users. Experiments on real data and on synthetic data that resembles the under- or over-rating behavior in the real data demonstrate that these models can recover the overall characteristics of the underlying data and predict the user's ratings on individual items. © 2019 Association for Computing Machinery.",Collaborative filtering; Matrix factorization; Recommender systems; User behavior modeling,Behavioral research; Factorization; Recommender systems; Information hiding; Matrix factorizations; Overall characteristics; Preference information; Privacy concerns; User behavior modeling; User behaviors; User's preferences; Collaborative filtering
Exploring a design space of graphical adaptive menus: Normal vs. Small screens,2019,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069636663&doi=10.1145%2f3237190&partnerID=40&md5=6362c734f11bfb1dd192621ac5a94f3c,"Graphical Adaptive Menus are Graphical User Interface menus whose predicted items of immediate use can be automatically rendered in a prediction window. Rendering this prediction window is a key question for adaptivity to enable the end-user to efficiently differentiate predicted items from normal ones and to consequently select appropriate items. Adaptivity for graphical menus has been investigated more for normal screens, such as desktops, than for small screens, such as smartphones, where real estate imposes severe rendering constraints. To address this question, this article defines and explores a design space where graphical adaptive menus are structured based on Bertin's eight visual variables (i.e., position, size, shape, value, color, orientation, texture, and motion) and their combination by comparing their rendering for small screens with respect to normal screens. Based on this design space, previously introduced graphical adaptive menus are revisited in terms of four stability properties (i.e., spatial, physical, format, and temporal), and new menu designs are introduced and discussed for both normal and small screens. The resulting set of graphical adaptive menu has been subject to a preference analysis from which a particular design emerged: the cloud menu, where predicted items are arranged in an adaptive tag cloud. We investigate empirically the effect of the cloud menu on the item selection time and the error rate with respect to a static menu and an adaptive linear menu. This article then suggests a set of usability guidelines for designers and practitioners to design graphical adaptive menus in general and cloud menus in particular. © 2019 Association for Computing Machinery.",Adaptation; Adaptive user interfaces; Adaptivity; Graphical adaptive menus; Intelligent user interfaces; Menu selection; Prediction scheme; Prediction window; Split interface,Forecasting; Rendering (computer graphics); Stability; Textures; Adaptation; Adaptive user interface; Adaptivity; Graphical adaptive menus; Intelligent User Interfaces; Menu selection; Prediction schemes; Graphical user interfaces
A comparison of techniques for sign language alphabet recognition using armband wearables,2019,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065186430&doi=10.1145%2f3150974&partnerID=40&md5=7f813a3923d9fbd0d52bd7a626abfdf2,"Recent research has shown that reliable recognition of sign language words and phrases using user-friendly and noninvasive armbands is feasible and desirable. This work provides an analysis and implementation of including fingerspelling recognition (FR) in such systems, which is a much harder problem due to lack of distinctive hand movements. A novel algorithm called DyFAV (Dynamic Feature Selection and Voting) is proposed for this purpose that exploits the fact that fingerspelling has a finite corpus (26 alphabets for the American Sign Language (ASL)). Detailed analysis of the algorithm used as well as comparisons with other traditional machine-learning algorithms is provided. The system uses an independent multiple-agent voting approach to identify letters with high accuracy. The independent voting of the agents ensures that the algorithm is highly parallelizable and thus recognition times can be kept low to suit real-time mobile applications. A thorough explanation and analysis is presented on results obtained on the ASL alphabet corpus for nine people with limited training. An average recognition accuracy of 95.36% is reported and compared with recognition results from other machine-learning techniques. This result is extended by including six additional validation users with data collected under similar settings as the previous dataset. Furthermore, a feature selection schema using a subset of the sensors is proposed and the results are evaluated. The mobile, noninvasive, and real-time nature of the technology is demonstrated by evaluating performance on various types of Android phones and remote server configurations. A brief discussion of the user interface is provided along with guidelines for best practices. © 2019 Association for Computing Machinery.",Activity recognition; Digital signal processing; Machine learning; Sign language recognition; Wearables,Digital signal processing; Feature extraction; Learning algorithms; Learning systems; Mobile agents; Multi agent systems; User interfaces; Wearable technology; Activity recognition; American sign language; Dynamic feature selections; Machine learning techniques; Nature of the technologies; Recognition accuracy; Sign Language recognition; Wearables; Machine learning
Interactive quality analytics of user-generated content: An integrated toolkit for the case of Wikipedia,2019,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065191460&doi=10.1145%2f3150973&partnerID=40&md5=e9eb93bfdf5d66dfe51cbae87e17b89f,"Digital libraries and services enable users to access large amounts of data on demand. Yet, quality assessment of information encountered on the Internet remains an elusive open issue. For example, Wikipedia, one of the most visited platforms on the Web, hosts thousands of user-generated articles and undergoes 12 million edits/contributions per month. User-generated content is undoubtedly one of the keys to its success but also a hindrance to good quality. Although Wikipedia has established guidelines for the “perfect article,” authors find it difficult to assert whether their contributions comply with them and reviewers cannot cope with the ever-growing amount of articles pending review. Great efforts have been invested in algorithmic methods for automatic classification of Wikipedia articles (as featured or non-featured) and for quality flaw detection. Instead, our contribution is an interactive tool that combines automatic classification methods and human interaction in a toolkit, whereby experts can experiment with new quality metrics and share them with authors that need to identify weaknesses to improve a particular article. A design study shows that experts are able to effectively create complex quality metrics in a visual analytics environment. In turn, a user study evidences that regular users can identify flaws, as well as high-quality content based on the inspection of automatic quality scores. © 2019 Association for Computing Machinery.",Information quality assessment; Text analytics; User-generated content; Visual analytics; Wikipedia,Digital libraries; Visualization; Information quality assessment; Text analytics; User-generated content; Visual analytics; Wikipedia; Data mining
Bi-level thresholding: Analyzing the effect of repeated errors in gesture input,2019,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065155833&doi=10.1145%2f3181672&partnerID=40&md5=0d4975b9a08daaa41ca6f8bdadd835f8,"In gesture recognition, one challenge that researchers and developers face is the need for recognition strategies that mediate between false positives and false negatives. In this article, we examine bi-level thresholding, a recognition strategy that uses two thresholds: a tighter threshold limits false positives and recognition errors, and a looser threshold prevents repeated errors (false negatives) by analyzing movements in sequence. We first describe early observations that led to the development of the bi-level thresholding algorithm. Next, using a Wizard-of-Oz recognizer, we hold recognition rates constant and adjust for fixed versus bi-level thresholding; we show that systems using bi-level thresholding result in significantly lower workload scores on the NASA-TLX and significantly lower accelerometer variance when performing gesture input. Finally, we examine the effect that bi-level thresholding has on a real-world dataset of wrist and finger gestures, showing an ability to significantly improve measures of precision and recall. Overall, these results argue for the viability of bi-level thresholding as an effective technique for balancing between false positives, recognition errors, and false negatives. © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Gesture; Handheld devices and mobile computing; Interaction design; Recognition; Thresholding; Usability testing and evaluation,NASA; Gesture; Handheld devices and mobile computing; Interaction design; Recognition; Thresholding; Usability testing and evaluation; Errors
Introduction to the special issue on highlights of ACM intelligent user interface (IUI) 2017,2019,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065161056&doi=10.1145%2f3301292&partnerID=40&md5=b742ad489587c90c8ad39c837a35f07c,[No abstract available],Environment-adaptive viewpoints; Gesture recognition; Human behavior; Human in the loop; Human personality; Knowledge from Wikipedia,
Toward universal spatialization through Wikipedia-based semantic enhancement,2019,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065157829&doi=10.1145%2f3213769&partnerID=40&md5=c4a2437f1b9b18ee2ea6bd1fe0813e55,"This article introduces Cartograph, a visualization system that harnesses the vast world knowledge encoded within Wikipedia to create thematic maps of almost any data. Cartograph extends previous systems that visualize non-spatial data using geographic approaches. Although these systems required data with an existing semantic structure, Cartograph unlocks spatial visualization for a much larger variety of datasets by enhancing input datasets with semantic information extracted from Wikipedia. Cartograph’s map embeddings use neural networks trained on Wikipedia article content and user navigation behavior. Using these embeddings, the system can reveal connections between points that are unrelated in the original datasets but are related in meaning and therefore embedded close together on the map. We describe the design of the system and key challenges we encountered. We present findings from two user studies exploring design choices and use of the system. © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Maps; Neural networks; Semantic relatedness; Thematic cartography; User studies; Wikidata; Wikipedia,Behavioral research; Embeddings; Maps; Neural networks; Semantics; Visualization; Semantic relatedness; Thematic cartography; User study; Wikidata; Wikipedia; Data visualization
Profiling personality traits with games,2019,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063239472&doi=10.1145%2f3230738&partnerID=40&md5=109644ad166d3b7bf3089d67cd38b1a2,"Trying to understand a player's characteristics with regards to a computer game is a major line of research known as player modeling. The purpose of player modeling is typically the adaptation of the game itself.We present two studies that extend player modeling into player profiling by trying to identify abstract personality traits, such as the need for cognition and self-esteem, through a player's in-game behavior.We present evidence that game mechanics that can be broadly adopted by several game genres, such as hints and a player's self-evaluation at the end of a level, correlate with the aforementioned personality traits. We conclude by presenting future directions for research regarding this topic, discuss the direct applications for the games industry, and explore how games can be developed as profiling tools with applications to other contexts. © 2019 Association for Computing Machinery.",Game design; Games user research; Need for cognition; Player modeling; Player profiling; Self-esteem,Artificial intelligence; Game design; Games user researches; Need for cognitions; Player modeling; Player Profiling; Self esteem; Computer games
HILC: Domain-independent PbD system via computer vision and follow-up questions,2019,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063213571&doi=10.1145%2f3234508&partnerID=40&md5=3b34ba04f718cee36fcf0bed239563a6,"Creating automation scripts for tasks involving Graphical User Interface (GUI) interactions is hard. It is challenging because not all software applications allow access to a program's internal state, nor do they all have accessibility APIs. Although much of the internal state is exposed to the user through the GUI, it is hard to programmatically operate the GUI's widgets. To that end, we developed a system prototype that learns by demonstration, called HILC (Help, It Looks Confusing). Users, both programmers and non-programmers, train HILC to synthesize a task script by demonstrating the task. A demonstration produces the needed screenshots and their corresponding mouse-keyboard signals. After the demonstration, the user answers follow-up questions. We propose a user-in-the-loop framework that learns to generate scripts of actions performed on visible elements of graphical applications. Although pure programming by demonstration is still unrealistic due to a computer's limited understanding of user intentions,we use quantitative and qualitative experiments to show that non-programming users are willing and effective at answering follow-up queries posed by our system, to help with confusing parts of the demonstrations. Our models of events and appearances are surprisingly simple but are combined effectively to cope with varying amounts of supervision. The best available baseline, Sikuli Slides, struggled to assist users in the majority of the tests in our user study experiments. The prototype with our proposed approach successfully helped users accomplish simple linear tasks, complicated tasks (monitoring, looping, and mixed), and tasks that span across multiple applications. Even when both systems could ultimately perform a task, ours was trained and refined by the user in less time. © 2019 Copyright held by the owner/author(s).",Action segmentation and recognition; GUI automation; Programming by demonstration; Visual-based programming by demonstration,Application programs; Computer vision; Demonstrations; Graphical user interfaces; Mammals; Action segmentation; Domain independents; Graphical applications; Graphical user interfaces (GUI); Multiple applications; Programming by demon-stration; Qualitative experiments; Software applications; Computer systems programming
A comparison of adaptive view techniques for exploratory 3D drone teleoperation,2019,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063219550&doi=10.1145%2f3232232&partnerID=40&md5=d6424b93d2b332afe25f92c59618351e,"Drone navigation in complex environments posesmany problems to teleoperators. Especially in three dimensional (3D) structures such as buildings or tunnels, viewpoints are often limited to the drone's current camera view, nearby objects can be collision hazards, and frequent occlusion can hinder accurate manipulation. To address these issues, we have developed a novel interface for teleoperation that provides a user with environment-adaptive viewpoints that are automatically configured to improve safety and provide smooth operation. This real-time adaptive viewpoint system takes robot position, orientation, and 3D point-cloud information into account to modify the user's viewpoint to maximize visibility. Our prototype uses simultaneous localization and mapping (SLAM) based reconstruction with an omnidirectional camera, and we use the resulting models as well as simulations in a series of preliminary experiments testing navigation of various structures. Results suggest that automatic viewpoint generation can outperform first- and third-person view interfaces for virtual teleoperators in terms of ease of control and accuracy of robot operation. © 2019 Association for Computing Machinery.",Drone navigation; View management,Cameras; Navigation; Remote control; Robotics; Robots; 3D point cloud; Complex environments; Omnidirectional cameras; Robot operations; Robot positions; Simultaneous localization and mapping; Three dimensional (3D) structures; View management; Drones
Wearables and social signal processing for smarter public presentations,2019,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063228932&doi=10.1145%2f3234507&partnerID=40&md5=cdb58a6c33a88a2fb4dd505c3296ac19,"Social Signal Processing techniques have given the opportunity to analyze in-depth human behavior in social face-to-face interactions. With recent advancements, it is henceforth possible to use these techniques to augment social interactions, especially human behavior in oral presentations. The goal of this study is to train a computational model able to provide a relevant feedback to a public speaker concerning his/her coverbal communication. Hence, the role of this model is to augment the social intelligence of the orator and then the relevance of his/her presentation. To this end, we present an original interaction setting in which the speaker is equipped with only wearable devices. Several coverbal modalities have been extracted and automatically annotated namely speech volume, intonation, speech rate, eye gaze, hand gestures, and body movements. In this article, which is an extension of our previous article published in IUI'17, we compare our Dynamic Bayesian Network design to classical J48/Multi-Layer Perceptron/Support Vector Machine classifiers, propose a subjective evaluation of presenter skills with a discussion in regards to our automatic evaluation, and we add a complementary study about using DBScan versus k-means algorithm in the design process of our Dynamic Bayesian Network. © 2019 Copyright held by the owner/author(s).",Dynamic Bayesian network; Multimodal behavior assessment; Oral presentation; Social intelligence; Wearable devices,Bayesian networks; Behavioral research; Biofeedback; Eye movements; K-means clustering; Technical presentations; Wearable technology; Dynamic Bayesian networks; Multi-modal; Oral presentations; Social intelligence; Wearable devices; Signal processing
Trusting virtual agents: The effect of personality,2019,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063219546&doi=10.1145%2f3232077&partnerID=40&md5=c5fc5c1b24229a6b176f1df776cad1fd,"We present artificial intelligent (AI) agents that act as interviewers to engage with a user in a text-based conversation and automatically infer the user's personality traits. We investigate how the personality of an AI interviewer and the inferred personality of a user influences the user's trust in the AI interviewer from two perspectives: the user's willingness to confide in and listen to an AI interviewer. We have developed two AI interviewers with distinct personalities and deployed them in a series of real-world events. We present findings fromfour such deployments involving 1,280 users, including 606 actual job applicants. Notably, users are more willing to confide in and listen to an AI interviewer with a serious, assertive personality in a highstakes job interview. Moreover, users' personality traits, inferred from their chat text, along with interview context, influence their perception of and their willingness to confide in and listen to an AI interviewer. Finally, we discuss the design implications of our work on building hyper-personalized, intelligent agents. © 2019 Association for Computing Machinery.",AI interviewer; Big 5 personality; Chatbots; Computer personality; Conversational agents; Human-machine trust; Individual differences; Personality inference,Human computer interaction; Intelligent agents; Big 5 personality; Chatbots; Conversational agents; Human-machine; Individual Differences; Personality inference; Surveys
Attentive video: A multimodal approach to quantify emotional responses to mobile advertisements,2019,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063198420&doi=10.1145%2f3232233&partnerID=40&md5=6bfc34007753821c4355d44d65189eb6,"Understanding a target audience's emotional responses to a video advertisement is crucial to evaluate the advertisement's effectiveness. However, traditionalmethods for collecting such information are slow, expensive, and coarse grained. We propose AttentiveVideo, a scalable intelligent mobile interface with corresponding inference algorithms to monitor and quantify the effects of mobile video advertising in real time.Without requiring additional sensors, AttentiveVideo employs a combination of implicit photoplethysmography (PPG) sensing and facial expression analysis (FEA) to detect the attention, engagement, and sentiment of viewers as they watch video advertisements on unmodified smartphones. In a 24-participant study, AttentiveVideo achieved good accuracy on a wide range of emotional measures (the best average accuracy = 82.6% across nine measures). While feature fusion alone did not improve prediction accuracy with a single model, it significantly improved the accuracy when working together with model fusion.We also found that the PPG sensing channel and the FEA technique have different strength in data availability, latency detection, accuracy, and usage environment. These findings show the potential for both low-cost collection and deep understanding of emotional responses to mobile video advertisements. © 2019 Association for Computing Machinery.",Computational advertising; Facial expression; Heart rate; Mobile interfaces,Inference engines; Computational advertisings; Facial expression analysis; Facial Expressions; Heart rates; Mobile advertisement; Mobile interface; Multi-modal approach; Photoplethysmography (PPG); Marketing
Modeling and computational characterization of twitter customer service conversations,2019,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063191136&doi=10.1145%2f3213014&partnerID=40&md5=87edc23ced349beb704c1bebe2ab5702,"Given the increasing popularity of customer service dialogue on Twitter, analysis of conversation data is essential to understanding trends in customer and agent behavior for the purpose of automating customer service interactions. In this work, we develop a novel taxonomy of fine-grained ""dialogue acts"" frequently observed in customer service, showcasing acts that are more suited to the domain than the more generic existing taxonomies. Using a sequential SVM-HMM model, we model conversation flow, predicting the dialogue act of a given turn in real time, and showcase this using our ""PredDial"" portal. We characterize differences between customer and agent behavior in Twitter customer service conversations and investigate the effect of testing our system on different customer service industries. Finally, we use a data-driven approach to predict important conversation outcomes: customer satisfaction, customer frustration, and overall problem resolution. We show that the type and location of certain dialogue acts in a conversation have a significant effect on the probability of desirable and undesirable outcomes and present actionable rules based on our findings. We explore the correlations between different dialogue acts and the outcome of the conversations in detail using an actionable-rule discovery task by leveraging a state-of-the-art sequential rule mining algorithm while modeling a set of conversations as a set of sequences. The patterns and rules we derive can be used as guidelines for outcome-driven automated customer service platforms. © 2019 Association for Computing Machinery.",Conversation modeling; Correlations; Customer service; Dialogue; Sequential rule mining; Twitter,Sales; Social networking (online); Taxonomies; Correlations; Customer services; Dialogue; Sequential rule; Twitter; Customer satisfaction
Enhancing deep learning with visual interactions,2019,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062519723&doi=10.1145%2f3150977&partnerID=40&md5=ab239cfee1eb5a4d38826b1209948ae8,"Deep learning has emerged as a powerful tool for feature-driven labeling of datasets. However, for it to be effective, it requires a large and finely labeled training dataset. Precisely labeling a large training dataset is expensive, time-consuming, and error prone. In this article,we present a visually driven deep-learning approach that starts with a coarsely labeled training dataset and iteratively refines the labeling through intuitive interactions that leverage the latent structures of the dataset. Our approach can be used to (a) alleviate the burden of intensive manual labeling that captures the fine nuances in a high-dimensional dataset by simple visual interactions, (b) replace a complicated (and therefore difficult to design) labeling algorithm by a simpler (but coarse) labeling algorithm supplemented by user interaction to refine the labeling, or (c) use low-dimensional features (such as the RGB colors) for coarse labeling and turn to higher-dimensional latent structures that are progressively revealed by deep learning, for fine labeling.We validate our approach through use cases on three high-dimensional datasets and a user study. © 2019 Association for Computing Machinery.",Deep learning; Dimensionality reduction; Knowledge discovery; Semantic interaction; Visual interaction,Data mining; Iterative methods; Large dataset; Semantics; Dimensionality reduction; High dimensional datasets; High-dimensional dataset; Higher-dimensional; Intuitive interaction; Labeling algorithms; Semantic interactions; Visual interaction; Deep learning
Developing a hand gesture recognition system for mapping symbolic hand gestures to analogous emojis in computer-mediated communication,2019,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062522521&doi=10.1145%2f3297277&partnerID=40&md5=d722834202878ce028e434ae61447c7d,"Recent trends in computer-mediated communication (CMC) have not only led to expanded instant messaging through the use of images and videos but have also expanded traditional text messaging with richer content in the form of visual communication markers (VCMs) such as emoticons, emojis, and stickers. VCMs could prevent a potential loss of subtle emotional conversation in CMC, which is delivered by nonverbal cues that convey affective and emotional information. However, as the number of VCMs grows in the selection set, the problem of VCM entry needs to be addressed. Furthermore, conventional means of accessing VCMs continue to rely on input entry methods that are not directly and intimately tied to expressive nonverbal cues. In this work, we aim to address this issue by facilitating the use of an alternative form of VCM entry: Hand gestures. To that end, we propose a user-defined hand gesture set that is highly representative of a number of VCMs and a two-stage hand gesture recognition system (trajectory-based, shape-based) that can identify these user defined hand gestures with an accuracy of 82%. By developing such a system, we aim to allow people using low-bandwidth forms of CMCs to still enjoy their convenient and discreet properties while also allowing them to experience more of the intimacy and expressiveness of higher-bandwidth online communication. © 2019 Association for Computing Machinery.",Computermediated communication; Emojis; Hand gesture recognition; Shape-based recognition; Trajectory-based recognition; Visual communication markers,Bandwidth; Palmprint recognition; Text messaging; Visual communication; Computer-mediated communication; Emojis; Hand-gesture recognition; Shape based; Trajectory-based; Gesture recognition
Miscommunication detection and recovery in situated human-robot dialogue,2019,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062322016&doi=10.1145%2f3237189&partnerID=40&md5=9ed8bdaf51b3dfc2d84b115ebb72b21b,"Even without speech recognition errors, robots may face difficulties interpreting natural-language instructions. We present a method for robustly handling miscommunication between people and robots in task-oriented spoken dialogue. This capability is implemented in TeamTalk, a conversational interface to robots that supports detection and recovery from the situated grounding problems of referential ambiguity and impossible actions. We introduce a representation that detects these problems and a nearest-neighbor learning algorithm that selects recovery strategies for a virtual robot. When the robot encounters a grounding problem, it looks back on its interaction history to consider how it resolved similar situations. The learning method is trained initially on crowdsourced data but is then supplemented by interactions from a longitudinal user study in which six participants performed navigation tasks with the robot. We compare results collected using a general model to user-specific models and find that user-specific models perform best on measures of dialogue efficiency, while the general model yields the highest agreement with human judges. Our overall contribution is a novel approach to detecting and recovering from miscommunication in dialogue by including situated context, namely, information from a robot's path planner and surroundings. © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Human-robot communication; Human-robot interaction; Language grounding; Physically situated dialogue; Spoken-dialogue systems,Human computer interaction; Learning algorithms; Recovery; Speech processing; Speech recognition; Conversational interface; Human-robot communication; Interaction history; Language grounding; Nearest neighbor learning; Physically situated dialogue; Recovery strategies; Spoken dialogue system; Human robot interaction
"Toward effective robot-child tutoring: Internal motivation, behavioral intervention, and learning outcomes",2019,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062219393&doi=10.1145%2f3213768&partnerID=40&md5=6bf6648e967fc586f964911bfcab690e,"Personalized learning environments have the potential to improve learning outcomes for children in a variety of educational domains, as they can tailor instruction based on the unique learning needs of individuals. Robot tutoring systems can further engage users by leveraging their potential for embodied social interaction and take into account crucial aspects of a learner, such as a student's motivation in learning. In this article, we demonstrate that motivation in young learners corresponds to observable behaviors when interacting with a robot tutoring system, which, in turn, impact learning outcomes. We first detail a user study involving children interacting one on one with a robot tutoring system over multiple sessions. Based on empirical data, we show that academic motivation stemming from one's own values or goals as assessed by the Academic Self-Regulation Questionnaire (SRQ-A) correlates to observed suboptimal help-seeking behavior during the initial tutoring session. We then show how an interactive robot that responds intelligently to these observed behaviors in subsequent tutoring sessions can positively impact both student behavior and learning outcomes over time. These results provide empirical evidence for the link between internal motivation, observable behavior, and learning outcomes in the context of robot-child tutoring. We also identified an additional suboptimal behavioral feature within our tutoring environment and demonstrated its relationship to internal factors of motivation, suggesting further opportunities to design robot intervention to enhance learning. We provide insights on the design of robot tutoring systems aimed to deliver effective behavioral intervention during learning interactions for children and present a discussion on the broader challenges currently faced by robot-child tutoring systems. © 2019 Association for Computing Machinery.",Child-robot interaction; Education; Motivation; Tutoring,Computer aided instruction; Education; Machine design; Motivation; Robots; Students; Academic motivations; Behavioral interventions; Child-robot interactions; Internal motivations; Learning interactions; Personalized learning environments; Social interactions; Tutoring; Learning systems
Visual exploration of air quality data with a time-correlation-partitioning tree based on information theory,2019,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062234053&doi=10.1145%2f3182187&partnerID=40&md5=809501068b7ab9ffec44086670b13ad7,"Discovering the correlations among variables of air quality data is challenging, because the correlation time series are long-lasting, multi-faceted, and information-sparse. In this article, we propose a novel visual representation, called Time-correlation-partitioning (TCP) tree, that compactly characterizes correlations of multiple air quality variables and their evolutions. A TCP tree is generated by partitioning the information-theoretic correlation time series into pieces with respect to the variable hierarchy and temporal variations, and reorganizing these pieces into a hierarchically nested structure. The visual exploration of a TCP tree provides a sparse data traversal of the correlation variations and a situation-aware analysis of correlations among variables. This can help meteorologists understand the correlations among air quality variables better. We demonstrate the efficiency of our approach in a real-world air quality investigation scenario. © 2019 Association for Computing Machinery.",Information theory; Multivariate time series; Sensor; Transfer entropy,Air quality; Information theory; Sensors; Time series; Transmission control protocol; Visualization; Information-theoretic correlations; Multivariate time series; Nested structures; Temporal variation; Time correlations; Transfer entropy; Visual exploration; Visual representations; Trees (mathematics)
Analysis of movement quality in full-body physical activities,2019,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055319523&doi=10.1145%2f3132369&partnerID=40&md5=fadd3a30b8d64fb49f6c02920de83927,"Full-body human movement is characterized by fine-grain expressive qualities that humans are easily capable of exhibiting and recognizing in others' movement. In sports (e.g., martial arts) and performing arts (e.g., dance), the same sequence of movements can be performed in a wide range of ways characterized by different qualities, often in terms of subtle (spatial and temporal) perturbations of the movement. Even a non-expert observer can distinguish between a top-level and average performance by a dancer or martial artist. The difference is not in the performed movements-the same in both cases-but in the “quality” of their performance. In this article, we present a computational framework aimed at an automated approximate measure of movement quality in full-body physical activities. Starting from motion capture data, the framework computes low-level (e.g., a limb velocity) and high-level (e.g., synchronization between different limbs) movement features. Then, this vector of features is integrated to compute a value aimed at providing a quantitative assessment of movement quality approximating the evaluation that an external expert observer would give of the same sequence of movements. Next, a system representing a concrete implementation of the framework is proposed. Karate is adopted as a testbed. We selected two different katas (i.e., detailed choreographies of movements in karate) characterized by different overall attitudes and expressions (aggressiveness, meditation), and we asked seven athletes, having various levels of experience and age, to perform them. Motion capture data were collected from the performances and were analyzed with the system. The results of the automated analysis were compared with the scores given by 14 karate experts who rated the same performances. Results show that the movement-quality scores computed by the system and the ratings given by the human observers are highly correlated (Pearson's correlations r = 0.84, p = 0.001 and r = 0.75, p = 0.005). © 2019 Copyright held by the owner/author(s).",Dance; Full-body movement; Gesture analysis; Karate; Movement quality,Artificial intelligence; Automated analysis; Computational framework; Dance; Full-body movement; Gesture analysis; Karate; Motion capture data; Quantitative assessments; Quality control
Toward an understanding of trust repair in human-robot interaction: Current research and future directions,2018,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057032964&doi=10.1145%2f3181671&partnerID=40&md5=56a8068ef239b0565e210a6fcd7e311a,"Gone are the days of robots solely operating in isolation, without direct interaction with people. Rather, robots are increasingly being deployed in environments and roles that require complex social interaction with humans. The implementation of human-robot teams continues to increase as technology develops in tandem with the state of human-robot interaction (HRI) research. Trust, a major component of human interaction, is an important facet of HRI. However, the ideas of trust repair and trust violations are understudied in the HRI literature. Trust repair is the activity of rebuilding trust after one party breaks the trust of another. These trust breaks are referred to as trust violations. Just as with humans, trust violations with robots are inevitable; as a result, a clear understanding of the process of HRI trust repair must be developed in order to ensure that a human-robot team can continue to perform well after a trust violation. Previous research on human-automation trust and human-human trust can serve as starting places for exploring trust repair in HRI. Although existing models of human-automation and human-human trust are helpful, they do not account for some of the complexities of building and maintaining trust in unique relationships between humans and robots. The purpose of this article is to provide a foundation for exploring human-robot trust repair by drawing upon prior work in the human-robot, human-automation, and human-human trust literature, concluding with recommendations for advancing this body of work. © 2018 Association for Computing Machinery.",Automation; Human-robot interaction; Human-robot trust; Trust; Trust repair,Automation; Man machine systems; Repair; Direct interactions; Human interactions; Human robot Interaction (HRI); Human robots; Human-robot-team; Social interactions; Trust; Trust repairs; Human robot interaction
ACM transactions on interactive intelligent systems (TIIS) special issue on trust and influence in intelligent human-machine interaction,2018,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057046305&doi=10.1145%2f3281451&partnerID=40&md5=b31f37304e1b59421483ddee07d516f1,[No abstract available],Human-robot interaction; Trust,
Trust-based multi-robot symbolic motion planning with a human-in-the-loop,2018,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057047865&doi=10.1145%2f3213013&partnerID=40&md5=6f1e447110c2288926b4efa8ed7d1a0d,"Symbolic motion planning for robots is the process of specifying and planning robot tasks in a discrete space, then carrying them out in a continuous space in a manner that preserves the discrete-level task specifications. Despite progress in symbolic motion planning, many challenges remain, including addressing scalability for multi-robot systems and improving solutions by incorporating human intelligence. In this article, distributed symbolic motion planning for multi-robot systems is developed to address scalability. More specifically, compositional reasoning approaches are developed to decompose the global planning problem, and atomic propositions for observation, communication, and control are proposed to address inter-robot collision avoidance. To improve solution quality and adaptability, a hypothetical dynamic, quantitative, and probabilistic human-to-robot trust model is developed to aid this decomposition. Furthermore, a trust-based real-time switching framework is proposed to switch between autonomous and manual motion planning for tradeoffs between task safety and efficiency. Deadlock- and livelock-free algorithms are designed to guarantee reachability of goals with a human-in-the-loop. A set of nontrivial multi-robot simulations with direct human inputs and trust evaluation is provided, demonstrating the successful implementation of the trust-based multi-robot symbolic motion planning methods. © 2018 Association for Computing Machinery.",Human-in-the-loop; Multi-robot systems; Symbolic motion planning; Trust,Industrial robots; Intelligent robots; Motion planning; Multipurpose robots; Robot learning; Scalability; Atomic propositions; Compositional reasoning; Human-in-the-loop; Motion planning methods; Multi-robot systems; Real-time switching; Safety and efficiencies; Trust; Robot programming
A classification model for sensing human trust in machines using EEG and GSR,2018,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052551737&doi=10.1145%2f3132743&partnerID=40&md5=9b4daf45d807fc7e79a980ccc5e29129,"Today, intelligent machines interact and collaborate with humans in a way that demands a greater level of trust between human and machine. A first step toward building intelligent machines that are capable of building and maintaining trust with humans is the design of a sensor that will enable machines to estimate human trust level in real time. In this article, two approaches for developing classifier-based empirical trust-sensor models are presented that specifically use electroencephalography and galvanic skin response measurements. Human subject data collected from 45 participants is used for feature extraction, feature selection, classifier training, and model validation. The first approach considers a general set of psychophysiological features across all participants as the input variables and trains a classifier-based model for each participant, resulting in a trust-sensor model based on the general feature set (i.e., a “general trust-sensor model”). The second approach considers a customized feature set for each individual and trains a classifier-based model using that feature set, resulting in improved mean accuracy but at the expense of an increase in training time. This work represents the first use of real-time psychophysiological measurements for the development of a human trust sensor. Implications of the work, in the context of trust management algorithm design for intelligent machines, are also discussed. © 2018 Association for Computing Machinery.",Classifiers; EEG; GSR; Human-machine interaction; Intelligent system; Modeling; Psychophysiological measurement; Trust in automation,Classification (of information); Classifiers; Electroencephalography; Electrophysiology; Feature extraction; Intelligent systems; Models; Algorithm design; Classification models; Classifier training; Galvanic skin response; Human machine interaction; Intelligent machine; Model validation; Trust management; Structural design
Cues of violent intergroup conflict diminish perceptions of robotic personhood,2018,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057055908&doi=10.1145%2f3181674&partnerID=40&md5=790bc378d1eb6d578467abec893d0390,"Convergent lines of evidence indicate that anthropomorphic robots are represented using neurocognitive mechanisms typically employed in social reasoning about other people. Relatedly, a growing literature documents that contexts of threat can exacerbate coalitional biases in social perceptions. Integrating these research programs, the present studies test whether cues of violent intergroup conflict modulate perceptions of the intelligence, emotional experience, or overall personhood of robots. In Studies 1 and 2, participants evaluated a large, bipedal all-terrain robot; in Study 3, participants evaluated a small, social robot with humanlike facial and vocal characteristics. Across all studies, cues of violent conflict caused significant decreases in perceived robotic personhood, and these shifts were mediated by parallel reductions in emotional connection with the robot (with no significant effects of threat on attributions of intelligence/skill). In addition, in Study 2, participants in the conflict condition estimated the large bipedal robot to be less effective in military combat, and this difference was mediated by the reduction in perceived robotic personhood. These results are discussed as they motivate future investigation into the links among threat, coalitional bias and human-robot interaction. © 2018 Association for Computing Machinery.",Empathy; Group prejudice; Human-robot interaction; Theory of mind; Threat detection,Anthropomorphic robots; Behavioral research; Group theory; Intelligent robots; Man machine systems; Robotics; Emotional connections; Emotional experiences; Empathy; Group prejudice; Research programs; Social perception; Theory of minds; Threat detection; Human robot interaction
Modeling the human-robot trust phenomenon: A conceptual framework based on risk,2018,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056996825&doi=10.1145%2f3152890&partnerID=40&md5=e725d5ba4b5a5fc7e8fe0530e77b6a36,"This article presents a conceptual framework for human-robot trust which uses computational representations inspired by game theory to represent a definition of trust, derived from social psychology. This conceptual framework generates several testable hypotheses related to human-robot trust. This article examines these hypotheses and a series of experiments we have conducted which both provide support for and also conflict with our framework for trust. We also discuss the methodological challenges associated with investigating trust. The article concludes with a description of the important areas for future research on the topic of human-robot trust. © 2018 Association for Computing Machinery.",Human-robot trust; Risk; Social robotics; Trust,Computation theory; Game theory; Risks; Robotics; Conceptual frameworks; Human robots; Social psychology; Social robotics; Trust; Robots
The effect of culture on trust in automation: Reliability and workload,2018,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056998686&doi=10.1145%2f3230736&partnerID=40&md5=ca0bd6cb1d2a88dee2aebfb7a469d843,"Trust in automation has become a topic of intensive study since the late 1990s and is of increasing importance with the advent of intelligent interacting systems. While the earliest trust experiments involved human interventions to correct failures/errors in automated control systems, a majority of subsequent studies have investigated information acquisition and analysis decision aiding tasks such as target detection for which automation reliability is more easily manipulated. Despite the high level of international dependence on automation in industry, almost all current studies have employed Western samples primarily from the U.S. The present study addresses these gaps by running a large sample experiment in three (U.S., Taiwan, and Turkey) diverse cultures using a “trust sensitive task” consisting of both automated control and target detection subtasks. This article presents results for the target detection subtask for which reliability and task load were manipulated. The current experiments allow us to determine whether reported effects are universal or specific to Western culture, vary in baseline or magnitude, or differ across cultures. Results generally confirm consistent effects of manipulations across the three cultures as well as cultural differences in initial trust and variation in effects of manipulations consistent with 10 cultural hypotheses based on Hofstede's Cultural Dimensions and Leung and Cohen's theory of Cultural Syndromes. These results provide critical implications and insights for correct trust calibration and to enhance human trust in intelligent automation systems across cultures. Additionally, our results would be useful in designing intelligent systems for users of different cultures. Our article presents the following contributions: First, to the best of our knowledge, this is the first set of studies that deal with cultural factors across all the cultural syndromes identified in the literature by comparing trust in the Honor, Face, Dignity cultures. Second, this is the first set of studies that uses a validated cross-cultural trust measure for measuring trust in automation. Third, our experiments are the first to study the dynamics of trust across cultures. © 2018 Association for Computing Machinery.",Cross-cultural research; Experimentation; Human factors; Performance; Reliability; Trust in automation,Control system analysis; Decision support systems; Human engineering; Intelligent systems; Reliability; Reliability analysis; Automated control systems; Automation reliabilities; Cross-cultural research; Experimentation; Hofstede's cultural dimensions; Information acquisitions; Intelligent automation systems; Performance; Automation
MobInsight: A framework using semantic neighborhood features for localized interpretations of urban mobility,2018,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053408688&doi=10.1145%2f3158433&partnerID=40&md5=fc8b9a9117eb6bb8006123bae486f0d9,"Collective urban mobility embodies the residents' local insights on the city. Mobility practices of the residents are produced from their spatial choices, which involve various considerations such as the atmosphere of destinations, distance, past experiences, and preferences. The advances in mobile computing and the rise of geo-social platforms have provided the means for capturing the mobility practices; however, interpreting the residents' insights is challenging due to the scale and complexity of an urban environment and its unique context. In this article, we present MobInsight, a framework for making localized interpretations of urban mobility that reflect various aspects of the urbanism. MobInsight extracts a rich set of neighborhood features through holistic semantic aggregation, and models the mobility between all-pairs of neighborhoods.We evaluate MobInsight with the mobility data of Barcelona and demonstrate diverse localized and semantically rich interpretations. © 2018 ACM.",Mobility; Neighborhood features; Semantic aggregation; Social annotations; Urban informatics,Carrier mobility; Regional planning; Barcelona; Mobility datum; Neighborhood features; Semantic aggregation; Social annotations; Urban environments; Urban Informatics; Urban mobility; Semantics
An active sleep monitoring framework using wearables,2018,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053384892&doi=10.1145%2f3185516&partnerID=40&md5=28a5050eb3c5f750d6a081418098f7b2,"Sleep is the most important aspect of healthy and active living. The right amount of sleep at the right time helps an individual to protect his or her physical, mental, and cognitive health and maintain his or her quality of life. The most durative of the Activities of Daily Living (ADL), sleep has a major synergic influence on a person's fuctional, behavioral, and cognitive health. A deep understanding of sleep behavior and its relationship with its physiological signals, and contexts (such as eye or body movements), is necessary to design and develop a robust intelligent sleep monitoring system. In this article, we propose an intelligent algorithm to detect the microscopic states of sleep that fundamentally constitute the components of good and bad sleeping behaviors and thus help shape the formative assessment of sleep quality. Our initial analysis includes the investigation of several classification techniques to identify and correlate the relationship of microscopic sleep states with overall sleep behavior. Subsequently, we also propose an online algorithm based on change point detection to process and classify the microscopic sleep states. We also develop a lightweight version of the proposed algorithm for real-time sleep monitoring, recognition, and assessment at scale. For a larger deployment of our proposed model across a community of individuals, we propose an active-learning-based methodology to reduce the effort of ground-truth data collection and labeling. Finally, we evaluate the performance of our proposed algorithms on real data traces and demonstrate the efficacy of our models for detecting and assessing the fine-grained sleep states beyond an individual. © 2018 ACM.",Active learning; Crowdsourcing; Gradient classifier; Sleep monitoring; Wearable technology,Artificial intelligence; Crowdsourcing; Eye movements; Wearable technology; Active Learning; Activities of Daily Living; Change point detection; Classification technique; Formative assessment; Intelligent Algorithms; Physiological signals; Sleep monitoring; Sleep research
Predicting user confidence during visual decision making,2018,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061741550&doi=10.1145%2f3185524&partnerID=40&md5=c5918c3ff1d24a1b36dd6579e45fc300,"People are not infallible consistent “oracles”: their confidence in decision-making may vary significantly between tasks and over time. We have previously reported the benefits of using an interface and algorithms that explicitly captured and exploited users’ confidence: error rates were reduced by up to 50% for an industrial multi-class learning problem; and the number of interactions required in a design-optimisation context was reduced by 33%. Having access to users’ confidence judgements could significantly benefit intelligent interactive systems in industry, in areas such as intelligent tutoring systems and in health care. There are many reasons for wanting to capture information about confidence implicitly. Some are ergonomic, but others are more “social”—such as wishing to understand (and possibly take account of) users’ cognitive state without interrupting them. We investigate the hypothesis that users’ confidence can be accurately predicted from measurements of their behaviour. Eye-tracking systems were used to capture users’ gaze patterns as they undertook a series of visual decision tasks, after each of which they reported their confidence on a 5-point Likert scale. Subsequently, predictive models were built using “conventional” machine learning approaches for numerical summary features derived from users’ behaviour. We also investigate the extent to which the deep learning paradigm can reduce the need to design features specific to each application by creating “gaze maps”—visual representations of the trajectories and durations of users’ gaze fixations—and then training deep convolutional networks on these images. Treating the prediction of user confidence as a two-class problem (confident/not confident), we attained classification accuracy of 88% for the scenario of new users on known tasks, and 87% for known users on new tasks. Considering the confidence as an ordinal variable, we produced regression models with a mean absolute error of ≈0.7 in both cases. Capturing just a simple subset of non-task-specific numerical features gave slightly worse, but still quite high accuracy (e.g., MAE ≈ 1.0). Results obtained with gaze maps and convolutional networks are competitive, despite not having access to longer-term information about users and tasks, which was vital for the “summary” feature sets. This suggests that the gaze-map-based approach forms a viable, transferable alternative to handcrafting features for each different application. These results provide significant evidence to confirm our hypothesis, and offer a way of substantially improving many interactive artificial intelligence applications via the addition of cheap non-intrusive hardware and computationally cheap prediction algorithms. © 2018 ACM",Confidence; Human-centred machine learning,Behavioral research; Computer aided instruction; Convolution; Decision making; Deep learning; Forecasting; Machine learning; Regression analysis; Classification accuracy; Confidence; Convolutional networks; Intelligent interactive systems; Intelligent tutoring system; Machine learning approaches; Prediction algorithms; Visual representations; Eye tracking
It's not just about accuracy: Metrics that matter when modeling expert sketching ability,2018,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053420750&doi=10.1145%2f3181673&partnerID=40&md5=1923486266a91c485aac1d260a450918,"Design sketching is an important skill for designers, engineers, and creative professionals, as it allows them to express their ideas and concepts in a visual medium. Being a critical and versatile skill for many different disciplines, courses on design sketching are often taught in universities. Courses today predominately rely on pen and paper; however, this traditional pedagogy is limited by the availability of human instructors, who can provide personalized feedback. Using a stylus-based intelligent tutoring system called SketchTivity, we aim to eventually mimic the feedback given by an instructor and assess student-drawn sketches to give students insight into areas for improvement. To provide effective feedback to users, it is important to identify what aspects of their sketches they should work on to improve their sketching ability. After consulting with several domain experts in sketching, we came up with several classes of features that could potentially differentiate expert and novice sketches. Because improvement on one metric, such as speed, may result in a decrease in another metric, such as accuracy, the creation of a single score may not mean much to the user. We attempted to create a single internal score that represents overall drawing skill so that the system can track improvement over time and found that this score correlates highly with expert rankings. We gathered over 2,000 sketches from 20 novices and four experts for analysis. We identified key metrics for quality assessment that were shown to significantly correlate with the quality of expert sketches and provide insight into providing intelligent user feedback in the future. © 2018 ACM.",Design education; Design sketching; Intelligent feedback; Intelligent tutoring system; Sketch recognition,Availability; Computer aided instruction; Curricula; Education computing; Teaching; Creative professionals; Design Education; Expert and novices; Intelligent tutoring system; Personalized feedback; Quality assessment; Sketch recognition; Traditional pedagogy; Drawing (graphics)
Using machine learning to support qualitative coding in social science: Shifting the focus to ambiguity,2018,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056839431&doi=10.1145%2f3185515&partnerID=40&md5=3a9f19724f37708dd02443e28329b5ca,"Machine learning (ML) has become increasingly influential to human society, yet the primary advancements and applications of ML are driven by research in only a few computational disciplines. Even applications that affect or analyze human behaviors and social structures are often developed with limited input from experts outside of computational fields. Social scientists—experts trained to examine and explain the complexity of human behavior and interactions in the world—have considerable expertise to contribute to the development of ML applications for human-generated data, and their analytic practices could benefit from more human-centered ML methods. Although a few researchers have highlighted some gaps between ML and social sciences [51, 57, 70], most discussions only focus on quantitative methods. Yet many social science disciplines rely heavily on qualitative methods to distill patterns that are challenging to discover through quantitative data. One common analysis method for qualitative data is qualitative coding. In this article, we highlight three challenges of applying ML to qualitative coding. Additionally, we utilize our experience of designing a visual analytics tool for collaborative qualitative coding to demonstrate the potential in using ML to support qualitative coding by shifting the focus to identifying ambiguity. We illustrate dimensions of ambiguity and discuss the relationship between disagreement and ambiguity. Finally, we propose three research directions to ground ML applications for social science as part of the progression toward human-centered machine learning. © 2018 ACM",Ambiguity; Computational social science; Human-centered machine learning; Machine learning; Qualitative coding; Social scientists,Codes (symbols); Learning systems; Machine learning; Ambiguity; Computational field; Computational social science; Qualitative coding; Qualitative method; Quantitative method; Science disciplines; Social scientists; Behavioral research
Dynamic handwriting signal features predict domain expertise,2018,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053381196&doi=10.1145%2f3213309&partnerID=40&md5=a1b47a71e4e2b42255c87d27c32f61d8,"As commercial pen-centric systems proliferate, they create a parallel need for analytic techniques based on dynamic writing.Within educational applications, recent empirical research has shown that signal-level features of students' writing, such as stroke distance, pressure and duration, are adapted to conserve total energy expenditure as they consolidate expertise in a domain. The present research examined how accurately three different machine-learning algorithms could automatically classify users' domain expertise based on signal features of their writing, without any content analysis. Compared with an unguided machine-learning classification accuracy of 71%, hybrid methods using empirical-statistical guidance correctly classified 79-92% of students by their domain expertise level. In addition to improved accuracy, the hybrid approach contributed a causal understanding of prediction success and generalization to new data. These novel findings open up opportunities to design new automated learning analytic systems and student-adaptive educational technologies for the rapidly expanding sector of commercial pen systems. © 2018 ACM.",Dynamic handwriting; Empirical and statistical sciences; Hybrid techniques; Machine learning; Multimodal learning analytics; Pen signal features; Prediction of domain expertise; Total energy expenditure,Artificial intelligence; Forecasting; Learning systems; Students; Domain expertise; Empirical and statistical sciences; Hybrid techniques; Multi-modal learning; Signal features; Total energy; Learning algorithms
Creating new technologies for companionable agents to support isolated older adults,2018,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053431661&doi=10.1145%2f3213050&partnerID=40&md5=bc5c16a957debf7f3d2014d299145c35,"This article reports on the development of capabilities for (on-screen) virtual agents and robots to support isolated older adults in their homes. A real-time architecture was developed to use a virtual agent or a robot interchangeably to interact via dialog and gesture with a human user. Users could interact with either agent on 12 different activities, some of which included on-screen games, and forms to complete. The article reports on a pre-study that guided the choice of interaction activities. A month-long study with 44 adults between the ages of 55 and 91 assessed differences in the use of the robot and virtual agent. © 2018 ACM.",Engagement; Human-agent interaction studies; Human-robot interaction; Human-robot interaction studies; Older adults; Realtime dialog systems; Robot compantions; Situated dialog systems; Social isolation; Virtual agent companions; Virtual agents,Man machine systems; Virtual reality; Dialog systems; Engagement; Human agent interactions; Older adults; Social isolation; Virtual agent; Human robot interaction
A human-in-the-loop system for sound event detection and annotation,2018,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068465689&doi=10.1145%2f3214366&partnerID=40&md5=ec283d51337527a5f2201f2b18e7f64e,"Labeling of audio events is essential for many tasks. However, finding sound events and labeling them within a long audio file is tedious and time-consuming. In cases where there is very little labeled data (e.g., a single labeled example), it is often not feasible to train an automatic labeler because many techniques (e.g., deep learning) require a large number of human-labeled training examples. Also, fully automated labeling may not show sufficient agreement with human labeling for many uses. To solve this issue, we present a human-in-the-loop sound labeling system that helps a user quickly label target sound events in a long audio. It lets a user reduce the time required to label a long audio file (e.g., 20 hours) containing target sounds that are sparsely distributed throughout the recording (10% or less of the audio contains the target) when there are too few labeled examples (e.g., one) to train a state-of-the-art machine audio labeling system. To evaluate the effectiveness of our tool, we performed a human-subject study. The results show that it helped participants label target sound events twice as fast as labeling them manually. In addition to measuring the overall performance of the proposed system, we also measure interaction overhead and machine accuracy, which are two key factors that determine the overall performance. The analysis shows that an ideal interface that does not have interaction overhead at all could speed labeling by as much as a factor of four. © 2018 ACM",Human-in-the-loop system; Interactive machine learning; Sound event detection,Deep learning; Fully automated; Human subjects; Human-in-the-loop; Interactive machine learning; Machine accuracy; Sound event detection; State of the art; Training example; Audio acoustics
Estimating collective attention toward a public display,2018,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053385944&doi=10.1145%2f3230715&partnerID=40&md5=184b3baf152ff274b27814a3b373a034,"Enticing groups of passers-by to focused interaction with a public display requires the display system to take appropriate action that depends on how much attention the group is already paying to the display. In the design of such a system, we might want to present the content so that it indicates that a part of the group that is looking head-on at the display has already been registered and is addressed individually, whereas it simultaneously emits a strong audio signal that makes the inattentive rest of the group turn toward it. The challenge here is to define and delimit adequate mixed attention states for groups of people, allowing for classifying collective attention based on inhomogeneous variants of individual attention, i.e., where some group members might be highly attentive, others even interacting with the public display, and some unperceptive. In this article, we present a model for estimating collective human attention toward a public display and investigate technical methods for practical implementation that employs measurement of physical expressive features of people appearing within the display's field of view (i.e., the basis for deriving a person's attention). We delineate strengths and weaknesses and prove the potentials of our model by experimentally exerting influence on the attention of groups of passers-by in a public gaming scenario. © 2018 ACM.",Attention estimation; Finite state machines; Multilayer perceptron; Neural networks; Support vector machines,Display devices; Finite automata; Neural networks; Support vector machines; Attention estimations; Audio signal; Display system; Field of views; Group members; Human attention; Public display; Multilayer neural networks
Visualizing ubiquitously sensed measures of motor ability in multiple sclerosis: Reflections on communicating machine learning in practice,2018,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068464931&doi=10.1145%2f3181670&partnerID=40&md5=0453bf705cd61483cc094fd387815d7f,"Sophisticated ubiquitous sensing systems are being used to measure motor ability in clinical settings. Intended to augment clinical decision-making, the interpretability of the machine-learning measurements underneath becomes critical to their use. We explore how visualization can support the interpretability of machine-learning measures through the case of Assess MS, a system to support the clinical assessment of Multiple Sclerosis. A substantial design challenge is to make visible the algorithm’s decision-making process in a way that allows clinicians to integrate the algorithm’s result into their own decision process. To this end, we present a series of design iterations that probe the challenges in supporting interpretability in a real-world system. The key contribution of this article is to illustrate that simply making visible the algorithmic decision-making process is not helpful in supporting clinicians in their own decision-making process. It disregards that people and algorithms make decisions in different ways. Instead, we propose that visualisation can provide context to algorithmic decision-making, rendering observable a range of internal workings of the algorithm from data quality issues to the web of relationships generated in the machine-learning process. © 2018 ACM",Health; Human-centred machine learning; In-the-wild study; Visualization,Behavioral research; Decision making; Flow visualization; Health; Learning algorithms; Visualization; Clinical assessments; Clinical decision making; Clinical settings; Decision making process; Decision process; Design challenges; Multiple sclerosis; Real-world system; Machine learning
Proactive information retrieval by capturing search intent from primary task context,2018,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053398921&doi=10.1145%2f3150975&partnerID=40&md5=38fd1b0da9ef81ca0d6321f120894ad1,"A significant fraction of information searches are motivated by the user's primary task. An ideal search engine would be able to use information captured from the primary task to proactively retrieve useful information. Previous work has shown that many information retrieval activities depend on the primary task in which the retrieved information is to be used, but fairly little research has been focusing on methods that automatically learn the informational intents from the primary task context.We study howthe implicit primary task context can be used tomodel the user's search intent and to proactively retrieve relevant and useful information. Data comprising of logs from a user study, in which users are writing an essay, demonstrate that users' search intents can be captured from the task and relevant and useful information can be proactively retrieved. Data from simulations with several datasets of different complexity show that the proposed approach of using primary task context generalizes to a variety of data.Our findings have implications for the design of proactive search systems that can infer users' search intent implicitly by monitoring users' primary task activities. © 2018 ACM.",Proactive search; Task-based information retrieval; User intent modeling,Information retrieval; Information use; Information search; Intent models; Primary task; Proactive search; Search intents; Search system; Task-based information; User study; Search engines
Perceptual validation for the generation of expressive movements from end-effector trajectories,2018,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053425605&doi=10.1145%2f3150976&partnerID=40&md5=48dece53031cc70737caa80ba94c0657,"Endowing animated virtual characters with emotionally expressive behaviors is paramount to improving the quality of the interactions between humans and virtual characters. Full-body motion, in particular, with its subtle kinematic variations, represents an effective way of conveying emotionally expressive content. However, before synthesizing expressive full-body movements, it is necessary to identify and understand what qualities of human motion are salient to the perception of emotions and how these qualities can be exploited to generate novel and equally expressive full-body movements. Based on previous studies, we argue that it is possible to perceive and generate expressive full-body movements from a limited set of joint trajectories, including end-effector trajectories and additional constraints such as pelvis and elbow trajectories. Hence, these selected trajectories define a significant and reduced motion space, which is adequate for the characterization of the expressive qualities of human motion and that is both suitable for the analysis and generation of emotionally expressive full-body movements. The purpose and main contribution of this work is the methodological framework we defined and used to assess the validity and applicability of the selected trajectories for the perception and generation of expressive full-body movements. This framework consists of the creation of a motion capture database of expressive theatrical movements, the development of a motion synthesis system based on trajectories re-played or re-sampled and inverse kinematics, and two perceptual studies. © 2018 ACM.",Emotion perception; Expressive motion analysis and synthesis; Full-body movements; Motion representation; Perceptual validation; Trajectories resampling,Behavioral research; End effectors; Inverse kinematics; Quality control; Virtual reality; Analysis and synthesis; Full-body movement; Motion representation; Perceptual validation; Resampling; Trajectories
Crowdsourcing ground truth for medical relation extraction,2018,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055101001&doi=10.1145%2f3152889&partnerID=40&md5=39718e6a50f271eb6c9297307f1059e5,"Cognitive computing systems require human-labeled data for evaluation and often for training. The standard practice used in gathering this data minimizes disagreement between annotators, and we have found this results in data that fails to account for the ambiguity inherent in language. We have proposed the CrowdTruth method for collecting ground truth through crowdsourcing, which reconsiders the role of people in machine learning based on the observation that disagreement between annotators provides a useful signal for phenomena such as ambiguity in the text. We report on using this method to build an annotated data set for medical relation extraction for the cause and treat relations and how this data performed in a supervised training experiment. We demonstrate that by modeling ambiguity, labeled data gathered from crowd workers can (1) reach the level of quality of domain experts for this task, while reducing the cost, and (2) provide better training data at scale than distant supervision. We further propose and validate new weighted measures for precision, recall, and F-measure, which account for ambiguity in both human and machine performance on this task. 2018 Copyright is held by the owner/author(s). Publication rights licensed to ACM.",Clinical natural language processing; Crowd Truth; CrowdTruth; Ground truth; Inter-annotator disagreement; Natural language ambiguity; Relation extraction,Cognitive systems; Extraction; Learning algorithms; Learning systems; Natural language processing systems; Crowd Truth; Ground truth; Inter-annotator disagreement; Natural languages; Relation extraction; Crowdsourcing
Introduction to the special issue on human-centered machine learning,2018,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058649913&doi=10.1145%2f3205942&partnerID=40&md5=6a92cc5379dbda4574d539a7144d2626,"Machine learning is one of the most important and successful techniques in contemporary computer science. Although it can be applied to myriad problems of human interest, research in machine learning is often framed in an impersonal way, as merely algorithms being applied to model data. However, this viewpoint hides considerable human work of tuning the algorithms, gathering the data, deciding what should be modeled in the first place, and using the outcomes of machine learning in the real world. Examining machine learning from a human-centered perspective includes explicitly recognizing human work, as well as reframing machine learning workflows based on situated human working practices, and exploring the co-adaptation of humans and intelligent systems. A human-centered understanding of machine learning in human contexts can lead not only to more usable machine learning tools, but to new ways of understanding what machine learning is good for and how to make it more useful. This special issue brings together nine articles that present different ways to frame machine learning in a human context. They represent very different application areas (from medicine to audio) and methodologies (including machine learning methods, human-computer interaction methods, and hybrids), but they all explore the human contexts in which machine learning is used. This introduction summarizes the articles in this issue and draws out some common themes. 2018 Copyright is held by the owner/author(s).",Human-centered machine learning; Interactive machine learning,Human computer interaction; Intelligent systems; Learning algorithms; Application area; Co-adaptation; Human context; Interactive machine learning; Machine learning methods; Model data; Situated human; Working practices; Learning systems
A review of user interface design for interactive machine learning,2018,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058625902&doi=10.1145%2f3185517&partnerID=40&md5=0d5b2e8b106ff5119baf0d50f88221e4,"Interactive Machine Learning (IML) seeks to complement human perception and intelligence by tightly integrating these strengths with the computational power and speed of computers. The interactive process is designed to involve input from the user but does not require the background knowledge or experience that might be necessary to work with more traditional machine learning techniques. Under the IML process, non-experts can apply their domain knowledge and insight over otherwise unwieldy datasets to find patterns of interest or develop complex data-driven applications. This process is co-adaptive in nature and relies on careful management of the interaction between human and machine. User interface design is fundamental to the success of this approach, yet there is a lack of consolidated principles on how such an interface should be implemented. This article presents a detailed review and characterisation of Interactive Machine Learning from an interactive systems perspective. We propose and describe a structural and behavioural model of a generalised IML system and identify solution principles for building effective interfaces for IML. Where possible, these emergent solution principles are contextualised by reference to the broader human-computer interaction literature. Finally, we identify strands of user interface research key to unlocking more efficient and productive non-expert interactive machine learning applications. © 2018 ACM",Interactive machine learning; Interface design,Artificial intelligence; Human computer interaction; Learning systems; Back-ground knowledge; Computational power; Interactive machine learning; Interactive process; Interactive system; Interface designs; Machine learning techniques; User interface designs; User interfaces
Motion-sound mapping through interaction: An approach to user-centered design of auditory feedback using machine learning,2018,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058673588&doi=10.1145%2f3211826&partnerID=40&md5=1ef6224ad8541a65a2424cf6978966e9,"Technologies for sensing movement are expanding toward everyday use in virtual reality, gaming, and artistic practices. In this context, there is a need for methodologies to help designers and users create meaningful movement experiences. This article discusses a user-centered approach for the design of interactive auditory feedback using interactive machine learning. We discuss Mapping through Interaction, a method for crafting sonic interactions from corporeal demonstrations of embodied associations between motion and sound. It uses an interactive machine learning approach to build the mapping from user demonstrations, emphasizing an iterative design process that integrates acted and interactive experiences of the relationships between movement and sound. We examine Gaussian Mixture Regression and Hidden Markov Regression for continuous movement recognition and real-time sound parameter generation. We illustrate and evaluate this approach through an application in which novice users can create interactive sound feedback based on coproduced gestures and vocalizations. Results indicate that Gaussian Mixture Regression and Hidden Markov Regression can efficiently learn complex motion-sound mappings from few examples. © 2018 ACM",Interactive machine learning; Movement; Music computing; Programming-by-demonstration; Sonification; Sound; User-centered design,Acoustic waves; Artificial intelligence; Association reactions; Demonstrations; Human computer interaction; Iterative methods; Learning systems; Mapping; Motion estimation; Regression analysis; Virtual reality; Interactive machine learning; Movement; Music computing; Programming by demon-stration; Sonifications; User centered design
Observation-level and parametric interaction for high-dimensional data analysis,2018,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058660305&doi=10.1145%2f3158230&partnerID=40&md5=e1a05fcf0fa9961e12b44cb13dc88bf6,"Exploring high-dimensional data is challenging. Dimension reduction algorithms, such as weighted multidimensional scaling, support data exploration by projecting datasets to two dimensions for visualization. These projections can be explored through parametric interaction, tweaking underlying parameterizations, and observation-level interaction, directly interacting with the points within the projection. In this article, we present the results of a controlled usability study determining the differences, advantages, and drawbacks among parametric interaction, observation-level interaction, and their combination. The study assesses both interaction technique effects on domain-specific high-dimensional data analyses performed by non-experts of statistical algorithms. This study is performed using Andromeda, a tool that enables both parametric and observation-level interaction to provide in-depth data exploration. The results indicate that the two forms of interaction serve different, but complementary, purposes in gaining insight through steerable dimension reduction algorithms. © 2018 ACM",Data analysis; Dimension reduction; Evaluation; Interaction; Usability; User interface; Visual analytics,Clustering algorithms; Data handling; Data mining; Data reduction; User interfaces; Visualization; Dimension reduction; Evaluation; Interaction; Usability; Visual analytics; Data visualization
Evaluation and refinement of clustered search results with the crowd,2018,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058637713&doi=10.1145%2f3158226&partnerID=40&md5=0bb27ea1eca15967a60f0997dfa983db,"When searching on the web or in an app, results are often returned as lists of hundreds to thousands of items, making it difficult for users to understand or navigate the space of results. Research has demonstrated that using clustering to partition search results into coherent, topical clusters can aid in both exploration and discovery. Yet clusters generated by an algorithm for this purpose are often of poor quality and do not satisfy users. To achieve acceptable clustered search results, experts must manually evaluate and refine the clustered results for each search query, a process that does not scale to large numbers of search queries. In this article, we investigate using crowd-based human evaluation to inspect, evaluate, and improve clusters to create high-quality clustered search results at scale. We introduce a workflow that begins by using a collection of well-known clustering algorithms to produce a set of clustered search results for a given query. Then, we use crowd workers to holistically assess the quality of each clustered search result to find the best one. Finally, the workflow has the crowd spot and fix problems in the best result to produce a final output. We evaluate this workflow on 120 top search queries from the Google Play Store, some of whom have clustered search results as a result of evaluations and refinements by experts. Our evaluations demonstrate that the workflow is effective at reproducing the evaluation of expert judges and also improves clusters in a way that agrees with experts and crowds alike. 2018 Copyright is held by the owner/author(s). Publication rights licensed to ACM.",Cluster evaluation; Clustered search results; Clusters; Crowdsourcing; Human computation; Mobile app stores; Search engines,Clustering algorithms; Crowdsourcing; Data mining; Search engines; Cluster evaluations; Clustered search results; Clusters; Human computation; Mobile app; Quality control
Visualizing research impact through citation data,2018,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064744447&doi=10.1145%2f3132744&partnerID=40&md5=021f7fcda614550411167da92627bbdb,"Research impact plays a critical role in evaluating the research quality and influence of a scholar, a journal, or a conference. Many researchers have attempted to quantify research impact by introducing different types of metrics based on citation data, such as h-index, citation count, and impact factor. These metrics are widely used in the academic community. However, quantitative metrics are highly aggregated in most cases and sometimes biased, which probably results in the loss of impact details that are important for comprehensively understanding research impact. For example, which research area does a researcher have great research impact on? How does the research impact change over time? How do the collaborators take effect on the research impact of an individual? Simple quantitative metrics can hardly help answer such kind of questions, since more detailed exploration of the citation data is needed. Previous work on visualizing citation data usually only shows limited aspects of research impact and may suffer from other problems including visual clutter and scalability issues. To fill this gap, we propose an interactive visualization tool, ImpactVis, for better exploration of research impact through citation data. Case studies and in-depth expert interviews are conducted to demonstrate the effectiveness of ImpactVis. © 2018 ACM 2160-6455/2018/03-ART5 $15.00",Publication and citation; Research impact; Visualization,Flow visualization; Visualization; Academic community; Interactive visualization tool; Publication and citation; Quantitative metrics; Research impacts; Research quality; Scalability issue; Visual clutter; Data visualization
A visual approach for interactive keyterm-based clustering,2018,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042521038&doi=10.1145%2f3181669&partnerID=40&md5=1feb5776d71f906ff2ef3041d2539606,"The keyterm-based approach is arguably intuitive for users to direct text-clustering processes and adapt results to various applications in text analysis. Its way of markedly influencing the results, for instance, by expressing important terms in relevance order, requires little knowledge of the algorithm and has predictable effect, speeding up the task. This article first presents a text-clustering algorithm that can easily be extended into an interactive algorithm. We evaluate its performance against state-of-the-art clustering algorithms in unsupervised mode. Next, we propose three interactive versions of the algorithm based on keyterm labeling, document labeling, and hybrid labeling. We then demonstrate that keyterm labeling is more effective than document labeling in text clustering. Finally, we propose a visual approach to support the keyterm-based version of the algorithm. Visualizations are provided for the whole collection as well as for detailed views of document and cluster relationships. We show the effectiveness and flexibility of our framework, Vis-Kt, by presenting typical clustering cases on real text document collections. A user study is also reported that reveals overwhelmingly positive acceptance toward keyterm-based clustering. © 2018 ACM.",Document clustering; Interactive; Keyterm-based clustering; Visualization,Cluster analysis; Flow visualization; Text processing; Visualization; Based clustering; Document Clustering; Interactive; Interactive algorithms; State of the art; Text Clustering; Text document; Text-clustering algorithm; Clustering algorithms
VisForum: A visual analysis system for exploring user groups in online forums,2018,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042492127&doi=10.1145%2f3162075&partnerID=40&md5=ec8e5173fd2ca55e1f49b859da2bc8c7,"User grouping in asynchronous online forums is a common phenomenon nowadays. People with similar backgrounds or shared interests like to get together in group discussions. As tens of thousands of archived conversational posts accumulate, challenges emerge for forum administrators and analysts to effectively explore user groups in large-volume threads and gain meaningful insights into the hierarchical discussions. Identifying and comparing groups in discussion threads are nontrivial, since the number of users and posts increases with time and noises may hamper the detection of user groups. Researchers in data mining fields have proposed a large body of algorithms to explore user grouping. However, the mining result is not intuitive to understand and difficult for users to explore the details. To address these issues, we present VisForum, a visual analytic system allowing people to interactively explore user groups in a forum.We work closely with two educators who have released courses in Massive Open Online Courses (MOOC) platforms to compile a list of design goals to guide our design. Then, we design and implement a multi-coordinated interface as well as several novel glyphs, i.e., group glyph, user glyph, and set glyph, with different granularities. Accordingly, we propose the group Detecting & Sorting Algorithm to reduce noises in a collection of posts, and employ the concept of ""forum-index"" for users to identify high-impact forummembers. Two case studies using real-world datasets demonstrate the usefulness of the system and the effectiveness of novel glyph designs. Furthermore, we conduct an in-lab user study to present the usability of VisForum. © 2018 ACM.",Application; Glyph design; MOOC forum,Applications; Data mining; Social networking (online); Design and implements; Different granularities; Group discussions; Massive open online course; MOOC forum; Real-world datasets; Sorting algorithm; Visual analysis; Curricula
A visual analytics framework for exploring theme park dynamics,2018,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042477174&doi=10.1145%2f3162076&partnerID=40&md5=cf203bdbc7c908379a03bb42a0ba5095,"In 2015, the top 10 largest amusement park corporations saw a combined annual attendance of over 400 million visitors. Daily average attendance in some of the most popular theme parks in the world can average 44,000 visitors per day. These visitors ride attractions, shop for souvenirs, and dine at local establishments; however, a critical component of their visit is the overall park experience. This experience depends on the wait time for rides, the crowd flow in the park, and various other factors linked to the crowd dynamics and human behavior. As such, better insight into visitor behavior can help theme parks devise competitive strategies for improved customer experience. Research into the use of attractions, facilities, and exhibits can be studied, and as behavior profiles emerge, park operators can also identify anomalous behaviors of visitors which can improve safety and operations. In this article, we present a visual analytics framework for analyzing crowd dynamics in theme parks. Our proposed framework is designed to support behavioral analysis by summarizing patterns and detecting anomalies. We provide methodologies to link visitor movement data, communication data, and park infrastructure data. This combination of data sources enables a semantic analysis of who, what, when, and where, enabling analysts to explore visitor-visitor interactions and visitorinfrastructure interactions. Analysts can identify behaviors at the macro level through semantic trajectory clustering views for group behavior dynamics, as well as at the micro level using trajectory traces and a novel visitor network analysis view.We demonstrate the efficacy of our framework through two case studies of simulated theme park visitors. © 2018 ACM.",Behavior; Semantic trajectories; Trajectory analysis; Visual analytics,Dynamics; Semantics; Trajectories; Visualization; Anomalous behavior; Behavior; Behavioral analysis; Competitive strategy; Customer experience; Semantic trajectories; Trajectory analysis; Visual analytics; Behavioral research
Guest editorial: Special issue on interactive visual analysis of human and crowd behaviors,2018,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042510079&doi=10.1145%2f3178569&partnerID=40&md5=9c35d9320bb49408e3720e4c4042afdc,"The analysis of human behaviors has impacted many social and commercial domains. How could interactive visual analytic systems be used to further provide behavioral insights? This editorial introduction features emerging research trend related to this question. The four articles accepted for this special issue represent recent progress: they identify research challenges arising from analysis of human and crowd behaviors, and present novel methods in visual analysis to address those challenges and help make behavioral data more useful. © Association for Computing Machinery. All rights reserved.",Behavioral data; Crowd behaviors; Human mobility; Interactive visualization; Social dynamics; Social network analysis; Visual analytics,Data visualization; Social networking (online); Visualization; Behavioral data; Crowd behavior; Human mobility; Interactive visualizations; Social dynamics; Visual analytics; Behavioral research
Chronodes: Interactive multifocus exploration of event sequences,2018,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041999215&doi=10.1145%2f3152888&partnerID=40&md5=fc344360ac8247fc305b63ba880b6905,"The advent of mobile health (mHealth) technologies challenges the capabilities of current visualizations, interactive tools, and algorithms. We present Chronodes, an interactive system that unifies data mining and human-centric visualization techniques to support explorative analysis of longitudinal mHealth data. Chronodes extracts and visualizes frequent event sequences that reveal chronological patterns across multiple participant timelines of mHealth data. It then combines novel interaction and visualization techniques to enable multifocus event sequence analysis, which allows health researchers to interactively define, explore, and compare groups of participant behaviors using event sequence combinations. Through summarizing insights gained from a pilot study with 20 behavioral and biomedical health experts, we discuss Chronodes's efficacy and potential impact in the mHealth domain. Ultimately, we outline important open challenges in mHealth, and offer recommendations and design guidelines for future research. © 2018 ACM.",Cohort discovery; Event alignment; MHealth; Mobile health sensor data; Sequence mining,Data visualization; Graphical user interfaces; Health; Health care; mHealth; Real time systems; Visualization; Cohort discovery; Event alignment; Interactive system; Mobile Health (M-Health); Potential impacts; Sensor data; Sequence mining; Visualization technique; Data mining
Quantifying collaboration with a co-creative drawing agent,2017,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85038444385&doi=10.1145%2f3009981&partnerID=40&md5=330b3155a72ab33b5e85db38261d1403,"This article describes a new technique for quantifying creative collaboration and applies it to the user study evaluation of a co-creative drawing agent. We present a cognitive framework called creative sense-making that provides a new method to visualize and quantify the interaction dynamics of creative collaboration, for example, the rhythm of interaction, style of turn taking, and the manner in which participants are mutually making sense of a situation. The creative sense-making framework includes a qualitative coding technique, interaction coding software, an analysis method, and the cognitive theory behind these applications. This framework and analysis method are applied to empirical studies of the Drawing Apprentice collaborative sketching system to compare human collaboration with a co-creative AI agent vs. a Wizard of Oz setup. The analysis demonstrates how the proposed technique can be used to analyze interaction data using continuous functions (e.g., integrations and moving averages) to measure and evaluate howcollaborations unfold through time. © 2017 ACM.",Collaboration; Creative agents; Creativity; Drawing; Evaluation methods; Interaction dynamics; Qualitative coding,Application programs; Drawing (graphics); Collaboration; Creativity; Evaluation methods; Interaction dynamics; Qualitative coding; Codes (symbols)
Supporting exploratory search with a visual user-driven approach,2017,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040125403&doi=10.1145%2f3009976&partnerID=40&md5=bcb75fb74b17f7889db8c5a3af2edbfc,"Whenever users engage in gathering and organizing new information, searching and browsing activities emerge at the core of the exploration process. As the process unfolds and new knowledge is acquired, interest drifts occur inevitably and need to be accounted for. Despite the advances in retrieval and recommender algorithms, real-world interfaces have remained largely unchanged: results are delivered in a relevance-ranked list. However, it quickly becomes cumbersome to reorganize resources along new interests, as any new search brings new results.We introduce an interactive user-driven tool that aims at supporting users in understanding, refining, and reorganizing documents on the fly as information needs evolve. Decisions regarding visual and interactive design aspects are tightly grounded on a conceptual model for exploratory search. In other words, the different views in the user interface address stages of awareness, exploration, and explanation unfolding along the discovery process, supported by a set of text-mining methods. A formal evaluation showed that gathering items relevant to a particular topic of interest with our tool incurs in a lower cognitive load compared to a traditional ranked list. A second study reports on usage patterns and usability of the various interaction techniques within a free, unsupervised setting. © 2017 ACM.",Advanced search interface; Exploratory search; Textual document ranking,Data mining; Search engines; Exploration process; Exploratory search; Interaction techniques; Real-world interface; Recommender algorithms; Search interfaces; Textual documents; User-driven approach; User interfaces
"Evaluation of facial expression recognition by a smart eyewear for facial direction changes, repeatability, and positional drift",2017,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040119589&doi=10.1145%2f3012941&partnerID=40&md5=39ba47d7f87a502f307728b6fda0a361,"This article presents a novel smart eyewear that recognizes the wearer's facial expressions in daily scenarios. Our device uses embedded photo-reflective sensors and machine learning to recognize the wearer's facial expressions. Our approach focuses on skin deformations around the eyes that occur when the wearer changes his or her facial expressions. With small photo-reflective sensors, we measure the distances between the skin surface on the face and the 17 sensors embedded in the eyewear frame. A Support Vector Machine (SVM) algorithm is then applied to the information collected by the sensors. The sensors can cover various facial muscle movements. In addition, they are small and light enough to be integrated into daily-use glasses. Our evaluation of the device shows the robustness to the noises from the wearer's facial direction changes and the slight changes in the glasses' position, as well as the reliability of the device's recognition capacity. The main contributions of our work are as follows: (1) We evaluated the recognition accuracy in daily scenes, showing 92.8% accuracy regardless of facial direction and removal/remount. Our device can recognize facial expressions with 78.1% accuracy for repeatability and 87.7% accuracy in case of its positional drift. (2)We designed and implemented the device by taking usability and social acceptability into account. The device looks like a conventional eyewear so that users can wear it anytime, anywhere. (3) Initial field trials in a daily life setting were undertaken to test the usability of the device. Our work is one of the first attempts to recognize and evaluate a variety of facial expressions with an unobtrusive wearable device. © 2017 ACM.",Affective computing; Eyewear computing; Facial expression; Wearable,Glass; Human computer interaction; Learning systems; Support vector machines; Wearable technology; Affective Computing; Eyewear; Facial expression recognition; Facial Expressions; Recognition accuracy; Recognition capacity; Support vector machine algorithm; Wearable; Face recognition
Adaptive contextualization methods: For combating selection bias during high-dimensional visualization,2017,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85035318524&doi=10.1145%2f3009973&partnerID=40&md5=8e4e783c214991996ad4df0658a69faf,"Large and high-dimensional real-world datasets are being gathered across a wide range of application disciplines to enable data-driven decision making. Interactive data visualization can play a critical role in allowing domain experts to select and analyze data from these large collections. However, there is a critical mismatch between the very large number of dimensions in complex real-world datasets and the much smaller number of dimensions that can be concurrently visualized using modern techniques. This gap in dimensionality can result in high levels of selection bias that go unnoticed by users. The bias can in turn threaten the very validity of any subsequent insights. This article describes Adaptive Contextualization (AC), a novel approach to interactive visual data selection that is specifically designed to combat the invisible introduction of selection bias. The AC approach (1) monitors and models a user's visual data selection activity, (2) computes metrics over that model to quantify the amount of selection bias after each step, (3) visualizes the metric results, and (4) provides interactive tools that help users assess and avoid bias-related problems. This article expands on an earlier article presented at ACM IUI 2016 [16] by providing a more detailed review of the AC methodology and additional evaluation results. © 2017 ACM.",Exploratory analysis; Intelligent visual interfaces; Selection bias; Visual analytics; Visualization,Data reduction; Decision making; Flow visualization; Visualization; Contextualization; Data driven decision; Evaluation results; Exploratory analysis; Real-world datasets; Selection bias; Visual analytics; Visual Interface; Data visualization
Exploring audience response in performing arts with a brain-adaptive digital performance system,2017,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85038633574&doi=10.1145%2f3009974&partnerID=40&md5=cb12427dc8ff995c94bd4f456b4bbd02,"Audience response is an important indicator of the quality of performing arts. Psychophysiological measurements enable researchers to perceive and understand audience response by collecting their bio-signals during a live performance. However, how the audience respond and how the performance is affected by these responses are the key elements but are hard to implement. To address this issue, we designed a brain-computer interactive system called Brain-Adaptive Digital Performance (BADP) for the measurement and analysis of audience engagement level through an interactive three-dimensional virtual theater. The BADP system monitors audience engagement in real time using electroencephalography (EEG) measurement and tries to improve it by applying content-related performing cues when the engagement level decreased. In this article, we generate EEG-based engagement level and build thresholds to determine the decrease and re-engage moments. In the experiment, we simulated two types of theatre performance to provide participants a high-fidelity virtual environment using the BADP system. We also create content-related performing cues for each performance under three different conditions. The results of these evaluations show that our algorithm could accurately detect the engagement status and the performing cues have a positive impact on regaining audience engagement across different performance types. Our findings open new perspectives in audience-based theatre performance design. © 2017 ACM.",Adaptive user interface; Audience engagement; Brain-computer interface (BCI); Digital performance,Electroencephalography; Electrophysiology; Interfaces (computer); Systems analysis; Theaters; User interfaces; Virtual addresses; Virtual reality; Adaptive user interface; Audience engagement; Digital performance; Engagement levels; Interactive system; Measurement and analysis; Performance design; Performance system; Brain computer interface
Active learning and visual analytics for stance classification with ALVA,2017,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032958347&doi=10.1145%2f3132169&partnerID=40&md5=3095ec5090b69335ca4ce781a2efb95f,"The automatic detection and classification of stance (e.g., certainty or agreement) in text data using natural language processing and machine-learning methods creates an opportunity to gain insight into the speakers' attitudes toward their own and other people's utterances. However, identifying stance in text presents many challenges related to training data collection and classifier training. To facilitate the entire process of training a stance classifier, we propose a visual analytics approach, called ALVA, for text data annotation and visualization. ALVA's interplay with the stance classifier follows an active learning strategy to select suitable candidate utterances for manual annotaion. Our approach supports annotation process management and provides the annotators with a clean user interface for labeling utterances with multiple stance categories. ALVA also contains a visualization method to help analysts of the annotation and training process gain a better understanding of the categories used by the annotators. The visualization uses a novel visual representation, called CatCombos, which groups individual annotation items by the combination of stance categories. Additionally, our system makes a visualization of a vector space model available that is itself based on utterances. ALVA is already being used by our domain experts in linguistics and computational linguistics to improve the understanding of stance phenomena and to build a stance classifier for applications such as social media monitoring. © 2017 ACM.",,Artificial intelligence; Classifiers; Data visualization; Learning algorithms; Learning systems; Linguistics; Natural language processing systems; Text processing; User interfaces; Vector spaces; Visualization; Active learning strategies; Automatic Detection; Classifier training; Machine learning methods; Social media monitoring; Vector space models; Visual representations; Visualization method; Classification (of information)
Empathy in virtual agents and robots: A survey,2017,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030721207&doi=10.1145%2f2912150&partnerID=40&md5=0ee6b019cd9a77afad3da844eab99772,"This article surveys the area of computational empathy, analysing different ways by which artificial agents can simulate and trigger empathy in their interactions with humans. Empathic agents can be seen as agents that have the capacity to place themselves into the position of a user's or another agent's emotional situation and respond appropriately. We also survey artificial agents that, by their design and behaviour, can lead users to respond emotionally as if they were experiencing the agent's situation. In the course of this survey, we present the research conducted to date on empathic agents in light of the principles and mechanisms of empathy found in humans. We end by discussing some of the main challenges that this exciting area will be facing in the future. Copyright is held by the owner/author(s).",Affective computing; Empathy; Human-computer interaction; Human-robot interaction; Social robots; Virtual agents,Human computer interaction; Robots; Surveys; Virtual reality; Affective Computing; Artificial agents; Empathy; Lead users; Social robots; Virtual agent; Human robot interaction
Detecting users' cognitive load by galvanic skin response with affective interference,2017,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030692417&doi=10.1145%2f2960413&partnerID=40&md5=24b199849a2fe7473b4db08404085b33,"Experiencing high cognitive load during complex and demanding tasks results in performance reduction, stress, and errors. However, these could be prevented by a system capable of constantly monitoring users' cognitive load fluctuations and adjusting its interactions accordingly. Physiological data and behaviors have been found to be suitable measures of cognitive load and are now available in many consumer devices. An advantage of these measures over subjective and performance-based methods is that they are captured in real time and implicitly while the user interacts with the system, which makes them suitable for real-world applications. On the other hand, emotion interference can change physiological responses and make accurate cognitive load measurement more challenging. In this work, we have studied six galvanic skin response (GSR) features in detection of four cognitive load levels with the interference of emotions. The data was derived from two arithmetic experiments and emotions were induced by displaying pleasant and unpleasant pictures in the background. Two types of classifiers were applied to detect cognitive load levels. Results from both studies indicate that the features explored can detect four and two cognitive load levels with high accuracy even under emotional changes. More specifically, rise duration and accumulative GSR are the common best features in all situations, having the highest accuracy especially in the presence of emotions. © 2017 ACM.",Cognitive load; Emotion interference; Galvanic skin response; Machine learning; Physiological data,Consumer behavior; Electrophysiology; Feature extraction; Human computer interaction; Learning systems; Physiological models; Physiology; Cognitive load measurement; Cognitive loads; Consumer devices; Emotional change; Galvanic skin response; Performance based method; Physiological data; Physiological response; Psychophysiology
"Effects of speed, cyclicity, and dimensionality on distancing, time, and preference in human-aerial vehicle interactions",2017,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030659953&doi=10.1145%2f2983927&partnerID=40&md5=e6e1434f8ccb7c0bad28cf6f7b30d95a,"This article will present a simulation-based approach to testing multiple variables in the behavior of a small Unmanned Aerial Vehicle (sUAV), inspired by insect and animal motions, to understand how these variables impact time of interaction, preference for interaction, and distancing in Human-Robot Interaction (HRI). Previous work has focused on communicating directionality of flight, intentionality of the robot, and perception of motion in sUAVs, while interactions involving direct distancing from these vehicles have been limited to a single study (likely due to safety concerns). This study takes place in a Cave Automatic Virtual Environment (CAVE) to maintain a sense of scale and immersion with the users, while also allowing for safe interaction. Additionally, the two-alternative forced-choice method is employed as a unique methodology to the study of collocated HRI in order to both study the impact of these variables on preference and allow participants to choose whether or not to interact with a specific robot. This article will be of interest to end-users of sUAV technologies to encourage appropriate distancing based on their application, practitioners in HRI to understand the use of this new methodology, and human-aerial vehicle researchers to understand the perception of these vehicles by 64 naive users. Results suggest that low speed (by 0.27m, p < 0.02) and high cyclicity (by 0.28m, p < 0.01) expressions can be used to increase distancing; that low speed (by 4.4s, p < 0.01) and three-dimensional (by 2.6s, p < 0.01) expressions can be used to decrease time of interaction; and low speed (by 10.4%, p < 0.01) expressions are less preferred for passability in human-aerial vehicle interactions. © 2017 ACM.",Biologically inspired motion; Human-robot interaction; Unmanned aerial vehicle (UAV); User studies,Caves; Human computer interaction; Man machine systems; Robots; Unmanned aerial vehicles (UAV); Vehicles; Virtual reality; Alternative forced choice; Biologically inspired; Cave automatic virtual environments; Human robot Interaction (HRI); Safety concerns; Simulation based approaches; Small unmanned aerial vehicles; User study; Human robot interaction
Interacting with recommenders-overview and research directions,2017,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027872893&doi=10.1145%2f3001837&partnerID=40&md5=e9656229fa26348bd69098d742707c8a,"Automated recommendations have become a ubiquitous part of today's online user experience. These systems point us to additional items to purchase in online shops, they make suggestions to us on movies to watch, or recommend us people to connect with on social websites. In many of today's applications, however, the only way for users to interact with the system is to inspect the recommended items. Often, no mechanisms are implemented for users to give the system feedback on the recommendations or to explicitly specify preferences, which can limit the potential overall value of the system for its users. Academic research in recommender systems is largely focused on algorithmic approaches for item selection and ranking. Nonetheless, over the years a variety of proposals were made on how to design more interactive recommenders. This work provides a comprehensive overview on the existing literature on user interaction aspects in recommender systems. We cover existing approaches for preference elicitation and result presentation, as well as proposals that consider recommendation as an interactive process. Throughout the work, we furthermore discuss examples of real-world systems and outline possible directions for future works. © 2017 ACM.",Interactive recommender systems; Literature survey; User interfaces,Recommender systems; User interfaces; Academic research; Algorithmic approach; Interactive process; Literature survey; Preference elicitation; Real-world system; System feedbacks; User interaction; Online systems
Making machine-learning applications for time-series sensor data graphical and interactive,2017,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027072162&doi=10.1145%2f2983924&partnerID=40&md5=6a22403ed61378ab2136045acdb5160f,"The recent profusion of sensors has given consumers and researchers the ability to collect significant amounts of data. However, understanding sensor data can be a challenge, because it is voluminous, multi-sourced, and unintelligible. Nonetheless, intelligent systems, such as activity recognition, require pattern analysis of sensor data streams to produce compelling results; machine learning (ML) applications enable this type of analysis. However, the number of ML experts able to proficiently classify sensor data is limited, and there remains a lack of interactive, usable tools to help intermediate users perform this type of analysis. To learn which features these tools must support, we conducted interviews with intermediate users of ML and conducted two probe-based studies with a prototype ML and visual analytics system, Gimlets. Our system implements ML applications for sensor-based time-series data as a novel domain-specific prototype that integrates interactive visual analytic features into the ML pipeline. We identify future directions for usable ML systems based on sensor data that will enable intermediate users to build systems that have been prohibitively difficult. © 2017 ACM.",Big sensor data; Machine learning tools; Visual analytic tools; Wearable devices,Artificial intelligence; Intelligent systems; Learning systems; Pattern recognition; Pattern recognition systems; Time series; Visualization; Activity recognition; Analytic tools; Machine learning applications; Pattern analysis; Sensor data; Time-series data; Visual analytics systems; Wearable devices; Wearable sensors
Have you lost the thread? Discovering ongoing conversations in scattered dialog blocks,2017,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027049028&doi=10.1145%2f2885501&partnerID=40&md5=236f4e953fa4c310b21d96dd04232366,"Finding threads in textual dialogs is emerging as a need to better organize stored knowledge. We capture this need by introducing the novel task of discovering ongoing conversations in scattered dialog blocks. Our aim in this article is twofold. First, we propose a publicly available testbed for the task by solving the insurmountable problem of privacy of Big Personal Data. In fact, we showed that personal dialogs can be surrogated with theatrical plays. Second, we propose a suite of computationally light learning models that can use syntactic and semantic features. With this suite, we showed that models for this challenging task should include features capturing shifts in language use and, possibly, modeling underlying scripts. © 2017 ACM.",Big Personal Data; Provacy,Data privacy; Semantics; Learning models; Novel task; Problem of privacy; Provacy; Semantic features; Modeling languages
Discovering user behavioral features to enhance information search on big data,2017,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027069921&doi=10.1145%2f2856059&partnerID=40&md5=35314ec14d69455eff54a56cd5d4f2df,"Due to the emerging Big Data paradigm, driven by the increasing availability of intelligent services easily accessible by a large number of users (e.g., social networks), traditional data management techniques are inadequate in many real-life scenarios. In particular, the availability of huge amounts of data pertaining to user social interactions, user preferences, and opinions calls for advanced analysis strategies to understand potentially interesting social dynamics. Furthermore, heterogeneity and high speed of user-generated data require suitable data storage and management tools to be designed fromscratch. This article presents a framework tailored for analyzing user interactions with intelligent systems while seeking some domain-specific information (e.g., choosing a good restaurant in a visited area). The framework enhances a user's quest for information by exploiting previous knowledge about their social environment, the extent of influence the users are potentially subject to, and the influence they may exert on other users. User influence spread across the network is dynamically computed as well to improve user search strategy by providing specific suggestions, represented as tailored faceted features. Such features are the result of data exchange activity (called data posting) that enriches information sources with additional background information and knowledge derived from experiences and behavioral properties of domain experts and users. The approach is tested in an important application scenario such as tourist recommendation, but it can be profitably exploited in several other contexts, for example, viral marketing and food education. © 2017 ACM.",Information extraction; Intelligent recommendation; NoSQL databases; Personal big data; User behavior,Behavioral research; Data mining; Digital storage; Electronic data interchange; Information management; Information retrieval; Intelligent systems; Knowledge management; Background information; Behavioral properties; Data management techniques; Domain-specific information; Intelligent recommendation; Intelligent Services; Nosql database; User behaviors; Big data
Introduction to the special issue on big personal data in interactive intelligent systems,2017,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027030513&doi=10.1145%2f3101102&partnerID=40&md5=80272eb82bd99c1da5507678a8fe16de,"This brief introduction begins with an overview of the types of research that are relevant to the special issue on Big Personal Data in Interactive Intelligent Systems. The overarching question is: How can big personal data be collected, analyzed, and exploited so as to provide new or improved forms of interaction with intelligent systems, and what new issues have to be taken into account? The three articles accepted for the special issue are then characterized in terms of the concepts of this overview. © 2017 ACM.",Big personal data; Intelligent systems; Intelligent user interfaces; Small data,User interfaces; Intelligent User Interfaces; Interactive intelligent systems; Small data; Intelligent systems
Added value of gaze-exploiting semantic representation to allow robots inferring human behaviors,2017,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017093904&doi=10.1145%2f2939381&partnerID=40&md5=40205607009443f59b05c39e99145a50,"Neuroscience studies have shown that incorporating gaze view with third view perspective has a great influence to correctly infer human behaviors. Given the importance of both first and third person observations for the recognition of human behaviors, we propose a method that incorporates these observations in a technical system to enhance the recognition of human behaviors, thus improving beyond third person observations in a more robust human activity recognition system. First, we present the extension of our proposed semantic reasoning method by including gaze data and external observations as inputs to segment and infer human behaviors in complex real-world scenarios. Then, from the obtained results we demonstrate that the combination of gaze and external input sources greatly enhance the recognition of human behaviors. Our findings have been applied to a humanoid robot to online segment and recognize the observed human activities with better accuracy when using both input sources; for example, the activity recognition increases from 77% to 82% in our proposed pancake-making dataset. To provide completeness of our system, we have evaluated our approach with another dataset with a similar setup as the one proposed in this work, that is, the CMU-MMAC dataset. In this case, we improved the recognition of the activities for the egg scrambling scenario from 54% to 86% by combining the external views with the gaze information, thus showing the benefit of incorporating gaze information to infer human behaviors across different datasets. © 2017 ACM.",Egocentric analysis; Human activity recognition; Robot learning by observation; Semantic reasoning,Anthropomorphic robots; Pattern recognition; Robots; Semantics; Social sciences; Activity recognition; Egocentric analysis; Human activity recognition; Human activity recognition systems; Learning by observation; Real-world scenario; Semantic reasoning; Semantic representation; Behavioral research
A user perception-based approach to create smiling embodied conversational agents,2017,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85008643951&doi=10.1145%2f2925993&partnerID=40&md5=ef1f21516d95460469cec13a37a65d1c,"In order to improve the social capabilities of embodied conversational agents, we propose a computational model to enable agents to automatically select and display appropriate smiling behavior during human-machine interaction. A smile may convey different communicative intentions depending on subtle characteristics of the facial expression and contextual cues. To construct such a model, as a first step, we explore the morphological and dynamic characteristics of different types of smiles (polite, amused, and embarrassed smiles) that an embodied conversational agent may display. The resulting lexicon of smiles is based on a corpus of virtual agents' smiles directly created by users and analyzed through a machine-learning technique. Moreover, during an interaction, a smiling expression impacts on the observer's perception of the interpersonal stance of the speaker. As a second step, we propose a probabilistic model to automatically compute the user's potential perception of the embodied conversational agent's social stance depending on its smiling behavior and on its physical appearance. This model, based on a corpus of users' perceptions of smiling and nonsmiling virtual agents, enables a virtual agent to determine the appropriate smiling behavior to adopt given the interpersonal stance it wants to express. An experiment using real human-virtual agent interaction provided some validation of the proposed model. © 2017 ACM 2160-6455/2017/01-ART4 $15.00.",Embodied conversational agent; H.5.2 user interfaces; Human-machine interaction; Smiles,Gesture recognition; Human computer interaction; Learning systems; Man machine systems; User interfaces; Virtual reality; Computational model; Dynamic characteristics; Embodied conversational agent; H.5.2 User Interfaces; Human machine interaction; Machine learning techniques; Probabilistic modeling; Smiles; Behavioral research
Introduction to the special issue on human interaction with artificial advice givers,2016,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85008932816&doi=10.1145%2f3014432&partnerID=40&md5=10f53463d683fa002c0fa4384d257522,"Many interactive systems in today's world can be viewed as providing advice to their users. Commercial examples include recommender systems, satellite navigation systems, intelligent personal assistants on smartphones, and automated checkout systems in supermarkets. We will call these systems that support people in making choices and decisions artificial advice givers (AAGs): They propose and evaluate options while involving their human users in the decision-making process. This special issue addresses the challenge of improving the interaction between artificial and human agents. It answers the question of how an agent of each type (human and artificial) can influence and understand the reasoning, working models, and conclusions of the other agent by means of novel forms of interaction. To address this challenge, the articles in the special issue are organized around three themes: (a) human factors to consider when designing interactions with AAGs (e.g., over- and under-reliance, overestimation of the system's capabilities), (b) methods for supporting interaction with AAGs (e.g., natural language, visualization, and argumentation), and (c) considerations for evaluating AAGs (both criteria and methodology for applying them). © 2016 ACM.",Advising agents; Agent-based interaction; Anthropomorphism; Argumentation; Emotions; Facial actions; Feedforward and feedback; Gestures; Human argumentation; Human decision making; Human-agent interaction; Human-like computing; Interaction paradigms; Recommendation; Reliance on automation; Use image; Vague language; Visualization,Behavioral research; Decision making; Flow visualization; Navigation systems; Satellite navigation aids; Visualization; Agent based; Anthropomorphism; Argumentation; Emotions; Facial action; Feed-Forward; Gestures; Human agent interactions; Human argumentation; Human decision making; Human like; Interaction paradigm; Recommendation; Use image; Vague language; C (programming language)
Providing arguments in discussions on the basis of the prediction of human argumentative behavior,2016,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006986872&doi=10.1145%2f2983925&partnerID=40&md5=f39e14b37c8dd3902687eed129c6510f,"Argumentative discussion is a highly demanding task. In order to help people in such discussions, this article provides an innovative methodology for developing agents that can support people in argumentative discussions by proposing possible arguments. By gathering and analyzing human argumentative behavior from more than 1000 human study participants, we show that the prediction of human argumentative behavior using Machine Learning (ML) is possible and useful in designing argument provision agents. This paper first demonstrates that ML techniques can achieve up to 76% accuracy when predicting people's top three argument choices given a partial discussion. We further show that well-established Argumentation Theory is not a good predictor of people's choice of arguments. Then, we present 9 argument provision agents, which we empirically evaluate using hundreds of human study participants. We show that the Predictive and Relevance-Based Heuristic agent (PRH), which uses ML prediction with a heuristic that estimates the relevance of possible arguments to the current state of the discussion, results in significantly higher levels of satisfaction among study participants compared with the other evaluated agents. These other agents propose arguments based on Argumentation Theory; propose predicted arguments without the heuristics or with only the heuristics; or use Transfer Learning methods. Our findings also show that people use the PRH agents proposed arguments significantly more often than those proposed by the other agents. © 2016 ACM.",,Forecasting; Heuristic methods; Learning systems; Argumentation theory; Human study; Innovative methodologies; Transfer learning methods; Behavioral research
"Diversity, serendipity, novelty, and coverage: A survey and empirical analysis of beyond-Accuracy objectives in recommender systems",2016,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85007593765&doi=10.1145%2f2926720&partnerID=40&md5=85c7ccb62b52355a08da4cd6903153ff,"What makes a good recommendation or good list of recommendations? Research into recommender systems has traditionally focused on accuracy, in particular how closely the recommender's predicted ratings are to the users' true ratings. However, it has been recognized that other recommendation qualities-such as whether the list of recommendations is diverse and whether it contains novel items-may have a significant impact on the overall quality of a recommender system. Consequently, in recent years, the focus of recommender systems research has shifted to include a wider range of ""beyond accuracy"" objectives. In this article, we present a survey of the most discussed beyond-Accuracy objectives in recommender systems research: diversity, serendipity, novelty, and coverage. We review the definitions of these objectives and corresponding metrics found in the literature.We also review works that propose optimization strategies for these beyond-Accuracy objectives. Since the majority of works focus on one specific objective, we find that it is not clear how the different objectives relate to each other. Hence, we conduct a set of offline experiments aimed at comparing the performance of different optimization approaches with a view to seeing how they affect objectives other than the ones they are optimizing. We use a set of state-of-The-Art recommendation algorithms optimized for recall along with a number of reranking strategies for optimizing the diversity, novelty, and serendipity of the generated recommendations. For each reranking strategy, we measure the effects on the other beyond-Accuracy objectives and demonstrate important insights into the correlations between the discussed objectives. For instance, we find that ratingbased diversity is positively correlated with novelty, and we demonstrate the positive influence of novelty on recommendation coverage.",Beyond Accuracy; Coverage; Diversity; Evaluation Metrics; Novelty; Serendipity,Surveys; Beyond Accuracy; Coverage; Diversity; Evaluation metrics; Novelty; Serendipity; Recommender systems
The effect of embodied interaction in visual-spatial navigation,2016,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85007574546&doi=10.1145%2f2953887&partnerID=40&md5=704087692a8c816a0e2cc9e9fd3539e7,"This article aims to assess the effect of embodied interaction on attention during the process of solving spatio-visual navigation problems. It presents a method that links operator's physical interaction, feedback, and attention. Attention is inferred through networks called Bayesian Attentional Networks (BANs). BANs are structures that describe cause-effect relationship between attention and physical action. Then, a utility function is used to determine the best combination of interaction modalities and feedback. Experiments involving five physical interaction modalities (vision-based gesture interaction, glove-based gesture interaction, speech, feet, and body stance) and two feedback modalities (visual and sound) are described. The main findings are: (i) physical expressions have an effect in the quality of the solutions to spatial navigation problems; (ii) the combination of feet gestures with visual feedback provides the best task performance.",attention inference; Bayesian network; Embodied interaction; gesture interaction; multimodal interaction,Navigation; Visual communication; attention inference; Cause-effect relationships; Embodied interaction; Gesture interaction; Multi-Modal Interactions; Physical interactions; Spatial navigation; Utility functions; Bayesian networks
"Updatable, accurate, diverse, and scalable recommendations for interactive applications",2016,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85007575680&doi=10.1145%2f2955101&partnerID=40&md5=ec725afd94bf44c961f12aa3aff672d7,"Recommender systems form the backbone of many interactive systems. They incorporate user feedback to personalize the user experience typically via personalized recommendation lists. As users interact with a system, an increasing amount of data about a user's preferences becomes available, which can be leveraged for improving the systems' performance. Incorporating these new data into the underlying recommendation model is, however, not always straightforward. Many models used by recommender systems are computationally expensive and, therefore, have to perform offline computations to compile the recommendation lists. For interactive applications, it is desirable to be able to update the computed values as soon as new user interaction data is available: updating recommendations in interactive time using new feedback data leads to better accuracy and increases the attraction of the system to the users. Additionally, there is a growing consensus that accuracy alone is not enough and user satisfaction is also dependent on diverse recommendations. In this work, we tackle this problem of updating personalized recommendation lists for interactive applications in order to provide both accurate and diverse recommendations. To that end, we explore algorithms that exploit random walks as a sampling technique to obtain diverse recommendations without compromising on efficiency and accuracy. Specifically, we present a novel graph vertex ranking recommendation algorithm called RP3 β that reranks items based on three-hop random walk transition probabilities. We show empirically that RP3 β provides accurate recommendations with high long-Tail item frequency at the top of the recommendation list. We also present approximate versions of RP3 β and the two most accurate previously published vertex ranking algorithms based on random walk transition probabilities and show that these approximations converge with an increasing number of samples. To obtain interactively updatable recommendations, we additionally show how our algorithm can be extended for online updates at interactive speeds. The underlying random walk sampling technique makes it possible to perform the updates without having to recompute the values for the entire dataset. In an empirical evaluation with three real-world datasets, we show that RP3 β provides highly accurate and diverse recommendations that can easily be updated with newly gathered information at interactive speeds (<100ms).",Bipartite Graph; Diversity; Evolving Graphs; Item Ranking; Long-Tail; Random Walks; Random Walks; Recommender Systems; Sampling; Top-N Recommendation; Updating Recommendations,Approximation algorithms; Human computer interaction; Interactive devices; Random processes; Recommender systems; Sampling; User interfaces; Bipartite graphs; Diversity; Evolving graphs; Item rankings; Long tail; Random Walk; Top-N Recommendation; Updating Recommendations; Graph theory
Inferring capabilities of intelligent agents from their external traits,2016,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84997402861&doi=10.1145%2f2963106&partnerID=40&md5=e415caf134cd1bc24b8a8bcab418c822,"We investigate the usability of humanlike agent-based interfaces for interactive advice-giving systems. In an experiment with a travel advisory system, we manipulate the ""humanlikeness"" of the agent interface. We demonstrate that users of the more humanlike agents try to exploit capabilities that were not signaled by the system. This severely reduces the usability of systems that look human but lack humanlikehumanlike capabilities (overestimation effect). We explain this effect by showing that users of humanlike agents form anthropomorphic beliefs (a user's ""mental model"") about the system: They act humanlike towards the system and try to exploit typical humanlike capabilities they believe the system possesses. Furthermore, we demonstrate that the mental model users form of an agent-based system is inherently integrated (as opposed to the compositional mental model they form of conventional interfaces): Cues provided by the system do not instill user responses in a one-to-one matter but are instead integrated into a single mental model. © 2016 ACM.",Agent-based interaction; Anthropomorphism; Design; Feedforward and feedback; H.1.2 [user/machine systems]: human factors; H.5.2 [user interfaces]: evaluation/methodology; H.5.2 [user interfaces]: interaction styles; H.5.2 [user interfaces]: natural language; Human factors; Measurement; Mental model; Theory; Usability,Cognitive systems; Design; Human computer interaction; Human engineering; Intelligent agents; Measurements; Agent based; Anthropomorphism; Evaluation/methodology; Feed-Forward; Interaction styles; Mental model; Natural languages; Theory; Usability; User/machine systems; User interfaces
Effects of the advisor and environment on requesting and complying with automated advice,2016,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84997040330&doi=10.1145%2f2905370&partnerID=40&md5=852ae8136bc19c02f8e4d1103bd2eddd,"Given the rapid technological advances in our society and the increase in artificial and automated advisors with whom we interact on a daily basis, it is becoming increasingly necessary to understand how users interact with and why they choose to request and follow advice from these types of advisors. More specifically, it is necessary to understand errors in advice utilization. In the present study, we propose a methodological framework for studying interactions between users and automated or other artificial advisors. Specifically, we propose the use of virtual environments and the tarp technique for stimulus sampling, ensuring sufficient sampling of important extreme values and the stimulus space between those extremes.We use this proposed framework to identify the impact of several factors on when and how advice is used. Additionally, because these interactions take place in different environments, we explore the impact of where the interaction takes place on the decision to interact. We varied the cost of advice, the reliability of the advisor, and the predictability of the environment to better understand the impact of these factors on the overutilization of suboptimal advisors and underutilization of optimal advisors. We found that less predictable environments, more reliable advisors, and lower costs for advice led to overutilization, whereas more predictable environments and less reliable advisors led to underutilization. Moreover, once advice was received, users took longer to make a final decision, suggesting less confidence and trust in the advisor when the reliability of the advisor was lower, the environment was less predictable, and the advice was not consistent with the environmental cues. These results contribute to a more complete understanding of advice utilization and trust in advisors. © 2016 ACM.",Automated decision AIDS; Decision making; Expert advice; Game as research method; Reliance on automation; Tarp technique,Decision making; Decision support systems; Importance sampling; Virtual reality; Automated decision aid; Environmental cues; Expert advice; Methodological frameworks; research methods; Stimulus spaces; Tarp technique; Technological advances; Automation
VizRec: Recommending personalized visualizations,2016,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84996968405&doi=10.1145%2f2983923&partnerID=40&md5=45ee12e93c2e904616b995f7bffe8add,"Visualizations have a distinctive advantage when dealing with the information overload problem: Because they are grounded in basic visual cognition, many people understand them. However, creating proper visualizations requires specific expertise of the domain and underlying data. Our quest in this article is to study methods to suggest appropriate visualizations autonomously. To be appropriate, a visualization has to follow known guidelines to find and distinguish patterns visually and encode data therein. A visualization tells a story of the underlying data; yet, to be appropriate, it has to clearly represent those aspects of the data the viewer is interested in. Which aspects of a visualization are important to the viewer? Can we capture and use those aspects to recommend visualizations? This article investigates strategies to recommend visualizations considering different aspects of user preferences. A multi-dimensional scale is used to estimate aspects of quality for visualizations for collaborative filtering. Alternatively, tag vectors describing visualizations are used to recommend potentially interesting visualizations based on content. Finally, a hybrid approach combines information on what a visualization is about (tags) and how good it is (ratings). We present the design principles behind VizRec, our visual recommender. We describe its architecture, the data acquisition approach with a crowd sourced study, and the analysis of strategies for visualization recommendation. © 2016 ACM.",Collaborative filtering; Crowd-sourcing; Personalized visualizations; Recommender systems; Visualization recommender,Collaborative filtering; Data acquisition; Data visualization; Recommender systems; Design Principles; Hybrid approach; Information overloads; ITS architecture; Multi dimensional; Study methods; Visual Cognition; Visualization
A multimodal approach to assessing user experiences with agent helpers,2016,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84997161428&doi=10.1145%2f2983926&partnerID=40&md5=81b7083cd4b316750fb9407d7b61a6a0,"The study of agent helpers using linguistic strategies such as vague language and politeness has often come across obstacles. One of these is the quality of the agent's voice and its lack of appropriate fit for using these strategies. The first approach of this article compares human vs. synthesised voices in agents using vague language. This approach analyses the 60,000-word text corpus of participant interviews to investigate the differences of user attitudes towards the agents, their voices and their use of vague language. It discovers that while the acceptance of vague language is still met with resistance in agent instructors, using a human voice yields more positive results than the synthesised alternatives. The second approach in this article discusses the development of a novel multimodal corpus of video and text data to create multiple analyses of human-agent interaction in agent-instructed assembly tasks. The second approach analyses user spontaneous facial actions and gestures during their interaction in the tasks. It found that agents are able to elicit these facial actions and gestures and posits that further analysis of this nonverbal feedback may help to create a more adaptive agent. Finally, the approaches used in this article suggest these can contribute to furthering the understanding of what it means to interact with software agents. © 2016 ACM.",Design; Emotions; Experimentation; Facial actions; Gestures; H.5.1 [multimedia information systems]: audio input/output; H.5.2 [user interfaces]: natural language; Human factors; Human-agent interaction; Instruction giving; Vague language,Audio systems; Design; Human engineering; User interfaces; Audio input; Emotions; Experimentation; Facial action; Gestures; Human agent interactions; Instruction giving; Natural languages; Vague language; Software agents
Read what you touch with intelligent audio system for non-visual interaction,2016,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84992198002&doi=10.1145%2f2822908&partnerID=40&md5=fbcde82518c95c3206019ba51d3c66ed,"Slate-type devices allow Individuals with Blindness or Severe Visual Impairment (IBSVI) to read in place with the touch of their fingertip by audio-rendering the words they touch. Such technologies are helpful for spatial cognition while reading. However, users have to move their fingers slowly, or they may lose their place on screen. Also, IBSVI may wander between lines without realizing they did. We addressed these two interaction problems by introducing a dynamic speech-touch interaction model and an intelligent reading support system. With this model, the speed of the speech will dynamically change with the user's finger speed. The proposed model is composed of (1) an Audio Dynamics Model and (2) an Off-line Speech Synthesis Technique. The intelligent reading support system predicts the direction of reading, corrects the reading word if the user drifts, and notifies the user using a sonic gutter to help him/her from straying off the reading line. We tested the new audio dynamics model, the sonic gutter, and the reading support model in two user studies. The participants' feedback helped us fine-tune the parameters of the two models. A decomposition study was conducted to evaluate the main components of the system. The results showed that both intelligent reading support with tactile feedback are required to achieve the best performance in terms of efficiency and effectiveness. Finally, we ran an evaluation study where the reading support system is compared to other VoiceOver technologies. The results showed preponderance to the reading support system with its audio dynamics and intelligent reading support components. © 2016 ACM.",Active reading; Assistive technology; Audio rendering; Blindness; Multi-touch interaction,Eye protection; Human computer interaction; Roofs; Sound reproduction; Speech synthesis; Active reading; Assistive technology; Audio rendering; Blindness; Multi-touch interactions; Dynamics
Multimodal analysis and prediction of persuasiveness in online social multimedia,2016,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84992176565&doi=10.1145%2f2897739&partnerID=40&md5=146051d19714e17ef724712a09fe5598,"Our lives are heavily influenced by persuasive communication, and it is essential in almost any type of social interaction from business negotiation to conversation with our friends and family. With the rapid growth of social multimedia websites, it is becoming ever more important and useful to understand persuasiveness in the context of social multimedia content online. In this article, we introduce a newly created multimedia corpus of 1,000 movie review videos with subjective annotations of persuasiveness and related high-level characteristics or attributes (e.g., confidence). This dataset will be made freely available to the research community. We designed our experiments around the following five main research hypotheses. First, we study if computational descriptors derived from verbal and nonverbal behavior can be predictive of persuasiveness. We further explore combining descriptors from multiple communication modalities (acoustic, verbal, para-verbal, and visual) for predicting persuasiveness and compare with using a single modality alone. Second, we investigate how certain high-level attributes, such as credibility or expertise, are related to persuasiveness and how the information can be used in modeling and predicting persuasiveness. Third, we investigate differences when speakers are expressing a positive or negative opinion and if the opinion polarity has any influence in the persuasiveness prediction. Fourth, we further study if gender has any influence in the prediction performance. Last, we test if it is possible to make comparable predictions of persuasiveness by only looking at thin slices (i.e., shorter time windows) of a speaker's behavior. © 2016 ACM.",Multimodal analysis; Multimodal behavior; Multimodal prediction; Persuasion; Persuasive opinion multimedia corpus; Persuasiveness; POM corpus; POM dataset; Social multimedia,Modal analysis; Multi-modal; Multimodal analysis; Persuasion; Persuasive opinion multimedia corpus; Persuasiveness; POM corpus; POM dataset; Social multimedia; Forecasting
Adapting the interactive activation model for context recognition and identification,2016,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84989169896&doi=10.1145%2f2873067&partnerID=40&md5=0a4c884abc8488a350a3de9362eae2ad,"In this article, we propose and implement a new model for context recognition and identification. Our work is motivated by the importance of ""working in context"" for knowledge workers to stay focused and productive. A computer application that can identify the current context in which the knowledge worker is working can (among other things) provide the worker with contextual support, for example, by suggesting relevant information sources, or give an overview of how he or she spent his or her time during the day. We present a descriptive model for the context of a knowledge worker. This model describes the contextual elements in the work environment of the knowledge worker and how these elements relate to each other. This model is operationalized in an algorithm, the contextual interactive activation model (CIA), which is based on the interactive activation model by Rumelhart and McClelland. It consists of a layered connected network through which activation flows. We have tested CIA in a context identification setting. In this case, the data that we use as input is low-level computer interaction logging data. We found that topical information and entities were the most relevant types of information for context identification. Overall the proposed CIA model is more effective than traditional supervised methods in identifying the active context from sparse input data, with less labelled training data. © 2016 ACM.",Algorithm; Computer interaction; Context identification; Context recognition; Interactive activation model; Network,Algorithms; Information management; Knowledge management; Network layers; Networks (circuits); Computer interaction; Connected networks; Context identification; Context recognition; Contextual elements; Contextual support; Information sources; Interactive activation; Chemical activation
Intelligent biohazard training based on real-time task recognition,2016,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84989807484&doi=10.1145%2f2883617&partnerID=40&md5=b7ec6957b9956cfdb009c6004449bb8e,"Virtual environments offer an ideal setting to develop intelligent training applications. Yet, their ability to support complex procedures depends on the appropriate integration of knowledge-based techniques and natural interaction. In this article, we describe the implementation of an intelligent rehearsal system for biohazard laboratory procedures, based on the real-time instantiation of task models from the trainee's actions A virtual biohazard laboratory has been recreated using the Unity3D engine, in which users interact with laboratory objects using keyboard/mouse input or hand gestures through a Kinect device. Realistic behavior for objects is supported by the implementation of a relevant subset of common sense and physics knowledge. User interaction with objects leads to the recognition of specific actions, which are used to progressively instantiate a task-based representation of biohazard procedures. The dynamics of this instantiation process supports trainee evaluation as well as real-time assistance. This system is designed primarily as a rehearsa system providing real-time advice and supporting user performance evaluation. We provide detailed examples illustrating error detection and recovery, and results from on-site testing with students from the Faculty of Medical Sciences at Kyushu University. In the study, we investigate the usability aspect by comparing interaction with mouse and Kinect devices and the effect of real-time task recognition on recovery time after user mistakes.",Bio-safety risk management; Training application; Virtual worlds,Knowledge based systems; Risk management; User interfaces; Virtual reality; Error detection and recovery; Instantiation process; Integration of knowledge; Laboratory procedures; Natural interactions; Safety risks; Training applications; Virtual worlds; Biohazards
Design and exploration of mid-air authentication gestures,2016,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84989172921&doi=10.1145%2f2832919&partnerID=40&md5=dc55fee24c8848b5752bccd2e50da2c8,"Authentication based on touchless mid-air gestures would benefit a multitude of ubiquitous computing applications, especially those that are used in clean environments (e.g., medical environments or clean rooms). In order to explore the potential of mid-air gestures for novel authentication approaches, we performed a series of studies and design experiments. First, we collected data from more then 200 users during a 3-day science event organized within a shopping mall. These data were used to investigate capabilities of the Leap Motion sensor, observe interaction in the wild, and to formulate an initial design problem. The design problem, as well as the design of mid-air gestures for authentication purposes, were iterated in subsequent design activities. In a final study with 13 participants, we evaluated two mid-air gestures for authentication purposes in different situations, including different body positions. Our results highlight a need for different mid-air gestures for differing situations and carefully chosen constraints for mid-air gestures. We conclude by proposing an exemplary system, which aims to provide tool-support for designers and engineers, allowing them to explore authentication gestures in the original context of use and thus support them with the design of contextual mid-air authentication gestures. © 2016 ACM.",Authentication; Clean rooms; Mid-air gestures,Clean rooms; Ubiquitous computing; Body positions; Clean environment; Computing applications; Context of use; Design activity; Design experiments; Design problems; Mid-air gestures; Authentication
Using respiration to predict who will speak next and when in multiparty meetings,2016,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006186478&doi=10.1145%2f2946838&partnerID=40&md5=81060e59a487207a9f617b20b70b39fc,"Techniques that use nonverbal behaviors to predict turn-changing situations-such as, in multiparty meetings, who the next speaker will be and when the next utterance will occur-have been receiving a lot of attention in recent research. To build a model for predicting these behaviors we conducted a research study to determine whether respiration could be effectively used as a basis for the prediction. Results of analyses of utterance and respiration data collected from participants in multiparty meetings reveal that the speaker takes a breath more quickly and deeply after the end of an utterance in turn-keeping than in turn-changing. They also indicate that the listener who will be the next speaker takes a bigger breath more quickly and deeply in turn-changing than the other listeners. On the basis of these results, we constructed and evaluated models for predicting the next speaker and the time of the next utterance in multiparty meetings. The results of the evaluation suggest that the characteristics of the speaker's inhalation right after an utterance unit-the points in time at which the inhalation starts and ends after the end of the utterance unit and the amplitude, slope, and duration of the inhalation phase-are effective for predicting the next speaker in multiparty meetings. They further suggest that the characteristics of listeners' inhalation-the points in time at which the inhalation starts and ends after the end of the utterance unit and the minimum and maximum inspiration, amplitude, and slope of the inhalation phase-are effective for predicting the next speaker. The start time and end time of the next speaker's inhalation are also useful for predicting the time of the next utterance in turn-changing.",Multiparty meetings; Next speaker prediction; Next-utterance timing prediction; Respiration; Turn-changing,Artificial intelligence; Multiparty meetings; Nonverbal behavior; Recent researches; Research studies; Respiration; Forecasting
Beyond the touchscreen: An exploration of extending interactions on commodity smartphones,2016,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006225987&doi=10.1145%2f2954003&partnerID=40&md5=71d724a9363196d620126f09c2b801e3,"Most smartphones today have a rich set of sensors that could be used to infer input (e.g., accelerometer, gyroscope, microphone); however, the primary mode of interaction is still limited to the front-facing touchscreen and several physical buttons on the case. To investigate the potential opportunities for interactions supported by built-in sensors, we present the implementation and evaluation of BeyondTouch, a family of interactions to extend and enrich the input experience of a smartphone. Using only existing sensing capabilities on a commodity smartphone, we offer the user a wide variety of additional inputs on the case and the surface adjacent to the smartphone. Although most of these interactions are implemented with machine learning methods, compact and robust rule-based detection methods can also be applied for recognizing some interactions by analyzing physical characteristics of tapping events on the phone. This article is an extended version of Zhang et al. [2015], which solely covered gestures implemented by machine learning methods. We extended our previous work by adding gestures implemented with rule-based methods, which works well with different users across devices without collecting any training data. We outline the implementation of both machine learning and rule-based methods for these interaction techniques and demonstrate empirical evidence of their effectiveness and usability. We also discuss the practicality of BeyondTouch for a variety of application scenarios and compare the two different implementation methods. © 2016 ACM 2160-6455/2016/08-ART16 $15.00.",Inertial sensors; Machine learning; Microphone; Mobile interactions; Rulebased method; Smartphones,Artificial intelligence; Microphones; Signal encoding; Smartphones; Speech processing; Application scenario; Inertial sensor; Interaction techniques; Machine learning methods; Mobile interaction; Physical characteristics; Rule based detection; Rule-based method; Learning systems
Teaching social communication skills through human-agent interaction,2016,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006201121&doi=10.1145%2f2937757&partnerID=40&md5=c0db227c54a00a1685551a043c605435,"There are a large number of computer-based systems that aim to train and improve social skills. However, most of these do not resemble the training regimens used by human instructors. In this article, we propose a computer-based training system that follows the procedure of social skills training (SST), a well-established method to decrease human anxiety and discomfort in social interaction, and acquire social skills. We attempt to automate the process of SST by developing a dialogue system named the automated social skills trainer, which teaches social communication skills through human-agent interaction. The system includes a virtual avatar that recognizes user speech and language information and gives feedback to users. Its design is based on conventional SST performed by human participants, including defining target skills, modeling, role-play, feedback, reinforcement, and homework. We performed a series of three experiments investigating (1) the advantages of using computer-based training systems compared to human-human interaction (HHI) by subjectively evaluating nervousness, ease of talking, and ability to talk well; (2) the relationship between speech language features and human social skills; and (3) the effect of computer-based training using our proposed system. Results ofour first experiment show that interaction with an avatar decreases nervousness and increases the user's subjective impression of his or her ability to talk well compared to interaction with an unfamiliar person. The experimental evaluation measuring the relationship between social skill and speech and language features shows that these features have a relationship with social skills. Finally, experiments measuring the effect of performing SST with the proposed application show that participants significantly improve their skill, as assessed by separate evaluators, by using the system for 50 minutes. A user survey also shows that the users thought our system is useful and easy to use, and that interaction with the avatar felt similar to HHI. © 2016 ACM 2160-6455/2016/08-ART18 $15.00.",Behavior detection; Computer-based training; Dialogue system; Embodied conversational avatar; Social skills training (SST),Computational linguistics; E-learning; Speech processing; Speech recognition; Behavior detection; Computer - based trainings; Dialogue systems; Embodied conversational avatar; Social skills training; Human computer interaction
The effects of interpersonal attitude of a group of agents on user's presence and proxemics behavior,2016,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006224575&doi=10.1145%2f2914796&partnerID=40&md5=cbc1ee0b31594ae67479f71e30543b54,"In the everyday world people form small conversing groups where social interaction takes place, and much of the social behavior takes place through managing interpersonal space (i.e., proxemics) and group formation, signaling their attentio to others (i.e., through gaze behavior), and expressing certain attitudes, for example, friendliness, by smiling, getting close through increased engagement and intimacy, and welcoming newcomers. Many real-time interactive systems feature virtual anthropomorphic characters in order to simulate conversing groups and add plausibility and believability to the simulated environments. However, only a few have dealt with autonomous behavior generation, and in those cases, the agents' exhibited behavior should be evaluated by users in terms of appropriateness, believability, and conveyed meaning (e.g., attitudes). In this article we present an integrated intelligent interactive system for generating believable nonverbal behavior exhibited by virtual agents in small simulated group conversations. The produced behavior supports group formation management and the expression of interpersonal attitudes (friendly vs. unfriendly) both among the agents in the group (i.e., in-group attitude) and towards an approaching user in an avatar-based interaction (out-group attitude). A user study investigating the effects of these attitudes on users' social presence evaluation and proxemics behavior (with their avatar) in a three-dimensional virtual city environment is presented. We divided the study into two trials according to the task assigned to users, that is, joining a conversing group and reaching a target destination behind the group. Results showed that the out-group attitude had a major impact on social presence evaluations in both trials, whereby friendly groups were perceived as more socially rich. The user's proxemics behavior depended on both out-group and in-group attitudes expressed by the agents. Implications of these results for the design and implementation of similar intelligent interactive systems for the autonomous generation of agents' multimodal behavior are briefly discussed. © 2016 ACM 2160-6455/2016/07-ART12 $15.00.",,Behavioral research; Human computer interaction; Intelligent virtual agents; Real time systems; Three dimensional computer graphics; Autonomous behaviors; Autonomous generation; Avatar-based interaction; Design and implementations; Intelligent interactive systems; Interpersonal attitudes; Simulated environment; Social interactions; Autonomous agents
Transfer learning for semisupervised collaborative recommendation,2016,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006220520&doi=10.1145%2f2835497&partnerID=40&md5=a867e7b3a1f48df47354b4df046480c8,"Users' online behaviors such as ratings and examination of items are recognized as one of the most valuable sources of information for learning users' preferences in order to make personalized recommendations. But most previous works focus on modeling only one type of users' behaviors such as numerical ratings or browsing records, which are referred to as explicit feedback and implicit feedback, respectively. In this article, we study a Semisupervised Collaborative Recommendation (SSCR) problem with labeled feedback (for explicit feedback) and unlabeled feedback (for implicit feedback), in analogy to the well-known Semisupervised Learning (SSL) setting with labeled instances and unlabeled instances. SSCR is associated with two fundamental challenges, that is, heterogeneity of two types of users' feedback and uncertainty of the unlabeled feedback. As a response, we design a novel Self-Transfer Learning (sTL) algorithm to iteratively identify and integrate likely positive unlabeled feedback, which is inspired by the general forward/backward process in machine learning. The merit of sTL is its ability to learn users' preferences from heterogeneous behaviors in a joint and selective manner. We conduct extensive empirical studies of sTL and several very competitive baselines on three large datasets. The experimental results show that our sTL is significantly better than the state-of-the-art methods. © 2016 ACM 2160-6455/2016/07-ART10 $15.00.",Collaborative recommendation; Labeled feedback; Unlabeled feedback,Artificial intelligence; Learning algorithms; Learning systems; Collaborative recommendation; Explicit feedback; Implicit feedback; Personalized recommendation; Semi-supervised learning (SSL); Sources of informations; State-of-the-art methods; Transfer learning; Iterative methods
Automatic analysis of naturalistic hand-over-face gestures,2016,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006224688&doi=10.1145%2f2946796&partnerID=40&md5=7b21e4032c67833cf3061b58da9a3f2c,"One of the main factors that limit the accuracy of facial analysis systems is hand occlusion. As the face becomes occluded, facial features are lost, corrupted, or erroneously detected. Hand-over-face occlusions are considered not only very common but also very challenging to handle. However, there is empirical evidence that some of these hand-over-face gestures serve as cues for recognition of cognitive mental states. In this article, we present an analysis of automatic detection and classification of hand-over-face gestures. We detect hand-over-face occlusions and classify hand-over-face gesture descriptors in videos of natural expressions using multi-modal fusion of different state-of-the-art spatial and spatio-temporal features. We show experimentally that we can successfully detect face occlusions with an accuracy of 83%. We also demonstrate that we can classify gesture descriptors (hand shape, hand action, and facial region occluded) significantly better than a naïve baseline. Our detailed quantitative analysis sheds some light on the challenges of automatic classification of hand-over-face gestures in natural expressions. © 2016 ACM.",Face touches; Facial landmarks; Hand gestures; Hand-over-face occlusions; Histograms of oriented gradient; Space-time interest points,Feature extraction; Gesture recognition; Palmprint recognition; Face occlusion; Face touches; Facial landmark; Hand gesture; Histograms of oriented gradients; Space time; Face recognition
The stability and usability of statistical topic models,2016,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006210712&doi=10.1145%2f2954002&partnerID=40&md5=cfc5e5a38e17ed020a7cc42c5102fdbb,"Statistical topic models have become a useful and ubiquitous tool for analyzing large text corpora. One common application of statistical topic models is to support topic-centric navigation and exploration of document collections. Existing work on topic modeling focuses on the inference of model parameters so the resulting model fits the input data. Since the exact inference is intractable, statistical inference methods, such as Gibbs Sampling, are commonly used to solve the problem. However, most of the existing work ignores an important aspect that is closely related to the end user experience: topic model stability. When the model is either re-trained with the same input data or updated with new documents, the topic previously assigned to a document may change under the new model, which may result in a disruption of end users' mental maps about the relations between documents and topics, thus undermining the usability of the applications. In this article, we propose a novel user-directed non-disruptive topic model update method that balances the tradeoff between finding the model that fits the data and maintaining the stability of the model from end users' perspective. It employs a novel constrained LDA algorithm to incorporate pairwise document constraints, which are converted from user feedback about topics, to achieve topic model stability. Evaluation results demonstrate the advantages of our approach over previous methods. © 2016 ACM.",Constrained topic model; LDA; Non-disruptive topic model update; Stability; Statistical topic model; Text analytics; Usability,Convergence of numerical methods; Input output programs; Stability; Statistics; Document collection; End-user experience; Evaluation results; Model parameters; Statistical inference; Text analytics; Topic Modeling; Usability; Sampling
A dynamic pen-based interface for writing and editing complex mathematical expressions with Math boxes,2016,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006226055&doi=10.1145%2f2946795&partnerID=40&md5=e8bee9a61324421a453c3a8290d36412,"Math boxes is a recently introduced pen-based user interface for simplifying the task of hand writing difficult mathematical expressions. Visible bounding boxes around subexpressions are automatically generated as the system detects relevant spatial relationships between symbols including superscripts, subscripts, and fractions. Subexpressions contained in a math box can then be extended by adding new terms directly into its given bounds. When new characters are accepted, box boundaries are dynamically resized and neighboring terms are translated to make room for the larger box. Feedback on structural recognition is given via the boxes themselves. In this work, we extend the math boxes interface to include support for subexpression modifications via a new set of pen-based interactions. Specifically, techniques to expand and rearrange terms in a given expression are introduced. To evaluate the usefulness of our proposed methods, we first conducted a user study in which participants wrote a variety of equations ranging in complexity from a simple polynomial to the more difficult expected value of the logistic distribution. The math boxes interface is compared against the commonly used offset typeset (small) method, where recognized expressions are typeset in a system font near the user's unmodified ink. In this initial study, we find that the fluidness of the offset method is preferred for simple expressions but that, as difficulty increases, our math boxes method is overwhelmingly preferred. We then conducted a second user study that focused only on modifying various mathematical expressions. In general, participants worked faster with the math boxes interface, and most new techniques were well received. On the basis of the two user studies, we discuss the implications of the math boxes interface and identify areas where improvements are possible. © 2016 ACM 2160-6455/2016/07-ART13 $15.00.",Design; Human factors; Measurement,Design; Human computer interaction; Human engineering; Measurements; Automatically generated; Logistic distributions; Mathematical expressions; Pen based user interfaces; Pen-based interaction; Pen-based interfaces; Spatial relationships; Structural recognition; User interfaces
Minimal interaction content discovery in recommender systems,2016,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006212950&doi=10.1145%2f2845090&partnerID=40&md5=2a4f541281f2f3c4551ac4f2d7f71973,"Many prior works in recommender systems focus on improving the accuracy of item rating predictions. In comparison, the areas of recommendation interfaces and user-recommender interaction remain underexplored. In this work, we look into the interaction of users with the recommendation list, aiming to devise a method that simplifies content discovery and minimizes the cost of reaching an item of interest. We quantify this cost by the number of user interactions (clicks and scrolls) with the recommendation list. To this end, we propose generalized linear search (GLS), an adaptive combination of the established linear and generalized search (GS) approaches. GLS leverages the advantages of these two approaches, and we prove formally that it performs at least as well as GS. We also conduct a thorough experimental evaluation of GLS and compare it to several baselines and heuristic approaches in both an offline and live evaluation. The results of the evaluation show that GLS consistently outperforms the baseline approaches and is also preferred by users. In summary, GLS offers an efficient and easy-to-use means for content discovery in recommender systems. © 2016 ACM.",Content discovery; Generalized linear search; Recommender systems; User-recommender interaction,Heuristic methods; Adaptive combinations; Content discoveries; Experimental evaluation; Heuristic approach; Linear search; Minimal interactions; User interaction; User-recommender interaction; Recommender systems
Using video to automatically detect learner affect in computer-enabled classrooms,2016,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006189367&doi=10.1145%2f2946837&partnerID=40&md5=4ce6d45c0bb709d7499b9cff27dbae1d,"Affect detection is a key component in intelligent educational interfaces that respond to students' affective states. We use computer vision and machine-learning techniques to detect students' affect from facial expressions (primary channel) and gross body movements (secondary channel) during interactions with an educational physics game. We collected data in the real-world environment of a school computer lab with up to 30 students simultaneously playing the game while moving around, gesturing, and talking to each other. The results were cross-validated at the student level to ensure generalization to new students. Classification accuracies, quantified as area under the receiver operating characteristic curve (AUC), were above chance (AUC of 0.5) for all the affective states observed, namely, boredom (AUC =.610), confusion (AUC =.649), delight (AUC =.867), engagement (AUC =.679), frustration (AUC =.631), and for off-task behavior (AUC =.816). Furthermore, the detectors showed temporal generalizability in that there was less than a 2% decrease in accuracy when tested on data collected from different times of the day and from different days. There was also some evidence of generalizability across ethnicity (as perceived by human coders) and gender, although with a higher degree of variability attributable to differences in affect base rates across subpopulations. In summary, our results demonstrate the feasibility of generalizable video-based detectors of naturalistic affect in a real-world setting, suggesting that the time is ripe for affect-sensitive interventions in educational games and other intelligent interfaces. © 2016 ACM.",Affect detection; Classroom data; Generalization; In the wild; Naturalistic facial expressions,Artificial intelligence; Computer games; Computer vision; Education; Face recognition; Human computer interaction; Interface states; Learning algorithms; Learning systems; Affect detection; Classification accuracy; Classroom data; Facial Expressions; Generalization; Machine learning techniques; Real world environments; Receiver operating characteristic curves; Students
Agents vs. users: Visual recommendation of research talks with multiple dimension of relevance,2016,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006226897&doi=10.1145%2f2946794&partnerID=40&md5=48b2b09fc81571d66aab7cef91fd9300,"Several approaches have been researched to help people deal with abundance of information. An important feature pioneered by social tagging systems and later used in other kinds of social systems is the ability to explore different community relevance prospects by examining items bookmarked by a specific user or items associated by various users with a specific tag. A ranked list of recommended items offered by a specific recommender engine can be considered as another relevance prospect. The problem that we address is that existing personalized social systems do not allow their users to explore and combine multiple relevance prospects. Only one prospect can be explored atany given time-a listof recommended items, alistof items bookmarked by a specific user, or a list of items marked with a specific tag. In this article, we explore the notion of combining multiple relevance prospects as a way to increase effectiveness and trust. We used a visual approach to recommend articles at a conference by explicitly presenting multiple dimensions of relevance. Suggestions offered by different recommendation techniques were embodied as recommender agents to put them on the same ground as users and tags. The results of two user studies performed at academic conferences allowed us to obtain interesting insights to enhance user interfaces of personalized social systems. More specifically, effectiveness and probability of item selection increase when users are able to explore and interrelate prospects of items relevance-that is, items bookmarked by users, recommendations and tags. Nevertheless, a less-technical audience may require guidance to understand the rationale of such intersections. © 2016 ACM 2160-6455/2016/07-ART11 $15.00.",Intelligent user interfaces; Recommender systems; Visualization,Flow visualization; Recommender systems; Academic conferences; Important features; Intelligent User Interfaces; Item selection; Multiple dimensions; Recommendation techniques; Recommender agent; Social tagging systems; User interfaces
See you see me: The role of Eye contact in multimodal human-robot interaction,2016,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991518491&doi=10.1145%2f2882970&partnerID=40&md5=4a7af028528212ed020f9b1d6623c678,"We focus on a fundamental looking behavior in human-robot interactions-gazing at each other's face. Eye contact and mutual gaze between two social partners are critical in smooth human-human interactions. Therefore, investigating at what moments and in what ways a robot should look at a human user's face as a response to the human's gaze behavior is an important topic. Toward this goal, we developed a gazecontingent human-robot interaction system, which relied on momentary gaze behaviors from a human user to control an interacting robot in real time. Using this system, we conducted an experiment in which human participants interacted with the robot in a joint-attention task. In the experiment, we systematically manipulated the robot's gaze toward the human partner's face in real time and then analyzed the human's gaze behavior as a response to the robot's gaze behavior. We found that more face looks from the robot led to more look-backs (to the robot's face) from human participants, and consequently, created more mutual gaze and eye contact between the two. Moreover, participants demonstrated more coordinated and synchronized multimodal behaviors between speech and gaze when more eye contact was successfully established and maintained. © 2016 ACM 2160-6455/2016/05-ART2 $15.00.",Gaze-based interaction; Human-robot interaction; Multimodal interface,Behavioral research; Human computer interaction; Man machine systems; Robots; Stereo vision; Eye contact; Gaze behavior; Gaze-based interaction; Gaze-contingent; Human-human interactions; Joint attention; Multi-modal interfaces; Mutual gazes; Human robot interaction
Prediction of who will be the next speaker and when using gaze behavior in multiparty meetings,2016,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84999672281&doi=10.1145%2f2757284&partnerID=40&md5=64c781b0373f360f989f0b396e9ce2f7,"In multiparty meetings, participants need to predict the end of the speaker's utterance and who will start speaking next, as well as consider a strategy for good timing to speak next. Gaze behavior plays an important role in smooth turn-changing. This article proposes a prediction model that features three processing steps to predict (I) whether turn-changing or turn-keeping will occur, (II) who will be the next speaker in turnchanging, and (III) the timing of the start of the next speaker's utterance. For the feature values of the model, we focused on gaze transition patterns and the timing structure of eye contact between a speaker and a listener near the end of the speaker's utterance. Gaze transition patterns provide information about the order in which gaze behavior changes. The timing structure of eye contact is defined as who looks at whom and who looks away first, the speaker or listener, when eye contact between the speaker and a listener occurs. We collected corpus data of multiparty meetings, using the data to demonstrate relationships between gaze transition patterns and timing structure and situations (I), (II), and (III). The results of our analyses indicate that the gaze transition pattern of the speaker and listener and the timing structure of eye contact have a strong association with turn-changing, the next speaker in turn-changing, and the start time of the next utterance. On the basis of the results, we constructed prediction models using the gaze transition patterns and timing structure. The gaze transition patterns were found to be useful in predicting turn-changing, the next speaker in turn-changing, and the start time of the next utterance. Contrary to expectations, we did not find that the timing structure is useful for predicting the next speaker and the start time. This study opens up new possibilities for predicting the next speaker and the timing of the next utterance using gaze transition patterns in multiparty meetings. © 2016 ACM.",Gaze behavior; Multiparty meetings; Next speaker prediction; Speech timing prediction; Turn-changing,Artificial intelligence; Feature values; Gaze behavior; Good timing; Multiparty meetings; Prediction model; Processing steps; Timing structure; Transition patterns; Forecasting
Introduction to the special issue on new directions in eye gaze for interactive intelligent systems,2016,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991519080&doi=10.1145%2f2893485&partnerID=40&md5=bb9ef27bcca42fb0a4a3e62bfa0b8157,"Eye gaze has been used broadly in interactive intelligent systems. The research area has grown in recent years to cover emerging topics that go beyond the traditional focus on interaction between a single user and an interactive system. This special issue presents five articles that explore new directions of gaze-based interactive intelligent systems, ranging from communication robots in dyadic and multiparty conversations to a driving simulator that uses eye gaze evidence to critique learners' behavior. © 2016 ACM.",Eye gaze; Interactive intelligent systems,Intelligent systems; Communication robot; Driving simulator; Emerging topics; Eye-gaze; Interactive intelligent systems; Interactive system; Multi-party conversations; Single users; Intelligent robots
Automatic classification of leading interactions in a string quartet,2016,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019538520&doi=10.1145%2f2818739&partnerID=40&md5=ea4a7069b6af5e07ee550d7e69c10745,"The aim of the present work is to analyze automatically the leading interactions between the musicians of a string quartet, using machine-learning techniques applied to nonverbal features of the musicians' behavior, which are detected through the help of a motion-capture system. We represent these interactions by a graph of ""influence"" of the musicians, which displays the relations ""is following"" and ""is not following"" with weighted directed arcs. The goal of the machine-learning problem investigated is to assign weights to these arcs in an optimal way. Since only a subset of the available training examples are labeled, a semisupervised support vector machine is used, which is based on a linear kernel to limit its model complexity. Specific potential applications within the field of human-computer interaction are also discussed, such as e-learning, networked music performance, and social active listening.",Automated analysis of nonverbal behavior; Head ancillary gestures; Support vector machines,Artificial intelligence; Directed graphs; E-learning; Learning systems; Support vector machines; Automatic classification; Head ancillary gestures; Machine learning problem; Machine learning techniques; Motion capture system; Music performance; Non-verbal features; Nonverbal behavior; Human computer interaction
Adaptive Body Gesture Representation for Automatic Emotion Recognition,2016,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84998865577&doi=10.1145%2f2818740&partnerID=40&md5=f618d3768a48b070674ee2bfbd6550d7,"We present a computational model and a system for the automated recognition of emotions starting from full-body ovement. Three-dimensional motion data of full-body movements are obtained either from professional optical motion-capture systems (Qualisys) or from low-cost RGB-D sensors (Kinect and Kinect2). A number of features are then automatically extracted at different levels, from kinematics of a single joint to more global expressive features inspired by psychology and humanistic theories (e.g., contraction index, fluidity, and impulsiveness). An abstraction layer based on dictionary learning further rocesses these movement features to increase the model enerality and to deal with intraclass variability, noise, and incomplete information characterizing emotion expression in human movement. The resulting feature vector is the input for a classifier performing real-time automatic emotion recognition based on linear support vector machines. The recognition erformance of the proposed model is presented and discussed, including the tradeoff between precision of the tracking easures (we compare the Kinect RGB-D sensor and the Qualisys motion-capture system) versus dimension of the training dataset. The resulting model and system have been successfully applied in the development of serious games for helping autistic children learn to recognize and express emotions by means of their full-body movement. © 2016 ACM.",automatic emotion recognition; Body; dictionary learning; motion capture,Abstracting; Adaptive optics; Computation theory; Face recognition; Speech recognition; Automatic emotion recognition; Body; Dictionary learning; Incomplete information; Linear Support Vector Machines; Motion capture; Optical motion capture; Three-dimensional motion; Psychology computing
High-volume hypothesis testing: Systematic exploration of event sequence comparisons,2016,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040765784&doi=10.1145%2f2890478&partnerID=40&md5=a8175c1559e8dc557d4acb1dbec66659,"Cohort comparison studies have traditionally been hypothesis driven and conducted in carefully controlled environments (such as clinical trials). Given two groups of event sequence data, researchers test a single hypothesis (e.g., does the group taking Medication A exhibit more deaths than the group taking Medication B?). Recently, however, researchers have been moving toward more exploratory methods of retrospective analysis with existing data. In this article, we begin by showing that the task of cohort comparison is specific enough to support automatic computation against a bounded set of potential questions and objectives, a method that we refer to as High-Volume Hypothesis Testing (HVHT). From this starting point, we demonstrate that the diversity of these objectives, both across and within different domains, as well as the inherent complexities of real-world datasets, still requires human involvement to determine meaningful insights. We explore how visualization and interaction better support the task of exploratory data analysis and the understanding of HVHT results (how significant they are, why they are meaningful, and whether the entire dataset has been exhaustively explored). Through interviews and case studies with domain experts, we iteratively design and implement visualization and interaction techniques in a visual analytics tool, CoCo. As a result of our evaluation, we propose six design guidelines for enabling users to explore large result sets of HVHT systematically and flexibly in order to glean meaningful insights more quickly. Finally, we illustrate the utility of this method with three case studies in the medical domain.",Cohort comparison; Event sequences; Visual analytics,Graphical user interfaces; Iterative methods; Medical applications; Visualization; Automatic computations; Cohort comparison; Controlled environment; Event sequence; Exploratory data analysis; Interaction techniques; Retrospective analysis; Visual analytics; Data visualization
A gaze-contingent adaptive virtual reality driving environment for intervention in individuals with autism spectrum disorders,2016,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019538543&doi=10.1145%2f2892636&partnerID=40&md5=6d2a73145267f40a7e5153e5e6f4ea93,"In addition to social and behavioral deficits, individuals with Autism Spectrum Disorder (ASD) often struggle to develop the adaptive skills necessary to achieve independence. Driving intervention in individuals with ASD is a growing area of study, but it is still widely under-researched. We present the development and preliminary assessment of a gaze-contingent adaptive virtual reality driving simulator that uses real-time gaze information to adapt the driving environment with the aim of providing a more individualized method of driving intervention. We conducted a small pilot study of 20 adolescents with ASD using our system: 10 with the adaptive gaze-contingent version of the system and 10 in a purely performance-based version. Preliminary results suggest that the novel intervention system may be beneficial in teaching driving skills to individuals with ASD. © 2016 ACM.",Autism spectrum disorders; Driving intervention; Gaze feedback,Automobile drivers; Virtual reality; Adaptive skills; Autism spectrum disorders; Driving environment; Driving intervention; Driving simulator; Gaze-contingent; Performance based; Preliminary assessment; Diseases
Interactive topic modeling for exploring asynchronous online conversations: Design and evaluation of conVisIT,2016,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84967309062&doi=10.1145%2f2854158&partnerID=40&md5=63abb9b36028f8b057d12db7dd6477a0,"Since the mid-2000s, there has been exponential growth of asynchronous online conversations, thanks to the rise of social media. Analyzing and gaining insights from such conversations can be quite challenging for a user, especially when the discussion becomes very long. A promising solution to this problem is topic modeling, since it may help the user to understand quickly what was discussed in a long conversation and to explore the comments of interest. However, the results of topic modeling can be noisy, and they may not match the user's current information needs. To address this problem, we propose a novel topic modeling system for asynchronous conversations that revises the model on the fly on the basis of users' feedback. We then integrate this system with interactive visualization techniques to support the user in exploring long conversations, as well as in revising the topic model when the current results are not adequate to fulfill the user's information needs. Finally, we report on an evaluation with real users that compared the resulting system with both a traditional interface and an interactive visual interface that does not support humanin- the-loop topic modeling. Both the quantitative results and the subjective feedback from the participants illustrate the potential benefits of our interactive topic modeling approach for exploring conversations, relative to its counterparts. © 2016 ACM.",Asynchronous conversation; Computer mediated communication; Interactive topic modeling; Text visualization,Information science; Visualization; Asynchronous conversation; Computer mediated communication; Design and evaluations; Interactive visualizations; Quantitative result; Subjective feedback; Text visualization; Topic Modeling; Social networking (online)
Supporting the design of machine learning workflows with a recommendation system,2016,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84967240012&doi=10.1145%2f2852082&partnerID=40&md5=28937c36062e7936cb514f5dd410b295,"Machine learning and data analytics tasks in practice require several consecutive processing steps. Rapid- Miner is a widely used software tool for the development and execution of such analytics workflows. Unlike many other algorithm toolkits, it comprises a visual editor that allows the user to design processes on a conceptual level. This conceptual and visual approach helps the user to abstract from the technical details during the development phase and to retain a focus on the core modeling task. The large set of preimplemented data analysis and machine learning operations available in the tool, as well as their logical dependencies, can, however, be overwhelming in particular for novice users. In this work, we present an add-on to the RapidMiner framework that supports the user during the modeling phase by recommending additional operations to insert into the currently developed machine learning workflow. First, we propose different recommendation techniques and evaluate them in an offline setting using a pool of several thousand existing workflows. Second, we present the results of a laboratory study, which show that our tool helps users to significantly increase the efficiency of the modeling process. Finally, we report on analyses using data that were collected during the real-world deployment of the plug-in component and compare the results of the live deployment of the tool with the results obtained through an offline analysis and a replay simulation. © 2016 ACM.",Data analysis workflows; RapidMiner; Visual process modeling,Artificial intelligence; Data handling; Information analysis; Development phase; Laboratory studies; Logical dependencies; RapidMiner; Real world deployment; Recommendation techniques; Visual process; Work-flows; Learning systems
"""i'll Be There Next"": A Multiplex Care Robot System that Conveys Service Order Using Gaze Gestures",2016,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84999635119&doi=10.1145%2f2844542&partnerID=40&md5=2d4c08e43de89e62e90aabfb75453bdc,"In this article, we discuss our findings from an ethnographic study at an elderly care center where we observed the utilization of two different functions of human gaze to convey service order (i.e., ""who is served first and who is served next""). In one case, when an elderly person requested assistance, the gaze of the care worker communicated that he/she would serve that client next in turn. In the other case, the gaze conveyed a request to the service seeker to wait until the care worker finished attending the current client. Each gaze function depended on the care worker's current engagement and other behaviors. We sought to integrate these findings into the development of a robot that might function more effectively in multiple human-robot party settings.We focused on the multiple functions of gaze and bodily actions, implementing those functions into our robot. We conducted three experiments to gauge a combination of gestures and gazes performed by our robot. This article demonstrates that the employment of gaze is an important consideration when developing robots that can interact effectively in multiple human-robot party settings. © 2016 ACM.",embodied; gaze; human-robot interface; Interaction analysis; mobile robot; multiparty interaction,Mobile robots; Robots; embodied; gaze; Human-Robot Interface; Interaction analysis; Multi-party interactions; Human robot interaction
A spatial constraint model for manipulating static visualizations,2024,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85197336076&doi=10.1145%2f3657642&partnerID=40&md5=cd6b636358611850febaaa2970349811,"We introduce a spatial constraint model to characterize the positioning and interactions in visualizations, thereby facilitating the activation of static visualizations. Our model provides users with the capability to manipulate visualizations through operations such as selection, filtering, navigation, arrangement, and aggregation. Building upon this conceptual framework, we propose a prototype system designed to activate pre-existing visualizations by imbuing them with intelligent interactions. This augmentation is accomplished through the integration of visual objects with forces. The instantiation of our spatial constraint model enables seamless animated transitions between distinct visualization layouts. To demonstrate the efficacy of our approach, we present usage scenarios that involve the activation of visualizations within real-world contexts. © 2024 Copyright held by the owner/author(s).",Constraint; Intelligent interaction; Interaction model,Abstracting; Chemical activation; User interfaces; Animated transitions; Conceptual frameworks; Constraint; Constraints models; Intelligent interactions; Interaction modeling; Prototype system; Spatial constraints; Static visualizations; Visual objects; Visualization
Cooperative multi-objective Bayesian design optimization,2024,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85197358028&doi=10.1145%2f3657643&partnerID=40&md5=1ef2eb11a89a6665a99a2eaa59dcd892,"Computational methods can potentially facilitate user interface design by complementing designer intuition, prior experience, and personal preference. Framing a user interface design task as a multi-objective optimization problem can help with operationalizing and structuring this process at the expense of designer agency and experience. While offering a systematic means of exploring the design space, the optimization process cannot typically leverage the designer’s expertise in quickly identifying that a given “bad” design is not worth evaluating. We here examine a cooperative approach where both the designer and optimization process share a common goal and work in partnership by establishing a shared understanding of the design space. We tackle the research question: How can we foster cooperation between the designer and a systematic optimization process in order to best leverage their combined strength? We introduce and present an evaluation of a cooperative approach that allows the user to express their design insight and work in concert with a multi-objective design process. We find that the cooperative approach successfully encourages designers to explore more widely in the design space than when they are working without assistance from an optimization process. The cooperative approach also delivers design outcomes that are comparable to an optimization process run without any direct designer input but achieves this with greater efficiency and substantially higher designer engagement levels. © 2024 Copyright held by the owner/author(s).",Bayesian optimization; Interaction technique; Interface design,Design; User interfaces; Bayesian design; Bayesian optimization; Design optimization; Design spaces; Interaction techniques; Interface designs; Multi objective; Personal preferences; Prior experience; User interface designs; Multiobjective optimization
generAItor: Tree-in-the-loop Text Generation for Language Model Explainability and Adaptation,2024,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85196563085&doi=10.1145%2f3652028&partnerID=40&md5=261f126be68a366475e14c4696c542c7,"Large language models (LLMs) are widely deployed in various downstream tasks, e.g., auto-completion, aided writing, or chat-based text generation. However, the considered output candidates of the underlying search algorithm are under-explored and under-explained. We tackle this shortcoming by proposing a tree-in-the-loop approach, where a visual representation of the beam search tree is the central component for analyzing, explaining, and adapting the generated outputs. To support these tasks, we present generAItor, a visual analytics technique, augmenting the central beam search tree with various task-specific widgets, providing targeted visualizations and interaction possibilities. Our approach allows interactions on multiple levels and offers an iterative pipeline that encompasses generating, exploring, and comparing output candidates, as well as fine-tuning the model based on adapted data. Our case study shows that our tool generates new insights in gender bias analysis beyond state-of-the-art template-based methods. Additionally, we demonstrate the applicability of our approach in a qualitative user study. Finally, we quantitatively evaluate the adaptability of the model to few samples, as occurring in text-generation use cases. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Beam search tree; Explainability; Language transformers; Large language models; Natural language generation; Visual analytics,Computational linguistics; Iterative methods; Natural language processing systems; Visual languages; Beam search; Beam search tree; Explainability; Language model; Language transformer; Large language model; Natural language generation; Search trees; Text generations; Visual analytics; Visualization
“It would work for me too”: How online communities shape software developers’ trust in AI-powered code generation tools,2024,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85196307305&doi=10.1145%2f3651990&partnerID=40&md5=6251d5849eb0acdda1664804f09a108a,"While revolutionary AI-powered code generation tools have been rising rapidly, we know little about how and how to help software developers form appropriate trust in those AI tools. Through a two-phase formative study, we investigate how online communities shape developers’ trust in AI tools and how we can leverage community features to facilitate appropriate user trust. Through interviewing 17 developers, we find that developers collectively make sense of AI tools using the experiences shared by community members and leverage community signals to evaluate AI suggestions. We then surface design opportunities and conduct 11 design probe sessions to explore the design space of using community features to support user trust in AI code generation systems. We synthesize our findings and extend an existing model of user trust in AI technologies with sociotechnical factors. We map out the design considerations for integrating user community into the AI code generation experience. © 2024 Copyright held by the owner/author(s).",Generative AI; Human-AI interaction; Online communities; Software engineering; Trust,Codes (symbols); Online systems; Software engineering; Code generation tools; Codegeneration; Design spaces; Generative AI; Human-AI interaction; On-line communities; Software developer; Surface design; Trust; Two phase; Social networking (online)
Talk2Data: A Natural Language Interface for Exploratory Visual Analysis via Question Decomposition,2024,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85197339214&doi=10.1145%2f3643894&partnerID=40&md5=fa6528c0c50c380b1b1cbd55d1ae9e97,"Through a natural language interface (NLI) for exploratory visual analysis, users can directly “ask” analytical questions about the given tabular data. This process greatly improves user experience and lowers the technical barriers of data analysis. Existing techniques focus on generating a visualization from a concrete question. However, complex questions, requiring multiple data queries and visualizations to answer, are frequently asked in data exploration and analysis, which cannot be easily solved with the existing techniques. To address this issue, in this article, we introduce Talk2Data, a natural language interface for exploratory visual analysis that supports answering complex questions. It leverages an advanced deep-learning model to resolve complex questions into a series of simple questions that could gradually elaborate on the users’ requirements. To present answers, we design a set of annotated and captioned visualizations to represent the answers in a form that supports interpretation and narration. We conducted an ablation study and a controlled user study to evaluate the Talk2Data’s effectiveness and usefulness. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",information visualization; Natural language interfaces,Deep learning; Information systems; Natural language processing systems; Query processing; User interfaces; Visual languages; Complex questions; Data exploration; Data query; Information visualization; Multiple data; Natural language interfaces; Tabular data; Technical barriers; Users' experiences; Visual analysis; Visualization
Man and the Machine: Effects of AI-assisted Human Labeling on Interactive Annotation of Real-time Video Streams,2024,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85196958625&doi=10.1145%2f3649457&partnerID=40&md5=07e8ebc75cd3d600e8ce5ef538532004,"AI-assisted interactive annotation is a powerful way to facilitate data annotation—a prerequisite for constructing robust AI models. While AI-assisted interactive annotation has been extensively studied in static settings, less is known about its usage in dynamic scenarios where the annotators operate under time and cognitive constraints, e.g., while detecting suspicious or dangerous activities from real-time surveillance feeds. Understanding how AI can assist annotators in these tasks and facilitate consistent annotation is paramount to ensure high performance for AI models trained on these data. We address this gap in interactive machine learning (IML) research, contributing an extensive investigation of the benefits, limitations, and challenges of AI-assisted annotation in dynamic application use cases. We address both the effects of AI on annotators and the effects of (AI) annotations on the performance of AI models trained on annotated data in real-time video annotations. We conduct extensive experiments that compare annotation performance at two annotator levels (expert and non-expert) and two interactive labeling techniques (with and without AI assistance). In a controlled study with N = 34 annotators and a follow-up study with 51,963 images and their annotation labels being input to the AI model, we demonstrate that the benefits of AI-assisted models are greatest for non-expert users and for cases where targets are only partially or briefly visible. The expert users tend to outperform or achieve similar performance as the AI model. Labels combining AI and expert annotations result in the best overall performance as the AI reduces overflow and latency in the expert annotations. We derive guidelines for the use of AI-assisted human annotation in real-time dynamic use cases. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",AI-assisted interface; annotation; Computer vision; deep learning; human-in-the-loop; intelligent user interface; machine learning; man-machine; object detection; videos,Computer vision; Deep learning; Learning systems; Object recognition; User interfaces; Video streaming; AI-assisted interface; Annotation; Deep learning; Human-in-the-loop; Intelligent User Interfaces; Machine-learning; Man machines; Objects detection; Performance; Video; Object detection
Entity Footprinting: Modeling Contextual User States via Digital Activity Monitoring,2024,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85196964789&doi=10.1145%2f3643893&partnerID=40&md5=2c73bc7062b30cc4a511a72dae2d8f15,"Our digital life consists of activities that are organized around tasks and exhibit different user states in the digital contexts around these activities. Previous works have shown that digital activity monitoring can be used to predict entities that users will need to perform digital tasks. There have been methods developed to automatically detect the tasks of a user. However, these studies typically support only specific applications and tasks, and relatively little research has been conducted on real-life digital activities. This article introduces user state modeling and prediction with contextual information captured as entities, recorded from real-world digital user behavior, called entity footprinting - a system that records users' digital activities on their screens and proactively provides useful entities across application boundaries without requiring explicit query formulation. Our methodology is to detect contextual user states using latent representations of entities occurring in digital activities. Using topic models and recurrent neural networks, the model learns the latent representation of concurrent entities and their sequential relationships. We report a field study in which the digital activities of 13 people were recorded continuously for 14 days. The model learned from this data is used to (1) predict contextual user states and (2) predict relevant entities for the detected states. The results show improved user state detection accuracy and entity prediction performance compared to static, heuristic, and basic topic models. Our findings have implications for the design of proactive recommendation systems that can implicitly infer users' contextual state by monitoring users' digital activities and proactively recommending the right information at the right time.  © 2024 held by the owner/author(s).",Entity footprinting; personal assistant; real-world tasks; user intent modeling,Behavioral research; Forecasting; Search engines; User profile; Activity monitoring; Digital activities; Digital life; Entity footprinting; Footprinting; Intent models; Personal assistants; Real-world task; Topic Modeling; User intent modeling; Recurrent neural networks
Toward Addressing Ambiguous Interactions and Inferring User Intent with Dimension Reduction and Clustering Combinations in Visual Analytics,2024,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85196208551&doi=10.1145%2f3588565&partnerID=40&md5=57d328fccbc0479eb7b51b041c1c361b,"Direct manipulation interactions on projections are often incorporated in visual analytics applications. These interactions enable analysts to provide incremental feedback to the system in a semi-supervised manner, demonstrating relationships that the analyst wishes to find within the data. However, determining the precise intent of the analyst is a challenge. When an analyst interacts with a projection, the inherent ambiguity of interactions can lead to a variety of possible interpretations that the system can infer. Previous work has demonstrated the utility of clusters as an interaction target to address this ""With Respect to What""problem in dimension-reduced projections. However, the introduction of clusters introduces interaction inference challenges as well. In this work, we discuss the interaction space for the simultaneous use of semi-supervised dimension reduction and clustering algorithms. We introduce a novel pipeline representation to disambiguate between interactions on observations and clusters, as well as which underlying model is responding to those analyst interactions. We use a prototype visual analytics tool to demonstrate the effects of these ambiguous interactions, their properties, and the insights that an analyst can glean from each.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesDimension reduction; clustering; intent; interaction; visual analytics,Reduction; Visualization; % reductions; Additional key word and phrasesdimension reduction; Clusterings; Dimension reduction; Inferring user intents; Intent; Interaction; Key words; Semi-supervised; Visual analytics; Clustering algorithms
Integrity-based Explanations for Fostering Appropriate Trust in AI Agents,2024,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85196154683&doi=10.1145%2f3610578&partnerID=40&md5=52407fc18477eae287479cfb45eab531,"Appropriate trust is an important component of the interaction between people and AI systems, in that ""inappropriate""trust can cause disuse, misuse, or abuse of AI. To foster appropriate trust in AI, we need to understand how AI systems can elicit appropriate levels of trust from their users. Out of the aspects that influence trust, this article focuses on the effect of showing integrity. In particular, this article presents a study of how different integrity-based explanations made by an AI agent affect the appropriateness of trust of a human in that agent. To explore this, (1) we provide a formal definition to measure appropriate trust, (2) present a between-subject user study with 160 participants who collaborated with an AI agent in such a task. In the study, the AI agent assisted its human partner in estimating calories on a food plate by expressing its integrity through explanations focusing on either honesty, transparency, or fairness. Our results show that (a) an agent who displays its integrity by being explicit about potential biases in data or algorithms achieved appropriate trust more often compared to being honest about capability or transparent about the decision-making process, and (b) subjective trust builds up and recovers better with honesty-like integrity explanations. Our results contribute to the design of agent-based AI systems that guide humans to appropriately trust them, a formal method to measure appropriate trust, and how to support humans in calibrating their trust in AI.  © 2024 Copyright held by the owner/author(s).",Additional Key Words and PhrasesIntegrity; appropriate trust; artificial agents; explanations; fairness; HCI; honesty; intelligent agents; transparency; trust,Behavioral research; Decision making; Formal methods; Human computer interaction; Transparency; Additional key word and phrasesintegrity; AI systems; Appropriate trust; Artificial agents; Explanation; Fairness; Formal definition; Honesty; Key words; Trust; Intelligent agents
Meaningful Explanation Effect on User’s Trust in an AI Medical System: Designing Explanations for Non-Expert Users,2023,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181447199&doi=10.1145%2f3631614&partnerID=40&md5=bb6c639b4c8e051241d8bf685c185af2,"Whereas most research in AI system explanation for healthcare applications looks at developing algorithmic explanations targeted at AI experts or medical professionals, the question we raise is: How do we build meaningful explanations for laypeople? And how does a meaningful explanation affect user’s trust perceptions? Our research investigates how the key factors affecting human-AI trust change in the light of human expertise, and how to design explanations specifically targeted at non-experts. By means of a stage-based design method, we map the ways laypeople understand AI explanations in a User Explanation Model. We also map both medical professionals and AI experts’ practice in an Expert Explanation Model. A Target Explanation Model is then proposed, which represents how experts’ practice and layperson’s understanding can be combined to design meaningful explanations. Design guidelines for meaningful AI explanations are proposed, and a prototype of AI system explanation for non-expert users in a breast cancer scenario is presented and assessed on how it affect users’ trust perceptions. © 2023 Copyright held by the owner/author(s)",artificial intelligence explanation; Explanation; trust,AI systems; Algorithmics; Artificial intelligence explanation; Expert users; Explanation; Health care application; Medical professionals; Medical systems; System designing; Trust; Design
The Role of Explainable AI in the Research Field of AI Ethics,2023,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178638517&doi=10.1145%2f3599974&partnerID=40&md5=25409b3ef0f9eb514878f5ffb9f387d1,"Ethics of Artificial Intelligence (AI) is a growing research field that has emerged in response to the challenges related to AI. Transparency poses a key challenge for implementing AI ethics in practice. One solution to transparency issues is AI systems that can explain their decisions. Explainable AI (XAI) refers to AI systems that are interpretable or understandable to humans. The research fields of AI ethics and XAI lack a common framework and conceptualization. There is no clarity of the field’s depth and versatility. A systematic approach to understanding the corpus is needed. A systematic review offers an opportunity to detect research gaps and focus points. This article presents the results of a systematic mapping study (SMS) of the research field of the Ethics of AI. The focus is on understanding the role of XAI and how the topic has been studied empirically. An SMS is a tool for performing a repeatable and continuable literature search. This article contributes to the research field with a Systematic Map that visualizes what, how, when, and why XAI has been studied empirically in the field of AI ethics. The mapping reveals research gaps in the area. Empirical contributions are drawn from the analysis. The contributions are reflected on in regards to theoretical and practical implications. As the scope of the SMS is a broader research area of AI ethics, the collected dataset opens possibilities to continue the mapping process in other directions. © 2023 Copyright held by the owner/author(s)",AI ethics; artificial intelligence; explainable AI; systematic mapping study,Artificial intelligence; Ethical technology; Mapping; Artificial intelligence ethic; Artificial intelligence systems; Explainable artificial intelligence; Focus points; Literature search; Research fields; Research focus; Research gaps; Systematic mapping studies; Systematic Review; Transparency
How Do Users Experience Traceability of AI Systems? Examining Subjective Information Processing Awareness in Automated Insulin Delivery (AID) Systems,2023,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174086515&doi=10.1145%2f3588594&partnerID=40&md5=f21f862e622633158d00d0c0539bf794,"When interacting with artificial intelligence (AI) in the medical domain, users frequently face automated information processing, which can remain opaque to them. For example, users with diabetes may interact daily with automated insulin delivery (AID). However, effective AID therapy requires traceability of automated decisions for diverse users. Grounded in research on human-automation interaction, we study Subjective Information Processing Awareness (SIPA) as a key construct to research users’ experience of explainable AI. The objective of the present research was to examine how users experience differing levels of traceability of an AI algorithm. We developed a basic AID simulation to create realistic scenarios for an experiment with N = 80, where we examined the effect of three levels of information disclosure on SIPA and performance. Attributes serving as the basis for insulin needs calculation were shown to users, who predicted the AID system’s calculation after over 60 observations. Results showed a difference in SIPA after repeated observations, associated with a general decline of SIPA ratings over time. Supporting scale validity, SIPA was strongly correlated with trust and satisfaction with explanations. The present research indicates that the effect of different levels of information disclosure may need several repetitions before it manifests. Additionally, high levels of information disclosure may lead to a miscalibration between SIPA and performance in predicting the system’s results. The results indicate that for a responsible design of XAI, system designers could utilize prediction tasks in order to calibrate experienced traceability. © 2023 Copyright held by the owner/author(s)",Explainability; human AI cooperation; human-centered AI; trust,Automation; Human engineering; User interfaces; Explainability; Human artificial intelligence cooperation; Human-centered artificial intelligence; Information disclosure; Insulin delivery; Insulin delivery system; Intelligence cooperation; Subjective information; Trust; Users' experiences; Insulin
LIMEADE: From AI Explanations to Advice Taking,2023,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181518881&doi=10.1145%2f3589345&partnerID=40&md5=01fde6f62b9d6e818b6fbf4a694d1d5a,"Research in human-centered AI has shown the benefits of systems that can explain their predictions. Methods that allow AI to take advice from humans in response to explanations are similarly useful. While both capabilities are well developed for transparent learning models (e.g., linear models and GA2Ms) and recent techniques (e.g., LIME and SHAP) can generate explanations for opaque models, little attention has been given to advice methods for opaque models. This article introduces LIMEADE, the first general framework that translates both positive and negative advice (expressed using high-level vocabulary such as that employed by post hoc explanations) into an update to an arbitrary, underlying opaque model. We demonstrate the generality of our approach with case studies on 70 real-world models across two broad domains: image classification and text recommendation. We show that our method improves accuracy compared to a rigorous baseline on the image classification domains. For the text modality, we apply our framework to a neural recommender system for scientific papers on a public website; our user study shows that our framework leads to significantly higher perceived user control, trust, and satisfaction. © 2023 Copyright held by the owner/author(s)",advice taking; explainable AI; Explainable recommendations; Human-AI interaction; interactive machine learning,Image enhancement; Lime; Machine learning; Text processing; Advice taking; Case-studies; Explainable AI; Explainable recommendation; Human-AI interaction; Images classification; Interactive machine learning; Learning models; Linear modeling; Real-world modeling; Image classification
Visual Analytics of Neuron Vulnerability to Adversarial Attacks on Convolutional Neural Networks,2023,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181499167&doi=10.1145%2f3587470&partnerID=40&md5=22c86a8cab3e7865e8455159f9c7121f,"Adversarial attacks on a convolutional neural network (CNN)—injecting human-imperceptible perturbations into an input image—could fool a high-performance CNN into making incorrect predictions. The success of adversarial attacks raises serious concerns about the robustness of CNNs, and prevents them from being used in safety-critical applications, such as medical diagnosis and autonomous driving. Our work introduces a visual analytics approach to understanding adversarial attacks by answering two questions: (1) Which neurons are more vulnerable to attacks? and (2) Which image features do these vulnerable neurons capture during the prediction? For the first question, we introduce multiple perturbation-based measures to break down the attacking magnitude into individual CNN neurons and rank the neurons by their vulnerability levels. For the second, we identify image features (e.g., cat ears) that highly stimulate a user-selected neuron to augment and validate the neuron’s responsibility. Furthermore, we support an interactive exploration of a large number of neurons by aiding with hierarchical clustering based on the neurons’ roles in the prediction. To this end, a visual analytics system is designed to incorporate visual reasoning for interpreting adversarial attacks. We validate the effectiveness of our system through multiple case studies as well as feedback from domain experts. © 2023 Copyright held by the owner/author(s)",adversarial attack; Convolutional neural networks; explainable machine learning,Convolution; Convolutional neural networks; Diagnosis; Forecasting; Machine learning; Safety engineering; Visualization; Adversarial attack; Autonomous driving; Convolutional neural network; Explainable machine learning; Image features; Input image; Machine-learning; Performance; Safety critical applications; Visual analytics; Neurons
XAutoML: A Visual Analytics Tool for Understanding and Validating Automated Machine Learning,2023,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181445629&doi=10.1145%2f3625240&partnerID=40&md5=faaf94ea0978fba6b1095d8ed68641b9,"In the last 10 years, various automated machine learning (AutoML) systems have been proposed to build end-to-end machine learning (ML) pipelines with minimal human interaction. Even though such automatically synthesized ML pipelines are able to achieve competitive performance, recent studies have shown that users do not trust models constructed by AutoML due to missing transparency of AutoML systems and missing explanations for the constructed ML pipelines. In a requirements analysis study with 36 domain experts, data scientists, and AutoML researchers from different professions with vastly different expertise in ML, we collect detailed informational needs for AutoML. We propose XAutoML, an interactive visual analytics tool for explaining arbitrary AutoML optimization procedures and ML pipelines constructed by AutoML. XAutoML combines interactive visualizations with established techniques from explainable artificial intelligence (XAI) to make the complete AutoML procedure transparent and explainable. By integrating XAutoML with JupyterLab, experienced users can extend the visual analytics with ad-hoc visualizations based on information extracted from XAutoML. We validate our approach in a user study with the same diverse user group from the requirements analysis. All participants were able to extract useful information from XAutoML, leading to a significantly increased understanding of ML pipelines produced by AutoML and the AutoML optimization itself. © 2023 Copyright held by the owner/author(s)",Automated machine learning; AutoML; Explainable AI; transparency; XAI,Automation; Machine learning; Pipelines; Requirements engineering; Visualization; Analytic tools; Automated machine learning; Automated machines; Explainable AI; Machine-learning; Visual analytics; XAI; Transparency
Effects of AI and Logic-Style Explanations on Users’ Decisions Under Different Levels of Uncertainty,2023,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168380597&doi=10.1145%2f3588320&partnerID=40&md5=e73ba838af3bf67a46e249f20c261e7b,"Existing eXplainable Artificial Intelligence (XAI) techniques support people in interpreting AI advice. However, although previous work evaluates the users’ understanding of explanations, factors influencing the decision support are largely overlooked in the literature. This article addresses this gap by studying the impact of user uncertainty, AI correctness, and the interaction between AI uncertainty and explanation logic-styles for classification tasks. We conducted two separate studies: one requesting participants to recognize handwritten digits and one to classify the sentiment of reviews. To assess the decision making, we analyzed the task performance, agreement with the AI suggestion, and the user’s reliance on the XAI interface elements. Participants make their decision relying on three pieces of information in the XAI interface (image or text instance, AI prediction, and explanation). Participants were shown one explanation style (between-participants design) according to three styles of logical reasoning (inductive, deductive, and abductive). This allowed us to study how different levels of AI uncertainty influence the effectiveness of different explanation styles. The results show that user uncertainty and AI correctness on predictions significantly affected users’ classification decisions considering the analyzed metrics. In both domains (images and text), users relied mainly on the instance to decide. Users were usually overconfident about their choices, and this evidence was more pronounced for text. Furthermore, the inductive style explanations led to overreliance on the AI advice in both domains—it was the most persuasive, even when the AI was incorrect. The abductive and deductive styles have complex effects depending on the domain and the AI uncertainty levels. © 2023 Copyright held by the owner/author(s)",AI correctness; AI uncertainty; CNNs; Explainable AI; explanations; intelligent user interfaces; logical reasoning; MNIST; neural networks; user uncertainty; Yelp Reviews,Behavioral research; Character recognition; Computer circuits; Decision making; Decision support systems; User interfaces; AI correctness; AI uncertainty; Explainable AI; Explanation; Intelligent User Interfaces; Logical reasoning; MNIST; Neural-networks; Uncertainty; User uncertainty; Yelp review; Uncertainty analysis
Directive Explanations for Actionable Explainability in Machine Learning Applications,2023,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85166776198&doi=10.1145%2f3579363&partnerID=40&md5=a3c05c90f79406eca4e29c6b596df32a,"In this article, we show that explanations of decisions made by machine learning systems can be improved by not only explaining why a decision was made but also explaining how an individual could obtain their desired outcome. We formally define the concept of directive explanations (those that offer specific actions an individual could take to achieve their desired outcome), introduce two forms of directive explanations (directive-specific and directive-generic), and describe how these can be generated computationally. We investigate people’s preference for and perception toward directive explanations through two online studies, one quantitative and the other qualitative, each covering two domains (the credit scoring domain and the employee satisfaction domain). We find a significant preference for both forms of directive explanations compared to non-directive counterfactual explanations. However, we also find that preferences are affected by many aspects, including individual preferences and social factors. We conclude that deciding what type of explanation to provide requires information about the recipients and other contextual information. This reinforces the need for a human-centered and context-specific approach to explainable AI. © 2023 Copyright held by the owner/author(s)",counterfactual explanations; directive explanations; Explainable AI,Counterfactual explanation; Counterfactuals; Credit scoring; Directive explanation; Employee satisfaction; Explainable AI; Machine learning applications; Machine learning systems; Online studies; Two domains; Machine learning
Explainable Activity Recognition in Videos using Deep Learning and Tractable Probabilistic Models,2023,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181515569&doi=10.1145%2f3626961&partnerID=40&md5=8a80be1dcd3aaaae02a0066239eb4c58,"We consider the following video activity recognition (VAR) task: given a video, infer the set of activities being performed in the video and assign each frame to an activity. Although VAR can be solved accurately using existing deep learning techniques, deep networks are neither interpretable nor explainable and as a result their use is problematic in high stakes decision-making applications (in healthcare, experimental Biology, aviation, law, etc.). In such applications, failure may lead to disastrous consequences and therefore it is necessary that the user is able to either understand the inner workings of the model or probe it to understand its reasoning patterns for a given decision. We address these limitations of deep networks by proposing a new approach that feeds the output of a deep model into a tractable, interpretable probabilistic model called a dynamic conditional cutset network that is defined over the explanatory and output variables and then performing joint inference over the combined model. The two key benefits of using cutset networks are: (a) they explicitly model the relationship between the output and explanatory variables and as a result, the combined model is likely to be more accurate than the vanilla deep model and (b) they can answer reasoning queries in polynomial time and as a result, they can derive meaningful explanations by efficiently answering explanation queries. We demonstrate the efficacy of our approach on two datasets, Textually Annotated Cooking Scenes (TACoS), and wet lab, using conventional evaluation measures such as the Jaccard Index and Hamming Loss, as well as a human-subjects study. © 2023 Copyright held by the owner/author(s)",cutset networks; dynamic Bayesian networks; Temporal models; tractable probabilistic models,Bayesian networks; Deep learning; Learning systems; Pattern recognition; Petroleum reservoir evaluation; Polynomial approximation; Query processing; Value engineering; Activity recognition; Cut sets; Cutset network; Dynamic Bayesian networks; Explanatory variables; Output variables; Probabilistic models; Temporal models; Tractable probabilistic model; Video activity; Decision making
Does this Explanation Help? Designing Local Model-agnostic Explanation Representations and an Experimental Evaluation Using Eye-tracking Technology,2023,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181464971&doi=10.1145%2f3607145&partnerID=40&md5=9c59918770212f7422521aa6af946846,"In Explainable Artificial Intelligence (XAI) research, various local model-agnostic methods have been proposed to explain individual predictions to users in order to increase the transparency of the underlying Artificial Intelligence (AI) systems. However, the user perspective has received less attention in XAI research, leading to a (1) lack of involvement of users in the design process of local model-agnostic explanations representations and (2) a limited understanding of how users visually attend them. Against this backdrop, we refined representations of local explanations from four well-established model-agnostic XAI methods in an iterative design process with users. Moreover, we evaluated the refined explanation representations in a laboratory experiment using eye-tracking technology as well as self-reports and interviews. Our results show that users do not necessarily prefer simple explanations and that their individual characteristics, such as gender and previous experience with AI systems, strongly influence their preferences. In addition, users find that some explanations are only useful in certain scenarios making the selection of an appropriate explanation highly dependent on context. With our work, we contribute to ongoing research to improve transparency in AI. © 2023 Copyright held by the owner/author(s)",explainability; eye-tracking; Machine learning; user-centric evaluation,Design; Eye tracking; Human engineering; Iterative methods; Transparency; Artificial intelligence systems; Design-process; Experimental evaluation; Explainability; Eye tracking technologies; Eye-tracking; Individual prediction; Local model; Machine-learning; User-centric evaluations; Machine learning
"Co-design of Human-centered, Explainable AI for Clinical Decision Support",2023,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160005164&doi=10.1145%2f3587271&partnerID=40&md5=29f0bf9be77c55daadd1040a4a9bfe27,"eXplainable AI (XAI) involves two intertwined but separate challenges: the development of techniques to extract explanations from black-box AI models and the way such explanations are presented to users, i.e., the explanation user interface. Despite its importance, the second aspect has received limited attention so far in the literature. Effective AI explanation interfaces are fundamental for allowing human decision-makers to take advantage and oversee high-risk AI systems effectively. Following an iterative design approach, we present the first cycle of prototyping-testing-redesigning of an explainable AI technique and its explanation user interface for clinical Decision Support Systems (DSS). We first present an XAI technique that meets the technical requirements of the healthcare domain: sequential, ontology-linked patient data, and multi-label classification tasks. We demonstrate its applicability to explain a clinical DSS, and we design a first prototype of an explanation user interface. Next, we test such a prototype with healthcare providers and collect their feedback with a two-fold outcome: First, we obtain evidence that explanations increase users’ trust in the XAI system, and second, we obtain useful insights on the perceived deficiencies of their interaction with the system, so we can re-design a better, more human-centered explanation interface. © 2023 Copyright held by the owner/author(s)",clinical decision support systems; Explainable artificial intelligence; human-computer interaction; user study,Artificial intelligence; Classification (of information); Decision making; Decision support systems; Health care; Hospital data processing; Human computer interaction; Iterative methods; Black boxes; Clinical decision support; Clinical decision support systems; Co-designs; Decision makers; Explainable artificial intelligence; Explanation interfaces; Human decisions; Limited attentions; User study; User interfaces
Enabling Efficient Web Data-record Interaction for People with Visual Impairments via Proxy Interfaces,2023,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163766240&doi=10.1145%2f3579364&partnerID=40&md5=9f14a78cf9fe2b9084cb71c07456020b,"Web data records are usually accompanied by auxiliary webpage segments, such as filters, sort options, search form, and multi-page links, to enhance interaction efficiency and convenience for end users. However, blind and visually impaired (BVI) persons are presently unable to fully exploit the auxiliary segments like their sighted peers, since these segments are scattered all across the screen, and as such assistive technologies used by BVI users, i.e., screen reader and screen magnifier, are not geared for efficient interaction with such scattered content. Specifically, for blind screen reader users, content navigation is predominantly one-dimensional despite the support for skipping content, and therefore navigating to-and-fro between different parts of the webpage is tedious and frustrating. Similarly, low vision screen magnifier users have to continuously pan back-and-forth between different portions of a webpage, given that only a portion of the screen is viewable at any instant due to content enlargement. The extant techniques to overcome inefficient web interaction for BVI users have mostly focused on general web-browsing activities, and as such they provide little to no support for data record-specific interaction activities such as filtering and sorting—activities that are equally important for facilitating quick and easy access to desired data records. To fill this void, we present InSupport, a browser extension that: (i) employs custom machine learning-based algorithms to automatically extract auxiliary segments on any webpage containing data records; and (ii) provides an instantly accessible proxy one-stop interface for easily navigating the extracted auxiliary segments using either basic keyboard shortcuts or mouse actions. Evaluation studies with 14 blind participants and 16 low vision participants showed significant improvement in web usability with InSupport, driven by an increased reduction in interaction time and user effort, compared to the state-of-the-art solutions. © 2023 Association for Computing Machinery. All rights reserved.",Additional Key Words and Phrases: Web accessibility; blind; data records; low vision; screen magnifier; screen reader; visual impairment,Machine learning; Mammals; Additional key word and phrase: web accessibility; Blind; Data record; Key words; Key-phrase; Low vision; Screen magnifier; Screen readers; Visual impairment; Web accessibility; Websites
When Biased Humans Meet Debiased AI: A Case Study in College Major Recommendation,2023,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186226422&doi=10.1145%2f3611313&partnerID=40&md5=fe01a9936a9f451c792ff0c57894df98,"Currently, there is a surge of interest in fair Artificial Intelligence (AI) and Machine Learning (ML) research which aims to mitigate discriminatory bias in AI algorithms, e.g., along lines of gender, age, and race. While most research in this domain focuses on developing fair AI algorithms, in this work, we examine the challenges which arise when humans and fair AI interact. Our results show that due to an apparent conflict between human preferences and fairness, a fair AI algorithm on its own may be insufficient to achieve its intended results in the real world. Using college major recommendation as a case study, we build a fair AI recommender by employing gender debiasing machine learning techniques. Our offline evaluation showed that the debiased recommender makes fairer career recommendations without sacrificing its accuracy in prediction. Nevertheless, an online user study of more than 200 college students revealed that participants on average prefer the original biased system over the debiased system. Specifically, we found that perceived gender disparity is a determining factor for the acceptance of a recommendation. In other words, we cannot fully address the gender bias issue in AI recommendations without addressing the gender bias in humans. We conducted a follow-up survey to gain additional insights into the effectiveness of various design options that can help participants to overcome their own biases. Our results suggest that making fair AI explainable is crucial for increasing its adoption in the real world. © 2023 Association for Computing Machinery. All rights reserved.",Additional Key Words and Phrases: AI; career recommendation; fairness; gender bias; machine learning,Students; Additional key word and phrase: artificial intelligence; Artificial intelligence algorithms; Career recommendation; Case-studies; Fairness; Gender bias; Key words; Key-phrase; Machine-learning; Real-world; Machine learning
The Impact of Intelligent Pedagogical Agents’ Interventions on Student Behavior and Performance in Open-Ended Game Design Environments,2023,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186262214&doi=10.1145%2f3578523&partnerID=40&md5=ce1dbc71e907ca43948a49f91d418a52,"Research has shown that free-form Game-Design (GD) environments can be very effective in fostering Computational Thinking (CT) skills at a young age. However, some students can still need some guidance during the learning process due to the highly open-ended nature of these environments. Intelligent Pedagogical Agents (IPAs) can be used to provide personalized assistance in real-time to alleviate this challenge. This paper presents our results in evaluating such an agent deployed in a real-word free-form GD learning environment to foster CT in the early K-12 education, Unity-CT. We focus on the effect of repetition by comparing student behaviors between no intervention, 1-shot, and repeated intervention groups for two different errors that are known to be challenging in the online lessons of Unity-CT. Our findings showed that the agent was perceived very positively by the students and the repeated intervention showed promising results in terms of helping students make fewer errors and more correct behaviors, albeit only for one of the two target errors. Building from these results, we provide insights on how to provide IPA interventions in free-form GD environments. © 2023 Association for Computing Machinery. All rights reserved.",Additional Key Words and Phrases: Pedagogical agent; computational thinking; game design; open-ended learning environments; real-time support,Computer aided instruction; Errors; Game design; Additional key word and phrase: pedagogical agent; Computational thinkings; Game design; Key words; Key-phrase; Learning environments; Open-ended learning environment; Pedagogical agents; Real- time; Real-time support; Students
Generalisable Dialogue-based Approach for Active Learning of Activities of Daily Living,2023,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186176728&doi=10.1145%2f3616017&partnerID=40&md5=27a9df16b01617ef23b6fad9cbcbc72a,"While Human Activity Recognition systems may benefit from Active Learning by allowing users to self-annotate their Activities of Daily Living (ADLs), many proposed methods for collecting such annotations are for short-term data collection campaigns for specific datasets. We present a reusable dialogue-based approach to user interaction for active learning in activity recognition systems, which utilises semantic similarity measures and a dataset of natural language descriptions of common activities (which we make publicly available). Our approach involves system-initiated dialogue, including follow-up questions to reduce ambiguity in user responses where appropriate. We apply this approach to two active learning scenarios: (i) using an existing CASAS dataset, demonstrating long-term usage; and (ii) using an online activity recognition system, which tackles the issue of online segmentation and labelling. We demonstrate our work in context, in which a natural language interface provides knowledge that can help interpret other multi-modal sensor data. We provide results highlighting the potential of our dialogue- and semantic similarity-based approach. We evaluate our work: (i) quantitatively, as an efficient way to seek users’ input for active learning of ADLs; and (ii) qualitatively, through a user study in which users were asked to compare our approach and an established method. Results show the potential of our approach as a hands-free interface for annotation of sensor data as part of an active learning system. We provide insights into the challenges of active learning for activity recognition under real-world conditions and identify potential ways to address them. © 2023 Association for Computing Machinery. All rights reserved.",Active Learning (AL); Additional Key Words and Phrases: Human-in-the-Loop (HITL) annotation; Human Activity Recognition (HAR) labelling; natural language; semantic similarity,Natural language processing systems; Pattern recognition; Semantics; User interfaces; Active Learning; Additional key word and phrase: human-in-the-loop  annotation; Human activity recognition; Human activity recognition  labeling; Human-in-the-loop; Key words; Key-phrase; Labelings; Natural languages; Semantic similarity; Learning systems
RadarSense: Accurate Recognition of Mid-air Hand Gestures with Radar Sensing and Few Training Examples,2023,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163708724&doi=10.1145%2f3589645&partnerID=40&md5=95c8e3f5ba2b070e3b9a9afc81806865,"Microwave radars bring many benefits to mid-air gesture sensing due to their large field of view and independence from environmental conditions, such as ambient light and occlusion. However, radar signals are highly dimensional and usually require complex deep learning approaches. To understand this landscape, we report results from a systematic literature review of (N = 118) scientific papers on radar sensing, unveiling a large variety of radar technology of different operating frequencies and bandwidths and antenna configurations but also various gesture recognition techniques. Although highly accurate, these techniques require a large amount of training data that depend on the type of radar. Therefore, the training results cannot be easily transferred to other radars. To address this aspect, we introduce a new gesture recognition pipeline that implements advanced full-wave electromagnetic modeling and inversion to retrieve physical characteristics of gestures that are radar independent, i.e., independent of the source, antennas, and radar-hand interactions. Inversion of radar signals further reduces the size of the dataset by several orders of magnitude, while preserving the essential information. This approach is compatible with conventional gesture recognizers, such as those based on template matching, which only need a few training examples to deliver high recognition accuracy rates. To evaluate our gesture recognition pipeline, we conducted user-dependent and user-independent evaluations on a dataset of 16 gesture types collected with the Walabot, a low-cost off-the-shelf array radar. We contrast these results with those obtained for the same gesture types collected with an ultra-wideband radar made of a vector network analyzer with a single horn antenna and with a computer vision sensor, respectively. Based on our findings, we suggest some design implications to support future development in radar-based gesture recognition. © 2023 Association for Computing Machinery. All rights reserved.",Additional Key Words and Phrases: Dimension reduction; hand gesture recognition; radar-based interaction,Computational electromagnetics; Deep learning; Electric network analyzers; Electromagnetic simulation; Horn antennas; Palmprint recognition; Pipelines; Template matching; Ultra-wideband (UWB); Additional key word and phrase: dimension reduction; Dimension reduction; Gestures recognition; Hand-gesture recognition; Key words; Key-phrase; Radar sensing; Radar signals; Radar-based interaction; Training example; Gesture recognition
Learning and Understanding User Interface Semantics from Heterogeneous Networks with Multimodal and Positional Attributes,2023,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173585902&doi=10.1145%2f3578522&partnerID=40&md5=c4ed02ae49603a0650c7cbbbf0c57feb,"User interfaces (UI) of desktop, web, and mobile applications involve a hierarchy of objects (e.g., applications, screens, view class, and other types of design objects) with multimodal (e.g., textual and visual) and positional (e.g., spatial location, sequence order, and hierarchy level) attributes. We can therefore represent a set of application UIs as a heterogeneous network with multimodal and positional attributes. Such a network not only represents how users understand the visual layout of UIs but also influences how users would interact with applications through these UIs. To model the UI semantics well for different UI annotation, search, and evaluation tasks, this article proposes the novel Heterogeneous Attention-based Multimodal Positional (HAMP) graph neural network model. HAMP combines graph neural networks with the scaled dot-product attention used in transformers to learn the embeddings of heterogeneous nodes and associated multimodal and positional attributes in a unified manner. HAMP is evaluated with classification and regression tasks conducted on three distinct real-world datasets. Our experiments demonstrate that HAMP significantly outperforms other state-of-the-art models on such tasks. To further provide interpretations of the contribution of heterogeneous network information for understanding the relationships between the UI structure and prediction tasks, we propose Adaptive HAMP (AHAMP), which adaptively learns the importance of different edges linking different UI objects. Our experiments demonstrate AHAMP’s superior performance over HAMP on a number of tasks, and its ability to provide interpretations of the contribution of multimodal and positional attributes, as well as heterogeneous network information to different tasks. © 2023 Association for Computing Machinery. All rights reserved.",Additional Key Words and Phrases: Graph neural networks; attention mechanism; heterogeneous networks; mobile application user interface; multimodal; supervised learning; transformers,Classification (of information); Graph neural networks; Machine learning; Petroleum reservoir evaluation; Semantic Web; Semantics; User profile; Additional key word and phrase: graph neural network; Attention mechanisms; Graph neural networks; Key words; Key-phrase; Learn+; Mobile application user interface; Mobile applications; Multi-modal; Transformer; Heterogeneous networks
Conversational Context-sensitive Ad Generation with a Few Core-Queries,2023,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184801751&doi=10.1145%2f3588578&partnerID=40&md5=61d9d2b1a9386b7aecd5312730e089c8,"When people are talking together in front of digital signage, advertisements that are aware of the context of the dialogue will work the most effectively. However, it has been challenging for computer systems to retrieve the appropriate advertisement from among the many options presented in large databases. Our proposed system, the Conversational Context-sensitive Advertisement generator (CoCoA), is the first attempt to apply masked word prediction to web information retrieval that takes into account the dialogue context. The novelty of CoCoA is that advertisers simply need to prepare a few abstract phrases, called Core-Queries, and then CoCoA automatically generates a context-sensitive expression as a complete search query by utilizing a masked word prediction technique that adds a word related to the dialogue context to one of the prepared Core-Queries. This automatic generation frees the advertisers from having to come up with context-sensitive phrases to attract users’ attention. Another unique point is that the modified Core-Query offers users speaking in front of the CoCoA system a list of context-sensitive advertisements. CoCoA was evaluated by crowd workers regarding the context-sensitivity of the generated search queries against the dialogue text of multiple domains prepared in advance. The results indicated that CoCoA could present more contextual and practical advertisements than other web-retrieval systems. Moreover, CoCoA acquired a higher evaluation in a particular conversation that included many travel topics to which the Core-Queries were designated, implying that it succeeded in adapting the Core-Queries for the specific ongoing context better than the compared method without any effort on the part of the advertisers. In addition, case studies with users and advertisers revealed that the context-sensitive advertisements generated by CoCoA also had an effect on the content of the ongoing dialogue. Specifically, since pairs unfamiliar with each other more frequently referred to the 5 advertisement CoCoA displayed, the advertisements had an effect on the topics about which the pairs spoke. Moreover, participants of an advertiser role recognized that some of the search queries generated by CoCoA fit the context of a conversation and that CoCoA improved the effect of the advertisement. In particular, they learned how to design of designing a good Core-Query at ease by observing the users’ response to the advertisements retrieved with the generated search queries. © 2023 Association for Computing Machinery. All rights reserved.",,Abstracting; Search engines; Automatic Generation; Complete search; Context-sensitive; Digital signage; Large database; Prediction techniques; Search queries; User attention; Web information retrieval; Word prediction; Information retrieval
Crowdsourcing Thumbnail Captions: Data Collection and Validation,2023,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186193736&doi=10.1145%2f3589346&partnerID=40&md5=49f7c48ad35d339e48a902b7b2d704f5,"Speech interfaces, such as personal assistants and screen readers, read image captions to users. Typically, however, only one caption is available per image, which may not be adequate for all situations (e.g., browsing large quantities of images). Long captions provide a deeper understanding of an image but require more time to listen to, whereas shorter captions may not allow for such thorough comprehension yet have the advantage of being faster to consume. We explore how to effectively collect both thumbnail captions—succinct image descriptions meant to be consumed quickly—and comprehensive captions—which allow individuals to understand visual content in greater detail. We consider text-based instructions and time-constrained methods to collect descriptions at these two levels of detail and find that a time-constrained method is the most effective for collecting thumbnail captions while preserving caption accuracy. Additionally, we verify that caption authors using this time-constrained method are still able to focus on the most important regions of an image by tracking their eye gaze. We evaluate our collected captions along human-rated axes—correctness, fluency, amount of detail, and mentions of important concepts—and discuss the potential for model-based metrics to perform large-scale automatic evaluations in the future. © 2023 Association for Computing Machinery. All rights reserved.",accessibility; Additional Key Words and Phrases: Image captions; annotation interfaces; crowdsourcing; gaze tracking,Eye tracking; Image annotation; Accessibility; Additional key word and phrase: image caption; Annotation interface; Constrained method; Data collection; Data validation; Gaze-tracking; Image caption; Key words; Key-phrase; Crowdsourcing
Visual Analytics of Co-Occurrences to Discover Subspaces in Structured Data,2023,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171791316&doi=10.1145%2f3579031&partnerID=40&md5=a8047837d0c8dad7d0449ce742b66fbc,"We present an approach that shows all relevant subspaces of categorical data condensed in a single picture. We model the categorical values of the attributes as co-occurrences with data partitions generated from structured data using pattern mining. We show that these co-occurrences are a-priori, allowing us to greatly reduce the search space, effectively generating the condensed picture where conventional approaches filter out several subspaces as these are deemed insignificant. The task of identifying interesting subspaces is common but difficult due to exponential search spaces and the curse of dimensionality. One application of such a task might be identifying a cohort of patients defined by attributes such as gender, age, and diabetes type that share a common patient history, which is modeled as event sequences. Filtering the data by these attributes is common but cumbersome and often does not allow a comparison of subspaces. We contribute a powerful multi-dimensional pattern exploration approach (MDPE-approach) agnostic to the structured data type that models multiple attributes and their characteristics as co-occurrences, allowing the user to identify and compare thousands of subspaces of interest in a single picture. In our MDPE-approach, we introduce two methods to dramatically reduce the search space, outputting only the boundaries of the search space in the form of two tables. We implement the MDPE-approach in an interactive visual interface (MDPE-vis) that provides a scalable, pixel-based visualization design allowing the identification, comparison, and sense-making of subspaces in structured data. Our case studies using a gold-standard dataset and external domain experts confirm our approach's and implementation's applicability. A third use case sheds light on the scalability of our approach and a user study with 15 participants underlines its usefulness and power. © 2023 Copyright held by the owner/author(s).",pattern mining; Structured data mining; subspace search,Data visualization; Filtration; Visualization; Categorical data; Co-occurrence; Data partition; Multi dimensional; Pattern mining; Search spaces; Structured data; Structured data mining; Subspace search; Visual analytics; Data mining
Explainable Activity Recognition for Smart Home Systems,2023,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171787865&doi=10.1145%2f3561533&partnerID=40&md5=2c356c86d1cd8c5ae37de83254d05407,"Smart home environments are designed to provide services that help improve the quality of life for the occupant via a variety of sensors and actuators installed throughout the space. Many automated actions taken by a smart home are governed by the output of an underlying activity recognition system. However, activity recognition systems may not be perfectly accurate, and therefore inconsistencies in smart home operations can lead users reliant on smart home predictions to wonder ""Why did the smart home do that?""In this work, we build on insights from Explainable Artificial Intelligence (XAI) techniques and introduce an explainable activity recognition framework in which we leverage leading XAI methods (Local Interpretable Model-agnostic Explanations, SHapley Additive exPlanations (SHAP), Anchors) to generate natural language explanations that explain what about an activity led to the given classification. We evaluate our framework in the context of a commonly targeted smart home scenario: autonomous remote caregiver monitoring for individuals who are living alone or need assistance. Within the context of remote caregiver monitoring, we perform a two-step evaluation: (a) utilize Machine Learning experts to assess the sensibility of explanations and (b) recruit non-experts in two user remote caregiver monitoring scenarios, synchronous and asynchronous, to assess the effectiveness of explanations generated via our framework. Our results show that the XAI approach, SHAP, has a 92% success rate in generating sensible explanations. Moreover, in 83% of sampled scenarios users preferred natural language explanations over a simple activity label, underscoring the need for explainable activity recognition systems. Finally, we show that explanations generated by some XAI methods can lead users to lose confidence in the accuracy of the underlying activity recognition model, while others lead users to gain confidence. Taking all studied factors into consideration, we make a recommendation regarding which existing XAI method leads to the best performance in the domain of smart home automation and discuss a range of topics for future work to further improve explainable activity recognition. © 2023 Copyright held by the owner/author(s).",Explainable AI; human-AI-interaction; smart home activity recognition,Artificial intelligence; Pattern recognition; Activity recognition; Explainable AI; Human-AI-interaction; Lead users; Natural language explanations; Recognition systems; Shapley; Smart home activity recognition; Smart homes; Smart-home system; Automation
GRAFS: Graphical Faceted Search System to Support Conceptual Understanding in Exploratory Search,2023,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171782885&doi=10.1145%2f3588319&partnerID=40&md5=03063f53743eb6d5a9effa665ae26780,"When people search for information about a new topic within large document collections, they implicitly construct a mental model of the unfamiliar information space to represent what they currently know and guide their exploration into the unknown. Building this mental model can be challenging as it requires not only finding relevant documents but also synthesizing important concepts and the relationships that connect those concepts both within and across documents. This article describes a novel interactive approach designed to help users construct a mental model of an unfamiliar information space during exploratory search. We propose a new semantic search system to organize and visualize important concepts and their relations for a set of search results. A user study (n=20) was conducted to compare the proposed approach against a baseline faceted search system on exploratory literature search tasks. Experimental results show that the proposed approach is more effective in helping users recognize relationships between key concepts, leading to a more sophisticated understanding of the search topic while maintaining similar functionality and usability as a faceted search system. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",conceptual learning; Exploratory search; faceted search; knowledge graph,Cognitive systems; Knowledge graph; Safety devices; Search engines; Conceptual learning; Conceptual understanding; Document collection; Exploratory search; Faceted search; Information spaces; Knowledge graphs; Mental model; People searches; Search system; Semantics
Combining the Projective Consciousness Model and Virtual Humans for Immersive Psychological Research: A Proof-of-concept Simulating a ToM Assessment,2023,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171730285&doi=10.1145%2f3583886&partnerID=40&md5=2ba23b9d6844e4fcf5d41523d5416fa3,"Relating explicit psychological mechanisms and observable behaviours is a central aim of psychological and behavioural science. One of the challenges is to understand and model the role of consciousness and, in particular, its subjective perspective as an internal level of representation (including for social cognition) in the governance of behaviour. Toward this aim, we implemented the principles of the Projective Consciousness Model (PCM) into artificial agents embodied as virtual humans, extending a previous implementation of the model. Our goal was to offer a proof-of-concept, based purely on simulations, as a basis for a future methodological framework. Its overarching aim is to be able to assess hidden psychological parameters in human participants, based on a model relevant to consciousness research, in the context of experiments in virtual reality. As an illustration of the approach, we focused on simulating the role of Theory of Mind (ToM) in the choice of strategic behaviours of approach and avoidance to optimise the satisfaction of agents' preferences. We designed a main experiment in a virtual environment that could be used with real humans, allowing us to classify behaviours as a function of order of ToM, up to the second order. We show that agents using the PCM demonstrated expected behaviours with consistent parameters of ToM in this experiment. We also show that the agents could be used to estimate correctly each other's order of ToM. Furthermore, in a supplementary experiment, we demonstrated how the agents could simultaneously estimate order of ToM and preferences attributed to others to optimize behavioural outcomes. Future studies will empirically assess and fine tune the framework with real humans in virtual reality experiments. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",active inference; behavioural science; causal role; Consciousness; emotion; navigation; projective geometry; virtual humans,Active inference; Behavioral science; Causal role; Consciousness; Emotion; Immersive; Projective geometry; Proof of concept; Theory of minds; Virtual humans; Behavioral research
Explaining Recommendations through Conversations: Dialog Model and the Effects of Interface Type and Degree of Interactivity,2023,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85166060560&doi=10.1145%2f3579541&partnerID=40&md5=32aba355101cfc0a7cc8586271742074,"Explaining system-generated recommendations based on user reviews can foster users' understanding and assessment of the recommended items and the recommender system (RS) as a whole. While up to now explanations have mostly been static, shown in a single presentation unit, some interactive explanatory approaches have emerged in explainable artificial intelligence (XAI), making it easier for users to examine system decisions and to explore arguments according to their information needs. However, little is known about how interactive interfaces should be conceptualized and designed to meet the explanatory aims of transparency, effectiveness, and trust in RS. Thus, we investigate the potential of interactive, conversational explanations in review-based RS and propose an explanation approach inspired by dialog models and formal argument structures. In particular, we investigate users' perception of two different interface types for presenting explanations, a graphical user interface (GUI)-based dialog consisting of a sequence of explanatory steps, and a chatbot-like natural-language interface.Since providing explanations by means of natural language conversation is a novel approach, there is a lack of understanding how users would formulate their questions with a corresponding lack of datasets. We thus propose an intent model for explanatory queries and describe the development of ConvEx-DS, a dataset containing intent annotations of 1,806 user questions in the domain of hotels, that can be used to to train intent detection methods as part of the development of conversational agents for explainable RS. We validate the model by measuring user-perceived helpfulness of answers given based on the implemented intent detection. Finally, we report on a user study investigating users' evaluation of the two types of interactive explanations proposed (GUI and chatbot), and to test the effect of varying degrees of interactivity that result in greater or lesser access to explanatory information. By using Structural Equation Modeling, we reveal details on the relationships between the perceived quality of an explanation and the explanatory objectives of transparency, trust, and effectiveness. Our results show that providing interactive options for scrutinizing explanatory arguments has a significant positive influence on the evaluation by users (compared to low interactive alternatives). Results also suggest that user characteristics such as decision-making style may have a significant influence on the evaluation of different types of interactive explanation interfaces. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",argumentation; conversational agent; dataset; explanations; intent detection; interactive interfaces; Recommender systems; user study,Decision making; Graphical user interfaces; Modeling languages; Natural language processing systems; Transparency; Argumentation; Chatbots; Conversational agents; Dataset; Dialogue models; Explanation; Intent detection; Interactive interfaces; Interactivity; User study; Recommender systems
Collaborative Robots and Tangled Passages of Tactile-Affects,2023,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164238333&doi=10.1145%2f3534090&partnerID=40&md5=0c4227b6bcdd83e739255633277f60a8,"Collaborative robots are increasingly entering industrial contexts and workflows. These contexts are not just locations for production, they are vibrant social and sensory environments. For better or for worse, their entry brings potential to reorganize established tactile and affective dynamics that encompass production processes. There is still much to be learned about these highly contextual and complex dynamics in HRI research and the design of industrial robotics; common approaches in industrial collaborative robotics are restricted to evaluating ""effective interface design,""whereas methods that seek to measure ""affective touch""have limited application to these industrial domains. This article offers an extended analytical framework and methodological approach to deepen understandings of affect and touch beyond emotional responses to direct human-robot interactions. These distinct contributions are grounded in fieldwork in a glass factory with newly installed collaborative robots. They are illustrated through an ethnographic narrative that traces the emergence and circulation of affect, across material, experiential, and social planes. Beyond this single case, ""tangled passages of tactile-affects""is offered as a novel and valuable concept that is distinct from the notion of ""affective touch""and holds potential to generate holistic and nuanced understandings of how human experiences can be effected by the introduction of new robots in ""the wild."" © 2023 Association for Computing Machinery.",Additional Key Words and PhrasesTactile-affects; collaborative robots; ethnographic narrative; ethnography; industrial robotics,Industrial research; Additional key word and phrasestactile-affect; Affective dynamics; Collaborative robots; Ethnographic narrative; Ethnography; Industrial context; Industrial robotics; Key words; Production process; Work-flows; Collaborative robots
Synthesizing Game Levels for Collaborative Gameplay in a Shared Virtual Environment,2023,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151817978&doi=10.1145%2f3558773&partnerID=40&md5=608284f3109e7966656917f89310fc62,"We developed a method to synthesize game levels that accounts for the degree of collaboration required by two players to finish a given game level. We first asked a game level designer to create playable game level chunks. Then, two artificial intelligence (AI) virtual agents driven by behavior trees played each game level chunk. We recorded the degree of collaboration required to accomplish each game level chunk by the AI virtual agents and used it to characterize each game level chunk. To synthesize a game level, we assigned to the total cost function cost terms that encode both the degree of collaboration and game level design decisions. Then, we used a Markov-chain Monte Carlo optimization method, called simulated annealing, to solve the total cost function and proposed a design for a game level. We synthesized three game levels (low, medium, and high degrees of collaboration game levels) to evaluate our implementation. We then recruited groups of participants to play the game levels to explore whether they would experience a certain degree of collaboration and validate whether the AI virtual agents provided sufficient data that described the collaborative behavior of players in each game level chunk. By collecting both in-game objective measurements and self-reported subjective ratings, we found that the three game levels indeed impacted the collaboration gameplay behavior of our participants. Moreover, by analyzing our collected data, we found moderate and strong correlations between the participants and the AI virtual agents. These results show that game developers can consider AI virtual agents as an alternative method for evaluating the degree of collaboration required to finish a game level.  © 2023 held by the owner/author(s).",AI agents; behavior trees; chunks; collaboration; Game level; optimization,Cost functions; Function evaluation; Interactive computer graphics; Markov processes; Monte Carlo methods; Virtual reality; Artificial intelligence agent; Behaviour Trees; Chunk; Collaboration; Collaborative gameplay; Degree of collaboration; Game level; Optimisations; Total cost function; Virtual agent; Simulated annealing
EDAssistant: Supporting Exploratory Data Analysis in Computational Notebooks with In Situ Code Search and Recommendation,2023,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151778931&doi=10.1145%2f3545995&partnerID=40&md5=3f669481974fc8a5fe644d09d7dbfc6a,"Using computational notebooks (e.g., Jupyter Notebook), data scientists rationalize their exploratory data analysis (EDA) based on their prior experience and external knowledge, such as online examples. For novices or data scientists who lack specific knowledge about the dataset or problem to investigate, effectively obtaining and understanding the external information is critical to carrying out EDA. This article presents EDAssistant, a JupyterLab extension that supports EDA with in situ search of example notebooks and recommendation of useful APIs, powered by novel interactive visualization of search results. The code search and recommendation are enabled by advanced machine learning models, trained on a large corpus of EDA notebooks collected online. A user study is conducted to investigate both EDAssistant and data scientists' current practice (i.e., using external search engines). The results demonstrate the effectiveness and usefulness of EDAssistant, and participants appreciated its smooth and in-context support of EDA. We also report several design implications regarding code recommendation tools.  © 2023 Association for Computing Machinery.",code search; computational notebooks; Exploratory data analysis; software visualization,Codes (symbols); Data handling; Data visualization; Information analysis; Visualization; Code search; Computational notebook; Exploratory data analysis; External informations; External knowledge; Interactive visualizations; Is-enabled; Prior experience; Software visualization; Specific knowledge; Search engines
Visualization and Visual Analytics Approaches for Image and Video Datasets: A Survey,2023,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151795355&doi=10.1145%2f3576935&partnerID=40&md5=d197fb0721d056569f7d0f7d098677d7,"Image and video data analysis has become an increasingly important research area with applications in different domains such as security surveillance, healthcare, augmented and virtual reality, video and image editing, activity analysis and recognition, synthetic content generation, distance education, telepresence, remote sensing, sports analytics, art, non-photorealistic rendering, search engines, and social media. Recent advances in Artificial Intelligence (AI) and particularly deep learning have sparked new research challenges and led to significant advancements, especially in image and video analysis. These advancements have also resulted in significant research and development in other areas such as visualization and visual analytics, and have created new opportunities for future lines of research. In this survey article, we present the current state of the art at the intersection of visualization and visual analytics, and image and video data analysis. We categorize the visualization articles included in our survey based on different taxonomies used in visualization and visual analytics research. We review these articles in terms of task requirements, tools, datasets, and application areas. We also discuss insights based on our survey results, trends and patterns, the current focus of visualization research, and opportunities for future research.  © 2023 Copyright held by the owner/author(s).",computer vision; image and video datasets; Survey; visual analytics,Computer vision; Data handling; Data visualization; Deep learning; Distance education; Image analysis; Information analysis; Remote sensing; Search engines; Security systems; Video recording; Virtual reality; Visual communication; 'current; Analytic approach; Different domains; Image data analysis; Image datasets; Research areas; Security surveillance; Video data analysis; Video dataset; Visual analytics; Visualization
A Personalized Interaction Mechanism Framework for Micro-moment Recommender Systems,2023,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151877108&doi=10.1145%2f3569586&partnerID=40&md5=eb0b93c56108c67e060b673c25ed9115,"The emergence of the micro-moment concept highlights the influence of context; recommender system design should reflect this trend. In response to different contexts, a micro-moment recommender system (MMRS) requires an effective interaction mechanism that allows users to easily interact with the system in a way that supports autonomy and promotes the creation and expression of self. We study four types of interaction mechanisms to understand which personalization approach is the most suitable design for MMRSs. We assume that designs that support micro-moment needs well are those that give users more control over the system and constitute a lighter user burden. We test our hypothesis via a two-week between-subject field study in which participants used our system and provided feedback. User-initiated and mix-initiated intention mechanisms show higher perceived active control, and the additional controls do not add to user burdens. Therefore, these two designs suit the MMRS interaction mechanism.  © 2023 Association for Computing Machinery.",interactive mechanism; Micro-moment recommender system; motivational affordance; personalization,User interfaces; Active control; Additional control; Affordances; Effective interactions; Field studies; Interaction mechanisms; Interactive mechanism; Micro-moment recommende system; Motivational affordance; Personalizations; Recommender systems
The Influence of Personality Traits on User Interaction with Recommendation Interfaces,2023,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151887786&doi=10.1145%2f3558772&partnerID=40&md5=6caa5f205dea827a0d780dbba50d2157,"Users' personality traits can take an active role in affecting their behavior when they interact with a computer interface. However, in the area of recommender systems (RS), though personality-based RS has been extensively studied, most works focus on algorithm design, with little attention paid to studying whether and how the personality may influence users' interaction with the recommendation interface. In this manuscript, we report the results of a user study (with 108 participants) that not only measured the influence of users' personality traits on their perception and performance when using the recommendation interface but also employed an eye-tracker to in-depth reveal how personality may influence users' eye-movement behavior. Moreover, being different from related work that has mainly been conducted in a single product domain, our user study was performed in three typical application domains (i.e., electronics like smartphones, entertainment like movies, and tourism like hotels). Our results show that mainly three personality traits, i.e., Openness to experience, Conscientiousness, and Agreeableness, significantly influence users' perception and eye-movement behavior, but the exact influences vary across the domains. Finally, we provide a set of guidelines that might be constructive for designing a more effective recommendation interface based on user personality.  © 2023 Association for Computing Machinery.",eye-tracking experiment; Recommendation interface; user personality,Eye movements; Algorithm design; Eye-tracking; Eye-tracking experiment; Movement behaviour; Performance; Personality traits; Recommendation interface; User interaction; User personalities; User study; Eye tracking
Special Issue on Highlights of IUI 2021: Introduction,2023,ACM Transactions on Interactive Intelligent Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85148000515&doi=10.1145%2f3561516&partnerID=40&md5=2f4677ae75ce196213cc2e69dcefd547,[No abstract available],,
