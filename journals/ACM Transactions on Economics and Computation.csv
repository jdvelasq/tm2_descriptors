Title,Year,Source title,Link,Abstract,Author Keywords,Index Keywords
Price of stability in polynomial congestion games,2015,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025834020&doi=10.1145%2f2841229&partnerID=40&md5=0fadad085910454a493e3e95359b45fe,"The price of anarchy (PoA) in congestion games has attracted a lot of research over the past decade. This has resulted in a thorough understanding of this concept. In contrast, the price of stability (PoS), which is an equally interesting concept, is much less understood. In this article, we consider congestion games with polynomial cost functions with nonnegative coefficients andmaximum degree d.We givematching bounds for the PoS in such games-that is, our technique provides the exact value for any degree d. For linear congestion games, tight bounds were previously known. Those bounds hold even for the more restricted case of dominant equilibria, which may not exist. We give a separation result showing that this is not possible for congestion games with quadratic cost functions-in other words, the PoA for the subclass of games that admit a dominant strategy equilibrium is strictly smaller than the PoS for the general class. © 2015 ACM.",Congestion games; Price of anarchy; Price of stability,Costs; Congestion Games; Dominant strategy; General class; Linear congestion games; Nonnegative coefficients; Price of anarchy; Price of Stability; Quadratic cost functions; Cost functions
On multiple keyword sponsored search auctions with budgets,2015,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045373123&doi=10.1145%2f2818357&partnerID=40&md5=6b1c713017e09b343c7b75a3ead7ef74,"We study multiple keyword sponsored search auctions with budgets. Each keyword has multiple ad slots with a click-through rate. The bidders have additive valuations, which are linear in the click-through rates, and budgets, which are restricting their overall payments. Additionally, the number of slots per keyword assigned to a bidder is bounded. We show the following results: (1) We give the first mechanism for multiple keywords, where click-through rates differ among slots. Our mechanism is incentive compatible in expectation, individually rational in expectation, and Pareto optimal. (2) We study the combinatorial setting, where each bidder is only interested in a subset of the keywords. We give an incentive compatible, individually rational, Pareto-optimal, and deterministic mechanism for identical click-through rates. (3) We give an impossibility result for incentive compatible, individually rational, Pareto-optimal, and deterministic mechanisms for bidders with diminishing marginal valuations. © 2015 ACM.",Auctions with budgets; Clinching ascending auction; Sponsored search auctions,Commerce; Pareto principle; Auctions with budgets; Click-through rate; Clinching ascending auction; Deterministic mechanism; Impossibility results; Incentive compatible; Pareto-optimal; Sponsored search auctions; Budget control
How to put through your agenda in collective binary decisions,2015,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045362511&doi=10.1145%2f2837467&partnerID=40&md5=9553c56508230f71dc2170b668c49bda,"We consider the following decision-making scenario: a society of voters has to find an agreement on a set of proposals, and every single proposal is to be accepted or rejected. Each voter supports a certain subset of the proposals-the favorite ballot of this voter-and opposes the remaining ones. He accepts a ballot if he supports more than half of the proposals in this ballot. The task is to decide whether there exists a ballot approving a specified number of selected proposals (agenda) such that all voters (or a strict majority of them) accept this ballot. We show that, on the negative side, both problems are NP-complete, and on the positive side, they are fixed-parameter tractable with respect to the total number of proposals or with respect to the total number of voters. We look into further natural parameters and study their influence on the computational complexity of both problems, thereby providing both tractability and intractability results. Furthermore, we provide tight combinatorial bounds on the worst-case size of an accepted ballot in terms of the number of voters. © 2015 ACM.",Approval balloting with majority threshold; Collective binary decision making; Control by proposal bundling; Voting on multiple issues,Bins; Approval balloting; Binary decision; Binary decision making; Combinatorial bounds; Multiple issue; Negative sides; NP Complete; Positive sides; Decision making
An expressive mechanism for auctions on the Web,2015,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029353197&doi=10.1145%2f2716312&partnerID=40&md5=516a7d06f99630866fbb7223168a6677,"Auctions are widely used on the Web. Applications range from sponsored search to platforms such as eBay. In these and in many other applications the auctions in use are single-/multi-item auctions with unit demand. The main drawback of standard mechanisms for this type of auctions, such as VCG and GSP, is the limited expressiveness that they offer to the bidders. The General Auction Mechanism (GAM) of Aggarwal et al. [2009] takes a first step toward addressing the problem of limited expressiveness by computing a bidder optimal, envy-free outcome for linear utility functions with identical slopes and a single discontinuity per bidder-item pair. We show that in many practical situations this does not suffice to adequately model the preferences of the bidders, and we overcome this problem by presenting the first mechanism for piecewise linear utility functions with nonidentical slopes and multiple discontinuities. Our mechanism runs in polynomial time. Like GAM it is incentive compatible for inputs that fulfill a certain nondegeneracy assumption, but our requirement is more general than the requirement of GAM. For discontinuous utility functions that are nondegenerate as well as for continuous utility functions the outcome of our mechanism is a competitive equilibrium. We also show how our mechanism can be used to compute approximately bidder optimal, envyfree outcomes for a general class of continuous utility functions via piecewise linear approximation. Finally, we prove hardness results for even more expressive settings. © 2015 ACM.",Bidder optimality; Competitive equilibrium; Envy freeness; Expressiveness; General Auction Mechanism; GSP; VCG,Piecewise linear techniques; Polynomial approximation; Auction mechanisms; Competitive equilibrium; Envy freeness; Expressiveness; GSP; Optimality; VCG; Commerce
Auctions for heterogeneous items and budget limits,2015,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045365171&doi=10.1145%2f2818351&partnerID=40&md5=ee2fecf788acabe395010ed8e6fee73d,"We study individual rational, Pareto-optimal, and incentive compatible mechanisms for auctions with heterogeneous items and budget limits. We consider settings with multiunit demand and additive valuations. For single-dimensional valuations we prove a positive result for randomized mechanisms, and a negative result for deterministic mechanisms. While the positive result allows for private budgets, the negative result is for public budgets. For multidimensional valuations and public budgets we prove an impossibility result that applies to deterministic and randomized mechanisms. Taken together this shows the power of randomization in certain settings with heterogeneous items, but it also shows its limitations. © 2015 ACM.",Algorithmic game theory; Auction theory; Budget limits; Clinching auction; Pareto optimality,Commerce; Game theory; Pareto principle; Algorithmic Game Theory; Auction theory; Budget limits; Clinching auction; Pareto-optimality; Budget control
Auctioning time: Truthful auctions of heterogeneous divisible goods,2015,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031933903&doi=10.1145%2f2833086&partnerID=40&md5=105a6c6b36da9fd6a1c41e43420df1d9,"We consider the problem of auctioning time - a one-dimensional continuously-divisible heterogeneous good - among multiple agents. Applications include auctioning time for using a shared device, auctioning TV commercial slots, and more. Different agents may have different valuations for the different possible intervals; the goal is to maximize the aggregate utility. Agents are self-interested and may misrepresent their true valuation functions if this benefits them. Thus, we seek auctions that are truthful. Considering the case that each agent may obtain a single interval, the challenge is twofold, as we need to determine both where to slice the interval, and who gets what slice. We consider two settings: discrete and continuous. In the discrete setting, we are given a sequence of m indivisible elements (e1, ⋯, em), and the auction must allocate each agent a consecutive subsequence of the elements. In the continuous setting, we are given a continuous, infinitely divisible interval, and the auction must allocate each agent a subinterval. The agents' valuations are nonatomic measures on the interval. We show that, for both settings, the associated computational problem is NP-complete even under very restrictive assumptions. Hence, we provide approximation algorithms. For the discrete case, we provide a truthful auctioning mechanism that approximates the optimal welfare to within a logmfactor. The mechanism works for arbitrary monotone valuations. For the continuous setting, we provide a truthful auctioning mechanism that approximates the optimal welfare to within an O(log n) factor (where n is the number of agents). Additionally, we provide a truthful 2-approximation mechanism for the case that all pieces must be of some fixed size. © 2015 ACM.",Auctions; Cake cutting; Resource allocation,Approximation algorithms; Commerce; Multi agent systems; One dimensional; Resource allocation; Auctions; Cake cuttings; Computational problem; Discrete settings; Infinitely divisible; Multiple agents; Truthful auctions; Valuation function; Optimization
Strategyproof matching with minimum quotas,2015,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84944693434&doi=10.1145%2f2841226&partnerID=40&md5=1941180252d6e1602dad8a608a6208ea,"We study matching markets in which institutions may have minimum and maximum quotas. Minimum quotas are important in many settings, such as hospital residency matching, military cadet matching, and school choice, but current mechanisms are unable to accommodate them, leading to the use of ad hoc solutions. We introduce two new classes of strategyproof mechanisms that allow for minimum quotas as an explicit input and show that our mechanisms improve welfare relative to existing approaches. Because minimum quotas cause a theoretical incompatibility between standard fairness and nonwastefulness properties, we introduce new second-best axioms and show that they are satisfied by our mechanisms. Last, we use simulations to quantify (1) the magnitude of the potential efficiency gains from our mechanisms and (2) how far the resulting assignments are from the first-best definitions of fairness and nonwastefulness. Combining both the theoretical and simulation results, we argue that our mechanisms will improve the performance of matching markets with minimum quota constraints in practice. Copyright © 2015 ACM.",Deferred acceptance; Envy; Fairness; Lower bounds; Minimum quotas; School choice; Strategyproofness,Deferred acceptance; Envy; Fairness; Lower bounds; Minimum quotas; School choice; Strategy-proofness; Commerce
Introduction to the special issue on WINE'13,2015,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045364768&doi=10.1145%2f2796538&partnerID=40&md5=a3d419566dff83f17b4b9e2f9dd17144,[No abstract available],,
Subsidized prediction mechanisms for risk-averse agents,2015,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045354633&doi=10.1145%2f2716327&partnerID=40&md5=715df0a11ba4f998bf09065e8dc46100,"In this article, we study the design and characterization of sequential prediction mechanisms in the presence of agents with unknown risk aversion. We formulate a collection of desirable properties for any sequential forecasting mechanism. We present a randomized mechanism that satisfies all of these properties, including a guarantee that it is myopically optimal for each agent to report honestly, regardless of her degree of risk aversion. We observe, however, that the mechanism has an undesirable side effect: each agent's expected reward, normalized against the inherent value of her private information, decreases exponentially with the number of agents. We prove a negative result showing that this is unavoidable: any mechanism that is myopically strategyproof for agents of all risk types, while also satisfying other natural properties of sequential forecasting mechanisms, must sometimes result in a player getting an exponentially small expected normalized reward.",Analytical modeling; Electronic financial markets; Electronic markets and auctions,Analytical models; Commerce; Degree of risks; Electronic markets and auctions; Natural properties; Prediction mechanisms; Private information; Randomized mechanism; Sequential prediction; Strategy proofs; Forecasting
Truthfulness flooded domains and the power of verification for mechanism design,2015,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014286495&doi=10.1145%2f2790086&partnerID=40&md5=5109b6af6e369787ba246211684b8ac4,"In this work, we investigate the reasons that make symmetric partial verification essentially useless in virtually all domains. Departing from previous work, we consider any possible (finite or infinite) domain and general symmetric verification. We identify a natural property, namely, that the correspondence graph of a symmetric verification M is strongly connected by finite paths along which the preferences are consistent with the preferences at the endpoints, and prove that this property is sufficient for the equivalence of truthfulness and M-truthfulness. In fact, defining appropriate versions of this property, we obtain this result for deterministic and randomized mechanisms with and without money. Moreover, we show that a slightly relaxed version of this property is also necessary for the equivalence of truthfulness and M-truthfulness. Our conditions provide a generic and convenient way of checking whether truthful implementation can take advantage of any symmetric verification scheme in any (finite or infinite) domain. Since the simplest symmetric verification is the local verification, specific cases of our result are closely related, in the case without money, to the research about the equivalence of local truthfulness and global truthfulness. To complete the picture, we consider asymmetric verification and prove that a social choice function is M-truthfully implementable by some asymmetric verification M if and only if f does not admit a cycle of profitable deviations.",Algorithmic mechanism design; Social choice; Verification,Verification; Algorithmic mechanism design; Mechanism design; Natural properties; Partial verification; Randomized mechanism; Social choice; Social choice functions; Strongly connected; Machine design
Query complexity of correlated equilibrium,2015,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009422347&doi=10.1145%2f2785668&partnerID=40&md5=64c9b87b0cd4792f7fa9e8330463879d,"We study lower bounds on the query complexity of determining correlated equilibrium. In particular, we consider a query model in which an n-player game is specified via a black box that returns players' utilities at pure action profiles. In this model, we establish that in order to compute a correlated equilibrium, any deterministic algorithm must query the black box an exponential (in n) number of times. © 2015 ACM 2167-8375/2015/07-ART22 $15.00.",Computation of equilibrium; Correlated equilibrium; Query complexity,Algorithms; Black boxes; Correlated equilibria; Deterministic algorithms; Lower bounds; N-player games; Query complexity; Query model; Computation theory
Restoring pure equilibria to weighted congestion games,2015,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85037027043&doi=10.1145%2f2781678&partnerID=40&md5=2f53e1bee563a8019bfb9fb5c286ec4a,"Congestion games model several interesting applications, including routing and network formation games, and also possess attractive theoretical properties, including the existence of and convergence of natural dynamics to a pure Nash equilibrium. Weighted variants of congestion games that rely on sharing costs proportional to players' weights do not generally have pure-strategy Nash equilibria. We propose a new way of assigning costs to players with weights in congestion games that recovers the important properties of the unweighted model. This method is derived from the Shapley value, and it always induces a game with a (weighted) potential function. For the special cases of weighted network cost-sharing and weighted routing games with Shapley value-based cost shares, we prove tight bounds on the worst-case inefficiency of equilibria. For weighted network cost-sharing games, we precisely calculate the price of stability for any given player weight vector, while for weighted routing games, we precisely calculate the price of anarchy, as a parameter of the set of allowable cost functions. © 2015 ACM.",Price of anarchy; Price of stability; Weighted congestion games; Weighted shapley value,Cost effectiveness; Cost functions; Game theory; Network routing; Telecommunication networks; Congestion Games; Network formation games; Potential function; Price of anarchy; Price of Stability; Pure Nash equilibrium; Weighted networks; Weighted shapley value; Costs
Revenue maximization with nonexcludable goods,2015,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045361531&doi=10.1145%2f2790131&partnerID=40&md5=f742e4dfb6d8767f94cb24e680efdb28,"We study the design of revenue-maximizing mechanisms for selling nonexcludable public goods. In particular, we study revenue-maximizing mechanisms in Bayesian settings for facility location problems on graphs where no agent can be excluded from using a facility that has been constructed. We show that the pointwise optimization problem involved in implementing the revenue optimal mechanism, namely, optimizing over arbitrary profiles of virtual values, is hard to approximate within a factor of Ω(n2-∈) (assuming P ≠ NP) even in star graphs. Furthermore, we show that optimizing the expected revenue is APX-hard. However, in a relevant special case, rooted version with identical distributions, we construct polynomial time truthful mechanisms that approximate the optimal expected revenue within a constant factor. We also study the effect of partially mitigating nonexcludability by collecting tolls for using the facilities. We show that such ""posted-price"" mechanisms obtain significantly higher revenue and often approach the optimal revenue obtainable with full excludability.",Nonexcludability; Pricing; Revenue maximization,Costs; Optimization; Polynomial approximation; Arbitrary profile; Facility location problem; Nonexcludability; Optimal mechanism; Optimization problems; Revenue maximization; Revenue maximizing; Truthful mechanisms; Economics
The efficiency of fair division with connected pieces,2015,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84955368347&doi=10.1145%2f2781776&partnerID=40&md5=ad541b3015771b0820c5ffc09d32f9e4,"The cake-cutting setting, in which a single heterogeneous good must be divided between multiple parties with different tastes, is a classic model for studying questions regarding fairness in resource allocation. In this work, we turn our attention to (economic) efficiency considerations in cake cutting, examining the possible trade-offs between meeting the fairness criteria, on the one hand, and maximizing social welfare, on the other. We focus on divisions that give each agent a single (contiguous) piece of the cake and provide tight bounds (or, in some cases, nearly tight) on the possible degradation in utilitarian and egalitarian welfare resulting from meeting the fairness requirements.",Cake cutting; Price of fairness; Social efficiency,Economic and social effects; Cake cuttings; Classic models; Fair divisions; Fairness criteria; Price of fairness; Social efficiency; Social welfare; Tight bound; Efficiency
Socially-optimal design of service exchange platforms with imperfect monitoring,2015,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014991794&doi=10.1145%2f2785627&partnerID=40&md5=5d729c67a8a6a5a60e5d10c884268059,"We study the design of service exchange platforms in which long-lived anonymous users exchange services with each other. The users are randomly and repeatedly matched into pairs of clients and servers, and each server can choose to provide high-quality or low-quality services to the client with whom it is matched. Since the users are anonymous and incur high costs (e.g., exert high effort) in providing high-quality services, it is crucial that the platform incentivizes users to provide high-quality services. Rating mechanisms have been shown to work effectively as incentive schemes in such platforms. A rating mechanism labels each user by a rating, which summarizes the user's past behaviors, recommends a desirable behavior to each server (e.g., provide higher-quality services for clients with higher ratings), and updates each server's rating based on the recommendation and its client's report on the service quality. Based on this recommendation, a low-rating user is less likely to obtain high-quality services, thereby providing users with incentives to obtain high ratings by providing high-quality services. However, if monitoring or reporting is imperfect-clients do not perfectly assess the quality or the reports are lost-a user's rating may not be updated correctly. In the presence of such errors, existing rating mechanisms cannot achieve the social optimum. In this article, we propose the first rating mechanism that does achieve the social optimum, even in the presence of monitoring or reporting errors. On one hand, the socially-optimal rating mechanism needs to be complicated enough, because the optimal recommended behavior depends not only on the current rating distribution, but also (necessarily) on the history of past rating distributions in the platform. On the other hand, we prove that the social optimum can be achieved by ""simple"" rating mechanisms that use binary rating labels and a small set of (three) recommended behaviors. We provide design guidelines of socially-optimal rating mechanisms and a low-complexity online algorithm for the rating mechanism to determine the optimal recommended behavior. © 2015 ACM 2167-8375/2015/07-ART25 $15.00.",Imperfect monitoring; Incentive schemes; Rating mechanism,Behavioral research; Computational complexity; High quality service; Imperfect monitoring; Incentive schemes; On-line algorithms; Optimal ratings; Quality services; Service Quality; Social optimums; Rating
Affine maximizers in domains with selfish valuations,2015,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044445528&doi=10.1145%2f2786014&partnerID=40&md5=0c91330be92c055849113ce62e6fa6c1,"We consider the domain of selfish and continuous preferences over a ""rich"" allocation space and show that onto, strategyproof and allocation non-bossy social choice functions are affine maximizers. Roberts [1979] proves this result for a finite set of alternatives and an unrestricted valuation space. In this article, we show that in a subdomain of the unrestricted valuations with the additional assumption of allocation non-bossiness, using the richness of the allocations, the strategyproof social choice functions can be shown to be affine maximizers. We provide an example to show that allocation non-bossiness is indeed critical for this result. This work shows that an affine maximizer result needs a certain amount of richness split across valuations and allocations. © 2015 ACM 2167-8375/2015/07-ART26 $15.00.",Affine maximizer; Allocation non-bossiness; Characterization; Selfish valuations; Social choice function; Strategyproofness,Affine maximizer; Allocation non-bossiness; Selfish valuations; Social choice functions; Strategy-proofness; Characterization
Near-optimal and robust mechanism design for covering problems with correlated players,2015,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045359939&doi=10.1145%2f2790133&partnerID=40&md5=13e0ab7eb69d382b3640742c15d03cca,"We consider the problem of designing incentive-compatible, ex-post individually rational (IR) mechanisms for covering problems in the Bayesian setting, where players' types are drawn from an underlying distribution and may be correlated, and the goal is to minimize the expected total payment made by the mechanism. We formulate a notion of incentive compatibility (IC) that we call support-based IC that is substantially more robust than Bayesian IC, and develop black-box reductions from support-based-IC mechanism design to algorithm design. For single-dimensional settings, this black-box reduction applies even when we only have an LP-relative approximation algorithm for the algorithmic problem. Thus, we obtain near-optimal mechanisms for various covering settings, including single-dimensional covering problems, multi-item procurement auctions, and multidimensional facility location.",Algorithmic mechanism design; Black-box reductions; Convex decomposition; Covering mechanism-design problems; Optimal mechanisms; Payment-minimization problems,Algorithms; Approximation algorithms; Machine design; Algorithmic mechanism design; Black-box reductions; Convex decomposition; Mechanism design; Minimization problems; Optimal mechanism; Design
An equilibrium analysis of scrip systems,2015,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034434719&doi=10.1145%2f2659006&partnerID=40&md5=d8c63c6b81345ba3191932e8ccbe139d,"A game-theoretic model of scrip (artificial currency) systems is analyzed. It is shown that relative entropy can be used to characterize the distribution of agent wealth when all agents use threshold strategies-that is, they volunteer to do work if and only if they have below a threshold amount of money. Monotonicity of agents' best-reply functions is used to show that scrip systems have pure strategy equilibria where all agents use threshold strategies. An algorithm is given that can compute such an equilibrium and the resulting distribution of wealth.",Artificial currency; Game theory; P2P networks; Scrip systems,Game theory; Artificial currency; Equilibrium analysis; Game-theoretic model; Monotonicity; P2P network; Relative entropy; Threshold strategy; Peer to peer networks
Bounded-distance network creation games,2015,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84987949760&doi=10.1145%2f2770639&partnerID=40&md5=11f36eb296ba75eb135be277eea2e0c3,"A network creation game simulates a decentralized and noncooperative construction of a communication network. Informally, there are n players sitting on the network nodes, which attempt to establish a reciprocal communication by activating, thereby incurring a certain cost, any of their incident links. The goal of each player is to have all the other nodes as close as possible in the resulting network, while buying as few links as possible. According to this intuition, any model of the game must then appropriately address a balance between these two conflicting objectives. Motivated by the fact that a player might have a strong requirement about her centrality in the network, we introduce a new setting in which a player whomaintains her (maximum or average) distance to the other nodes within a given bound incurs a cost equal to the number of activated edges; otherwise her cost is unbounded. We study the problem of understanding the structure of pure Nash equilibria of the resulting games, which we call MAXBD and SUMBD, respectively. For both games, we show that when distance bounds associated with players are nonuniform, then equilibria can be arbitrarily bad. On the other hand, for MAXBD, we show that when nodes have a uniform bound D≥ 3 on the maximum distance, then the price of anarchy (PoA) is lower and upper bounded by 2 and O(n 1/⌊log3 D⌋+1), respectively (i.e., PoA is constant as soon as D is Ω(nε), for any ε > 0), while for the interesting case D=2, we are able to prove that the PoA is Ω(√n) and O(√ nlog n). For the uniform SUMBD, we obtain similar (asymptotically) results and moreover show that PoA becomes constant as soon as the bound on the average distance is 2ω(√ log n).",Degree-diameter problem; Game theory; Network creation games; Price of anarchy; Pure Nash equilibrium,Artificial intelligence; Computer networks; Congestion control (communication); Costs; Telecommunication networks; Average Distance; Conflicting objectives; Degree-diameter problem; Maximum distance; Network creation; Price of anarchy; Pure Nash equilibrium; Reciprocal communications; Game theory
Incentives in large random two-sided markets,2015,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045362573&doi=10.1145%2f2656202&partnerID=40&md5=3a3c04eea3c8e7796300a7e5dedddaa2,"Many centralized two-sided markets form a matching between participants by running a stable matching algorithm. It is a well-known fact that no matching mechanism based on a stable matching algorithm can guarantee truthfulness as a dominant strategy for participants. However, we show that in a probabilistic setting where the preference lists on one side of the market are composed of only a constant (independent of the size of the market) number of entries, each drawn from an arbitrary distribution, the number ofparticipants that have more than one stable partner is vanishingly small. This proves (and generalizes) aconjecture of Roth and Peranson [1999]. As a corollary of this result, we show that, with high probability, the truthful strategy is the best response for a random player when the other players are truthful. We alsoanalyze equilibria of the deferred acceptance stable matching game. We show that the game with complete information has an equilibrium in which, in expectation, a (1-o(1)) fraction of the strategies are truthful. In the more realistic setting of a game of incomplete information, we will show that the set of truthful stratiegs form a (1 + o(1))-approximate Bayesian-Nash equilibrium for uniformly random preferences. Our results have implications in many practical settings and are inspired by the work of Roth and Peranson [1999] on the National Residency Matching Program.",Incentives; Large markets; Stable matching,Algorithms; Probability distributions; Approximate Bayesian; Arbitrary distribution; Complete information; Game of incomplete informations; Incentives; Matching mechanisms; National residency matching programs; Stable matching; Commerce
Display advertising auctions with arbitrage,2015,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015796024&doi=10.1145%2f2668033&partnerID=40&md5=294f9d1fbd082d66552ed56454a6a776,"Online display advertising exchanges connectWeb publishers with advertisers seeking to place ads. In many cases, the advertiser obtains value from an ad impression (a viewing by a user) only if it is clicked, and frequently advertisers prefer to pay contingent on this occurring. But at the same time, many publishers demand payment independent of clicks. Arbitragers with good estimates of click-probabilities can resolve this conflict by absorbing the risk and acting as an intermediary, paying the publisher on allocation and being paid only if a click occurs. This article examines the incentives of advertisers and arbitragers and contributes an efficient mechanism with truthful bidding by the advertisers and truthful reporting of click predictions by arbitragers as dominant strategies while, given that a hazard rate condition is satisfied, yielding increased revenue to the publisher. We provide empirical evidence based on bid data from Yahoo's Right Media Exchange suggesting that the mechanism would increase revenue in practice.",Arbitrage; Auctions; Efficiency; Mechanism design; Online advertising,Commerce; Efficiency; Machine design; Social networking (online); Arbitrage; Auctions; Display advertisings; Dominant strategy; Evidence-based; Hazard rates; Mechanism design; Online advertising; Marketing
Single-call mechanisms,2015,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045363745&doi=10.1145%2f2741027&partnerID=40&md5=58fdce0bde2631ed8859ba45372c47cb,"Truthfulness is fragile and demanding. It is oftentimes harder to guarantee truthfulness when solving a problem than it is to solve the problem itself. Even worse, truthfulness can be utterly destroyed by small uncertainties in a mechanism's outcome. One obstacle is that truthful payments depend on outcomes other than the one realized, such as the lengths of non-shortest-paths in a shortest-path auction. Single-call mechanisms are a powerful tool that circumvents this obstacle: they implicitly charge truthful payments, guaranteeing truthfulness in expectation using only the outcome realized by the mechanism. The cost of such truthfulness is a trade-off between the expected quality of the outcome and the risk of large payments. We study two of the most general domains for truthful mechanisms and largely settle when and to what extent single-call mechanisms are possible. The first single-call construction was discovered by Babaioff et al. [2010] in single-parameter domains. They give a transformation that turns any monotone, singleparameter allocation rule into a truthful-in-expectation single-call mechanism. Our first result is a natural complement to Babaioff et al. [2010]: we give a new transformation that produces a single-call VCG mechanism from any allocation rule for which VCG payments are truthful. Second, in both the single-parameter and VCG settings, we precisely characterize the possible transformations, showing that a wide variety of transformations are possible but that all take a very simple form. Finally, we study the inherent trade-off between the expected quality of the outcome and the risk of large payments. We show that our construction and that of Babaioff et al. [2010] simultaneously optimize a variety of metrics in their respective domains. Our study is motivated by settings where uncertainty in a mechanism renders other known techniques untruthful, and we offer a variety of examples where such uncertainty can arise. In particular, we analyze pay-per-click advertising auctions, where the truthfulness of the standard VCG-based auction is easily broken when the auctioneer's estimated click-through-rates are imprecise. © 2015 ACM.",Black-box reductions; Efficiency; Incentive compatibility; Single-call,Aluminum; Efficiency; Graph theory; Allocation rule; Black-box reductions; Click-through rate; Incentive compatibility; Pay per clicks; Single parameter; Single-call; Truthful mechanisms; Economic and social effects
Secondary spectrum auctions for symmetric and submodular bidders,2015,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85037645961&doi=10.1145%2f2739041&partnerID=40&md5=ca65e0bb66fdc0b39a3e484dfcda48d5,"We study truthful auctions for secondary spectrum usage in wireless networks. In this scenario, n communication requests need to be allocated to k available channels that are subject to interference and noise. We present the first truthful mechanisms for secondary spectrum auctions with symmetric or submodular valuations. Our approach to model interference uses an edge-weighted conflict graph, and our algorithms provide asymptotically almost optimal approximation bounds for conflict graphs with a small inductive independence number ρ 蠐 n. This approach covers a large variety of interference models such as, for instance, the protocol model or the recently popular physical model of interference. For unweighted conflict graphs and symmetric valuations we use LP-rounding to obtain O(ρ) -approximate mechanisms; for weighted conflict graphs we get a factor of O(ρ · (logn + logk)). For submodular users we combine the convex rounding framework of Dughmi et al. [2011] with randomized metarounding to obtain O(ρ)-approximate mechanisms for matroid-rank-sum valuations; for weighted conflict graphs we can fully drop the dependence on k to get O(ρ · logn). We conclude with promising initial results for deterministically truthful mechanisms that allow approximation factors based on ρ.",Combinatorial auctions; Convex rounding; Inductive independence number; SINR,Commerce; Graphic methods; Approximation factor; Combinatorial auction; Convex rounding; Independence number; Optimal approximation; Secondary spectrums; SINR; Truthful mechanisms; Approximation algorithms
Safe opponent exploitation,2015,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045369272&doi=10.1145%2f2716322&partnerID=40&md5=64737ee809d82418508a8efe0f67b893,"We consider the problem of playing a repeated two-player zero-sum game safety: that is, guaranteeing at least the value of the game per period in expectation regardless of the strategy used by the opponent. Playing a stage-game equilibrium strategy at each time step clearly guarantees safety, and prior work has (incorrectly) stated that it is impossible to simultaneously deviate from a stage-game equilibrium (in hope of exploiting a suboptimal opponent) and to guarantee safety. We show that such profitable deviations are indeed possible specifically in games where certain types of ""gift"" strategies exist, which we define formally. We show that the set of strategies constituting such gifts can be strictly larger than the set of iteratively weakly-dominated strategies; this disproves another recent assertion which states that all noniteratively weakly dominated strategies are best responses to each equilibrium strategy of the other player. We present a full characterization of safe strategies, and develop efficient algorithms for exploiting suboptimal opponents while guaranteeing safety. We also provide analogous results for extensive-form games of perfect and imperfect information, and present safe exploitation algorithms and full characterizations of safe strategies for those settings as well. We present experimental results in Kuhn poker, a canonical test problem for game-theoretic algorithms. Our experiments show that (1) aggressive safe exploitation strategies significantly outperform adjusting the exploitation within stage-game equilibrium strategies only and (2) all the safe exploitation strategies significantly outperform a (nonsafe) best response strategy against strong dynamic opponents. © 2015 ACM.",Game theory; Multiagent learning; Opponent exploitation,Algorithms; Iterative methods; Multi agent systems; Best response; Equilibrium strategy; Extensive-form games; Game-theoretic; Imperfect information; Multi-agent learning; Opponent exploitation; Zero-sum game; Game theory
Improving the effectiveness of time-based display advertising,2015,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045373488&doi=10.1145%2f2716323&partnerID=40&md5=bec07127dd04892e3223eff0e4d0f853,"Display advertisements are typically sold by the impression, where one impression is simply one download of an ad. Previous work has shown that the longer an ad is in view, the more likely a user is to remember it and that there are diminishing returns to increased exposure time [Goldstein et al. 2011]. Since a pricing scheme that is at least partially based on time is more exact than one based solely on impressions, time-based advertising may become an industry standard. We answer an open question concerning time-based pricing schemes: how should time slots for advertisements be divided? We provide evidence that ads can be scheduled in a way that leads to greater total recollection, which advertisers value, and increased revenue, which publishers value. We document two main findings. First, we show that displaying two shorter ads results in more total recollection than displaying one longer ad of twice the duration. Second, we show that this effect disappears as the duration of these ads increases. We conclude with a theoretical prediction regarding the circumstances under which the display advertising industry would benefit if it moved to a partially or fully time-based standard.",Advertising; Display; Exposure; Memory; Recall; Recognition; Time,Data storage equipment; Display devices; Economics; Display advertisings; Exposure; Exposure-time; Industry standards; Pricing scheme; Recall; Recognition; Time; Marketing
Traffic shaping to optimize ad delivery,2015,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045363799&doi=10.1145%2f2739010&partnerID=40&md5=dc91aee8b3185b3ee9781a2a23b6875c,"Web publishers must balance two objectives: how to keep users engaged by directing them to relevant content, and how to properly monetize this user traffic. The standard approach is to solve each problem in isolation, for example, by displaying content that is tailored to the user's interests so as to maximize clickthrough rates (CTR), and also by building a standalone ad serving system that displays ads depending on the user's characteristics, the article being viewed by the user, and advertiser-specified constraints. However, showing the user only those articles with highest expected CTR precludes the display of some ads; if the publisher had previously guaranteed the delivery of a certain volume of impressions to such ads, then underdelivery penalties might have to be paid. We propose a joint optimization of article selection and ad serving that minimizes underdelivery by shaping some of the incoming traffic to pages where underperforming ads can be displayed, while incurring only minor drops in CTR. In addition to formulating the problem, we design an online optimization algorithm that can find the optimal traffic shaping probabilities for each new user using only a cache of one number per ad contract. Experiments on a large real-world ad-serving Web portal demonstrate significant advantages over the standalone approach: a threefold reduction in underdelivery with only 10% drop in CTR, or a 2.6-fold reduction with a 4% CTR drop, and similar results over a wide range.",Ad serving; Online reconstruction; Underdelivery,Algorithms; Drops; Optimization; Portals; Ad serving; Clickthrough rates (CTR); Incoming traffic; Joint optimization; Online optimization algorithms; Online reconstruction; Underdelivery; User's interest; Social networking (online)
To match or not to match: Economics of cookie matching in online advertising,2015,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85037652053&doi=10.1145%2f2745801&partnerID=40&md5=d8b7004bf181597f257dc86d9c1de930,"Modern online advertising increasingly relies on the ability to follow the same user across the Internet using technology called cookie matching to increase efficiency in ad allocation. Web publishers today use this technology to share information about the websites a user has visited, making it possible to target advertisements to users based on their prior history. This begs the question: do publishers (who are competitors for advertising money) always have the incentive to share online information? Intuitive arguments as well as anecdotal evidence suggest that sometimes a premium publisher might suffer information sharing through an effect called information leakage: by sharing user information with the advertiser, the advertiser will be able to target the same user elsewhere on cheaper publishers, leading to a dilution of the value of the supply on the premium publishers. The goal of this article is to explore this aspect of online information sharing. We show that, when advertisers are homogeneous in the sense that their relative valuations of users are consistent, publishers always agree about the benefits of cookie matching in equilibrium: either all publishers' revenues benefit, or all suffer, from cookie matching. We also show using a simple model that, when advertisers are not homogeneous, the information leakage indeed can occur, with cookie matching helping one publisher's revenues while harming the other. © 2015 ACM.",Ad auctions; Cookie matching; Information in auctions,Commerce; Economics; Electronic document exchange; Information analysis; Marketing; Social networking (online); Ad auctions; Anecdotal evidences; Cookie matching; Information in auctions; Information leakage; Information sharing; On-line information; Online advertising; Information dissemination
"Approximate pure Nash equilibria in weighted congestion games: Existence, efficient computation, and structure",2015,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85037662054&doi=10.1145%2f2614687&partnerID=40&md5=2a552248ef4067c7a7b9a4ce7f706dc9,"We consider structural and algorithmic questions related to the Nash dynamics of weighted congestion games. In weighted congestion games with linear latency functions, the existence of pure Nash equilibria is guaranteed by a potential function argument. Unfortunately, this proof of existence is inefficient and computing pure Nash equilibria in such games is a PLS-hard problem even when all players have unit weights. The situation gets worse when superlinear (e.g., quadratic) latency functions come into play; in this case, the Nash dynamics of the game may contain cycles and pure Nash equilibria may not even exist. Given these obstacles, we consider approximate pure Nash equilibria as alternative solution concepts. A ρ-approximate pure Nash equilibrium is a state of a (weighted congestion) game from which no player has any incentive to deviate in order to improve her cost by a multiplicative factor higher than ρ. Do such equilibria exist for small values of ρ? And if so, can we compute them efficiently? We provide positive answers to both questions for weighted congestion games with polynomial latency functions by exploiting an ""approximation"" of such games by a new class of potential games that we call Ψ-games. This allows us to show that these games have d!-approximate pure Nash equilibria, where d is the maximum degree of the latency functions. Our main technical contribution is an efficient algorithm for computing O(1)-approximate pure Nash equilibria when d is a constant. For games with linear latency functions, the approximation guarantee is 3+√5/2+O(γ) for arbitrarily small γ > 0; for latency functions with maximum degree d ≥ 2, it is d2d+o(d). The running time is polynomial in the number of bits in the representation of the game and 1/γ. As a byproduct of our techniques, we also show the following interesting structural statement for weighted congestion games with polynomial latency functions of maximum degree d ≥ 2: polynomially-long sequences of best-response moves from any initial state to a dO(d2)-approximate pure Nash equilibrium exist and can be efficiently identified in such games as long as d is a constant. To the best of our knowledge, these are the first positive algorithmic results for approximate pure Nash equilibria in weighted congestion games. Our techniques significantly extend our recent work on unweighted congestion games through the use of Ψ-games. The concept of approximating nonpotential games by potential ones is interesting in itself and might have further applications. © 2015 ACM.",Nash dynamics; Potential games; Pure Nash equilibria,Algorithms; Computation theory; Congestion control (communication); Dynamics; Game theory; Polynomial approximation; Polynomials; Telecommunication networks; Alternative solutions; Efficient computation; Multiplicative factors; Nash dynamics; Potential function; Potential games; Pure Nash equilibrium; Technical contribution; Computer games
Payment rules through discriminant-based classifiers,2015,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85037608306&doi=10.1145%2f2559049&partnerID=40&md5=40e2328bff54a53603b5450bee789829,"In mechanism design it is typical to impose incentive compatibility and then derive an optimal mechanism subject to this constraint. By replacing the incentive compatibility requirement with the goal of minimizing expected ex post regret, we are able to adapt statistical machine learning techniques to the design of payment rules. This computational approach to mechanism design is applicable to domains with multidimensional types and situations where computational efficiency is a concern. Specifically, given an outcome rule and access to a type distribution, we train a support vector machine with a specific structure imposed on the discriminant function, such that it implicitly learns a corresponding payment rule with desirable incentive properties. We extend the framework to adopt succinct k-wise dependent valuations, leveraging a connection with maximum a posteriori assignment on Markov networks to enable training to scale up to settings with a large number of items; we evaluate this construction in the case where k = 2. We present applications to multiparameter combinatorial auctions with approximate winner determination, and the assignment problem with an egalitarian outcome rule. Experimental results demonstrate that the construction produces payment rules with low ex post regret, and that penalizing classification error is effective in preventing failures of ex post individual rationality.",Computational mechanism design; Support vector machines,Artificial intelligence; Combinatorial optimization; Computational efficiency; Design; Learning systems; Machine design; Classification errors; Combinatorial auction; Computational approach; Computational mechanism design; Discriminant functions; Incentive compatibility; Individual rationality; Statistical machine learning; Support vector machines
Dynamic pricing with limited supply,2015,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85037633842&doi=10.1145%2f2559152&partnerID=40&md5=fd9e6d5a11663fea5172bd206ce349e2,"We consider the problem of designing revenue-maximizing online posted-price mechanisms when the seller has limited supply. A seller has k identical items for sale and is facing n potential buyers (""agents"") that are arriving sequentially. Each agent is interested in buying one item. Each agent's value for an item is an independent sample from some fixed (but unknown) distribution with support [0, 1]. The seller offers a take-it-or-leave-it price to each arriving agent (possibly different for different agents), and aims to maximize his expected revenue. We focus on mechanisms that do not use any information about the distribution; such mechanisms are called detail-free (or prior-independent). They are desirable because knowing the distribution is unrealistic in many practical scenarios. We study how the revenue of such mechanisms compares to the revenue of the optimal offline mechanism that knows the distribution (""offline benchmark""). We present a detail-free online posted-price mechanism whose revenue is at most O((k logn) 2/3) less than the offline benchmark, for every distribution that is regular. In fact, this guarantee holds without any assumptions if the benchmark is relaxed to fixed-price mechanisms. Further, we prove a matching lower bound. The performance guarantee for the same mechanism can be improved to O(√k logn), with a distribution-dependent constant, if the ratio k/n is sufficiently small. We show that, in the worst case over all demand distributions, this is essentially the best rate that can be obtained with a distribution-specific constant. On a technical level, we exploit the connection to multiarmed bandits (MAB). While dynamic pricing with unlimited supply can easily be seen as an MAB problem, the intuition behind MAB approaches breaks when applied to the setting with limited supply. Our high-level conceptual contribution is that even the limited supply setting can be fruitfully treated as a bandit problem. © 2015 ACM.",Dynamic pricing; Mechanism design; Multiarmed bandits; Posted price auctions; Regret; Revenue maximization,Costs; Machine design; Social networking (online); Dynamic pricing; Mechanism design; Multi armed bandit; Price auction; Regret; Revenue maximization; Economics
"Beyond dominant resource fairness: Extensions, limitations, and indivisibilities",2015,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85037613647&doi=10.1145%2f2739040&partnerID=40&md5=c7184355ecba6f9da20ebada8a0f41bd,"We study the problem of allocating multiple resources to agents with heterogeneous demands. Technological advances such as cloud computing and data centers provide a new impetus for investigating this problem under the assumption that agents demand the resources in fixed proportions, known in economics as Leontief preferences. In a recent paper, Ghodsi et al. [2011] introduced the dominant resource fairness (DRF) mechanism, which was shown to possess highly desirable theoretical properties under Leontief preferences. We extend their results in three directions. First, we show that DRF generalizes to more expressive settings, and leverage a new technical framework to formally extend its guarantees. Second, we study the relation between social welfare and properties such as truthfulness; DRF performs poorly in terms of social welfare, but we show that this is an unavoidable shortcoming that is shared by every mechanism that satisfies one of three basic properties. Third, and most importantly, we study a realistic setting that involves indivisibilities. We chart the boundaries of the possible in this setting, contributing a new relaxed notion of fairness and providing both possibility and impossibility results. © 2015 ACM.",Fair division; Resource allocation,Distributed computer systems; Resource allocation; Data centers; Fair divisions; Heterogeneous demand; Impossibility results; Multiple resources; Social welfare; Technical frameworks; Technological advances; Economics
Introduction to the special issue on EC'12,2015,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045362161&doi=10.1145%2f2742678&partnerID=40&md5=8063ff5aa79e6508819ccba3605b9391,[No abstract available],,
The price of anarchy in games of incomplete information,2015,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85037611632&doi=10.1145%2f2737816&partnerID=40&md5=4e10f900fbef092fb8a7cb8d856a6f59,"We define smooth games of incomplete information. We prove an ""extension theorem"" for such games: price of anarchy bounds for pure Nash equilibria for all induced full-information games extend automatically, without quantitative degradation, to all mixed-strategy Bayes-Nash equilibria with respect to a product prior distribution over players' preferences. We also note that, for Bayes-Nash equilibria in games with correlated player preferences, there is no general extension theorem for smooth games. We give several applications of our definition and extension theorem. First, we show that many games of incomplete information for which the price of anarchy has been studied are smooth in our sense. Our extension theorem unifies much of the known work on the price of anarchy in games of incomplete information. Second, we use our extension theorem to prove new bounds on the price of anarchy of Bayes-Nash equilibria in routing games with incomplete information. © 2015 ACM.",Auctions; Bayes-Nash equilibria; Price of anarchy; Routing games,Auctions; Full Information Game; Games with incomplete information; Incomplete information; Nash equilibria; Price of anarchy; Pure Nash equilibrium; Routing games; Costs
On the power of deterministic mechanisms for facility location games,2014,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014101200&doi=10.1145%2f2665005&partnerID=40&md5=1040db6847963ce4fd4c7ce3970fd906,"We consider K-Facility Location games, where n strategic agents report their locations in a metric space and a mechanism maps them to K facilities. The agents seek to minimize their connection cost, namely the distance of their true location to the nearest facility, and may misreport their location. We are interested in deterministic mechanisms that are strategyproof, that is, ensure that no agent can benefit from misreporting her location, do not resort to monetary transfers, and achieve a bounded approximation ratio to the total connection cost of the agents (or to the Lp norm of the connection costs, for some p ∈ [1,∞) or for p = ∞). Our main result is an elegant characterization of deterministic strategyproof mechanisms with a bounded approximation ratio for 2-Facility Location on the line. In particular, we show that for instances with n ≥ 5 agents, any such mechanism either admits a unique dictator or always places the facilities at the leftmost and the rightmost location of the instance. As a corollary, we obtain that the best approximation ratio achievable by deterministic strategyproof mechanisms for the problem of locating 2 facilities on the line to minimize the total connection cost is precisely n - 2. Another rather surprising consequence is that the TWO-EXTREMES mechanism of Procaccia and Tennenholtz [2009] is the only deterministic anonymous strategyproof mechanism with a bounded approximation ratio for 2-Facility Location on the line. The proof of the characterization employs several new ideas and technical tools, which provide new insights into the behavior of deterministic strategyproof mechanisms for K-Facility Location games and may be of independent interest. Employing one of these tools, we show that for every K ≥ 3, there do not exist any deterministic anonymous strategyproof mechanisms with a bounded approximation ratio for K-Facility Location on the line, even for simple instances with K + 1 agents. Moreover, building on the characterization for the line, we show that there do not exist any deterministic strategyproof mechanisms with a bounded approximation ratio for 2-Facility Location and instances with n ≥ 3 agents located in a star. © 2014 ACM.",Algorithmic mechanism design; Algorithms; Economics; F.2.0 [analysis of algorithms and problem complexity]: general; Facility location games; J.4 [social and behavioral sciences]: economics; Social choice; Theory,Algorithms; Behavioral research; Characterization; Computational complexity; Costs; Economics; Game theory; Machine design; Parallel processing systems; Algorithmic mechanism design; Facility location games; Problem complexity; Social and behavioral science; Social choice; Theory; Location
Implementation and computation of a value for generalized characteristic function games,2014,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032865026&doi=10.1145%2f2665007&partnerID=40&md5=d4d289b9a76a820f29423fe08393e27a,"Generalized characteristic function games are a variation of characteristic function games, in which the value of a coalition depends not only on the identities of its members, but also on the order in which the coalition is formed. This class of games is a useful abstraction for a number of realistic settings and economic situations, such as modeling relationships in social networks. To date, two main extensions of the Shapley value have been proposed for generalized characteristic function games: the Nowak-Radzik [1994] value and the Sánchez-Bergantiños [1997] value. In this context, the present article studies generalized characteristic function games from the point of view of implementation and computation. Specifically, the article makes two key contributions. First, building upon the mechanism by Dasgupta and Chiu [1998], we present a non-cooperative mechanism that implements both the Nowak-Radzik value and the Sánchez-Bergantiños value in Subgame-Perfect Nash Equilibria in expectations. Second, in order to facilitate an efficient computation supporting the implementation mechanism, we propose the Generalized Marginal-Contribution Nets representation for this type of game. This representation extends the results of Ieong and Shoham [2006] and Elkind et al. [2009] for characteristic function games and retains their attractive computational properties. © 2014 ACM.",Algorithms; Economics; Generalized characteristic function games; Implementation; J.4 [social and behavioral sciences]; Representation; Shapley value; Theory,Algorithms; Behavioral research; Computation theory; Economics; Characteristic functions; Implementation; Representation; Shapley value; Social and behavioral science; Theory; Game theory
Altruism and its impact on the price of anarchy,2014,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84983762119&doi=10.1145%2f2597893&partnerID=40&md5=598ab0b1426989330852d3a3429f38aa,"We study the inefficiency of equilibria for congestion games when players are (partially) altruistic.We model altruistic behavior by assuming that player i's perceived cost is a convex combination of 1 - αi times his direct cost and αi times the social cost. Tuning the parameters αi allows smooth interpolation between purely selfish and purely altruistic behavior. Within this framework, we study primarily altruistic extensions of (atomic and nonatomic) congestion games, but also obtain some results on fair cost-sharing games and valid utility games. We derive (tight) bounds on the price of anarchy of these games for several solution concepts. Thereto, we suitably adapt the smoothness notion introduced by Roughgarden and show that it captures the essential properties to determine the robust price of anarchy of these games. Our bounds show that for atomic congestion games and cost-sharing games, the robust price of anarchy gets worse with increasing altruism, while for valid utility games, it remains constant and is not affected by altruism. However, the increase in the price of anarchy is not a universal phenomenon: For general nonatomic congestion games with uniform altruism, the price of anarchy improves with increasing altruism. For atomic and nonatomic symmetric singleton congestion games, we derive bounds on the pure price of anarchy that improve as the average level of altruism increases. (For atomic games, we only derive such bounds when cost functions are linear.) Since the bounds are also strictly lower than the robust price of anarchy, these games exhibit natural examples in which pure Nash equilibria are more efficient than more permissive notions of equilibrium. © 2014 ACM.",Altruism; Congestion games; Cost-sharing games; Economics; F.0 [theory of computation]: general; Price of anarchy; Selfishness; Theory; Valid utility games,Atoms; Computation theory; Cost effectiveness; Cost functions; Economics; Altruism; Congestion Games; Cost-sharing games; Price of anarchy; Selfishness; Theory; Theory of computation; Valid utility games; Costs
Near-optimality in covering games by exposing global information,2014,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84992495462&doi=10.1145%2f2597890&partnerID=40&md5=fa2b3ab669f8ede9fe899b65a61db6f9,"Mechanism design for distributed systems is fundamentally concerned with aligning individual incentives with social welfare to avoid socially inefficient outcomes that can arise from agents acting autonomously. One simple and natural approach is to centrally broadcast nonbinding advice intended to guide the system to a socially near-optimal state while still harnessing the incentives of individual agents. The analytical challenge is proving fast convergence to near optimal states, and in this article we give the first results that carefully constructed advice vectors yield stronger guarantees. We apply this approach to a broad family of potential games modeling vertex cover and set cover optimization problems in a distributed setting. This class of problems is interesting because finding exact solutions to their optimization problems is NP-hard yet highly inefficient equilibria exist, so a solution in which agents simply locally optimize is not satisfactory. We show that with an arbitrary advice vector, a set cover game quickly converges to an equilibrium with cost of the same order as the square of the social cost of the advice vector. More interestingly, we show how to efficiently construct an advice vector with a particular structure with cost O(log n) times the optimal social cost, and we prove that the system quickly converges to an equilibrium with social cost of this same order. 2014 Copyright is held by the author/owner(s).",Algorithmic game theory; Algorithms; C.4 [performance of systems]; Economics; F.2.0 [analysis of algorithms and problem complexity]: General; Mechanism design; Price of anarchy; Theory,Algorithms; Autonomous agents; Computational complexity; Costs; Economics; Game theory; Machine design; Parallel processing systems; Vectors; Algorithmic Game Theory; Mechanism design; Performance of systems; Price of anarchy; Problem complexity; Theory; Optimization
"Weighted congestion games: The price of anarchy, universal worst-case examples, and tightness",2014,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929202071&doi=10.1145%2f2629666&partnerID=40&md5=d36772a645487cfaa8335fc5ccb38643,"We characterize the Price of Anarchy (POA) in weighted congestion games, as a function of the allowable resource cost functions. Our results provide as thorough an understanding of this quantity as is already known for nonatomic and unweighted congestion games, and take the form of universal (cost function-independent) worst-case examples. One noteworthy by-product of our proofs is the fact that weighted congestion games are ""tight,"" which implies that the worst-case price of anarchy with respect to pure Nash equilibria, mixed Nash equilibria, correlated equilibria, and coarse correlated equilibria are always equal (under mild conditions on the allowable cost functions). Another is the fact that, like nonatomic but unlike atomic (unweighted) congestion games, weighted congestion games with trivial structure already realize the worst-case POA, at least for polynomial cost functions. We also prove a new result about unweighted congestion games: the worst-case price of anarchy in symmetric games is as large as in their more general asymmetric counterparts. © 2014 ACM.",Algorithms; Economics; F.2.2 [analysis of algorithms and problem complexity]: nonnumerical algorithms and problems; Nash equilibria; Price of anarchy; Theory; Weighted congestion games,Algorithms; Computational complexity; Costs; Economics; Congestion Games; Nash equilibria; Price of anarchy; Problem complexity; Theory; Cost functions
The query complexity of scoring rules,2014,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045362051&doi=10.1145%2f2632228&partnerID=40&md5=843c6f831c37410f857c2b6b37c2a152,"Proper scoring rules are crucial tools to elicit truthful information from experts. A scoring rule maps χ, an expert-provided distribution over the set of all possible states of the world, and ω, a realized state of the world, to a real number representing the expert's reward for his provided information. To compute this reward, a scoring rule queries the distribution χ at various states. The number of these queries is thus a natural measure of the complexity of the scoring rule. We prove that any bounded and strictly proper scoring rule that is deterministic must make a number of queries to its input distribution that is a quarter of the number of states of the world. When the state space is very large, this makes the computation of such scoring rules impractical. We also show a new stochastic scoring rule that is bounded, strictly proper, and which makes only two queries to its input distribution. Thus, using randomness allows us to have significant savings when computing scoring rules. © 2014 ACM.",Algorithms; Congestion games; Economics; J.4 [social and behavioral sciences]: economics; K.4.4 [computers and society]: electronic commerce; Price of anarchy; Selfish routing; Theory,Algorithms; Behavioral research; Economics; Stochastic systems; Computers and societies; Congestion Games; Price of anarchy; Selfish routing; Social and behavioral science; Theory; Computer games
On random sampling auctions for digital goods,2014,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014211395&doi=10.1145%2f2517148&partnerID=40&md5=bf9b2fd1eca32c5b71bdaa823f706c3c,"In the context of auctions for digital goods, an interesting random sampling auction has been proposed by Goldberg et al. [2001]. This auction has been analyzed by Feige et al. [2005], who have shown that it obtains in expectation at least 1/15 fraction of the optimal revenue, which is substantially better than the previously proven constant bounds but still far from the conjectured lower bound of 1/4. In this article, we prove that the aforementioned random sampling auction obtains at least 1/4 fraction of the optimal revenue for a large class of instances where the number of bids above (or equal to) the optimal sale price is at least 6. We also show that this auction obtains at least 1/4.68 fraction of the optimal revenue for the small class of remaining instances, thus leaving a negligible gap between the lower and upper bound. We employ a mix of probabilistic techniques and dynamic programming to compute these bounds. © 2014 ACM.",Algorithms; Auction; Design; Economics; F.2.0 [theory of computation]: analysis of algorithms and problem complexity - general; G.3 [mathematics of computing]: probability and statistics - probabilistic algorithms; Mechanism design; Random sampling; Theory,Algorithms; Commerce; Computational complexity; Design; Dynamic programming; Economics; Machine design; Analysis of algorithms; Auction; Mechanism design; Probability and statistics; Random sampling; Theory; Sampling
Ex-post equilibrium and VCG mechanisms,2014,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045368302&doi=10.1145%2f2594565&partnerID=40&md5=3da3e4555a354ff4505ff2e83f0520a2,"Consider an abstract social choice setting with incomplete information, where the number of alternatives is large. Albeit natural, implementing VCG mechanisms is infeasible due to the prohibitive communication constraints. However, if players restrict attention to a subset of the alternatives, feasibility may be recovered. This article characterizes the class of subsets that induce an ex-post equilibrium in the original game. It turns out that a crucial condition for such subsets to exist is the availability of a type-independent optimal social alternative for each player. We further analyze the welfare implications of these restrictions. This work follows that of Holzman et al. [2004] and Holzman and Monderer [2004] where similar analysis is done for combinatorial auctions. © 2014 ACM.",Algorithms communication complexity; Economics; Ex-post equilibrium; H.3.3 [information storage and retrieval]: information search and retrieval-query formulation; Mechanism design; Theory,Abstracting; Economics; Machine design; Combinatorial auction; Communication complexity; Communication constraints; H.3.3 [information storage and retrieval]: information search and retrievals; Incomplete information; Mechanism design; Theory; Welfare implications; Computational complexity
Privacy auctions for recommender systems,2014,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029899687&doi=10.1145%2f2629665&partnerID=40&md5=98dfd9a7b600b3e7e643aaaca694f617,"We study a market for private data in which a data analyst publicly releases a statistic over a database of private information. Individuals that own the data incur a cost for their loss of privacy proportional to the differential privacy guarantee given by the analyst at the time of the release. The analyst incentivizes individuals by compensating them, giving rise to a privacy auction. Motivated by recommender systems, the statistic we consider is a linear predictor function with publicly known weights. The statistic can be viewed as a prediction of the unknown data of a new individual, based on the data of individuals in the database. We formalize the trade-off between privacy and accuracy in this setting, and show that a simple class of estimates achieves an order-optimal trade-off. It thus suffices to focus on auction mechanisms that output such estimates. We use this observation to design a truthful, individually rational, proportional-purchase mechanism under a fixed budget constraint. We show that our mechanism is 5-approximate in terms of accuracy compared to the optimal mechanism, and that no truthful mechanism can achieve a 2 - ε approximation, for any ε > 0. © 2014 ACM.",Economics; J.4 [social and behavioral sciences]: economics; K.4.4 [computers and society]: electronic commerce; Linear predictor; Mechanism design; Privacy auction; Theory,Behavioral research; Budget control; Commerce; Economic and social effects; Economics; Machine design; Recommender systems; Computers and societies; Linear predictors; Mechanism design; Social and behavioral science; Theory; Electronic commerce
Eliciting predictions and recommendations for decision making,2014,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019292287&doi=10.1145%2f2556271&partnerID=40&md5=c8ac0a43ed55a36100125d1a599e2b3c,"When making a decision, a decision maker selects one of several possible actions and hopes to achieve a desirable outcome. To make a better decision, the decision maker often asks experts for advice. In this article, we consider two methods of acquiring advice for decision making. We begin with a method where one or more experts predict the effect of each action and the decision maker then selects an action based on the predictions. We characterize strictly proper decision making, where experts have an incentive to accurately reveal their beliefs about the outcome of each action. However, strictly proper decision making requires the decision maker use a completely mixed strategy to choose an action. To address this limitation, we consider a second method where the decision maker asks a single expert to recommend an action. We show that it is possible to elicit the decision maker's most preferred action for a broad class of preferences of the decision maker, including when the decision maker is an expected value maximizer. © 2014 ACM.",Decision making; Decision markets; Economics; F.0 [theory of computation]: general; Information elicitation; J.4 [social and behavioral sciences]; Market design; Prediction markets; Scoring rules; Theory,Behavioral research; Commerce; Computation theory; Decision theory; Economics; Forecasting; Information elicitation; Market design; Prediction markets; Scoring rules; Social and behavioral science; Theory; Theory of computation; Decision making
The price of anarchy for selfish ring routing is two,2014,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045360367&doi=10.1145%2f2548545&partnerID=40&md5=09adc3d7fede083fa39e155c2bfa4e48,"We analyze the network congestion game with atomic players, asymmetric strategies, and the maximum latency among all players as social cost. This important social cost function is much less understood than the average latency. We show that the price of anarchy is at most two, when the network is a ring and the link latencies are linear. Our bound is tight. This is the first sharp bound for the maximum latency objective. © 2014 ACM.",C.2.1 [computer-communication networks]: Network architecture and design - Network communications; C.2.2 [computer-communication networks]: Network protocols - Routing protocols; Congestion games; Performance; Price of anarchy; Selfish routing; Theory,Computer games; Cost functions; Costs; Network protocols; Computer communication networks; Congestion Games; Network communications; Performance; Price of anarchy; Selfish routing; Theory; Network architecture
Signaling schemes for revenue maximization,2014,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013008116&doi=10.1145%2f2594564&partnerID=40&md5=8f11479a283981458b39fd0e3d1b90cf,"Signaling is an important topic in the study of asymmetric information in economic settings. In particular, the transparency of information available to a seller in an auction setting is a question of major interest. We introduce the study of signaling when conducting a second price auction of a probabilistic good whose actual instantiation is known to the auctioneer but not to the bidders. This framework can be used to model impressions selling in display advertising. We establish several results within this framework. First, we study the problem of computing a signaling scheme that maximizes the auctioneer's revenue in a Bayesian setting. We show that this problem is polynomially solvable for some interesting special cases, but computationally hard in general. Second, we establish a tight bound on the minimum number of signals required to implement an optimal signaling scheme. Finally, we show that at least half of the maximum social welfare can be preserved within such a scheme. © 2014 held by the Owner/Author.",Algorithms; Asymmetric information; Economics; F.2.2 [analysis of algorithms and problem complexity]: nonnumerical algorithms and problems - complexity of proof procedures; J.4 [social and behavioral sciences]: economics; Probabilistic auctions; Signaling,Algorithms; Behavioral research; Computational complexity; Economic analysis; Economics; Signal processing; Signaling; Asymmetric information; Display advertisings; Polynomially solvable; Probabilistic auctions; Problem complexity; Revenue maximization; Second-price auction; Social and behavioral science; Problem solving
Convergence of position auctions under myopic best-response dynamics,2014,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84998704670&doi=10.1145%2f2632226&partnerID=40&md5=25e29b84f3ed33b69e66068702c0d58d,"We study the dynamics of multiround position auctions, considering both the case of exogenous click-through rates and the case in which click-through rates are determined by an endogenous consumer search process. In both contexts, we demonstrate that dynamic position auctions converge to their associated static, envy-free equilibria. Furthermore, convergence is efficient, and the entry of low-quality advertisers does not slow convergence. Because our approach predominantly relies on assumptions common in the sponsored search literature, our results suggest that dynamic position auctions converge more generally. © 2014 ACM.",Best-response bidding; Consumer search; Dynamic auctions; Economics; J.4 [social and behavioral sciences]: economics; K.4.4 [computers and society]: electronic commerce; Position auctions; Theory,Behavioral research; Economics; Best response; Computers and societies; Consumer search; Position auctions; Social and behavioral science; Theory; Commerce
Signaling competition and social welfare,2014,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045371525&doi=10.1145%2f2560766&partnerID=40&md5=de402c57a39c1c75188d6fc0cbd7205e,"We consider an environment where sellers compete over buyers. All sellers are a-priori identical and strategically signal buyers about the product they sell. In a setting motivated by online advertising in display ad exchanges, where firms use second price auctions, a firm's strategy is a decision about its signaling scheme for a stream of goods (e.g., user impressions), and a buyer's strategy is a selection among the firms. In this setting, a single seller will typically provide partial information, and consequently, a product may be allocated inefficiently. Intuitively, competition among sellers may induce sellers to provide more information in order to attract buyers and thus increase efficiency. Surprisingly, we show that such a competition among firms may yield significant loss in consumers' social welfare with respect to the monopolistic setting. Although we also show that in some cases, the competitive setting yields gain in social welfare, we provide a tight bound on that gain, which is shown to be small with respect to the preceding possible loss. Our model is tightly connected with the literature on bundling in auctions. © 2014 ACM.",Competition; Economics; Efficiency; Equilibrium; J.4 [computer applications]: social and behavioral sciences - economics; Market; Social welfare; Theory,Behavioral research; Commerce; Economics; Efficiency; Marketing; Phase equilibria; Sales; Ad exchanges; Buyer's strategies; Online advertising; Partial information; Second-price auction; Social and behavioral science; Social welfare; Theory; Competition
On Nash equilibria for a network creation game,2014,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84944538929&doi=10.1145%2f2560767&partnerID=40&md5=6d10dc791fa8a0ad238399291a1015f0,"We study a basic network creation game proposed by Fabrikant et al. [2003]. In this game, each player (vertex) can create links (edges) to other players at a cost of α per edge. The goal of every player is to minimize the sum consisting of (a) the cost of the links he has created and (b) the sum of the distances to all other players. Fabrikant et al. conjectured that there exists a constant A such that, for any α > A, all nontransient Nash equilibria graphs are trees. They showed that if a Nash equilibrium is a tree, the price of anarchy is constant. In this article, we disprove the tree conjecture. More precisely, we show that for any positive integer n0, there exists a graph built by n ≥ n0 players which contains cycles and forms a nontransient Nash equilibrium, for any á with 1 < α ≤ √n/2. Our construction makes use of some interesting results on finite affine planes. On the other hand, we show that, for α ≥ 12n⌈log n⌉, every Nash equilibrium forms a tree. Without relying on the tree conjecture, Fabrikant et al. proved an upper bound on the price of anarchy of O(√α), where α ∈ [2, n2]. We improve this bound. Specifically, we derive a constant upper bound for α ∈ O(√n) and for α ≥ 12n⌈log n⌉. For the intermediate values, we derive an improved bound of O(1 + (min{α2/n, n2α})1/3). Additionally, we develop characterizations of Nash equilibria and extend our results to a weighted network creation game as well as to scenarios with cost sharing. © 2014 ACM.",Economics; F [theory of computation]; Nash equilibrium; Network design; Networks; Price of anarchy; Theory,Aluminum; Artificial intelligence; Computation theory; Computer networks; Cost effectiveness; Costs; Economics; Forestry; Game theory; Networks (circuits); Nash equilibria; Network design; Price of anarchy; Theory; Theory of computation; Trees (mathematics)
Goodness-of-fit measures for revealed preference tests: Complexity results and algorithms,2014,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991604155&doi=10.1145%2f2560793&partnerID=40&md5=fa5ad2a7a0d43e2556a75510ba4ad345,"We provide results on the computational complexity of goodness-of-fit measures (i.e., Afriat's efficiency index, Varian's efficiency vector-index, and the Houtman-Maks index) associated with several revealed preference axioms (i.e., WARP, SARP, GARP, and HARP). These results explain the computational difficulties that have been observed in literature when computing these indices. Our NP-hardness results are obtained by reductions from the independent set problem. We also show that this reduction can be used to prove that no approximation algorithm achieving a ratio of O(n1-δ), δ > 0 exists for Varian's index, nor for Houtman-Maks'index (unless P = NP). Finally, we give an exact polynomial-time algorithm for finding Afriat's efficiency index. © 2014 ACM.",Algorithms; Computational complexity; Economics; F.2.0 [analysis of algorithms and problem complexity]: general; G.2.2 [discrete mathematics]: graph theory - graph algorithms; Goodness-of-fit measures; NP-complete; Revealed preference; Testing individual rationality,Algorithms; Approximation algorithms; Economics; Efficiency; Graph theory; Polynomial approximation; Trees (mathematics); Goodness-of-fit measure; Graph algorithms; Individual rationality; NP Complete; Problem complexity; Revealed preference; Computational complexity
Rating protocols in online communities,2014,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032173330&doi=10.1145%2f2560794&partnerID=40&md5=07d6a24ec557353a06548197c093a4ba,"Sustaining cooperation among self-interested agents is critical for the proliferation of emerging online communities. Providing incentives for cooperation in online communities is particularly challenging because of their unique features: a large population of anonymous agents having asymmetric interests and dynamically joining and leaving the community, operation errors, and agents trying to whitewash when they have a low standing in the community. In this article, we take these features into consideration and propose a framework for designing and analyzing a class of incentive schemes based on rating protocols, which consist of a rating scheme and a recommended strategy. We first define the concept of sustainable rating protocols under which every agent has the incentive to follow the recommended strategy given the deployed rating scheme. We then formulate the problem of designing an optimal rating protocol, which selects the protocol that maximizes the overall social welfare among all sustainable rating protocols. Using the proposed framework, we study the structure of optimal rating protocols and explore the impact of one-sided rating, punishment lengths, and whitewashing on optimal rating protocols. Our results show that optimal rating protocols are capable of sustaining cooperation, with the amount of cooperation varying depending on the community characteristics. © 2014 ACM.",Design; Economics; H.4.0 [information system applications]: general; Human factors; Incentive schemes; J.4 [computer applications]: social and behavioral sciences - economics; Online communities; Rating schemes; Recommended strategy; Whitewashing,Behavioral research; Design; Economics; Human engineering; Online systems; Social networking (online); Structural optimization; Incentive schemes; On-line communities; Rating schemes; Recommended strategy; Social and behavioral science; System applications; Whitewashing; Rating
Optimal payments in dominant-strategy mechanisms for single-parameter domains,2013,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84899417376&doi=10.1145%2f2399187.2399191&partnerID=40&md5=3ec4ddb5f1945de1edfb98953709cac5,"We study dominant-strategy mechanisms in allocation domains where agents have one-dimensional types and quasilinear utilities. Taking an allocation function as an input, we present an algorithmic technique for finding optimal payments in a class of mechanism design problems, including utilitarian and egalitarian allocation of homogeneous items with nondecreasing marginal costs. Our results link optimality of payment functions to a geometric condition involving triangulations of polytopes. When this condition is satisfied, we constructively show the existence of an optimal payment function that is piecewise linear in agent types. © 2013 ACM.",Dominant-strategy implementation; Groves mechanisms; Mechanism design; Redistribution mechanisms; Triangulation; VCG,Machine design; Piecewise linear techniques; Surveying; Algorithmic techniques; Allocation domains; Allocation function; Dominant strategy; Geometric conditions; Groves mechanism; Mechanism design; VCG; Triangulation
Binary opinion dynamics with stubborn agents,2013,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85036452155&doi=10.1145%2f2538508&partnerID=40&md5=3292ef2a16f4c9febbbaf33c1660871f,"We study binary opinion dynamics in a social network with stubborn agents who influence others but do not change their opinions. We focus on a generalization of the classical voter model by introducing nodes (stubborn agents) that have a fixed state. We show that the presence of stubborn agents with opposing opinions precludes convergence to consensus; instead, opinions converge in distribution with disagreement and fluctuations. In addition to the first moment of this distribution typically studied in the literature, we study the behavior of the second moment in terms of network properties and the opinions and locations of stubborn agents. We also study the problem of optimal placement of stubborn agents where the location of a fixed number of stubborn agents is chosen to have the maximum impact on the long-run expected opinions of agents. © 2013 ACM.",Algorithms; Economics; Theory,Algorithms; Economics; First moments; Fixed numbers; Network properties; Opinion dynamics; Optimal placements; Second moments; Theory; Voter models; Bins
Sequential rationality in cryptographic protocols,2013,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019855503&doi=10.1145%2f2399187.2399189&partnerID=40&md5=b368325beb751e9a9a781f6f7110ce2b,"Much of the literature on rational cryptography focuses on analyzing the strategic properties of cryptographic protocols. However, due to the presence of computationally-bounded players and the asymptotic nature of cryptographic security, a definition of sequential rationality for this setting has thus far eluded researchers. We propose a new framework for overcoming these obstacles, and provide the first definitions of computational solution concepts that guarantee sequential rationality. We argue that natural computational variants of subgame perfection are too strong for cryptographic protocols. As an alternative, we introduce a weakening called threat-free Nash equilibrium that is more permissive but still eliminates the undesirable ""empty threats"" of nonsequential solution concepts. To demonstrate the applicability of our framework, we revisit the problem of implementing a mediator for correlated equilibria [Dodis et al 2000], and propose a variant of their protocol that is sequentially rational for a nontrivial class of correlated equilibria. Our treatment provides a better understanding of the conditions under which mediators in a correlated equilibrium can be replaced by a stable protocol. © 2013 ACM.",Correlated equilibrium; Cryptographic protocols; Nash equilibrium; Rational cryptography; Sequential rationality; Subgame perfect equilibrium,Computation theory; Game theory; Correlated equilibria; Cryptographic protocols; Nash equilibria; Rational cryptographies; Sequential rationality; Cryptography
A game-theoretic analysis of the ESP game,2013,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006700221&doi=10.1145%2f2399187.2399190&partnerID=40&md5=a8270b9fee012deee99464d983afb959,"""Games with a Purpose"" are interactive games that users play because they are fun, with the added benefit that the outcome of play is useful work. The ESP game, developed byy von Ahn and Dabbish [2004], is an example of such a game devised to label images on the web. Since labeling images is a hard problem for computer vision algorithms and can be tedious and time-consuming for humans, the ESP game provides humans with incentive to do useful work by being enjoyable to play. We present a simple game-theoretic model of the ESP game and characterize the equilibrium behavior in our model. Our equilibrium analysis supports the fact that users appear to coordinate on low effort words. We provide an alternate model of user preferences, modeling a change that could be induced through a different scoring method, and show that equilibrium behavior in this model coordinates on high-effort words. We also give sufficient conditions for coordinating on high-effort words to be a Bayesian-Nash equilibrium. Our results suggest the possibility of formal incentive design in achieving desirable system-wide outcomes for the purpose of human computation, complementing existing considerations of robustness against cheating and human factors. © 2013 ACM.",,Computer vision; Game theory; Bayesian Nash equilibria; Computer vision algorithms; Equilibrium analysis; Equilibrium behavior; Game theoretic analysis; Games with a purpose; Human computation; Interactive games; Computer games
The ACM transactions on economics and computation: An introduction,2013,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045367955&doi=10.1145%2f2399187.2399188&partnerID=40&md5=04b2340be3b2703308d0e6712685575d,[No abstract available],,
Learning Strong Substitutes Demand via Queries,2022,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161725875&doi=10.1145%2f3546604&partnerID=40&md5=61879e9b103ad63e14880e652431c8bd,"This article addresses the computational challenges of learning strong substitutes demand when given access to a demand (or valuation) oracle. Strong substitutes demand generalises the well-studied gross substitutes demand to a multi-unit setting. Recent work by Baldwin and Klemperer shows that any such demand can be expressed in a natural way as a finite list of weighted bid vectors. A simplified version of this bidding language has been used by the Bank of England.Assuming access to a demand oracle, we provide an algorithm that computes the unique list of weighted bid vectors corresponding to a bidder's demand preferences. In the special case where their demand can be expressed using positive bids only, we have an efficient algorithm that learns this list in linear time. We also show super-polynomial lower bounds on the query complexity of computing the list of bids in the general case where bids may be positive and negative. Our algorithms constitute the first systematic approach for bidders to construct a bid list corresponding to non-trivial demand, allowing them to participate in ""product-mix""auctions. © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",bidding language; Learning demand; preference elicitation; product-mix auction; query protocol; strong substitutes,Bank of England; Bidding languages; Computational challenges; Learning demand; Multi-unit; Preference elicitation; Product mix; Product-mix auction; Query protocol; Strong substitute; Query processing
Introduction to the Special Issue on WINE'20: Part 1,2022,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161664809&doi=10.1145%2f3555339&partnerID=40&md5=211df2412a46d71fccbfa87e77089476,[No abstract available],,
A Fine-grained View on Stable Many-to-one Matching Problems with Lower and Upper Quotas,2022,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147408494&doi=10.1145%2f3546605&partnerID=40&md5=c97c7b364ab1cb45eaa74b9426dba401,"In the NP-hard Hospital Residents problem with lower and upper quotas (HR-QLU), the goal is to find a stable matching of residents to hospitals where the number of residents matched to a hospital is either between its lower and upper quota or zero. We analyze this problem from a parameterized complexity perspective using several natural parameters such as the number of hospitals and the number of residents. Moreover, answering an open question of Biró et al. [TCS 2010], we present an involved polynomial-time algorithm that finds a stable matching (if it exists) on instances with maximum lower quota two. Alongside HR-QLU, we also consider two closely related models of independent interest, namely, the special case of HR-QLU where each hospital has only a lower quota but no upper quota and the variation of HR-QLU where hospitals do not have preferences over residents, which is also known as the House Allocation problem with lower and upper quotas. Last, we investigate the parameterized complexity of these three NP-hard problems when preferences may contain ties. © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Hospital Residents problem; House Allocation problem; lower and upper quotas; parameterized complexity; stable matching,Computational complexity; Parameterization; Polynomial approximation; A-stable; Allocation problems; Fine grained; Hospital resident problem; House allocation problem; Low and upper quota; Many-to-one; Matching problems; Parameterized complexity; Stable matching; Hospitals
Nash Social Welfare in Selfish and Online Load Balancing,2022,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143254093&doi=10.1145%2f3544978&partnerID=40&md5=1ac8e74a81561f30f61549bc60e4de48,"In load-balancing problems there is a set of clients, each wishing to select a resource from a set of permissible ones to execute a certain task. Each resource has a latency function, which depends on its workload, and a client's cost is the completion time of her chosen resource. Two fundamental variants of load-balancing problems are selfish load balancing (a.k.a. load-balancing games), where clients are non-cooperative selfish players aimed at minimizing their own cost solely, and online load balancing, where clients appear online and have to be irrevocably assigned to a resource without any knowledge about future requests. We revisit both problems under the objective of minimizing the Nash Social Welfare, i.e., the geometric mean of the clients' costs. To the best of our knowledge, despite being a celebrated welfare estimator in many social contexts, the Nash Social Welfare has not been considered so far as a benchmarking quality measure in load-balancing problems. We provide tight bounds on the price of anarchy of pure Nash equilibria and on the competitive ratio of the greedy algorithm under very general latency functions, including polynomial ones. For this particular class, we also prove that the greedy strategy is optimal, as it matches the performance of any possible online algorithm. © 2022 Association for Computing Machinery.",Congestion games; Nash social welfare; online algorithms; price of anarchy; pure Nash equilibrium,Game theory; Congestion Games; Latency function; Load balancing problem; Load-Balancing; Nash social welfare; On-line algorithms; Online load balancing; Price of anarchy; Pure Nash equilibrium; Social welfare; Benchmarking
Revenue-Optimal Deterministic Auctions for Multiple Buyers with Ordinal Preferences over Fixed-Price Items,2022,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161651309&doi=10.1145%2f3555045&partnerID=40&md5=272493bcc32e50ac0afb535bd990e66b,"In this article, we introduce a Bayesian revenue-maximizing mechanism design model where the items have fixed, exogenously given prices. Buyers are unit-demand and have an ordinal ranking over purchasing either one of these items at its given price or purchasing nothing. This model arises naturally from the assortment optimization problem, in that the single-buyer optimization problem over deterministic mechanisms reduces to deciding on an assortment of items to ""show.""We study its multi-buyer generalization in the simplest setting of single-winner auctions or, more broadly, any service-constrained environment. Our main result is that if the buyer rankings are drawn independently from Markov chain choice models, then the optimal mechanism is computationally tractable, and structurally a virtual welfare maximizer. We also show that for ranking distributions not induced by Markov chains, the optimal mechanism may not be a virtual welfare maximizer. Finally, we apply our virtual valuation notion for Markov chains, in conjunction with existing prophet inequalities, to improve algorithmic guarantees for online assortment problems. © 2022 Association for Computing Machinery.",assortment optimization; Bayesian mechanism design; prophet inequalities,Machine design; Markov processes; Sales; Assortment optimization; Bayesian; Bayesian mechanism designs; Deterministics; Fixed prices; Optimal mechanism; Optimization problems; Ordinal preference; Prophet inequality; Revenue maximizing; Optimization
Discounted Repeated Games Having Computable Strategies with No Computable Best Response under Subgame-Perfect Equilibria,2022,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130965183&doi=10.1145%2f3505585&partnerID=40&md5=f49f4e31fcf084dfc7d244fbb10ba243,"A classic result in computational game theory states that there are infinitely repeated games where one player has a computable strategy that has a best response, but no computable best response. For games with discounted payoff, the result is known to hold for a specific class of games-essentially generalizations of Prisoner's Dilemma-but until now, no necessary and sufficient condition is known. To be of any value, the computable strategy having no computable best response must be part of a subgame-perfect equilibrium, as otherwise a rational, self-interested player would not play the strategy.We give the first necessary and sufficient conditions for a two-player repeated game to have such a computable strategy with no computable best response for all discount factors above some threshold. The conditions involve existence of a Nash equilibrium of the repeated game whose discounted payoffs satisfy certain conditions involving the min-max payoffs of the underlying stage game. We show that it is decidable in polynomial time in the size of the payoff matrix of whether it satisfies these conditions.  © 2022 Association for Computing Machinery.",best response strategies; discounted payoff; Nash equilibria; Repeated games; subgame-perfect equilibria,Computation theory; Polynomial approximation; Best response; Best response strategy; Computational game theories; Condition; Discounted payoff; Generalisation; Nash equilibria; Repeated games; Response strategies; Specific class; Game theory
"Routing Games in the Wild: Efficiency, Equilibration, Regret, and a Price of Anarchy Bound via Long Division",2022,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130982004&doi=10.1145%2f3512747&partnerID=40&md5=289f4c3e62158bfd11ac1a9dee6edf62,"Routing games are amongst the most well studied domains of game theory. How relevant are these pen-And-paper calculations to understanding the reality of everyday traffic routing? We focus on a semantically rich dataset that captures detailed information about the daily behavior of thousands of Singaporean commuters and examine the following basic questions:-Does the traffic stabilize?-Is the system behavior consistent with latency-minimizing agents?-Is the resulting system efficient?In order to capture the efficiency of the traffic network in a way that agrees with our everyday intuition we introduce a new metric, the Free-flow Index (FFI), which reflects the inefficiency resulting from system congestion. Along the way, we provide the first model-free computation of an upper bound to the price of anarchy utilizing only real world measurements of traffic data.  © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",behavioral game theory; Congestion games; crowdsensing,Computation theory; Efficiency; Traffic congestion; Behavioral game theory; Congestion Games; Crowdsensing; Daily behaviors; Free flow; Price of anarchy; Routings; System behaviors; Traffic networks; Traffic routing; Game theory
Routing Games with Edge Priorities,2022,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130910684&doi=10.1145%2f3488268&partnerID=40&md5=5319f04470a3bb7b4be43ffcddc686f4,"Routing games over time are widely studied due to various applications, e.g., transportation, road and air traffic control, logistic in production systems, communication networks like the internet, and financial flows. In this article, we present a new competitive packet routing game with edge priorities motivated by traffic and transportation. In this model a set of selfishly acting players travels through the network over time. If the number of players who want to enter an edge at the same time exceeds the inflow capacity of this edge, then edge priorities with respect to the preceding edge are used to resolve these conflicts, which is similar to right-of-way rules in traffic. We analyze the efficiency of pure Nash equilibria, present an efficient algorithm for computing equilibria in symmetric games, and show that it is NP-hard to decide whether a Nash equilibrium exists in an asymmetric game. Furthermore, we address the problem of constructing optimal priorities.  © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",complexity; Nash equilibrium; Packet routing; price of anarchy; priority policy,Air traffic control; Computation theory; Computer games; Air traffics; Complexity; Nash equilibria; Packet routing; Price of anarchy; Priority policies; Production system; Road traffic; Routings; Transportation traffic; Game theory
Redesigning Bitcoin's Fee Market,2022,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130949455&doi=10.1145%2f3530799&partnerID=40&md5=b31e92b4f3d874ef6d04ab29b055e4f0,"The Bitcoin payment system involves two agent types: users that transact with the currency and pay fees and miners in charge of authorizing transactions and securing the system in return for these fees. Two of Bitcoin's challenges are (i) securing sufficient miner revenues as block rewards decrease, and (ii) alleviating the throughput limitation due to a small maximal block size cap. These issues are strongly related as increasing the maximal block size may decrease revenue due to Bitcoin's pay-your-bid approach. To decouple them, we analyze the ""monopolistic auction""[16], showing (i) its revenue does not decrease as the maximal block size increases, (ii) it is resilient to an untrusted auctioneer (the miner), and (iii) simplicity for transaction issuers (bidders), as the average gain from strategic bid shading (relative to bidding one's value) diminishes as the number of bids increases.  © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",auction-Theory; Bitcoin; blockchain; cryptocurrency; fee-market,Bitcoin; Miners; Auction theory; Block sizes; Block-chain; Cryptocurrency; Fee-market; Monopolistics; Payment systems; Two agents; Blockchain
Cost Sharing over Combinatorial Domains,2022,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130964705&doi=10.1145%2f3505586&partnerID=40&md5=1a864fbd8d28ad2517236b888ad886c9,"We study the problem of designing cost-sharing mechanisms for combinatorial domains. Suppose that multiple items or services are available to be shared among a set of interested agents. The outcome of a mechanism in this setting consists of an assignment, determining for each item the set of players who are granted service, together with respective payments. Although there are several works studying specialized versions of such problems, there has been almost no progress for general combinatorial cost-sharing domains until recently [9]. Still, many questions about the interplay between strategyproofness, cost recovery, and economic efficiency remain unanswered.The main goal of our work is to further understand this interplay in terms of budget balance and social cost approximation. Towards this, we provide a refinement of cross-monotonicity (which we term trace-monotonicity) that is applicable to iterative mechanisms. The trace here refers to the order in which players become finalized. On top of this, we also provide two parameterizations (complementary to a certain extent) of cost functions, which capture the behavior of their average cost-shares.Based on our trace-monotonicity property, we design an Iterative Ascending Cost-Sharing Mechanism, which is applicable to the combinatorial cost-sharing setting with symmetric submodular valuations. Using our first cost function parameterization, we identify conditions under which our mechanism is weakly group-strategyproof,-budget-balanced, and-Approximate with respect to the social cost. Furthermore, we show that our mechanism is budget-balanced and-Approximate if both the valuations and the cost functions are symmetric submodular; given existing impossibility results, this is best possible.Finally, we consider general valuation functions and exploit our second parameterization to derive a more fine-grained analysis of the Sequential Mechanism introduced by Moulin. This mechanism is budget balanced by construction, but in general, only guarantees a poor social cost approximation of . We identify conditions under which the mechanism achieves improved social cost approximation guarantees. In particular, we derive improved mechanisms for fundamental cost-sharing problems, including Vertex Cover and Set Cover.  © 2022 Association for Computing Machinery.",Cost-sharing; social cost approximation; strategy-proofness; subadditive functions; symmetric functions,Budget control; Cost functions; Costs; Parameterization; Cost sharing; Cost sharing mechanism; Cost-function; Social cost; Social cost approximation; Strategy-proofness; Subadditive function; Submodular; Symmetric functions; Symmetrics; Cost effectiveness
On Communication Complexity of Fixed Point Computation,2021,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118632749&doi=10.1145%2f3485004&partnerID=40&md5=06680ae4768e06f49b5204c3ae1e5492,"Brouwer's fixed point theorem states that any continuous function from a compact convex space to itself has a fixed point. Roughgarden and Weinstein (FOCS 2016) initiated the study of fixed point computation in the two-player communication model, where each player gets a function from to , and their goal is to find an approximate fixed point of the composition of the two functions. They left it as an open question to show a lower bound of for the (randomized) communication complexity of this problem, in the range of parameters which make it a total search problem. We answer this question affirmatively. Additionally, we introduce two natural fixed point problems in the two-player communication model.Each player is given a function from to , and their goal is to find an approximate fixed point of the concatenation of the functions.Each player is given a function from to , and their goal is to find an approximate fixed point of the mean of the functions. We show a randomized communication complexity lower bound of for these problems (for some constant approximation factor). Finally, we initiate the study of finding a panchromatic simplex in a Sperner-coloring of a triangulation (guaranteed by Sperner's lemma) in the two-player communication model: A triangulation of the-simplex is publicly known and one player is given a set and a coloring function from to , and the other player is given a set and a coloring function from to , such that , and their goal is to find a panchromatic simplex. We show a randomized communication complexity lower bound of for the aforementioned problem as well (when is large). On the positive side, we show that if then there is a deterministic protocol for the Sperner problem with bits of communication. © 2021 Association for Computing Machinery.",Brouwer's fixed point theorem; communication complexity; Nash equilibrium; Sperner's lemma,Computational complexity; Functional analysis; Information theory; Topology; Triangulation; Approximate fixed point; Brouwer fixed point theorem; Communication complexity; Communications modeling; Continuous functions; Convex spaces; Fixed-point computation; Low bound; Nash equilibria; Sperner's lemma; Fixed point arithmetic
Matchings under Preferences: Strength of Stability and Tradeoffs,2021,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118635870&doi=10.1145%2f3485000&partnerID=40&md5=a152665b33e7a13a5b8c5b817bcc479f,"We propose two solution concepts for matchings under preferences: Robustness and near stability. The former strengthens while the latter relaxes the classical definition of stability by Gale and Shapley (1962). Informally speaking, robustness requires that a matching must be stable in the classical sense, even if the agents slightly change their preferences. Near stability, however, imposes that a matching must become stable (again, in the classical sense) provided the agents are willing to adjust their preferences a bit. Both of our concepts are quantitative; together they provide means for a fine-grained analysis of the stability of matchings. Moreover, our concepts allow the exploration of tradeoffs between stability and other criteria of social optimality, such as the egalitarian cost and the number of unmatched agents. We investigate the computational complexity of finding matchings that implement certain predefined tradeoffs. We provide a polynomial-time algorithm that, given agent preferences, returns a socially optimal robust matching (if it exists), and we prove that finding a socially optimal and nearly stable matching is computationally hard. © 2021 Association for Computing Machinery.",approximation algorithms; concepts of stability; exact algorithms; NP-hardness; parameterized complexity analysis; Stable matchings,Commerce; Polynomial approximation; Stability criteria; Complexity analysis; Concept of stability; Exact algorithms; Matchings; NP-hardness; Parameterized complexity; Parameterized complexity analyse; Shapley; Solution concepts; Stable matching; Approximation algorithms
Finding Fair and Efficient Allocations for Matroid Rank Valuations,2021,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118624381&doi=10.1145%2f3485006&partnerID=40&md5=ac88e69e09612d399a41d913a4757ee5,"In this article, we present new results on the fair and efficient allocation of indivisible goods to agents whose preferences correspond to matroid rank functions. This is a versatile valuation class with several desirable properties (such as monotonicity and submodularity), which naturally lends itself to a number of real-world domains. We use these properties to our advantage; first, we show that when agent valuations are matroid rank functions, a socially optimal (i.e., utilitarian social welfare-maximizing) allocation that achieves envy-freeness up to one item (EF1) exists and is computationally tractable. We also prove that the Nash welfare-maximizing and the leximin allocations both exhibit this fairness/efficiency combination by showing that they can be achieved by minimizing any symmetric strictly convex function over utilitarian optimal outcomes. To the best of our knowledge, this is the first valuation function class not subsumed by additive valuations for which it has been established that an allocation maximizing Nash welfare is EF1. Moreover, for a subclass of these valuation functions based on maximum (unweighted) bipartite matching, we show that a leximin allocation can be computed in polynomial time. Additionally, we explore possible extensions of our results to fairness criteria other than EF1 as well as to generalizations of the above valuation classes. © 2021 Association for Computing Machinery.",Envy-freeness; fair division; matroid rank valuations; Pareto-optimality; resource allocation,Functions; Pareto principle; Efficient allocations; Envy-freeness; Fair allocation; Fair divisions; Leximin; Matroid rank valuation; Pareto-optimality; Property; Rank functions; Resources allocation; Polynomial approximation
Sequential Equilibrium in Games of Imperfect Recall,2021,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118683619&doi=10.1145%2f3485002&partnerID=40&md5=f81ea7b112bfa7de2e30f0a25185c24c,"Although the definition of sequential equilibrium can be applied without change to games of imperfect recall, doing so leads to arguably inappropriate results. We redefine sequential equilibrium so that the definition agrees with the standard definition in games of perfect recall while still giving reasonable results in games of imperfect recall. The definition can be viewed as trying to capture a notion of ex ante sequential equilibrium. The picture here is that players choose their strategies before the game starts and are committed to it, but they choose it in such a way that it remains optimal even off the equilibrium path. A notion of interim sequential equilibrium is also considered. © 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Absentmindedness; imperfect recall; sequential equilibrium,Absentmindedness; Equilibrium path; Ex antes; Imperfect recall; Perfect recalls; Sequential equilibrium; Standard definitions
Two-Sided Random Matching Markets: Ex-Ante Equivalence of the Deferred Acceptance Procedures,2021,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118622811&doi=10.1145%2f3485010&partnerID=40&md5=628a19e7fa7b97a7729ff02528002980,"Stable matching in a community consisting of men and women is a classical combinatorial problem that has been the subject of intense theoretical and empirical study since its introduction in 1962 in a seminal work by Gale and Shapley.When the input preference profile is generated from a distribution, we study the output distribution of two stable matching procedures: Women-proposing-deferred-acceptance and men-proposing-deferred-acceptance. We show that the two procedures are ex-ante equivalent-that is, under certain conditions on the input distribution, their output distributions are identical.In terms of technical contributions, we generalize (to the non-uniform case) an integral formula, due to Knuth and Pittel, which gives the probability that a fixed matching is stable. Using an inclusion-exclusion principle on the set of rotations, we give a new formula that gives the probability that a fixed matching is the women/men-optimal stable matching. © 2021 Association for Computing Machinery.",Deferred acceptance; Distributive lattice; Random input model; Stable matching,Deferred acceptance; Distributive lattice; Ex antes; Input models; Matchings; Output distribution; Random input; Random input model; Random matching; Stable matching
The Fluid Mechanics of Liquid Democracy,2021,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118679178&doi=10.1145%2f3485012&partnerID=40&md5=10a208a186544ed317f3345e619eb6f1,"Liquid democracy is the principle of making collective decisions by letting agents transitively delegate their votes. Despite its significant appeal, it has become apparent that a weakness of liquid democracy is that a small subset of agents may gain massive influence. To address this, we propose to change the current practice by allowing agents to specify multiple delegation options instead of just one. Much like in nature, where-fluid mechanics teaches us-liquid maintains an equal level in connected vessels, we seek to control the flow of votes in a way that balances influence as much as possible. Specifically, we analyze the problem of choosing delegations to approximately minimize the maximum number of votes entrusted to any agent by drawing connections to the literature on confluent flow. We also introduce a random graph model for liquid democracy and use it to demonstrate the benefits of our approach both theoretically and empirically. © 2021 Association for Computing Machinery.",democratic participation; Liquid democracy; power of two choices; preferential attachment; random graphs,Fluid mechanics; Liquids; Collective decision; Current practices; Democratic participation; Liquid democracy; Power of two choice; Power-of-two; Preferential attachments; Random graph models; Random graphs; Graph theory
On the Value of Penalties in Time-Inconsistent Planning,2021,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113160173&doi=10.1145%2f3456768&partnerID=40&md5=274c0656d371427d23fdcdd521ff884d,"People tend to behave inconsistently over time due to an inherent present bias. As this may impair performance, social and economic settings need to be adapted accordingly. Common tools to reduce the impact of time-inconsistent behavior are penalties and prohibition. Such tools are called commitment devices. In recent work Kleinberg and Oren [6, 7] connect the design of prohibition-based commitment devices to a combinatorial problem in which edges are removed from a task graph G with n nodes. However, this problem is NP-hard to approximate within a ratio less than n/3 [2]. To address this issue, we propose a penalty-based commitment device that does not delete edges but raises their cost. The benefits of our approach are twofold. On the conceptual side, we show that penalties are up to 1/β times more efficient than prohibition, where β μ 0,1] parameterizes the present bias. On the computational side, we significantly improve approximability by presenting a 2-approximation algorithm for allocating the penalties. To complement this result, we prove that optimal penalties are NP-hard to approximate within a ratio of 1.08192.  © 2021 Owner/Author.",Behavioral economics; computational complexity; incentive design; present-biased planning; time-inconsistent behavior,Graph theory; NP-hard; Approximability; Combinatorial problem; Economic setting; Optimal penalty; Task graph; Law enforcement
Optimal Taxes in Atomic Congestion Games,2021,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108101251&doi=10.1145%2f3457168&partnerID=40&md5=9dfd281ff9ad930ec94d56ec92900f7d,"How can we design mechanisms to promote efficient use of shared resources? Here, we answer this question in relation to the well-studied class of atomic congestion games, used to model a variety of problems, including traffic routing. Within this context, a methodology for designing tolling mechanisms that minimize the system inefficiency (price of anarchy) exploiting solely local information is so far missing in spite of the scientific interest. In this article, we resolve this problem through a tractable linear programming formulation that applies to and beyond polynomial congestion games. When specializing our approach to the polynomial case, we obtain tight values for the optimal price of anarchy and corresponding tolls, uncovering an unexpected link with load balancing games. We also derive optimal tolling mechanisms that are constant with the congestion level, generalizing the results of Caragiannis et al. [8] to polynomial congestion games and beyond. Finally, we apply our techniques to compute the efficiency of the marginal cost mechanism. Surprisingly, optimal tolling mechanism using only local information perform closely to existing mechanism that utilize global information, e.g., Bilò and Vinci [6], while the marginal cost mechanism, known to be optimal in the continuous-flow model, has lower efficiency than that encountered levying no toll. All results are tight for pure Nash equilibria and extend to coarse correlated equilibria.  © 2021 ACM.",approximation algorithms; coarse correlated equilibrium; Congestion games; Nash equilibrium; optimal taxation mechanisms; price of anarchy; selfish routing,Efficiency; Linear programming; Continuous flows; Correlated equilibria; Design mechanisms; Global informations; Linear programming formulation; Local information; Price of anarchy; Pure Nash equilibrium; Polynomials
Characterizing the Usage Intensity of Public Cloud,2021,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113137391&doi=10.1145%2f3456760&partnerID=40&md5=5b6830d508b3a89b0ab13a4ad8e77870,"This article uses precise and novel data on country-level Cloud IaaS and PaaS revenue to measure the intensive margin of technology diffusion across countries and within countries over time. We horse race diffusion models and find that cloud diffusion exhibits both Log-Log and Logistic Growth patterns. We use cross validation on nearly 100 features to determine what correlates with cross-country differences. We find that increases in features impacting Gross Domestic Product, Internet Connectivity, and Human Capital are associated with increases in intensity of cloud adoption. We finally compare the relative impacts of these variables using a random coefficients model. Although correlative, our algorithmic research design motivates data-driven hypothesis generation and further causal work regarding how policymakers can encourage more cloud computing adoption and technology adoption more broadly.  © 2021 ACM.",Cloud computing; technology diffusion,Economic and social effects; Algorithmic research; Country differences; Gross domestic products; Hypothesis generation; Internet connectivity; Random coefficients model; Technology adoption; Technology diffusion; Diffusion
Weighted Envy-freeness in Indivisible Item Allocation,2021,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120761856&doi=10.1145%2f3457166&partnerID=40&md5=3affa811c956d072dc893baff33f6bfe,"We introduce and analyze new envy-based fairness concepts for agents with weights that quantify their entitlements in the allocation of indivisible items. We propose two variants of weighted envy-freeness up to one item (WEF1): strong, where envy can be eliminated by removing an item from the envied agent's bundle, and weak, where envy can be eliminated either by removing an item (as in the strong version) or by replicating an item from the envied agent's bundle in the envying agent's bundle. We show that for additive valuations, an allocation that is both Pareto optimal and strongly WEF1 always exists and can be computed in pseudo-polynomial time; moreover, an allocation that maximizes the weighted Nash social welfare may not be strongly WEF1, but it always satisfies the weak version of the property. Moreover, we establish that a generalization of the round-robin picking sequence algorithm produces in polynomial time a strongly WEF1 allocation for an arbitrary number of agents; for two agents, we can efficiently achieve both strong WEF1 and Pareto optimality by adapting the adjusted winner procedure. Our work highlights several aspects in which weighted fair division is richer and more challenging than its unweighted counterpart.  © 2021 ACM.",Envy-freeness; fair division; resource allocation; unequal entitlements,Pareto principle; Envy-freeness; Fair divisions; Generalisation; Pareto-optimal; Polynomial-time; Property; Resources allocation; Round Robin; Social welfare; Unequal entitlement; Polynomial approximation
Allocation Problems in Ride-sharing Platforms: Online Matching with Offline Reusable Resources,2021,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113151457&doi=10.1145%2f3456756&partnerID=40&md5=b1ef44e8ebd657bbb060a9c276f754c2,"Bipartite-matching markets pair agents on one side of a market with agents, items, or contracts on the opposing side. Prior work addresses online bipartite-matching markets, where agents arrive over time and are dynamically matched to a known set of disposable resources. In this article, we propose a new model, Online Matching with (offline) Reusable Resources under Known Adversarial Distributions (OM-RR-KAD), in which resources on the offline side are reusable instead of disposable; that is, once matched, resources become available again at some point in the future. We show that our model is tractable by presenting an LP-based non-adaptive algorithm that achieves an online competitive ratio of ½- μ for any given constant μ > 0. We also show that no adaptive algorithm can achieve a ratio of ½ + o(1) based on the same benchmark LP. Through a data-driven analysis on a massive openly available dataset, we show our model is robust enough to capture the application of taxi dispatching services and ride-sharing systems. We also present heuristics that perform well in practice.  © 2021 ACM.",matching markets; Online-matching; randomized algorithms; rideshare,Adaptive algorithms; Commerce; Taxicabs; Allocation problems; Bipartite matchings; Data-driven analysis; Disposable resources; Nonadaptive algorithm; On-line matching; Online competitive ratios; Reusable resources; Computer software reusability
A QPTAS for ϵ-Envy-Free Profit-Maximizing Pricing on Line Graphs,2021,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113172998&doi=10.1145%2f3456762&partnerID=40&md5=d5ab779dd25121184ee2b0e1ddb76211,"We consider the problem of pricing edges of a line graph so as to maximize the profit made from selling intervals to single-minded customers. An instance is given by a set E of n edges with a limited supply for each edge, and a set of m clients, where each client specifies one interval of E she is interested in and a budget Bj which is the maximum price she is willing to pay for that interval. An envy-free pricing is one in which every customer is allocated an (possibly empty) interval maximizing her utility. Grandoni and Rothvoss (SIAM J. Comput. 2016) proposed a polynomial-time approximation scheme (PTAS) for the unlimited supply case with running time (nm)O((1/ϵ) 1/ϵ), which was extended to the limited supply case by Grandoni and Wiese (ESA 2019). By utilizing the known hierarchical decomposition of doubling metrics, we give a PTAS with running time (nm)O(1/ ϵ2) for the unlimited supply case. We then consider the limited supply case, and the notion of ϵ-envy-free pricing in which a customer gets an allocation maximizing her utility within an additive error of ϵ. For this case, we develop an approximation scheme with running time (nm)O(log 5/2maxeHe/ϵ3), where He= Bmax(e)/Bmin(e) is the maximum ratio of the budgets of any two customers demanding edge e. This yields a PTAS in the uniform budget case, and a quasi-PTAS for the general case. The best approximation known, in both cases, for the exact envy-free pricing version is O(log cmax), where cmax is the maximum item supply. Our method is based on the known hierarchical decomposition of doubling metrics, and can be applied to other problems, such as the maximum feasible subsystem problem with interval matrices.  © 2021 ACM.",Algorithmic pricing; approximation scheme; hierarchical decomposition; highway problem; limited supply,Budget control; Graph theory; Polynomial approximation; Profitability; Sales; Approximation scheme; Best approximations; Doubling metrics; Envy-free pricing; Hierarchical decompositions; Interval matrix; Maximum feasible subsystem problem; Polynomial time approximation schemes; Costs
ER-complete Decision Problems about (Symmetric) Nash Equilibria in (Symmetric) Multi-player Games,2021,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113188529&doi=10.1145%2f3456758&partnerID=40&md5=34d73644b50c1abd722020df37f546a9,"A decision problem about Nash equilibria is concerned with whether a given game has a Nash equilibrium with certain natural properties. We settle the complexity of such decision problems over multi-player games, establishing that (nearly) all decision problems that were before shown NP-complete over 2-player games in References [5, 12, 18] become -complete over multi-player games. [27] is the class capturing the complexity of deciding the Existential Theory of the Reals. Specifically, we present a simple, unifying, parametric polynomial time reduction from the problem of deciding, given a 3-player (symmetric) game, whether there is a (symmetric) Nash equilibrium where all strategies played with non-zero probability come from a given set, which was shown -complete in Reference [17]. By suitably working on the tuning parameters, our reduction delivers two extensive catalogs of -complete decision problems in multi-player games. The first addresses Nash equilibria in general games, while the second encompasses symmetric Nash equilibria in symmetric games. These results resolve completely the main open problem from Reference [17] to enlarge the class of ., -complete decision problems about (symmetric) Nash equilibria in multi-player (symmetric) games.  © 2021 ACM.",existential theory of the reals; Multi-player games; Nash equilibria,Game theory; Polynomial approximation; Decision problems; Multiplayer games; Nash equilibria; Natural properties; Non-zero probability; Polynomial-time reduction; Symmetric games; Tuning parameter; Metal drawing
A Simple Mechanism for a Budget-Constrained Buyer,2021,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106899787&doi=10.1145%2f3434419&partnerID=40&md5=a8f6d3c650fcd413bcd4a0e6ad7e7891,"We study a classic Bayesian mechanism design setting of monopoly problem for an additive buyer in the presence of budgets. In this setting, a monopolist seller with m heterogeneous items faces a single buyer and seeks to maximize her revenue. The buyer has a budget and additive valuations drawn independently for each item from (non-identical) distributions. We show that when the buyer's budget is publicly known, it is better to sell each item separately; selling the grand bundle extracts a constant fraction of the optimal revenue. When the budget is private, we consider a standard Bayesian setting where buyer's budget b is drawn from a known distribution B. We show that if b is independent of the valuations (which is necessary) and distribution B satisfies monotone hazard rate condition, then selling items separately or in a grand bundle is still approximately optimal. © 2021 ACM.",Auctions; budget; revenue maximization; simple mechanisms,Additives; Machine design; Sales; Bayesian; Bayesian mechanism designs; Hazard rates; Non-identical; Budget control
"Ordinal Approximation for Social Choice, Matching, and Facility Location Problems Given Candidate Positions",2021,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106948969&doi=10.1145%2f3434417&partnerID=40&md5=cb7309285c83362ba727da538c50d9d9,"In this work, we consider general facility location and social choice problems, in which sets of agents A and facilities F are located in a metric space, and our goal is to assign agents to facilities (as well as choose which facilities to open) to optimize the social cost. We form new algorithms to do this in the presence of only ordinal information, i.e., when the true costs or distances of the agents from the facilities are unknown, and only the ordinal preferences of the agents for the facilities are available. The main difference between our work and previous work in this area is that, while we assume that only ordinal information about agent preferences is known, we also know the exact locations of the possible facilities F. Due to this extra information about the facilities, we are able to form powerful algorithms that have small distortion, i.e., perform almost as well as omniscient algorithms (which know the true numerical distances between agents and facilities) but use only ordinal information about agent preferences. For example, we present natural social choice mechanisms for choosing a single facility to open with distortion of at most 3 for minimizing both the total and the median social cost; this factor is provably the best possible. We analyze many general problems including matching, k-center, and k-median, and we present black-box reductions from omniscient approximation algorithms with approximation factor β to ordinal algorithms with approximation factor 1+2β doing this gives new ordinal algorithms for many important problems and establishes a toolkit for analyzing such problems in the future. © 2021 ACM.",approximation algorithms; distortion; facility location; matching; ordinal information; Social choice,Approximation algorithms; Location; Approximation factor; Black-box reductions; Candidate positions; Facility location problem; Facility locations; Ordinal information; Ordinal preference; Social choice problems; Software agents
The Fair Division of Hereditary Set Systems,2021,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106894731&doi=10.1145%2f3434410&partnerID=40&md5=88e25e8d44ee14be2d773bfee7f448e5,"We consider the fair division of indivisible items using the maximin shares measure. Recent work on the topic has focused on extending results beyond the class of additive valuation functions. In this spirit, we study the case where the items form a hereditary set system. We present a simple algorithm that allocates each agent a bundle of items whose value is at least 0.3666 times the maximin share of the agent. This improves upon the current best known guarantee of 0.2 due to Ghodsi et al. The analysis of the algorithm is almost tight; we present an instance where the algorithm provides a guarantee of at most 0.3738. We also show that the algorithm can be implemented in polynomial time given a valuation oracle for each agent. © 2021 ACM.",Algorithmic game theory; allocation problems; fair division,Polynomial approximation; Set theory; Fair divisions; Maximin; Polynomial-time; Set system; SIMPLE algorithm; Valuation function; Software agents
Timing Matters: Online Dynamics in Broadcast Games,2021,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106955308&doi=10.1145%2f3434425&partnerID=40&md5=f1aa04857f3b21bcec90a25ba105fa2c,"This article studies the equilibrium states that can be reached in a network design game via natural game dynamics. First, we show that an arbitrarily interleaved sequence of arrivals and departures of players can lead to a polynomially inefficient solution at equilibrium. This implies that the central controller must have some control over the timing of agent arrivals and departures to ensure efficiency of the system at equilibrium. Indeed, we give a complementary result showing that if the central controller is allowed to restore equilibrium after every set of arrivals/departures via improving moves, then the eventual equilibrium states reached have exponentially better efficiency. © 2021 ACM.",Broadcast games; game dynamics; online algorithms,Efficiency; Equilibrium state; Interleaved sequence; Network design; Timing circuits
Introduction to the Special Issue on WINE'18: Part 2,2021,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106883465&doi=10.1145%2f3447512&partnerID=40&md5=fa8c07e0700e639ac5fc2b421b235d99,[No abstract available],,
Simple and Efficient Budget Feasible Mechanisms for Monotone Submodular Valuations,2021,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103449281&doi=10.1145%2f3434421&partnerID=40&md5=cfe1d801258f1a6a60a3e83766a6ae98,"We study the problem of a budget limited buyer who wants to buy a set of items, each from a different seller, to maximize her value. The budget feasible mechanism design problem requires the design a mechanism that incentivizes the sellers to truthfully report their cost and maximizes the buyer's value while guaranteeing that the total payment does not exceed her budget. Such budget feasible mechanisms can model a buyer in a crowdsourcing market interested in recruiting a set of workers (sellers) to accomplish a task for her. This budget feasible mechanism design problem was introduced by Singer in 2010. We consider the general case where the buyer's valuation is a monotone submodular function. There are a number of truthful mechanisms known for this problem. We offer two general frameworks for simple mechanisms, and by combining these frameworks, we significantly improve on the best known results, while also simplifying the analysis. For example, we improve the approximation guarantee for the general monotone submodular case from 7.91 to 5 and for the case of large markets (where each individual item has negligible value) from 3 to 2.58. More generally, given an r approximation algorithm for the optimization problem (ignoring incentives), our mechanism is a r + 1 approximation mechanism for large markets, an improvement from 2r2. We also provide a mechanism without the large market assumption, where we achieve a 4r + 1 approximation guarantee. We also show how our results can be used for the problem of a principal hiring in a Crowdsourcing Market to select a set of tasks subject to a total budget. © 2021 ACM.",algorithmic game theory; algorithmic mechanism design; Budget feasible mechanisms; submodular valuations,Approximation algorithms; Commerce; Crowdsourcing; Machine design; Sales; A-monotone; Mechanism design; Optimization problems; R-approximation algorithms; Submodular; Submodular functions; Total budget; Truthful mechanisms; Budget control
Learning Convex Partitions and Computing Game-Theoretic Equilibria from Best-response Queries,2021,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103465700&doi=10.1145%2f3434412&partnerID=40&md5=b9a05ca35b42fa5065c21a6915658706,"Suppose that an m-simplex is partitioned into n convex regions having disjoint interiors and distinct labels, and we may learn the label of any point by querying it. The learning objective is to know, for any point in the simplex, a label that occurs within some distance ϵ from that point. We present two algorithms for this task: Constant-Dimension Generalised Binary Search (CD-GBS), which for constant m uses poly(n, log (1/ϵ)) queries, and Constant-Region Generalised Binary Search (CR-GBS), which uses CD-GBS as a subroutine and for constant n uses poly(m, log (1/ϵ)) queries. We show via Kakutani's fixed-point theorem that these algorithms provide bounds on the best-response query complexity of computing approximate well-supported equilibria of bimatrix games in which one of the players has a constant number of pure strategies. We also partially extend our results to games with multiple players, establishing further query complexity bounds for computing approximate well-supported equilibria in this setting. © 2021 ACM.",equilibrium computation; Query protocol; revealed preferences,Fixed point arithmetic; Game theory; Best response; Bimatrix games; Binary search; Disjoint interiors; Fixed point theorems; Game-theoretic; Learning objectives; Query complexity; Computer games
Combinatorial Assortment Optimization,2021,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103462040&doi=10.1145%2f3434415&partnerID=40&md5=fcfb3bcf04ce459cb22914cb0fedb6e0,"Assortment optimization refers to the problem of designing a slate of products to offer potential customers, such as stocking the shelves in a convenience store. The price of each product is fixed in advance, and a probabilistic choice function describes which product a customer will choose from any given subset. We introduce the combinatorial assortment problem, where each customer may select a bundle of products. We consider a choice model in which each consumer selects a utility-maximizing bundle subject to a private valuation function, and study the complexity of the resulting optimization problem. Our main result is an exact algorithm for additive k-demand valuations, under a model of vertical differentiation in which customers agree on the relative value of each pair of items but differ in their absolute willingness to pay. For valuations that are vertically differentiated but not necessarily additive k-demand, we show how to obtain constant approximations under a ""well-priced""condition, where each product's price is sufficiently high. We further show that even for a single customer with known valuation, any sub-polynomial approximation to the problem requires exponentially many demand queries when the valuation function is XOS and that no FPTAS exists even when the valuation is succinctly representable. © 2021 ACM.",Assortment optimization; combinatorial valuations,Additives; Polynomial approximation; Sales; Assortment optimization; Convenience stores; Optimization problems; Potential customers; Probabilistic choices; Valuation function; Vertical differentiation; Willingness to pay; Product design
Pairwise Preferences in the Stable Marriage Problem,2021,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103467213&doi=10.1145%2f3434427&partnerID=40&md5=9dacfcc368a0356b99181e90ba9e7e86,"We study the classical, two-sided stable marriage problem under pairwise preferences. In the most general setting, agents are allowed to express their preferences as comparisons of any two of their edges, and they also have the right to declare a draw or even withdraw from such a comparison. This freedom is then gradually restricted as we specify six stages of orderedness in the preferences, ending with the classical case of strictly ordered lists. We study all cases occurring when combining the three known notions of stability-weak, strong, and super-stability-under the assumption that each side of the bipartite market obtains one of the six degrees of orderedness. By designing three polynomial algorithms and two NP-completeness proofs, we determine the complexity of all cases not yet known and thus give an exact boundary in terms of preference structure between tractable and intractable cases. © 2021 ACM.",acyclic preferences; intransitivity; poset; Stable marriage; strongly stable matching; super stable matching; weakly stable matching,Np-completeness; Polynomial algorithm; Preference structures; Six stages; Stable marriage problem; Superstability; Game theory
Introduction to the Special Issue onWINE’18: Part 1,2021,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103451698&doi=10.1145%2f3447510&partnerID=40&md5=76cd84a07c277575c4008dac160af9a9,[No abstract available],,
Optimal Pricing for MHR and λ-regular Distributions,2021,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103470706&doi=10.1145%2f3434423&partnerID=40&md5=d9ba51f6d7dab7fc7bd11059c97efe38,"We study the performance of anonymous posted-price selling mechanisms for a standard Bayesian auction setting, where n bidders have i.i.d. valuations for a single item. We show that for the natural class of Monotone Hazard Rate (MHR) distributions, offering the same, take-it-or-leave-it price to all bidders can achieve an (asymptotically) optimal revenue. In particular, the approximation ratio is shown to be 1+O(ln ln n/ ln n), matched by a tight lower bound for the case of exponential distributions. This improves upon the previously best-known upper bound of e/(e-1)≈ 1.58 for the slightly more general class of regular distributions. In the worst case (over n), we still show a global upper bound of 1.35. We give a simple, closed-form description of our prices, which, interestingly enough, relies only on minimal knowledge of the prior distribution, namely, just the expectation of its second-highest order statistic. Furthermore, we extend our techniques to handle the more general class of λ-regular distributions that interpolate between MHR (λ =0) and regular (λ =1). Our anonymous pricing rule now results in an asymptotic approximation ratio that ranges smoothly, with respect to λ, from 1 (MHR distributions) to e/(e-1) (regular distributions). Finally, we explicitly give a class of continuous distributions that provide matching lower bounds, for every λ. © 2021 ACM.",hazard rate distributions; optimal auctions; Pricing; regular distributions; λ-regularity,Economics; Approximation ratios; Asymptotic approximation; Auction settings; Continuous distribution; Exponential distributions; Order statistics; Prior distribution; Regular distribution; Costs
The Revelation Principle for Mechanism Design with Signaling Costs,2021,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103449735&doi=10.1145%2f3434408&partnerID=40&md5=382d6fd6e18f4601bdf3cdc3b4659403,"The revelation principle is a key tool in mechanism design. It allows the designer to restrict attention to truthful mechanisms, greatly facilitating analysis. This is also borne out algorithmically, allowing certain computational problems in mechanism design to be solved in polynomial time. Unfortunately, when not every type can misreport every other type (the partial verification model) or-more generally-misreporting can be costly, the revelation principle can fail to hold. This also leads to NP-hardness results. The primary contribution of this article consists of characterizations of conditions under which the revelation principle still holds when reporting can be costly. (These are generalizations of conditions given earlier for the partial verification case [11, 21].) Furthermore, our results extend to cases where, instead of reporting types directly, agents send signals that do not directly correspond to types. In this case, we obtain conditions for when the mechanism designer can restrict attention to a given (but arbitrary) mapping from types to signals without loss of generality. © 2021 ACM.",Automated mechanism design; costly misrepresentation; evidence; partial verification; revelation principle; signaling costs,NP-hard; Polynomial approximation; Computational problem; Mechanism design; Np-hardness results; Partial verification; Primary contribution; Revelation principle; Signaling costs; Truthful mechanisms; Machine design
Group Fairness in Committee Selection,2020,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096986405&doi=10.1145%2f3417750&partnerID=40&md5=9d5ed8164be491b4cb3da1c48b96d088,"In this article, we study fairness in committee selection problems. We consider a general notion of fairness via stability: A committee is stable if no coalition of voters can deviate and choose a committee of proportional size, so that all these voters strictly prefer the new committee to the existing one. Our main contribution is to extend this definition to stability of a distribution (or lottery) over committees. We consider two canonical voter preference models: the Approval Set setting where each voter approves a set of candidates and prefers committees with larger intersection with this set; and the Ranking Representative setting where each voter ranks committees based on how much she likes her favorite candidate in a committee. Our main result is to show that stable lotteries always exist for these canonical preference models. Interestingly, given preferences of voters over committees, the procedure for computing an approximately stable lottery is the same for both models and therefore extends to the setting where some voters have the former preference structure and others have the latter. Our existence proof uses the probabilistic method and a new large deviation inequality that may be of independent interest.  © 2020 ACM.",committee selection; Fairness; social choice,Large deviations; Preference models; Preference structures; Probabilistic methods; Selection problems
Fair Mixing: The Case of Dichotomous Preferences,2020,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096952861&doi=10.1145%2f3417738&partnerID=40&md5=977f1792cfe5778309a0aef264ab5475,"We consider a setting in which agents vote to choose a fair mixture of public outcomes. The agents have dichotomous preferences: Each outcome is liked or disliked by an agent. We discuss three outstanding voting rules. The Conditional Utilitarian rule, a variant of the random dictator, is strategyproof and guarantees to any group of like-minded agents an influence proportional to its size. It is easier to compute and more efficient than the familiar Random Priority rule. We show, both formally and by numerical experiments, that its inefficiency is low when the number of agents is low. The efficient Egalitarian rule protects individual agents but not coalitions. It is excludable strategyproof: An agent does not want to lie if she cannot consume outcomes she claims to dislike. The efficient Max Nash Product rule offers the strongest welfare guarantees to coalitions, which can force any outcome with a probability proportional to their size. But it even fails the excludable form of strategyproofness.  © 2020 ACM.",approval voting; Participatory budgeting; portioning; proportional representation; strategyproofness; time-sharing,Individual agent; Numerical experiments; Priority rules; Probability proportional; Product rule; Strategy proofs; Strategy-proofness; Voting rules
Influence Maximization on Undirected Graphs: Toward Closing the (1 - 1/e) Gap,2020,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096988960&doi=10.1145%2f3417748&partnerID=40&md5=b7e1ba4447ba00e391c05b6ca5a6c37d,"We study the influence maximization problem in undirected networks, specifically focusing on the independent cascade and linear threshold models. We prove APX-hardness (NP-hardness of approximation within factor (1-τ) for some constant τ > 0) for both models, which improves the previous NP-hardness lower bound for the linear threshold model. No previous hardness result was known for the independent cascade model. As part of the hardness proof, we show some natural properties of these cascades on undirected graphs. For example, we show that the expected number of infections of a seed set S is upper bounded by the size of the edge cut of S in the linear threshold model and a special case of the independent cascade model, the weighted independent cascade model. Motivated by our upper bounds, we present a suite of highly scalable local greedy heuristics for the influence maximization problem on both the linear threshold model and the weighted independent cascade model on undirected graphs that, in practice, find seed sets that on average obtain 97.52% of the performance of the much slower greedy algorithm for the linear threshold model and 97.39% of the performance of the greedy algorithm for the weighted independent cascade model. Our heuristics also outperform other popular local heuristics, such as the degree discount heuristic by Chen et al. [7].  © 2020 ACM.",independent cascade model; Influence maximization; linear threshold model; local greedy heuristic,Hardness; NP-hard; Stream flow; Greedy algorithms; Greedy heuristics; Hardness of approximation; Influence maximizations; Linear threshold models; Natural properties; Undirected graph; Undirected network; Graph algorithms
The Complexity of Black-Box Mechanism Design with Priors,2020,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096994319&doi=10.1145%2f3417744&partnerID=40&md5=e8f3ea87e2e0c920b351fe0b79413274,"We study black-box reductions from mechanism design to algorithm design for welfare maximization in settings of incomplete information. Given oracle access to an algorithm for an underlying optimization problem, the goal is to simulate an incentive compatible mechanism. The mechanism will be evaluated on its expected welfare, relative to the algorithm provided, and its complexity is measured by the time (and queries) needed to simulate the mechanism on any input. While it is known that black-box reductions are not possible in many prior-free settings, settings with priors appear more promising: there are known reductions for Bayesian incentive compatible (BIC) mechanism design for general classes of welfare maximization problems. This dichotomy begs the question: which mechanism design problems admit black-box reductions, and which do not? Our main result is that black-box mechanism design is impossible under two of the simplest settings not captured by known positive results. First, for the problem of allocating n goods to a single buyer whose valuation is additive and independent across the goods, subject to a downward-closed constraint on feasible allocations, we show that there is no polytime (in n) BIC black-box reduction for expected welfare maximization. Second, for the setting of multiple single-parameter agents - where polytime BIC reductions are known - we show that no polytime reductions exist when the incentive requirement is tightened to Max-In-Distributional-Range. In each case, we show that achieving a sub-polynomial approximation to the expected welfare requires exponentially many queries, even when the set of feasible allocations is known to be downward-closed.  © 2020 ACM.",Black-box reductions; mechanism design; welfare maximization,Polynomial approximation; Algorithm design; Black-box reductions; Incentive compatible; Incentive compatible mechanisms; Incomplete information; Optimization problems; Single-parameter agents; Welfare maximizations; Machine design
Optimal Budget-Feasible Mechanisms for Additive Valuations,2020,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096990479&doi=10.1145%2f3417746&partnerID=40&md5=9b996f0e296eb8f8c7ded8e1546438f9,"In this article, we show a tight approximation guarantee for budget-feasible mechanisms with an additive buyer. We propose a new simple randomized mechanism with approximation ratio of 2, improving the previous best known result of 3. Our bound is tight with respect to either the optimal offline benchmark or its fractional relaxation. We also present a simple deterministic mechanism with the tight approximation guarantee of 3 against the fractional optimum, improving the best known result of (2+ √ 2) for the weaker integral benchmark.  © 2020 ACM.",approximation; Budget-feasible auction; mechanism design,Additives; Approximation ratios; Deterministic mechanism; Offline; Randomized mechanism; Budget control
Introduction to the Special Issue on EC'19,2020,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096994885&doi=10.1145%2f3424651&partnerID=40&md5=6e9fe31ce93c057260c30ead86f745e8,[No abstract available],,
How Do Classifiers Induce Agents to Invest Effort Strategically?,2020,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096994805&doi=10.1145%2f3417742&partnerID=40&md5=9f7d69ef62ab63d9019e6fe15bf2c9c0,"Algorithms are often used to produce decision-making rules that classify or evaluate individuals. When these individuals have incentives to be classified a certain way, they may behave strategically to influence their outcomes. We develop a model for how strategic agents can invest effort in order to change the outcomes they receive, and we give a tight characterization of when such agents can be incentivized to invest specified forms of effort into improving their outcomes as opposed to ""gaming""the classifier. We show that whenever any ""reasonable""mechanism can do so, a simple linear mechanism suffices.  © 2020 ACM.",effort allocation; Principal-agent; strategic classification,Decision-making rules; Decision making
Voting and Bribing in Single-Exponential Time,2020,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85093653671&doi=10.1145%2f3396855&partnerID=40&md5=f77db6e99c09e799d659a5c650a5f2fd,"We introduce a general problem about bribery in voting systems. In the R-Multi-Bribery problem, the goal is to bribe a set of voters at minimum cost such that a desired candidate is a winner in the perturbed election under the voting rule R. Voters assign prices for withdrawing their vote, for swapping the positions of two consecutive candidates in their preference order, and for perturbing their approval count to favour candidates. As our main result, we show that R-Multi-Bribery is fixed-parameter tractable parameterized by the number of candidates |C| with only a single-exponential dependence on |C|, for many natural voting rules R, including all natural scoring protocols, maximin rule, Bucklin rule, Fallback rule, SP-AV, and any C1 rule. The vast majority of previous work done in the setting of few candidates proceeds by grouping voters into at most |C|! types by their preference, constructing an integer linear program with |C|!2 variables, and solving it by Lenstra's algorithm in time |C|!|C|!2, hence double-exponential in |C|. Note that it is not possible to encode a large number of different voter costs in this way and still obtain a fixed-parameter algorithm, as that would increase the number of voter types and hence the dimension. These two obstacles of double-exponential complexity and restricted costs have been formulated as ""Challenges #1 and #2""of the ""Nine Research Challenges in Computational Social Choice""by Bredereck et al. Hence, our result resolves the parameterized complexity of R-Swap-Bribery for the aforementioned voting rules plus Kemeny's rule, and for all rules except Kemeny brings the dependence on |C| down to single-exponential. The engine behind our progress is the use of a new integer linear programming formulation, using so-called ""n-fold integer programming.""Since its format is quite rigid, we introduce ""extended n-fold IP,""which allows many useful modeling tricks. Then, we model R-Multi-Bribery as an extended n-fold IP and apply an algorithm of Hemmecke et al. [Math. Prog. 2013].  © 2020 Owner/Author.",integer programming; swap bribery; Voting system,Costs; Internet protocols; Computational social choices; Double exponential; Exponential dependence; Fixed-parameter algorithms; Integer linear programming formulation; Integer linear programs; Parameterized complexity; Research challenges; Integer programming
How Effective Can Simple Ordinal Peer Grading Be?,2020,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85093702095&doi=10.1145%2f3412347&partnerID=40&md5=b84faf379d553d6c39e79865793c0902,"Ordinal peer grading has been proposed as a simple and scalable solution for computing reliable information about student performance in massive open online courses. The idea is to outsource the grading task to the students themselves as follows. After the end of an exam, each student is asked to rank - in terms of quality - a bundle of exam papers by fellow students. An aggregation rule then combines the individual rankings into a global one that contains all students. We define a broad class of simple aggregation rules, which we call type-ordering aggregation rules, and present a theoretical framework for assessing their effectiveness. When statistical information about the grading behaviour of students is available (in terms of a noise matrix that characterizes the grading behaviour of the average student from a student population), the framework can be used to compute the optimal rule from this class with respect to a series of performance objectives that compare the ranking returned by the aggregation rule to the underlying ground-truth ranking. For example, a natural rule known as Borda is proved to be optimal when students grade correctly. In addition, we present extensive simulations that validate our theory and prove it to be extremely accurate in predicting the performance of aggregation rules even when only rough information about grading behaviour (i.e., an approximation of the noise matrix) is available. Both in the application of our theoretical framework and in our simulations, we exploit data about grading behaviour of students that have been extracted from two field experiments in the University of Patras.  © 2020 ACM.",MOOCs; ordinal peer grading; rank aggregation; social choice theory,Computation theory; Distance education; Matrix algebra; Population statistics; Students; Extensive simulations; Massive open online course; Performance objective; Scalable solution; Statistical information; Student performance; Student populations; Theoretical framework; Grading
The Possibilities and Limitations of Private Prediction Markets,2020,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85093650520&doi=10.1145%2f3412348&partnerID=40&md5=142554ee0aa532d2c5795f88d4a094d0,"We consider the design of private prediction markets, financial markets designed to elicit predictions about uncertain events without revealing too much information about market participants' actions or beliefs. Our goal is to design market mechanisms in which participants' trades or wagers influence the market's behavior in a way that leads to accurate predictions, yet no single participant has too much influence over what others are able to observe. We study the possibilities and limitations of such mechanisms using tools from differential privacy. We begin by designing a private one-shot wagering mechanism in which bettors specify a belief about the likelihood of a future event and a corresponding monetary wager. Wagers are redistributed among bettors in a way that more highly rewards those with accurate predictions. We provide a class of wagering mechanisms that are guaranteed to satisfy truthfulness, budget balance on expectation, and other desirable properties while additionally guaranteeing ϵ-joint differential privacy in the bettors' reported beliefs, and analyze the trade-off between the achievable level of privacy and the sensitivity of a bettor's payment to her own report. We then ask whether it is possible to obtain privacy in dynamic prediction markets, focusing our attention on the popular cost-function framework in which securities with payments linked to future events are bought and sold by an automated market maker. We show that under general conditions, it is impossible for such a market maker to simultaneously achieve bounded worst-case loss and ϵ-differential privacy without allowing the privacy guarantee to degrade extremely quickly as the number of trades grows (at least logarithmically in number of trades), making such markets impractical in settings in which privacy is valued. We conclude by suggesting several avenues for potentially circumventing this lower bound.  © 2020 ACM.",cost function market maker; Differential privacy; prediction markets; wagering mechanisms,Budget control; Cost functions; Economic and social effects; Forecasting; Uncertainty analysis; Accurate prediction; Design markets; Differential privacies; Dynamic prediction; Market participants; Number of trades; Prediction markets; Uncertain events; Privacy by design
Approval Voting and Incentives in Crowdsourcing,2020,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85093648327&doi=10.1145%2f3396863&partnerID=40&md5=8ebbb3f460b5c2b5b37fbe172ef02a03,"The growing need for labeled training data has made crowdsourcing a vital tool for developing machine learning applications. Here, workers on a crowdsourcing platform are typically shown a list of unlabeled items, and for each of these items, are asked to choose a label from one of the provided options. The workers in crowdsourcing platforms are not experts, thereby making it essential to judiciously elicit the information known to the workers. With respect to this goal, there are two key shortcomings of current systems: (i) the incentives of the workers are not aligned with those of the requesters; and (ii) the interface does not allow workers to convey their knowledge accurately by forcing them to make a single choice among a set of options. In this article, we address these issues by introducing approval voting to utilize the expertise of workers who have partial knowledge of the true answer and coupling it with two strictly proper scoring rules. We additionally establish attractive properties of optimality and uniqueness of our scoring rules. We also conduct preliminary empirical studies on Amazon Mechanical Turk, and the results of these experiments validate our approach.  © 2020 Owner/Author.",incentives; labeling; Proper scoring rules,Amazon mechanical turks; Crowdsourcing platforms; Empirical studies; Labeled training data; Machine learning applications; Partial knowledge; Proper scoring rules; Scoring rules; Crowdsourcing
The Price of Quota-based Diversity in Assignment Problems,2020,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85093658932&doi=10.1145%2f3411513&partnerID=40&md5=fe11a5b30c44f8ec6cded85b07d2653b,"In this article, we introduce and analyze an extension to the matching problem on a weighted bipartite graph (i.e., the assignment problem): Assignment with Type Constraints. Here, the two parts of the graph are each partitioned into subsets, called types and blocks, respectively; we seek a matching with the largest sum of weights under the constraint that there is a pre-specified cap on the number of vertices matched in every type-block pair. Our primary motivation stems from the large-scale public housing program run by the state of Singapore, accounting for over 70% of its residential real estate. To promote ethnic diversity within its housing projects, Singapore imposes ethnicity quotas: The population is divided into ethnicity-based groups and each new housing development into blocks of flats such that each group must not own more than a certain percentage of flats in a block. However, other domains use similar hard capacity constraints to maintain diversity: These include matching prospective students to schools or medical residents to hospitals. Limiting agents' choices for ensuring diversity in this manner naturally entails some welfare loss. One of our goals is to study the tradeoff between diversity and (utilitarian) social welfare in such settings. We first show that, while the classic assignment program is polynomial-time computable, adding diversity constraints makes the problem computationally intractable; however, we identify a ½-approximation algorithm, as well as reasonable assumptions on the structure of utilities (or weights) that permit poly-time algorithms. Next, we provide two upper bounds on the price of diversity - a measure of the loss in welfare incurred by imposing diversity constraints - as functions of natural problem parameters. We conclude the article with simulations based on publicly available data from two diversity-constrained allocation problems - Singapore Public Housing and Chicago School Choice - which shed light on how the constrained maximization as well as lottery-based variants perform in practice.  © 2020 ACM.",Assignment problem; diversity constraints; price of diversity,Approximation algorithms; Combinatorial optimization; Graph theory; Polynomial approximation; Allocation problems; Assignment problems; Assignment programs; Capacity constraints; Housing development; Medical residents; Poly-time algorithms; Problem parameters; Housing
Distributed Signaling Games,2020,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085734304&doi=10.1145%2f3381529&partnerID=40&md5=2b2a35b82f9c58beba7e9c0526c8c634,"A recurring theme in recent computer science literature is that proper design of signaling schemes is a crucial aspect of effective mechanisms aiming to optimize social welfare or revenue. One of the research endeavors of this line of work is understanding the algorithmic and computational complexity of designing efficient signaling schemes. In reality, however, information is typically not held by a central authority but is distributed among multiple sources (third-party ""mediators""), a fact that dramatically changes the strategic and combinatorial nature of the signaling problem. In this article, we introduce distributed signaling games, while using display advertising as a canonical example for introducing this foundational framework. A distributed signaling game may be a pure coordination game (i.e., a distributed optimization task) or a non-cooperative game. In the context of pure coordination games, we show a wide gap between the computational complexity of the centralized and distributed signaling problems, proving that distributed coordination on revenue-optimal signaling is a much harder problem than its ""centralized"" counterpart. In the context of non-cooperative games, the outcome generated by the mediators' signals may have different value to each. The reason for that is typically the desire of the auctioneer to align the incentives of the mediators with his own by a compensation relative to the marginal benefit from their signals. We design a mechanism for this problem via a novel application of Shapley's value and show that it possesses some interesting properties; in particular, it always admits a pure Nash equilibrium, and it never decreases the revenue of the auctioneer (relative to his a priori revenue when there are no mediators). © 2020 ACM.",distributed mechanism design; Shapley value; Signaling,Computational complexity; Game theory; Parallel processing systems; Signal processing; Display advertisings; Distributed coordination; Distributed optimization; Effective mechanisms; Noncooperative game; Novel applications; Optimal signaling; Pure Nash equilibrium; Signaling
Existence and Efficiency of Equilibria for Cost-Sharing in Generalized Weighted Congestion Games,2020,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085737003&doi=10.1145%2f3391434&partnerID=40&md5=c5b0e1552b50c8177494fb83cbc15a23,"This work studies the impact of cost-sharing methods on the existence and efficiency of (pure) Nash equilibria in weighted congestion games. We also study generalized weighted congestion games, where each player may control multiple commodities. Our results are fairly general; we only require that our cost-sharing method and our set of cost functions satisfy certain natural conditions. For general weighted congestion games, we study the existence of pure Nash equilibria in the induced games, and we exhibit a separation from the standard single-commodity per player model by proving that the Shapley value is the only cost-sharing method that guarantees existence of pure Nash equilibria. With respect to efficiency, we present general tight bounds on the price of anarchy, which are robust and apply to general equilibrium concepts. Our analysis provides a tight bound on the price of anarchy, which depends only on the used cost-sharing method and the set of allowable cost functions. Interestingly, the same bound applies to weighted congestion games and generalized weighted congestion games. We then turn to the price of stability and prove an upper bound for the Shapley value cost-sharing method, which holds for general sets of cost functions and which is tight in special cases of interest, such as bounded degree polynomials. Also for bounded degree polynomials, we provide a somewhat surprising result, showing that a slight deviation from the Shapley value has a huge impact on the price of stability. In fact, for this case, the price of stability becomes as bad as the price of anarchy. Again, our bounds on the price of stability are independent on whether players are single or multi-commodity. © 2020 ACM.",congestion games; Existence of equilibria; multi-commodity players; price of anarchy; price of stability; selfish routing; Shapley value,Cost effectiveness; Cost functions; Costs; Efficiency; Game theory; Stability; Congestion Games; Cost-sharing methods; General equilibrium; Nash equilibria; Natural conditions; Price of anarchy; Price of Stability; Pure Nash equilibrium; Cost benefit analysis
Privacy Games,2020,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085729770&doi=10.1145%2f3381533&partnerID=40&md5=b777056a1ea878d0fc3f0a6bd3a64e0f,"The problem of analyzing the effect of privacy concerns on the behavior of selfish utility-maximizing agents has received much attention lately. Privacy concerns are often modeled by altering the utility functions of agents to consider also their privacy loss [4, 14, 20, 28]. Such privacy-aware agents prefer to take a randomized strategy even in very simple games in which non-privacy-aware agents play pure strategies. In some cases, the behavior of privacy-aware agents follows the framework of Randomized Response, a well-known mechanism that preserves differential privacy. Our work is aimed at better understanding the behavior of agents in settings where their privacy concerns are explicitly given. We consider a toy setting where agent A, in an attempt to discover the secret type of agent B, offers B a gift that one type of B agent likes and the other type dislikes. As opposed to previous works, B's incentive to keep her type a secret isn't the result of ""hardwiring"" B's utility function to consider privacy, but rather takes the form of a payment between B and A. We investigate three different types of payment functions and analyze B's behavior in each of the resulting games. As we show, under some payments, B's behavior is very different than the behavior of agents with hardwired privacy concerns and might even be deterministic. Under a different payment, we show that B's BNE strategy does fall into the framework of Randomized Response. © 2020 ACM.",Bayes-Nash equilibrium; Differential privacy; privacy modeling,Differential privacies; Payment functions; Privacy aware; Privacy concerns; Randomized response; Randomized strategy; Simple games; Utility functions
Linear Regression from Strategic Data Sources,2020,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085739233&doi=10.1145%2f3391436&partnerID=40&md5=ba5fe48df4c935701a7d6f36df1970e0,"Linear regression is a fundamental building block of statistical data analysis. It amounts to estimating the parameters of a linear model that maps input features to corresponding outputs. In the classical setting where the precision of each data point is fixed, the famous Aitken/Gauss-Markov theorem in statistics states that generalized least squares (GLS) is a so-called ""Best Linear Unbiased Estimator"" (BLUE). In modern data science, however, one often faces strategic data sources; namely, individuals who incur a cost for providing high-precision data. For instance, this is the case for personal data, whose revelation may affect an individual's privacy-which can be modeled as a cost-or in applications such as recommender systems, where producing an accurate estimate entails effort. In this article, we study a setting in which features are public but individuals choose the precision of the outputs they reveal to an analyst. We assume that the analyst performs linear regression on this dataset, and individuals benefit from the outcome of this estimation. We model this scenario as a game where individuals minimize a cost composed of two components: (a) an (agent-specific) disclosure cost for providing high-precision data; and (b) a (global) estimation cost representing the inaccuracy in the linear model estimate. In this game, the linear model estimate is a public good that benefits all individuals. We establish that this game has a unique non-trivial Nash equilibrium. We study the efficiency of this equilibrium and we prove tight bounds on the price of stability for a large class of disclosure and estimation costs. Finally, we study the estimator accuracy achieved at equilibrium. We show that, in general, Aitken's theorem does not hold under strategic data sources, though it does hold if individuals have identical disclosure costs (up to a multiplicative factor). When individuals have non-identical costs, we derive a bound on the improvement of the equilibrium estimation cost that can be achieved by deviating from GLS, under mild assumptions on the disclosure cost functions. © 2020 ACM.",Aitken theorem; Gauss-Markov theorem; Linear regression; potential game; price of stability; strategic data sources,Cost estimating; Cost functions; Data privacy; Linear regression; Best linear unbiased estimator; Fundamental building blocks; Generalized least square; Linear modeling; Multiplicative factors; Nash equilibria; Price of Stability; Statistical data analysis; Cost benefit analysis
Pricing (and Bidding) Strategies for Delay Differentiated Cloud Services,2020,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085737440&doi=10.1145%2f3381531&partnerID=40&md5=3737c08399176ae959b9ecc99aaadf28,"We consider a cloud provider that seeks to maximize its revenue by offering services with different trade-offs between cost and timeliness of job completion. Spot instances and preemptible instances are examples of such services, with, in both cases, possible service interruptions delaying a job's completion. Our focus is on exploiting heterogeneity across jobs in terms of value and sensitivity to execution delay, with a joint distribution that determines their relationship across the user population. We characterize optimal (revenue maximizing) pricing strategies and, in the case of spot instances, optimal bidding strategies as well as identify conditions under which bidding at a fixed price is optimal. We show that correlation between delay sensitivity and job value needs to exceed a certain threshold for a service offering that differentiates based on speed of execution to be beneficial to the provider. We further assess the results' robustness under more general assumptions, and we offer guidelines for users and providers. © 2020 ACM.",delay sensitivity; Pricing; service differentiation,Economic and social effects; Economics; Web services; Cloud providers; Cloud services; Joint distributions; Optimal bidding strategy; Pricing strategy; Revenue maximizing; Service interruption; Service offering; Costs
Approximately Efficient Two-Sided Combinatorial Auctions,2020,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086628740&doi=10.1145%2f3381523&partnerID=40&md5=e4c93f0490f14deff9315e135ab72700,"We develop and extend a line of recent work on the design of mechanisms for two-sided markets. The markets we consider consist of buyers and sellers of a number of items, and the aim of a mechanism is to improve the social welfare by arranging purchases and sales of the items. A mechanism is given prior distributions on the agents' valuations of the items, but not the actual valuations; thus, the aim is to maximise the expected social welfare over these distributions. As in previous work, we are interested in the worst-case ratio between the social welfare achieved by a truthful mechanism and the best social welfare possible. Our main result is an incentive compatible and budget balanced constant-factor approximation mechanism in a setting where buyers have XOS valuations and sellers' valuations are additive. This is the first such approximation mechanism for a two-sided market setting where the agents have combinatorial valuation functions. To achieve this result, we introduce a more general kind of demand query that seems to be needed in this situation. In the simpler case that sellers have unit supply (each having just one item to sell), we give a new mechanism whose welfare guarantee improves on a recent one in the literature. We also introduce a more demanding version of the strong budget balance (SBB) criterion, aimed at ruling out certain ""unnatural"" transactions satisfied by SBB. We show that the stronger version is satisfied by our mechanisms. © 2020 ACM.",combinatorial auctions; Mechanism design; posted price mechanisms; two-sided markets,Budget control; Commerce; Buyers and sellers; Combinatorial auction; Constant factor approximation; Incentive compatible; Prior distribution; Truthful mechanisms; Two-sided markets; Valuation function; Sales
Introduction to the Special Issue on EC'17,2020,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160272470&doi=10.1145%2f3383321&partnerID=40&md5=22cf59693b3cdc538676cd5922eaecc3,[No abstract available],,
Peer Prediction with Heterogeneous Users,2020,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086632480&doi=10.1145%2f3381519&partnerID=40&md5=727e42d9d7ba0905b5c50a76b97a57a6,"Peer prediction mechanisms incentivize agents to truthfully report their signals, in the absence of a verification mechanism, by comparing their reports with those of their peers. Prior work in this area is essentially restricted to the case of homogeneous agents, whose signal distributions are identical. This is limiting in many domains, where we would expect agents to differ in taste, judgment, and reliability. Although the Correlated Agreement (CA) mechanism [Shnayder et al. 2016a] can be extended to handle heterogeneous agents, there is a new challenge of efficiently estimating agent signal types. We solve this problem by clustering agents based on their reporting behavior, proposing a mechanism that works with clusters of agents, and designing algorithms that learn such a clustering. In this way, we also connect peer prediction with the Dawid and Skene [1979] literature on latent types. We retain the robustness against coordinated misreports of the CA mechanism, achieving an approximate incentive guarantee of ϵ-informed truthfulness. We show on real data that this incentive approximation is reasonable in practice, even with a small number of clusters. © 2020 ACM.",clustering; information elicitation; Peer prediction; tensor decompsition,Heterogeneous agents; Heterogeneous users; Homogeneous agents; Number of clusters; Prediction mechanisms; Signal distribution; Forecasting
Multidimensional Dynamic Pricing for Welfare Maximization,2020,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086631489&doi=10.1145%2f3381527&partnerID=40&md5=4f64c5566ce62dc6b24491a49200b8cf,"We study the problem of a seller dynamically pricing d distinct types of indivisible goods, when faced with the online arrival of unit-demand buyers drawn independently from an unknown distribution. The goods are not in limited supply, but can only be produced at a limited rate and are costly to produce. The seller observes only the bundle of goods purchased at each day, but nothing else about the buyer's valuation function. Our main result is a dynamic pricing algorithm for optimizing welfare (including the seller's cost of production) that runs in time and a number of rounds that are polynomial in d and the approximation parameter. We are able to do this despite the fact that (i) the price-response function is not continuous, and even its fractional relaxation is a non-concave function of the prices, and (ii) the welfare is not observable to the seller. We derive this result as an application of a general technique for optimizing welfare over divisible goods, which is of independent interest. When buyers have strongly concave, Hölder continuous valuation functions over d divisible goods, we give a general polynomial time dynamic pricing technique. We are able to apply this technique to the setting of unit-demand buyers despite the fact that in that setting the goods are not divisible, and the natural fractional relaxation of a unit-demand valuation is not strongly concave. To apply our general technique, we introduce a novel price randomization procedure that has the effect of implicitly inducing buyers to ""regularize"" their valuations with a strongly concave function. Finally, we also extend our results to a limited-supply setting in which the supply of each good cannot be replenished. © 2020 ACM.",convex optimization; limited supply; Multidimensional dynamic pricing; online learning; revealed preferences,Approximation algorithms; Distribution of goods; Molecular physics; Polynomial approximation; Sales; Continuous valuations; Cost of productions; Indivisible good; Multi-dimensional dynamics; Price-response functions; Randomization procedure; Valuation function; Welfare maximizations; Costs
Combinatorial Auctions Do Need Modest Interaction,2020,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086631386&doi=10.1145%2f3381521&partnerID=40&md5=2090aa696543f45e9d53176449a47b36,"We study the necessity of interaction for obtaining efficient allocations in combinatorial auctions with subadditive bidders. This problem was originally introduced by Dobzinski, Nisan, and Oren (STOC'14) as the following simple market scenario: m items are to be allocated among n bidders in a distributed setting where bidders valuations are private and hence communication is needed to obtain an efficient allocation. The communication happens in rounds: In each round, each bidder, simultaneously with others, broadcasts a message to all parties involved. At the end, the central planner computes an allocation solely based on the communicated messages. Dobzinski et al. showed that (at least some) interaction is necessary for obtaining even an approximately efficient allocation: No non-interactive (1-round) protocol with polynomial communication (in the number of items and bidders) can achieve approximation ratio better than ω(m1/4), while for any r ≥ 1, there exists r-round protocols that achieve Õ(r · m1/(r+1)) approximation with polynomial communication. This in particular implies that O(log m) rounds of interaction suffice to obtain an approximately efficient allocation-namely, a polylog(m)-Approximation. A natural question at this point is to identify the ""right"" level of interaction (i.e., number of rounds) necessary to obtain approximately efficient allocations. In this article, we resolve this question by providing an almost tight round-Approximation tradeoff for this problem: We show that for any r ≥ 1, any r-round protocol that uses poly(m, n) bits of communication can only approximate the social welfare up to a factor of ω(1/r · m1/(2r+1)). This in particular implies that ω(log m/log log m) rounds of interaction are necessary for obtaining any allocation with a reasonable welfare approximation-namely, a constant or even a polylog(m)-Approximation. Our work builds on the multi-party round-elimination technique of Alon, Nisan, Raz, and Weinstein (FOCS'15)-used to prove similar-in-spirit lower bounds for round-Approximation tradeoff in unit-demand markets-and settles an open question posed initially by Dobzinski et al. (STOC'14) and subsequently by Alon et al. (FOCS'15). © 2020 ACM.",Combinatorial auctions; multi-party communication complexity; round-Approximation tradeoff,Commerce; Approximation ratios; Combinatorial auction; Efficient allocations; Elimination techniques; Lower bounds; Social welfare; Polynomial approximation
Approximation Algorithms for Maximin Fair Division,2020,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086642702&doi=10.1145%2f3381525&partnerID=40&md5=989e209fe3058b78772027e242cf8d2b,"We consider the problem of allocating indivisible goods fairly among n agents who have additive and submodular valuations for the goods. Our fairness guarantees are in terms of the maximin share, which is defined to be the maximum value that an agent can ensure for herself, if she were to partition the goods into n bundles, and then receive a minimum valued bundle. Since maximin fair allocations (i.e., allocations in which each agent gets at least her maximin share) do not always exist, prior work has focused on approximation results that aim to find allocations in which the value of the bundle allocated to each agent is (multiplicatively) as close to her maximin share as possible. In particular, Procaccia and Wang (2014) along with Amanatidis et al. (2015) have shown that under additive valuations, a 2/3-Approximate maximin fair allocation always exists and can be found in polynomial time. We complement these results by developing a simple and efficient algorithm that achieves the same approximation guarantee. Furthermore, we initiate the study of approximate maximin fair division under submodular valuations. Specifically, we show that when the valuations of the agents are nonnegative, monotone, and submodular, then a 0.21-Approximate maximin fair allocation is guaranteed to exist. In fact, we show that such an allocation can be efficiently found by using a simple round-robin algorithm. A technical contribution of the article is to analyze the performance of this combinatorial algorithm by employing the concept of multilinear extensions. © 2020 ACM.",Fair division; maximin shares; multilinear extensions; submodular valuations,Additives; Polynomial approximation; Scheduling algorithms; Approximation results; Combinatorial algorithm; Fairness guarantee; Indivisible good; Multilinear extension; Round Robin algorithms; Simple and efficient algorithms; Technical contribution; Approximation algorithms
Subgame perfect equilibria of sequential matching games,2020,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078945285&doi=10.1145%2f3373717&partnerID=40&md5=41aac820f3c53e85389736b64d23a1e0,"We study a decentralized matching market in which firms sequentially make offers to potential workers. For each offer, the worker can choose ""accept"" or ""reject,"" but the decision is irrevocable. The acceptance of an offer guarantees her job at the firm, but it may also eliminate chances of better offers from other firms in the future. We formulate this market as a perfect-information extensive-form game played by the workers. Each instance of this game has a unique subgame perfect equilibrium (SPE), which does not necessarily lead to a stable matching and has some perplexing properties. We show a dichotomy result that characterizes the complexity of computing the SPE. The computation is tractable if each firm makes offers to at most two workers or each worker receives offers from at most two firms. In contrast, it is PSPACE-hard even if both firms and workers are related to at most three offers. We also study engineering aspects of this matching market. It is shown that, for any preference profile, we can design an offering schedule of firms so that the worker-optimal stable matching is realized in the SPE. © 2020 Association for Computing Machinery.",PSPACE-completeness; Stable matching; Subgame perfect equilibrium,Game theory; Can design; Dichotomy results; Engineering aspects; Extensive-form games; Matching game; Perfect informations; Pspace completeness; Stable matching; Commerce
Pricing multi-unit markets,2020,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078891273&doi=10.1145%2f3373715&partnerID=40&md5=feb5ac71c12ea23ee5920541c1936af0,"We study the power and limitations of posted prices in multi-unit markets, where agents arrive sequentially in an arbitrary order. We prove upper and lower bounds on the largest fraction of the optimal social welfare that can be guaranteed with posted prices under a range of assumptions about the designer's information and agents' valuations. Our results provide insights about the relative power of uniform and non-uniform prices, the relative difficulty of different valuation classes, and the implications of different informational assumptions. Among other results, we prove constant-factor guarantees for agents with subadditive valuations over identical items, even in an incomplete-information setting and with uniform prices. We also show that no constant-factor guarantee is possible for general valuations over identical items, even in a full-information setting and with non-uniform prices. © 2020 Association for Computing Machinery.",Mechanism design; Posted prices; Simple auctions; Welfare maximization,Commerce; Machine design; Constant factors; Full informations; Incomplete information; Mechanism design; Posted prices; Simple auctions; Upper and lower bounds; Welfare maximizations; Costs
The better half of selling separately,2019,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076983906&doi=10.1145%2f3369927&partnerID=40&md5=38da9fce6fc86c1f3591e4704e6c25f0,"Separate selling of two independent goods is shown to yield at least 62% of the optimal revenue, and at least 73% when the goods satisfy the Myerson regularity condition. This improves the 50% result of Hart and Nisan [2017, originally circulated in 2012]. © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Approximation bound; Monopolistic pricing; Optimal revenue; Selling separately; Selling two independent goods,Economics; Approximation bounds; Regularity condition; Sales
Dynamic matching and allocation of tasks,2019,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076975738&doi=10.1145%2f3369925&partnerID=40&md5=da082d5c00d819b2ae02c1cb53199b85,"In many two-sided markets, the parties to be matched have incomplete information about their characteristics. We consider the settings where the parties engaged are extremely patient and are interested in long-term partnerships. Hence, once the final matches are determined, they persist for a long time. Each side has an opportunity to learn (some) relevant information about the other before final matches are made. For instance, clients seeking workers to perform tasks often conduct interviews that require the workers to perform some tasks and thereby provide information to both sides. The performance of a worker in such an interview-and hence the information revealed-depends both on the inherent characteristics of the worker and the task and also on the actions taken by the worker (e.g., the effort expended), which are not observed by the client. Thus, there is moral hazard. Our goal is to derive a dynamic matching mechanism that facilitates learning on both sides before final matches are achieved and ensures that the worker side does not have incentive to obscure learning of their characteristics through their actions. We derive such a mechanism that leads to final matching that achieves optimal performance (revenue) in equilibrium. We show that the equilibrium strategy is long-run coalitionally stable, which means there is no subset of workers and clients that can gain by deviating from the equilibrium strategy. We derive all the results under the modeling assumption that the utilities of the agents are defined as limit of means of the utility obtained in each interaction. © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Matching; Mechanism design,Economics; Machine design; Risk management; Equilibrium strategy; Incomplete information; Inherent characteristics; Long-term partnerships; Matching; Mechanism design; Model assumptions; Optimal performance; Dynamics
Introduction to the special issue on EC'16,2019,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075624312&doi=10.1145%2f3357235&partnerID=40&md5=b170a8a1a9a2750342ecdc0be479446c,[No abstract available],,
"The good, the bad, and the unflinchingly selfish: Pro-sociality can beWell predicted using payoffs and three behavioral types",2019,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075620225&doi=10.1145%2f3355947&partnerID=40&md5=9d8214b0cb5f29df06e7e2dd8c4bafe8,"The human willingness to pay costs to benefit anonymous others is often explained by social preferences: rather than only valuing their own material payoff, people also include the payoffs of others in their utility function. But how successful is this concept of outcome-based social preferences for actually predicting out-of-sample behavior? We investigate this question by having 1,067 participants each make 20 Dictator Game decisions with randomized parameters (e.g., outcomes for the self, for the other, benefit/cost ratio of pro-sociality). We then use machine learning to try to predict behavior by each participant in each decision. A representative agent model (a small, shared, set of parameters) predicts better than random but still quite poorly (AUC = 0.69). Allowing for full heterogeneity across individuals in the mapping from decisionparameters to outcome yields good predictive performance (AUC = 0.89). However, this heterogeneous model is complicated and unwieldy, thuswe also investigate whether a simpler model can yield similar performance. We find that the vast majority of the predictive power (AUC = 0.88) is achieved by a model that allows for three behavioral types. Finally, we show that cannot be well proxied for by other measures in psychology. This final analysis adds further evidence to the literature that human ""cooperative phenotypes"" are indeed meaningful, relatively orthogonal person-level traits. © 2019 Copyright held by the owner/author(s).",Behavioral economics; Machine learning; Prosociality; Social preferences,Learning systems; Machine learning; Social aspects; Behavioral economics; Benefit/cost ratios; Heterogeneous modeling; Predictive performance; Prosociality; Social preference; Utility functions; Willingness to pay; Economics
Minimizing regret with multiple reserves,2019,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075592379&doi=10.1145%2f3355900&partnerID=40&md5=6c700a08215587036e40979eb583222a,"We study the problem of computing and learning non-anonymous reserve prices to maximize revenue. We first define the Maximizing Multiple Reserves (MMR) problem in single-parameter matroid environments, where the input ism valuation profiles v1, . , vm, indexed by the same n bidders, and the goal is to compute the vector r of (non-anonymous) reserve prices that maximizes the total revenue obtained on these profiles by the VCG mechanism with reserves r. We prove that the problem is APX-hard, even in the special case of single-item environments, and give a polynomial-time 12 -approximation algorithm for it in arbitrary matroid environments. We then consider the online no-regret learning problem and show how to exploit the special structure of the MMR problem to translate our offline approximation algorithm into an online learning algorithm that achieves asymptotically time-averaged revenue at least 12 times that of the best fixed reserve prices in hindsight. On the negative side, we show that, quite generally, computational hardness for the offline optimization problem translates to computational hardness for obtaining vanishing time-averaged regret. Thus, our hardness result for theMMRproblem implies that computationally efficient online learning requires approximation, even in the special case of single-item auction environments. © 2019 Association for Computing Machinery.",No alpha-regret; Non-anonymous reserve prices; Second-price auction,Combinatorial mathematics; Costs; E-learning; Learning algorithms; Polynomial approximation; Computational hardness; Computationally efficient; No-regret learning; Off-line optimization; Online learning algorithms; Reserve price; Second-price auction; Special structure; Approximation algorithms
The unreasonable fairness of maximum Nash welfare,2019,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072667648&doi=10.1145%2f3355902&partnerID=40&md5=70394bd54daee098dbf53dbea8e1da34,"The maximum Nash welfare (MNW) solution-which selects an allocation that maximizes the product of utilities-is known to provide outstanding fairness guarantees when allocating divisible goods. And while it seems to lose its luster when applied to indivisible goods, we show that, in fact, the MNW solution is strikingly fair even in that setting. In particular, we prove that it selects allocations that are envy-free up to one good-a compelling notion that is quite elusive when coupled with economic efficiency. We also establish that the MNW solution provides a good approximation to another popular (yet possibly infeasible) fairness property, the maximin share guarantee, in theory and-even more so-in practice. While finding the MNW solution is computationally hard, we develop a nontrivial implementation and demonstrate that it scales well on real data. These results establish MNW as a compelling solution for allocating indivisible goods and underlie its deployment on a popular fair-division website. © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Fair division; Nash welfare; Resource allocation,Economic efficiency; Envy free; Fair divisions; Fairness guarantee; Fairness properties; Indivisible good; Maximin; Nash welfare; Resource allocation
Dynamic taxes for polynomial congestion games,2019,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072678261&doi=10.1145%2f3355946&partnerID=40&md5=0686e52f07508c67a5f9c49e5f5c8f59,"We consider the efficiency of taxation in congestion games with polynomial latency functions along the line of research initiated by Caragiannis et al. [ACM Transactions on Algorithms, 2010], who focused on both pure and mixed Nash equilibria in games with affine latencies only. By exploiting the primal-dual method [Bilò, Proceedings of the 10th Workshop on Approximation and Online Algorithms, 2012], we obtain interesting upper bounds with respect to a variety of different solution concepts ranging from approximate pure Nash equilibria up to approximate coarse correlated equilibria, and including also approximate one-round walks starting from the empty state. Our findings show a high beneficial effect of taxation that increases more than linearly with the degree of the latency functions. In some cases, a tight relationship with some well-studied polynomials in Combinatorics and Number Theory, such as the Touchard and the Geometric polynomials, arises. In these cases, we can also show matching lower bounds, albeit under mild assumptions; interestingly, our upper bounds are derived by exploiting the combinatorial definition of these polynomials, while our lower bounds are constructed by relying on their analytical characterization. © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Approximation algorithms; Congestion games; Nash equilibrium; Online algorithms; Price of anarchy; Taxes,Number theory; Polynomials; Taxation; Analytical characterization; Congestion Games; Correlated equilibria; Nash equilibria; On-line algorithms; Price of anarchy; Primal-dual methods; Pure Nash equilibrium; Approximation algorithms
Dynamics of evolving social groups,2019,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072673167&doi=10.1145%2f3355948&partnerID=40&md5=0444aa2c0cd96ccf34425b0029e6754a,"Exclusive social groups are ones in which the group members decide whether or not to admit a candidate to the group. Examples of exclusive social groups include academic departments and fraternal organizations. In this article, we introduce an analytic framework for studying the dynamics of exclusive social groups. In our model, every group member is characterized by his opinion, which is represented as a point on the real line. The group evolves in discrete time steps through a voting process carried out by the group's members. Due to homophily, each member votes for the candidate who is more similar to him (i.e., closer to him on the line). An admission rule is then applied to determine which candidate, if any, is admitted. We consider several natural admission rules including majority and consensus. We ask: How do different admission rules affect the composition of the group in the long run? We study both growing groups (where new members join old ones) and fixed-size groups (where new members replace those who quit). Our analysis reveals intriguing phenomena and phase transitions, some of which are quite counterintuitive. © 2019 Association for Computing Machinery.",Computational social choice; Price of anarchy; Social networks,Academic department; Computational social choices; Discrete time; Group members; New members; Price of anarchy; Social groups; Voting process; Social networking (online)
The stochastic matching problem with (very) few queries,2019,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072666086&doi=10.1145%2f3355903&partnerID=40&md5=fe0eef4b30855e8bd0b0d7a6119c0504,"Motivated by an application in kidney exchange, we study the following stochastic matching problem: We are given a graph G(V, E) (not necessarily bipartite), where each edge in E is realized with some constant probability p > 0, and the goal is to find a maximum matching in the realized graph. An algorithm in this setting is allowed to make queries to edges in E to determine whether or not they are realized. We design an adaptive algorithm for this problem that, for any graph G, computes a (1 − ε)-approximate maximum matching in the realized graph Gp with high probability, while making O(log (εp1/εp) ) queries per vertex, where the edges to query are chosen adaptively in O(log (εp1/εp) ) rounds. We further present a non-adaptive algorithm that makes O(log (εp1/εp) ) queries per vertex and computes a (21 − ε)-approximate maximum matching in Gp with high probability. Both our adaptive and non-adaptive algorithms achieve the same approximation factor as the previous best algorithms of Blum et al. (EC 2015) for this problem, while requiring an exponentially smaller number of per-vertex queries (and rounds of adaptive queries for the adaptive algorithm). Our results settle an open problem raised by Blum et al. by achieving only a polynomial dependency on both ε and p. Moreover, the approximation guarantee of our algorithms is instance-wise as opposed to only being competitive in expectation as is the case for Blum et al. This is of particular relevance to the key application of stochastic matching in kidney exchange. We obtain our results via two main techniques-namely, matching-covers and vertex sparsification-that may be of independent interest. © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Kidney exchange; Stochastic matching,Adaptive algorithms; Approximation algorithms; Polynomial approximation; Probability; Stochastic systems; Approximation factor; High probability; Kidney exchanges; Maximum matchings; Nonadaptive algorithm; Probability p; Sparsification; Stochastic matching; Graph theory
Knapsack voting for participatory budgeting,2019,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074122870&doi=10.1145%2f3340230&partnerID=40&md5=df94c90424448032f1c8195b9ef5aa42,"We address the question of aggregating the preferences of voters in the context of participatory budgeting. We scrutinize the voting method currently used in practice, underline its drawbacks, and introduce a novel scheme tailored to this setting, which we call ""Knapsack Voting.""We study its strategic properties-we show that it is strategy-proof under a natural model of utility (a dis-utility given by the 1 distance between the outcome and the true preference of the voter) and ""partially"" strategy-proof under general additive utilities. We extend Knapsack Voting to more general settings with revenues, deficits, or surpluses and prove a similar strategy-proofness result. To further demonstrate the applicability of our scheme, we discuss its implementation on the digital voting platform that we have deployed in partnership with the local government bodies in many cities across the nation. From voting data thus collected, we present empirical evidence that Knapsack Voting works well in practice. © 2019 Association for Computing Machinery.",Digital voting; Participatory budgeting; Social choice,Electronic voting; Digital voting; IS strategy; Local government; Natural models; Social choice; Strategy proofs; Strategy-proofness; Voting method; Budget control
Sequential equilibrium in computational games,2019,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069486118&doi=10.1145%2f3340232&partnerID=40&md5=c6a89de4398d8da631d127d1f8b0bcb6,"We examine sequential equilibrium in the context of computational games (Halpern and Pass 2015), where agents are charged for computation. In such games, an agent can rationally choose to forget, so issues of imperfect recall arise. In this setting, we consider two notions of sequential equilibrium. One is an ex ante notion, where a player chooses his strategy before the game starts and is committed to it, but chooses it in such a way that it remains optimal even off the equilibrium path. The second is an interim notion, where a player can reconsider at each information set whether he is doing the “right” thing, and if not, can change his strategy. The two notions agree in games of perfect recall, but not in games of imperfect recall. Although the interim notion seems more appealing, in a companion article (Halpern and Pass 2016), we argue that there are some deep conceptual problems with it in standard games of imperfect recall. We show that the conceptual problems largely disappear in the computational setting. Moreover, in this setting, under natural assumptions, the two notions coincide. © 2019 Copyright held by the owner/author(s).",(computational) nash equilibrium; (computational) sequential equilibrium; Computational games,Computational games; Computational settings; Conceptual problems; Equilibrium path; Imperfect recall; Information set; Nash equilibria; Sequential equilibrium
Earning and utility limits in fisher markets,2019,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069508623&doi=10.1145%2f3340234&partnerID=40&md5=ad3bc20a8fe003d520bab221a1b35213,"Earning limits and utility limits are novel aspects in the classic Fisher market model. Sellers with earning limits have bounds on their income and lower the supply they bring to the market if income exceeds the limit. Buyers with utility limits have an upper bound on the amount of utility that they want to derive and lower the budget they bring to the market if utility exceeds the limit. Markets with these properties can have multiple equilibria with different characteristics. We analyze earning limits and utility limits in markets with linear and spending-constraint utilities. For markets with earning limits and spending-constraint utilities, we show that equilibrium price vectors form a lattice and the spending of buyers is unique in non-degenerate markets. We provide a scaling-based algorithm to compute an equilibrium in time O(n3 log( + nU)), where n is the number of agents, ≥ n a bound on the segments in the utility functions, and U the largest integer in the market representation. We show how to refine any equilibrium in polynomial time to one with minimal prices or one with maximal prices (if it exists). Moreover, our algorithm can be used to obtain in polynomial time a 2-approximation for maximizing Nash social welfare in multi-unit markets with indivisible items that come in multiple copies. For markets with utility limits and linear utilities, we show similar results—lattice structure of price vectors, uniqueness of allocation in non-degenerate markets, and polynomial-time refinement procedures to obtain equilibria with minimal and maximal prices. We complement these positive results with hardness results for related computational questions. We prove that it is NP-hard to compute a market equilibrium that maximizes social welfare, and it is PPAD-hard to find any market equilibrium with utility functions with separate satiation points for each buyer and each good. © 2019 Copyright held by the owner/author(s).",Earning limits; Equilibrium computation; Market equilibrium; Spending-constraint utilities; Utility limits,Budget control; Costs; Polynomial approximation; Sales; Equilibrium price; Hardness result; Lattice structures; Market equilibria; Multiple equilibrium; Spending-constraint utilities; Utility functions; Utility limits; Commerce
Bid-limited targeting,2019,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067263843&doi=10.1145%2f3327968&partnerID=40&md5=9efca9bece161ed987502aca6a245bc3,"This article analyzes a mechanism for selling items in auctions in which the auctioneer specifies a cap on the ratio between the maximum and minimum bids that bidders may use in the auctions. Such a mechanism is widely used in online advertising through the caps that companies impose on the minimum and maximum bid multipliers that advertisers may use in targeting. When bidders' values are independent and identically distributed, this mechanism results in greater revenue than allowing bidders to condition their bids on the targeting information in an arbitrary way and also almost always results in greater revenue than not allowing bidders to target. Choosing the optimal cap on the ratio between the maximum and minimum bids can also be more important than introducing additional competition in the auction. However, if bidders' values are not identically distributed, pure-strategy equilibria may fail to exist. © 2019 Association for Computing Machinery.",Bid multipliers; Mechanism design; Online auctions; Targeting,Machine design; Mechanism design; Online advertising; Online auctions; Targeting; Commerce
Simple pricing schemes for the cloud,2019,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067256223&doi=10.1145%2f3327973&partnerID=40&md5=20ef26f0f9e03d10a34c7eeaa6e1fb91,"The problem of pricing the cloud has attractedmuch recent attention due to the widespread use of cloud computing and cloud services. From a theoretical perspective, several mechanisms that provide strong efficiency or fairness guarantees and desirable incentive properties have been designed. However, these mechanisms often rely on a rigidmodel, with several parameters needing to be precisely known for the guarantees to hold. In this article, we consider a stochastic model and show that it is possible to obtain good welfare and revenue guarantees with simple mechanisms that do not make use of the information on some of these parameters. In particular, we prove that a mechanism that sets the same price per timestep for jobs of any length achieves at least 50% of the welfare and revenue obtained by a mechanism that can set different prices for jobs of different lengths, and the ratio can be improved if we have more specific knowledge of some parameters. Similarly, a mechanism that sets the same price for all servers even though the servers may receive different kinds of jobs can provide a reasonable welfare and revenue approximation compared to a mechanism that is allowed to set different prices for different servers. © 2019 Association for Computing Machinery.",Cloud computing; Mechanism design; Pricing; Simple auctions,Cloud computing; Economics; Machine design; Stochastic models; Stochastic systems; Cloud services; Fairness guarantee; Mechanism design; Pricing scheme; Simple auctions; Specific knowledge; Time step; Costs
Fractional hedonic games,2019,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067245837&doi=10.1145%2f3327970&partnerID=40&md5=8b78edac7117164901a3bdf7b4369cf1,"The work we present in this article initiated the formal study of fractional hedonic games (FHGs), coalition formation games in which the utility of a player is the average value he ascribes to the members of his coalition. Among other settings, this covers situations in which players only distinguish between friends and non-friends and desire to be in a coalition in which the fraction of friends is maximal. FHGs thus not only constitute a natural class of succinctly representable coalition formation games but also provide an interesting framework for network clustering.We propose a number of conditions under which the core of FHGs is nonempty and provide algorithms for computing a core stable outcome. By contrast, we show that the core may be empty in other cases, and that it is computationally hard in general to decide non-emptiness of the core. © 2019 Association for Computing Machinery.",Coalition formation; Cooperative game theory; Core; Hedonic games,Computation theory; Computer games; Average values; Coalition formation games; Coalition formations; Cooperative game theory; Core; Formal studies; Hedonic games; Game theory
Distributed protocols for leader election: A game-theoretic perspective,2019,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062338292&doi=10.1145%2f3303712&partnerID=40&md5=4a4feb67c10e4f3a43bb389143a8238b,"We do a game-theoretic analysis of leader election, under the assumption that each agent prefers to have some leader than no leader at all. We show that it is possible to obtain a fair Nash equilibrium, where each agent has an equal probability of being elected leader, in a completely connected network, in a bidirectional ring, and a unidirectional ring, in the synchronous setting. In the asynchronous setting, Nash equilibrium is not quite the right solution concept. Rather, we must consider ex post Nash equilibrium; this means that we have a Nash equilibrium no matter what a scheduling adversary does. We show that ex post Nash equilibrium is attainable in the asynchronous setting in all the networks we consider, using a protocol with bounded running time. However, in the asynchronous setting, we require that n > 2. We show that we can get a fair ex post ϵ-Nash equilibrium if n = 2 in the asynchronous setting under some cryptographic assumptions (specifically, the existence of a one-way functions), using a commitment protocol. We then generalize these results to a setting where we can have deviations by a coalition of size k. In this case, we can get what we call a fair k-resilient equilibrium in a completely connected network if n > 2k; under the same cryptographic assumptions, we can a get a k-resilient equilibrium in a completely connected network, unidirectional ring, or bidirectional ring if n > k. Finally, we show that under minimal assumptions, not only do our protocols give a Nash equilibrium, they also give a sequential equilibrium, so players even play optimally off the equilibrium path. © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Ex post Nash equilibrium; Leader election,Computation theory; Cryptography; Internet protocols; Commitment protocols; Cryptographic assumptions; Distributed protocols; Ex post Nash equilibrium; Game theoretic analysis; Game-theoretic perspectives; Leader election; Sequential equilibrium; Game theory
Aggregation of votes with multiple positions on each issue,2019,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061213307&doi=10.1145%2f3296675&partnerID=40&md5=1a24428b767e8c8c96e1d716a2135593,"We consider the problem of aggregating votes cast by a society on a fixed set of issues, where each member of the society may vote for one of several positions on each issue, but the combination of votes on the various issues is restricted to a set of feasible voting patterns. We follow the aggregation framework used by Dokow and Holzman [Aggregation of non-binary evaluations, Advances in Applied Mathematics, 45:4, 487-504, 2010], in which both preference aggregation and judgment aggregation can be cast. We require the aggregation to be independent on each issue, and also supportive, i.e., for every issue, the corresponding component of every aggregator, when applied to a tuple of votes, must take as value one of the votes in that tuple.We prove that, in such a setup, non-dictatorial aggregation of votes in a society of an arbitrary size is possible if and only if either there is a non-dictatorial aggregator for two voters or there is an aggregator for three voters such that, for each issue, the corresponding component of the aggregator, when restricted to two-element sets of votes, is a majority operation or a minority operation. We then introduce a notion of a uniform non-dictatorial aggregator, which is an aggregator such that on every issue, and when restricted to arbitrary two-element subsets of the votes for that issue, it differs from all projection functions. We first give a characterization of sets of feasible voting patterns that admit a uniform non-dictatorial aggregator. After this and by making use of Bulatov's dichotomy theorem for conservative constraint satisfaction problems, we connect social choice theory with the computational complexity of constraint satisfaction by proving that if a set of feasible voting patterns has a uniform non-dictatorial aggregator of some arity, then themulti-sorted conservative constraint satisfaction problem on that set (with each issue representing a different sort) is solvable in polynomial time; otherwise, it is NP-complete. © 2019 Association for Computing Machinery.",Aggregation theory; Dichotomy theorem; Judgment aggregation; Multisorted constraint satisfaction problems; Possibility domains,Polynomial approximation; Aggregation theory; Applied mathematics; Constraint Satisfaction; Dichotomy theorem; Polynomial-time; Preference aggregations; Projection function; Social choice theory; Constraint satisfaction problems
Committee scoring rules: Axiomatic characterization and hierarchy,2019,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061210484&doi=10.1145%2f3296672&partnerID=40&md5=58805bfa76e1aa7ce53e2209ef04a44d,"Committee scoring voting rules are multiwinner analogues of positional scoring rules, which constitute an important subclass of single-winner voting rules. We identify several natural subclasses of committee scoring rules, namely, weakly separable, representation-focused, top-k-counting, OWA-based, and decomposable rules.We characterize SNTV, Bloc, and k-Approval Chamberlin-Courant as the only nontrivial rules in pairwise intersections of these classes. We provide some axiomatic characterizations for these classes, where monotonicity properties appear to be especially useful. The class of decomposable rules is new to the literature. We show that it strictly contains the class of OWA-based rules and describe some of the applications of decomposable rules. © 2019 Copyright held by the owner/author(s).",Axiomatic characterization; Axioms; Classification; Committee election; Multiwinner voting,Axiomatic characterization; Axioms; Committee election; Monotonicity property; Multiwinner voting; Positional scoring rules; Scoring rules; Voting rules; Classification (of information)
An information theoretic framework for designing information elicitation mechanisms that reward truth-telling,2019,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061199627&doi=10.1145%2f3296670&partnerID=40&md5=af5d7a7053ae113535205b441c7be3ec,"In the setting where information cannot be verified, we propose a simple yet powerful information theoretical framework-the Mutual Information Paradigm-for information elicitation mechanisms. Our framework pays every agent a measure of mutual information between her signal and a peer's signal. We require that the mutual information measurement has the key property that any ""data processing"" on the two random variables will decrease the mutual information between them. We identify such information measures that generalize Shannon mutual information. Our Mutual Information Paradigm overcomes the two main challenges in information elicitation without verification: (1) how to incentivize high-quality reports and avoid agents colluding to report random or identical responses; (2) how to motivate agents who believe they are in the minority to report truthfully. Aided by the information measures, we found (1) we use the paradigm to design a family of novel mechanisms where truth-telling is a dominant strategy and pays better than any other strategy profile (in the multi-question, detail free, minimal setting where the number of questions is large); (2) we show the versatility of our framework by providing a unified theoretical understanding of existing mechanisms-Bayesian Truth Serum Prelec (2004) and Dasgupta and Ghosh (2013)-by mapping them into our framework such that theoretical results of those existing mechanisms can be reconstructed easily. We also give an impossibility result that illustrates, in a certain sense, the the optimality of our framework. © 2019 Copyright held by the owner/author(s).",Crowdsourcing; Information theory; Mechanism design; Peer prediction,Crowdsourcing; Data handling; Machine design; Dominant strategy; Impossibility results; Information measures; Mechanism design; Mutual informations; Shannon mutual information; Theoretical framework; Truth-telling; Information theory
"Public projects, boolean functions, and the borders of border's theorem",2018,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061177161&doi=10.1145%2f3274645&partnerID=40&md5=c74b2787c4ecefddb0e44ba64ddfcab0,"Border's theorem gives an intuitive linear characterization of the feasible interim allocation rules of a Bayesian single-item environment, and it has several applications in economic and algorithmic mechanism design. All known generalizations of Border's theorem either restrict attention to relatively simple settings or resort to approximation. This article identifies a complexity-theoretic barrier that indicates, assuming standard complexity class separations, that Border's theorem cannot be extended significantly beyond the state of the art.We also identify a surprisingly tight connection between Myerson's optimal auction theory, when applied to public project settings, and some fundamental results in the analysis of Boolean functions. © 2018 Association for Computing Machinery.",Auctions; Border's theorem; Correlated values; Interdependence; Myerson theory; Optimal auctions; Prior-independence; Revenue-maximization,Commerce; Machine design; Auctions; Border's theorem; Correlated values; Interdependence; Myerson theory; Optimal auction; Prior-independence; Revenue maximization; Boolean functions
Simple mechanisms for a subadditive buyer and applications to revenue monotonicity,2018,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056776896&doi=10.1145%2f3105448&partnerID=40&md5=30471b4e7cbaa942c19d9f2f9cfe1f9d,"We study the revenue maximization problem of a seller with n heterogeneous items for sale to a single buyer whose valuation function for sets of items is unknown and drawn from some distribution D. We show that if D is a distribution over subadditive valuations with independent items, then the better of pricing each item separately or pricing only the grand bundle achieves a constant-factor approximation to the revenue of the optimal mechanism. This includes buyers who are k-demand, additive up to a matroid constraint, or additive up to constraints of any downward-closed set system (and whose values for the individual items are sampled independently), as well as buyers who are fractionally subadditive with item multipliers drawn independently. Our proof makes use of the core-tail decomposition framework developed in prior work showing similar results for the significantly simpler class of additive buyers. In the second part of the article, we develop a connection between approximately optimal simple mechanisms and approximate revenue monotonicity with respect to buyers' valuations. Revenue non-monotonicity is the phenomenon that sometimes strictly increasing buyers' values for every set can strictly decrease the revenue of the optimal mechanism. Using our main result, we derive a bound on how bad this degradation can be (and dub such a bound a proof of approximate revenue monotonicity); we further show that better bounds on approximate monotonicity imply a better analysis of our simple mechanisms. © 2018 Association for Computing Machinery.",Combinatorial valuations; Revenue monotonicity; Revenue optimization; Simple auctions,Costs; Sales; Combinatorial valuations; Constant factor approximation; Independent items; Monotonicity; Revenue maximization problem; Revenue optimization; Simple auctions; Valuation function; Economics
"Integrating market makers, limit orders, and continuous trade in prediction markets",2018,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056748581&doi=10.1145%2f3274643&partnerID=40&md5=8220d065603050782514cec1578a985d,"We provide the first concrete algorithm for combining market makers and limit orders in a prediction market with continuous trade. Our mechanism is general enough to handle both bundle orders and arbitrary securities defined over combinatorial outcome spaces. We define the notion of an ϵ-fair trading path, a path in security space along which no order executes at a price more than ϵ above its limit, and every order executes when its market price falls more than ϵ below its limit. We show that, under a certain supermodularity condition, a fair trading path exists for which the endpoint is efficient, but that under general conditions reaching an efficient endpoint via an ϵ-fair trading path is not possible. We develop an algorithm for operating a continuous market maker with limit orders that respects the ϵ-fairness conditions in the general case. We conduct simulations of our algorithm using real combinatorial predictions made during the 2008 US presidential election and evaluate it against a natural baseline according to trading volume, social welfare, and violations of the two fairness conditions. © 2018 Association for Computing Machinery.",Automated market makers; Combinatorial prediction markets; Continuous double auction; Limit orders; Market design,Electronic trading; Forecasting; Continuous double auction; Limit orders; Market design; Market-maker; Prediction markets; Commerce
Near-optimum online ad allocation for targeted advertising,2018,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056767966&doi=10.1145%2f3105447&partnerID=40&md5=83e2948eb8848701a5f908e2fb1b4d83,"Motivated by Internet targeted advertising, we address several ad allocation problems. Prior work has established that these problems admit no randomized online algorithm better than (1 −e                             1 )-competitive (see Karp et al. (1990) and Mehta et al. (2007)), yet simple heuristics have been observed to perform much better in practice. We explain this phenomenon by studying a generalization of the bounded-degree inputs considered by Buchbinder et al. (2007), graphs which we call (k, d)-bounded. In such graphs the maximal degree on the online side is at most d and the minimal degree on the offline side is at least k. We prove that, for such graphs, these problems' natural greedy algorithms attain a competitive ratio of 1 −k                             d                             +d                             −                             −                             1                             1 , tending to 1 as d/k tends to zero. We prove this bound is tight for these algorithms. Next, we develop deterministic primal-dual algorithms for the above problems, achieving a competitive ratio of 1 − (1 −d                             1 )k > 1 −ek                             1                             /d , or exponentially better loss as a function of k/d, and strictly better than 1 −e                             1 whenever k ≥ d. We complement our lower bounds with matching upper bounds for the vertex-weighted problem. Finally, we use our deterministic algorithms to prove by dual-fitting that simple randomized algorithms achieve the same bounds in expectation. Our algorithms and analysis differ from previous ad allocation algorithms, which largely scale bids based on the spent fraction of their bidder's budget, whereas we scale bids according to the number of times the bidder could have spent as much as her current bid. Our algorithms differ from previous online primal-dual algorithms in that they do not maintain dual feasibility but only a primal-to-dual ratio and only attain dual feasibility upon termination. We believe our techniques could find applications to other well-behaved online packing problems. © 2018 Association for Computing Machinery.",Online ad allocation; Online matching; Sponsored search; Targeted advertising,Budget control; Allocation algorithm; Deterministic algorithms; On-line matching; Online ad allocation; Primal dual algorithms; Randomized Algorithms; Sponsored searches; Targeted advertising; Marketing
Private Pareto optimal exchange,2018,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056795478&doi=10.1145%2f3105445&partnerID=40&md5=3457619c3b70777d0932d03a0c0a9164,"We consider the problem of implementing an individually rational, asymptotically Pareto optimal allocation in a barter-exchange economy where agents are endowed with goods and preferences over the goods of others, but may not use money as a medium of exchange. Because one of the most important instantiations of such economies is kidney exchange-where the “input” to the problem consists of sensitive patient medical records-we ask to what extent such exchanges can be carried out while providing formal privacy guarantees to the participants. We show that individually rational allocations cannot achieve any non-trivial approximation to Pareto optimality if carried out under the constraint of differential privacy-or even the relaxation of joint-differential privacy, under which it is known that asymptotically optimal allocations can be computed in two sided markets (Hsu et al. STOC 2014). We therefore consider a further relaxation that we call marginaldifferential privacy-which promises, informally, that the privacy of every agent i is protected from every other agent j i so long as j does not collude or share allocation information with other agents. We show that under marginal differential privacy, it is possible to compute an individually rational and asymptotically Pareto optimal allocation in such exchange economies. © 2018 Association for Computing Machinery.",Differential privacy; Exchange markets; Mechanism design without money,Commerce; Machine design; Asymptotically optimal; Differential privacies; Exchange economies; Exchange markets; Mechanism design; Medium of exchange; Pareto-optimality; Two-sided markets; Pareto principle
Leximin allocations in the real world,2018,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056754564&doi=10.1145%2f3274641&partnerID=40&md5=be2ecdac147640879f184a404c7b7b74,"As part of a collaboration with a major California school district, we study the problem of fairly allocating unused classrooms in public schools to charter schools. Our approach revolves around the randomized leximin mechanism. We extend previous work to show that the leximin mechanism is proportional, envy-free, Pareto optimal, and group strategyproof, not only in our classroom allocation setting, but in a general framework that subsumes a number of settings previously studied in the literature. We also prove that the leximin mechanism provides a (worst-case) 4-approximation to the maximum number of classrooms that can possibly be allocated. Our experiments, which are based on real data, show that a non-trivial implementation of the leximin mechanism scales gracefully in terms of running time (even though the problem is intractable in theory), and performs extremely well with respect to a number of efficiency objectives. We establish the practicability of our approach, and discuss issues related to its deployment. © 2018 Association for Computing Machinery.",Fair division; Leximin; Random assignment,Fair divisions; Leximin; Non-trivial; Pareto-optimal; Public schools; Random assignment; Running time; Strategy proofs; Pareto principle
Team performance with test scores,2018,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056779576&doi=10.1145%2f3274644&partnerID=40&md5=a46b3a1cd1425ad606f34fc9b6f57753,"Team performance is a ubiquitous area of inquiry in the social sciences, and it motivates the problem of team selection-choosing the members of a team for maximum performance. Influential work of Hong and Page has argued that testing individuals in isolation and then assembling the highest scoring ones into a team is not an effective method for team selection. For a broad class of performance measures, based on the expected maximum of random variables representing individual candidates, we show that tests directly measuring individual performance are indeed ineffective, but that a more subtle family of tests used in isolation can provide a constant-factor approximation for team performance. These new tests measure the “potential” of individuals, in a precise sense, rather than performance; to our knowledge they represent the first time that individual tests have been shown to produce near-optimal teams for a nontrivial team performance measure. We also show families of subdmodular and supermodular team performance functions for which no test applied to individuals can produce near-optimal teams, and we discuss implications for submodular maximization via hill-climbing. © 2018 Association for Computing Machinery.",Algorithms; Game theory; Learning,Algorithms; Game theory; Optimization; Constant factor approximation; Hill climbing; Individual performance; Learning; Near-optimal; Performance measure; Supermodular; Team performance; Testing
Allocation with traffic spikes: Mixing adversarial and stochastic models,2018,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056795897&doi=10.1145%2f3105446&partnerID=40&md5=f0826e8f31c852423c15b5b969c6eb96,"Motivated by Internet advertising applications, online allocation problems have been studied extensively in various adversarial and stochastic models. While the adversarial arrival models are too pessimistic, many of the stochastic (such as i.i.d. or random-order) arrival models do not realistically capture uncertainty in predictions. A significant cause for such uncertainty is the presence of unpredictable traffic spikes, often due to breaking news or similar events. To address this issue, a simultaneous approximation framework has been proposed to develop algorithms that work well both in the adversarial and stochastic models; however, this framework does not enable algorithms that make good use of partially accurate forecasts when making online decisions. In this article, we propose a robust online stochastic model that captures the nature of traffic spikes in online advertising. In our model, in addition to the stochastic input for which we have good forecasting, an unknown number of impressions arrive that are adversarially chosen. We design algorithms that combine a stochastic algorithm with an online algorithm that adaptively reacts to inaccurate predictions. We provide provable bounds for our new algorithms in this framework. We accompany our positive results with a set of hardness results showing that our algorithms are not far from optimal in this framework. As a byproduct of our results, we also present improved online algorithms for a slight variant of the simultaneous approximation framework. Copyright © 2018 Association for Computing Machinery. All Rights Reserved.",Online advertisement; Online budgeted allocation; Online matching; Traffic spikes,Approximation algorithms; Budget control; Forecasting; Marketing; Stochastic systems; Allocation problems; Internet advertising; On-line algorithms; On-line matching; Online advertisements; Online budgeted allocation; Simultaneous approximation; Stochastic algorithms; Stochastic models
Revenue maximization and ex-post budget constraints,2018,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056756039&doi=10.1145%2f3274647&partnerID=40&md5=19efa1b1067fcc7a3b56f9b5b0140a59,"We consider the problem of a revenue-maximizing seller with m items for sale to n additive bidders with hard budget constraints, assuming that the seller has some prior distribution over bidder values and budgets. The prior may be correlated across items and budgets of the same bidder, but is assumed independent across bidders. We target mechanisms that are Bayesian incentive compatible, but that are ex-post individually rational and ex-post budget respecting. Virtually no such mechanisms are known that satisfy all these conditions and guarantee any revenue approximation, even with just a single item. We provide a computationally efficient mechanism that is a 3-approximation with respect to all BIC, ex-post IR, and ex-post budget respecting mechanisms. Note that the problem is NP-hard to approximate better than a factor of 16/15, even in the case where the prior is a point mass. We further characterize the optimal mechanism in this setting, showing that it can be interpreted as a distribution over virtual welfare maximizers. We prove our results by making use of a black-box reduction from mechanism to algorithm design developed by Cai et al. Our main technical contribution is a computationally efficient 3-approximation algorithm for the algorithmic problem that results from an application of their framework to this problem. The algorithmic problem has a mixed-sign objective and is NP-hard to optimize exactly, so it is surprising that a computationally efficient approximation is possible at all. In the case of a single item (m = 1), the algorithmic problem can be solved exactly via exhaustive search, leading to a computationally efficient exact algorithm and a stronger characterization of the optimal mechanism as a distribution over virtual value maximizers. © 2018 Association for Computing Machinery.",Budget constraints; Generalized assignment problem; Revenue optimization; Virtual welfare,Combinatorial optimization; Computational efficiency; Optimization; Black-box reductions; Budget constraint; Computationally efficient; Generalized assignment problem; Revenue maximization; Revenue optimization; Technical contribution; Virtual welfare; Budget control
Introduction to the special issue on EC'15,2018,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056749296&doi=10.1145%2f3274531&partnerID=40&md5=dd256ef4971ad21908880142f93125fa,[No abstract available],,
Learning what's going on: Reconstructing preferences and priorities from opaque transactions,2018,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056759325&doi=10.1145%2f3274642&partnerID=40&md5=bdd48bc9d852c5ffc18b9bed8e668b66,"We consider a setting where n buyers, with combinatorial preferences over m items, and a seller, running a priority-based allocation mechanism, repeatedly interact. Our goal, from observing limited information about the results of these interactions, is to reconstruct both the preferences of the buyers and the mechanism of the seller. More specifically, we consider an online setting where at each stage, a subset of the buyers arrive and are allocated items, according to some unknown priority that the seller has among the buyers. Our learning algorithm observes only which buyers arrive and the allocation produced (or some function of the allocation, such as just which buyers received positive utility and which did not), and its goal is to predict the outcome for future subsets of buyers. For this task, the learning algorithm needs to reconstruct both the priority among the buyers and the preferences of each buyer. We derive mistake bound algorithms for additive, unit-demand and single-minded buyers. We also consider the case where buyers' utilities for a fixed bundle can change between stages due to different (observed) prices. Our algorithms are efficient both in computation time and in the maximum number of mistakes (both polynomial in the number of buyers and items). © 2018 Association for Computing Machinery.",Learning from revealed preferences; Mechanism design,Computational efficiency; Machine design; Sales; Allocation mechanism; Computation time; Limited information; Mechanism design; Mistake bounds; On-line setting; Priority-based; Revealed preference; Learning algorithms
Redesigning the Israeli medical internship match,2018,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054270380&doi=10.1145%2f3274646&partnerID=40&md5=d623a9aaa7045c5584b8485806445ad8,"The final step in getting an Israeli MD is performing a year-long internship in one of the hospitals in Israel. Internships are decided upon by a lottery, which is known as the Internship Lottery. In 2014, we redesigned the lottery, replacing it with a more efficient one. This article presents the market, the redesign process, and the new mechanism that is now in use. In this article, we describe the redesign and focus on two-body problems that we faced in the new mechanism. Specifically, we show that decomposing stochastic assignment matrices to deterministic allocations is NP-hard in the presence of couples, and present a polynomial-time algorithm with the optimal worst case guarantee. We also study the performance of our algorithm on real-world and simulated data. © 2018 Association for Computing Machinery.",Assignment problem; Market design; Matching with couples,Combinatorial optimization; Commerce; Stochastic systems; Assignment problems; Market design; Matching with couples; New mechanisms; Polynomial-time algorithms; Real-world; Two-body problem; Polynomial approximation
Equitable rent division,2018,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061206773&doi=10.1145%2f3274528&partnerID=40&md5=103653e1821028659e93a48ec48bc248,"How should a group of roommates allocate the rooms and contributions to rent in the house they lease? Economists have provided partial answers to this question in a literature that spans the last 40 years. Unfortunately, these results were developed in a non-linear fashion, which obscures them to the non-specialist. Recently, computer scientists have developed an interest in this problem, advancing from an algorithmic complexity perspective. With this new interest gaining traction, there is an evident need for a coherent development of the results in economics literature. This article does so. In particular, we build connections among results that were seemingly unrelated and considerably simplify their development, fill in non-trivial gaps, and identify open questions. Our focus is on incentives issues, the area in which we believe economists have more to contribute in this discussion. © 2018 Association for Computing Machinery.",Efficiency; Equal-income competitive allocations; Indivisible goods; No-envy; Rent division; Rental harmony,Computational complexity; Efficiency; Parallel processing systems; Algorithmic complexity; Computer scientists; Equal-income competitive allocations; Indivisible good; Non linear; Non-trivial; Rent division; Rental harmony; Economics
"The value of privacy: Strategic data subjects, incentive mechanisms, and fundamental limits",2018,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053392911&doi=10.1145%2f3232863&partnerID=40&md5=21586be80bf3b0c3511179558a3ba325,"We study the value of data privacy in a game-theoretic model of trading private data, where a data collector purchases private data from strategic data subjects (individuals) through an incentive mechanism. One primary goal of the data collector is to learn some desired information from the elicited data. Specifically, this information is modeled by an underlying state, and the private data of each individual represents his of her knowledge about the state. Departing from most of the existing work on privacy-aware surveys, our model does not assume the data collector to be trustworthy. Further, an individual takes full control of his or her own data privacy and reports only a privacy-preserving version of his or her data. In this article, the value of ϵ units of privacy is measured by the minimum payment among all nonnegative payment mechanisms, under which an individual's best response at a Nash equilibrium is to report his or her data in an ϵ-locally differentially private manner. The higher ϵ is, the less private the reported data is. We derive lower and upper bounds on the value of privacy that are asymptotically tight as the number of data subjects becomes large. Specifically, the lower bound assures that it is impossible to use a lower payment to buy ϵ units of privacy, and the upper bound is given by an achievable payment mechanism that we design. Based on these fundamental limits, we further derive lower and upper bounds on the minimum total payment for the data collector to achieve a given accuracy target for learning the underlying state and show that the total payment of the designed mechanism is at most one individual's payment away from the minimum. © 2018 ACM",Data collection; Differential privacy; Randomized response,Game theory; Data collection; Differential privacies; Game-theoretic model; Incentive mechanism; Lower and upper bounds; Nonnegative payments; Privacy preserving; Randomized response; Data privacy
Valuation compressions in VCG-based combinatorial auctions,2018,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053396742&doi=10.1145%2f3232860&partnerID=40&md5=bfc84a6343fdb23fb1d6947f19d49392,"The focus of classic mechanism design has been on truthful direct-revelation mechanisms. In the context of combinatorial auctions, the truthful direct-revelation mechanism that maximizes social welfare is the Vickrey-Clarke-Groves mechanism. For many valuation spaces, computing the allocation and payments of the VCG mechanism, however, is a computationally hard problem. We thus study the performance of the VCG mechanism when bidders are forced to choose bids from a subspace of the valuation space for which the VCG outcome can be computed efficiently. We prove improved upper bounds on the welfare loss for restrictions to additive bids and upper and lower bounds for restrictions to non-additive bids. These bounds show that increased expressiveness can give rise to additional equilibria of poorer efficiency. © 2018 ACM",Combinatorial auctions with item bidding; Price of anarchy; Simplified mechanisms,Machine design; Classic mechanism; Combinatorial auction; Hard problems; Price of anarchy; Social welfare; Upper and lower bounds; VCG mechanism; Vickrey-Clarke-Groves mechanism; Commerce
Causal strategic inference in a game-theoretic model of multiplayer networked microfinance markets,2018,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053420045&doi=10.1145%2f3232861&partnerID=40&md5=c1c49a45adf261649ef50500d8addc30,"Performing interventions is a major challenge in economic policy-making. We present causal strategic inference as a framework for conducting interventions and apply it to large, networked microfinance economies. The basic solution platform consists of modeling a microfinance market as a networked economy, learning the model using single-sample real-world microfinance data, and designing algorithms for various causal questions. For a special case of our model, we show that an equilibrium point always exists and that the equilibrium interest rates are unique. For the general case, we give a constructive proof of the existence of an equilibrium point. Our empirical study is based on microfinance data from Bangladesh and Bolivia, which we use to first learn our models. We show that causal strategic inference can assist policy-makers by evaluating the outcomes of various types of interventions, such as removing a loss-making bank from the market, imposing an interest-rate cap, and subsidizing banks. © 2018 ACM",Artificial intelligence; Causal inference; Computational economics; Economic networks; Game theory; Markets; Microfinance economy,Artificial intelligence; Commerce; Computation theory; Decision making; Distributed computer systems; Game theory; Marketing; Public policy; Causal inferences; Computational economics; Constructive proof; Economic networks; Economic policies; Existence of an equilibrium points; Game-theoretic model; Microfinance; Finance
Cardinal contests,2018,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053379374&doi=10.1145%2f3232862&partnerID=40&md5=430aba8c6cd3a04e6cda4163cd7234da,"We model and analyze cardinal contests, where a principal running a rank-order tournament has access to an absolute measure of the quality of agents' submissions in addition to their relative rankings. We show that a mechanism that compares each agent's output quality against a threshold to decide whether to award her the prize corresponding to her rank is optimal amongst the set of all mixed cardinal-ordinal mechanisms where the jth-ranked submission receives a fraction of the jth prize that is a non-decreasing function of the submission's quality. Furthermore, the optimal threshold mechanism uses exactly the same threshold for each rank. © 2018 Copyright is held by the owner/author(s). Publication rights licensed to ACM.",Contests; Crowdsourcing; Game theory; Optimal contest design,Crowdsourcing; Contest designs; Contests; Non-decreasing functions; Optimal threshold; Output quality; Rank order; Relative rankings; Game theory
∃ℝ-completeness for decision versions of multi-player (Symmetric) Nash equilibria,2018,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045356563&doi=10.1145%2f3175494&partnerID=40&md5=fc747c649118e425fd0296ab0e59d709,"As a result of a series of important works [7-9, 15, 23], the complexity of two-player Nash equilibrium is by now well understood, even when equilibria with special properties are desired and when the game is symmetric. However, for multi-player games, when equilibria with special properties are desired, the only result known is due to Schaefer and Štefankovic [28]: that checking whether a three-player Nash Equilibrium (3-Nash) instance has an equilibrium in a ball of radius half in l∞-norm is ∃ℝ-complete, where ∃ℝ is the class of decision problems that can be reduced in polynomial time to Existential Theory of the Reals. Building on their work, we show that the following decision versions of 3-Nash are also ∃ℝ-complete: checking whether (i) there are two or more equilibria, (ii) there exists an equilibrium in which each player gets at least h payoff, where h is a rational number, (iii) a given set of strategies are played with non-zero probability, and (iv) all the played strategies belong to a given set. Next, we give a reduction from 3-Nash to symmetric 3-Nash, hence resolving an open problem of Papadimitriou [25]. This yields ∃ℝ-completeness for symmetric 3-Nash for the last two problems stated above as well as completeness for the class FIXPa, a variant of FIXP for strong approximation. All our results extend to k-Nash for any constant k ≥ 3. © 2018 ACM.",Decision problems; Existential theory of reals; FIXP; Nash equilibrium; Symmetric games,Computation theory; Decision theory; Polynomial approximation; Decision problems; Existential theory of reals; FIXP; Nash equilibria; Symmetric games; Game theory
Pricing equilibria and graphical valuations,2018,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045354378&doi=10.1145%2f3175495&partnerID=40&md5=df7f9ad6cb4c57e7477ca4329c9066d4,"We study pricing equilibria for graphical valuations, whichare a class of valuations that admit a compact representation. These valuations are associated with a value graph, whose nodes correspond to items, and edges encode (pairwise) complementarities/substitutabilities between items. It is known that for graphical valuations aWalrasian equilibrium (a pricing equilibrium that relies on anonymous item prices) does not exist in general. On the other hand, a pricing equilibrium exists when the seller uses an agent-specific graphical pricing rule that involves prices for each item and markups/discounts for pairs of items.We study the existence of pricing equilibria with simpler pricing rules which either (i) require anonymity (so that prices are identical for all agents) while allowing for pairwise markups/discounts or (ii) involve offering prices only for items. We show that a pricing equilibrium with the latter pricing rule exists if and only if a Walrasian equilibrium exists,whereas the former pricing rule may guarantee the existence of a pricing equilibrium even for graphical valuations that do not admit aWalrasian equilibrium. Interestingly, by exploiting a novel connection between the existence of a pricing equilibrium and the partitioning polytope associated with the underlying graph, we also establish that for simple (series-parallel) value graphs, a pricing equilibrium with anonymous graphical pricing rule exists if and only if a Walrasian equilibrium exists. These equivalence results imply that simpler pricing rules (i) and (ii) do not guarantee the existence of a pricing equilibrium for all graphical valuations.",Efficient allocation; Graphical valuations; Pricing equilibrium,Compact representation; Efficient allocations; Graphical valuations; Polytopes; Pricing rules; Series-parallel; Underlying graphs; Walrasian equilibrium; Costs
Competitive packet routing with priority lists,2018,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053276588&doi=10.1145%2f3184137&partnerID=40&md5=9570980487776e12400af0dd64abde09,"In competitive packet routing games, the packets are routed selfishly through a network and scheduling policies at edges determine which packets are forwarded first if there is not enough capacity on an edge to forward all packets at once. We analyze the impact of priority lists on the worst-case quality of pure Nash equilibria. A priority list is an ordered list of players that may or may not depend on the edge. Whenever the number of packets entering an edge exceeds the inflow capacity, packets are processed in list order. We derive several new bounds on the price of anarchy and stability for global and local priority policies. We also consider the question of the complexity of computing an optimal priority list. It turns out that even for very restricted cases, i.e., for routing on a tree, the computation of an optimal priority list is APX-hard. © 2018 ACM.",Complexity; Nash equilibrium; Packet routing; Price of anarchy; Priority policy,Complexity; Nash equilibria; Packet routing; Price of anarchy; Priority policies; Complex networks
The random assignment problem with submodular constraints on goods,2018,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045369674&doi=10.1145%2f3175496&partnerID=40&md5=9ccc9f19172c1cddf728bb9fcbf59067,"Problems of allocating indivisible goods to agents in an efcient and fair manner without money have long been investigated in literature. The random assignment problem is one of them, where we are given a fxed feasible (available) set of indivisible goods and a profle of ordinal preferences over the goods, one for each agent. Then, using lotteries, we determine an assignment of goods to agents in a randomized way. A seminal paper of Bogomolnaia and Moulin (2001) shows a probabilistic serial (PS) mechanism to give an ordinally efcient and envy-free solution to the assignment problem. In this article, we consider an extension of the random assignment problem to submodular constraints on goods. We show that the approach of the PS mechanism by Bogomolnaia and Moulin is powerful enough to solve the random assignment problem with submodular (matroidal and polymatroidal) constraints. Under the agents' ordinal preferences over goods we show the following: (1) The obtained PS solution for the problem with unit demands and matroidal constraints is ordinally efcient, envy-free, and weakly strategy-proof with respect to the associated stochastic dominance relation. (2) For the multiunit demand and polymatroidal constraint problem, the PS solution is ordinally efcient and envy-free but is not strategy-proof in general. However, we show that under a mild condition (that is likely to be satisfed in practice) the PS solution is a weak Nash equilibrium. © 2018 ACM.",,Combinatorial mathematics; Stochastic systems; Assignment problems; Constraint problems; Indivisible good; Nash equilibria; Ordinal preference; Random assignment problems; Stochastic Dominance; Submodular constraints; Combinatorial optimization
Applications of α-strongly regular distributions to Bayesian auctions,2017,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045370945&doi=10.1145%2f3157083&partnerID=40&md5=bb8665c79a4a4b465f5300edd612b16a,"Two classes of distributions that arewidely used in the analysis of Bayesian auctions are the monotone hazard rate (MHR) and regular distributions. They can both be characterized in terms of the rate of change of the associated virtual value functions: forMHR distributions, the condition is that for valuesv < v' φ(v ) - φ(v') ≥ v-v, and for regular distributions, φ(v') - φ(v) ge; 0. Cole and Roughgarden introduced the interpolating class of α-strongly regular distributions (α-SR distributions for short), for which φ(v') - φ(v) ge; α (v-v), for 0 ≤ α ≤ 1. In this article, we investigate five distinct auction settings for which good expected revenue bounds are knownwhen the bidders' valuations are given by MHR distributions. In every case,we showthat these bounds degrade gracefully when extended to α-SR distributions. For four of these settings, the auction mechanism requires knowledge of these distributions (in the remaining setting, the distributions are needed only to ensure good bounds on the expected revenue). In these cases, we also investigate what happens when the distributions are known only approximately via samples, specifically how to modify the mechanisms so that they remain effective and how the expected revenue depends on the number of samples. © 2017 ACM.",Bayesian auctions; Sample complexity; α-strongly regular distributions; λ-regular distributions; ρ-concave distributions,Auction mechanisms; Auction settings; Bayesian; Expected revenue; Number of samples; Regular distribution; Sample complexity; Value functions; Commerce
The VCG mechanism for Bayesian scheduling,2017,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045358008&doi=10.1145%2f3105968&partnerID=40&md5=0aab6f43080f4958f56c33c7f9e4a089,"We study the problem of scheduling m tasks to n selfish, unrelated machines in order to minimize the makespan, in which the execution times are independent random variables, identical across machines. We show that the VCG mechanism, which myopically allocates each task to its best machine, achieves an approximation ratio of O( ln n ln ln n ). This improves significantly on the previously best known bound of O(mn ) for prior-independent mechanisms, given by Chawla et al. [7] under the additional assumption of Monotone Hazard Rate (MHR) distributions. Although we demonstrate that this is tight in general, if we do maintain the MHR assumption, then we get improved, (small) constant bounds form = n lnn i.i.d. tasks. We also identify a sufficient condition on the distribution that yields a constant approximation ratio regardless of the number of tasks. © 2017 ACM.",Balls-in-bins; Bayesian mechanism design; Scheduling; VCG mechanism,Machine design; Approximation ratios; Bayesian; Bayesian mechanism designs; Hazard rates; Independent random variables; Makespan; Unrelated machines; VCG mechanism; Scheduling
Fast convergence in the double oral auction,2017,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045373220&doi=10.1145%2f3084358&partnerID=40&md5=661939c4504412503ec97b3b47cd25d7,"A classical trading experiment consists of a set of unit demand buyers and unit supply sellers with identical items. Each agent's value or opportunity cost for the item is his private information, and preferences are quasilinear. Trade between agents employs a double oral auction (DOA) in which both buyers and sellers call out bids or offers that an auctioneer recognizes. Transactions resulting from accepted bids and offers are recorded. This continues until there are no more acceptable bids or offers. Remarkably, the experiment consistently terminates in a Walrasian price. The main result of this article is a mechanism in the spirit of the DOA that converges to a Walrasian equilibrium in a polynomial number of steps, thus providing a theoretical basis for the empirical phenomenon described previously. It is well known that computation of a Walrasian equilibrium for this market corresponds to solving a maximum weight bipartite matching problem. The uncoordinated but mildly rational responses of agents thus solve in a distributed fashion a maximum weight bipartite matching problem that is encoded by their private valuations. We show, furthermore, that everyWalrasian equilibrium is reachable by some sequence of responses. This is in contrast to thewell-known auction algorithms for this problem that only allow one side to make offers and thus essentially choose an equilibrium that maximizes the surplus for the side making offers. Our results extend to the setting where not every agent pair is allowed to trade with each other. © 2017 ACM.",Double oral auction,Auction algorithms; Bipartite matching problems; Buyers and sellers; Double oral auction; Opportunity costs; Polynomial number; Private information; Walrasian equilibrium; Commerce
Impartial selection and the power of up to two choices,2017,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045362798&doi=10.1145%2f3107922&partnerID=40&md5=abef51b0e81dee3cbfd76589db1f9fbb,"We study mechanisms that select members of a set of agents based on nominations by other members and that are impartial in the sense that agents cannot influence their own chance of selection. Prior work has shown that deterministic mechanisms for selecting any fixed number k of agents are severely limited and cannot extract a constant fraction of the nominations of the k most highly nominated agents.We prove here that this impossibility result can be circumvented by allowing the mechanism to sometimes but not always select fewer than k agents. This added flexibility also improves the performance of randomized mechanisms, for which we show a separation between mechanisms that make exactly two or up to two choices and give upper and lower bounds for mechanisms allowed more than two choices. © 2017 ACM.",Impartial selection; Mechanism design,Deterministic mechanism; Fixed numbers; Impartial selection; Impossibility results; Mechanism design; Randomized mechanism; Upper and lower bounds; Machine design
Computation of stackelberg equilibria of finite sequential games,2017,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045352741&doi=10.1145%2f3133242&partnerID=40&md5=74dcc4b8ee894b1fa6095be8bf17d8c4,"The Stackelberg equilibrium is a solution concept that describes optimal strategies to commit to: Player 1 (the leader) first commits to a strategy that is publicly announced, then Player 2 (the follower) plays a best response to the leader's choice. We study the problem of computing Stackelberg equilibria in finite sequential (i.e., extensive-form) games and provide new exact algorithms, approximation algorithms, and hardness results for finding equilibria for several classes of such two-player games. © 2017 ACM.",Algorithmic game theory; Extensive-form correlated equilibrium; Extensive-form games; Finite sequential games; Stackelberg equilibrium,Approximation algorithms; Computation theory; Computer games; Algorithmic Game Theory; Correlated equilibria; Extensive-form games; Sequential games; Stackelberg equilibrium; Game theory
Sequential posted-price mechanisms with correlated valuations,2017,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045366748&doi=10.1145%2f3157085&partnerID=40&md5=68ee27c6e42bfa10d1d9a9be1104f80b,"We study the revenue performance of sequential posted-price mechanisms and some natural extensions for a setting where the valuations of the buyers are drawn from a correlated distribution. Sequential posted-price mechanisms are conceptually simple mechanisms that work by proposing a ""take-it-or-leave-it"" offer to each buyer. We apply sequential posted-price mechanisms to single-parameter multiunit settings in which each buyer demands only one item and the mechanism can assign the service to at most k of the buyers. For standard sequential posted-price mechanisms, we prove that with the valuation distribution having finite support, no sequential posted-price mechanism can extract a constant fraction of the optimal expected revenue, even with unlimited supply.We extend this result to the case of a continuous valuation distribution when various standard assumptions hold simultaneously (i.e., everywhere-supported, continuous, symmetric, and normalized (conditional) distributions that satisfy regularity, the MHR condition, and affiliation). In fact, it turns out that the best fraction of the optimal revenue that is extractable by a sequential posted-price mechanism is proportional to the ratio of the highest and lowest possible valuation. We prove that a simple generalization of these mechanisms achieves a better revenue performance; namely, if the sequential posted-price mechanism has for each buyer the option of either proposing an offer or asking the buyer for its valuation, then a ω(1/ max{1,d}) fraction of the optimal revenue can be extracted, where d denotes the degree of dependence of the valuations, ranging from complete independence (d = 0) to arbitrary dependence (d = n - 1). © 2017 ACM.",Approximation; Correlated values; Interdependence; Mechanism design; Posted pricing,Economics; Machine design; Sales; Approximation; Continuous valuations; Correlated distributions; Correlated values; Interdependence; Mechanism design; Natural extension; Standard assumptions; Costs
Envy-free pricing in large markets: Approximating revenue and welfare,2017,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045371767&doi=10.1145%2f3105786&partnerID=40&md5=a9b27da23a3199d5942e6637eb3d994e,"We study the classic setting of envy-free pricing, in which a single seller chooses prices for its many items, with the goal of maximizing revenue once the items are allocated. Despite the large body of work addressing such settings, most versions of this problem have resisted good approximation factors for maximizing revenue; this is true even for the classic unit-demand case. In this article, we study envy-free pricing with unit-demand buyers, but unlike previous work we focus on large markets: ones in which the demand of each buyer is infinitesimally small compared to the size of the overall market. We assume that the buyer valuations for the items they desire have a nice (although reasonable) structure, that is, that the aggregate buyer demand has a monotone hazard rate and that the values of every buyer type come from the same support. For such large markets, our main contribution is a 1.88-approximation algorithm for maximizing revenue, showing that good pricing schemes can becomputed when the number of buyers islarge. We also givea (e,2)-bicriteria algorithm that simultaneously approximates both maximum revenue and welfare, thus showing that it is possible to obtain both good revenue and welfare at the same time. We further generalize our results by relaxing some of our assumptions and quantify the necessary tradeoffs between revenue and welfare in our setting. Our results are the first known approximations for large markets and crucially rely on new lower bounds, which we prove for the revenue-maximizing prices. ©2017 ACM.",Approximation algorithms; Envy-free; Pricing; Revenue; Social welfare,Approximation algorithms; Commerce; Costs; Earnings; Sales; Approximation factor; Bicriteria algorithm; Envy free; Envy-free pricing; Maximum revenue; Pricing scheme; Revenue maximizing; Social welfare; Economics
Dynamics at the boundary of game theory and distributed computing,2017,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045363205&doi=10.1145%2f3107182&partnerID=40&md5=6f836ba833c7a5b0455e26d130c67d81,"We use ideas from distributed computing and game theory to study dynamic and decentralized environments in which computational nodes, or decision makers, interact strategically and with limited information. In such environments, which arise in many real-world settings, the participants act as both economic and computational entities. We exhibit a general non-convergence result for a broad class of dynamics in asynchronous settings. We consider implications of our result across a wide variety of interesting and timely applications: game dynamics, circuit design, social networks, Internet routing, and congestion control. We also study the computational and communication complexity of testing the convergence of asynchronous dynamics. Our work opens a new avenue for research at the intersection of distributed computing and game theory. © 2017 ACM.",Adaptive heuristics; Game dynamics; Self stabilization,Computer games; Decision making; Decision theory; Distributed computer systems; Dynamics; Adaptive heuristics; Asynchronous dynamics; Communication complexity; Computational entities; Computational nodes; Convergence results; Limited information; Self stabilization; Game theory
A geometric perspective on minimal peer prediction,2017,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057202178&doi=10.1145%2f3070903&partnerID=40&md5=b3662d39b96f54b9f7fa9a42b9f54e74,"Minimal peer prediction mechanisms truthfully elicit private information (e.g., opinions or experiences) from rational agents without the requirement that ground truth is eventually revealed. In this article, we use a geometric perspective to prove that minimal peer prediction mechanisms are equivalent to power diagrams, a type of weighted Voronoi diagram. Using this characterization and results from computational geometry, we show that many of the mechanisms in the literature are unique up to affine transformations. We also show that classical peer prediction is “complete” in that every minimal mechanism can be written as a classical peer prediction mechanism for some scoring rule. Finally, we use our geometric characterization to develop a general method for constructing new truthful mechanisms, and we show how to optimize for the mechanisms’ effort incentives and robustness. © 2017 ACM.",And Phrases: Peer prediction; Information elicitation; Mechanism design,Computational geometry; Machine design; Affine transformations; Geometric characterization; Information elicitation; Mechanism design; Prediction mechanisms; Private information; Truthful mechanisms; Weighted voronoi diagram; Forecasting
A truthful mechanism for the generalized assignment problem,2017,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045355451&doi=10.1145%2f3105787&partnerID=40&md5=21a3e94d105a3bccca7a73739f79ece2,"We propose a truthful-in-expectation, (1 1/e)-approximation mechanism for a strategic variant of the generalized assignment problem (GAP). In GAP, a set of items has to be optimally assigned to a set of bins without exceeding the capacity of any singular bin. In the strategic variant of the problem we study, values for assigning items to bins are the private information of bidders and the mechanism should provide bidders with incentives to truthfully report their values. The approximation ratio of the mechanism is a significant improvement over the approximation ratio of the existing truthful mechanism for GAP. The proposed mechanism comprises a novel convex optimization program as the allocation rule as well as an appropriate payment rule. To implement the convex program in polynomial time, we propose a fractional local search algorithm which approximates the optimal solution within an arbitrarily small error leading to an approximately truthful-in-expectation mechanism. The proposed algorithm improves upon the existing optimization algorithms for GAP in terms of simplicity and runtime while the approximation ratio closely matches the best approximation ratio known for GAP when all inputs are publicly known. © 2017 ACM.",,Approximation algorithms; Bins; Combinatorial optimization; Convex optimization; Polynomial approximation; Polynomials; Approximation ratios; Best approximations; Generalized assignment problem; Local search algorithm; Optimization algorithms; Optimization programs; Private information; Truthful mechanisms; Optimization
Farewell Editorial: Looking Back on Our Terms Editing ACM TEAC and into the Future,2017,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045358441&doi=10.1145%2f3079047&partnerID=40&md5=d03c65d7a1383f93947a7f86915719ce,[No abstract available],,
Editorial from the new TEAC co-editors-in-chief,2017,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045366863&doi=10.1145%2f3084545&partnerID=40&md5=0ffb7faeaa370ce9b1846250d3a07f76,[No abstract available],,
Provision-after-wait with common preferences,2017,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045368084&doi=10.1145%2f3038910&partnerID=40&md5=4f8e4ce4c07bf8fdb5845585d3b467be,"In this article, we study the Provision-after-Wait problem in healthcare (Braverman, Chen, and Kannan, 2016). In this setting, patients seek a medical procedure that can be performed by different hospitals of different costs. Each patient has a value for each hospital and a budget-constrained government/planner pays for the expenses of the patients. Patients are free to choose hospitals, but the planner controls how much money each hospital gets paid and thus how many patients each hospital can serve (in one budget period, say, one month or one year).Waiting times are used in order to balance the patients' demand, and the planner's goal is to find a stable assignment that maximizes the social welfare while keeping the expenses within the budget. It has been shown that the optimal stable assignment is NP-hard to compute, and, beyond this, little is known about the complexity of the Provision-after-Wait problem. We start by showing that this problem is in fact strongly NP-hard, and thus does not have a Fully Polynomial-Time Approximation Scheme (FPTAS). We then focus on the common preference setting, where the patients have the same ranking over the hospitals. Even when the patients perceive the hospitals' values to them based on the same quality measurement-referred to as proportional preferences, which has been widely studied in resource allocation-the problem is still NP-hard. However, in a more general setting where the patients are ordered according to the differences of their values between consecutive hospitals, we construct an FPTAS for it. To develop our results, we characterize the structure of optimal stable assignments and their social welfare, and we consider a new combinatorial optimization problem that may be of independent interest, the ordered Knapsack problem. Optimal stable assignments are deterministic and ex-post individually rational for patients. The downside is that waiting times are dead-loss to patients and may burn a lot of social welfare. If randomness is allowed, then the planner can use lotteries as a rationing tool: The hope is that they reduce the patients' waiting times, although they are interim individually rational instead of ex-post. Previous study has only considered lotteries for two hospitals. In our setting, for arbitrary number of hospitals, we characterize the structure of the optimal lottery scheme and conditions under which using lotteries generates better (expected) social welfare than using waiting times. © 2017 ACM.",Budget; Healthcare; Lottery; Resource allocation; Waiting time,Budget control; Combinatorial optimization; Hamiltonians; Health care; Hospitals; Polynomial approximation; Resource allocation; Structural optimization; Budget; Combinatorial optimization problems; Fully polynomial time approximation schemes; Knapsack problems; Lottery; Medical procedures; Quality measurements; Waiting-time; Optimization
Distributed matching with mixed maximum-minimum utilities,2017,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045365981&doi=10.1145%2f3038911&partnerID=40&md5=7c69cc6a4109ea50af8e1f74e31b6866,"In this article, we study distributed agent matching with search friction in environments characterized by costly exploration, where each agent's utility from forming a partnership is influenced by some linear combination of themaximum and the minimum among the two agents' competence. The article provides a cohesive analysis for such case, proving the equilibrium structure for the different min-max linear combinations that may be used. The article presents an extensive equilibrium analysis of such settings, proving three distinct resulting patterns of the acceptance thresholds used by the different agents. The first relates to settings where a greater emphasis is placed on the minimum type, or in the extreme case where the minimum type solely determines the output. In these cases, the assortative matching characteristic holds, where all agents set their threshold below their own type and the greater is the agent type, the greater is its threshold. When the utility from the partnership formation is solely determined by the maximum type, we show that there exists a type x where partnerships form if and only if one of the agents has a type equal to or greater than x. When a greater emphasis is placed on the maximum type (but not only), we prove that assortative matching never holds, and the change in the agents' acceptance thresholds can frequently shift from an increase to a decrease. © 2017 ACM.",Distributed matching; Equilibrium; Two-sided search,Phase equilibria; Distributed agents; Distributed matching; Equilibrium analysis; Equilibrium structures; Linear combinations; Maximum types; Two agents; Two-sided search; Mergers and acquisitions
Posting prices with unknown distributions,2017,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045358099&doi=10.1145%2f3037382&partnerID=40&md5=ddaf09ec315c4f73be7e6f14522c1b63,"We consider a dynamic auction model, where bidders sequentially arrive to the market. The values of the bidders for the item for sale are independently drawn from a distribution, but this distribution is unknown to the seller. The seller offers a personalized take-it-or-leave-it price for each arriving bidder and aims to maximize revenue. We study how well can such sequential posted-price mechanisms approximate the optimal revenue that would be achieved if the distribution was known to the seller. On the negative side, we show that sequential posted-price mechanisms cannot guarantee a constant fraction of this revenue when the class of candidate distributions is unrestricted. We show that this impossibility holds even for randomized mechanisms and even if the set of possible distributions is very small or when the seller has a prior distribution over the candidate distributions. On the positive side, we devise a simple postedprice mechanism that guarantees a constant fraction of the known-distribution revenue when all candidate distributions exhibit the monotone hazard rate property. © 2017 ACM.",Auctions; Hazard rate; Mechanism design; Posted prices; Pricing,Economics; Hazards; Machine design; Auctions; Hazard rates; Mechanism design; Negative sides; Posted prices; Price mechanism; Prior distribution; Randomized mechanism; Costs
The AND-OR game,2016,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045358518&doi=10.1145%2f2897186&partnerID=40&md5=9e7ddc1218110e310daa39462632d6eb,"We consider a simple simultaneous first price auction for two identical items in a complete information setting. Our goal is to analyze this setting for a simple, yet highly interesting, AND-OR game, where one agent is single minded and the other is unit demand. We find a mixed equilibrium of this game and show that every other equilibrium admits the same expected allocation and payments. In addition, we study the equilibrium, highlighting the change in revenue and social welfare as a function of the players' valuations. © 2016 ACM.",AND player; Auctions; Equilibrium; F. [theory of computation]: algorithmic game theory and mechanism design/market equilibriums; Theory,Game theory; Machine design; Phase equilibria; Algorithmic Game Theory; AND player; Auctions; Complete information; First price auction; Social welfare; Theory; Computation theory
Risk sensitivity of price of anarchy under uncertainty,2016,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85037053650&doi=10.1145%2f2930956&partnerID=40&md5=7df4a6ac9e3e04f813458d2168a99ee8,"In game theory, the price of anarchy framework studies efficiency loss in decentralized environments. Optimization and decision theory, on the other hand, explore tradeoffs between optimality and robustness in the case of single-agent decision making under uncertainty. What happens when we combine both approaches? We examine connections between the efficiency loss due to decentralization and the efficiency loss due to uncertainty and establish tight performance guarantees for distributed systems in uncertain environments. We present applications based on novel variants of atomic congestion games with uncertain costs, for which we provide tight performance bounds under a wide range of risk attitudes. Our results establish that the individual's attitude toward uncertainty has a critical effect on system performance and therefore should be a subject of close and systematic investigation. © 2016 ACM.",Congestion games; Robust optimization; Stochastic optimization,Costs; Decision theory; Efficiency; Game theory; Optimization; Congestion Games; Decision making under uncertainty; Distributed systems; Performance bounds; Performance guarantees; Robust optimization; Stochastic optimizations; Uncertain environments; Decision making
On the Limitations of Greedy Mechanism Design for Truthful Combinatorial Auctions,2016,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045371996&doi=10.1145%2f2956585&partnerID=40&md5=26e91b2db97f40c4eb6eb142964dac6e,"We study mechanisms for the combinatorial auction (CA) problem, in which m objects are sold to rational agents and the goal is to maximize social welfare. Of particular interest is the special case of s-CAs, where agents are interested in sets of size at most s, for which a simple greedy algorithm obtains an s + 1 approximation, but no polynomial time deterministic truthful mechanism is known to attain an approximation ratio better than O(m/√logm). We view this not only as an extreme gap between the power of greedy auctions and truthful greedy auctions, but also as exemplifying the gap between the known power of truthful and non-truthful polynomial time deterministic algorithms. We associate the notion of greediness with a broad class of algorithms, known as priority algorithms, which encapsulate many natural auction methods. This motivates us to ask: how well can a truthful greedy algorithm approximate the optimal social welfare for CA problems? We show that no truthful greedy priority algorithm can obtain an approximation to the CA problem that is sublinear in m, even for s-CAs with s ≥ 2. Our inapproximations are independent of any time constraints on the mechanism and are purely a consequence of the greedy-type restriction. We conclude that any truthful combinatorial auction mechanism with a non-trivial approximation factor must fall outside the scope of many natural auction methods. © 2016 ACM.",Combinatorial auctions; Greedy algorithms; Truthfulness,Commerce; Machine design; Polynomial approximation; Approximation factor; Approximation ratios; Combinatorial auction; Greedy algorithms; Polynomial time deterministic algorithm; Priority algorithms; Truthful mechanisms; Truthfulness; Approximation algorithms
Robust Quantitative Comparative Statics for a Multimarket Paradox,2016,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045360552&doi=10.1145%2f2956580&partnerID=40&md5=b703eeaed2725482cb81bb3d387d7b3a,"We introduce a quantitative approach to comparative statics that allows to bound the maximum effect of an exogenous parameter change on a system's equilibrium. The motivation for this approach is a well-known paradox in multimarket Cournot competition, where a positive price shock on a monopoly market may actually reduce the monopolist's profit. We use our approach to quantify for the first time the worst-case profit reduction for multimarket oligopolies exposed to arbitrary positive price shocks. For markets with affine price functions and firms with convex cost technologies, we show that the relative profit loss of any firm is at most 25%, no matter how many firms compete in the oligopoly. We further investigate the impact of positive price shocks on total profit of all firms as well as on social welfare. We find tight bounds also for thesemeasures showing that total profit and social welfare decreases by at most 25% and 16.6%, respectively. Finally, we show that in our model, mixed, correlated, and coarse correlated equilibria are essentially unique, thus, all our bounds apply to these game solutions, as well. © 2016 ACM.",Efficiency loss; Multimarket Cournot competition; Price shocks,Commerce; Competition; Profitability; Comparative statics; Correlated equilibria; Cournot competition; Efficiency loss; Monopoly markets; Parameter changes; Price shocks; Quantitative approach; Costs
An antifolk theorem for large repeated games,2016,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044336364&doi=10.1145%2f2976734&partnerID=40&md5=a478644ca1c81caccf0fdaf84cc17e25,"In this article, we study infinitely repeated games in settings of imperfect monitoring. We first prove a family of theorems showing that when the signals observed by the players satisfy a condition known as (∈, γ )-differential privacy, the folk theorem has little bite: for values of ∈ and γ sufficiently small, for a fixed discount factor, any equilibrium of the repeated game involves players playing approximate equilibria of the stage game in every period. Next we argue that in large games (n player games in which unilateral deviations by single players have only a small impact on the utility of other players), many monitoring settings naturally lead to signals that satisfy (∈, γ )-differential privacy for ∈and γ tending to zero as the number of players n grows large.We conclude that in such settings, the set of equilibria of the repeated game collapses to the set of equilibria of the stage game. Our results nest and generalize previous results of Green [1980] and Sabourian [1990], suggesting that differential privacy is a natural measure of the ""largeness"" of a game. Further, techniques from the literature on differential privacy allow us to prove quantitative bounds, where the existing literature focuses on limiting results.",Differential privacy; Folk theorem; Repeated games,Differential privacies; Discount factors; Folk theorems; Imperfect monitoring; N-player games; Natural measure; nocv1; Quantitative bounds; Repeated games
Computing dominance-based solution concepts,2016,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85043615955&doi=10.1145%2f2963093&partnerID=40&md5=e516b531117c61433c4df81fe0e207aa,"Two common criticisms of Nash equilibrium are its dependence on very demanding epistemic assumptions and its computational intractability. We study the computational properties of less demanding set-valued solution concepts that are based on varying notions of dominance. These concepts are intuitively appealing, always exist, and admit unique minimal solutions in important subclasses of games. Examples include Shapley's saddles, Harsanyi and Selten's primitive formations, Basu and Weibull's CURB sets, and Dutta and Laslier's minimal covering set. Based on a unifying framework proposed by Duggan and Le Breton, we formulate two generic algorithms for computing these concepts and investigate for which classes of games and which properties of the underlying dominance notion the algorithms are sound and efficient.We identify two sets of conditions that are sufficient for polynomial-Time computability and show that the conditions are satisfied, for instance, by saddles and primitive formations in normal-form games, minimal CURB sets in two-player games, and the minimal covering set in symmetric matrix games. Our positive algorithmic results explain regularities observed in the literature, but also apply to several solution concepts whose computational complexity was previously unknown. © 2016 ACM.",Curb sets; Game theory; Shapley's saddles; Solution concepts,Computer games; Computer programming; Curbs; Parallel processing systems; Polynomial approximation; Computational properties; Generic algorithm; Minimal solutions; Normal form games; Polynomial time computability; Shapley; Solution concepts; Symmetric matrices; Game theory
Do capacity constraints constrain coalitions?,2016,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045359420&doi=10.1145%2f2955090&partnerID=40&md5=d0fc970783c971e4d6df952165eb5edd,"We study strong equilibria in symmetric capacitated cost-sharing connection games. In these games, a graph with designated source s and sink t is given, and each edge is associated with some cost. Each agent chooses strategically an s-t path, knowing that the cost of each edge is shared equally between all agents using it. Two settings of cost-sharing connection games have been previously studied: (i) games where coalitions can form, and (ii) games where edges are associated with capacities; both settings are inspired by real-life scenarios. In this work we combine these scenarios and analyze strong equilibria (profiles where no coalition can deviate) in capacitated games. This combination gives rise to new phenomena that do not occur in the previous settings. Our contribution is twofold. First, we provide a topological characterization of networks that always admit a strong equilibrium. Second, we establish tight bounds on the efficiency loss that may be incurred due to strategic behavior, as quantified by the strong price of anarchy (and stability) measures. Interestingly, our results qualitatively differ from those obtained in the analysis of each scenario alone, and the combination of coalitions and capacities entails the introduction of more refined topology classes than previously studied. © 2016 ACM.",Capacities; Coalitions; Cost-sharing games; Network congestion games; Network design games; Network topology; Strong equilibrium; Strong price of anarchy,Cost effectiveness; Topology; Capacities; Coalitions; Cost-sharing games; Network congestion game; Network design; Network topology; Price of anarchy; Strong equilibrium; Costs
When Does Improved Targeting Increase Revenue?,2016,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85035079142&doi=10.1145%2f2956586&partnerID=40&md5=1ef277ef05480658bca7c946786f3715,"In second-price auctions, we find that improved targeting via enhanced information disclosure decreases revenue when there are two bidders and increases revenue if there are at least four symmetric bidders with values drawn from a distribution with a monotone hazard rate. With asymmetries, improved targeting increases revenue if themost frequent winner wins less than 30.4% of the time under amodel in which shares are well defined, but can decrease revenue otherwise. We derive analogous results for position auctions. Finally, we show that revenue can vary nonmonotonically with the number of bidders who are able to take advantage of improved targeting. © 2016 ACM.",Advertising; Online auctions; Position auctions; Revenue; Targeting,Earnings; Marketing; A-monotone; Hazard rates; Information disclosure; Online auctions; Position auctions; Second-price auction; Targeting; Commerce
A rational convex program for linear Arrow-Debreu markets,2016,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032838636&doi=10.1145%2f2930658&partnerID=40&md5=1e264f2c46e88fbb762543f73b6e8303,"We present a new flow-type convex program describing equilibrium solutions to linear Arrow-Debreu markets. Whereas convex formulations were previously known ([Nenakov and Primak 1983; Jain 2007; Cornet 1989]), our program exhibits several new features. It provides a simple necessary and sufficient condition and a concise proof of the existence and rationality of equilibria, settling an open question raised by Vazirani [2012]. As a consequence, we also obtain a simple new proof of the result in Mertens [2003] that the equilibrium prices form a convex polyhedral set. © 2016 ACM.",Convex programming; Linear exchange market; Market equilibrium,Convex optimization; Convex polyhedral; Convex programs; Equilibrium price; Equilibrium solutions; Exchange markets; Flow type; Market equilibria; Commerce
Truthful mechanisms for combinatorial allocation of electric power in alternating current electric systems for smart grid,2016,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979277417&doi=10.1145%2f2955089&partnerID=40&md5=7c9cd718b3a92a893fafd0809ea05e5a,"Traditional studies of combinatorial auctions often only consider linear constraints. The rise of smart grid presents a new class of auctions, characterized by quadratic constraints. This article studies the complexdemand knapsack problem, inwhich the demands are complex valued and the capacity of supplies is described by the magnitude of total complex-valued demand. This naturally captures the power constraints in alternating current electric systems. In this article, we provide a more complete study and generalize the problem to the multi-minded version, beyond the previously known 1/2-approximation algorithm for only a subclass of the problem. More precisely, we give a truthful polynomial-time approximation scheme (PTAS) for the case φ ϵ [0, π/2-δ] and a truthful fully polynomial-time approximation scheme (FPTAS), which fully optimizes the objective function but violates the capacity constraint by at most (1 + ϵ), for the case φ ϵ ( π/2 , π - δ], where φ is the maximum argument of any complex-valued demand and ϵ δ > 0 are arbitrarily small constants. We complement these results by showing that, unless P=NP, neither a PTAS for the case φ ϵ ( π/2 , π - δ] nor any bi-criteria approximation algorithm with polynomial guarantees for the case when φ is arbitrarily close to π (that is, when δ is arbitrarily close to 0) can exist. © 2016 ACM.",Combinatorial power allocation; Complex-demand knapsack problem; Mechanism design; Multi-unit combinatorial auctions; Smart grid,Approximation algorithms; Approximation theory; Combinatorial optimization; Commerce; Electric impedance measurement; Electric power transmission networks; Machine design; Polynomial approximation; Polynomials; Knapsack problems; Mechanism design; Multi-unit combinatorial auctions; Power allocations; Smart grid; Smart power grids
"Recency, records, and recaps: Learning and nonequilibrium behavior in a simple decision problem",2016,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054003571&doi=10.1145%2f2956581&partnerID=40&md5=26b97aefe3b6c04d02f1b3c3806122fc,"Nash equilibrium takes optimization as a primitive, but suboptimal behavior can persist in simple stochastic decision problems. This has motivated the development of other equilibrium concepts such as cursed equilibrium and behavioral equilibrium. We experimentally study a simple adverse selection (or ""lemons"") problem and find that learning models that heavily discount past information (i.e., display recency bias) explain patterns of behavior better than Nash, cursed, or behavioral equilibrium. Providing counterfactual information or a record of past outcomes does little to aid convergence to optimal strategies, but providing sample averages (""recaps"") gets individuals most of the way to optimality. Thus, recency effects are not solely due to limited memory but stem from some other form of cognitive constraints. Our results show the importance of going beyond static optimization and incorporating features of human learning into economic models used in both understanding phenomena and designing market institutions. © 2016 ACM.",Behavioral economics; Equilibrium concepts; Learning; Recency,Risk management; Stochastic systems; Behavioral economics; Decision problems; Learning; Nonequilibrium behavior; Optimal strategies; Recency; Simple stochastic; Static optimization; Economics
The complexity of fairness through equilibrium,2016,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054646124&doi=10.1145%2f2956583&partnerID=40&md5=6c2faf46f67dcf9e081513acf7a8b4d0,"Competitive equilibrium from equal incomes (CEEI) is a well-known fair allocation mechanism with desirable fairness and efficiency properties; however, with indivisible resources, a CEEI may not exist [Foley 1967; Varian 1974; Thomson and Varian 1985]. It was shown in Budish [2011] that in the case of indivisible resources, there is always an allocation, called A-CEEI, that is approximately fair, approximately truthful, and approximately efficient for some favorable approximation parameters. A heuristic search that attempts to find this approximation is used in practice to assign business school students to courses. In this article, we show that finding the A-CEEI allocation guaranteed to exist by Budish's theorem is PPAD-complete. We further show that finding an approximate equilibrium with better approximation guarantees is even harder: NP-complete. © 2016 ACM.",Economics; Theory,Economics; Approximate equilibriums; Business schools; Competitive equilibrium; Fair allocation; Heuristic search; NP Complete; Theory; Thomson; Heuristic algorithms
Local computation mechanism design,2016,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045357969&doi=10.1145%2f2956584&partnerID=40&md5=c47c80ad9135c218d87f120dfca638c2,"We introduce the notion of local computation mechanism design-designing game-theoretic mechanisms that run in polylogarithmic time and space. Local computation mechanisms reply to each query in polylogarithmic time and space, and the replies to different queries are consistent with the same global feasible solution. When the mechanism employs payments, the computation of the payments is also done in polylogarithmic time and space. Furthermore, the mechanism needs to maintain incentive compatibility with respect to the allocation and payments. We present local computation mechanisms for two classical game-theoretical problems: stable matching and job scheduling. For stable matching, some of our techniques may have implications to the global (non-LCA (Local Computation Algorithm)) setting. Specifically, we show that when the men's preference lists are bounded, we can achieve an arbitrarily good approximation to the stable matching within a fixed number of iterations of the Gale-Shapley algorithm. © 2016 ACM 2167-8375/2016/08-ART21 $15.00.",Local computation algorithms; Mechanism design; Stable matching,Algorithms; Approximation algorithms; Machine design; Feasible solution; Gale-shapley algorithms; Incentive compatibility; Local computation; Mechanism design; Polylogarithmic time; Preference lists; Stable matching; Game theory
Introduction to the special issue on EC'14,2016,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045372557&doi=10.1145%2f2953046&partnerID=40&md5=14edf6b724bc996c5927c766501a381b,[No abstract available],,
Finding approximate Nash equilibria of bimatrix games via payoff queries,2016,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045373518&doi=10.1145%2f2956579&partnerID=40&md5=90c3e6cff522d3f6724a275e473fb758,"We study the deterministic and randomized query complexity of finding approximate equilibria in a k × k bimatrix game. We show that the deterministic query complexity of finding an ∈-Nash equilibrium when ∈ < 1/2 is Ω(k2), even in zero-one constant-sum games. In combination with previous results [Fearnley et al. 2013], this provides a complete characterization of the deterministic query complexity of approximate Nash equilibria. We also study randomized querying algorithms. We give a randomized algorithm for finding a (3-√5/2 + ∈)-Nash equilibrium using O(k·log k/∈2) payoff queries, which shows that the 1/2 barrier for deterministic algorithms can be broken by randomization. For well-supported Nash equilibria (WSNE), we first give a randomized algorithm for finding an ∈-WSNE of a zero-sum bimatrix game using O(k·log k/∈4) payoff queries, and we then use this to obtain a randomized algorithm for finding a (2/3 +∈)-WSNE in a general bimatrix game using O(k·log k/∈4) payoff queries. Finally, we initiate the study of lower bounds against randomized algorithms in the context of bimatrix games, by showing that randomized algorithms require Ω(k2) payoff queries in order to find an ∈-Nash equilibrium with ∈ < 1/4k, even in zero-one constant-sum games. In particular, this rules out query-efficient randomized algorithms for finding exact Nash equilibria. © 2016 ACM.",Approximate Nash equilibrium; Bimatrix game; Payoff query complexity; Randomized algorithms,Algorithms; Computation theory; Game theory; Telecommunication networks; Bimatrix games; Constant-sum game; Deterministic algorithms; Lower bounds; Nash equilibria; Query complexity; Randomized Algorithms; Zero sums; Computational complexity
Optimal contest design for simple agents,2016,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031907563&doi=10.1145%2f2930955&partnerID=40&md5=1ccb531fd0a70f324178a21358a83f0a,"Incentives are more likely to elicit desired outcomes when they are designed based on accurate models of agents' strategic behavior. A growing literature, however, suggests that people do not quite behave like standard economic agents in a variety of environments, both online and offline. What consequences might such differences have for the optimal design ofmechanisms in these environments? In this article, we explore this question in the context of optimal contest design for simple agents-agents who strategically reason about whether or not to participate in a system, but not about the input they provide to it. Specifically, consider a contest where n potential contestants with types (qi , ci ) each choose between participating and producing a submission of quality qi at cost ci , versus not participating at all to maximize their utilities. How should a principal distribute a total prize V among the n ranks to maximize some increasing function of the qualities of elicited submissions in a contest with such simple agents? We first solve the optimal contest design problem for settings where agents have homogenous participation costs ci = c. Here, the contest that maximizes every increasing function of the elicited contributions is always a simple contest, awarding equal prizes of V/j∗ each to the top j∗ = V/c - Θ(contestants. This is in contrast to the optimal contest structure in comparable models with strategic effort choices, where the optimal contest is either a winner-take-all contest or awards possibly unequal prizes, depending on the curvature of agents' effort cost functions. We next address the general case with heterogenous costs where agents' types (qi , ci ) are inherently two dimensional, significantly complicating equilibrium analysis. With heterogenous costs, the optimal contest depends on the objective being maximized: our main result here is that the winner-take-all contest is a 3-approximation of the optimal contest when the principal's objective is to maximize the quality of the best elicited contribution. The proof of this result hinges around a ""subequilibrium"" lemma establishing a stochastic dominance relation between the distribution of qualities elicited in an equilibrium and a subequilibrium-a strategy profile that is a best response for all agents who choose to participate in that strategy profile; this relation between equilibria and subequilibria may be of more general interest. © 2016 ACM.",-User-generated content; Contest design; Crowdsourcing; Mechanism design,Behavioral research; Cost functions; Crowdsourcing; Machine design; Stochastic systems; Structural optimization; Contest designs; Equilibrium analysis; Increasing functions; Mechanism design; Stochastic Dominance; Strategic Behavior; User-generated content; Winner take alls; Cost benefit analysis
Bounds for the query complexity of approximate equilibria,2016,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041109090&doi=10.1145%2f2956582&partnerID=40&md5=f05cf8762da786a9bc0a5c756adbd0bb,"We analyze the number of payoff queries needed to compute approximate equilibria of multi-player games. We find that query complexity is an effective tool for distinguishing the computational difficulty of alternative solution concepts, and we develop new techniques for upper- and lower bounding the query complexity. For binary-choice games, we show logarithmic upper and lower bounds on the query complexity of approximate correlated equilibrium. For well-supported approximate correlated equilibrium (a restriction where a player's behavior must always be approximately optimal, in the worst case over draws from the distribution) we show a linear lower bound, thus separating the query complexity of well supported approximate correlated equilibrium from the standard notion of approximate correlated equilibrium. Finally, we give a query-efficient reduction from the problem of computing an approximate well-supported Nash equilibrium to the problem of verifying a well supported Nash equilibrium, where the additional query overhead is proportional to the description length of the game. This gives a polynomial-query algorithm for computing well supported approximate Nash equilibria (and hence correlated equilibria) in concisely represented games. We identify a class of games (which includes congestion games) in which the reduction can be made not only query efficient, but also computationally efficient. © 2016 ACM.",Correlated equilibrium; Payoff queries,Computation theory; Game theory; Telecommunication networks; Alternative solutions; Computationally efficient; Congestion Games; Correlated equilibria; Multiplayer games; Payoff queries; Query algorithms; Upper and lower bounds; Computer games
"Incentives, gamification, and game theory: An economic approach to badge design",2016,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045359008&doi=10.1145%2f2910575&partnerID=40&md5=0738dc6edddb02f8b44af72394b6f2f8,"Gamification is growing increasingly prevalent as a means to incentivize user engagement of social media sites that rely on user contributions. Badges, or equivalent rewards, such as top-contributor lists that are used to recognize a user's contributions on a site, clearly appear to be valued by users who actively pursue and compete for them. However, different sites use different badge designs, varying how, and for what, badges are awarded. Some sites, such as Stack Overflow, award badges for meeting fixed levels of contribution. Other sites, such as Amazon and Y! Answers, reward users for being among some top set of contributors on the site, corresponding to a competitive standard of performance. Given that users value badges, and that contributing to a site requires effort, how badges are designed will affect the incentives-therefore the participation and effort-elicited from strategic users on a site. We take a game-theoretic approach to badge design, analyzing the incentives created by widely used badge designs in a model in which winning a badge is valued, effort is costly, and potential contributors to the site endogenously decide whether or not to participate, and how much total effort to put into their contributions to the site. We analyze equilibrium existence, as well as equilibrium participation and effort, in an absolute standards mechanism Mα in which badges are awarded for meeting some absolute level of (observed) effort, and a relative standards mechanism Mρ corresponding to competitive standards, as in a top-ρ contributor badge. We find that equilibria always exist in both mechanisms, even when the value from winning a badge depends endogenously on the number of other winners. However, Mα has zero-participation equilibria for standards that are too high, whereas all equilibria in Mρ elicit nonzero participation for all possible ρ, provided that ρ is specified as a fixed number rather than as a fraction of actual contributors (note that the two are not equivalent in a setting with endogenous participation). Finally, we ask whether or not a site should explicitly announce the number of users winning a badge. The answer to this question is determined by the curvature of the value of winning the badge as a function of the number of other winners. © 2016 ACM.",Badges; Game theory; Gamification; Incentives; Social media; User-generated content,Design; Social networking (online); Standards; Badges; Gamification; Incentives; Social media; User-generated content; Game theory
Ranking and tradeoffs in sponsored search auctions,2016,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045358782&doi=10.1145%2f2910576&partnerID=40&md5=a5a5e1c6889e6c4db282d87941368676,"In a sponsored search auction, decisions about how to rank ads impose tradeoffs between objectives, such as revenue and welfare. In this article, we examine how these tradeoffs should be made. We begin by arguing that the most natural solution concept to evaluate these tradeoffs is the lowest symmetric Nash equilibrium (SNE). As part of this argument, we generalise the well-known connection between the lowest SNE and the VCG outcome. We then propose a new ranking algorithm, loosely based on the revenue-optimal auction, that uses a reserve price to order the ads (not just to filter them) and give conditions under which it raises more revenue than simply applying that reserve price. Finally, we conduct extensive simulations examining the tradeoffs enabled by different ranking algorithms and show that our proposed algorithm enables superior operating points by a variety of metrics. © 2016 ACM.",Generalized second price; Keyword auctions; Reserve price; Sponsored search,Costs; Extensive simulations; Generalized second price; Keyword auctions; Ranking algorithm; Reserve price; Solution concepts; Sponsored search auctions; Sponsored searches; Commerce
Introduction,2016,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045360955&doi=10.1145%2f2916701&partnerID=40&md5=7cd17f7db846437769cf8c4db34f0b87,[No abstract available],,
Optimal and robust mechanism design with interdependent values,2016,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045354783&doi=10.1145%2f2910577&partnerID=40&md5=75bed9b07d22d83dce3af8016e9f3b04,"We study interdependent value settings and extend several results from the well-studied independent private values model to these settings. For revenue-optimal mechanism design, we give conditions under which Myerson's virtual value-based mechanism remains optimal with interdependent values. One of these conditions is robustness of the truthfulness and individual rationality guarantees, in the sense that they are required to hold ex-post. We then consider an even more robust class of mechanisms called ""prior independent"" (""detail free""), and show that, by simply using one of the bidders to set a reserve price, it is possible to extract near-optimal revenue in an interdependent values setting. This shows that a considerable level of robustness is achievable for interdependent values in single-parameter environments. © 2016 ACM.",Correlated values; Interdependence; Myerson theory; Optimal auctions; Prior independence,Correlated values; Interdependence; Myerson theory; Optimal auction; Prior independence; Machine design
Whole-page optimization and submodular welfare maximization with online bidders,2016,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049890010&doi=10.1145%2f2892563&partnerID=40&md5=d13e36524d641789e80f996da2a8b99b,"In the context of online ad serving, display ads may appear on different types of webpages, where each page includes several ad slots and therefore multiple ads can be shown on each page. The set of ads that can be assigned to ad slots of the same page needs to satisfy various prespecified constraints including exclusion constraints, diversity constraints, and the like. Upon arrival of a user, the ad serving system needs to allocate a set of ads to the current webpage respecting these per-page allocation constraints. Previous slot-based settings ignore the important concept of a page and may lead to highly suboptimal results in general. In this article, motivated by these applications in display advertising and inspired by the submodular welfare maximization problem with online bidders, we study a general class of page-based ad allocation problems, present the first (tight) constant-factor approximation algorithms for these problems, and confirm the performance of our algorithms experimentally on real-world datasets. A key technical ingredient of our results is a novel primal-dual analysis for handling free disposal, which updates dual variables using a “level function” instead of a single level and unifies with previous analyses of related problems. This new analysis method allows us to handle arbitrarily complicated allocation constraints for each page. Our main result is an algorithm that achieves a 1 −1 e − o(1)-competitive ratio. Moreover, our experiments on real-world datasets show significant improvements of our page-based algorithms compared to the slot-based algorithms. Finally, we observe that our problem is closely related to the submodular welfare maximization (SWM) problem. In particular, we introduce a variant of the SWM problem with online bidders and show how to solve this problem using our algorithm for whole-page optimization. c 2016 ACM",Media access control; Multichannel; Radio interference; Time synchronization; Wireless sensor networks,Approximation algorithms; Medium access control; Radio interference; Storm sewers; Websites; Wireless sensor networks; Allocation problems; Constant-factor approximation algorithms; Display advertisings; Media access control; Multichannel; Real-world datasets; Time synchronization; Welfare maximizations; Problem solving
Truthful mechanisms for agents that value privacy,2016,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041497345&doi=10.1145%2f2892555&partnerID=40&md5=94a595ece102ea2bfeefbe927503c057,"Recent work has constructed economic mechanisms that are both truthful and differentially private. In these mechanisms, privacy is treated separately from truthfulness; it is not incorporated in players' utility functions (and doing so has been shown to lead to nontruthfulness in some cases). In this work, we propose a new, general way of modeling privacy in players' utility functions. Specifically, we only assume that if an outcome o has the property that any report of player i would have led to o with approximately the same probability, then o has a small privacy cost to player i. We give three mechanisms that are truthful with respect to our modeling of privacy: for an election between two candidates, for a discrete version of the facility location problem, and for a general social choice problem with discrete utilities (via a VCG-like mechanism). As the number n of players increases, the social welfare achieved by our mechanisms approaches optimal (as a fraction of n). © 2016 ACM.",Differential privacy; Mechanism design; Social choice; Truthfulness; VCG,Machine design; Differential privacies; Economic mechanisms; Facility location problem; Mechanism design; Social choice; Social choice problems; Truthful mechanisms; Truthfulness; Decision theory
When do noisy votes reveal the truth?,2016,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025831141&doi=10.1145%2f2892565&partnerID=40&md5=0b971b2d657a7206a83e5d7577f253cd,"A well-studied approach to the design of voting rules views them as maximum likelihood estimators; given votes that are seen as noisy estimates of a true ranking of the alternatives, the rule must reconstruct the most likely true ranking.We argue that this is too stringent a requirement and instead ask: how many votes does a voting rule need to reconstruct the true ranking? We define the family of pairwise-majority consistent rules and show that for all rules in this family, the number of samples required from Mallows's noise model is logarithmic in the number of alternatives, and that no rule can do asymptotically better (while some rules like plurality do much worse). Taking a more normative point of view, we consider voting rules that surely return the true ranking as the number of samples tends to infinity (we call this property accuracy in the limit); this allows us to move to a higher level of abstraction. We study families of noise models that are parameterized by distance functions and find voting rules that are accurate in the limit for all noise models in such general families. We characterize the distance functions that induce noise models for which pairwise-majority consistent rules are accurate in the limit and provide a similar result for another novel family of position-dominance consistent rules. These characterizations capture three well-known distance functions. © 2016 ACM.",Computational social choice; Mallows's model; Sample complexity,Maximum likelihood; Computational social choices; Distance functions; Level of abstraction; Maximum likelihood estimator; Noise modeling; Number of samples; Parameterized; Sample complexity; Maximum likelihood estimation
Network cost-sharing without anonymity,2016,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84999226393&doi=10.1145%2f2841228&partnerID=40&md5=326193929223418948989700c8ee5d88,"We consider network cost-sharing games with nonanonymous cost functions, where the cost of each edge is a submodular function of its users, and this cost is shared using the Shapley value. Nonanonymous cost functions model asymmetries between the players, which can arise from different bandwidth requirements, durations of use, services needed, and so on. These games can possess multiple Nash equilibria of wildly varying quality. The goal of this article is to identify well-motivated equilibrium refinements that admit good worst-case approximation bounds. Our primary results are tight bounds on the cost of strong Nash equilibria and potential function minimizers in network cost-sharing games with nonanonymous cost functions, parameterized by the set C of allowable submodular cost functions. These two worst-case bounds coincide for every set C, and equal the summability parameter introduced in Roughgarden and Sundararajan [2009] to characterize efficiency loss in a family of cost-sharing mechanisms. Thus, a single parameter simultaneously governs the worst-case inefficiency of network cost-sharing games (in two incomparable senses) and cost-sharing mechanisms. This parameter is always at most the kth Harmonic number Hk ≈ ln k, where k is the number of players, and is constant for many function classes of interest.",Equilibrium refinement; Network cost-sharing games; Potential function minimizer; Strong nash equilibria; Summability,Cost functions; Costs; Number theory; Equilibrium refinements; Nash equilibria; Network costs; Potential function; Summability; Cost effectiveness
Truthfulness and stochastic dominance with monetary transfers,2016,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045373341&doi=10.1145%2f2847522&partnerID=40&md5=4e42691bbd425444dd4c96c117b1c8e2,"We consider truthfulness concepts for auctions with payments based on first- and second-order stochastic dominance. We assume bidders consider wealth in standard quasilinear form as valuation minus payments. Additionally, they are sensitive to risk in the distribution of wealth stemming from randomized mechanisms. First- and second-order stochastic dominance are well known to capture risk sensitivity, and we apply these concepts to capture truth-telling incentives for bidders. As our first main result, we provide a complete characterization of all social-choice functions over binary single-parameter domains that can be implemented by a mechanism that is truthful in first- and secondorder stochastic dominance. We show that these are exactly the social-choice functions implementable by truthful-in-expectation mechanisms, and we provide a novel payment rule that guarantees stochastic dominance. As our second main result we extend the celebrated randomized metarounding approach for truthfulin- expectation mechanisms in packing domains. We design mechanisms that are truthful in first-order stochastic dominance by spending only a logarithmic factor in the approximation guarantee. © 2016 ACM.",Combinatorial auctions; Mechanism design; Randomized metarounding; Stochastic dominance; Truthfulness,Commerce; Decision theory; Economics; Machine design; Combinatorial auction; Mechanism design; Randomized metarounding; Stochastic Dominance; Truthfulness; Stochastic systems
Tight bounds for the price of anarchy of simultaneous,2016,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029356882&doi=10.1145%2f2847520&partnerID=40&md5=6891213ab9a3df638183bf1c1a850be9,"We study the price of anarchy (PoA) of simultaneous first-price auctions (FPAs) for buyers with submodular and subadditive valuations. The current best upper bounds for the Bayesian price of anarchy (BPoA) of these auctions are e/(e - 1) [Syrgkanis and Tardos 2013] and 2 [Feldman et al. 2013], respectively. We provide matching lower bounds for both cases even for the case of full information and for mixed Nash equilibria via an explicit construction. We present an alternative proof of the upper bound of e/(e - 1) for FPAs with fractionally subadditive valuations that reveals the worst-case price distribution, which is used as a building block for the matching lower bound construction. We generalize our results to a general class of item bidding auctions that we call bid-dependent auctions (including FPAs and all-pay auctions) where the winner is always the highest bidder and each bidder's payment depends only on his own bid. Finally, we apply our techniques to discriminatory price multiunit auctions. We complement the results of de Keijzer et al. [2013] for the case of subadditive valuations by providing a matching lower bound of 2. For the case of submodular valuations, we provide a lower bound of 1.109. For the same class of valuations, we were able to reproduce the upper bound of e/(e - 1) using our nonsmooth approach.",Auctions; Item bidding; Price of anarchy,Commerce; Focal plane arrays; Auctions; Building blockes; Explicit constructions; First price auction; Full informations; Item bidding; Multi-unit auction; Price of anarchy; Costs
Mechanism design with strategic mediators,2016,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042895795&doi=10.1145%2f2841227&partnerID=40&md5=4b3d9d1ee85ba3d8fed18c0e731aec69,"We consider the problem of designing mechanisms that interact with strategic agents through strategic intermediaries (or mediators), and investigate the cost to society due to the mediators' strategic behavior. Selfish agents with private information are each associated with exactly one strategic mediator, and can interact with the mechanism exclusively through that mediator. Each mediator aims to optimize the combined utility of his agents, while the mechanism aims to optimize the combined utility of all agents. We focus on the problem of facility location on a metric induced by a publicly known tree. With nonstrategic mediators, there is a dominant strategy mechanism that is optimal. We show that when both agents and mediators act strategically, there is no dominant strategy mechanism that achieves any approximation. We, thus, slightly relax the incentive constraints, and define the notion of a two-sided incentive compatible mechanism. We show that the 3-competitive deterministic mechanism suggested by Procaccia and Tennenholtz [2013] and Dekel et al. [2010] for lines extends naturally to trees, and is still 3-competitive as well as two-sided incentive compatible. This is essentially the best possible (follows from Dekel et al. [2010] and Procaccia and Tennenholtz [2013]). We then show that by allowing randomization one can construct a 2-competitive randomized mechanism that is two-sided incentive compatible, and this is also essentially tight. This result also reduces a gap left in the work of Procaccia and Tennenholtz [2013] and Lu et al. [2009] for the problem of designing strategy-proof mechanisms for weighted agents with no mediators on a line. We also investigate a generalization of the preceding setting where there are multiple levels of mediators.",Facility location; Mechanism design; Mediators,Forestry; Machine design; Deterministic mechanism; Facility locations; Incentive compatible; Incentive compatible mechanisms; Mechanism design; Mediators; Private information; Randomized mechanism; Aluminum
Matching Tasks and Workers under Known Arrival Distributions: Online Task Assignment with Two-sided Arrivals,2024,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85196291168&doi=10.1145%2f3652021&partnerID=40&md5=cdca45de21c32b7e02271607a5fc7bfd,"Efficient allocation of tasks to workers is a central problem in crowdsourcing. In this article, we consider a setting inspired by spatial crowdsourcing platforms, where both workers and tasks arrive at different times, and each worker-task assignment yields a given reward. The key challenge is to address the uncertainty in the stochastic arrivals from both workers and the tasks. In this work, we consider a ubiquitous scenario where the arrival patterns of worker ""types""and task ""types""are not erratic but can be predicted from historical data. Specifically, we consider a finite time horizon T and assume that in each time-step the arrival of a worker and a task can be seen as an independent sample from two (different) distributions.Our model, called Online Task Assignment with Two-sided Arrival (OTA-TSA), is a significant generalization of the classical online task assignment problem when all the tasks are statically available. For the general case of OTA-TSA, we present an optimal non-adaptive algorithm (NADAP), which achieves a competitive ratio (CR) of at least 0.295. For a special case of OTA-TSA when the reward depends only on the worker type, we present two adaptive algorithms, which achieve CRs of at least 0.343 and 0.355, respectively. On the hardness side, we show that (1) no non-adaptive can achieve a CR larger than that of NADAP, establishing the optimality of NADAP among all non-adaptive algorithms; and (2) no (adaptive) algorithm can achieve a CR better than 0.581 (unconditionally) or 0.423 (conditionally on the benchmark linear program), respectively. All aforementioned negative results apply to even unweighted OTA-TSA when every assignment yields a uniform reward. At the heart of our analysis is a new technical tool, called two-stage birth-death process, which is a refined notion of the classical birth-death process. We believe it may be of independent interest. Finally, we perform extensive numerical experiments on a real-world rideshare dataset collected in Chicago and a synthetic dataset, and results demonstrate the effectiveness of our proposed algorithms in practice. © 2024 Copyright held by the owner/author(s).",crowdsourcing markets; matching markets; Online matching; randomized algorithms,Adaptive algorithms; Combinatorial optimization; Commerce; Linear programming; Stochastic systems; Competitive ratio; Crowdsourcing market; Matching market; Matchings; Nonadaptive algorithm; On-line matching; Randomized Algorithms; Tasks assignments; Worker types; Workers'; Crowdsourcing
Convexity in Real-time Bidding and Related Problems,2024,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85196320515&doi=10.1145%2f3656552&partnerID=40&md5=bd92eac1f83fbe58410f23da2c346eeb,"We study problems arising in real-time auction markets, common in e-commerce and computational advertising, where bidders face the problem of calculating optimal bids. We focus upon a contract management problem where a demand aggregator is subject to multiple contractual obligations requiring them to acquire items of heterogeneous types at a specified rate and where they will seek to fulfill these obligations at minimum cost. Our main results show that, through a transformation of variables, this problem can be formulated as a convex optimization problem, for both first and second price auctions. The resulting duality theory admits rich structure and interpretations. Additionally, we show that the transformation of variables can be used to guarantee the convexity of optimal bidding problems studied by other authors, who did not leverage convexity in their analysis. The main point of our work is to emphasize that the natural convexity properties arising in first and second price auctions are not being fully exploited. Finally, we show direct analogies to problems in financial markets: the expected cost of bidding in second price auctions is formally identical to certain transaction costs when submitting market orders, and that a related conjugate function arises in dark pool financial markets. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",auction theory; computational advertising; convex optimization; first price auction; Markowitz portfolio; Real-time bidding; second price auction; transaction costs,Commerce; Computation theory; Costs; Financial markets; Marketing; Auction theory; Computational advertisings; Convex optimisation; First price auction; Markowitz's portfolios; Real- time; Real-time bidding; Second-price auction; Transaction cost; Transformation of variables; Convex optimization
Optimal Price Discrimination for Randomized Mechanisms,2024,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85196275622&doi=10.1145%2f3650107&partnerID=40&md5=ed3800769bcbd7d3a05bc7c397dc1272,"We study the power of price discrimination via an intermediary in bilateral trade, when there is a revenue-maximizing seller selling an item to a buyer with a private value drawn from a prior. Between the seller and the buyer, there is an intermediary that can segment the market by releasing information about the true values to the seller. This is termed signaling, and enables the seller to price discriminate. In this setting, Bergemann et al. [7] showed the existence of a signaling scheme that simultaneously raises the optimal consumer surplus, guarantees the item always sells, and ensures the seller's revenue does not increase.Our work extends the positive result of Bergemann et al. to settings where the type space is larger, and where the optimal auction is randomized, possibly over a menu that can be exponentially large. In particular, we consider two settings motivated by budgets: The first is when there is a publicly known budget constraint on the price the seller can charge [12] and the second is the FedEx problem [19] where the buyer has a private deadline or service level (equivalently, a private budget that is guaranteed to never bind). For both settings, we present a novel signaling scheme and its analysis via a continuous construction process that recreates the optimal consumer surplus guarantee of Bergemann et al. and further subsumes their signaling scheme as a special case. In effect, our results show settings where even though the optimal auction is randomized over a possibly large menu, there is a market segmentation such that for each segment, the optimal auction is a simple posted price scheme where the item is always sold.The settings we consider are special cases of the more general problem where the buyer has a private budget constraint in addition to a private value. We finally show that our positive results do not extend to this more general setting, particularly when the budget can bind in the optimal auction, and when the seller's mechanism allows for all-pay auctions. Here, we show that any efficient signaling scheme necessarily transfers almost all the surplus to the seller instead of the buyer. © 2024 Copyright held by the owner/author(s).",auctions; Bilateral trade; signaling,Budget control; Sales; Signaling; Auction; Bilateral trade; Budget constraint; Consumer's surplus; Optimal auction; Power; Price discrimination; Randomized mechanism; Revenue maximizing; Signaling; Commerce
Blockchain-based Decentralized Reward Sharing: The Case of Mining Pools,2024,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85187548983&doi=10.1145%2f3641120&partnerID=40&md5=7ef3dfb48d236aac0820016daa248b5d,"We introduce a reciprocity protocol, an innovative approach to coordinating and sharing rewards in blockchains. Inherently decentralized and implementable, it puts emphasis on incentives rather than forcing specific sharing rules from the outset. Analyzing the non-cooperative game the protocol induces, we identify a robust, strict, and Pareto-dominant symmetric equilibrium. In it, even self-centered participants show extensive systemic reciprocity. Thus, despite a setting that is generally unfavorable to reciprocal behavior, the protocol enables users to build trust between themselves by taking on a role akin to a social contract.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Blockchain; Nash equilibrium; protocol design; reciprocity,Game theory; Block-chain; Decentralised; Forcings; Innovative approaches; Nash equilibria; Noncooperative game; Protocol design; Reciprocity; Sharing rules; Symmetric equilibrium; Blockchain
Price of Anarchy in Algorithmic Matching of Romantic Partners,2024,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85187554534&doi=10.1145%2f3627985&partnerID=40&md5=625417d9f13e41c076b4b934fe265d4b,"Algorithmic matching is a pervasive mechanism in our social lives and is becoming a major medium through which people find romantic partners and potential spouses. However, romantic matching markets pose a principal-Agent problem with the potential for moral hazard. The agent's (or system's) interest is to maximize the use of the matching website, while the principal's (or user's) interest is to find the best possible match. This creates a conflict of interest: The optimal matching of users may not be aligned with the platform's goal of maximizing engagement, as it could lead to long-Term relationships and fewer users using the site over time. Here, we borrow the notion of price of anarchy from game theory to quantify the decrease in social efficiency of online algorithmic matching sites where engagement is in tension with user utility. We derive theoretical bounds on the price of anarchy and show that it can be bounded by a constant that does not depend on the number of users in the system. This suggests that as online matching sites grow, their potential benefits scale up without sacrificing social efficiency. Further, we conducted experiments with human subjects in a matching market and compared the social welfare achieved by an optimal matching service against a self-interested matching algorithm. We show that introducing competition among matching sites aligns the self-interested behavior of platform designers with their users and increases social efficiency.  © 2024 Copyright held by the owner/author(s).",Additional Key Words and PhrasesOnline matching markets; online dating; price of anarchy,Commerce; Efficiency; Risk management; Social networking (online); Additional key word and phrasesonline matching market; Algorithmics; Key words; Matchings; Online dating; Optimal matching; Price of anarchy; Principal-agent problems; Social efficiency; Social life; Game theory
Tractable Binary Contests,2024,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85187557119&doi=10.1145%2f3630109&partnerID=40&md5=97a200114f7121dd99566a58cf05d9f4,"Much of the work on multi-Agent contests is focused on determining the equilibrium behavior of contestants. This capability is essential for the principal for choosing the optimal parameters for the contest (e.g., prize amount). As it turns out, many contests exhibit not one, but many possible equilibria, hence precluding contest design optimization and contestants' behavior prediction. In this article, we examine a variation of the classic contest that alleviates this problem by having contestants make the decisions sequentially rather than in parallel. We study this model in the setting of a binary contest, wherein contestants only choose whether or not to participate, while their performance level is exogenously set. We show that by switching to the sequential mechanism not only does there emerge a unique equilibrium behavior, but also that the principal can design this behavior to be as good, and, at times, better, than any pure-strategy equilibrium of the parallel setting (assuming the principal's profit is either the maximum performance or the sum of performances). We also show that in the sequential setting, the optimal prize, which is inherently a continuous parameter, can be effectively computed and reduced to a set of discrete values to be evaluated. The theoretical analysis is complemented by comprehensive experiments with people over Amazon Mechanical Turk. Here, we find that the modified mechanism offers great benefit for the principal in terms of an increased over-participation in the contest (compared to theoretical expectations). The effect on the principal average profit, however, depends on its goal in the contest-when benefiting from the maximum performance the modified mechanism results in increased average profit, while when benefiting from the sum of performances, it is preferred to stay with the original (parallel) contest.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesNon-cooperative games; auctions and market-based systems; economic paradigms,Multi agent systems; Additional key word and phrasesnon-cooperative game; Auction and market-based system; Cooperative game; Economic paradigm; Equilibrium behavior; Key words; Market-Based Systems; Modified mechanism; Multi agent; Performance; Profitability
Optimized Distortion and Proportional Fairness in Voting,2024,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85187555200&doi=10.1145%2f3640760&partnerID=40&md5=15a12cf72c9db9ebb41b79093bce3d5b,"A voting rule decides on a probability distribution over a set of m alternatives, based on rankings of those alternatives provided by agents. We assume that agents have cardinal utility functions over the alternatives, but voting rules have access to only the rankings induced by these utilities. We evaluate how well voting rules do on measures of social welfare and of proportional fairness, computed based on the hidden utility functions.In particular, we study the distortion of voting rules, which is a worst-case measure. It is an approximation ratio comparing the utilitarian social welfare of the optimum outcome to the social welfare produced by the outcome selected by the voting rule, in the worst case over possible input profiles and utility functions that are consistent with the input. The previous literature has studied distortion with unit-sum utility functions (which are normalized to sum to 1), and left a small asymptotic gap in the best possible distortion. Using tools from the theory of fair multi-winner elections, we propose the first voting rule which achieves the optimal distortion for unit-sum utilities. Our voting rule also achieves optimum distortion for a larger class of utilities, including unit-range and approval (0/1) utilities.We then take a similar worst-case approach to a quantitative measure of the fairness of a voting rule, called proportional fairness. Informally, it measures whether the influence of cohesive groups of agents on the voting outcome is proportional to the group size. We show that there is a voting rule which, without knowledge of the utilities, can achieve a (log m)-Approximation to proportional fairness. As a consequence of its proportional fairness, we show that this voting rule achieves (log m) distortion with respect to the Nash welfare, and selects a distribution that provides a (log m)-Approximation to the core, making it interesting for applications in participatory budgeting. For all three approximations, we show that (log m) is the best possible approximation.  © 2024 Copyright held by the owner/author(s).",Additional Key Words and PhrasesComputational social choice; core; distortion; Nash welfare; participatory budgeting; proportional fairness; social welfare; voting,Probability distributions; Additional key word and phrasescomputational social choice; Core; Key words; Nash welfare; Participatory budgeting; Proportional fairness; Social choice; Social welfare; Voting; Voting rules; Budget control
Topological Bounds on the Price of Anarchy of Clustering Games on Networks,2023,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181562375&doi=10.1145%2f3625689&partnerID=40&md5=6df9073a962f0d05efa7c44f0a5bb35e,"We consider clustering games in which the players are embedded into a network and want to coordinate (or anti-coordinate) their strategy with their neighbors. The goal of a player is to choose a strategy that maximizes her utility given the strategies of her neighbors. Recent studies show that even very basic variants of these games exhibit a large Price of Anarchy: A large inefficiency between the total utility generated in centralized outcomes and equilibrium outcomes in which players selfishly maximize their utility. Our main goal is to understand how structural properties of the network topology impact the inefficiency of these games. We derive topological bounds on the Price of Anarchy for different classes of clustering games. These topological bounds provide a more informative assessment of the inefficiency of these games than the corresponding worst-case Price of Anarchy bounds. More specifically, depending on the type of clustering game, our bounds reveal that the Price of Anarchy depends on the maximum subgraph density or the maximum degree of the graph. Among others, these bounds enable us to derive bounds on the Price of Anarchy for clustering games on Erdős-Rényi random graphs. Depending on the graph density, these bounds stand in stark contrast to the known worst-case Price of Anarchy bounds. Additionally, we characterize the set of distribution rules that guarantee the existence of a pure Nash equilibrium or the convergence of best-response dynamics. These results are of a similar spirit as the work of Gopalakrishnan et al. [19] and complement work of Anshelevich and Sekar [4]. © 2023 Association for Computing Machinery. All rights reserved.",clustering games; coordination games; Price of Anarchy; random graphs,Graph theory; Centralised; Clustering game; Clusterings; Coordination game; Different class; Maximum degree; Network topology; Price of anarchy; Random graphs; Subgraph densities; Network topology
Unified Fair Allocation of Goods and Chores via Copies,2023,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181537205&doi=10.1145%2f3618116&partnerID=40&md5=0962394c07801e8e6efa3bb4f3bcc622,"We consider fair allocation of indivisible items in a model with goods, chores, and copies, as a unified framework for studying: (1) the existence of EFX and other solution concepts for goods with copies; (2) the existence of EFX and other solution concepts for chores. We establish a tight relation between these issues via two conceptual contributions: First, a refinement of envy-based fairness notions that we term envy without commons (denoted EFXWC when applied to EFX). Second, a formal duality theorem relating the existence of a host of (refined) fair allocation concepts for copies to their existence for chores. We demonstrate the usefulness of our duality result by using it to characterize the existence of EFX for chores through the dual environment, as well as to prove EFX existence in the special case of leveled preferences over the chores. We further study the hierarchy among envy-freeness notions without commons and their α-MMS guarantees, showing, for example, that any EFXWC allocation guarantees at least 11                         4 -MMS for goods with copies. © 2023 Association for Computing Machinery. All rights reserved.",approximate envy-freeness; Fair division; resource allocation,Approximate envy-freeness; Duality theorems; Fair allocation; Fair divisions; Resources allocation; Solution concepts; Unified framework; Computation theory
Social Cost Analysis of Shared/Buy-in Computing Systems,2023,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181719412&doi=10.1145%2f3624355&partnerID=40&md5=9ca3e0b7391e7293e2c85506a3bc2c49,"Shared/buy-in computing systems offer users the option to select between buy-in and shared services. In such systems, idle buy-in resources are made available to other users for sharing. With strategic users, resource purchase and allocation in such systems can be cast as a non-cooperative game, whose corresponding Nash equilibrium does not necessarily result in the optimal social cost. In this study, we first derive the optimal social cost of the game in closed form, by casting it as a convex optimization problem and establishing related properties. Next, we derive a closed-form expression for the social cost at the Nash equilibrium, and show that it can be computed in linear time. We further show that the strategy profiles of users at the optimum and the Nash equilibrium are directly proportional. We measure the inefficiency of the Nash equilibrium through the price of anarchy, and show that it can be quite large in certain cases, e.g., when the operating expense ratio is low or when the distribution of user workloads is relatively homogeneous. To improve the efficiency of the system, we propose and analyze two subsidy policies, which are shown to converge using best-response dynamics. © 2023 Association for Computing Machinery. All rights reserved.",aggregative games; Computing clusters; efficiency; price of anarchy; pricing; social welfare,Computation theory; Computer games; Convex optimization; Cost benefit analysis; Game theory; Aggregative game; Computing clusters; Computing system; Cost analysis; Nash equilibria; Noncooperative game; Price of anarchy; Shared service; Social cost; Social welfare; Efficiency
Prophet Inequalities with Linear Correlations and Augmentations,2023,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181580888&doi=10.1145%2f3623273&partnerID=40&md5=36163da898465c9d3cb500055607a134,"In a classical online decision problem, a decision-maker who is trying to maximize her value inspects a sequence of arriving items to learn their values (drawn from known distributions) and decides when to stop the process by taking the current item. The goal is to prove a “prophet inequality”: that she can do approximately as well as a prophet with foreknowledge of all the values. In this work, we investigate this problem when the values are allowed to be correlated. Since nontrivial guarantees are impossible for arbitrary correlations, we consider a natural “linear” correlation structure introduced by Bateni et al. as a generalization of the common-base value model of Chawla et al. A key challenge is that threshold-based algorithms, which are commonly used for prophet inequalities, no longer guarantee good performance for linear correlations. We relate this roadblock to another “augmentations” challenge that might be of independent interest: many existing prophet inequality algorithms are not robust to slight increases in the values of the arriving items. We leverage this intuition to prove bounds (matching up to constant factors) that decay gracefully with the amount of correlation of the arriving items. We extend these results to the case of selecting multiple items by designing a new (1 + o(1))-approximation ratio algorithm that is robust to augmentations. © 2023 Association for Computing Machinery. All rights reserved.",online algorithms; posted price mechanisms; Robust stopping time algorithms,Approximation algorithms; Decision makers; Learn+; Linear correlation; On-line algorithms; Online decision problem; Posted price mechanism; Price mechanism; Robust stopping time algorithm; Stopping time; Time algorithms; Decision making
Editorial from the New Co-Editors-in-Chief of ACM Transactions on Economics and Computation,2023,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181563092&doi=10.1145%2f3631669&partnerID=40&md5=4db0bcd62a15b4a4dc1f213ef85cc21a,[No abstract available],,
An Auction Algorithm for Market Equilibrium with Weak Gross Substitute Demands,2023,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181695936&doi=10.1145%2f3624558&partnerID=40&md5=18b6bcf41619ee3212c121c799951664,We consider the Arrow–Debreu exchange market model under the assumption that the agents’ demands satisfy the weak gross substitutes (WGS) property. We present a simple auction algorithm that obtains an approximate market equilibrium for WGS demands assuming the availability of a price update oracle. We exhibit specific implementations of such an oracle for WGS demands with bounded price elasticities and for Gale demand systems. © 2023 Association for Computing Machinery. All rights reserved.,Auction algorithm; Fisher equilibrium; Gale equilibrium; Weak gross substitutes,Auction algorithms; Exchange markets; Fisher equilibriums; Gale equilibrium; Market equilibrium; Market model; Price-elasticity; Property; Simple++; Weak gross substitute; Combinatorial optimization
Reaching Individually Stable Coalition Structures,2023,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164245009&doi=10.1145%2f3588753&partnerID=40&md5=ef38e78e64990336a55fe35eff7ae938,"The formal study of coalition formation in multi-agent systems is typically realized in the framework of hedonic games, which originate from economic theory. The main focus of this branch of research has been on the existence and the computational complexity of deciding the existence of coalition structures that satisfy various stability criteria. The actual process of forming coalitions based on individual behavior has received little attention. In this article, we study the convergence of simple dynamics leading to stable partitions in a variety of established classes of hedonic games, including anonymous, dichotomous, fractional, and hedonic diversity games. The dynamics we consider is based on individual stability: an agent will join another coalition if she is better off and no member of the welcoming coalition is worse off. Our results are threefold. First, we identify conditions for the (fast) convergence of our dynamics. To this end, we develop new techniques based on the simultaneous usage of multiple intertwined potential functions and establish a reduction uncovering a close relationship between anonymous hedonic games and hedonic diversity games. Second, we provide elaborate counterexamples determining tight boundaries for the existence of individually stable partitions. Third, we study the computational complexity of problems related to the coalition formation dynamics. In particular, we settle open problems suggested by Bogomolnaia and Jackson, Brandl et al., and Boehmer and Elkind.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesCoalition formation; game dynamics; hedonic games; individual stability,Computational complexity; Economics; Game theory; Multi agent systems; Stability criteria; Additional key word and phrasescoalition formation; Coalition formations; Coalition structure; Economic theories; Formal studies; Game dynamic; Hedonic games; Individual stability; Key words; Stable coalition; Dynamics
A Framework for Computing the Nucleolus via Dynamic Programming,2023,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164235041&doi=10.1145%2f3580375&partnerID=40&md5=ee3ab384308fe4d35f4ebc2e50d9fb63,"This article defines a general class of cooperative games for which the nucleolus is efficiently computable. This class includes new members for which the complexity of computing their nucleolus was not previously known. We show that when the minimum excess coalition problem of a cooperative game can be formulated as a hypergraph dynamic program, its nucleolus is efficiently computable. This gives a general technique for designing efficient algorithms for computing the nucleolus of a cooperative game. This technique is inspired by a recent result of Pashkovich [27] on weighted voting games. However, our technique significantly extends beyond the capabilities of previous work. We demonstrate this by applying it to give an algorithm for computing the nucleolus of b-matching games in polynomial time on graphs of bounded treewidth.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesCombinatorial optimization; algorithmic game theory; dynamic programming,Computer games; Game theory; Polynomial approximation; Additional key word and phrasescombinatorial optimization; Algorithmic Game Theory; Cooperative game; Dynamic programs; General class; Hyper graph; Key words; New members; Optimisations; Weighted voting games; Dynamic programming
The Price of Anarchy of Two-Buyer Sequential Multiunit Auctions,2023,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164234891&doi=10.1145%2f3584864&partnerID=40&md5=ee13d6db85f6b42f545e45d038227a47,"We study the efficiency of first-/second-price sequential multiunit auctions with two buyers and complete information. Extending the primal-dual framework for obtaining efficiency bounds to this sequential setting, we obtain tight price of anarchy bounds. For general valuation functions, we show that the price of anarchy is exactly 1/T for auctions with T items for sale. For concave valuation functions, we show that the price of anarchy is bounded below by 1-1/e = 0.632. This bound is asymptotically tight as the number of items sold tends to infinity.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesSequential auctions; efficiency; multiunit auctions; price of anarchy,Commerce; Additional key word and phrasessequential auction; Complete information; Efficiency bounds; Key words; Multi-unit auction; Price of anarchy; Primal-dual; Valuation function; Efficiency
Catastrophe by Design in Population Games: A Mechanism to Destabilize Inefficient Locked-in Technologies,2023,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164240082&doi=10.1145%2f3583782&partnerID=40&md5=31c940d53b5a77584ef76f5783cf0f0d,"In multi-agent environments in which coordination is desirable, the history of play often causes lock-in at sub-optimal outcomes. Notoriously, technologies with significant environmental footprint or high social cost persist despite the successful development of more environmentally friendly and/or socially efficient alternatives. The displacement of the status quo is hindered by entrenched economic interests and network effects. To exacerbate matters, the standard mechanism design approaches based on centralized authorities with the capacity to use preferential subsidies to effectively dictate system outcomes are not always applicable to modern decentralized economies. What other types of mechanisms are feasible? In this article, we develop and analyze a mechanism that induces transitions from inefficient lock-ins to superior alternatives. This mechanism does not exogenously favor one option over another; instead, the phase transition emerges endogenously via a standard evolutionary learning model, Q-learning, where agents trade off exploration and exploitation. Exerting the same transient influence to both the efficient and inefficient technologies encourages exploration and results in irreversible phase transitions and permanent stabilization of the efficient one. On a technical level, our work is based on bifurcation and catastrophe theory, a branch of mathematics that deals with changes in the number and stability properties of equilibria. Critically, our analysis is shown to be structurally robust to significant and even adversarially chosen perturbations to the parameters of both our game and our behavioral model.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesPopulation games; bifurcations; catastrophes; efficiency; equilibria; mechanism design; Q-learning; status quo,Bifurcation (mathematics); Disasters; Environmental technology; Locks (fasteners); Machine design; Multi agent systems; Stability; Additional key word and phrasespopulation game; Bifurcation; Catastrophe; Equilibrium; Key words; Mechanism design; Multi-agent environment; Population games; Q-learning; Status quo; Economic and social effects
A Learning Framework for Distribution-Based Game-Theoretic Solution Concepts,2023,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164234910&doi=10.1145%2f3580374&partnerID=40&md5=4d64e1c69829872a25faa2eee89ac493,"The past few years have seen several works exploring learning economic solutions from data, including optimal auction design, function optimization, stable payoffs in cooperative games, and more. In this work, we provide a unified learning-theoretic methodology for modeling such problems and establish tools for determining whether a given solution concept can be efficiently learned from data. Our learning-theoretic framework generalizes a notion of function space dimension - the graph dimension - adapting it to the solution concept learning domain. We identify sufficient conditions for efficient solution learnability and show that results in existing works can be immediately derived using our methodology. Finally, we apply our methods in other economic domains, yielding learning variants of competitive equilibria and Condorcet winners.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesSolution concepts; economic learning; sample complexity,Game theory; Additional key word and phrasessolution concept; Auction design; Economic learning; Economic solutions; Game-theoretic; Key words; Learning frameworks; Optimal auction; Sample complexity; Solution concepts; Learning systems
Sequential and Swap Mechanisms for Public Housing Allocation with Quotas and Neighbourhood-based Utilities,2023,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153255577&doi=10.1145%2f3569704&partnerID=40&md5=087a4fb77c6ed2d94c4314b81f920d29,"We consider the problem of allocating indivisible items to agents where both agents and items are partitioned into disjoint groups. Following previous works on public housing allocation, each item (or house) belongs to a block (or building) and each agent is assigned a type (e.g., ethnicity group). The allocation problem consists in assigning at most one item to each agent in a good way while respecting diversity constraints. Based on Schelling's seminal work, we introduce a generic individual utility function where the welfare of an agent not only relies on her preferences over the items but also takes into account the fraction of agents of her own type in her own block. In this context, we investigate the issue of stability, understood here as the absence of mutually improving swaps, and we define the cost of requiring it. Then, we study the behaviour of two existing allocation mechanisms: an adaptation of the sequential mechanism used in Singapore and a distributed procedure based on mutually improving swaps of items. We first present the theoretical properties of these two allocation mechanisms, and we then compare their performances in practice through an experimental study.  © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",computational social choice; distributed allocation mechanisms; diversity constraints; Multiagent resource allocation,Multi agent systems; Allocation mechanism; Allocation problems; Computational social choices; Distributed allocation; Distributed allocation mechanism; Diversity constraint; Multiagent resource allocation; Neighbourhood; Public housing; Schelling; Housing
Fairness Maximization among Offline Agents in Online-Matching Markets,2023,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152619442&doi=10.1145%2f3569705&partnerID=40&md5=9c31a8d1b2a52c627ba96638924bd637,"Online matching markets (OMMs) are commonly used in today's world to pair agents from two parties (whom we will call offline and online agents) for mutual benefit. However, studies have shown that the algorithms making decisions in these OMMs often leave disparities in matching rates, especially for offline agents. In this article, we propose online matching algorithms that optimize for either individual or group-level fairness among offline agents in OMMs. We present two linear-programming (LP) based sampling algorithms, which achieve competitive ratios at least 0.725 for individual fairness maximization and 0.719 for group fairness maximization. We derive further bounds based on fairness parameters, demonstrating conditions under which the competitive ratio can increase to 100%. There are two key ideas helping us break the barrier of 1-1/∼ 63.2% for competitive ratio in online matching. One is boosting, which is to adaptively re-distribute all sampling probabilities among only the available neighbors for every arriving online agent. The other is attenuation, which aims to balance the matching probabilities among offline agents with different mass allocated by the benchmark LP. We conduct extensive numerical experiments and results show that our boosted version of sampling algorithms are not only conceptually easy to implement but also highly effective in practical instances of OMMs where fairness is a concern.  © 2022 Association for Computing Machinery.",Fairness maximization; online matching markets,Commerce; Learning algorithms; Software agents; Competitive ratio; Fairness maximization; Linear-programming; Making decision; Matchings; Mutual benefit; Offline; On-line matching; Online matching market; Sampling algorithm; Linear programming
Stable Matchings with Restricted Preferences: Structure and Complexity,2023,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149047108&doi=10.1145%2f3565558&partnerID=40&md5=0bd87ec7a4bd2db4531cc13d3e7cc86d,"In the stable marriage (SM) problem, there are two sets of agents-traditionally referred to as men and women-and each agent has a preference list that ranks (a subset of) agents of the opposite sex. The goal is to find a matching between men and women that is stable in the sense that no man-woman pair mutually prefers each other to their assigned partners. In a seminal work, Gale and Shapley [16] showed that stable matchings always exist and described an efficient algorithm for finding one. Irving and Leather [24] defined the rotation poset of an SM instance and showed that it determines the structure of the set of stable matchings of the instance. They further showed that every finite poset can be realized as the rotation poset of some SM instance. Consequently, many problems-such as counting stable matchings and finding certain ""fair""stable matchings-are computationally intractable (NP-hard) in general. In this article, we consider SM instances in which certain restrictions are placed on the preference lists. We show that three natural preference models-k-bounded, k-Attribute, and (k1, k2)-list-can realize arbitrary rotation posets for constant values of k. Hence, even in these highly restricted preference models, many stable matching problems remain intractable. In contrast, we show that for any fixed constant k, the rotation posets of k-range instances are highly restricted. As a consequence, we show that exactly counting and uniformly sampling stable matchings, finding median, sex-equal, and balanced stable matchings, are fixed-parameter tractable when parameterized by the range of the instance. Thus, these problems can be solved in polynomial time on instances of the k-range model for any fixed constant k.  © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Parameterized algorithms; parameterized complexity; Restricted preferences; Rotation poset; Stable matchings,Computational complexity; Parameter estimation; Parameterization; Polynomial approximation; Set theory; Parameterized algorithm; Parameterized complexity; Preference lists; Preference models; Preference structures; Restricted preference; Rotation poset; Stable marriage problem; Stable marriages; Stable matching; Rotation
Two Strongly Truthful Mechanisms for Three Heterogeneous Agents Answering One Question,2023,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152623176&doi=10.1145%2f3565560&partnerID=40&md5=a93aa140c2b7720a0173f88a4113bb59,"Peer prediction mechanisms incentivize self-interested agents to truthfully report their signals even in the absence of verification by comparing agents' reports with their peers. We propose two new mechanisms, Source and Target Differential Peer Prediction, and prove very strong guarantees for a very general setting. Our Differential Peer Prediction mechanisms are strongly truthful: Truth-telling is a strict Bayesian Nash equilibrium. Also, truth-telling pays strictly higher than any other equilibria, excluding permutation equilibria, which pays the same amount as truth-telling. The guarantees hold for asymmetric priors among agents, which the mechanisms need not know (detail-free) in the single question setting. Moreover, they only require three agents, each of which submits a single item report: two report their signals (answers), and the other reports her forecast (prediction of one of the other agent's reports). Our proof technique is straightforward, conceptually motivated, and turns on the logarithmic scoring rule's special properties. Moreover, we can recast the Bayesian Truth Serum mechanism [20] into our framework. We can also extend our results to the setting of continuous signals with a slightly weaker guarantee on the optimality of the truthful equilibrium.  © 2022 Copyright held by the owner/author(s).",log scoring rule; Peer prediction; prediction market,Heterogeneous agents; Log scoring rule; New mechanisms; Peer prediction; Prediction markets; Prediction mechanisms; Scoring rules; Self-interested agents; Truth-telling; Truthful mechanisms; Forecasting
Surrogate Scoring Rules,2023,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149032406&doi=10.1145%2f3565559&partnerID=40&md5=453cf58c77366463cbfdf6d8a5bfaea4,"Strictly proper scoring rules (SPSR) are incentive compatible for eliciting information about random variables from strategic agents when the principal can reward agents after the realization of the random variables. They also quantify the quality of elicited information, with more accurate predictions receiving higher scores in expectation. In this article, we extend such scoring rules to settings in which a principal elicits private probabilistic beliefs but only has access to agents' reports. We name our solution Surrogate Scoring Rules (SSR). SSR is built on a bias correction step and an error rate estimation procedure for a reference answer defined using agents' reports. We show that, with a little information about the prior distribution of the random variables, SSR in a multi-Task setting recover SPSR in expectation, as if having access to the ground truth. Therefore, a salient feature of SSR is that they quantify the quality of information despite the lack of ground truth, just as SPSR do for the setting with ground truth. As a by-product, SSR induce dominant uniform strategy truthfulness in reporting. Our method is verified both theoretically and empirically using data collected from real human forecasters. © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesStrictly proper scoring rules; dominant strategy incentive compatibility; information calibration; information elicitation without verification; peer prediction,Additional key word and phrasesstrictly proper scoring rule; Dominant strategy; Dominant strategy incentive compatibility; Incentive compatibility; Information calibration; Information elicitation without verification; Key words; Peer prediction; Proper scoring rules; Scoring rules; Random variables
Robust Revenue Maximization under Minimal Statistical Information,2023,ACM Transactions on Economics and Computation,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149041183&doi=10.1145%2f3546606&partnerID=40&md5=8fd5ab6f291e4c1a04c44e7705a21e99,"We study the problem of multi-dimensional revenue maximization when selling m items to a buyer that has additive valuations for them, drawn from a (possibly correlated) prior distribution. Unlike traditional Bayesian auction design, we assume that the seller has a very restricted knowledge of this prior: They only know the mean μj and an upper bound σj on the standard deviation of each item's marginal distribution. Our goal is to design mechanisms that achieve good revenue against an ideal optimal auction that has full knowledge of the distribution in advance. Informally, our main contribution is a tight quantification of the interplay between the dispersity of the priors and the aforementioned robust approximation ratio. Furthermore, this can be achieved by very simple selling mechanisms.More precisely, we show that selling the items via separate price lotteries achieves an O(log r) approximation ratio where r = maxj(σj/μj) is the maximum coefficient of variation across the items. To prove the result, we leverage a price lottery for the single-item case. If forced to restrict ourselves to deterministic mechanisms, this guarantee degrades to O(r2). Assuming independence of the item valuations, these ratios can be further improved by pricing the full bundle. For the case of identical means and variances, in particular, we get a guarantee of O(log (r/m)) that converges to optimality as the number of items grows large. We demonstrate the optimality of the preceding mechanisms by providing matching lower bounds. Our tight analysis for the single-item deterministic case resolves an open gap from the work of Azar and Micali (ITCS'13).As a by-product, we also show how one can directly use our upper bounds to improve and extend previous results related to the parametric auctions of Azar et al. (SODA'13).  © 2022 Association for Computing Machinery.",Additional Key Words and PhrasesPricing; Optimal auctions; Parametric auctions; Revenue maximization; Robust optimization,Approximation theory; Commerce; Costs; Sales; Additional key word and phrasespricing; Approximation ratios; Key words; Optimal auction; Optimality; Parametric auction; Revenue maximization; Robust optimization; Statistical information; Upper Bound; Optimization
